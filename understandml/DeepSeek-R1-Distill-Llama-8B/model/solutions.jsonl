{"task_id": 0, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {}\n    links['identity'] = {'link': lambda x: x, 'inv_link': lambda x: x, 'derivative': lambda x: 1}\n    links['log'] = {'link': lambda x: np.log(x), 'inv_link': lambda x: np.exp(x), 'derivative': lambda x: 1.0 / x}\n    links['logit'] = {'link': lambda x: np.log(x / (1 - x)), 'inv_link': lambda x: np.log(x / (1 - x)), 'derivative': lambda x: 1.0 / (x * (1 - x))}\n    return links"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 7, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    if not transactions:\n        return {}\n    N = len(transactions)\n    required_support = math.ceil(minsup * N)\n    transactions_sets = [set(t) for t in transactions]\n    all_items = set()\n    for t in transactions:\n        all_items.update(t)\n    all_items = list(all_items)\n    result = {}\n    for k in range(1, len(all_items) + 1):\n        for combo in combinations(all_items, k):\n            count = 0\n            for t_set in transactions_sets:\n                if all((item in t_set for item in combo)):\n                    count += 1\n            if count >= required_support:\n                sorted_combo = tuple(sorted(combo))\n                result[sorted_combo] = count\n    return result"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    if k < 1 or k > n_samples:\n        raise ValueError('k must be between 1 and the number of samples')\n    old_centers = data[:k]\n    for _ in range(max_iter):\n        clusters = np.zeros(len(data), dtype=int)\n        for (i, sample) in enumerate(data):\n            min_dist = float('inf')\n            min_center = 0\n            for (j, center) in enumerate(old_centers):\n                dist = np.sum((sample - center) ** 2)\n                if dist < min_dist:\n                    min_dist = dist\n                    min_center = j\n            clusters[i] = min_center\n        empty_clusters = []\n        for j in range(k):\n            if clusters.count(j) == 0:\n                empty_clusters.append(j)\n        for j in empty_clusters:\n            idx = np.random.randint(len(data))\n            old_centers[j] = data[idx]\n        new_centers = []\n        for j in range(k):\n            cluster_samples = data[clusters == j]\n            new_centers.append(np.mean(cluster_samples, axis=0))\n        max_diff = 0\n        for i in range(k):\n            diff = np.abs(old_centers[i] - new_centers[i])\n            if diff > max_diff:\n                max_diff = diff\n        if max_diff < epsilon:\n            break\n        old_centers = new_centers\n    rounded_centers = [[round(c, 4) for c in center] for center in old_centers]\n    cluster_labels = clusters\n    return (rounded_centers, cluster_labels)"}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    (n_user, n_item) = data.shape\n    if n_user == 0:\n        return []\n    item_users = [set() for _ in range(n_item)]\n    for user in range(n_user):\n        for i in range(n_item):\n            if data[user, i] > 0:\n                item_users[i].add(user)\n    S = np.zeros((n_item, n_item))\n    for i in range(n_item):\n        for j in range(i + 1, n_item):\n            common_users = item_users[i].intersection(item_users[j])\n            if not common_users:\n                S[i, j] = 0.0\n                S[j, i] = 0.0\n            else:\n                v1 = [data[u, i] for u in common_users]\n                v2 = [data[u, j] for u in common_users]\n                if criterion == 'cosine':\n                    mean_v1 = np.mean(v1)\n                    mean_v2 = np.mean(v2)\n                    if np.std(v1) > 0.001:\n                        v1_centered = v1 - mean_v1\n                    else:\n                        v1_centered = v1\n                    if np.std(v2) > 0.001:\n                        v2_centered = v2 - mean_v2\n                    else:\n                        v2_centered = v2\n                    dot = np.dot(v1_centered, v2_centered)\n                    len_v1 = np.linalg.norm(v1_centered)\n                    len_v2 = np.linalg.norm(v2_centered)\n                    if len_v1 == 0 or len_v2 == 0:\n                        sim = 0.0\n                    else:\n                        sim = dot / (len_v1 * len_v2)\n                    S[i, j] = sim\n                    S[j, i] = sim\n                elif criterion == 'pearson':\n                    correlation = np.corrcoef(v1, v2)[0, 0]\n                    S[i, j] = correlation\n                    S[j, i] = correlation\n    rated_items = np.where(data[user_ind, :] > 0)[0]\n    if len(rated_items) == n_item:\n        return []\n    r = data[user_ind, rated_items]\n    unrated_items = [t for t in range(n_item) if t not in rated_items]\n    scores = []\n    for t in unrated_items:\n        sum_product = 0.0\n        sum_abs = 0.0\n        for i in rated_items:\n            sum_product += r[i] * S[t, i]\n            sum_abs += abs(S[t, i])\n        if sum_abs == 0:\n            score = 0.0\n        else:\n            score = sum_product / sum_abs\n        scores.append((score, t))\n    scores.sort(key=lambda x: (-x[0], x[1]))\n    result = [t for (score, t) in scores[:k]]\n    return result"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    x = np.asarray(x)\n    sig_x = 1 / (1 + np.exp(-x))\n    grad_x = sig_x * (1 - sig_x)\n    rounded_sig = np.round(sig_x, 4)\n    rounded_grad = np.round(grad_x, 4)\n    if x.ndim == 0:\n        return (rounded_sig.item(), rounded_grad.item())\n    else:\n        return (rounded_sig.tolist(), rounded_grad.tolist())"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters:\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns:\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly 1 or -1.\n    \"\"\"\n    kernel = np.exp(-gamma * (X_train - X_test) @ (X_train - X_test).T)\n    scores = alpha * y_train @ kernel + b\n    predicted_labels = np.sign(scores).astype(int)\n    return predicted_labels.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns (-1, -1).\n    \"\"\"\n    m = len(X)\n    if m == 0:\n        return (-1, -1)\n    n = len(X[0])\n    if k < 1 or k > min(m, n):\n        return (-1, -1)\n    X_array = np.array(X)\n    (U, S, V) = np.svd(X_array)\n    U_k = U[:, :k]\n    S_k = S[:k, :]\n    V_k = V\n    X_hat = np.dot(U_k, np.dot(S_k, V.T))\n    mse = np.mean(np.square(X_array - X_hat))\n    X_hat_rounded = np.round(X_hat, 4)\n    mse_rounded = np.round(mse, 4)\n    X_hat_list = X_hat_rounded.tolist()\n    return (X_hat_list, mse_rounded)"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Generate a binary tensor with 0.0 and 1.0 values based on the given shape and sparsity.\n\n    Parameters:\n        shape (tuple[int, ...]): The shape of the output tensor.\n        sparsity (float, optional): Probability of generating 1.0. Must be in [0, 1]. Default is 0.5.\n        seed (int, optional): Seed for reproducibility. If provided, np.random.seed is called. Default is None.\n\n    Returns:\n        numpy.ndarray: A tensor of float dtype with 0.0 and 1.0 values. If sparsity is invalid, returns a float array with -1.0 in the same shape.\n    \"\"\"\n    if not 0.0 <= sparsity <= 1.0:\n        return np.full(shape, -1.0, dtype=np.float64)\n    if seed is not None:\n        np.random.seed(seed)\n    rand_arr = np.random.rand(*shape)\n    threshold = 1.0 - sparsity\n    binary_arr = np.where(rand_arr >= threshold, 1.0, 0.0)\n    return binary_arr"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger values.\n    \"\"\"\n    n = data.shape[0]\n    sorted_indices = np.argpartition(data[:, d])\n    median_value = data[sorted_indices[n // 2]]\n    pivot = sorted_indices[n // 2]\n    left = []\n    right = []\n    for i in range(n):\n        if data[i][d] < median_value:\n            left.append(i)\n        elif data[i][d] > median_value:\n            right.append(i)\n    return (pivot, left, right)"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool=False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    if x.ndim != 1:\n        raise AssertionError('x must be 1-D')\n    if stride < 1:\n        raise AssertionError('stride must be >= 1')\n    if x.size < frame_width:\n        raise AssertionError('x must be at least frame_width in length')\n    n_frames = (x.size - frame_width) // stride + 1\n    view = as_strided(x, shape=(n_frames, frame_width), strides=(stride, 1))\n    view.flags.writeable = writeable\n    return view"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef is_stochastic(X: list[list[float]] | 'np.ndarray') -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    if isinstance(X, np.ndarray):\n        if X.ndim < 2:\n            return False\n        if not np.all((X >= 0) & (X <= 1)):\n            return False\n        row_sums = np.sum(X, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-08):\n            return False\n        return True\n    else:\n        if not all((isinstance(row, list) for row in X)):\n            return False\n        row_length = len(X[0])\n        for row in X:\n            if len(row) != row_length:\n                return False\n        if len(X) == 0:\n            return False\n        try:\n            X_np = np.array(X)\n        except:\n            return False\n        if not np.issubdtype(X_np.dtype, np.floating):\n            return False\n        if not np.all((X_np >= 0) & (X_np <= 1)):\n            return False\n        row_sums = np.sum(X_np, axis=1)\n        if not np.all(np.abs(row_sums - 1) < 1e-08):\n            return False\n        return True"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef rms_prop(X: np.ndarray, y: np.ndarray, epsilon: float=0.0001, max_iter: int=10000, eta: float=0.01, rho: float=0.9, batch_size: int=32, eps_station: float=1e-08) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n = X.shape[0]\n    d = X.shape[1]\n    w = np.zeros(d, dtype=np.float64)\n    s = np.zeros(d, dtype=np.float64)\n    current_pos = 0\n    for iteration in range(max_iter):\n        if n < batch_size:\n            batch_x = X\n            batch_y = y\n        else:\n            start = current_pos\n            end = start + batch_size\n            if end > n:\n                take_end = n - start\n                take_start = batch_size - take_end\n                if take_start < 0:\n                    take_start = 0\n                batch_x = X[start:start + take_end] + X[:take_start]\n                batch_y = y[start:start + take_end] + y[:take_start]\n            else:\n                batch_x = X[start:end]\n                batch_y = y[start:end]\n            current_pos = end % n\n        Xw = batch_x @ w\n        residuals = Xw - batch_y\n        g = 1.0 / n * np.dot(X.T, residuals)\n        g_squared = g ** 2\n        s = rho * s + (1 - rho) * g_squared\n        sqrt_s = np.sqrt(s)\n        denominator = sqrt_s + eps_station\n        w = w - eta * g / denominator\n        norm_g = np.linalg.norm(g)\n        if norm_g < epsilon:\n            break\n        if iteration == max_iter - 1:\n            break\n    w_rounded = np.round(w, 4)\n    return w_rounded.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef softplus(x, deriv: bool=False):\n    \"\"\"Compute the SoftPlus activation function and its derivative (sigmoid).\"\"\"\n    if isinstance(x, (int, float)):\n        if deriv:\n            return round(1.0 / (1 + np.exp(-x)), 4)\n        elif x > 0:\n            return round(x + np.log(1 + np.exp(-x)), 4)\n        else:\n            return round(np.log(1 + np.exp(x)), 4)\n    else:\n        x = np.array(x)\n        if deriv:\n            sig = 1 / (1 + np.exp(-x))\n            sig_rounded = np.round(sig, 4)\n            return sig_rounded.tolist()\n        else:\n            exp_x = np.exp(x)\n            log_val = np.where(x > 0, x + np.log(1 + np.exp(-x)), np.log(1 + exp_x))\n            log_val_rounded = np.round(log_val, 4)\n            return log_val_rounded.tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    The function builds an N-gram count table from the *corpus* and returns the total log-probability\n    of the *sequence* under the resulting unsmoothed language model. If any required count is zero,\n    it returns ``float('-inf')``.\n\n    Args:\n        corpus: List of word tokens.\n        sequence: List of word tokens to compute the log-probability for.\n        N: Order of the N-gram model.\n\n    Returns:\n        The total log-probability rounded to 4 decimal places, or ``float('-inf')`` if any required count is zero.\n    \"\"\"\n    if N == 0:\n        return float('-inf')\n    if N == 1:\n        ngram_counts = defaultdict(int)\n        for word in corpus:\n            ngram_counts[word] += 1\n    else:\n        ngram_counts = defaultdict(lambda : defaultdict(int))\n        for i in range(len(corpus) - N + 1):\n            current_gram = tuple(corpus[i:i + N])\n            prefix = tuple(corpus[i:i + N - 1])\n            ngram_counts[prefix][current_gram] += 1\n    total = 0.0\n    if N == 1:\n        total_words = len(corpus)\n        for word in sequence:\n            if word not in ngram_counts or ngram_counts[word] == 0:\n                return float('-inf')\n            prob = math.log(ngram_counts[word] / total_words)\n            total += prob\n    else:\n        for i in range(len(sequence) - N + 1):\n            window = tuple(sequence[i:i + N])\n            prefix = tuple(sequence[i:i + N - 1])\n            if prefix not in ngram_counts or window not in ngram_counts[prefix]:\n                return float('-inf')\n            count_window = ngram_counts[prefix][window]\n            count_prefix = sum(ngram_counts[prefix].values())\n            if count_window == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = math.log(count_window / count_prefix)\n            total += prob\n    return round(total, 4)"}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to the nearest 6th decimal.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    N = len(A)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n    beta_prev = [1.0 for _ in range(N)]\n    for t in range(T - 2, -1, -1):\n        current_obs = obs[t + 1]\n        beta_current = [0.0 for _ in range(N)]\n        for i in range(N):\n            for j in range(N):\n                beta_current[i] += A[i][j] * B[j][current_obs] * beta_prev[j]\n        beta_prev = beta_current\n    result = 0.0\n    for i in range(N):\n        result += pi[i] * B[i][obs[0]] * beta_prev[i]\n    return round(result, 6)"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    n = len(S)\n    if n == 0:\n        return -1\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    m = len(B[0]) if B else 0\n    for row in B:\n        if len(row) != m:\n            return -1\n    for obs in observations:\n        if not isinstance(obs, int):\n            return -1\n        if obs < 0 or obs >= m:\n            return -1\n    if len(observations) == 0:\n        return -1\n    current_state = np.array(S, dtype=np.float64)\n    for obs in observations:\n        current_state = current_state.dot(A)\n        current_state = current_state.dot(B)\n    total_probability = np.sum(current_state)\n    return round(total_probability, 4)"}
{"task_id": 75, "completion_id": 0, "solution": "def knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    if not isinstance(k, int) or k < 1 or k > len(X_train):\n        return -1\n    if len(X_train) == 0:\n        return -1\n    result = []\n    for x_test in X_test:\n        distances = []\n        for (x_train, y) in zip(X_train, y_train):\n            dist = sum(((x_test[i] - x_train[i]) ** 2 for i in range(len(x_test))))\n            distances.append((dist, y))\n        distances.sort()\n        k_neighbors = distances[:k]\n        y_values = [y for (d, y) in k_neighbors]\n        freq = {}\n        for y in y_values:\n            freq[y] = freq.get(y, 0) + 1\n        max_freq = max(freq.values())\n        candidates = [y for y in freq if freq[y] == max_freq]\n        prediction = min(candidates)\n        result.append(prediction)\n    return result"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    epsilon = 1e-10\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    sum_loss = np.sum(y * np.log(y_pred + epsilon))\n    L = -sum_loss\n    return round(L, 4)"}
{"task_id": 77, "completion_id": 0, "solution": "import numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    L = len(parameters) // 2\n    A_prev = X\n    caches = []\n    for i in range(1, L + 1):\n        W = parameters[f'W{i}']\n        b = parameters[f'b{i}']\n        Z = np.dot(W, A_prev) + b\n        if i < L:\n            A = np.maximum(0, Z)\n            caches.append((Z, 'ReLU'))\n        else:\n            A = 1 / (1 + np.exp(-Z))\n            caches.append((Z, 'sigmoid'))\n        A_prev = A\n    AL = np.round(A, 4)\n    return AL.tolist()"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    \u03b5 = 1e-15\n    A2_clipped = np.clip(A2, \u03b5, 1 - \u03b5)\n    y_part = Y * np.log(A2_clipped)\n    one_minus_y_part = (1 - Y) * np.log(1 - A2_clipped)\n    cost = np.sum(y_part + one_minus_y_part)\n    return round(cost, 6)"}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample = [tree[i] for tree in predictions]\n        label_type = type(sample[0])\n        counts = {}\n        for label in sample:\n            counts[label] = counts.get(label, 0) + 1\n        max_count = max(counts.values())\n        tied_labels = [label for (label, cnt) in counts.items() if cnt == max_count]\n        if len(tied_labels) == 1:\n            chosen_label = tied_labels[0]\n        else:\n            if isinstance(label_type, (int, float)):\n                sorted_labels = sorted(tied_labels, key=lambda x: float(x))\n            else:\n                sorted_labels = sorted(tied_labels)\n            chosen_label = sorted_labels[0]\n        result.append(chosen_label)\n    return result"}
{"task_id": 96, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a raw text string into a fixed-length list of integer token ids.\"\"\"\n    encoded = []\n    for char in text:\n        if char in vocabulary:\n            encoded.append(vocabulary[char])\n        else:\n            encoded.append(0)\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer ids back to text, ignoring padding tokens (0).\"\"\"\n    filtered_indices = [i for i in indices if i != 0]\n    return ''.join([inverse_vocab[i] for i in filtered_indices])\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Runs the model on the input text and returns the decoded prediction.\"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    encoded_array = np.array(encoded, dtype=np.int64).reshape(1, -1)\n    predicted = model.predict(encoded_array)\n    predicted_ids = np.argmax(predicted, axis=1)\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    outputs = []\n    for example in examples:\n        prediction_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        prediction = ''.join(prediction_chars)\n        print(f'input: {example}')\n        print(f'output: {prediction}')\n        outputs.append(prediction)\n    return outputs"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels -1 or 1.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf: Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    m = X_train.shape[0]\n    if m == 0:\n        return []\n    n = X_train.shape[1]\n    weights = np.ones(m) / m\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature = -1\n        best_split = 0\n        for j in range(n):\n            sorted_indices = np.argsort(X_train[:, j])\n            sorted_weights = [weights[i] for i in sorted_indices]\n            min_error_j = float('inf')\n            best_split_j = 0\n            for i in range(len(sorted_weights) - 1):\n                left_y = y_train[sorted_indices[:i + 1]]\n                left_weights = sorted_weights[:i + 1]\n                right_y = y_train[sorted_indices[i + 1:]]\n                right_weights = sorted_weights[i + 1:]\n                error = 0.0\n                for k in range(i + 1):\n                    if left_y[k] == 1:\n                        error += left_weights[k]\n                for k in range(len(right_weights)):\n                    if right_y[k] == -1:\n                        error += right_weights[k]\n                if error < min_error_j:\n                    min_error_j = error\n                    best_split_j = i + 1\n            if min_error_j < min_error:\n                min_error = min_error_j\n                best_feature = j\n                best_split = best_split_j\n        threshold = X_train[sorted_indices[best_split - 1], best_feature]\n        epsilon_t = min_error\n        alpha_t = 0.5 * np.log((1 - epsilon_t) / epsilon_t)\n        classifiers.append((alpha_t, threshold))\n        for i in range(m):\n            if (X_train[i, best_feature] >= threshold) != y_train[i]:\n                weights[i] *= np.exp(2 * alpha_t)\n    predictions = []\n    for x in X_test:\n        total = 0.0\n        for (alpha, threshold) in classifiers:\n            if x >= threshold:\n                total += alpha\n            else:\n                total -= alpha\n        if total >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components == 0:\n        return []\n    (m, d) = X.shape\n    if d == 0:\n        return []\n    mu = np.mean(X, axis=0)\n    S_W = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = y == c\n        mu_c = np.mean(X[mask], axis=0)\n        sum_ww = np.cov(X[mask], rowvar=False)\n        S_W += sum_ww\n    S_B = np.zeros((d, d), dtype=np.float64)\n    for c in np.unique(y):\n        mask = y == c\n        mu_c = np.mean(X[mask], axis=0)\n        n_c = np.sum(mask)\n        diff = mu_c - mu\n        S_B += n_c * (diff[:, np.newaxis] @ diff[:, np.newaxis].T)\n    try:\n        A = np.linalg.pinv(S_W) @ S_B\n    except np.linalg.LinAlgError:\n        A = np.linalg.pinv(S_W) @ S_B\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A)\n    eigenvalues = eigenvalues[::-1]\n    eigenvectors = eigenvectors[::-1]\n    for i in range(len(eigenvectors)):\n        vec = eigenvectors[i]\n        first_non_zero = np.argmax(np.abs(vec) > 1e-09)\n        if first_non_zero != 0:\n            if vec[first_non_zero] < 0:\n                eigenvectors[i] = -vec\n    W = eigenvectors[:n_components]\n    projected = X @ W\n    projected_rounded = np.round(projected, 4)\n    return projected_rounded.tolist()"}
{"task_id": 165, "completion_id": 0, "solution": "import numpy as np\ndef hamming_distance(x: list[int] | 'np.ndarray', y: list[int] | 'np.ndarray') -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    x = np.array(x) if isinstance(x, list) else x\n    y = np.array(y) if isinstance(y, list) else y\n    if x.size != y.size:\n        return -1\n    if x.size == 0:\n        return 0.0\n    diffs = x != y\n    sum_diffs = np.sum(diffs)\n    distance = sum_diffs / x.size\n    return round(distance, 4)"}
{"task_id": 178, "completion_id": 0, "solution": "import numpy as np\ndef particle_swarm_optimisation(n_dims: int, num_particles: int, num_iterations: int, seed: int=1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    if not (isinstance(n_dims, int) and isinstance(num_particles, int) and isinstance(num_iterations, int)):\n        return -1\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    rng = np.random.default_rng(seed)\n    x = rng.uniform(-1, 1, size=(num_particles, n_dims))\n    v = np.zeros_like(x)\n    p_best = x.copy()\n    f_val = np.sum(x ** 2)\n    g_best = f_val\n    current_min = f_val\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    for _ in range(num_iterations):\n        r1 = rng.uniform(0, 1, size=num_particles)\n        r2 = rng.uniform(0, 1, size=num_particles)\n        v_new = w * v + c1 * r1 * (p_best - x) + c2 * r2 * (g_best - x)\n        x_new = x + v_new\n        x_new = np.clip(x_new, -1, 1)\n        f_new = np.sum(x_new ** 2)\n        for i in range(num_particles):\n            if f_new[i] < p_best[i]:\n                p_best[i] = x_new[i]\n        g_best = np.min(p_best)\n        current_min = g_best\n        x = x_new\n        v = v_new\n    return round(current_min, 4)"}
