{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b:\n        return -1\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    np_array = np.array(a)\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = np_array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        return [sum((matrix[i][j] for i in range(len(matrix)))) / len(matrix) for j in range(num_columns)]\n    else:\n        raise ValueError(\"Invalid mode. Use 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": ""}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = discriminant ** 0.5\n    lambda1 = (trace + sqrt_discriminant) / 2\n    lambda2 = (trace - sqrt_discriminant) / 2\n    return [lambda1, lambda2] if lambda1 >= lambda2 else [lambda2, lambda1]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    result = T_inv @ A_np @ S_np\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_a = d / determinant\n    inv_b = -b / determinant\n    inv_c = -c / determinant\n    inv_d = a / determinant\n    return [[inv_a, inv_b], [inv_c, inv_d]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the linear system Ax = b using the Jacobi method for n iterations.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of shape (m, m)\n    b (np.ndarray): Right-hand side vector of shape (m,)\n    n (int): Number of iterations\n    \n    Returns:\n    list: Approximate solution x after n iterations, rounded to 4 decimal places\n    \"\"\"\n    m = A.shape[0]\n    x = np.zeros(m)\n    for _ in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            x_new[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix.')\n    theta = 0.0\n    max_iterations = 1000\n    tolerance = 1e-08\n    for _ in range(max_iterations):\n        off_diag = A[0, 1] + A[1, 0]\n        if abs(off_diag) < tolerance:\n            break\n        tan_theta = 2 * A[0, 1] / (A[0, 0] - A[1, 1])\n        theta = np.arctan(tan_theta) / 2\n        c = np.cos(theta)\n        s = np.sin(theta)\n        R = np.array([[c, -s], [s, c]])\n        A = R.T @ A @ R\n    sigma1 = np.sqrt(A[0, 0])\n    sigma2 = np.sqrt(A[1, 1])\n    return (round(sigma1, 4), round(sigma2, 4))"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        return mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) - mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) + mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0])\n    det = 0\n    for col in range(4):\n        minor = [[matrix[row][col_idx] for col_idx in range(4) if col_idx != col] for row in range(1, 4)]\n        sign = (-1) ** col\n        det += sign * matrix[0][col] * determinant_3x3(minor)\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_np\n    return [round(coeff, 4) for coeff in theta]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        y_pred = X.dot(theta)\n        error = y_pred - y\n        gradient = 1 / len(y) * X.T.dot(error)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_list = np.round(standardized_data, 4).tolist()\n    min_max_list = np.round(min_max_scaled_data, 4).tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target vector of shape (n_samples,)\n        k (int): Number of folds\n        shuffle (bool): Whether to shuffle the data before splitting\n        random_seed (int or None): Random seed for reproducibility\n\n    Returns:\n        list of tuples: Each tuple contains (train_indices, test_indices)\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    folds = []\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_indices, test_indices))\n        start = end\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigen_values, eigen_vectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigen_values)[::-1]\n    sorted_eigen_vectors = eigen_vectors[:, sorted_indices]\n    principal_components = sorted_eigen_vectors[:, :k]\n    return [list(map(lambda x: round(x, 4), principal_components[:, i])) for i in range(k)]"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    best_attr = find_best_split(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    values = set((example[best_attr] for example in examples))\n    for value in values:\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attrs = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"Calculate the entropy of the target attribute in the examples.\"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    total = len(examples)\n    return -sum((count / total * math.log2(count / total) for count in counts.values()))\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain of splitting on the given attribute.\"\"\"\n    total_entropy = entropy(examples, target_attr)\n    attr_values = set((example[attr] for example in examples))\n    weighted_entropy = 0\n    for value in attr_values:\n        subset = [example for example in examples if example[attr] == value]\n        weight = len(subset) / len(examples)\n        weighted_entropy += weight * entropy(subset, target_attr)\n    return total_entropy - weighted_entropy\ndef find_best_split(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"Find the attribute with the highest information gain.\"\"\"\n    best_gain = -1\n    best_attr = None\n    for attr in attributes:\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    return best_attr"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a kernel SVM using the Pegasos algorithm (deterministic version).\n\n    Parameters:\n        data (np.ndarray): 2D array of shape (n_samples, n_features)\n        labels (np.ndarray): 1D array of shape (n_samples,)\n        kernel (str): 'linear' or 'rbf'\n        lambda_val (float): Regularization parameter\n        iterations (int): Number of training iterations\n        sigma (float): Bandwidth for RBF kernel\n\n    Returns:\n        tuple: (alpha_list, bias) where alpha is a list of coefficients and bias is a float.\n    \"\"\"\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n\n    def kernel_func(x1, x2):\n        if kernel == 'linear':\n            return np.dot(x1, x2)\n        elif kernel == 'rbf':\n            return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n        else:\n            raise ValueError('Unknown kernel type')\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel_func(data[i], data[j])\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            pred = bias + np.sum(alpha * K[:, i] * labels)\n            if labels[i] * pred <= 1:\n                alpha[i] += eta_t * labels[i]\n            alpha = alpha / (1 + eta_t * lambda_val)\n    bias = 0.0\n    for i in range(n_samples):\n        bias += labels[i] * alpha[i] * kernel_func(data[i], data[i])\n    bias /= n_samples\n    alpha_rounded = np.round(alpha, 4).tolist()\n    bias_rounded = round(bias, 4)\n    return (alpha_rounded, bias_rounded)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    if len(features) != len(labels):\n        raise ValueError('Features and labels must have the same number of examples.')\n    weighted_sums = []\n    for i in range(len(features)):\n        weighted_sum = sum((features[i][j] * weights[j] for j in range(len(weights)))) + bias\n        weighted_sums.append(weighted_sum)\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predicted_probabilities = [sigmoid(z) for z in weighted_sums]\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predicted_probabilities, labels))) / len(labels)\n    predicted_probabilities_rounded = [round(p, 4) for p in predicted_probabilities]\n    mse_rounded = round(mse, 4)\n    predicted_probabilities_rounded = np.array(predicted_probabilities_rounded).tolist()\n    return (predicted_probabilities_rounded, mse_rounded)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    features = np.array(features)\n    labels = np.array(labels)\n    weights = np.array(initial_weights)\n    bias = initial_bias\n    mse_history = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        a = 1 / (1 + np.exp(-z))\n        mse = np.mean((a - labels) ** 2)\n        mse_history.append(round(mse, 4))\n        error = a - labels\n        delta = error * a * (1 - a)\n        weights -= learning_rate * np.dot(features.T, delta)\n        bias -= learning_rate * np.sum(delta)\n    final_weights = np.round(weights, 4).tolist()\n    final_bias = round(bias, 4)\n    mse_history = [round(mse, 4) for mse in mse_history]\n    return (final_weights, final_bias, mse_history)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_topo(child)\n                topo.append(node)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    C_inv = np.linalg.inv(C_matrix)\n    P = np.dot(C_inv, B_matrix)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix.')\n    ATA = A.T @ A\n    (eigenvalues, V) = np.linalg.eig(ATA)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    V = V[:, idx]\n    S = np.sqrt(eigenvalues)\n    S_inv = np.zeros((2, 2))\n    for i in range(2):\n        if S[i] > 1e-10:\n            S_inv[i, i] = 1.0 / S[i]\n    U = A @ V @ S_inv\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    return (U, S, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in X and y while maintaining their correspondence.\n    \n    Parameters:\n    X (np.ndarray): Feature array.\n    y (np.ndarray): Label array.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing the shuffled X and y as Python lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterable function that yields batches of data from X and optionally y.\n\n    Parameters:\n    - X (np.ndarray): Input data.\n    - y (np.ndarray, optional): Target data. Defaults to None.\n    - batch_size (int): Size of each batch. Defaults to 64.\n\n    Yields:\n    - list: A batch of X (and optionally y) as a Python list after reshaping.\n    \"\"\"\n    data_len = X.shape[0]\n    for i in range(0, data_len, batch_size):\n        end = i + batch_size\n        batch_X = X[i:end].tolist()\n        if y is not None:\n            batch_y = y[i:end].tolist()\n            yield [batch_X, batch_y]\n        else:\n            yield batch_X"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of the feature at index feature_i\n    is greater than or equal to the threshold.\n\n    Parameters:\n    X (np.ndarray): The input dataset (2D array-like).\n    feature_i (int): Index of the feature to evaluate.\n    threshold (float): Threshold value for splitting.\n\n    Returns:\n    tuple: Two lists representing the two subsets.\n    \"\"\"\n    feature_column = X[:, feature_i]\n    mask_ge = feature_column >= threshold\n    mask_lt = feature_column < threshold\n    subset_ge = X[mask_ge].tolist()\n    subset_lt = X[mask_lt].tolist()\n    return (subset_ge, subset_lt)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    features = []\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), d):\n            product = np.ones(n_samples)\n            for i in indices:\n                product *= X[:, i]\n            features.append(product)\n    return np.column_stack(features).tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of the given dataset.\n\n    Parameters:\n    - X: 2D numpy array, feature matrix.\n    - y: 1D numpy array, target vector.\n    - n_subsets: int, number of subsets to generate.\n    - replacements: bool, whether to sample with replacement.\n    - seed: int, random seed for reproducibility.\n\n    Returns:\n    - list of tuples, each containing (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n\n    Parameters:\n    x (np.ndarray): 1D array of integer values representing categories.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If not provided, it is determined from the input.\n\n    Returns:\n    list: One-hot encoded array as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((len(x), n_col), dtype=np.int32)\n    one_hot[np.arange(len(x)), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y.\n    \n    Parameters:\n    - X: 2D numpy array (n_samples, n_features)\n    - Y: Optional 2D numpy array (n_samples, n_features), default is None\n    \n    Returns:\n    - 2D numpy array (correlation matrix) rounded to 4 decimal places,\n      converted to a Python list using tolist()\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - X.mean(axis=0)\n    Y_centered = Y - Y.mean(axis=0)\n    cov_matrix = np.cov(X_centered, Y_centered, rowvar=False)\n    std_devs = np.std(X, axis=0)\n    std_devs[std_devs == 0] = 1.0\n    correlation_matrix = cov_matrix / (std_devs[:, None] * std_devs[None, :])\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_feature = -1\n        best_threshold = 0.0\n        best_sign = 1\n        for feat_idx in range(n_features):\n            sorted_indices = np.argsort(X[:, feat_idx])\n            X_sorted = X[sorted_indices, feat_idx]\n            y_sorted = y[sorted_indices]\n            for i in range(1, n_samples):\n                threshold = (X_sorted[i - 1] + X_sorted[i]) / 2\n                pred = np.ones(n_samples)\n                pred[X[:, feat_idx] < threshold] = -1\n                error = np.sum(w * (pred != y))\n                if error < best_error:\n                    best_error = error\n                    best_feature = feat_idx\n                    best_threshold = threshold\n                    best_sign = 1\n                pred = np.ones(n_samples)\n                pred[X[:, feat_idx] > threshold] = -1\n                error = np.sum(w * (pred != y))\n                if error < best_error:\n                    best_error = error\n                    best_feature = feat_idx\n                    best_threshold = threshold\n                    best_sign = -1\n        if best_error == 0:\n            best_error = 1e-10\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        pred = np.ones(n_samples)\n        if best_sign == 1:\n            pred[X[:, best_feature] < best_threshold] = -1\n        else:\n            pred[X[:, best_feature] > best_threshold] = -1\n        w *= np.exp(-alpha * y * pred)\n        w /= np.sum(w)\n        classifiers.append({'feature': best_feature, 'threshold': round(best_threshold, 4), 'alpha': round(alpha, 4)})\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)\n    log_softmax_values = np.log(exp_scores / np.sum(exp_scores))\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        super().__init__()\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be defined before initialization.')\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_W = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            roi = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(roi * kernel)\n    return np.round(output, 4).tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_penalty = alpha * np.dot(w, w)\n    loss = mse + l2_penalty\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    return z if z >= 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with MSE loss.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples,)\n    - weights: numpy array of shape (n_features,)\n    - learning_rate: float\n    - n_iterations: int\n    - batch_size: int (default=1 for SGD)\n    - method: str ('batch', 'stochastic', 'mini-batch')\n    \n    Returns:\n    - list of weights after training, rounded to 4 decimal places\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights)\n    if X.shape[1] == weights.shape[0]:\n        pass\n    else:\n        X = np.c_[np.ones(X.shape[0]), X]\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    if method not in ['batch', 'stochastic', 'mini-batch']:\n        raise ValueError(\"method must be one of 'batch', 'stochastic', or 'mini-batch'\")\n    weights = weights.copy()\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradients = 2 / n_samples * X.T @ (X @ weights - y)\n        elif method == 'stochastic':\n            idx = np.random.randint(n_samples)\n            X_i = X[idx]\n            y_i = y[idx]\n            gradients = 2 * (X_i @ weights - y_i) * X_i\n        elif method == 'mini-batch':\n            idx = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[idx]\n            y_batch = y[idx]\n            gradients = 2 / batch_size * X_batch.T @ (X_batch @ weights - y_batch)\n        else:\n            raise ValueError('Invalid method')\n        weights -= learning_rate * gradients\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix to its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (np.ndarray): A 2D NumPy array representing the matrix.\n\n    Returns:\n    list: A Python list representation of the RREF matrix.\n    \"\"\"\n    mat = matrix.astype(float)\n    (m, n) = mat.shape\n    lead = 0\n    for r in range(m):\n        if lead >= n:\n            break\n        pivot_row = None\n        for i in range(r, m):\n            if abs(mat[i, lead]) > 1e-10:\n                pivot_row = i\n                break\n        if pivot_row is None:\n            lead += 1\n            continue\n        mat[[r, pivot_row]] = mat[[pivot_row, r]]\n        mat[r] = mat[r] / mat[r, lead]\n        for i in range(m):\n            if i != r and abs(mat[i, lead]) > 1e-10:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n\n    Parameters:\n    - f: Objective function to be minimized.\n    - grad: Gradient of the objective function.\n    - x0: Initial parameter values (numpy array).\n    - learning_rate: Step size (default: 0.001)\n    - beta1: Exponential decay rate for first moment estimates (default: 0.9)\n    - beta2: Exponential decay rate for second moment estimates (default: 0.999)\n    - epsilon: Small constant for numerical stability (default: 1e-8)\n    - num_iterations: Number of iterations to run the optimizer (default: 1000)\n\n    Returns:\n    - Optimized parameters as a Python list rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        error = y_pred - y\n        grad_w = 1 / n_samples * X.T.dot(error) + alpha * np.sign(w)\n        grad_b = 1 / n_samples * np.sum(error)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        if np.linalg.norm(grad_w) < tol:\n            break\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = np.round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true: List of true binary labels (0 or 1)\n    - y_pred: List of predicted binary labels (0 or 1)\n\n    Returns:\n    - Recall value rounded to three decimal places.\n      If denominator (TP + FN) is zero, returns 0.0 to avoid division by zero.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    return round(tp / (tp + fn), 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism.\n\n    Parameters:\n    X (np.ndarray): Input sequence of shape (seq_len, d_model)\n    W_q (np.ndarray): Query weights of shape (d_model, d_k)\n    W_k (np.ndarray): Key weights of shape (d_model, d_k)\n    W_v (np.ndarray): Value weights of shape (d_model, d_v)\n\n    Returns:\n    np.ndarray: Self-attention output of shape (seq_len, d_v), rounded to 4 decimals\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = W_q.shape[1]\n    attn_scores = Q @ K.T / np.sqrt(d_k)\n    exp_scores = np.exp(attn_scores)\n    softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    output = softmax @ V\n    return np.round(output, 4).tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_seq = np.array(input_sequence)\n    h0 = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = h0.copy()\n    for x in input_seq:\n        x = np.array(x)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    return np.round(h, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions:\n    P ~ N(mu_p, sigma_p^2)\n    Q ~ N(mu_q, sigma_q^2)\n\n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n    float: KL divergence D_KL(P || Q)\n    \"\"\"\n    sigma_p = max(sigma_p, 1e-08)\n    sigma_q = max(sigma_q, 1e-08)\n    kl = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return float(kl)"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a system of linear equations Ax = b using the Gauss-Seidel method.\n\n    Parameters:\n    - A: 2D numpy array (square matrix of coefficients)\n    - b: 1D numpy array (right-hand side vector)\n    - n: int (number of iterations)\n    - x_ini: 1D numpy array (optional initial guess, default is zero vector)\n\n    Returns:\n    - 1D list of the solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            sum_val = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sum_val) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j] -= factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = x.shape[0]\n        h = initial_hidden_state\n        c = initial_cell_state\n        hidden_states = []\n        for t in range(T):\n            xt = x[t].reshape(-1, 1)\n            combined = np.vstack((h, xt))\n            ft = self._sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = self._sigmoid(np.dot(self.Wi, combined) + self.bi)\n            ct_tilde = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            ot = self._sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c = ft * c + it * ct_tilde\n            h = ot * np.tanh(c)\n            hidden_states.append(h)\n        hidden_states = [np.round(h, 4).tolist() for h in hidden_states]\n        final_hidden_state = np.round(h, 4).tolist()\n        final_cell_state = np.round(c, 4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\n\n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    vocab = set((word for doc in corpus for word in doc))\n    tf_list = []\n    for doc in corpus:\n        tf = {}\n        total_words = len(doc)\n        if total_words == 0:\n            tf_list.append(tf)\n            continue\n        word_counts = Counter(doc)\n        for (word, count) in word_counts.items():\n            tf[word] = count / total_words\n        tf_list.append(tf)\n    df = {}\n    for word in vocab:\n        count = 0\n        for doc in corpus:\n            if word in doc:\n                count += 1\n        df[word] = count\n    idf = {}\n    N = len(corpus)\n    for word in vocab:\n        idf[word] = np.log((N + 1) / (df[word] + 1)) + 1\n    result = []\n    for (doc_index, doc) in enumerate(corpus):\n        tf_idf_scores = []\n        for q_word in query:\n            if q_word in tf_list[doc_index]:\n                tf = tf_list[doc_index][q_word]\n                if q_word in idf:\n                    tf_idf = tf * idf[q_word]\n                else:\n                    tf_idf = 0.0\n            else:\n                tf_idf = 0.0\n            tf_idf_scores.append(round(tf_idf, 5))\n        result.append(tf_idf_scores)\n    return np.array(result).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with weights set to random values multiplied by 0.01 and biases set to zero.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, last inputs, and hidden states.\n        \"\"\"\n        T = len(input_sequence)\n        h = np.zeros((hidden_size, 1))\n        h_states = [h]\n        outputs = []\n        for t in range(T):\n            x = input_sequence[t].reshape(-1, 1)\n            h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n            y = np.dot(W_hy, h) + b_y\n            outputs.append(y)\n            h_states.append(h)\n        return (outputs, h_states[-1], h_states)\n\n    def rnn_backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        delta = outputs[-1] - expected_output\n        dW_hy += np.dot(delta, last_hiddens[-1].T)\n        db_y += delta\n        delta_prev = np.dot(W_hy.T, delta)\n        for t in reversed(range(T)):\n            x = input_sequence[t].reshape(-1, 1)\n            h = last_hiddens[t]\n            h_next = last_hiddens[t + 1]\n            dtanh = 1 - h ** 2\n            delta = delta_prev * dtanh\n            dW_xh += np.dot(delta, x.T)\n            dW_hh += np.dot(delta, h.T)\n            db_h += delta\n            delta_prev = np.dot(W_hh.T, delta)\n        W_xh -= learning_rate * dW_xh\n        W_hh -= learning_rate * dW_hh\n        W_hy -= learning_rate * dW_hy\n        b_h -= learning_rate * db_h\n        b_y -= learning_rate * db_y\n        return (W_xh, W_hh, W_hy, b_h, b_y)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimals and converted to list)\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    r_dot_r = r.dot(r)\n    for _ in range(n):\n        Ap = A @ p\n        alpha = r_dot_r / p.dot(Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_new_dot_r_new = r.dot(r)\n        if np.sqrt(r_new_dot_r_new) < tol:\n            break\n        beta = r_new_dot_r_new / r_dot_r\n        p = r + beta * p\n        r_dot_r = r_new_dot_r_new\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    gini = 1 - np.sum(((count / len(y)) ** 2 for count in counts))\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_idx, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, column_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((x * y for (x, y) in zip(v, L)))\n    norm_squared = sum((x * x for x in L))\n    scalar = dot_product / norm_squared\n    projection = [round(scalar * x, 3) for x in L]\n    return projection"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, pivots) = np.linalg.qr(A, mode='r')\n    rank = np.sum(pivots != 0)\n    pivot_columns = np.where(np.any(A != 0, axis=0))[0]\n    basis = A[:, pivot_columns]\n    return np.round(basis, 8).tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    ss_total = np.sum((y_true - y_mean) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_res / ss_total\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    num_pixels = row_length * len(img)\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual target values.\n    y_pred (array-like): Predicted target values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are not of the same length or are empty.\n    TypeError: If inputs are not arrays or lists.\n    \"\"\"\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise TypeError('Inputs must be list-like or numpy arrays.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Input arrays must have the same shape. Got {y_true.shape} and {y_pred.shape}.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    intersection = np.sum(y_true & y_pred)\n    true_positives = np.sum(y_true)\n    pred_positives = np.sum(y_pred)\n    if true_positives + pred_positives == 0:\n        return 0.0\n    dice = 2 * intersection / (true_positives + pred_positives)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The value hypervectors are created using the same seed from random_seeds for reproducibility.\n    All feature hypervectors are then bundled to form the composite hypervector for the row.\n    \n    Args:\n        row: A dictionary where keys are feature names and values are their corresponding values.\n        dim: The dimensionality of the hypervectors.\n        random_seeds: A dictionary mapping feature names to seeds for reproducibility.\n    \n    Returns:\n        A composite hypervector as a list.\n    \"\"\"\n    composite_hv = np.zeros(dim, dtype=np.int8)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, 0)\n        np.random.seed(seed)\n        feature_hv = np.random.randint(0, 2, size=dim, dtype=np.int8)\n        np.random.seed(seed)\n        value_hv = np.random.randint(0, 2, size=dim, dtype=np.int8)\n        binding_hv = np.bitwise_xor(feature_hv, value_hv)\n        composite_hv = np.bitwise_xor(composite_hv, binding_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if np.all(v1 == 0) or np.all(v2 == 0):\n        raise ValueError('Vectors cannot be empty or have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in (0, 1) for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n\n    Parameters:\n    data (list or numpy.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing the following keys:\n        - 'mean'\n        - 'median'\n        - 'mode'\n        - 'variance'\n        - 'standard_deviation'\n        - '25th_percentile'\n        - '50th_percentile'\n        - '75th_percentile'\n        - 'interquartile_range'\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=False).mode[0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(interquartile_range, 4)}\n    return result"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = 1 / (std_dev * math.sqrt(2 * math.pi)) * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError('k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('lam must be a positive number.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.size == 0:\n        raise ValueError('Input image is empty.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    return max_pixel - min_pixel"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point,\n                           rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    n = len(data)\n    result = []\n    for x in data:\n        features = [1.0]\n        for d in range(1, degree + 1):\n            for indices in combinations_with_replacement(range(n), d):\n                product = 1.0\n                for idx in indices:\n                    product *= x[idx]\n                features.append(product)\n        result.append([round(f, 8) for f in features])\n    return result"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for i in range(position):\n        for j in range(0, d_model, 2):\n            angle = i / 10000 ** (j / d_model)\n            pos_enc[i, j] = np.sin(angle)\n            if j + 1 < d_model:\n                pos_enc[i, j + 1] = np.cos(angle)\n    return pos_enc.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value (scalar or numpy array)\n    :param grad: Current gradient (scalar or numpy array)\n    :param m: First moment estimate (scalar or numpy array)\n    :param v: Second moment estimate (scalar or numpy array)\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(10, 10)}, 'mlp': {'c_fc': np.random.rand(10, 40), 'c_proj': np.random.rand(40, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    current_token_ids = token_ids.copy()\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(current_token_ids)\n        wte = params['wte']\n        wpe = params['wpe']\n        token_embeddings = wte[current_token_ids]\n        positional_embeddings = wpe[:seq_len]\n        hidden_states = token_embeddings + positional_embeddings\n        for block in params['blocks']:\n            c_attn = block['attn']['c_attn']\n            qkv = np.dot(hidden_states, c_attn)\n            (q, k, v) = np.split(qkv, 3, axis=-1)\n            attn_scores = np.dot(q, k.T) / np.sqrt(hparams['n_head'])\n            attn_weights = np.softmax(attn_scores, axis=-1)\n            attn_output = np.dot(attn_weights, v)\n            c_proj = block['attn']['c_proj']\n            attn_output = np.dot(attn_output, c_proj)\n            residual = hidden_states\n            hidden_states = residual + attn_output\n            ln_1 = block['ln_1']\n            hidden_states = ln_1['g'] * (hidden_states - np.mean(hidden_states, axis=-1, keepdims=True)) / np.std(hidden_states, axis=-1, keepdims=True) + ln_1['b']\n            mlp = block['mlp']\n            fc = np.dot(hidden_states, mlp['c_fc'])\n            gelu = np.maximum(0, fc)\n            proj = np.dot(gelu, mlp['c_proj'])\n            residual = hidden_states\n            hidden_states = residual + proj\n            ln_2 = block['ln_2']\n            hidden_states = ln_2['g'] * (hidden_states - np.mean(hidden_states, axis=-1, keepdims=True)) / np.std(hidden_states, axis=-1, keepdims=True) + ln_2['b']\n        ln_f = params['ln_f']\n        hidden_states = ln_f['g'] * (hidden_states - np.mean(hidden_states, axis=-1, keepdims=True)) / np.std(hidden_states, axis=-1, keepdims=True) + ln_f['b']\n        last_token_embedding = hidden_states[-1]\n        logits = np.dot(last_token_embedding, wte.T)\n        probabilities = np.exp(logits) / np.sum(np.exp(logits))\n        next_token_id = np.random.choice(len(probabilities), p=probabilities)\n        current_token_ids.append(next_token_id)\n    generated_text = encoder.decode(current_token_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        sum_exp = np.sum(exp_values)\n        return exp_values / sum_exp\n    queries = np.array(crystal_values).reshape(n, 1)\n    keys = np.array(crystal_values).reshape(n, 1)\n    values = np.array(crystal_values).reshape(n, 1)\n    attention_scores = np.dot(queries, keys.T) / np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    weighted_values = np.dot(attention_weights, values)\n    return [round(float(val), 4) for val in weighted_values]"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus based on the query.\n\n    Parameters:\n        corpus (list of str): List of documents.\n        query (str): The search query.\n        k1 (float): Free parameter for term frequency saturation (default: 1.5).\n        b (float): Free parameter for document length normalization (default: 0.75).\n\n    Returns:\n        list: A list of BM25 scores for each document, rounded to three decimal places.\n    \"\"\"\n    query_terms = query.lower().split()\n    doc_terms_list = [doc.lower().split() for doc in corpus]\n    num_docs = len(doc_terms_list)\n    doc_term_freqs = [Counter(doc_terms) for doc_terms in doc_terms_list]\n    doc_lengths = [len(doc_terms) for doc_terms in doc_terms_list]\n    total_terms = sum(doc_lengths)\n    idf_scores = {}\n    for term in set(query_terms):\n        doc_count = sum((1 for doc_terms in doc_terms_list if term in doc_terms))\n        idf = np.log((num_docs - doc_count + 0.5) / (doc_count + 0.5))\n        idf_scores[term] = idf\n    bm25_scores = []\n    for (i, doc_terms) in enumerate(doc_terms_list):\n        score = 0.0\n        doc_length = doc_lengths[i]\n        for term in set(query_terms):\n            tf = doc_term_freqs[i].get(term, 0)\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * doc_length / total_terms)\n            score += idf_scores[term] * (numerator / denominator)\n        bm25_scores.append(round(score, 3))\n    return bm25_scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must be of the same length.')\n    tp = 0\n    fp = 0\n    fn = 0\n    for (t, p) in zip(y_true, y_pred):\n        if t == 1 and p == 1:\n            tp += 1\n        elif t == 0 and p == 1:\n            fp += 1\n        elif t == 1 and p == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom math import ceil\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for i in range(len(consumption_data)):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended = consumption_data[i] - fluctuation\n        detrended_data.append(detrended)\n    X = np.array(range(1, len(detrended_data) + 1)).reshape(-1, 1)\n    y = np.array(detrended_data)\n    model = np.polyfit(X.flatten(), y, 1)\n    (slope, intercept) = model\n    predicted_base = slope * 15 + intercept\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_total = predicted_base + day_15_fluctuation\n    rounded_prediction = round(predicted_total)\n    final_prediction = ceil(rounded_prediction * 1.05)\n    return final_prediction"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform self-attention on Q, K, V.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)\n    attn_weights = np.softmax(scores, axis=-1)\n    output = attn_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    (batch_size, seq_len, d_model) = Q.shape\n    d_head = d_model // n_heads\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    K = K.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    V = V.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\n    outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q[i], K[i], V[i])\n        outputs.append(head_output)\n    combined_output = np.concatenate(outputs, axis=-1)\n    combined_output = combined_output.reshape(batch_size, seq_len, d_model)\n    return np.round(combined_output, 4).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of equal length.')\n    a = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 1))\n    b = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 0))\n    c = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 1))\n    d = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 0))\n    n = len(x)\n    phi = (a * d - b * c) / ((a + b) * (a + c) * (d + b) * (d + c)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return x / 5.0 + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to 4 decimal places\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    if x > 20:\n        return x\n    elif x < -20:\n        return 0.0\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rho_clip = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    objective = np.sum(rho_clip * A) - beta * kl_divergence\n    return round(objective, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * (math.exp(x) - 1)), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X: np.ndarray, shape (N, M), input features\n        y: np.ndarray, shape (N,), class labels (0-based)\n        learning_rate: float, step size for gradient descent\n        iterations: int, number of iterations to perform\n\n    Returns:\n        B: list[float], CxM updated parameter vector rounded to 4 floating points\n        losses: list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = len(np.unique(y))\n    M = X.shape[1]\n    B = np.random.randn(C, M)\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    y_one_hot = np.zeros((y.size, C))\n    y_one_hot[np.arange(y.size), y] = 1\n    losses = []\n    for _ in range(iterations):\n        logits = X @ B.T\n        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n        probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-15), axis=1))\n        losses.append(round(loss, 4))\n        grad = (probs - y_one_hot).T @ X\n        B -= learning_rate * grad\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n        X (np.ndarray): Input features matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target labels array of shape (n_samples,)\n        learning_rate (float): Learning rate for gradient descent\n        iterations (int): Number of iterations for training\n        \n    Returns:\n        tuple: A tuple containing:\n            - list[float]: Optimized coefficients (weights) after training\n            - list[float]: List of loss values at each iteration (rounded to 4 decimals)\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    weights = np.zeros(X.shape[1])\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        h = 1 / (1 + np.exp(-z))\n        gradient = np.dot(X.T, h - y) / len(y)\n        weights -= learning_rate * gradient\n        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n        losses.append(round(loss, 4))\n    return (weights.tolist(), losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    attn_scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attn_scores += mask * -1000000000.0\n    attn_weights = np.softmax(attn_scores, axis=-1)\n    output = np.matmul(attn_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy as the measure of disorder.\n    \"\"\"\n    if not apples:\n        return 0.0\n    color_counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small value to avoid division by zero\n    \n    Returns:\n    - Normalized X as a list rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_norm + beta\n    return np.round(Y, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    ref_unigrams = Counter(ref_tokens)\n    cand_unigrams = Counter(cand_tokens)\n    overlap = 0\n    for token in cand_unigrams:\n        if token in ref_unigrams:\n            overlap += min(cand_unigrams[token], ref_unigrams[token])\n    precision = overlap / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = overlap / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    if precision + recall == 0:\n        f_mean = 0.0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    match_count = 0\n    prev_match = False\n    for i in range(len(ref_tokens)):\n        if i < len(cand_tokens) and ref_tokens[i] == cand_tokens[i]:\n            match_count += 1\n            prev_match = True\n        else:\n            prev_match = False\n    fragments = 0\n    prev_match = False\n    for i in range(len(ref_tokens)):\n        if i < len(cand_tokens) and ref_tokens[i] == cand_tokens[i]:\n            if not prev_match:\n                fragments += 1\n            prev_match = True\n        else:\n            prev_match = False\n    if fragments == 0:\n        order_penalty = 0.0\n    else:\n        order_penalty = gamma * (1.0 / fragments)\n    score = (1 - order_penalty) * f_mean\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n\n    Parameters:\n    - joint_counts: int or float, the number of times both events occurred together.\n    - total_counts_x: int or float, the total number of times the first event occurred.\n    - total_counts_y: int or float, the total number of times the second event occurred.\n    - total_samples: int or float, the total number of samples in the dataset.\n\n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    if total_counts_x == 0 or total_counts_y == 0 or total_samples == 0:\n        return 0.0\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    return [round((num - min_val) / (max_val - min_val), 4) for num in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    output = a2 + x\n    final_output = np.maximum(0, output)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D array (height, width, channels).\n    \n    Args:\n        x: A 3D NumPy array of shape (height, width, channels).\n        \n    Returns:\n        A 1D NumPy array of shape (channels,) containing the average of each feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Applies Batch Normalization to a 4D input tensor in BCHW format.\n    \n    Args:\n        X: Input tensor of shape (batch, channels, height, width)\n        gamma: Scale parameter of shape (channels,)\n        beta: Shift parameter of shape (channels,)\n        epsilon: Small value to avoid division by zero\n    \n    Returns:\n        Normalized tensor with shape (batch, channels, height, width), rounded to 4 decimals and converted to list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma.reshape(1, -1, 1, 1) * X_norm + beta.reshape(1, -1, 1, 1)\n    return np.round(Y, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of the polynomial term c * x^n at x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which to evaluate the derivative.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n        vectors (list[list[float]]): A list of 2D vectors.\n        tol (float): Tolerance to determine linear independence.\n\n    Returns:\n        list[list[float]]: A list of orthonormal vectors, each rounded to 4 decimal places.\n    \"\"\"\n    vectors = np.array(vectors, dtype=float)\n    basis = []\n    for i in range(vectors.shape[0]):\n        v = vectors[i].copy()\n        for b in basis:\n            proj = np.dot(v, b) / np.dot(b, b) * b\n            v -= proj\n        if np.linalg.norm(v) > tol:\n            v /= np.linalg.norm(v)\n            basis.append(v)\n    return [vec.round(4).tolist() for vec in basis]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): First 3D vector.\n    b (list or np.ndarray): Second 3D vector.\n    \n    Returns:\n    list: The cross product vector, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    return np.round(x, 4).tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p, dtype=np.float64)\n    q = np.array(q, dtype=np.float64)\n    bc = np.sqrt(p * q).sum()\n    if bc == 0:\n        return float('inf')\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        log_probs = []\n        returns = []\n        states = []\n        actions = []\n        for (state, action, reward) in episode:\n            states.append(state)\n            actions.append(action)\n            returns.append(reward)\n        G = 0\n        for t in reversed(range(len(episode))):\n            G += returns[t]\n            prob = np.exp(theta[states[t], actions[t]]) / np.sum(np.exp(theta[states[t], :]))\n            log_prob = np.log(prob)\n            log_probs.append(log_prob)\n        log_probs = log_probs[::-1]\n        for t in range(len(episode)):\n            grad = log_probs[t] * G\n            gradient[states[t], actions[t]] += grad\n    if len(episodes) > 0:\n        gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * d_in * d_out + d_in * n_experts\n    if dense_flops == 0:\n        savings_percent = 0.0\n    else:\n        savings_percent = (dense_flops - moe_flops) / dense_flops * 100\n    return (round(dense_flops, 1), round(moe_flops, 1), round(savings_percent, 1))"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X @ W_g.T\n    noisy_logits = logits + W_noise @ N.T\n    top_k_indices = np.argsort(-noisy_logits, axis=1)[:, :k]\n    (batch_size, num_experts) = logits.shape\n    mask = np.zeros_like(logits, dtype=bool)\n    mask[np.arange(batch_size)[:, None], top_k_indices] = True\n    top_k_logits = np.where(mask, noisy_logits, -np.inf)\n    exp_logits = np.exp(top_k_logits - np.max(top_k_logits, axis=1, keepdims=True))\n    probabilities = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    probabilities = np.round(probabilities, 4)\n    return probabilities.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, seq_len, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gate_scores = x @ Wg\n    gate_probs = np.exp(gate_scores) / np.sum(np.exp(gate_scores), axis=-1, keepdims=True)\n    top_k_indices = np.argpartition(-gate_probs, top_k, axis=-1)[:, :, :top_k]\n    top_k_probs = np.take_along_axis(gate_probs, top_k_indices, axis=-1)\n    top_k_probs /= np.sum(top_k_probs, axis=-1, keepdims=True)\n    x_reshaped = x.reshape(-1, input_dim)\n    expert_outputs = []\n    for i in range(n_experts):\n        expert_output = x_reshaped @ We[i]\n        expert_outputs.append(expert_output)\n    expert_outputs = np.stack(expert_outputs, axis=0)\n    top_k_indices_flat = top_k_indices.reshape(-1, top_k)\n    top_k_outputs = np.take_along_axis(expert_outputs, top_k_indices_flat[..., np.newaxis, np.newaxis], axis=0)\n    top_k_outputs = top_k_outputs.reshape(batch_size * seq_len, top_k, output_dim)\n    final_output = np.sum(top_k_outputs * top_k_probs[:, :, np.newaxis], axis=1)\n    final_output = final_output.reshape(batch_size, seq_len, output_dim)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    out = gamma * X_norm + beta\n    return np.round(out, 4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the x value where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    \n    Parameters:\n        start_x (float): Initial guess for x.\n        learning_rate (float): Step size in gradient descent.\n        tolerance (float): Stopping criterion for convergence.\n        max_iters (int): Maximum number of iterations to prevent infinite loops.\n        \n    Returns:\n        float: The x value where f(x) is minimized, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        df_dx = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * df_dx\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    x = np.array(x)\n    tanh_x = np.tanh(x)\n    scaled_tanh = alpha * tanh_x + gamma\n    normalized_tanh = (scaled_tanh - np.mean(scaled_tanh)) / (np.std(scaled_tanh) + 1e-08)\n    final_output = beta * normalized_tanh + gamma\n    return np.round(final_output, 4).tolist()"}
