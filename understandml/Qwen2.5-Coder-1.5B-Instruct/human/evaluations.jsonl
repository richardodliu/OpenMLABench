{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [sum((row[i] * b[i] for i in range(len(row)))) for row in a]\n    return result\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Compute the transpose of a given matrix.\n\n    Args:\n    a (list[list[int|float]]): A 2D list representing the input matrix.\n\n    Returns:\n    list[list[int|float]]: The transposed matrix.\n    \"\"\"\n    transposed = [[0 for _ in range(len(a))] for _ in range(len(a[0]))]\n    for i in range(len(a)):\n        for j in range(len(a[i])):\n            transposed[j][i] = a[i][j]\n    return transposed\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    arr = np.array(a)\n    if arr.size != np.prod(new_shape):\n        return []\n    reshaped_arr = arr.reshape(*new_shape)\n    return reshaped_arr.tolist()\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Parameters:\n    - matrix: A 2D list of floats representing the matrix.\n    - mode: A string indicating whether to calculate the mean by rows ('row') or columns ('column').\n\n    Returns:\n    - A list of means calculated by the specified mode.\n    \"\"\"\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        means = [sum(column) / len(column) for column in zip(*matrix)]\n    else:\n        raise ValueError(\"Invalid mode. Please choose 'row' or 'column'.\")\n    return means\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List, Union\ndef scalar_multiply(matrix: List[List[Union[int, float]]], scalar: Union[int, float]) -> List[List[Union[int, float]]]:\n    return [[element * scalar for element in row] for row in matrix]\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": false, "result": "failed: Matrix is singular and does not have real eigenvalues.", "solution": "\nfrom typing import List\ndef calculate_eigenvalues(matrix: List[List[float | int]]) -> List[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix.\n\n    Args:\n        matrix (List[List[float|int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n        List[float]: A list containing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    discriminant = b ** 2 - 4 * a * c\n    if discriminant < 0:\n        raise ValueError('Matrix is singular and does not have real eigenvalues.')\n    lambda1 = (-b + discriminant ** 0.5) / (2 * a)\n    lambda2 = (-b - discriminant ** 0.5) / (2 * a)\n    return [lambda1, lambda2]\nmatrix = [[3, 1], [1, 2]]\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if not np.linalg.det(T) or not np.linalg.det(S):\n        return -1\n    result = np.dot(np.dot(np.linalg.inv(T), A), np.linalg.inv(S))\n    result_rounded = np.round(result, 4).tolist()\n    return result_rounded\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 3]]\nS = [[-1, 0], [0, 5]]\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef inverse_2x2(matrix: List[List[float]]) -> List[List[float]]:\n    if len(matrix) != 2 or any((len(row) != 2 for row in matrix)):\n        return None\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nfrom typing import List\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n\n    Args:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n\n    Returns:\n    List[List[float]]: A covariance matrix as a list of lists.\n    \"\"\"\n    num_features = len(vectors[0])\n    num_vectors = len(vectors)\n    covariance_matrix = [[0.0] * num_features for _ in range(num_features)]\n    means = [sum(feature) / num_vectors for feature in zip(*vectors)]\n    for i in range(num_features):\n        for j in range(i, num_features):\n            variance_sum = 0.0\n            for vector in vectors:\n                variance_sum += (vector[i] - means[i]) * (vector[j] - means[j])\n            covariance_matrix[i][j] = variance_sum / (num_vectors - 1)\n            if i != j:\n                covariance_matrix[j][i] = covariance_matrix[i][j]\n    return covariance_matrix\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n\n    Parameters:\n    - A (np.ndarray): Coefficient matrix of the system of equations.\n    - b (np.ndarray): Right-hand side vector of the system of equations.\n    - n (int): Number of iterations for the Jacobi method.\n\n    Returns:\n    - list: An approximate solution to the system of equations, rounded to 4 decimal places.\n    \"\"\"\n    x = np.zeros_like(b)\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(len(A)):\n            sum_term = np.dot(A[i, :i], new_x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            new_x[i] = (b[i] - sum_term) / A[i, i]\n        x = np.round(new_x, 4).tolist()\n    return x\nA = np.array([[2, 1], [3, 5]])\nb = np.array([8, 19])\nn = 10\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 square matrix.\n    \n    Returns:\n    tuple: A tuple containing the singular values of the matrix in descending order.\n    \"\"\"\n    n = A.shape[0]\n    if n != 2:\n        raise ValueError('Matrix must be 2x2')\n    J11 = np.array([[A[1, 1], -A[1, 0]], [-A[1, 0], A[0, 0]]])\n    J12 = np.array([[-A[0, 1], A[0, 0]], [A[0, 1], -A[1, 1]]])\n    J21 = np.array([[A[1, 1], -A[1, 0]], [-A[1, 0], A[0, 0]]])\n    J22 = np.array([[-A[0, 1], A[0, 0]], [A[0, 1], -A[1, 1]]])\n    det_J11 = np.linalg.det(J11)\n    det_J12 = np.linalg.det(J12)\n    det_J21 = np.linalg.det(J21)\n    det_J22 = np.linalg.det(J22)\n    lambda_1 = det_J11 / (det_J11 + det_J12)\n    lambda_2 = det_J22 / (det_J21 + det_J22)\n    singular_values = sorted((lambda_1, lambda_2), reverse=True)\n    singular_values_rounded = tuple((round(value, 4) for value in singular_values))\n    return singular_values_rounded\nA = np.array([[3, 1], [1, 2]])\nsingular_values = svd_2x2_singular_values(A)\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": false, "result": "failed: list index out of range", "solution": "\nfrom typing import List, Union\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    det = 0\n    sign = 1\n    for j in range(4):\n        sub_matrix = [row[:j] + row[j + 1:] for row in matrix[1:]]\n        det += sign * matrix[0][j] * determinant_4x4(sub_matrix)\n        sign *= -1\n    return det\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: Singular matrix", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n\n    Args:\n        X (list[list[float]]): Features matrix (n_samples, n_features).\n        y (list[float]): Target vector (n_samples,).\n\n    Returns:\n        list[float]: Coefficients of the linear regression model (n_features + 1,).\n    \"\"\"\n    X = np.column_stack((np.ones(len(X)), X))\n    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n    return [round(coef, 4) for coef in beta]\nX = [[1, 2], [3, 4], [5, 6]]\ny = [7, 8, 9]\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    X = np.hstack((np.ones((m, 1)), X))\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        error = h - y\n        gradient = np.dot(X.T, error) / m\n        theta -= alpha * gradient\n    return theta.round(4).tolist()\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a given 2D numpy array.\n\n    Args:\n        data (np.ndarray): A 2D numpy array of shape (n_samples, n_features).\n\n    Returns:\n        tuple: Two lists of lists containing the scaled data. The first list contains\n               the standardized data, and the second list contains the normalized data.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    return (standardized_data.tolist(), normalized_data.tolist())\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": false, "result": "failed: name 'k_means_clustering' is not defined", "solution": "import numpy as np\nimport numpy as np\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,) (8,) ", "solution": "import numpy as np\nimport numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Args:\n        X (np.ndarray): Input features matrix.\n        y (np.ndarray): Target vector.\n        k (int): Number of folds.\n        shuffle (bool): Whether to shuffle the data before splitting.\n        random_seed (int): Random seed for reproducibility.\n\n    Returns:\n        List[int]: A list containing the train-test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('The number of samples in X must be equal to the number of samples in y.')\n    if shuffle:\n        combined = np.hstack((X, y.reshape(-1, 1)))\n        np.random.shuffle(combined)\n        (X_shuffled, y_shuffled) = (combined[:, :-1], combined[:, -1])\n    fold_size = len(X) // k\n    train_test_indices = []\n    for i in range(k):\n        start_index = i * fold_size\n        end_index = start_index + fold_size\n        train_indices = np.arange(start_index, end_index)\n        test_indices = np.arange(end_index, len(X))\n        train_test_indices.append((train_indices, test_indices))\n    return train_test_indices\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,3) (3,2) ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a given 2D NumPy array.\n\n    Parameters:\n    - data (np.ndarray): A 2D NumPy array where rows represent data samples and columns represent features.\n    - k (int): The number of principal components to return.\n\n    Returns:\n    - list[list[float]]: A list containing the principal components (eigenvectors).\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    return sorted_eigenvectors[:k].round(4)\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: maximum recursion depth exceeded in comparison", "solution": "import math\nfrom collections import Counter\nimport math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(data):\n        labels = [example[target_attr] for example in data]\n        label_counts = Counter(labels)\n        total_count = len(labels)\n        entropy = 0.0\n        for count in label_counts.values():\n            probability = count / total_count\n            entropy -= probability * math.log2(probability)\n        return entropy\n    if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n        return {target_attr: examples[0][target_attr]}\n    best_attribute = None\n    min_entropy = float('inf')\n    for attr in attributes:\n        unique_values = set([example[attr] for example in examples])\n        entropy_sum = 0.0\n        for value in unique_values:\n            subset = [example for example in examples if example[attr] == value]\n            subset_entropy = entropy(subset)\n            entropy_sum += len(subset) / len(examples) * subset_entropy\n        if entropy_sum < min_entropy:\n            min_entropy = entropy_sum\n            best_attribute = attr\n    tree = {best_attribute: {}}\n    for value in set([example[best_attribute] for example in examples]):\n        subset = [example for example in examples if example[best_attribute] == value]\n        subtree = learn_decision_tree(subset, attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: shapes (4,) and (2,) not aligned: 4 (dim 0) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Train a kernel SVM classifier using the Pegasos algorithm.\n\n    Parameters:\n    - data: A 2D numpy array representing the input features.\n    - labels: A 1D numpy array representing the corresponding labels.\n    - kernel: A string indicating the type of kernel ('linear' or 'rbf').\n    - lambda_val: The regularization parameter for the SVM.\n    - iterations: The number of iterations for the Pegasos algorithm.\n    - sigma: The standard deviation for the Gaussian kernel (used only if kernel=='rbf').\n\n    Returns:\n    - A list containing the alpha coefficients and the bias of the trained model.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0\n    for _ in range(iterations):\n        for i in range(n_samples):\n            xi = data[i]\n            yi = labels[i]\n            if kernel == 'linear':\n                prediction = np.dot(alpha, xi) + bias\n                if yi * prediction <= 1:\n                    alpha[i] += lambda_val / (n_samples * sigma ** 2)\n                    bias += yi\n            elif kernel == 'rbf':\n                dists = np.linalg.norm(xi - data, axis=1)\n                prediction = np.sum(np.exp(-dists ** 2 / (2 * sigma ** 2)) * alpha) + bias\n                if yi * prediction <= 1:\n                    alpha[i] += lambda_val / (n_samples * sigma ** 2)\n                    bias += yi\n    alpha_reshaped = alpha.reshape(1, -1)\n    return alpha_reshaped.tolist()\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nlabels = np.array([1, -1, 1])\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input value z.\n    \n    Args:\n    z (float): The input value for which the sigmoid function is to be computed.\n    \n    Returns:\n    float: The output of the sigmoid function after rounding to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n    scores (list[float]): A list of floating-point numbers representing the input scores.\n\n    Returns:\n    list[float]: A list of softmax values corresponding to the input scores, rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    total_exp = sum(exp_scores)\n    softmax_values = [exp / total_exp for exp in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values\nscores = [1.0, 2.0, 3.0]\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for -: 'list' and 'list'", "solution": "import math\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features = np.array(features)\n    weighted_sum = np.dot(features, weights) + bias\n    predictions = [sigmoid(feature) for feature in weighted_sum]\n    mse = np.mean((predictions - labels) ** 2)\n    predictions_rounded = [round(prediction, 4) for prediction in predictions]\n    mse_rounded = round(mse, 4)\n    return (predictions_rounded, mse_rounded)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        errors = predictions - labels\n        bias -= learning_rate * np.mean(errors)\n        gradients = features.T @ errors / len(labels)\n        weights -= learning_rate * gradients\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n    return (weights, bias, mse_values.tolist())\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_bias = 0\nlearning_rate = 0.5\nepochs = 100\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\nclass Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (self.data > 0) * out.grad\n        out._backward = _backward\n        return out\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.inv(C).dot(B)\n    return P.tolist()\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    AT_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(AT_A)\n    singular_values = np.sqrt(eigenvalues)\n    normalized_eigenvectors = eigenvectors / np.linalg.norm(eigenvectors, axis=0)\n    U = np.dot(np.diag(singular_values), normalized_eigenvectors)\n    return (U.tolist(), singular_values.tolist(), normalized_eigenvectors.tolist())\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)", "solution": "import numpy as np\nimport numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Perform a random shuffle of the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n\n    Parameters:\n    - X (numpy.ndarray): First input array.\n    - y (numpy.ndarray): Second input array.\n    - seed (int): Optional seed for reproducibility.\n\n    Returns:\n    - list: A list containing the shuffled X and y arrays.\n    \"\"\"\n    combined = np.hstack((X, y))\n    X_shuffled = combined[:, :-1]\n    y_shuffled = combined[:, -1:]\n    if seed is not None:\n        np.random.seed(seed)\n    np.random.shuffle(combined)\n    X_shuffled = combined[:, :-1]\n    y_shuffled = combined[:, -1:]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches from a numpy array X and an optional numpy array y.\n    \n    Parameters:\n    - X: numpy.ndarray, the input data.\n    - y: numpy.ndarray, optional, the target labels.\n    - batch_size: int, the size of each batch.\n    \n    Returns:\n    - A generator yielding batches of data or (X, y) pairs if y is provided.\n    \"\"\"\n    num_samples = len(X)\n    for i in range(0, num_samples, batch_size):\n        end_index = min(i + batch_size, num_samples)\n        batch_X = X[i:end_index]\n        if y is not None:\n            batch_y = y[i:end_index]\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset into two subsets based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the dataset.\n    feature_i (int): The index of the feature to be used for division.\n    threshold (float): The threshold value for dividing the dataset.\n\n    Returns:\n    tuple: A tuple containing two numpy arrays. The first array contains rows where the value of the specified feature is greater than or equal to the threshold,\n           and the second array contains rows where the value of the specified feature is less than the threshold.\n    \"\"\"\n    mask_greater = X[:, feature_i] >= threshold\n    mask_less = X[:, feature_i] < threshold\n    subset_greater = X[mask_greater]\n    subset_less = X[mask_less]\n    subset_greater_list = subset_greater.tolist()\n    subset_less_list = subset_less.tolist()\n    return (subset_greater_list, subset_less_list)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": false, "result": "failed: need at least one array to concatenate", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    - X (np.ndarray): A 2D numpy array where each row represents a data point.\n    - degree (int): The degree of the polynomial features to be generated.\n\n    Returns:\n    - np.ndarray: A 2D numpy array containing the polynomial features.\n    \"\"\"\n    poly_features = []\n    for r in range(degree + 1):\n        for combo in combinations_with_replacement(range(len(X[0])), r):\n            power_values = [X[:, i] ** power for (i, power) in enumerate(combo)]\n            feature_vector = np.concatenate(power_values)\n            poly_features.append(feature_vector)\n    poly_features_array = np.array(poly_features).T\n    return poly_features_array.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a given dataset.\n\n    Parameters:\n        X (np.ndarray): A 2D numpy array representing the dataset features.\n        y (np.ndarray): A 1D numpy array representing the dataset labels.\n        n_subsets (int): The number of random subsets to generate.\n        replacements (bool): Whether to allow subsets to contain duplicate elements (True) or not (False).\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        List of tuples containing random subsets of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    indices = np.arange(len(y))\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            selected_indices = np.random.choice(indices, size=len(y), replace=True)\n        else:\n            selected_indices = np.random.choice(indices, size=len(y), replace=False)\n        X_subset = X[selected_indices]\n        y_subset = y[selected_indices]\n        subsets.append((X_subset, y_subset))\n    return subsets.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])\nn_subsets = 3\nsubsets = get_random_subsets(X, y, n_subsets)\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": false, "result": "failed: 'list' object has no attribute 'tolist'", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    - x (np.ndarray): A 1D numpy array of integer values.\n    - n_col (int, optional): Number of columns for the one-hot encoded array. Defaults to None.\n\n    Returns:\n    - list: One-hot encoded array as a list of lists.\n    \"\"\"\n    if n_col is None:\n        n_col = max(x) + 1\n    one_hot_encoded = [[0] * n_col for _ in range(len(x))]\n    for (i, val) in enumerate(x):\n        one_hot_encoded[i][val] = 1\n    return one_hot_encoded.tolist()\nx = np.array([1, 2, 3, 4, 5])\none_hot_encoded = to_categorical(x)\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array to a diagonal matrix.\n\n    Args:\n    x (np.array): A 1D numpy array of shape (n,) where n is the number of elements in the diagonal.\n\n    Returns:\n    np.array: A 2D numpy array representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Args:\n    - y_true (np.array): An array containing the true labels.\n    - y_pred (np.array): An array containing the predicted labels.\n\n    Returns:\n    - float: The accuracy score as a float, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true_bool = y_true.astype(bool)\n    y_pred_bool = y_pred.astype(bool)\n    correct_predictions = np.sum(y_true_bool == y_pred_bool)\n    total_samples = len(y_true)\n    accuracy = correct_predictions / total_samples\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 0, 1])\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    X (np.array): A 2D numpy array representing the data.\n    Y (np.array, optional): An optional 2D numpy array representing another dataset. If None, calculates the correlation matrix of X with itself.\n\n    Returns:\n    np.array: A 2D numpy array representing the correlation matrix.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X)\n    else:\n        correlation_matrix = np.corrcoef(X, Y)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_list = correlation_matrix_rounded.tolist()\n    return correlation_list\nX = np.array([[1, 2], [3, 4]])\nY = np.array([[5, 6], [7, 8]])\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    n_samples = X.shape[0]\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_thresholds = [np.inf] * X.shape[1]\n        best_error = float('inf')\n        for feature_idx in range(X.shape[1]):\n            sorted_indices = np.argsort(X[:, feature_idx])\n            thresholds = np.unique(X[sorted_indices, feature_idx])\n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = X[:, feature_idx] > threshold\n                left_error = sum(sample_weights[left_mask] * (y[left_mask] != 1))\n                right_error = sum(sample_weights[right_mask] * (y[right_mask] != -1))\n                total_error = left_error + right_error\n                if total_error < best_error:\n                    best_error = total_error\n                    best_threshold = threshold\n                    alpha = 0.5 * math.log((1 - best_error) / best_error)\n                    for i in range(n_samples):\n                        if X[i, feature_idx] <= best_threshold:\n                            sample_weights[i] *= math.exp(-alpha)\n                        else:\n                            sample_weights[i] *= math.exp(alpha)\n                    sample_weights /= sum(sample_weights)\n                    classifiers.append({'feature': feature_idx, 'threshold': best_threshold, 'alpha': alpha})\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    exp_scores = np.exp(scores_array)\n    sum_exp_scores = np.sum(exp_scores)\n    log_softmax_values = np.log(exp_scores) - np.log(sum_exp_scores)\n    return log_softmax_values.tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: Dense.initialize() takes 1 positional argument but 2 were given", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nimport numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n    '\\n    implement the Dense class based on the above structure\\n    '\n\n    def initialize(self):\n        if self.input_shape is not None:\n            self.W = np.random.uniform(-1 / np.sqrt(self.input_shape[0]), 1 / np.sqrt(self.input_shape[0]), (self.input_shape[0], self.n_units))\n            self.w0 = np.zeros((1, self.n_units))\n            self.optimizer_W = copy.deepcopy(self.input_shape)\n            self.optimizer_w0 = copy.deepcopy(self.n_units)\n\n    def parameters(self):\n        return sum([self.W.size, self.w0.size])\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        dW = np.dot(self.layer_input.T, accum_grad)\n        dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W -= self.optimizer_W * dW\n            self.w0 -= self.optimizer_w0 * dw0\n        return dW\n\n    def output_shape(self):\n        return (self.n_units,)\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    out_height = (input_matrix.shape[0] - kernel.shape[0] + 2 * padding) // stride + 1\n    out_width = (input_matrix.shape[1] - kernel.shape[1] + 2 * padding) // stride + 1\n    if padding > 0:\n        padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    else:\n        padded_input = input_matrix\n    output_matrix = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            patch = padded_input[i * stride:i * stride + kernel.shape[0], j * stride:j * stride + kernel.shape[1]]\n            output_matrix[i, j] = np.sum(patch * kernel)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 2\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the given input.\n\n    Args:\n    z (float): Input value\n\n    Returns:\n    float: Output after applying the ReLU function\n    \"\"\"\n    return max(0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        w (np.ndarray): Coefficients vector of shape (n_features,).\n        y_true (np.ndarray): True labels of shape (n_samples,).\n        alpha (float): Regularization parameter.\n\n    Returns:\n        float: Ridge loss value.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization_term = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization_term\n    return round(total_loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function to the input z.\n\n    Parameters:\n    - z: A float representing the input value.\n    - alpha: A float representing the slope for negative inputs, with a default value of 0.01.\n\n    Returns:\n    - The result of the Leaky ReLU activation function applied to z.\n    \"\"\"\n    if z < 0:\n        return alpha * z\n    else:\n        return z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n\n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: The computed linear kernel value.\n    \"\"\"\n    return np.dot(x1, x2)\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Args:\n    y_true (np.ndarray): A 1D numpy array containing the true binary labels.\n    y_pred (np.ndarray): A 1D numpy array containing the predicted binary labels.\n\n    Returns:\n    float: The precision score.\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp > 0:\n        precision_score = tp / (tp + fp)\n    else:\n        precision_score = 0\n    return precision_score\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    if method == 'sgd':\n        for _ in range(n_iterations):\n            random_index = np.random.randint(0, len(X))\n            x_batch = X[random_index].reshape(1, -1)\n            y_batch = y[random_index]\n            gradient = 2 * x_batch.T.dot(x_batch.dot(weights) - y_batch) / batch_size\n            weights -= learning_rate * gradient\n    elif method == 'batch':\n        for _ in range(n_iterations):\n            gradient = 2 * X.T.dot(X.dot(weights) - y) / len(X)\n            weights -= learning_rate * gradient\n    elif method == 'mini_batch':\n        for i in range(0, len(X), batch_size):\n            x_batch = X[i:i + batch_size]\n            y_batch = y[i:i + batch_size]\n            gradient = 2 * x_batch.T.dot(x_batch.dot(weights) - y_batch) / batch_size\n            weights -= learning_rate * gradient\n    return weights.tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    A = np.array(matrix)\n    rank = np.linalg.matrix_rank(A)\n    A = np.round(A, decimals=6)\n    rref_matrix = np.zeros((rank, A.shape[1]))\n    for i in range(rank):\n        pivot_row = np.argmax(np.abs(A[i:, i])) + i\n        if pivot_row != i:\n            A[[i, pivot_row]] = A[[pivot_row, i]]\n        A[i] /= A[i, i]\n        for j in range(i + 1, rank):\n            factor = A[j, i]\n            A[j] -= factor * A[i]\n    rref_matrix = rref_matrix[:rank, :].tolist()\n    return rref_matrix\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for i in range(num_iterations):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** (i + 1))\n        v_hat = v / (1 - beta2 ** (i + 1))\n        x0 -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x0.tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (2,3) (2,) ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = np.mean(X.T * (y_pred - y) + alpha * np.sign(w), axis=1)\n        db = np.mean(y_pred - y)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        loss = np.mean((y_pred - y) ** 2) + alpha * np.sum(np.abs(w))\n        if np.abs(loss - previous_loss) < tol:\n            break\n        previous_loss = loss\n    return (w.tolist(), b.tolist())\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1\n    return dp[m][n]\nsource = 'caper'\ntarget = 'acer'\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric in binary classification.\n\n    Parameters:\n    - y_true: List of true binary labels (0 or 1).\n    - y_pred: List of predicted binary labels (0 or 1).\n\n    Returns:\n    - float: The recall value rounded to three decimal places.\n    \"\"\"\n    TP = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 1))\n    FN = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 0))\n    recall_value = TP / (TP + FN) if TP + FN != 0 else 0.0\n    return round(recall_value, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": false, "result": "failed: module 'numpy' has no attribute 'softmax'", "solution": "import numpy as np\nimport numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n\n    Args:\n        X (numpy.ndarray): Input data with shape (batch_size, seq_len, embedding_dim).\n        W_q (numpy.ndarray): Query matrix with shape (embedding_dim, hidden_dim).\n        W_k (numpy.ndarray): Key matrix with shape (embedding_dim, hidden_dim).\n        W_v (numpy.ndarray): Value matrix with shape (embedding_dim, hidden_dim).\n\n    Returns:\n        numpy.ndarray: Self-attention output with shape (batch_size, seq_len, hidden_dim).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attn_scores = np.matmul(Q, K.T) / np.sqrt(W_k.shape[1])\n    attn_weights = np.softmax(attn_scores, axis=1)\n    context_vector = np.sum(attn_weights * V, axis=1)\n    return context_vector.tolist()\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": false, "result": "failed: name 'next_hidden_state_np' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    output = []\n    input_sequence_np = np.array(input_sequence)\n    initial_hidden_state_np = np.array(initial_hidden_state)\n    (seq_len, input_dim) = input_sequence_np.shape\n    num_units = len(Wx[0])\n    input_dim = len(Wx)\n    initial_hidden_state_np = np.reshape(initial_hidden_state_np, (1, num_units))\n    for t in range(seq_len):\n        weighted_sum = np.dot(input_sequence_np[t], Wx) + np.dot(initial_hidden_state_np, Wh) + b\n        next_hidden_state = np.tanh(weighted_sum)\n        output.append(next_hidden_state.tolist())\n        initial_hidden_state_np = next_hidden_state_np\n    return output\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": false, "result": "failed: shapes (3,3) and (2,3) not aligned: 3 (dim 1) != 2 (dim 0)", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Args:\n    - points (list): A list of [x, y] coordinates.\n    - tx (float): The translation distance in the x direction.\n    - ty (float): The translation distance in the y direction.\n\n    Returns:\n    - list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    return translated_points.tolist()\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    - mu_p: Mean of the first normal distribution (float)\n    - sigma_p: Standard deviation of the first normal distribution (float)\n    - mu_q: Mean of the second normal distribution (float)\n    - sigma_q: Standard deviation of the second normal distribution (float)\n\n    Returns:\n    - The KL divergence as a float\n    \"\"\"\n    if sigma_p == 0 or sigma_q == 0:\n        raise ValueError('Both standard deviations must be non-zero.')\n    kl_div = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 1 / 2\n    return kl_div\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x_ini = np.zeros_like(b)\n    A = np.array(A)\n    b = np.array(b)\n    m = A.shape[0]\n    x = x_ini\n    for _ in range(n):\n        for i in range(m):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return x.tolist()\nA = [[3, 2, 1], [1, 4, 2], [2, 1, 3]]\nb = [5, 8, 9]\nn = 100\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    n = A.shape[0]\n    for i in range(n - 1):\n        max_row_index = i\n        for j in range(i + 1, n):\n            if abs(A[j, i]) > abs(A[max_row_index, i]):\n                max_row_index = j\n        A[[i, max_row_index]] = A[[max_row_index, i]]\n        b[[i, max_row_index]] = b[[max_row_index, i]]\n        A[i, :] /= A[i, i]\n        for j in range(i + 1, n):\n            factor = A[j, i]\n            A[j, :] -= factor * A[i, :]\n            b[j] -= factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return [round(val, 4) for val in x.tolist()]\nA = np.array([[3, 2, 1], [1, 4, 5], [6, 7, 8]])\nb = np.array([9, 12, 15])\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": false, "result": "failed: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        batch_size = x.shape[0]\n        num_steps = x.shape[1]\n        hidden_states = []\n        cell_states = []\n        h = initial_hidden_state\n        c = initial_cell_state\n        for t in range(num_steps):\n            concat_input = np.hstack((x[t], h))\n            f = sigmoid(concat_input.dot(self.Wf) + self.bf)\n            i = sigmoid(concat_input.dot(self.Wi) + self.bi)\n            g = tanh(concat_input.dot(self.Wc) + self.bc)\n            c = f * c + i * g\n            o = sigmoid(concat_input.dot(self.Wo) + self.bo)\n            h = o * tanh(c)\n            hidden_states.append(h.reshape(-1))\n            cell_states.append(c.reshape(-1))\n        return (hidden_states, h.reshape(-1), c.reshape(-1))\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\ninput_size = 5\nhidden_size = 3\nnum_steps = 10\nbatch_size = 2\nx = np.random.rand(batch_size, num_steps, input_size)\ninitial_hidden_state = np.zeros((batch_size, hidden_size))\ninitial_cell_state = np.zeros((batch_size, hidden_size))\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: 'dict' object has no attribute 'index'", "solution": "import numpy as np\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores for a query against a given corpus of documents.\n\n    Parameters:\n    - corpus: List of documents, where each document is a list of words.\n    - query: List of words for which you want to compute the TF-IDF scores.\n\n    Returns:\n    - A list of lists containing the TF-IDF scores for the query words in each document,\n      rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus cannot be empty.')\n    df = {}\n    for doc in corpus:\n        for word in doc:\n            if word in df:\n                df[word] += 1\n            else:\n                df[word] = 1\n    num_docs = len(corpus)\n    tf = []\n    for doc in corpus:\n        term_freq = {}\n        for word in doc:\n            if word in term_freq:\n                term_freq[word] += 1\n            else:\n                term_freq[word] = 1\n        tf.append(term_freq)\n    idf = []\n    for (word, count) in df.items():\n        idf.append(np.log(num_docs / count))\n    tfidf_scores = []\n    for (i, doc) in enumerate(tf):\n        doc_tfidf = []\n        for (word, freq) in doc.items():\n            doc_tfidf.append(freq * idf[df.index(word)])\n        tfidf_scores.append(doc_tfidf)\n    tfidf_scores = [[round(score, 5) for score in row] for row in tfidf_scores]\n    return tfidf_scores.tolist()\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: 'SimpleRNN' object has no attribute 'output_size'", "solution": "import numpy as np\nimport numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_t = np.zeros((self.hidden_size, 1))\n        y_t = np.zeros((self.output_size, 1))\n        for t in range(x.shape[0]):\n            h_t = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h_t) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n        return (y_t, h_t)\n\n    def backward(self, W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Perform backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(W_xh)\n        dW_hh = np.zeros_like(W_hh)\n        dW_hy = np.zeros_like(W_hy)\n        db_h = np.zeros_like(b_h)\n        db_y = np.zeros_like(b_y)\n        delta_y = outputs[-1] - expected_output\n        dW_hy += np.dot(delta_y, last_hiddens[-1].T)\n        db_y += delta_y\n        dh_next = np.dot(W_hy.T, delta_y)\n        for t in reversed(range(len(outputs) - 1)):\n            delta_h = dh_next * (1 - outputs[t] ** 2)\n            dW_hh += np.dot(delta_h, last_hiddens[t].T)\n            db_h += delta_h\n            dh_next = np.dot(W_hh.T, delta_h)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": false, "result": "failed: local variable 'x' referenced before assignment", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    r = b - np.dot(A, x0)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n        if np.linalg.norm(r) < tol:\n            break\n    return x.tolist()\nA = np.array([[4, 2], [2, 5]])\nb = np.array([3, 7])\nx = conjugate_gradient(A, b, 100)\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = {}\n    for label in y:\n        if label in class_counts:\n            class_counts[label] += 1\n        else:\n            class_counts[label] = 1\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        proportion = count / total_samples\n        gini -= proportion ** 2\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0])\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                column_indices.append(j)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)\ndense_matrix = [[0, 1, 0], [4, 0, 6], [0, 7, 0]]\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    magnitude_squared_L = np.dot(L, L)\n    scalar_projection = dot_product / magnitude_squared_L\n    projection = scalar_projection * L\n    return [round(x, 3) for x in projection.tolist()]\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * cols\n    for i in range(rows):\n        for j in range(cols):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n                column_pointer[j] += 1\n    for i in range(1, cols):\n        column_pointer[i] += column_pointer[i - 1]\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space of a given matrix A.\n\n    Parameters:\n    A (np.ndarray): The input matrix.\n\n    Returns:\n    list: A list of basis vectors that span the column space of A.\n    \"\"\"\n    R = np.linalg.matrix_rank(A)\n    row_echelon_form = np.array([A[i] for i in range(R)])\n    basis_vectors = row_echelon_form[:, :R]\n    basis_vectors_rounded = [np.round(vec, 8) for vec in basis_vectors.tolist()]\n    return basis_vectors_rounded\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression analysis.\n\n    Parameters:\n    - y_true (np.array): Array of true values.\n    - y_pred (np.array): Array of predicted values.\n\n    Returns:\n    - float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    mean_y_true = np.mean(y_true)\n    tss = np.sum((y_true - mean_y_true) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r_squared_value = 1 - rss / tss\n    return round(r_squared_value, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport numpy as np\ndef calculate_brightness(img):\n    if not img or len(img) == 0:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    average_brightness = round(total_brightness / pixel_count, 2)\n    return average_brightness\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.array): Array containing the actual values.\n    y_pred (np.array): Array containing the predicted values.\n    \n    Returns:\n    float: The RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the input arrays have different shapes or are empty.\n    TypeError: If the input arrays are not of type np.ndarray.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Both y_true and y_pred must be of type np.ndarray.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_diffs = (y_true - y_pred) ** 2\n    mean_squared_diff = np.mean(squared_diffs)\n    rmse_value = np.sqrt(mean_squared_diff)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n\n    Parameters:\n    - y_true: numpy.ndarray, binary array of true labels\n    - y_pred: numpy.ndarray, binary array of predicted labels\n\n    Returns:\n    - float, Jaccard Index rounded to three decimal places\n    \"\"\"\n    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    TN = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n    FP = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if TP + FN + TN + FP == 0:\n        return 1.0\n    else:\n        Jaccard = TP / (TP + FN + TN + FP)\n        return round(Jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Args:\n    y_true (np.array): Binary array representing true labels.\n    y_pred (np.array): Binary array representing predicted labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    num_true = np.sum(y_true)\n    num_pred = np.sum(y_pred)\n    if num_true == 0 or num_pred == 0:\n        dice_score = 0.0\n    else:\n        dice_score = 2 * intersection / (num_true + num_pred)\n    return round(dice_score, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    - row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Returns:\n    - A composite hypervector representing the entire row as a list.\n    \"\"\"\n    composite_hv = []\n    for (feature_name, feature_value) in row.items():\n        feature_seed = random_seeds.get(feature_name)\n        feature_hv = np.random.randn(dim) * feature_seed\n        composite_hv.extend(feature_hv.tolist())\n    return composite_hv\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Args:\n        data (list of lists): Each inner list contains two elements [y_true, y_pred],\n                             representing the true and predicted labels for one observation.\n\n    Returns:\n        list of lists: A 2x2 confusion matrix.\n    \"\"\"\n    tp = sum((1 for (y_true, y_pred) in data if y_true == 1 and y_pred == 1))\n    fp = sum((1 for (y_true, y_pred) in data if y_true == 0 and y_pred == 1))\n    fn = sum((1 for (y_true, y_pred) in data if y_true == 1 and y_pred == 0))\n    tn = sum((1 for (y_true, y_pred) in data if y_true == 0 and y_pred == 0))\n    confusion_mat = [[tn, fp], [fn, tp]]\n    return confusion_mat\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n\n    Args:\n    v1 (np.ndarray): The first vector.\n    v2 (np.ndarray): The second vector.\n\n    Returns:\n    float: The cosine similarity of the two vectors, rounded to three decimal places.\n    \"\"\"\n    if len(v1) != len(v2) or len(v1) == 0:\n        raise ValueError('Vectors must be non-empty and of the same length.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: 'bool' object is not iterable", "solution": "from collections import Counter\nfrom collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate various performance metrics for a binary classification problem.\n\n    Args:\n    actual: List of actual class labels (1 for positive, 0 for negative).\n    predicted: List of predicted class labels from the model.\n\n    Returns:\n    A tuple containing:\n    - confusion_matrix: A 2x2 matrix.\n    - accuracy: A float representing the accuracy of the model.\n    - f1_score: A float representing the F1 score of the model.\n    - specificity: A float representing the specificity of the model.\n    - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    actual_counter = Counter(actual)\n    predicted_counter = Counter(predicted)\n    confusion_matrix = [[actual_counter.get(1, 0) - predicted_counter.get(1, 0), actual_counter.get(0, 0) + predicted_counter.get(1, 0)], [actual_counter.get(0, 0) - predicted_counter.get(0, 0), actual_counter.get(1, 0) + predicted_counter.get(0, 0)]]\n    total_samples = sum(actual)\n    correct_predictions = sum((actual[i] == predicted[i] for i in range(total_samples)))\n    accuracy = round(correct_predictions / total_samples, 3)\n    precision = round(correct_predictions / sum(predicted if actual else 0), 3)\n    recall = round(correct_predictions / sum(actual), 3)\n    f1_score = round(2 * precision * recall / (precision + recall), 3)\n    true_negatives = sum((not actual[i] and (not predicted[i]) for i in range(total_samples)))\n    specificity = round(true_negatives / sum(not actual), 3)\n    false_positives = sum((actual[i] and (not predicted[i]) for i in range(total_samples)))\n    negative_predictive_value = round(false_positives / sum(not predicted), 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Args:\n    - data: A list or NumPy array of numerical values.\n\n    Returns:\n    A dictionary containing the calculated descriptive statistics metrics.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    try:\n        mode = np.bincount(data).argmax()\n    except ValueError:\n        mode = 'No unique mode'\n    variance = np.var(data)\n    std_dev = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = np.diff(percentiles)\n    return {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(iqr[0], 4)}\ndata = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30]\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials rounded to five decimal places\n    \"\"\"\n    if not (isinstance(n, int) and isinstance(k, int) and isinstance(p, float)):\n        raise ValueError('All inputs must be integers or floats.')\n    if not (0 <= n <= 100 and 0 <= k <= n and (0 <= p <= 1)):\n        raise ValueError('Invalid input values for n, k, and p.')\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    Args:\n    - x: The value at which the PDF is evaluated.\n    - mean: The mean (\u03bc) of the distribution.\n    - std_dev: The standard deviation (\u03c3) of the distribution.\n    \n    Returns:\n    - The PDF value of the normal distribution for the given x, rounded to 5 decimal places.\n    \"\"\"\n    exponent = -1 / (2 * std_dev ** 2) * (x - mean) ** 2\n    pdf_value = math.exp(exponent) / math.sqrt(2 * math.pi * std_dev ** 2)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be non-negative and lam must be positive.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": false, "result": "failed: Input must be a single-channel uint8 numpy array.", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.dtype != np.uint8 or img.shape[2] != 1:\n        raise ValueError('Input must be a single-channel uint8 numpy array.')\n    max_val = np.max(img)\n    min_val = np.min(img)\n    if max_val == min_val:\n        return 0\n    contrast = (max_val - min_val) / max_val\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('Vectors must be of the same length')\n    result = np.dot(vec1, vec2)\n    return result\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_features = []\n    for point in data:\n        poly_features = [point ** i for i in range(degree + 1)]\n        poly_features = [round(feature, 8) for feature in poly_features]\n        transformed_features.append(poly_features)\n    return transformed_features\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: could not broadcast input array from shape (2,) into shape (8,)", "solution": "import numpy as np\nimport numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encodings for a sequence length (position) and model dimensionality (d_model).\n    \n    Parameters:\n    - position: int, the position in the sequence.\n    - d_model: int, the dimensionality of the model.\n    \n    Returns:\n    - List[np.float16], the calculated positional encodings.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for i in range(0, position // 2):\n        val = 1 / 10000 ** (2 * i / d_model)\n        pe[i] = [val, 0]\n        pe[position - 1 - i] = [-val, 0]\n    for i in range(position // 2, position):\n        val = 1 / 10000 ** (2 * (i - position // 2) / d_model)\n        pe[i] = [np.sin(val), np.cos(val)]\n    return pe.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_corrected = m / (1 - beta1 ** t)\n    v_corrected = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_corrected\n    return [np.round(updated_parameter, 5).tolist(), np.round(m_corrected, 5).tolist(), np.round(v_corrected, 5).tolist()]\nlearning_rate = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (1,3) (1024,10) ", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\nimport numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_ids = encoder.encode(prompt)\n    hiddens = np.zeros((hparams['n_ctx'], params['wte'].shape[1]))\n    generated_tokens = []\n    for _ in range(n_tokens_to_generate):\n        pos_embeddings = np.expand_dims(np.arange(len(prompt_ids)), axis=1) * np.sqrt(hparams['n_ctx'])\n        combined_embeddings = np.dot(params['wte'][prompt_ids], params['wte'].T) + pos_embeddings + params['wpe']\n        attentions = np.dot(combined_embeddings, params['wte'].T)\n        attentions /= np.linalg.norm(attentions, axis=-1, keepdims=True)\n        attention_weights = np.dot(attns, params['ln_f']['g'] / (np.abs(params['ln_f']['g']) + 1e-08))\n        feed_forward_output = np.maximum(0, np.dot(combined_embeddings, params['wte'].T))\n        final_output = np.dot(feed_forward_output, params['ln_f']['g'] / (np.abs(params['ln_f']['g']) + 1e-08)) + params['ln_f']['b']\n        next_token_index = np.argmax(final_output)\n        next_token_id = prompt_ids[next_token_index]\n        generated_tokens.append(next_token_id)\n        hiddens = np.roll(hiddens, -1, axis=0)\n        hiddens[-1] = final_output\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        \"\"\"\n        Compute the softmax function for a given array of values.\n        \n        Args:\n        values (np.array): An array of numerical values.\n        \n        Returns:\n        np.array: An array of softmax probabilities corresponding to the input values.\n        \"\"\"\n        exp_values = np.exp(values)\n        sum_exp_values = np.sum(exp_values)\n        return exp_values / sum_exp_values\n    attention_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            similarity = np.dot(crystal_values[i], crystal_values[j]) / (np.linalg.norm(crystal_values[i]) * np.linalg.norm(crystal_values[j]))\n            attention_matrix[i][j] = softmax(similarity)\n            attention_matrix[j][i] = attention_matrix[i][j]\n    weighted_patterns = []\n    for i in range(n):\n        pattern = np.zeros(dimension)\n        for j in range(n):\n            if attention_matrix[i][j] > 0:\n                pattern += attention_matrix[i][j] * crystal_values[j]\n        weighted_patterns.append(round(np.linalg.norm(pattern), 4))\n    return weighted_patterns\nn = 5\ncrystal_values = np.random.rand(5, 3)\ndimension = 3\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    num_docs = len(corpus)\n    total_terms = sum((len(doc) for doc in corpus))\n    term_freqs = {}\n    for (doc_id, doc) in enumerate(corpus):\n        term_freqs[doc_id] = Counter(doc)\n    idf = {}\n    for term in set(query):\n        df = sum((1 for doc in corpus if term in doc))\n        idf[term] = np.log((num_docs - df + 0.5) / (df + 0.5))\n    bm25_scores = []\n    for (doc_id, doc) in enumerate(corpus):\n        score = 0\n        for term in query:\n            tf = term_freqs[doc_id][term]\n            doc_len = len(doc)\n            norm_doc_len = doc_len ** (1 - b) / (1 - b + b * doc_len)\n            score += tf * idf[term] * (k1 + 1) / (tf + k1 * (1 - b + b * doc_len))\n        bm25_scores.append(round(score, 3))\n    return bm25_scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": false, "result": "failed: 'NoneType' object is not callable", "solution": "\nfrom sklearn.metrics import f1_score\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    return round(f1, 3)\ny_true = [0, 1, 2, 1, 0, 2, 1]\ny_pred = [0, 2, 2, 2, 0, 2, 1]\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": false, "result": "timed out", "solution": "import math\nPI = 3.14159\nimport numpy as np\nfrom scipy.stats import linregress\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Calculates the forecasted base power consumption of the Mars colony on day 15,\n    after removing the daily fluctuation and adding a 5% safety margin.\n\n    Parameters:\n    consumption_data (list): A list of integers representing daily power usage measurements.\n\n    Returns:\n    int: The forecasted base power consumption, rounded up to the nearest integer.\n    \"\"\"\n    detrended_data = [consumption - 10 * math.sin(2 * PI * i / 10) for (i, consumption) in enumerate(consumption_data)]\n    (slope, intercept, r_value, p_value, std_err) = linregress(range(len(detrended_data)), detrended_data)\n    predicted_consumption = slope * 14 + intercept\n    forecasted_base_consumption = predicted_consumption + 10 * math.sin(2 * PI * 14 / 10)\n    forecasted_base_consumption = math.ceil(forecasted_base_consumption * 1.05)\n    return forecasted_base_consumption\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must have the same length.')\n    abs_diff = np.abs(y_true - y_pred)\n    mae_value = np.mean(abs_diff)\n    return round(mae_value, 3)\ny_true = np.array([10, 20, 30, 40, 50])\ny_pred = np.array([9, 21, 31, 39, 51])\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: compute_qkv() takes 4 positional arguments but 5 were given", "solution": "import numpy as np\nimport numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute query, key, and value matrices for multi-head attention.\n\n    Args:\n    X (np.ndarray): Input data of shape (batch_size, sequence_length, input_dim).\n    W_q (np.ndarray): Query weight matrix of shape (input_dim, head_dim * n_heads).\n    W_k (np.ndarray): Key weight matrix of shape (input_dim, head_dim * n_heads).\n    W_v (np.ndarray): Value weight matrix of shape (input_dim, head_dim * n_heads).\n\n    Returns:\n    tuple: A tuple containing computed query, key, and value matrices of shape (batch_size, sequence_length, head_dim * n_heads).\n    \"\"\"\n    (batch_size, sequence_length, input_dim) = X.shape\n    head_dim = input_dim // n_heads\n    q = np.dot(X, W_q)\n    k = np.dot(X, W_k)\n    v = np.dot(X, W_v)\n    return (q, k, v)\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray, mask: np.ndarray=None) -> np.ndarray:\n    \"\"\"\n    Perform self-attention on the input query, key, and value matrices.\n\n    Args:\n    q (np.ndarray): Query matrix of shape (batch_size, sequence_length, head_dim * n_heads).\n    k (np.ndarray): Key matrix of shape (batch_size, sequence_length, head_dim * n_heads).\n    v (np.ndarray): Value matrix of shape (batch_size, sequence_length, head_dim * n_heads).\n    mask (np.ndarray): Masking matrix of shape (batch_size, sequence_length, sequence_length) used to prevent future information from being accessed.\n\n    Returns:\n    np.ndarray: The output of the self-attention operation of shape (batch_size, sequence_length, head_dim * n_heads).\n    \"\"\"\n    scores = np.dot(q, k.T) / np.sqrt(head_dim)\n    if mask is not None:\n        scores = scores - 10000 * mask\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, v)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention on the input data.\n\n    Args:\n    X (np.ndarray): Input data of shape (batch_size, sequence_length, input_dim).\n    W_q (np.ndarray): Query weight matrix of shape (input_dim, head_dim * n_heads).\n    W_k (np.ndarray): Key weight matrix of shape (input_dim, head_dim * n_heads).\n    W_v (np.ndarray): Value weight matrix of shape (input_dim, head_dim * n_heads).\n    n_heads (int): Number of heads for multi-head attention.\n\n    Returns:\n    list: A list containing the output of each head's self-attention operation of shape (batch_size, sequence_length, head_dim).\n    \"\"\"\n    (q, k, v) = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    outputs = [self_attention(q[:, :, i], k[:, :, i], v[:, :, i]) for i in range(n_heads)]\n    outputs = [output.tolist() for output in outputs]\n    return outputs\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nfrom math import sqrt\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    tp = sum((xi * yi for (xi, yi) in zip(x, y)))\n    fp = sum((xi * (1 - yi) for (xi, yi) in zip(x, y)))\n    tn = sum(((1 - xi) * (1 - yi) for (xi, yi) in zip(x, y)))\n    fn = sum(((1 - xi) * yi for (xi, yi) in zip(x, y)))\n    E_o = n * (n + 1) / 2\n    O_o = tp + fn + fp + tn\n    phi = (O_o - (tp * tn - fp * fn)) / sqrt(E_o * (E_o - tp) * (E_o - tn) * (E_o - fp))\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [0, 1, 0, 1, 0, 0]\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return x / 4 + 0.5\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x < 0:\n        return alpha * (math.exp(x) - 1)\n    else:\n        return x\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": false, "result": "timed out", "solution": "\nimport torch\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    else:\n        return x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place\n    \"\"\"\n    if x < -709:\n        return -math.inf\n    elif x > 709:\n        return math.inf\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nfrom typing import *\nimport numpy as np\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(np.sign(x) * np.log(1 + np.abs(x)), 4)\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for /: 'list' and 'list'", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rho_clipped = [min(1.0, max(epsilon, r)) for r in rhos]\n    kl_divergence_gradient = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_objective_value = -np.mean(rho_clipped * A + beta * kl_divergence_gradient)\n    return round(grpo_objective_value, 6)\nrhos = [0.85, 0.75, 0.9, 0.8]\nA = [0.2, 0.3, 0.4, 0.5]\npi_theta_old = [0.1, 0.2, 0.3, 0.4]\npi_theta_ref = [0.05, 0.1, 0.15, 0.2]\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": false, "result": "failed: test case failed: swish(1)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    return x * np.tanh(math.log(1 + x))\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": false, "result": "failed: test case failed: selu(1.0)", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    return round(scale * (math.exp(x) - alpha if x > 0 else scale * x), 4)\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    scores = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-scores))\n    binary_predictions = (probabilities >= 0.5).astype(int)\n    return binary_predictions.tolist()\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 4)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (m, n) = X.shape\n    C = len(np.unique(y))\n    B = np.zeros((C, n))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(B, X.T)\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        cross_entropy_loss = -np.mean(y * np.log(probabilities))\n        losses.append(round(cross_entropy_loss, 4))\n        grad_B = (probabilities - y).T @ X\n        B -= learning_rate * grad_B\n    return (B.tolist(), losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    y (np.ndarray): Target vector of shape (n_samples,)\n    learning_rate (float): Learning rate for gradient descent\n    iterations (int): Number of iterations for training\n    \n    Returns:\n    tuple[list[float], ...]: A tuple containing two lists: \n        - Coefficients of the logistic regression model\n        - Loss values at each iteration\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.random.randn(n_features + 1)\n    losses = []\n    for _ in range(iterations):\n        X_with_bias = np.hstack((X, np.ones((n_samples, 1))))\n        predictions = sigmoid(np.dot(X_with_bias, weights))\n        loss = binary_cross_entropy(y, predictions)\n        losses.append(loss)\n        dw = 1 / n_samples * np.dot(X_with_bias.T, predictions - y)\n        weights -= learning_rate * dw\n    return (weights.tolist(), losses)\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function used in logistic regression.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Binary cross-entropy loss function.\n    \"\"\"\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\nX = np.array([[0, 1], [1, 0], [1, 1]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.01\niterations = 1000\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: operands could not be broadcast together with shapes (6,8) (6,6) ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    Q_reshaped = Q.reshape(-1, Q.shape[-1])\n    K_reshaped = K.reshape(-1, K.shape[-1])\n    V_reshaped = V.reshape(-1, V.shape[-1])\n    masked_QK = Q_reshaped * mask\n    attn_weights = np.dot(masked_QK, K_reshaped.T)\n    attn_weights /= np.sqrt(K_reshaped.shape[1])\n    context_vector = np.dot(attn_weights, V_reshaped)\n    context_vector_reshaped = context_vector.reshape(Q.shape[:2], -1)\n    return context_vector_reshaped.tolist()\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([0,0,0,0,0,1,2,3])", "solution": "\nfrom typing import List\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    :param apples: A list of integers representing the colors of the apples.\n    :return: A float value representing the disorder of the apples.\n    \"\"\"\n    if not apples:\n        return 0.0\n    color_counts = {}\n    for color in apples:\n        if color in color_counts:\n            color_counts[color] += 1\n        else:\n            color_counts[color] = 1\n    total_apples = len(apples)\n    disorder = (total_apples - max(color_counts.values())) / total_apples\n    return round(disorder, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n\n    Args:\n    - X (np.ndarray): A 3D array of shape (batch_size, seq_length, num_features).\n    - gamma (np.ndarray): Scaling parameter of shape (num_features,).\n    - beta (np.ndarray): Shifting parameter of shape (num_features,).\n    - epsilon (float): Small value to prevent division by zero.\n\n    Returns:\n    - list: The normalized tensor rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    return normalized_X.tolist()\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR (Modification of the Error Rate) score for evaluating machine translation quality.\n\n    Args:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float): The alpha parameter for the penalty calculation.\n    - beta (float): The beta parameter for the penalty calculation.\n    - gamma (float): The gamma parameter for the penalty calculation.\n\n    Returns:\n    - float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    ref_counts = Counter(ref_words)\n    cand_counts = Counter(cand_words)\n    common_words = set(ref_words).intersection(set(cand_words))\n    num_unique_ref = len(ref_counts)\n    num_unique_cand = len(cand_counts)\n    num_common_unique = len(common_words)\n    total_words = num_unique_ref + num_unique_cand - num_common_unique\n    if total_words == 0:\n        precision = 0\n    else:\n        precision = num_common_unique / total_words\n    if num_unique_cand == 0:\n        recall = 0\n    else:\n        recall = num_common_unique / num_unique_cand\n    f_measure = 2 * (precision * recall) / (precision + recall)\n    penalty = alpha * (1 - num_unique_cand / num_unique_ref)\n    meteor_score = f_measure - penalty\n    return round(meteor_score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters:\n    - joint_counts (int): The count of occurrences of both events together.\n    - total_counts_x (int): The total count of event X.\n    - total_counts_y (int): The total count of event Y.\n    - total_samples (int): The total number of samples.\n\n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x * p_y == 0:\n        pmi = 0\n    else:\n        pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": false, "result": "failed: division by zero", "solution": "\nfrom typing import List\ndef min_max(x: List[int]) -> List[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n\n    Args:\n    x (List[int]): A list of integers to be normalized.\n\n    Returns:\n    List[float]: A list of normalized floating-point numbers, each scaled between 0 and 1.\n    \"\"\"\n    min_val = min(x)\n    max_val = max(x)\n    diff = max_val - min_val\n    normalized_values = [(val - min_val) / diff for val in x]\n    return [round(val, 4) for val in normalized_values]\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: local variable 'z' referenced before assignment", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Create a simple residual block using NumPy.\n\n    Parameters:\n    x (np.ndarray): Input 1D array of shape (n,)\n    w1 (np.ndarray): Weight matrix for the first layer, shape (n, n)\n    w2 (np.ndarray): Weight matrix for the second layer, shape (n, n)\n\n    Returns:\n    list: List containing the processed input after the residual block, rounded to 4 decimal places.\n    \"\"\"\n    z = np.dot(x, w1) + np.dot(z, w2)\n    z = np.maximum(0, z)\n    z += x\n    z = np.maximum(0, z)\n    return round(list(z), 4)\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[1.5, -0.5], [-0.5, 1.5]])\nw2 = np.array([[0.5, 0.5], [0.5, 0.5]])\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels), representing feature maps from a convolutional layer.\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,), where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch, channels, height, width).\n    gamma (np.ndarray): Scale parameter tensor of shape (channels,).\n    beta (np.ndarray): Shift parameter tensor of shape (channels,).\n    epsilon (float): Epsilon value for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized tensor of shape (batch, channels, height, width).\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    std = np.sqrt(var + epsilon)\n    normalized_X = (X - mean) / std\n    output = normalized_X * gamma + beta\n    return output.tolist()\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a polynomial term of the form c * x^n at a given point x.\n\n    Parameters:\n    - c (float): Coefficient of the term.\n    - x (float): Point at which to evaluate the derivative.\n    - n (float): Exponent of the term.\n\n    Returns:\n    - float: The value of the derivative, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors.\n\n    Args:\n        vectors: A list of 2D vectors represented as lists of floats.\n        tol: Tolerance value for determining linear independence.\n\n    Returns:\n        A list of orthonormal vectors (unit length and orthogonal to each other).\n    \"\"\"\n    basis = [vectors[0] / np.linalg.norm(vectors[0])]\n    for v in vectors[1:]:\n        projection = sum([np.dot(v, b) * b for b in basis])\n        orthogonal_vector = v - projection\n        normalized_vector = orthogonal_vector / np.linalg.norm(orthogonal_vector)\n        basis.append(normalized_vector)\n    orthonormal_vectors = [np.round(vec, 4).tolist() for vec in basis]\n    return orthonormal_vectors\nvectors = [[1, 0], [0, 1], [1, 1]]\nbasis = orthonormal_basis(vectors)\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n\n    Parameters:\n    a (list): A list of three numbers representing the first vector.\n    b (list): A list of three numbers representing the second vector.\n\n    Returns:\n    list: A list containing the components of the cross product vector, rounded to 4 decimal places.\n    \"\"\"\n    vec_a = np.array(a)\n    vec_b = np.array(b)\n    result_vector = np.cross(vec_a, vec_b)\n    rounded_result = result_vector.round(4).tolist()\n    return rounded_result\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": false, "result": "failed: 1-dimensional array given. Array must be at least two-dimensional", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    if len(A) != len(b):\n        raise ValueError('The number of rows in A must match the length of b.')\n    n = len(A)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    C = np.column_stack((A, b))\n    x = [np.linalg.det(C[:, i]) / det_A for i in range(n)]\n    return np.round(x, 4).tolist()\nA = np.array([[2, 3], [4, 5]])\nb = np.array([6, 7])\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    Bhattacharyya_dist = -np.sum(np.log2(p * q))\n    return round(Bhattacharyya_dist, 4)\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List, Union\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> List[Union[int, float]]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": false, "result": "failed: local variable 'log_prob' referenced before assignment", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the policy gradient using the REINFORCE algorithm.\n\n    Args:\n        theta: A 2D NumPy array of shape (num_states, num_actions) representing the policy parameters.\n        episodes: A list of episodes, where each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n        A list containing the average gradient of the log-policy multiplied by the return at each time step.\n    \"\"\"\n    num_states = theta.shape[0]\n    num_actions = theta.shape[1]\n    gradients = np.zeros(num_states * num_actions)\n    for episode in episodes:\n        total_return = 0\n        prev_state = None\n        for (state, action, reward) in episode:\n            if prev_state is not None:\n                log_prob = np.log(np.sum(np.exp(theta[state, :])))\n                total_return += reward + gamma * np.max(np.exp(theta[next_state, :]))\n            gradients += log_prob * total_return\n            prev_state = state\n    gradients /= len(episodes)\n    return gradients.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": false, "result": "failed: name 'flops_dense' is not defined", "solution": "\nimport math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    fops_dense = n_experts * k_active * d_in * d_out\n    fops_moe = n_experts * k_active * d_in + n_experts * d_out * d_out\n    flops_savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    return round(flops_savings_percentage, 1)\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the final gating probabilities matrix for Noisy Top-K gating.\n\n    Args:\n    - X: Input data of shape (n_samples, n_features).\n    - W_g: Weight matrix for gating, of shape (n_experts, n_features).\n    - W_noise: Noise matrix for gating, of shape (n_experts, n_features).\n    - N: Pre-sampled noise matrix, of shape (n_samples, n_experts).\n    - k: Number of experts to consider.\n\n    Returns:\n    - A numpy array representing the final gating probabilities, of shape (n_samples, n_experts), rounded to 4th decimal.\n    \"\"\"\n    gating_scores = np.dot(X, W_g)\n    noisy_scores = gating_scores + W_noise * N\n    sorted_indices = np.argsort(noisy_scores, axis=1)[::-1]\n    top_k_scores = noisy_scores[np.arange(X.shape[0]), sorted_indices[:, :k]]\n    normalized_scores = top_k_scores / np.sum(top_k_scores, axis=1, keepdims=True)\n    final_gate_probs = normalized_scores.reshape(X.shape)\n    return final_gate_probs.tolist()\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": false, "result": "failed: shapes (2,4) and (2,3,2) not aligned: 4 (dim 1) != 3 (dim 1)", "solution": "import numpy as np\nimport numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Compute the final MoE output by selecting the top-k experts per token,\n    applying their transformations, and aggregating the results weighted by the normalized gating probabilities.\n\n    :param x: Input tensor shape (batch_size, sequence_length)\n    :param We: Expert weight matrices shape (n_experts, embedding_dim)\n    :param Wg: Gating weight matrix shape (1, embedding_dim)\n    :param n_experts: Number of experts\n    :param top_k: Top-k value for selecting experts\n    :return: Final MoE output shape (batch_size, sequence_length)\n    \"\"\"\n    g = np.dot(Wg, x)\n    g = np.exp(g) / np.sum(np.exp(g), axis=1, keepdims=True)\n    selected_experts = np.argsort(-g, axis=1)[:, :top_k]\n    transformed_experts = np.array([We[i] @ x[:, j] for i in range(n_experts) for j in selected_experts[:, i]])\n    aggregated_output = np.sum(transformed_experts * g[:, :, np.newaxis], axis=0)\n    return aggregated_output.tolist()\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": false, "result": "failed: cannot reshape array of size 524288 into shape (8192,8)", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (batch_size, channels, height, width) = X.shape\n    X_reshaped = X.reshape(batch_size * height * width, channels // num_groups)\n    mean = np.mean(X_reshaped, axis=0)\n    var = np.var(X_reshaped, axis=0)\n    var += epsilon\n    std = np.sqrt(var)\n    normalized_data = (X_reshaped - mean) / std\n    normalized_data_reshaped = normalized_data.reshape(batch_size, height, width, channels)\n    output = normalized_data_reshaped * gamma + beta\n    return output.tolist()\nX = np.random.rand(32, 64, 16, 16)\ngamma = np.random.rand(64)\nbeta = np.random.rand(64)\nnum_groups = 8\noutput = group_normalization(X, gamma, beta, num_groups)\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    current_x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * current_x ** 3 - 9 * current_x ** 2\n        current_x -= learning_rate * derivative\n        if abs(derivative) < tolerance:\n            break\n    return round(current_x, 4)\nstart_x = 1.0\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": false, "result": "failed: type numpy.ndarray doesn't define __round__ method", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh activation function to the input array x.\n\n    Args:\n        x (np.ndarray): The input array of shape (n_samples, n_features).\n        alpha (float): Scaling factor for the Tanh function.\n        gamma (float): Shift factor for the Tanh function.\n        beta (float): Offset factor for the Tanh function.\n\n    Returns:\n        list[float]: A list containing the transformed values of x.\n    \"\"\"\n    dyt_values = alpha * np.tanh(gamma * x + beta)\n    dyt_rounded = [round(value, 4) for value in dyt_values]\n    return dyt_rounded\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
