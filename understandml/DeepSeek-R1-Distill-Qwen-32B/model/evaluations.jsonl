{"task_id": 2, "completion_id": 0, "passed": false, "result": "failed: 'link_prime'", "solution": "import numpy as np\nimport numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {'identity': {'link': lambda mu: mu, 'inv_link': lambda eta: eta, 'derivative': lambda mu: np.ones_like(mu)}, 'log': {'link': np.log, 'inv_link': np.exp, 'derivative': lambda mu: 1 / mu}, 'logit': {'link': lambda mu: np.log(mu / (1 - mu)), 'inv_link': lambda eta: 1 / (1 + np.exp(-eta)), 'derivative': lambda mu: 1 / (mu * (1 - mu))}}\n    return links\nassert np.allclose(glm_links()[\"identity\"][\"link\"](np.array([3.0,-1.0])), np.array([3.0,-1.0])), \"test case failed: identity link\"\nassert np.allclose(glm_links()[\"identity\"][\"inv_link\"](np.array([-0.5,2.6])), np.array([-0.5,2.6])), \"test case failed: identity inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link\"](np.array([1.0,2.0])), np.log(np.array([1.0,2.0]))), \"test case failed: log link\"\nassert np.allclose(glm_links()[\"log\"][\"inv_link\"](np.array([0.0,1.0])), np.exp(np.array([0.0,1.0]))), \"test case failed: log inverse link\"\nassert np.allclose(glm_links()[\"log\"][\"link_prime\"](np.array([2.0,4.0])), np.array([0.5,0.25])), \"test case failed: log link derivative\"\nassert np.allclose(glm_links()[\"logit\"][\"link\"](np.array([0.2,0.8])), np.array([-1.38629436,1.38629436])), \"test case failed: logit link\"\nassert np.allclose(glm_links()[\"logit\"][\"inv_link\"](np.array([-1.38629436,1.38629436])), np.array([0.2,0.8])), \"test case failed: logit inverse link\"\nassert np.allclose(glm_links()[\"logit\"][\"link_prime\"](np.array([0.2,0.8])), 1.0/(np.array([0.2,0.8])*(1.0-np.array([0.2,0.8])))), \"test case failed: logit link derivative\"\nimport numpy as _np; _rng=_np.random.default_rng(1); _mu=_rng.uniform(0.05,0.95,50); _eta=glm_links()[\"logit\"][\"link\"](_mu); assert _np.allclose(glm_links()[\"logit\"][\"inv_link\"](_eta), _mu), \"test case failed: random roundtrip\"\nmu_test = np.array([0.3,0.6]).reshape(1,2); eta_test = glm_links()[\"logit\"][\"link\"](mu_test); assert eta_test.shape == (1,2) and glm_links()[\"logit\"][\"inv_link\"](eta_test).shape == (1,2), \"test case failed: shape preservation\""}
{"task_id": 3, "completion_id": 0, "passed": false, "result": "failed: test case failed: n=1", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n\n    class Node:\n\n        def __init__(self):\n            self.is_leaf = False\n            self.left = None\n            self.right = None\n            self.feature = None\n            self.value = None\n            self.indices = None\n\n    def build_node(indices):\n        if len(indices) < 3:\n            node = Node()\n            node.is_leaf = True\n            node.indices = indices\n            return node\n        else:\n            f = np.random.choice(data.shape[1])\n            min_val = data[indices, f].min()\n            max_val = data[indices, f].max()\n            v = np.random.uniform(min_val, max_val)\n            left_indices = []\n            right_indices = []\n            for idx in indices:\n                if data[idx, f] <= v:\n                    left_indices.append(idx)\n                else:\n                    right_indices.append(idx)\n            node = Node()\n            node.feature = f\n            node.value = v\nassert isolation_tree_path_length(np.arange(1).reshape(-1, 1)) == [0], \"test case failed: n=1\"\nassert isolation_tree_path_length(np.arange(2).reshape(-1, 1)) == [0, 0], \"test case failed: n=2\"\nassert isolation_tree_path_length(np.arange(3).reshape(-1, 1)) == [1, 1, 1], \"test case failed: n=3\"\nassert isolation_tree_path_length(np.arange(4).reshape(-1, 1)) == [1, 1, 1, 1], \"test case failed: n=4\"\nassert isolation_tree_path_length(np.arange(5).reshape(-1, 1)) == [2, 2, 2, 1, 1], \"test case failed: n=5\"\nassert isolation_tree_path_length(np.arange(6).reshape(-1, 1)) == [2, 2, 2, 2, 2, 2], \"test case failed: n=6\"\nassert isolation_tree_path_length(np.arange(7).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2], \"test case failed: n=7\"\nassert isolation_tree_path_length(np.arange(8).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2, 2], \"test case failed: n=8\"\nassert isolation_tree_path_length(np.arange(9).reshape(-1, 1)) == [3, 3, 3, 2, 2, 2, 2, 2, 2], \"test case failed: n=9\"\nassert isolation_tree_path_length(np.arange(10).reshape(-1, 1)) == [3, 3, 3, 2, 2, 3, 3, 3, 2, 2], \"test case failed: n=10\""}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    row_sums = np.sum(X ** 2, axis=1)\n    outer = row_sums[:, np.newaxis] + row_sums[np.newaxis, :]\n    dot_product = X.dot(X.T)\n    D = outer - 2 * dot_product\n    D_rounded = np.round(D, decimals=4)\n    return D_rounded.tolist()\nassert l2_distance(np.array([[0, 0], [1, 0]], dtype=float)) == [[0.0, 1.0], [1.0, 0.0]], \"failed on two points, axis-aligned\"\nassert l2_distance(np.array([[1, 2]], dtype=float)) == [[0.0]], \"failed on single sample\"\nassert l2_distance(np.array([[1, 2], [3, 4], [5, 6]], dtype=float)) == [[0.0, 8.0, 32.0], [8.0, 0.0, 8.0], [32.0, 8.0, 0.0]], \"failed on three points 2-D\"\nassert l2_distance(np.array([[-1, -1], [1, 1]], dtype=float)) == [[0.0, 8.0], [8.0, 0.0]], \"failed on negative coordinates\"\nassert l2_distance(np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]], dtype=float)) == [[0.0, 3.0, 12.0], [3.0, 0.0, 3.0], [12.0, 3.0, 0.0]], \"failed on 3-D points\"\nassert l2_distance(np.array([[0.5, 0.5], [1.5, 1.5]], dtype=float)) == [[0.0, 2.0], [2.0, 0.0]], \"failed on float coordinates\"\nassert l2_distance(np.zeros((0, 5), dtype=float)) == [], \"failed on empty input\"\nrnd = np.random.RandomState(0); X_rand = rnd.randn(4, 3); D = l2_distance(X_rand); assert all(abs(D[i][i]) < 1e-8 for i in range(4)), \"diagonal not zero on random data\"\nX_same = np.array([[2, 3], [2, 3], [2, 3]], dtype=float); assert l2_distance(X_same) == [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], \"failed on identical points\""}
{"task_id": 8, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef logistic_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, n_iterations: int=4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\"\"\"\n    m = X_train.shape[0]\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(n_iterations):\n        z = np.dot(X_train, w) + b\n        sigmoid_z = 1 / (1 + np.exp(-z))\n        error = sigmoid_z - y_train\n        grad_w = X_train.T.dot(error) / m\n        grad_b = np.sum(error) / m\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    z_test = np.dot(X_test, w) + b\n    sigmoid_test = 1 / (1 + np.exp(-z_test))\n    predictions = [int(round(p)) for p in sigmoid_test]\n    return predictions\nassert logistic_regression_predict(np.array([[0],[10]]), np.array([0,1]), np.array([[0],[10]])) == [0,1], \"failed: simple two-point case\"\nassert logistic_regression_predict(np.array([[-10],[10]]), np.array([0,1]), np.array([[-5],[5]])) == [0,1], \"failed: symmetric case\"\nassert logistic_regression_predict(np.array([[-3],[-2],[-1],[1],[2],[3]]), np.array([0,0,0,1,1,1]), np.array([[-4],[4]])) == [0,1], \"failed: clustered 1-D case\"\ntrain_X4 = np.array([[1,1],[2,2],[3,3],[-1,-1],[-2,-2],[-3,-3]]); train_y4 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X4, train_y4, np.array([[0.5,0.5],[-2.5,-2.5]])) == [1,0], \"failed: 2-D diagonal case\"\ntrain_X5 = np.array([[1,4],[2,5],[3,6],[-1,-4],[-2,-5],[-3,-6]]); train_y5 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X5, train_y5, np.array([[4,7],[-4,-7]])) == [1,0], \"failed: 2-D magnitudes case\"\ntrain_X6 = np.array([[0,0],[0,1],[0,2],[4,0],[4,1],[4,2]]); train_y6 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X6, train_y6, np.array([[0,1.5],[4,1.5]])) == [0,1], \"failed: vertical split\"\ntrain_X7 = np.array([[-50],[-40],[-30],[30],[40],[50]]); train_y7 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X7, train_y7, np.array([[-25],[25]])) == [0,1], \"failed: wide 1-D separation\"\ntrain_X8 = np.array([[-5],[-4],[-3],[-2],[-1],[1],[2],[3],[4],[5]]); train_y8 = np.array([0,0,0,0,0,1,1,1,1,1]); assert logistic_regression_predict(train_X8, train_y8, np.array([[-6],[6]])) == [0,1], \"failed: dense clusters\"\ntrain_X9 = np.array([[-1,-3],[-2,-5],[-3,-4],[1,3],[2,5],[3,4]]); train_y9 = np.array([0,0,0,1,1,1]); assert logistic_regression_predict(train_X9, train_y9, np.array([[-2,-4],[2,4]])) == [0,1], \"failed: mixed 2-D clusters\"\ntrain_X10 = np.array([[2,1],[3,1],[4,1],[-2,-1],[-3,-1],[-4,-1]]); train_y10 = np.array([1,1,1,0,0,0]); assert logistic_regression_predict(train_X10, train_y10, np.array([[3,1],[-3,-1]])) == [1,0], \"failed: rotated 2-D split\""}
{"task_id": 12, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef power_spectrum(frames: np.ndarray, scale: bool=False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True the resulting power spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        The power spectrum of each frame (only positive frequencies), rounded\n        to four decimal places and returned as a regular Python list.\n    \"\"\"\n    fft_frames = np.fft.rfft(frames, axis=1)\n    power = np.abs(fft_frames) ** 2\n    if scale:\n        n = frames.shape[1]\n        scale_factor = n // 2 + 1\n        power = power / scale_factor\n    power = np.round(power, 4)\n    return power.tolist()\nassert power_spectrum(np.array([[1, 2, 1, 0], [0, 1, 0, 1]]), False) == [[16.0, 4.0, 0.0], [4.0, 0.0, 4.0]], \"test case failed: power_spectrum([[1,2,1,0],[0,1,0,1]], False)\"\nassert power_spectrum(np.array([[1, 2, 1, 0], [0, 1, 0, 1]]), True) == [[5.3333, 1.3333, 0.0], [1.3333, 0.0, 1.3333]], \"test case failed: power_spectrum([[1,2,1,0],[0,1,0,1]], True)\"\nassert power_spectrum(np.array([[1, 1, 1, 1]]), False) == [[16.0, 0.0, 0.0]], \"test case failed: power_spectrum([[1,1,1,1]], False)\"\nassert power_spectrum(np.array([[1, 1, 1, 1]]), True) == [[5.3333, 0.0, 0.0]], \"test case failed: power_spectrum([[1,1,1,1]], True)\"\nassert power_spectrum(np.zeros((1, 4)), False) == [[0.0, 0.0, 0.0]], \"test case failed: power_spectrum(zeros, False)\"\nassert power_spectrum(np.array([[1, -1]]), False) == [[0.0, 4.0]], \"test case failed: power_spectrum([[1,-1]], False)\"\nassert power_spectrum(np.array([[1, -1]]), True) == [[0.0, 2.0]], \"test case failed: power_spectrum([[1,-1]], True)\"\nassert power_spectrum(np.array([[2, 0, 2, 0]]), False) == [[16.0, 0.0, 16.0]], \"test case failed: power_spectrum([[2,0,2,0]], False)\"\nassert power_spectrum(np.array([[0, 1, 1, 0]]), False) == [[4.0, 2.0, 0.0]], \"test case failed: power_spectrum([[0,1,1,0]], False)\""}
{"task_id": 14, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimals.\n    \"\"\"\n    p = 1 / (1 + np.exp(-y_pred))\n    p = np.clip(p, 1e-15, 1 - 1e-15)\n    loss = -(y * np.log(p) + (1 - y) * np.log(1 - p))\n    mean_loss = np.mean(loss)\n    gradient = p - y\n    hessian = p * (1 - p)\n    mean_loss = np.round(mean_loss, 4)\n    gradient = [np.round(g, 4) for g in gradient]\n    hessian = [np.round(h, 4) for h in hessian]\n    return (mean_loss, gradient, hessian)\nassert logistic_loss_metrics(np.array([1, 0, 1]), np.array([0.0, 0.0, 0.0])) == (0.6931, [-0.5, 0.5, -0.5], [0.25, 0.25, 0.25]), \"failed on zeros logits\"\nassert logistic_loss_metrics(np.array([0, 1]), np.array([2.0, -2.0])) == (2.1269, [0.8808, -0.8808], [0.105, 0.105]), \"failed on mixed logits 2 & -2\"\nassert logistic_loss_metrics(np.array([1]), np.array([1.0])) == (0.3133, [-0.2689], [0.1966]), \"failed on single positive logit\"\nassert logistic_loss_metrics(np.array([0]), np.array([-1.0])) == (0.3133, [0.2689], [0.1966]), \"failed on single negative logit\"\nassert logistic_loss_metrics(np.array([1, 0]), np.array([3.0, 3.0])) == (1.5486, [-0.0474, 0.9526], [0.0452, 0.0452]), \"failed on identical large positive logits\"\nassert logistic_loss_metrics(np.zeros(4), np.zeros(4)) == (0.6931, [0.5, 0.5, 0.5, 0.5], [0.25, 0.25, 0.25, 0.25]), \"failed on all-zero labels\"\nassert logistic_loss_metrics(np.ones(4), np.zeros(4)) == (0.6931, [-0.5, -0.5, -0.5, -0.5], [0.25, 0.25, 0.25, 0.25]), \"failed on all-one labels\"\nassert logistic_loss_metrics(np.array([0, 1, 0, 1]), np.array([-2.0, -2.0, 2.0, 2.0])) == (1.1269, [0.1192, -0.8808, 0.8808, -0.1192], [0.105, 0.105, 0.105, 0.105]), \"failed on alternating logits\"\nassert logistic_loss_metrics(np.array([0]), np.array([5.0])) == (5.0067, [0.9933], [0.0066]), \"failed on large positive logit with label 0\"\nassert logistic_loss_metrics(np.array([1]), np.array([-5.0])) == (5.0067, [-0.9933], [0.0066]), \"failed on large negative logit with label 1\""}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import Any, Iterable, List, Tuple\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i + N]) for i in range(L - N + 1)]\nassert ngrams([1, 2, 3, 4, 5], 2) == [(1, 2), (2, 3), (3, 4), (4, 5)], \"Test 1 failed: ngrams([1, 2, 3, 4, 5], 2)\"\nassert ngrams([1], 1) == [(1,)], \"Test 2 failed: ngrams([1], 1)\"\nassert ngrams([1], 2) == [], \"Test 3 failed: ngrams([1], 2)\"\nassert ngrams(\"hello\", 2) == [('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')], \"Test 4 failed: ngrams('hello', 2)\"\nassert ngrams((1, 2, 3, 4), 3) == [(1, 2, 3), (2, 3, 4)], \"Test 5 failed: ngrams((1,2,3,4), 3)\"\nassert ngrams(range(6), 4) == [(0, 1, 2, 3), (1, 2, 3, 4), (2, 3, 4, 5)], \"Test 6 failed: ngrams(range(6), 4)\"\nassert ngrams(['I', 'love', 'AI'], 2) == [('I', 'love'), ('love', 'AI')], \"Test 7 failed: ngrams(['I','love','AI'],2)\"\nassert ngrams([True, False, True, False], 3) == [(True, False, True), (False, True, False)], \"Test 8 failed: ngrams([True,False,True,False],3)\"\nassert ngrams([1, 1, 1, 1], 2) == [(1, 1), (1, 1), (1, 1)], \"Test 9 failed: ngrams([1,1,1,1],2)\"\nassert ngrams([7, 8, 9], 3) == [(7, 8, 9)], \"Test 10 failed: ngrams([7,8,9],3)\""}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\"\"\"\n    if not items:\n        return {}\n    root_index = None\n    for (i, p) in enumerate(parents):\n        if p == -1:\n            root_index = i\n            break\n    children_map = {}\n    for (i, p) in enumerate(parents):\n        if p != -1:\n            if p not in children_map:\n                children_map[p] = []\n            children_map[p].append(i)\n\n    def build_node(index):\n        node = {'item': items[index], 'count': counts[index], 'children': []}\n        for child in children_map.get(index, []):\n            node['children'].append(build_node(child))\n        return node\n    return build_node(root_index)\nassert build_tree([], [], []) == {}, \"test case failed: empty input lists\"\nassert build_tree(['A'], [5], [-1]) == {'item':'A','count':5,'children':[]}, \"test case failed: single node\"\nassert build_tree(['A','B','C','D'], [5,3,2,1], [-1,0,0,0]) == {'item':'A','count':5,'children':[{'item':'B','count':3,'children':[]},{'item':'C','count':2,'children':[]},{'item':'D','count':1,'children':[]}]}, \"test case failed: star tree\"\nassert build_tree(['A','B','C','D'], [7,6,5,4], [-1,0,1,2]) == {'item':'A','count':7,'children':[{'item':'B','count':6,'children':[{'item':'C','count':5,'children':[{'item':'D','count':4,'children':[]}]}]}]}, \"test case failed: linear chain\"\nassert build_tree(['R','A','B','C','D','E','F'], [9,4,4,2,2,1,1], [-1,0,0,1,1,2,2]) == {'item':'R','count':9,'children':[{'item':'A','count':4,'children':[{'item':'C','count':2,'children':[]},{'item':'D','count':2,'children':[]}]},{'item':'B','count':4,'children':[{'item':'E','count':1,'children':[]},{'item':'F','count':1,'children':[]}]}]}, \"test case failed: balanced tree\"\nassert build_tree(['root','n1','n2','n3','n4','n5','n6','n7','n8'], [9,8,7,6,5,4,3,2,1], [-1,0,0,1,1,2,2,3,6]) == {'item':'root','count':9,'children':[{'item':'n1','count':8,'children':[{'item':'n3','count':6,'children':[{'item':'n7','count':2,'children':[]}]},{'item':'n4','count':5,'children':[]}]},{'item':'n2','count':7,'children':[{'item':'n5','count':4,'children':[]},{'item':'n6','count':3,'children':[{'item':'n8','count':1,'children':[]}]}]}]}, \"test case failed: complex tree\"\nassert build_tree(['P','Q','R','S'], [1,1,1,1], [-1,0,1,2])['children'][0]['children'][0]['children'][0]['item'] == 'S', \"test case failed: verify deep child label\"\nassert len(build_tree(['A','B','C'], [1,1,1], [-1,0,0])['children']) == 2, \"test case failed: number of children\"\nassert build_tree(['A','B','C','D','E'], [6,4,3,2,1], [-1,0,1,1,2]) == {'item':'A','count':6,'children':[{'item':'B','count':4,'children':[{'item':'C','count':3,'children':[{'item':'E','count':1,'children':[]} ]},{'item':'D','count':2,'children':[]} ]} ]}, \"test case failed: provided example\""}
{"task_id": 18, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)\nassert zero(5).shape == (5,), \"test case failed: zero(5) shape incorrect\"\nassert np.array_equal(zero(5), np.zeros(5)), \"test case failed: zero(5) values incorrect\"\nassert zero((2, 3)).shape == (2, 3), \"test case failed: zero((2, 3)) shape incorrect\"\nassert np.array_equal(zero((2, 3)), np.zeros((2, 3))), \"test case failed: zero((2, 3)) values incorrect\"\nassert zero((2, 3), dtype=int).dtype == np.int_, \"test case failed: dtype=int not respected\"\nassert zero((1, 1, 1)).shape == (1, 1, 1), \"test case failed: triple-dim shape\"\nassert zero((4,)).ndim == 1, \"test case failed: ndim for 1-tuple shape\"\nassert zero(()).shape == (), \"test case failed: zero-dimensional array shape\"\nassert zero(0).size == 0, \"test case failed: zero length vector size\"\nassert zero((2, 0, 3)).size == 0, \"test case failed: array with a zero axis size\""}
{"task_id": 20, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        sigmoid = 1.0 / (1.0 + np.exp(-x))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        if isinstance(x, list):\n            x = np.array(x)\n        sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n        gradient_x = sigmoid_x * (1 - sigmoid_x)\n        sigmoid_rounded = np.round(sigmoid_x, 4)\n        gradient_rounded = np.round(gradient_x, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\nassert sigmoid_activation(0) == (0.5, 0.25), \"failed on scalar 0\"\nassert sigmoid_activation([-1, 0, 1]) == ([0.2689, 0.5, 0.7311], [0.1966, 0.25, 0.1966]), \"failed on list [-1,0,1]\"\nassert sigmoid_activation(np.array([-3, 3])) == ([0.0474, 0.9526], [0.0452, 0.0452]), \"failed on np.array([-3,3])\"\nassert sigmoid_activation([10]) == ([1.0], [0.0]), \"failed on list [10]\"\nassert sigmoid_activation(10) == (1.0, 0.0), \"failed on scalar 10\"\nassert sigmoid_activation(-1000) == (0.0, 0.0), \"failed on large negative scalar\"\nassert sigmoid_activation([0]) == ([0.5], [0.25]), \"failed on list [0]\""}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef random_tensor(shape: tuple[int, ...], standardize: bool=False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    offset = np.random.randint(low=-300, high=300, size=shape)\n    u = np.random.rand(*shape)\n    tensor = offset + u\n    if standardize:\n        mean = tensor.mean(axis=0)\n        std = tensor.std(axis=0)\n        epsilon = np.finfo(float).eps\n        tensor = (tensor - mean) / (std + epsilon)\n    return tensor\nassert random_tensor((5, 3), True).shape == (5, 3), \"test case failed: random_tensor((5, 3), True) shape check\"\nassert (random_tensor((7, 1), False) >= -300).all(), \"test case failed: value range check\"\nx = random_tensor((4, 2), True); import numpy as _np; assert _np.allclose(x.mean(axis=0), 0, atol=1e-10), \"test case failed: column means not zero\"\nx = random_tensor((4, 2), True); import numpy as _np; assert _np.allclose(x.std(axis=0), 1, atol=1e-10), \"test case failed: column stds not one\"\nx = random_tensor((3, 4), False); import numpy as _np; assert not _np.allclose(x.std(axis=0), 1, atol=1e-2), \"test case failed: standardization wrongly applied\"\nassert random_tensor((1, 5), False).shape == (1, 5), \"test case failed: random_tensor((1,5), False) shape check\"\nassert random_tensor((6, 6), True).shape == (6, 6), \"test case failed: random_tensor((6,6), True) shape check\"\nx = random_tensor((10, 2), True); import numpy as _np; assert _np.isfinite(x).all(), \"test case failed: finite values\"\nassert random_tensor((2, 3), False).dtype == float, \"test case failed: dtype check\"\nx = random_tensor((8, 4), True); import numpy as _np; assert _np.allclose(x.mean(axis=0), 0, atol=1e-10) and _np.allclose(x.std(axis=0), 1, atol=1e-10), \"test case failed: standardization (8,4)\""}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    w_np = np.array(w)\n    if len(velocity) == 0:\n        velocity_np = np.zeros_like(w_np)\n    else:\n        velocity_np = np.array(velocity)\n    tilde_w = w_np - momentum * velocity_np\n    g = grad_func(tilde_w)\n    g = np.array(g)\n    g_clipped = np.clip(g, -1, 1)\n    new_velocity = momentum * velocity_np + learning_rate * g_clipped\n    new_w = w_np - new_velocity\n    new_w_list = np.round(new_w, 4).tolist()\n    new_velocity_list = np.round(new_velocity, 4).tolist()\n    return (new_w_list, new_velocity_list)\nassert nesterov_update([1.5, -0.8], [0, 0], lambda x: 2 * x, 0.1, 0.9) == ([1.4, -0.7], [0.1, -0.1]), \"failed on basic 2-D update\"\nassert nesterov_update([1.4, -0.7], [0.1, -0.1], lambda x: 2 * x, 0.1, 0.9) == ([1.21, -0.51], [0.19, -0.19]), \"failed on consecutive 2-D step\"\nassert nesterov_update([2.0], [], lambda x: 4 * x, 0.05, 0.8) == ([1.95], [0.05]), \"failed on 1-D first step\"\nassert nesterov_update([-0.2, 0.2], [0, 0], lambda x: np.array([10, -10]), 0.2, 0.0) == ([-0.4, 0.4], [0.2, -0.2]), \"failed on gradient clipping\"\nassert nesterov_update([0.0, 0.0], [0, 0], lambda x: np.array([-0.5, 0.3]), 0.1, 0.5) == ([0.05, -0.03], [-0.05, 0.03]), \"failed on mixed-sign gradient\"\nassert nesterov_update([0.5, -0.5], [0.2, -0.1], lambda x: x, 0.1, 0.9) == ([0.288, -0.369], [0.212, -0.131]), \"failed on non-zero initial velocity\"\nassert nesterov_update([1.0, 1.0], [0, 0], lambda x: np.zeros_like(x), 0.1, 0.85) == ([1.0, 1.0], [0.0, 0.0]), \"failed on zero gradient\"\nassert nesterov_update([0.1], [0], lambda x: np.array([100]), 0.01, 0.0) == ([0.09], [0.01]), \"failed on extreme gradient clipping\"\nassert nesterov_update([2.0, -1.0, 0.5], [0, 0, 0], lambda x: 0.5 * x, 0.2, 0.4) == ([1.8, -0.9, 0.45], [0.2, -0.1, 0.05]), \"failed on 3-D first step\"\nassert nesterov_update([1.8, -0.9, 0.45], [0.2, -0.1, 0.05], lambda x: 0.5 * x, 0.2, 0.4) == ([1.548, -0.774, 0.387], [0.252, -0.126, 0.063]), \"failed on 3-D consecutive step\""}
{"task_id": 25, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    dot_products = X_train.dot(X_test.T)\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    K = np.exp(-gamma * dist_sq)\n    alpha_y = alpha * y_train\n    g = alpha_y.dot(K) + b\n    signs = np.where(g >= 0, 1, -1)\n    return signs.tolist()\nassert gaussian_svm_predict(np.array([[1,2],[2,3]]),np.array([1,-1]),np.array([0.6,0.4]),0.1,0.5,np.array([[1.5,2.5]]))==[1],\"Failed on single test sample.\"\nassert gaussian_svm_predict(np.array([[1,0],[0,1]]),np.array([1,-1]),np.array([0.9,0.9]),0.0,2.0,np.array([[0.9,0.1],[0.1,0.9]]))==[1,-1],\"Failed orthogonal samples.\"\nassert gaussian_svm_predict(np.array([[1,2],[3,4]]),np.array([1,1]),np.array([0.5,0.5]),-0.6,0.3,np.array([[2,3]]))==[-1],\"Bias impact failed.\"\nassert gaussian_svm_predict(np.array([[0,0],[0,1],[1,0],[1,1]]),np.array([1,-1,-1,1]),np.array([0.3,0.3,0.3,0.3]),0.0,1.0,np.array([[0.5,0.5],[1.5,1.5]]))==[1,1],\"Failed on XOR-like points.\"\nassert gaussian_svm_predict(np.array([[2]]),np.array([-1]),np.array([0.9]),0.0,1.0,np.array([[1],[3]]))==[-1,-1],\"Failed 1-D input.\"\nassert gaussian_svm_predict(np.array([[1,2,3],[4,5,6]]),np.array([1,-1]),np.array([0.4,0.6]),0.0,0.2,np.array([[1,2,3],[4,5,6]]))==[1,-1],\"Failed predictions identical to training points.\"\nassert gaussian_svm_predict(np.array([[1,2]]),np.array([1]),np.array([1.0]),-1.5,0.1,np.array([[10,20]]))==[-1],\"Far away point should follow bias.\""}
{"task_id": 26, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(0, x_array)\n    gradient = np.where(x_array >= 0, 1, 0)\n    return (activation.tolist(), gradient.tolist())\nassert relu([-2,-1,0,1,3]) == ([0,0,0,1,3],[0,0,1,1,1]), \"failed on 1-D mixed values\"\nassert relu([0,0,0]) == ([0,0,0],[1,1,1]), \"failed on all zeros\"\nassert relu([[1,-2],[3,-4]]) == ([[1,0],[3,0]],[[1,0],[1,0]]), \"failed on simple 2-D array\"\nassert relu([[-1.5,2.7,-3.2]]) == ([[0,2.7,0]],[[0,1,0]]), \"failed on float values\"\nassert relu([5]) == ([5],[1]), \"failed on single positive scalar\"\nassert relu([-5]) == ([0],[0]), \"failed on single negative scalar\"\nassert relu([[0.0,-0.0]]) == ([[0.0,0.0]],[[1,1]]), \"failed on signed zeros\"\nassert relu([[1000,-1000],[0,0.0001]]) == ([[1000,0],[0,0.0001]],[[1,0],[1,1]]), \"failed on large magnitude values\"\nassert relu([[-2,-1],[-0.5,0.5],[1,2]]) == ([[0,0],[0,0.5],[1,2]],[[0,0],[0,1],[1,1]]), \"failed on rectangular 2-D list\""}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not 0 <= sparsity <= 1:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array\nassert (random_binary_tensor((2,3),0.3,42)==np.array([[0.,1.,1.],[0.,0.,0.]])).all(), \"test failed: shape (2,3), sparsity 0.3, seed 42\"\nassert (random_binary_tensor((3,),0.8,1)==np.array([1.,1.,0.])).all(), \"test failed: shape (3,), sparsity 0.8, seed 1\"\nassert (random_binary_tensor((2,2),0)==np.zeros((2,2))).all(), \"test failed: all zeros when sparsity 0\"\nassert (random_binary_tensor((2,2),1)==np.ones((2,2))).all(), \"test failed: all ones when sparsity 1\"\nassert random_binary_tensor((1,),-0.1)==-1, \"test failed: invalid sparsity -0.1\"\nassert random_binary_tensor((1,),1.1)==-1, \"test failed: invalid sparsity 1.1\"\narr=random_binary_tensor((1000,),0.4,123); assert abs(arr.mean()-0.4)<0.05, \"test failed: empirical sparsity deviates >5% for seed 123\"\nassert random_binary_tensor((5,4),0.5).shape==(5,4), \"test failed: incorrect shape (5,4)\"\ntensor=random_binary_tensor((2,3,4),0.6,7); assert tensor.dtype.kind=='f' and set(np.unique(tensor)).issubset({0.0,1.0}), \"test failed: dtype or values incorrect for 3-D shape\"\nassert random_binary_tensor((10,),0.25,55).sum()==(random_binary_tensor((10,),0.25,55)).sum(), \"test failed: function must be deterministic with same seed\""}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "from typing import Any\nfrom typing import Any\nclass node:\n    \"\"\"A minimal tree node for decision-tree-like structures.\"\"\"\n\n    def __init__(self, fea: int=-1, res: Any | None=None, child: dict | None=None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\"\"\"\n    current = root\n    while current.fea != -1:\n        if current.fea >= len(sample):\n            return -1\n        val = sample[current.fea]\n        if val not in current.child:\n            return -1\n        current = current.child[val]\n    return current.res\nassert classify(node(res=\"Yes\"), []) == \"Yes\", \"test case failed: classify(node(res='Yes'), [])\"\nroot1 = node(fea=0, child={\"sunny\": node(res=\"No\"), \"rainy\": node(res=\"Yes\")})\nassert classify(root1, [\"sunny\"]) == \"No\", \"test case failed: classify(root1, ['sunny'])\"\nassert classify(root1, [\"rainy\"]) == \"Yes\", \"test case failed: classify(root1, ['rainy'])\"\nassert classify(root1, [\"cloudy\"]) == -1, \"test case failed: classify(root1, ['cloudy'])\"\nassert classify(root1, []) == -1, \"test case failed: classify(root1, [])\"\nsub = node(fea=1, child={\"Y\": node(res=\"Yes\"), \"Z\": node(res=\"No\")})\nroot2 = node(fea=0, child={\"A\": node(res=\"X\"), \"B\": sub})\nassert classify(root2, [\"B\", \"Y\"]) == \"Yes\", \"test case failed: classify(root2, ['B','Y'])\"\nassert classify(root2, [\"B\", \"Z\"]) == \"No\", \"test case failed: classify(root2, ['B','Z'])\"\nassert classify(root2, [\"B\", \"Unknown\"]) == -1, \"test case failed: classify(root2, ['B','Unknown'])\"\nassert classify(root2, [\"A\"]) == \"X\", \"test case failed: classify(root2, ['A'])\"\nlevel3 = node(fea=2, child={\"D\": node(res=\"OK\")})\nroot3 = node(fea=0, child={\"R\": node(fea=1, child={\"C\": level3})})\nassert classify(root3, [\"R\", \"C\", \"D\"]) == \"OK\", \"test case failed: classify(root3, ['R','C','D'])\""}
{"task_id": 36, "completion_id": 0, "passed": false, "result": "failed: name 'predict_tree' is not defined", "solution": "from typing import List, Optional\n\nassert predict_tree(None, [[1, 2], [3, 4]]) == [], \"failed on empty tree\"\nsingle_leaf = Node(None, None, (None, 42))\nassert predict_tree(single_leaf, [[0], [100]]) == [42, 42], \"failed on single-leaf tree\"\nleaf0 = Node(None, None, (None, 0))\nleaf1 = Node(None, None, (None, 1))\nroot = Node(leaf0, leaf1, (0, 2.5))\nassert predict_tree(root, [[1.4, 0], [4.0, 0]]) == [0, 1], \"failed on depth-1 tree\"\nleaf_left = Node(None, None, (None, 0))\nleaf_mid  = Node(None, None, (None, 1))\nleaf_right= Node(None, None, (None, 2))\nright_child = Node(leaf_mid, leaf_right, (1, 1.7))\nroot2 = Node(leaf_left, right_child, (0, 5))\nassert predict_tree(root2, [[2, 0], [7, 1.0], [7, 4]]) == [0, 1, 2], \"failed on depth-2 tree, assorted samples\"\nassert predict_tree(root2, [[1, 100], [3, -5]]) == [0, 0], \"failed when all samples end in the same leaf\"\nl1 = Node(None, None, (None, -1))\nl2 = Node(None, None, (None, -2))\nl3 = Node(None, None, (None, -3))\nint_node = Node(l2, l3, (2, 0.0))\nroot3 = Node(l1, int_node, (0, 10.0))\nassert predict_tree(root3, [[5, 0, -1], [20, 0, -2], [20, 0, 1]]) == [-1, -2, -3], \"failed on multi-feature tree\"\nleaf_neg = Node(None, None, (None, 9))\nleaf_pos = Node(None, None, (None, 8))\nroot4 = Node(leaf_neg, leaf_pos, (0, -3.0))\nassert predict_tree(root4, [[-10], [0]]) == [9, 8], \"failed on negative threshold splitting\"\nleaf_a = Node(None, None, (None, 0.1))\nleaf_b = Node(None, None, (None, 0.9))\nroot5 = Node(leaf_a, leaf_b, (0, 0.123))\nassert predict_tree(root5, [[0.0], [0.5]]) == [0.1, 0.9], \"failed on fractional values\"\nbig_batch = [[i] for i in range(100)]\nexpected  = [0 if i <= 50 else 1 for i in range(100)]\nbig_root  = Node(Node(None, None, (None, 0)), Node(None, None, (None, 1)), (0, 50))\nassert predict_tree(big_root, big_batch) == expected, \"failed on large batch\"\nleaf_c = Node(None, None, (None, 'left'))\nleaf_d = Node(None, None, (None, 'right'))\nroot6  = Node(leaf_c, leaf_d, (2, 3))\nassert predict_tree(root6, [[0, 0, 2], [0, 0, 5]]) == ['left', 'right'], \"failed when splitting on non-first feature\""}
{"task_id": 38, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    p_list = []\n    for a in range(n_arms):\n        A_a = np.array(A[a])\n        A_a_inv = np.linalg.inv(A_a)\n        b_a = np.array(b[a])\n        theta_hat_a = A_a_inv.dot(b_a)\n        c_a = context[:, a]\n        term1 = theta_hat_a.dot(c_a)\n        term2 = alpha * np.sqrt(c_a.dot(A_a_inv.dot(c_a)))\n        p_a = term1 + term2\n        p_list.append(p_a)\n    selected_arm = np.argmax(p_list)\n    return selected_arm\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [np.eye(2), 2*np.eye(2)], [np.zeros(2), np.zeros(2)], 1.0) == 0, \"failed on orthogonal identity contexts\"\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [np.eye(2), 5*np.eye(2)], [np.zeros(2), np.zeros(2)], 3.0) == 0, \"failed high-alpha exploration preference\"\nassert linucb_select_arm(np.array([[1,0,0],[0,1,0],[0,0,1]]), [np.eye(3), np.eye(3), 2*np.eye(3)], [np.zeros(3), np.array([1,0,0]), np.zeros(3)], 1.0) == 0, \"failed tie-breaking smallest index\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.5]], 0.2) == 1, \"failed single-feature small-alpha exploitation\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.5]], 5.0) == 1, \"failed single-feature large-alpha\"\nassert linucb_select_arm(np.array([[1,0],[1,0]]), [np.eye(2), np.eye(2)], [np.array([2,2]), np.array([1,1])], 0.1) == 0, \"failed higher theta\"\nassert linucb_select_arm(np.array([[1,0],[0,1]]), [10*np.eye(2), np.eye(2)], [np.zeros(2), np.zeros(2)], 5.0) == 1, \"failed uncertainty driven choice\"\nassert linucb_select_arm(np.array([[1,1]]), [[[1.0]], [[1.0]]], [[0.0], [0.0]], 0.5) == 0, \"failed identical arms pick smallest index\""}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1.0\n    n = len(actual)\n    if n == 0:\n        return -1.0\n    for a in actual:\n        if a < 0:\n            return -1.0\n    for p in predicted:\n        if p < 0:\n            return -1.0\n    log_actual = np.log(np.array(actual) + 1)\n    log_predicted = np.log(np.array(predicted) + 1)\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    mean_squared = np.mean(squared_diff)\n    rmsle_value = np.sqrt(mean_squared)\n    return round(rmsle_value, 4)\nassert rmsle([3, 5, 2.5, 7], [2.5, 5, 4, 8]) == 0.1993, \"failed: example case\"\nassert rmsle([1, 2, 3], [1, 2, 3]) == 0.0, \"failed: perfect prediction\"\nassert rmsle([100], [150]) == 0.4022, \"failed: single element\"\nassert rmsle([1, 2, 3], [1, 2]) == -1, \"failed: length mismatch\"\nassert rmsle([-1, 2], [0, 2]) == -1, \"failed: negative value in actual\"\nassert rmsle([1000, 2000], [1000, 2000]) == 0.0, \"failed: large identical numbers\"\nassert rmsle([0, 0], [0, 0]) == 0.0, \"failed: all zeros prediction\"\nassert rmsle([0], [1]) == 0.6931, \"failed: zero actual, positive prediction\"\nassert rmsle([1], [0]) == 0.6931, \"failed: positive actual, zero prediction\"\nassert rmsle([5, 10, 15], [6, 12, 18]) == 0.1645, \"failed: moderate deviation\""}
{"task_id": 44, "completion_id": 0, "passed": false, "result": "failed: name 'tanh_activation' is not defined", "solution": "import numpy as np\n\nassert tanh_activation([0, 1, -1], False) == [0.0, 0.7616, -0.7616], \"failed on basic forward pass\"\nassert tanh_activation([2], False) == [0.9640], \"failed on single positive value\"\nassert tanh_activation([2], True) == [0.0707], \"failed on gradient single\"\nassert tanh_activation([-2], False) == [-0.9640], \"failed on single negative value\"\nassert tanh_activation([-2], True) == [0.0707], \"failed on gradient negative\"\nassert tanh_activation([3, -3], False) == [0.9951, -0.9951], \"failed on larger magnitudes\"\nassert tanh_activation([3, -3], True) == [0.0099, 0.0099], \"failed on larger gradient\"\nassert tanh_activation([0.5, -0.5], False) == [0.4621, -0.4621], \"failed on half values\"\nassert tanh_activation([0.5, -0.5], True) == [0.7864, 0.7864], \"failed on half gradient\""}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        current_sum = 0\n        for n in range(N - k):\n            current_sum += x[n] * x[n + k]\n        result.append(current_sum)\n    return result\nassert autocorrelate_1d([1, 2, 3]) == [14, 8, 3], \"failed: autocorrelate_1d([1, 2, 3])\"\nassert autocorrelate_1d([1]) == [1], \"failed: autocorrelate_1d([1])\"\nassert autocorrelate_1d([1, 0, 0]) == [1, 0, 0], \"failed: autocorrelate_1d([1, 0, 0])\"\nassert autocorrelate_1d([1, -1, 1, -1]) == [4, -3, 2, -1], \"failed: autocorrelate_1d([1, -1, 1, -1])\"\nassert autocorrelate_1d([0.5, 0.5]) == [0.5, 0.25], \"failed: autocorrelate_1d([0.5, 0.5])\"\nassert autocorrelate_1d([1, 2, 3, 4]) == [30, 20, 11, 4], \"failed: autocorrelate_1d([1, 2, 3, 4])\"\nassert autocorrelate_1d([-1, 2, -3]) == [14, -8, 3], \"failed: autocorrelate_1d([-1, 2, -3])\"\nassert autocorrelate_1d([]) == [], \"failed: autocorrelate_1d([])\"\nassert autocorrelate_1d([2, 2]) == [8, 4], \"failed: autocorrelate_1d([2, 2])\"\nassert autocorrelate_1d([1, 3, 5, 7, 9]) == [165, 116, 71, 34, 9], \"failed: autocorrelate_1d([1, 3, 5, 7, 9])\""}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float=1e-06, max_iter: int=1000) -> list[list[float]]:\n    \"\"\"Implement Newton's method for solving a two-variable linear least-squares problem.\"\"\"\n    n = X.shape[0]\n    w = np.array([[1.0], [1.0]])\n    for _ in range(max_iter):\n        res = X.dot(w) - y\n        grad = 2.0 / n * X.T.dot(res)\n        H = 2.0 / n * X.T.dot(X)\n        H_inv = np.linalg.pinv(H)\n        delta_w = H_inv.dot(grad)\n        w = w - delta_w\n        if np.linalg.norm(grad) < epsilon:\n            break\n    rounded_w = np.round(w, 4)\n    return rounded_w.tolist()\nassert newton(np.array([[1, 2], [3, 4], [5, 6]]), np.array([[3], [7], [11]])) == [[1.0], [1.0]], \"test case failed: newton([[1,2],[3,4],[5,6]],[3,7,11])\"\nassert newton(np.array([[1, 0], [0, 1], [1, 1]]), np.array([[2], [3], [5]])) == [[2.0], [3.0]], \"test case failed: newton([[1,0],[0,1],[1,1]],[2,3,5])\"\nassert newton(np.array([[2, 4], [1, 3], [0, 1], [1, 2]]), np.array([[-3], [-2.5], [-1], [-1.5]])) == [[0.5], [-1.0]], \"test case failed: newton([[2,4],[1,3],[0,1],[1,2]],y)\"\nassert newton(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([[3], [5], [7], [9]])) == [[1.0], [1.0]], \"test case failed: newton(sequential data)\"\nassert newton(np.array([[1, 0], [0, 1], [2, 2]]), np.array([[2], [1], [6]])) == [[2.0], [1.0]], \"test case failed: newton([[1,0],[0,1],[2,2]],y)\"\nassert newton(np.array([[1, 3], [2, 1], [3, 2]]), np.array([[1], [7], [10]])) == [[4.0], [-1.0]], \"test case failed: newton([[1,3],[2,1],[3,2]],y)\"\nassert newton(np.array([[1, 2], [2, 1], [3, 3], [4, 1]]), np.array([[-1], [1], [0], [3]])) == [[1.0], [-1.0]], \"test case failed: newton(mixed signs)\"\nassert newton(np.array([[1, 4], [2, 2], [3, 1]]), np.array([[-0.5], [2], [4]])) == [[1.5], [-0.5]], \"test case failed: newton([[1,4],[2,2],[3,1]],y)\"\nassert newton(np.array([[1, 0], [0, 2], [2, 1], [3, 4], [4, 2]]), np.array([[0.75], [2.5], [2.75], [7.25], [5.5]])) == [[0.75], [1.25]], \"test case failed: newton(varied data)\"\n\n"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    N = len(pi)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n    beta = [1.0 for _ in range(N)]\n    for t in range(T - 2, -1, -1):\n        new_beta = [0.0] * N\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                sum_val += A[i][j] * B[j][obs[t + 1]] * beta[j]\n            new_beta[i] = sum_val\n        beta = new_beta\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    return round(prob, 6)\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [0,1,2]) == 0.033612, \"test case failed: example sequence\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [2,1]) == 0.1246, \"test case failed: sequence [2,1]\"\nassert backward_prob([[1.0]], [[0.2,0.8]], [1.0], [0,1,1,0]) == 0.0256, \"test case failed: single-state model 1\"\nassert backward_prob([[1.0]], [[0.5,0.5]], [1.0], [1,1,1]) == 0.125, \"test case failed: single-state model 2\"\nassert backward_prob([[0.7,0.3],[0.4,0.6]], [[0.1,0.4,0.5],[0.6,0.3,0.1]], [0.6,0.4], [1]) == 0.36, \"test case failed: length-1 sequence\"\nassert backward_prob([[0.5,0.5,0.0],[0.2,0.3,0.5],[0.0,0.0,1.0]], [[0.1,0.9],[0.7,0.3],[0.4,0.6]], [1.0,0.0,0.0], [0,1]) == 0.06, \"test case failed: 3-state model\"\nassert backward_prob([[1,0],[0,1]], [[0.6,0.4],[0.2,0.8]], [0.5,0.5], [0,1]) == 0.2, \"test case failed: deterministic transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [0,0]) == 0.25, \"test case failed: symmetric transitions\"\nassert backward_prob([[0.5,0.5],[0.5,0.5]], [[1,0],[0,1]], [0.5,0.5], [1]) == 0.5, \"test case failed: length-1 identity emissions\"\nassert backward_prob([[1.0]], [[0.3,0.7]], [1.0], [1,1,0]) == 0.147, \"test case failed: single-state model 3\""}
{"task_id": 68, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float=0.01, momentum: float=0.0, prev_update: np.ndarray | None=None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    w_new = w - learning_rate * new_update\n    rounded_w = np.round(w_new, 4).tolist()\n    rounded_update = np.round(new_update, 4).tolist()\n    return (rounded_w, rounded_update)\nassert sgd_momentum_update(np.array([1.0, 2.0]), np.array([0.1, -0.2]), 0.1, 0.9, None) == ([0.999, 2.002], [0.01, -0.02]), \"failed on basic initial step\"\nassert sgd_momentum_update(np.array([0.5]), np.array([0.2]), 0.05, 0.8, np.array([0.03])) == ([0.4968], [0.064]), \"failed when previous update is given\"\nassert sgd_momentum_update(np.array([2.0, -3.0]), np.array([-0.5, 0.5]), 0.01, 0.0, None) == ([2.005, -3.005], [-0.5, 0.5]), \"failed with zero momentum\"\nassert sgd_momentum_update(np.array([1.0]), np.array([0.1]), 0.1, 1.0, np.array([0.2])) == ([0.98], [0.2]), \"failed with momentum equal to 1\"\nassert sgd_momentum_update(np.array([10.0]), np.array([1.0]), 0.001, 0.5, None) == ([9.9995], [0.5]), \"failed scalar update with 0.5 momentum\"\nassert sgd_momentum_update(np.array([1.0, 2.0, 3.0]), np.array([0.0, 0.0, 1.0]), 0.1, 0.5, np.zeros(3)) == ([1.0, 2.0, 2.95], [0.0, 0.0, 0.5]), \"failed on higher-dim vector\"\nassert sgd_momentum_update(np.zeros(2), np.ones(2), 0.5, 0.8, None) == ([-0.1, -0.1], [0.2, 0.2]), \"failed on zero parameters\"\nassert sgd_momentum_update(np.array([5.0]), np.array([-2.0]), 0.01, 0.7, np.array([-0.5])) == ([5.0095], [-0.95]), \"failed on negative gradient and prev_update\"\nassert sgd_momentum_update(np.array([[0.0,1.0],[1.0,0.0]]), np.array([[0.5,0.0],[0.0,0.5]]), 0.2, 0.6, np.full((2,2),0.1)) == ([[-0.052,0.988],[0.988,-0.052]], [[0.26,0.06],[0.06,0.26]]), \"failed on mixed matrix with previous velocity\""}
{"task_id": 69, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    if not S or not A or (not B) or (not observations):\n        return -1\n    n = len(S)\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    T = len(observations)\n    f = [S[i] * B[i][observations[0]] for i in range(n)]\n    for t in range(1, T):\n        next_f = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += f[j] * A[j][i]\n            next_f[i] = sum_val * B[i][observations[t]]\n        f = next_f\n    result = sum(f)\n    return round(result, 4)\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [0,1,2]) == 0.0363, \"failed: basic 2-state example\"\nassert forward_algorithm([0.6,0.4], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [1]) == 0.36, \"failed: single observation\"\nassert forward_algorithm([1.0,0.0], [[0.5,0.5],[0.2,0.8]], [[0.6,0.4],[0.3,0.7]], [0,1,1]) == 0.2004, \"failed: asymmetric model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4,0.1],[0.1,0.3,0.6]], [2,0]) == 0.097, \"failed: different start distribution\"\nassert forward_algorithm([1.0], [[1.0]], [[0.2,0.3]], [1,1,0]) == 0.018, \"failed: single-state model\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.5]], [0]) == -1, \"failed: B wrong dimensions\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [-1,0]) == -1, \"failed: negative observation index\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3],[0.4,0.6]], [[0.5,0.4],[0.5,0.6]], [0,2]) == -1, \"failed: observation index out of range\"\nassert forward_algorithm([], [], [], [0]) == -1, \"failed: empty S\"\nassert forward_algorithm([0.5,0.5], [[0.7,0.3]], [[0.5,0.5],[0.5,0.5]], [0]) == -1, \"failed: A not square\""}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    for sample in data:\n        label = sample[-1]\n        label_counts[label] = label_counts.get(label, 0) + 1\n    return label_counts\nassert label_uniq_cnt([[1, 2, 'A'], [3, 5, 'B'], [7, 1, 'A']]) == {'A': 2, 'B': 1}, \"test case failed: multiple labels\"\nassert label_uniq_cnt([[0, 'cat'], [4, 'dog'], [9, 'dog'], [3, 'cat'], [1, 'cat']]) == {'cat': 3, 'dog': 2}, \"test case failed: string labels\"\nassert label_uniq_cnt([[0.2, 5.1, 1], [1.3, 3.3, 1], [9.0, 2.2, 2]]) == {1: 2, 2: 1}, \"test case failed: numeric labels\"\nassert label_uniq_cnt([[42, 'single']]) == {'single': 1}, \"test case failed: single sample\"\nassert label_uniq_cnt([[5, 7, 'same'], [1, 2, 'same'], [3, 4, 'same']]) == {'same': 3}, \"test case failed: all same label\"\nassert label_uniq_cnt([]) == {}, \"test case failed: empty data set\"\nassert label_uniq_cnt([[1,2,3,'x'],[2,3,4,'y'],[3,4,5,'x'],[4,5,6,'z']]) == {'x':2,'y':1,'z':1}, \"test case failed: longer samples\"\nassert label_uniq_cnt([[True, False, True, 'yes'], [False, True, False, 'no'], [True, True, True, 'yes']]) == {'yes':2,'no':1}, \"test case failed: boolean features\"\nassert label_uniq_cnt([[('tuple',1), 'T'], [('tuple',2), 'T'], [('tuple',3), 'S']]) == {'T':2,'S':1}, \"test case failed: tuple feature\"\nassert label_uniq_cnt([[1,'A'],[2,'B'],[3,'C'],[4,'D']]) == {'A':1,'B':1,'C':1,'D':1}, \"test case failed: all unique labels\""}
{"task_id": 72, "completion_id": 0, "passed": false, "result": "failed: name 'variance_reduction' is not defined", "solution": "import numpy as np\n\nassert variance_reduction(np.array([3,7,4,6]), np.array([3,4]), np.array([7,6])) == 2.25, \"failed on simple 1-D split\"\nassert variance_reduction(np.array([1,1,1,1]), np.array([1,1]), np.array([1,1])) == 0.0, \"failed when all variances are zero\"\nassert variance_reduction(np.array([[1,2],[3,4],[5,6]]), np.array([[1,2]]), np.array([[3,4],[5,6]])) == 4.0, \"failed on 2-D target\"\nassert variance_reduction(np.array([-3,-1,-4,-2]), np.array([-3,-4]), np.array([-1,-2])) == 1.0, \"failed on negative values\"\nassert variance_reduction(np.column_stack((np.arange(5), np.arange(5,10))), np.column_stack((np.arange(3), np.arange(5,8))), np.column_stack((np.arange(3,5), np.arange(8,10)))) == 3.0, \"failed on larger 2-D input\"\nassert variance_reduction(np.array([[2,2],[2,2],[2,2]]), np.array([[2,2]]), np.array([[2,2],[2,2]])) == 0.0, \"failed on identical multi-output\"\nassert variance_reduction(np.array([0,1]), np.array([0]), np.array([1])) == 0.25, \"failed on minimal split\""}
{"task_id": 74, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef magnitude_spectrum(frames: 'np.ndarray') -> 'list[list[float]]':\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    if frames.ndim == 1:\n        frames = frames.reshape(1, -1)\n    fft = np.fft.rfft(frames, axis=1)\n    magnitude = np.abs(fft)\n    rounded_magnitude = np.round(magnitude, 4)\n    return rounded_magnitude.tolist()\nassert magnitude_spectrum(np.array([[1.0, 0.0, -1.0, 0.0]])) == [[0.0, 2.0, 0.0]], \"test case failed: single frame [1,0,-1,0]\"\nassert magnitude_spectrum(np.array([[1,1,1,1]])) == [[4.0, 0.0, 0.0]], \"test case failed: constant frame ones length 4\"\nassert magnitude_spectrum(np.array([[1,0,0,0,0,0,0,0]])) == [[1.0,1.0,1.0,1.0,1.0]], \"test case failed: impulse length 8\"\nassert magnitude_spectrum(np.array([[0,0,0,0]])) == [[0.0,0.0,0.0]], \"test case failed: all zeros\"\nassert magnitude_spectrum(np.array([1.0, 0.0, -1.0, 0.0])) == [[0.0, 2.0, 0.0]], \"test case failed: 1-D input\"\nassert magnitude_spectrum(np.array([[1,1,1,1],[1,0,-1,0]])) == [[4.0,0.0,0.0],[0.0,2.0,0.0]], \"test case failed: two frames batch\"\nassert magnitude_spectrum(np.zeros((2,6))) == [[0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0]], \"test case failed: zeros batch 6 samples\"\nassert magnitude_spectrum(np.array([[1,2,3,4]])) == [[10.0,2.8284,2.0]], \"test case failed: frame [1,2,3,4]\"\nassert magnitude_spectrum(np.array([[2,2,2,2],[0,0,0,0]])) == [[8.0,0.0,0.0],[0.0,0.0,0.0]], \"test case failed: constant twos and zeros\""}
{"task_id": 76, "completion_id": 0, "passed": false, "result": "failed: unsupported operand type(s) for |: 'type' and 'str'", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0]], [[0.8, 0.1, 0.1], [0.2, 0.5, 0.3]]) == 0.9163, \"failed: basic 3-class example\"\nassert cross_entropy_loss([[1, 0], [0, 1]], [[0.6, 0.4], [0.3, 0.7]]) == 0.8675, \"failed: 2-class example\"\nassert cross_entropy_loss([[1, 0, 0, 0]], [[0.25, 0.25, 0.25, 0.25]]) == 1.3863, \"failed: uniform prediction\"\nassert cross_entropy_loss([[1, 0], [0, 1], [0, 1]], [[0.9, 0.1], [0.4, 0.6], [0.2, 0.8]]) == 0.8393, \"failed: three samples\"\nassert cross_entropy_loss([[1, 0]], [[1.0, 0.0]]) == 0.0, \"failed: perfect prediction\"\nassert cross_entropy_loss([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0.7, 0.2, 0.1], [0.3, 0.5, 0.2], [0.1, 0.3, 0.6]]) == 1.5606, \"failed: three-class batch\"\nassert cross_entropy_loss([[0, 1, 0]], [[0.1, 0.7, 0.2]]) == 0.3567, \"failed: single sample\"\nassert cross_entropy_loss([[0, 0, 1], [1, 0, 0]], [[0.05, 0.15, 0.8], [0.9, 0.05, 0.05]]) == 0.3285, \"failed: swapped classes\"\nassert cross_entropy_loss([[0, 1], [1, 0]], [[0.5, 0.5], [0.2, 0.8]]) == 2.3026, \"failed: high loss case\"\nassert cross_entropy_loss([[1], [1]], [[0.99], [0.98]]) == 0.0303, \"failed: single-class edge case\""}
{"task_id": 78, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef adamax_step(params: list[float], grads: list[float], m: list[float], u: list[float], t: int, learning_rate: float=0.002, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\"\"\"\n    beta1_power_t = beta1 ** t\n    scaling = learning_rate / (1 - beta1_power_t)\n    new_params = []\n    new_m = []\n    new_u = []\n    for i in range(len(params)):\n        new_m_i = beta1 * m[i] + (1 - beta1) * grads[i]\n        new_u_i = max(beta2 * u[i], abs(grads[i]))\n        step_i = scaling * new_m_i / (new_u_i + epsilon)\n        new_params_i = params[i] - step_i\n        new_params.append(new_params_i)\n        new_m.append(new_m_i)\n        new_u.append(new_u_i)\n    new_params = [round(x, 6) for x in new_params]\n    new_m = [round(x, 6) for x in new_m]\n    new_u = [round(x, 6) for x in new_u]\n    new_t = t + 1\n    return (new_params, new_m, new_u, new_t)\nassert adamax_step([1.0, 2.0], [0.1, -0.2], [0.0, 0.0], [0.0, 0.0], 1) == ([0.998, 2.002], [0.01, -0.02], [0.1, 0.2], 2), \"test case failed: basic two-dim update\"\nassert adamax_step([1.5], [0.5], [0.0], [0.0], 1) == ([1.498], [0.05], [0.5], 2), \"test case failed: positive grad single dim\"\nassert adamax_step([-1.0], [-1.0], [0.0], [0.0], 1) == ([-0.998], [-0.1], [1.0], 2), \"test case failed: negative grad single dim\"\nassert adamax_step([0.0, 0.0, 0.0], [1.0, 2.0, 3.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], 1) == ([-0.002, -0.002, -0.002], [0.1, 0.2, 0.3], [1.0, 2.0, 3.0], 2), \"test case failed: three-dim positive grads\"\nassert adamax_step([5.0, -3.0], [-2.0, 4.0], [0.0, 0.0], [0.0, 0.0], 1) == ([5.002, -3.002], [-0.2, 0.4], [2.0, 4.0], 2), \"test case failed: mixed sign grads\"\nassert adamax_step([1.498], [0.5], [0.05], [0.5], 2) == ([1.496], [0.095], [0.5], 3), \"test case failed: consecutive update positive grad\"\nassert adamax_step([-0.998], [-1.0], [-0.1], [1.0], 2) == ([-0.996], [-0.19], [1.0], 3), \"test case failed: consecutive update negative grad\"\nassert adamax_step([-0.002, -0.002, -0.002], [1.0, 2.0, 3.0], [0.1, 0.2, 0.3], [1.0, 2.0, 3.0], 2) == ([-0.004, -0.004, -0.004], [0.19, 0.38, 0.57], [1.0, 2.0, 3.0], 3), \"test case failed: consecutive update three-dim\"\nassert adamax_step([0.0], [0.3], [0.0], [0.0], 1) == ([-0.002], [0.03], [0.3], 2), \"test case failed: small positive grad\"\nassert adamax_step([0.0], [-0.3], [0.0], [0.0], 1) == ([0.002], [-0.03], [0.3], 2), \"test case failed: small negative grad\""}
{"task_id": 80, "completion_id": 0, "passed": false, "result": "failed: name 'adadelta_update' is not defined", "solution": "import numpy as np\nimport numpy as np\nassert adadelta_update([1, 2], [0, 0]) == [1.0, 2.0], \"test failed: zero gradient\"\nassert adadelta_update([1, 2], [1, 1]) == [0.9955, 1.9955], \"test failed: grad=[1,1]\"\nassert adadelta_update([1, 2], [1, -1]) == [0.9955, 2.0045], \"test failed: grad=[1,-1]\"\nassert adadelta_update([1, 2], [10, -10]) == [0.9955, 2.0045], \"test failed: large gradients\"\nassert adadelta_update([0, 0, 0], [1, -2, 0.5]) == [-0.0045, 0.0045, -0.0045], \"test failed: mixed gradients\"\nassert adadelta_update([-1, -2], [5, -5]) == [-1.0045, -1.9955], \"test failed: negative weights\"\nassert adadelta_update([3.1415], [0]) == [3.1415], \"test failed: single weight zero grad\"\nassert adadelta_update([100, 200], [50, 50]) == [99.9955, 199.9955], \"test failed: large weights\"\nassert adadelta_update([0.5, -0.5], [-0.1, 0.1]) == [0.5045, -0.5045], \"test failed: sign check\"\nassert adadelta_update([1, 1, 1, 1], [0, 1, 2, 3]) == [1.0, 0.9955, 0.9955, 0.9955], \"test failed: partial zero grad\""}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = -1 / m * np.sum(sum_terms)\n    return round(cost, 6)\nassert compute_cost(np.array([[0.8, 0.9, 0.4, 0.2]]), np.array([[1, 1, 0, 0]])) == 0.265618, \"Failed on example 1\"\nassert compute_cost(np.array([[0.999, 0.001]]), np.array([[1, 0]])) == 0.001001, \"Failed on example 2\"\nassert compute_cost(np.array([[0.5, 0.5, 0.5]]), np.array([[1, 0, 1]])) == 0.693147, \"Failed on example 3\"\nassert compute_cost(np.array([[0.2, 0.8, 0.6, 0.4]]), np.array([[0, 1, 1, 0]])) == 0.366985, \"Failed on example 4\"\nassert compute_cost(np.array([[0.999, 0.001, 0.999, 0.001]]), np.array([[1, 0, 1, 0]])) == 0.001001, \"Failed on example 5\"\nassert compute_cost(np.array([[0.1, 0.2, 0.3, 0.9, 0.8]]), np.array([[1, 1, 1, 0, 0]])) == 1.805604, \"Failed on example 6\"\nassert compute_cost(np.array([[0.7]]), np.array([[1]])) == 0.356675, \"Failed on example 7\"\nassert compute_cost(np.array([[0.05]]), np.array([[0]])) == 0.051293, \"Failed on example 8\"\nassert compute_cost(np.array([[0.5, 0.5]]), np.array([[0, 1]])) == 0.693147, \"Failed on example 9\"\nassert compute_cost(np.array([[0.0, 1.0]]), np.array([[0, 1]])) == 0.0, \"Failed on example 10\""}
{"task_id": 84, "completion_id": 0, "passed": false, "result": "failed: name 'mse_line_search' is not defined", "solution": "import numpy as np\n\nassert mse_line_search(np.array([1.0]), np.array([0.0]), np.array([1.0])) == 1.0, \"failed on single element perfect fit\"\nassert mse_line_search(np.array([3, -0.5, 2]), np.array([2.5, 0.0, 2]), np.array([-0.5, 0.5, 0])) == -1.0, \"failed on example with negative gamma\"\nassert mse_line_search(np.array([0, 0]), np.array([0, 0]), np.array([1, 1])) == 0.0, \"failed on zero numerator\"\nassert mse_line_search(np.array([1, 2, 3]), np.array([1, 2, 3]), np.array([1, 2, 3])) == 0.0, \"failed when residuals are zero\"\nassert mse_line_search(np.array([1, 2]), np.array([0, 0]), np.array([0, 0])) == 1.0, \"failed when h_pred is all zeros\"\nassert mse_line_search(np.array([2, 4, 6]), np.array([1, 3, 5]), np.array([1, 1, 1])) == 1.0, \"failed on uniform positive residuals\"\nassert mse_line_search(np.array([1, 2, 3, 4]), np.array([4, 3, 2, 1]), np.array([-3, -1, 1, 3])) == 1.0, \"failed on symmetric residuals\"\nassert mse_line_search(np.array([1, 3]), np.array([2, 2]), np.array([0.5, -0.5])) == -2.0, \"failed on fractional h_pred\"\nassert mse_line_search(np.array([1]), np.array([1]), np.array([0])) == 1.0, \"failed on single zero h_pred\"\nassert mse_line_search(np.array([10, 20, 30, 40, 50]), np.array([8, 18, 26, 39, 49]), np.array([2, 2, 4, 1, 1])) == 1.0, \"failed on larger array\""}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample_predictions = [tree[i] for tree in predictions]\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        max_labels = [label for (label, cnt) in counts.items() if cnt == max_count]\n        max_labels.sort()\n        result.append(max_labels[0])\n    return result\nassert aggregate_random_forest_votes([[1,0,1,1],[0,0,1,1],[1,1,1,0]]) == [1,0,1,1], \"test case failed: basic majority voting\"\nassert aggregate_random_forest_votes([[1,2],[2,1]]) == [1,1], \"test case failed: numeric tie breaking\"\nassert aggregate_random_forest_votes([[\"cat\",\"dog\",\"cat\"],[\"dog\",\"dog\",\"cat\"]]) == [\"cat\",\"dog\",\"cat\"], \"test case failed: string votes\"\nassert aggregate_random_forest_votes([[42,42,42]]) == [42,42,42], \"test case failed: single tree\"\nassert aggregate_random_forest_votes([[1],[0],[1],[0]]) == [0], \"test case failed: single sample tie\"\nassert aggregate_random_forest_votes([[3,3,2,2],[2,2,3,3],[3,2,3,2]]) == [3,2,3,2], \"test case failed: alternating ties\"\nassert aggregate_random_forest_votes([]) == [], \"test case failed: empty input\"\nassert aggregate_random_forest_votes([[1,1,1],[1,1,1]]) == [1,1,1], \"test case failed: identical votes\"\nassert aggregate_random_forest_votes([[5,4,3,2,1],[1,2,3,4,5],[5,4,3,2,1]]) == [5,4,3,2,1], \"test case failed: diverse votes\"\nassert aggregate_random_forest_votes([[\"a\",\"b\",\"a\",\"c\"],[\"b\",\"b\",\"a\",\"c\"],[\"a\",\"c\",\"a\",\"c\"]]) == [\"a\",\"b\",\"a\",\"c\"], \"test case failed: mixed string votes\""}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\nassert layer_sizes(np.zeros((3, 5)), np.zeros((1, 5))) == (3, 10, 1), \"failed on (3x5,1x5)\"\nassert layer_sizes(np.zeros((1, 10)), np.zeros((1, 10))) == (1, 10, 1), \"failed on (1x10,1x10)\"\nassert layer_sizes(np.zeros((8, 2)), np.zeros((4, 2))) == (8, 10, 4), \"failed on (8x2,4x2)\"\nassert layer_sizes(np.zeros((6, 9)), np.zeros((3, 9))) == (6, 10, 3), \"failed on (6x9,3x9)\"\nassert layer_sizes(np.zeros((10, 1)), np.zeros((5, 1))) == (10, 10, 5), \"failed on (10x1,5x1)\"\nassert layer_sizes(np.zeros((2, 7)), np.zeros((2, 7))) == (2, 10, 2), \"failed on (2x7,2x7)\"\nassert layer_sizes(np.zeros((4, 4)), np.zeros((6, 4))) == (4, 10, 6), \"failed on (4x4,6x4)\"\nassert layer_sizes(np.zeros((9, 3)), np.zeros((3, 3))) == (9, 10, 3), \"failed on (9x3,3x3)\"\nassert layer_sizes(np.zeros((12, 6)), np.zeros((2, 6))) == (12, 10, 2), \"failed on (12x6,2x6)\"\nassert layer_sizes(np.zeros((5, 20)), np.zeros((8, 20))) == (5, 10, 8), \"failed on (5x20,8x20)\""}
{"task_id": 88, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        s = np.logaddexp(0.0, z)\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\nassert softplus(-1000) == 0.0, \"failed: softplus(-1000)\"\nassert softplus(0) == 0.6931, \"failed: softplus(0)\"\nassert softplus(1) == 1.3133, \"failed: softplus(1)\"\nassert softplus(20) == 20.0, \"failed: softplus(20)\"\nassert softplus([-1, 0, 1]) == [0.3133, 0.6931, 1.3133], \"failed: softplus([-1,0,1])\"\nimport numpy as np\nassert softplus(np.array([-2, 5])) == [0.1269, 5.0067], \"failed: softplus(np.array([-2,5]))\"\nassert softplus(np.array([[0]])) == [[0.6931]], \"failed: softplus(np.array([[0]]))\"\nassert softplus(-0.5) == 0.4741, \"failed: softplus(-0.5)\"\nassert softplus([1000, -1000]) == [1000.0, 0.0], \"failed: softplus([1000,-1000])\""}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    if dA_np.shape != Z_np.shape:\n        return -1\n    dZ = dA_np * (Z_np > 0)\n    return dZ.tolist()\nassert relu_backward([[-1, 2], [3, -4]], [[1, -2], [0, 5]]) == [[-1, 0], [0, -4]], \"failed on mixed signs\"\nassert relu_backward([[1, 2], [3, 4]], [[5, 6], [7, 8]]) == [[1, 2], [3, 4]], \"failed on all positive Z\"\nassert relu_backward([[1, 2], [3, 4]], [[-5, -6], [-7, -8]]) == [[0, 0], [0, 0]], \"failed on all negative Z\"\nassert relu_backward([[0.5, -1.2, 3.0], [4.1, -2.2, 0.0], [7.7, 8.8, -9.9]], [[1, -1, 0], [2, 0, -2], [-3, 3, 4]]) == [[0.5, 0.0, 0.0], [4.1, 0.0, 0.0], [0.0, 8.8, -9.9]], \"failed on 3x3 mixed\"\nassert relu_backward([[1, 2, 3, 4]], [[-1, 0, 2, -3]]) == [[0, 0, 3, 0]], \"failed on 1x4 vector\"\nassert relu_backward([[1.5, -2.5], [3.5, -4.5]], [[0.0, 2.0], [0.0, -2.0]]) == [[0.0, -2.5], [0.0, 0.0]], \"failed on zeros in Z\"\nassert relu_backward([[1, 2], [3, 4]], [[1, 2], [3, 4]]) == [[1, 2], [3, 4]], \"failed when gradients passthrough\"\nassert relu_backward([[10, -10], [5, -5]], [[-1, 1], [-1, 1]]) == [[0, -10], [0, -5]], \"failed on alternating Z\"\nassert relu_backward([[3]], [[-7]]) == [[0]], \"failed on single negative element\"\nassert relu_backward([[3]], [[7]]) == [[3]], \"failed on single positive element\""}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numbers\nimport numbers\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and (not isinstance(a, bool))\nassert is_number(10) is True, \"test case failed: is_number(10)\"\nassert is_number(3.14) is True, \"test case failed: is_number(3.14)\"\nassert is_number(1+2j) is True, \"test case failed: is_number(1+2j)\"\nassert is_number(\"5\") is False, \"test case failed: is_number('5')\"\nassert is_number([1,2]) is False, \"test case failed: is_number([1,2])\"\nassert is_number(True) is False, \"test case failed: is_number(True)\"\nassert is_number(None) is False, \"test case failed: is_number(None)\"\nassert is_number(-7) is True, \"test case failed: is_number(-7)\"\nassert is_number(float('nan')) is True, \"test case failed: is_number(float('nan'))\"\nassert is_number({'num': 5}) is False, \"test case failed: is_number({'num': 5})\""}
{"task_id": 93, "completion_id": 0, "passed": false, "result": "failed: test case failed: relu(5)", "solution": "import numpy as np\nimport numpy as np\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_np = np.array(Z)\n    if Z_np.ndim == 0:\n        Z_np = Z_np.reshape(1)\n    A_np = np.maximum(0, Z_np)\n    A = A_np.tolist()\n    cache = Z_np.tolist()\n    return (A, cache)\nassert relu(np.array([[-1, 2], [3, -4]])) == ([[0, 2], [3, 0]], [[-1.0, 2.0], [3.0, -4.0]]), \"test case failed: relu([[-1,2],[3,-4]])\"\nassert relu(np.array([[0, 0], [0, 0]])) == ([[0, 0], [0, 0]], [[0.0, 0.0], [0.0, 0.0]]), \"test case failed: relu([[0,0],[0,0]])\"\nassert relu(np.array([[1, -1, 0]])) == ([[1, 0, 0]], [[1.0, -1.0, 0.0]]), \"test case failed: relu([[1,-1,0]])\"\nassert relu(np.array([-5, 4, -3, 2, 0])) == ([0, 4, 0, 2, 0], [-5.0, 4.0, -3.0, 2.0, 0.0]), \"test case failed: relu([-5,4,-3,2,0])\"\nassert relu(np.array(5)) == (5.0, 5.0), \"test case failed: relu(5)\"\nassert relu(np.array(-7)) == (0.0, -7.0), \"test case failed: relu(-7)\"\nassert relu(np.zeros((3, 3))) == ([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), \"test case failed: relu(zeros)\"\nassert relu([[1, 2], [3, 4]]) == ([[1, 2], [3, 4]], [[1.0, 2.0], [3.0, 4.0]]), \"test case failed: relu([[1,2],[3,4]])\"\nassert relu([[-0.1, -0.2, -0.3]]) == ([[0, 0, 0]], [[-0.1, -0.2, -0.3]]), \"test case failed: relu([[-0.1,-0.2,-0.3]])\"\nassert relu(np.arange(-4, 5)) == ([0, 0, 0, 0, 0, 1, 2, 3, 4], [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]), \"test case failed: relu(arange(-4,5))\""}
{"task_id": 94, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for (label, count) in counts.items() if count == max_count]\n    return min(max_labels)\nassert knn_majority_vote(['cat', 'dog', 'cat', 'bird']) == 'cat', \"failed on ['cat', 'dog', 'cat', 'bird']\"\nassert knn_majority_vote([1, 2, 2, 3, 1, 2]) == 2, \"failed on [1, 2, 2, 3, 1, 2]\"\nassert knn_majority_vote(['apple', 'banana', 'apple', 'banana']) == 'apple', \"failed on ['apple', 'banana', 'apple', 'banana']\"\nassert knn_majority_vote([42]) == 42, \"failed on [42]\"\nassert knn_majority_vote(['a', 'b', 'c']) == 'a', \"failed on ['a', 'b', 'c']\"\nassert knn_majority_vote([3, 3, 2, 2]) == 2, \"failed on [3, 3, 2, 2]\"\nassert knn_majority_vote([5, 4, 5, 4, 4, 5]) == 4, \"failed on [5, 4, 5, 4, 4, 5]\"\nassert knn_majority_vote(['x', 'y', 'y', 'x', 'z', 'z']) == 'x', \"failed on ['x', 'y', 'y', 'x', 'z', 'z']\"\nassert knn_majority_vote(list('aabbccd')) == 'a', \"failed on list('aabbccd')\"\nassert knn_majority_vote([0, 0, 1]) == 0, \"failed on [0, 0, 1]\""}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        return (Z > 0).astype(float).tolist()\n    elif isinstance(Z, list):\n        return [drelu(element) for element in Z]\n    else:\n        raise ValueError('Unsupported input type')\nassert drelu([-2, -1, 0, 1, 5]) == [0.0, 0.0, 0.0, 1.0, 1.0], \"failed: drelu([-2, -1, 0, 1, 5])\"\nassert drelu([[1, -1], [-0.5, 2]]) == [[1.0, 0.0], [0.0, 1.0]], \"failed: drelu([[1, -1], [-0.5, 2]])\"\nassert drelu(3) == 1.0, \"failed: drelu(3)\"\nassert drelu(0) == 0.0, \"failed: drelu(0)\"\nassert drelu([[[-1, 2], [3, 0]], [[0, -2], [4, 5]]]) == [[[0.0, 1.0], [1.0, 0.0]], [[0.0, 0.0], [1.0, 1.0]]], \"failed: multi-dimensional input\"\nassert drelu(np.array([-3, -0.1, 0, 0.1, 5])) == [0.0, 0.0, 0.0, 1.0, 1.0], \"failed: ndarray input\"\nassert drelu(np.zeros((2, 3))) == [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], \"failed: zeros array\"\nassert drelu([-9999, 0.0001]) == [0.0, 1.0], \"failed: extreme values\"\nassert drelu([0, 0, 0]) == [0.0, 0.0, 0.0], \"failed: list of zeros\"\nassert drelu([[5]]) == [[1.0]], \"failed: single element 2-D list\""}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The function keeps the container type of the input:\n    scalar \u2192 scalar, list \u2192 list, NumPy array \u2192 NumPy array.\n\n    Args:\n        Z (int | float | list | numpy.ndarray): Input data \u2013 scalar, 1-D or 2-D.\n\n    Returns:\n        Same type as *Z* with all negative values clipped to 0.\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n    elif isinstance(Z, list):\n        arr = np.array(Z)\n        relu_arr = np.maximum(0, arr)\n        return relu_arr.tolist()\n    elif isinstance(Z, np.ndarray):\n        return np.maximum(0, Z)\n    else:\n        raise TypeError('Unsupported input type')\nassert relu_(np.array([-1, 2, -3, 4])) .tolist() == [0, 2, 0, 4], \"failed: relu_(np.array([-1, 2, -3, 4]))\"\nassert relu_([-5, -4, -3]) == [0, 0, 0], \"failed: relu_([-5, -4, -3])\"\nassert relu_(np.array([[1, -1], [0, 3]])).tolist() == [[1, 0], [0, 3]], \"failed: relu_(np.array([[1, -1], [0, 3]]))\"\nassert relu_([[1, -2, 3], [-4, 5, -6]]) == [[1, 0, 3], [0, 5, 0]], \"failed: relu_([[1, -2, 3], [-4, 5, -6]])\"\nassert relu_(5) == 5, \"failed: relu_(5)\"\nassert relu_(-7) == 0, \"failed: relu_(-7)\"\nassert relu_([0.5, -0.2, 3.3]) == [0.5, 0, 3.3], \"failed: relu_([0.5, -0.2, 3.3])\"\nassert relu_(np.array([[0, -0.1], [-2.5, 4.4]])).tolist() == [[0, 0], [0, 4.4]], \"failed: relu_(np.array([[0, -0.1], [-2.5, 4.4]]))\"\nassert relu_(0) == 0, \"failed: relu_(0)\"\nassert relu_(np.array([-0.0001]))[0] == 0, \"failed: relu_(np.array([-0.0001]))\""}
{"task_id": 100, "completion_id": 0, "passed": false, "result": "failed: name 'classification_error' is not defined", "solution": "import numpy as np\n\nassert classification_error([1, 0, 1, 1], [1, 0, 1, 1]) == 0.0, \"Failed: identical arrays\"\nassert classification_error([0, 1, 2, 2, 1], [0, 0, 2, 1, 1]) == 0.4, \"Failed: basic example\"\nassert classification_error([0, 0, 0], [1, 1, 1]) == 1.0, \"Failed: all mismatched\"\nassert classification_error([1, 2, 3], [1, 2]) == -1, \"Failed: length mismatch\"\nassert classification_error(np.array([3, 3, 2, 1]), np.array([3, 2, 2, 1])) == 0.25, \"Failed: numpy arrays\"\nassert classification_error((5, 6, 7, 8), (5, 6, 0, 0)) == 0.5, \"Failed: tuples\"\nassert classification_error([\"cat\", \"dog\"], [\"dog\", \"dog\"]) == 0.5, \"Failed: string labels\"\nassert classification_error([True, False, True], [True, True, False]) == 0.6667, \"Failed: boolean labels\"\nassert classification_error(range(8), [0,1,2,3,4,5,6,7]) == 0.0, \"Failed: range vs list\"\nassert classification_error(np.array([1]), np.array([2])) == 1.0, \"Failed: single element mismatch\""}
{"task_id": 101, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softmax(x: np.ndarray, axis: int=1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax = exp_x / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]]), 1) == [[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]], \"test case failed: softmax([[1,2,3],[4,5,6]], axis=1)\"\nassert softmax(np.array([[1, 1], [2, 2]]), 0) == [[0.2689, 0.2689], [0.7311, 0.7311]], \"test case failed: softmax([[1,1],[2,2]], axis=0)\"\nassert softmax(np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]]), 2) == [[[0.2689, 0.7311], [0.2689, 0.7311]], [[0.2689, 0.7311], [0.2689, 0.7311]]], \"test case failed: softmax(3-D array, axis=2)\"\nassert softmax(np.array([[[1, 2]], [[3, 4]]]), 1) == [[[1.0, 1.0]], [[1.0, 1.0]]], \"test case failed: softmax(shape=(2,1,2), axis=1)\"\nassert softmax(np.array([[1000, 1000], [1000, 1000]]), 1) == [[0.5, 0.5], [0.5, 0.5]], \"test case failed: softmax(large values)\"\nassert softmax(np.array([[0, 1, 2], [2, 3, 4]]), 0) == [[0.1192, 0.1192, 0.1192], [0.8808, 0.8808, 0.8808]], \"test case failed: softmax([[0,1,2],[2,3,4]], axis=0)\"\nassert softmax(np.zeros((2, 2, 3)), -1) == [[[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]], [[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]]], \"test case failed: softmax(zeros 3-D, axis=-1)\"\nassert softmax(np.array([[[1, 2, 3], [4, 5, 6]]]), 2) == [[[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]]], \"test case failed: softmax(shape=(1,2,3), axis=2)\"\nassert softmax(np.array([[-1, -1, -1], [0, 0, 0]]), 1) == [[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]], \"test case failed: softmax(equal values)\""}
{"task_id": 105, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    x_arr = np.asarray(x)\n    x_arr = x_arr.flatten()\n    if x_arr.size == 0:\n        return []\n    x_arr = np.atleast_1d(x_arr)\n    exponent = -(x_arr - mean) ** 2 / (2 * sigma ** 2)\n    denominator = np.sqrt(2 * np.pi) * sigma\n    pdf = 1 / denominator * np.exp(exponent)\n    pdf_rounded = np.round(pdf, 5)\n    return pdf_rounded.tolist()\nassert gaussian_pdf([1, 2, 3], 2, 1) == [0.24197, 0.39894, 0.24197], \"test case failed: gaussian_pdf([1, 2, 3], 2, 1)\"\nassert gaussian_pdf([0], 0, 1) == [0.39894], \"test case failed: gaussian_pdf([0], 0, 1)\"\nassert gaussian_pdf([0], 0, 0.5) == [0.79788], \"test case failed: gaussian_pdf([0], 0, 0.5)\"\nassert gaussian_pdf([-1, 0, 1], 0, 1) == [0.24197, 0.39894, 0.24197], \"test case failed: gaussian_pdf([-1, 0, 1], 0, 1)\"\nassert gaussian_pdf([2], 1, 1) == [0.24197], \"test case failed: gaussian_pdf([2], 1, 1)\"\nassert gaussian_pdf([1], 0, -1) == -1, \"test case failed: gaussian_pdf([1], 0, -1)\"\nassert gaussian_pdf([1], 0, 0) == -1, \"test case failed: gaussian_pdf([1], 0, 0)\"\nassert gaussian_pdf([5], 2, 3) == [0.08066], \"test case failed: gaussian_pdf([5], 2, 3)\"\nassert gaussian_pdf([2.5, 3.5], 3, 0.5) == [0.48394, 0.48394], \"test case failed: gaussian_pdf([2.5, 3.5], 3, 0.5)\"\nassert gaussian_pdf([], 0, 1) == [], \"test case failed: gaussian_pdf([], 0, 1)\""}
{"task_id": 107, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_step(theta: np.ndarray, grad: np.ndarray, m_prev: np.ndarray, v_prev: np.ndarray, t: int, alpha: float=0.01, beta1: float=0.9, beta2: float=0.99, epsilon: float=1e-09) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\"\"\"\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * grad ** 2\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    m_t_hat = m_t / (1 - beta1_power)\n    v_t_hat = v_t / (1 - beta2_power)\n    theta_new = theta - alpha * (m_t_hat / (np.sqrt(v_t_hat) + epsilon))\n    theta_new_rounded = np.round(theta_new, 6).tolist()\n    m_t_rounded = np.round(m_t, 6).tolist()\n    v_t_rounded = np.round(v_t, 6).tolist()\n    return (theta_new_rounded, m_t_rounded, v_t_rounded, t + 1)\nassert adam_step(np.array([1.0, 2.0]), np.array([0.1, -0.2]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([0.99, 2.01], [0.01, -0.02], [0.0001, 0.0004], 2), \"test case failed: adam_step([1,2],[0.1,-0.2])\"\nassert adam_step(np.array([1.0]), np.array([0.0]), np.array([0.0]), np.array([0.0]), 1) == ([1.0], [0.0], [0.0], 2), \"test case failed: zero gradient\"\nassert adam_step(np.array([-1.0, 2.0, -3.0]), np.array([-0.3, -0.5, 0.7]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 1) == ([-0.99, 2.01, -3.01], [-0.03, -0.05, 0.07], [0.0009, 0.0025, 0.0049], 2), \"test case failed: mixed signs\"\nassert adam_step(np.array([0.0]), np.array([1.0]), np.array([0.0]), np.array([0.0]), 1) == ([-0.01], [0.1], [0.01], 2), \"test case failed: positive gradient scalar\"\nassert adam_step(np.array([0.0, 0.0]), np.array([1.0, 1.0]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([-0.01, -0.01], [0.1, 0.1], [0.01, 0.01], 2), \"test case failed: identical gradients\"\nassert adam_step(np.array([2.0]), np.array([-2.0]), np.array([0.0]), np.array([0.0]), 1) == ([2.01], [-0.2], [0.04], 2), \"test case failed: large negative gradient\"\nassert adam_step(np.array([1.0]), np.array([0.3]), np.array([0.0]), np.array([0.0]), 2) == ([0.992575], [0.03], [0.0009], 3), \"test case failed: second step\"\nassert adam_step(np.array([1.0]), np.array([-1.0]), np.array([0.0]), np.array([0.0]), 1) == ([1.01], [-0.1], [0.01], 2), \"test case failed: negative gradient scalar\"\nassert adam_step(np.array([1.0, 2.0, 3.0]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.0, 0.0]), 1) == ([1.0, 2.0, 3.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], 2), \"test case failed: all zero gradients\"\nassert adam_step(np.array([1.2, 3.4]), np.array([0.2, 0.2]), np.array([0.0, 0.0]), np.array([0.0, 0.0]), 1) == ([1.19, 3.39], [0.02, 0.02], [0.0004, 0.0004], 2), \"test case failed: small identical gradients\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: name 'kmeans' is not defined", "solution": "import numpy as np\nimport random\n\nassert kmeans(np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]]),2,random_state=42)==[[1.0,2.0],[10.0,2.0]],\"failed: basic 2-cluster example\"\nassert kmeans(np.array([[-5,0],[-6,-1],[-4,1],[0,5],[1,6],[-1,4],[5,0],[6,1],[4,-1]]),3,random_state=0)==[[-5.0,0.0],[0.0,5.0],[5.0,0.0]],\"failed: three clearly separated clusters\"\nassert kmeans(np.array([[0,0],[0,1],[10,0],[10,1]]),2,random_state=7)==[[0.0,0.5],[10.0,0.5]],\"failed: two vertical stripes\"\nassert kmeans(np.array([[1],[2],[8],[9]]),2,random_state=3)==[[1.5],[8.5]],\"failed: one-dimensional data\"\nassert kmeans(np.array([[1],[2],[3],[4],[5]]),1,random_state=11)==[[3.0]],\"failed: single cluster\"\nassert kmeans(np.array([[0,0,0],[0,1,0],[0,0,1],[10,0,0],[10,1,0],[10,0,1]]),2,random_state=13)==[[0.0,0.3333,0.3333],[10.0,0.3333,0.3333]],\"failed: 3-D example\"\nassert kmeans(np.array([[1,1],[2,2],[3,3]]),3,random_state=19)==[[1.0,1.0],[2.0,2.0],[3.0,3.0]],\"failed: K equals number of points\"\nassert kmeans(np.array([[-1,0],[-2,0],[1,0],[2,0]]),2,random_state=23)==[[-1.5,0.0],[1.5,0.0]],\"failed: symmetric clusters on x-axis\"\nassert kmeans(np.array([[0,0],[0,1],[1,0],[1,1]]),4,random_state=29)==[[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]],\"failed: each point its own cluster\"\nassert kmeans(np.array([[2,2],[2,4],[8,2],[8,4]]),2,random_state=5)==[[2.0,3.0],[8.0,3.0]],\"failed: square split into two rectangles\""}
{"task_id": 111, "completion_id": 0, "passed": false, "result": "failed: name '_identity_predict' is not defined", "solution": "import numpy as np\nimport numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a string into a fixed-length list of integer token IDs.\"\"\"\n    substring = text[:time_steps]\n    encoded = []\n    for c in substring:\n        encoded.append(vocabulary.get(c, 0))\n    while len(encoded) < time_steps:\n        encoded.append(0)\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token IDs back to a string, ignoring padding.\"\"\"\n    s = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        if idx in inverse_vocab:\n            s.append(inverse_vocab[idx])\n    return ''.join(s)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes text, runs through model, and decodes the prediction.\"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    encoded_batch = np.array([encoded])\n    predictions = model.predict(encoded_batch)\n    predicted_ids = np.argmax(predictions, axis=2)\n    predicted_ids = predicted_ids[0]\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hello') == 'hello', 'failed on \"hello\"'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'HELLO') == 'hello', 'failed on upper-case input'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '') == '', 'failed on empty string'\na20 = 'a' * 25\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, a20) == 'a' * 20, 'failed on long input truncation'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'hi!') == 'hi', 'failed on unknown character removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'data science') == 'data science', 'failed on string with space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, '       ') == '       ', 'failed on only spaces'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc xyz') == 'abc xyz', 'failed on mixed letters and space'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'abc_def') == 'abcdef', 'failed on underscore removal'\nassert run_example(_identity_predict, INPUT_VOCAB, INV_VOCAB, 'padding test') == 'padding test', 'failed on general case'"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "import re\nfrom collections import Counter\nimport re\nfrom collections import Counter\nclass Token:\n\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    words = re.findall('[a-zA-Z]+', text)\n    lower_words = [word.lower() for word in words]\n    counts = Counter(lower_words)\n    tokens = []\n    for (word, count) in counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    return tokens\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"Hello, hello world!\")] == [(\"hello\", 2), (\"world\", 1)], \"test case failed: 'Hello, hello world!'\"\nassert tokenize_and_count(\"\") == [], \"test case failed: empty string\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"A a A a b B\")] == [(\"a\", 4), (\"b\", 2)], \"test case failed: 'A a A a b B'\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"b a\")] == [(\"a\", 1), (\"b\", 1)], \"test case failed: 'b a'\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"This is a test. This test is only a test.\")] == [(\"test\", 3), (\"a\", 2), (\"is\", 2), (\"this\", 2), (\"only\", 1)], \"test case failed: sentence with punctuation\"\nassert tokenize_and_count(\"123 456 !!!\") == [], \"test case failed: numeric input\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"co-op co op\")] == [(\"co\", 2), (\"op\", 2)], \"test case failed: hyphenated words\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"Python\")] == [(\"python\", 1)], \"test case failed: single word\"\nassert [(tok.word, tok.count) for tok in tokenize_and_count(\"one1 two2 two2 one1 one1\")] == [(\"one\", 3), (\"two\", 2)], \"test case failed: alphanumeric words\"\nlarge_text = \"word \" * 1000 + \"test \" * 500\nassert [(tok.word, tok.count) for tok in tokenize_and_count(large_text)][:2] == [(\"word\", 1000), (\"test\", 500)], \"test case failed: large repetition\""}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: name 'run_example' is not defined", "solution": "\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions\nassert run_examples(lambda s: s[::-1], None, None, [\"abc\"]) == [\"cba\"], \"test case failed: single example reversal\"\nassert run_examples(lambda s: s.upper(), None, None, [\"hello\", \"world\"]) == [\"HELLO\", \"WORLD\"], \"test case failed: uppercase mapping\"\nassert run_examples(lambda s: s, None, None, []) == [], \"test case failed: empty example list\"\nassert run_examples(lambda s: s[::-1], None, None, [\"\", \"a\"]) == [\"\", \"a\"], \"test case failed: empty and single char strings\"\nassert run_examples(lambda s: s*2, None, None, [\"cat\"]) == [\"catcat\"], \"test case failed: duplication model\"\nassert run_examples(lambda s: ''.join(sorted(s)), None, None, [\"cba\", \"fed\"] ) == [\"abc\", \"def\"], \"test case failed: sort characters\"\nassert run_examples(lambda s: ''.join(chr(ord(c)+1) for c in s), None, None, [\"abc\"]) == [\"bcd\"], \"test case failed: shift characters\"\nassert run_examples(lambda s: s[::-1].upper(), None, None, [\"Python\", \"AI\"]) == [\"NOHTYP\", \"IA\"], \"test case failed: reverse and uppercase\"\nassert run_examples(lambda s: ''.join('*' for _ in s), None, None, [\"mask\"]) == [\"****\"], \"test case failed: masking model\"\nassert run_examples(lambda s: s.strip(), None, None, [\"  spaced  \"]) == [\"spaced\"], \"test case failed: strip whitespaces\""}
{"task_id": 114, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef selu(x: np.ndarray, derivative: bool=False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    lambda_val = 1.0507009873554805\n    if derivative:\n        result = np.where(x >= 0, lambda_val, lambda_val * alpha * np.exp(x))\n    else:\n        result = np.where(x >= 0, lambda_val * x, lambda_val * alpha * (np.exp(x) - 1))\n    result = np.round(result, decimals=6)\n    return result.tolist()\nimport numpy as np\nassert selu(np.array([-1, 0, 1])) == [-1.111331, 0.0, 1.050701], \"test case failed: selu([-1,0,1])\"\nassert selu(np.array([0])) == [0.0], \"test case failed: selu([0])\"\nassert selu(np.array([0]), derivative=True) == [1.050701], \"test case failed: selu([0], True)\"\nassert selu(np.array([])) == [], \"test case failed: empty array activation\"\nassert selu(np.array([]), derivative=True) == [], \"test case failed: empty array derivative\""}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: name '_expected' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    y = y.ravel()\n    w = w.ravel()\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -(y * np.log(p) + (1 - y) * np.log(1 - p)).sum() / m\n    loss = round(loss, 4)\n    gradient = X.T.dot(p - y) / m\n    gradient = np.round(gradient, 4)\n    gradient = gradient.reshape(-1, 1).tolist()\n    return (loss, gradient)\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])) == _expected(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), np.array([0, 0, 0, 1]), np.array([[0.5], [0.5]])), \"test case failed: #1\"\nassert logistic_loss_and_gradient(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])) == _expected(np.array([[1, 2], [3, 4]]), np.array([1, 0]), np.array([[0.0], [0.0]])), \"test case failed: #2\"\nassert logistic_loss_and_gradient(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])) == _expected(np.array([[1, 1], [2, 2], [3, 3]]), np.array([0, 1, 1]), np.array([[0.1], [-0.2]])), \"test case failed: #3\"\nassert logistic_loss_and_gradient(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])) == _expected(np.array([[2, 3, 4], [1, 0, 1], [0, 1, 1]]), np.array([1, 0, 1]), np.array([[0.2], [0.3], [-0.1]])), \"test case failed: #4\"\nassert logistic_loss_and_gradient(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])) == _expected(np.array([[1], [2], [3]]), np.array([0, 0, 1]), np.array([[0.0]])), \"test case failed: #5\"\nassert logistic_loss_and_gradient(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])) == _expected(np.array([[0.5, 1.5], [1.5, 0.5]]), np.array([1, 0]), np.array([[0.2], [0.2]])), \"test case failed: #6\"\nassert logistic_loss_and_gradient(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])) == _expected(np.array([[10, 10], [-10, -10]]), np.array([1, 0]), np.array([[0.01], [0.01]])), \"test case failed: #7\"\nassert logistic_loss_and_gradient(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])) == _expected(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([0, 1, 0]), np.array([[0.1], [0.2], [0.3]])), \"test case failed: #8\"\nassert logistic_loss_and_gradient(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])) == _expected(np.array([[5, 1], [2, 3], [3, 5], [1, 1]]), np.array([1, 0, 1, 0]), np.array([[0.2], [-0.3]])), \"test case failed: #9\"\nassert logistic_loss_and_gradient(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])) == _expected(np.array([[0, 0], [0, 0]]), np.array([0, 0]), np.array([[0], [0]])), \"test case failed: #10\""}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    N = len(y)\n    if N == 0:\n        return 0.0\n    mean_y = np.mean(y)\n    parent_mse = np.mean((y - mean_y) ** 2)\n    mse_children = 0.0\n    for split in splits:\n        len_split = len(split)\n        if len_split == 0:\n            continue\n        mean_split = np.mean(split)\n        split_mse = np.mean((split - mean_split) ** 2)\n        weight = len_split / N\n        mse_children += weight * split_mse\n    delta_mse = parent_mse - mse_children\n    return round(delta_mse, 4)\nimport numpy as np\nassert mse_criterion(np.array([1, 2, 3, 4]), [np.array([1, 2]), np.array([3, 4])]) == 1.0, \"test case failed: basic two-way split\"\nassert mse_criterion(np.array([1, 2, 3]), [np.array([1, 2, 3])]) == 0.0, \"test case failed: no real split\"\nassert mse_criterion(np.array([1, 1, 1]), [np.array([1, 1]), np.array([1])]) == 0.0, \"test case failed: zero variance data\"\nassert mse_criterion(np.array([0, 10, 20, 30]), [np.array([0, 10]), np.array([20, 30])]) == 100.0, \"test case failed: large variance reduction\"\nassert mse_criterion(np.array([1, 4, 7, 10, 13]), [np.array([1, 4, 7]), np.array([10, 13])]) == 13.5, \"test case failed: uneven split sizes\"\nassert mse_criterion(np.array([2, 2, 2, 2]), [np.array([2, 2]), np.array([2, 2])]) == 0.0, \"test case failed: identical targets\"\nassert mse_criterion(np.array([1, 2]), [np.array([1]), np.array([2])]) == 0.25, \"test case failed: singletons\"\nassert mse_criterion(np.array([1, 2, 3, 4, 5, 6]), [np.array([1, 2, 3]), np.array([4, 5, 6])]) == 2.25, \"test case failed: six elements split in half\"\nassert mse_criterion(np.array([1, 4, 7, 10]), [np.array([1]), np.array([4]), np.array([7, 10])]) == 10.125, \"test case failed: three-way split\"\nassert mse_criterion(np.array([0, 0, 10, 10, 20, 20]), [np.array([0, 0, 10]), np.array([10, 20, 20])]) == 44.4444, \"test case failed: mixed targets split\""}
{"task_id": 122, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_augmented = np.insert(X_np, 0, 1, axis=1)\n    X_pseudo = np.linalg.pinv(X_augmented)\n    weights = X_pseudo @ y_np\n    weights_rounded = np.round(weights, 4).tolist()\n    return weights_rounded\nassert linear_regression_weights([[0], [1], [2]], [1, 2, 3]) == [1.0, 1.0], \"failed on y = x + 1\"\nassert linear_regression_weights([[0], [1], [2], [3]], [5, 5, 5, 5]) == [5.0, 0.0], \"failed on constant target\"\nassert linear_regression_weights([[1], [2], [3]], [2, 4, 6]) == [0.0, 2.0], \"failed on y = 2x\"\nassert linear_regression_weights([[1, 2], [2, 3], [3, 4], [4, 5]], [3, 5, 7, 9]) == [0.0, 1.0, 1.0], \"failed on y = x1 + x2\"\nassert linear_regression_weights([[1, 0], [0, 1], [1, 1], [2, 3]], [3, 3, 4, 7]) == [2.0, 1.0, 1.0], \"failed on y = 2 + x1 + x2\"\nassert linear_regression_weights([[4], [5], [6]], [9, 11.5, 14]) == [-1.0, 2.5], \"failed on y = -1 + 2.5x\"\nassert linear_regression_weights([[2, 1], [0, 1], [1, 3]], [0, 2, 5]) == [0.0, -1.0, 2.0], \"failed on y = -x1 + 2x2\"\nassert linear_regression_weights([[1], [2], [3]], [1.5, 1.5, 1.5]) == [1.5, 0.0], \"failed on y = 1.5\"\nassert linear_regression_weights([[0, 1], [1, 0], [2, 2]], [0, 1.5, 1.5]) == [0.5, 1.0, -0.5], \"failed on mixed weights\"\nassert linear_regression_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]], [1, 2, 3, 6]) == [0.0, 1.0, 2.0, 3.0], \"failed on 3-feature dataset\""}
{"task_id": 123, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique = np.unique(y)\n    result = []\n    for element in y:\n        row = []\n        for cat in unique:\n            row.append(1 if element == cat else 0)\n        result.append(row)\n    return result\nassert one_hot_encoding(np.array([0, 1, 2])) == [[1, 0, 0], [0, 1, 0], [0, 0, 1]], \"Test case failed: integers [0,1,2]\"\nassert one_hot_encoding(np.array([2, 2, 1, 0])) == [[0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0]], \"Test case failed: integers with repetition [2,2,1,0]\"\nassert one_hot_encoding(np.array([-1, 0, -1])) == [[1, 0], [0, 1], [1, 0]], \"Test case failed: negative integers [-1,0,-1]\"\nassert one_hot_encoding(np.array([\"cat\", \"dog\", \"cat\"])) == [[1, 0], [0, 1], [1, 0]], \"Test case failed: strings ['cat','dog','cat']\"\nassert one_hot_encoding([3, 1, 3, 3]) == [[0, 1], [1, 0], [0, 1], [0, 1]], \"Test case failed: list [3,1,3,3]\"\nassert one_hot_encoding(np.array([42])) == [[1]], \"Test case failed: single element [42]\"\nassert one_hot_encoding(np.array([1, 1, 2, 2])) == [[1, 0], [1, 0], [0, 1], [0, 1]], \"Test case failed: sorted integers [1,1,2,2]\"\nlarge_input = np.arange(10)\nexpected_large = np.eye(10, dtype=int).tolist()\nassert one_hot_encoding(large_input) == expected_large, \"Test case failed: large range 0-9\"\nassert one_hot_encoding([\"a\", \"b\", \"c\", \"a\"]) == [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], \"Test case failed: list of strings ['a','b','c','a']\"\nassert one_hot_encoding(np.array([\"x\", \"z\", \"x\", \"y\"])) == [[1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]], \"Test case failed: strings ['x','z','x','y']\""}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef polynomial_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, d: int=3, gamma: float | None=None, c0: float=1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    dot_product = X_np.dot(Y_np.T)\n    C = X_np.shape[1]\n    if gamma is None:\n        gamma_val = 1.0 / C\n    else:\n        gamma_val = gamma\n    kernel = (gamma_val * dot_product + c0) ** d\n    kernel_rounded = np.round(kernel, decimals=4)\n    result = kernel_rounded.tolist()\n    return result\nassert polynomial_kernel([[1, 2], [3, 4]]) == [[42.875, 274.625], [274.625, 2460.375]], \"test failed: default parameters with 2\u00d72 input\"\nassert polynomial_kernel([[1, 0], [0, 1]], d=2, gamma=1, c0=0) == [[1.0, 0.0], [0.0, 1.0]], \"test failed: homogeneous degree-2 kernel on identity\"\nassert polynomial_kernel([[1, 2, 1]], [[0, 1, 0]], d=2, gamma=0.5, c0=1) == [[4.0]], \"test failed: explicit X, Y with gamma=.5\"\nassert polynomial_kernel([[2, 3]], [[2, 3]], d=1, gamma=1, c0=0) == [[13.0]], \"test failed: degree-1 (linear) kernel, c0=0\"\nassert polynomial_kernel([[2, 3]], [[2, 3]], d=1, gamma=1, c0=5) == [[18.0]], \"test failed: degree-1 with bias term\"\nassert polynomial_kernel([[1, 2], [3, 4]], [[5, 6], [7, 8]], d=2, gamma=1, c0=1) == [[324.0, 576.0], [1600.0, 2916.0]], \"test failed: 2\u00d72 vs 2\u00d72 different matrices\"\nassert polynomial_kernel([[1, 2, 3]], d=3, gamma=None, c0=0) == [[101.6296]], \"test failed: automatic gamma = 1/C\"\nassert polynomial_kernel([[0, 0]], [[0, 0]], d=3, gamma=2, c0=1) == [[1.0]], \"test failed: zero vectors\"\nassert polynomial_kernel([[1, 1]], [[-1, -1]], d=2, gamma=1, c0=0) == [[4.0]], \"test failed: negative correlation\"\nassert polynomial_kernel([[1, 2]], [[3, 4]], d=2, gamma=0.5, c0=0) == [[30.25]], \"test failed: gamma=0.5, degree=2\""}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    unique_true = set(y_true)\n    unique_pred = set(y_pred)\n    all_classes = unique_true.union(unique_pred)\n    classes = sorted(all_classes)\n    class_to_idx = {c: i for (i, c) in enumerate(classes)}\n    num_classes = len(classes)\n    matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n    for (yt, yp) in zip(y_true, y_pred):\n        i = class_to_idx[yt]\n        j = class_to_idx[yp]\n        matrix[i][j] += 1\n    return matrix\nassert confusion_matrix(['cat','dog','cat'],['cat','cat','dog']) == [[1,1],[1,0]], \"failed on example with strings\"\nassert confusion_matrix([1,2,3,4],[4,3,2,1]) == [[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]], \"failed on reversed labels\"\nassert confusion_matrix([0,0,0,0],[0,0,0,0]) == [[4]], \"failed on single class perfect match\"\nassert confusion_matrix([0,1,1,1],[1,1,1,1]) == [[0,1],[0,3]], \"failed on partial mismatch\"\nassert confusion_matrix([],[]) == [], \"failed on empty lists\"\nassert confusion_matrix([1,2],[1]) == -1, \"failed on unequal length detection\"\nassert confusion_matrix([2,2,1,0],[2,1,1,0]) == [[1,0,0],[0,1,0],[0,1,1]], \"failed on unsorted class order\"\nassert confusion_matrix(['a','b','c'],['c','b','a']) == [[0,0,1],[0,1,0],[1,0,0]], \"failed on character labels\"\nassert confusion_matrix([True,False,True],[True,True,False]) == [[0,1],[1,1]], \"failed on boolean labels\""}
{"task_id": 129, "completion_id": 0, "passed": true, "result": "passed", "solution": "from itertools import islice, cycle\nfrom itertools import islice, cycle\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    c = cycle(sequence)\n    return list(islice(c, samples))\nassert cycle_sequence([1,2,3], 8) == [1,2,3,1,2,3,1,2], \"failed on cycle_sequence([1,2,3], 8)\"\nassert cycle_sequence(['x'], 4) == ['x','x','x','x'], \"failed on single-element sequence\"\nassert cycle_sequence([], 5) == [], \"failed on empty sequence\"\nassert cycle_sequence([0,1], 0) == [], \"failed on zero samples\"\nassert cycle_sequence([0,1], -3) == [], \"failed on negative samples\"\nassert cycle_sequence(['a','b','c'], 1) == ['a'], \"failed on one sample\"\nassert cycle_sequence(['a','b','c'], 2) == ['a','b'], \"failed on two samples\"\nassert cycle_sequence(['a','b','c'], 3) == ['a','b','c'], \"failed on exactly full length\"\nassert cycle_sequence(['a','b','c'], 4) == ['a','b','c','a'], \"failed on length+1 samples\"\nassert cycle_sequence([True, False], 7) == [True, False, True, False, True, False, True], \"failed on boolean sequence\""}
{"task_id": 130, "completion_id": 0, "passed": true, "result": "passed", "solution": "from collections import Counter\nfrom collections import Counter\nfrom typing import Any, List, Tuple\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]], state: Any, action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    filtered = [t for t in transitions if t[0] == state and t[1] == action]\n    if not filtered:\n        return []\n    counts = Counter((t[3] for t in filtered))\n    total = len(filtered)\n    probs = []\n    for (next_state, count) in counts.items():\n        prob = count / total\n        rounded_prob = round(prob, 4)\n        probs.append([next_state, rounded_prob])\n    probs.sort(key=lambda x: x[0])\n    return probs\nassert outcome_probs([(0,'left',-1,1),(0,'left',-1,1),(0,'left',-1,2),(0,'right',0,3)],0,'left') == [[1,0.6667],[2,0.3333]], \"failed: basic two-outcome distribution\"\nassert outcome_probs([(1,'up',0,2),(1,'up',0,2),(1,'up',0,2)],1,'up') == [[2,1.0]], \"failed: single deterministic outcome\"\nassert outcome_probs([(0,'left',-1,1),(0,'right',0,3)],99,'none') == [], \"failed: unseen pair\"\nassert outcome_probs([(2,0,1,3),(2,0,0,3),(2,0,2,4)],2,0) == [[3,0.6667],[4,0.3333]], \"failed: numeric action\"\nassert outcome_probs([('s0','jump',0,'s1'),('s0','jump',0,'s2'),('s0','jump',0,'s1')],'s0','jump') == [['s1',0.6667],['s2',0.3333]], \"failed: string labels\"\nassert outcome_probs([(10,'a',0,i) for i in range(10)] + [(10,'a',0,0)],10,'a') == [[0,0.1818]] + [[i,0.0909] for i in range(1,10)], \"failed: many outcomes\"\nassert outcome_probs([(1.2,'go',0,2),(1.2,'go',0,2),(1.2,'go',0,3)],1.2,'go') == [[2,0.6667],[3,0.3333]], \"failed: float state\"\nassert outcome_probs([(1.2,'go',0,2)],1.2,'stop') == [], \"failed: unseen action\"\nassert outcome_probs([(0,'a',0,1)]*333 + [(0,'a',0,2)]*667,0,'a') == [[1,0.333],[2,0.667]], \"failed: rounding\"\nassert outcome_probs([(5,'x',0,7),(5,'x',0,6),(5,'x',0,7)],5,'x') == [[6,0.3333],[7,0.6667]], \"failed: sorting order\""}
{"task_id": 131, "completion_id": 0, "passed": false, "result": "failed: name '_SHARED_SEQUENCES' is not defined", "solution": "\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    seq = _SHARED_SEQUENCES[uid]\n    if not seq:\n        return None\n    if i >= 0:\n        effective_i = i\n    else:\n        effective_i = len(seq) + i\n    if effective_i < 0 or effective_i >= len(seq):\n        return None\n    return seq[effective_i]\nassert get_index(1, 0) == 5, \"test case failed: get_index(1, 0)\"\nassert get_index(2, 0) is None, \"test case failed: get_index(2, 0) \u2013 empty sequence should return None\"\nassert get_index(1, -1) == 2, \"test case failed: get_index(1, -1)\"\nassert get_index(0, 3) is None, \"test case failed: get_index(0, 3) \u2013 out of range\"\n_SHARED_SEQUENCES.append([100, 200])\nassert get_index(3, 1) == 200, \"test case failed: get_index(3, 1) after append\"\n_SHARED_SEQUENCES[0] = ['a', 'b']\nassert get_index(0, 1) == 'b', \"test case failed: get_index(0, 1) after replacement\"\n_SHARED_SEQUENCES.append([])\nassert get_index(4, 0) is None, \"test case failed: get_index(4, 0) \u2013 newly appended empty sequence\""}
{"task_id": 133, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    for seq in X_train:\n        for token in seq:\n            if token < 0 or token >= vocab_length:\n                return -1\n    Y_train = []\n    for seq in X_train:\n        one_hot_seq = []\n        for token in seq:\n            vector = np.zeros(vocab_length, dtype=int)\n            vector[token] = 1\n            one_hot_seq.append(vector.tolist())\n        Y_train.append(one_hot_seq)\n    return Y_train\nassert one_hot_targets([[0, 2, 1], [1, 3, 2]], 4) == [[[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]], [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]], \"failed: basic example\"\nassert one_hot_targets([[3, 3, 3]], 4) == [[[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]]], \"failed: repeated tokens\"\nassert one_hot_targets([[0]], 1) == [[[1]]], \"failed: single token single vocab\"\nassert one_hot_targets([[1, 0], [0, 1]], 2) == [[[0, 1], [1, 0]], [[1, 0], [0, 1]]], \"failed: two-class alternating\"\nassert one_hot_targets([], 5) == [], \"failed: empty input list\"\nassert one_hot_targets([[0, 4]], 4) == -1, \"failed: index equal to vocab_length should be invalid\"\nassert one_hot_targets([[0, -1]], 3) == -1, \"failed: negative index should be invalid\"\nassert one_hot_targets([[2, 1, 0]], 3) == [[[0, 0, 1], [0, 1, 0], [1, 0, 0]]], \"failed: descending indices\"\nassert one_hot_targets([[1, 2, 1, 0]], 3) == [[[0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]], \"failed: longer single sequence\"\nassert one_hot_targets([[2, 2], [2, 2], [2, 2]], 3) == [[[0, 0, 1], [0, 0, 1]], [[0, 0, 1], [0, 0, 1]], [[0, 0, 1], [0, 0, 1]]], \"failed: all same tokens multiple samples\""}
{"task_id": 134, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    index = payoff_probs.index(max_p)\n    return (max_p, index)\nassert best_arm([0.15, 0.6, 0.35]) == (0.6, 1), \"failed on [0.15, 0.6, 0.35]\"\nassert best_arm([0.8, 0.8, 0.5]) == (0.8, 0), \"failed on tie with first max\"\nassert best_arm([0.0, 0.0, 0.0]) == (0.0, 0), \"failed on all zeros\"\nassert best_arm([1.0]) == (1.0, 0), \"failed on single arm\"\nassert best_arm([]) == (-1.0, -1), \"failed on empty list\"\nassert best_arm([0.2, -0.1, 0.3]) == (-1.0, -1), \"failed on probability < 0\"\nassert best_arm([0.2, 1.1, 0.3]) == (-1.0, -1), \"failed on probability > 1\"\nassert best_arm([0.333, 0.333, 0.3329]) == (0.333, 0), \"failed on close tie\"\nassert best_arm([0.25, 0.5, 0.75, 0.74]) == (0.75, 2), \"failed on normal case\"\nassert best_arm([0.9, 0.1]) == (0.9, 0), \"failed on two arms\""}
{"task_id": 136, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softmax(z: np.ndarray, axis: int=-1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Divide by the sum of exponentials along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: -1).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    max_z = np.max(z, axis=axis, keepdims=True)\n    shifted_z = z - max_z\n    exp_z = np.exp(shifted_z)\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    softmax = exp_z / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()\nassert softmax(np.array([1, 2, 3])) == [0.09, 0.2447, 0.6652], \"test 1 failed: softmax([1,2,3])\"\nassert softmax(np.array([0, 0])) == [0.5, 0.5], \"test 2 failed: softmax([0,0])\"\nassert softmax(np.array([-1, -2, -3])) == [0.6652, 0.2447, 0.09], \"test 3 failed: softmax([-1,-2,-3])\"\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]])) == [[0.09, 0.2447, 0.6652], [0.09, 0.2447, 0.6652]], \"test 4 failed: row-wise softmax\"\nassert softmax(np.array([[1, 2, 3], [4, 5, 6]]), axis=0) == [[0.0474, 0.0474, 0.0474], [0.9526, 0.9526, 0.9526]], \"test 5 failed: column-wise softmax\"\nassert softmax(np.array([[1000, 1000], [1000, 1000]]), axis=1) == [[0.5, 0.5], [0.5, 0.5]], \"test 6 failed: large identical values\"\nassert softmax(np.array([1000, 1000])) == [0.5, 0.5], \"test 7 failed: vector of large identical values\"\nassert softmax(np.array([[1], [2]]), axis=0) == [[0.2689], [0.7311]], \"test 8 failed: column vector, axis=0\"\nassert softmax(np.array([[1], [2]]), axis=1) == [[1.0], [1.0]], \"test 9 failed: column vector, axis=1\"\nassert softmax(np.array([-1, 1])) == [0.1192, 0.8808], \"test 10 failed: softmax([-1,1])\""}
{"task_id": 138, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    count = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    n = len(y_true)\n    accuracy = count / n\n    return round(accuracy, 4)\nassert accuracy_score([1, 0, 1, 1], [1, 0, 0, 1]) == 0.75, \"test case failed: basic numerical example\"\nassert accuracy_score([1, 1, 1], [1, 1, 1]) == 1.0, \"test case failed: all correct predictions\"\nassert accuracy_score([0, 0, 0], [1, 1, 1]) == 0.0, \"test case failed: no correct predictions\"\nassert accuracy_score([\"cat\", \"dog\", \"cat\"], [\"cat\", \"cat\", \"cat\"]) == 0.6667, \"test case failed: string labels\"\nassert accuracy_score([True, False, True, False], [True, False, False, False]) == 0.75, \"test case failed: boolean labels\"\nassert accuracy_score([1.2, 3.4, 5.6], [1.2, 3.5, 5.6]) == 0.6667, \"test case failed: float labels\"\nassert accuracy_score([42], [42]) == 1.0, \"test case failed: single element correct\"\nassert accuracy_score([], []) == -1, \"test case failed: empty input sequences\"\nassert accuracy_score([1, 2, 3], [1, 2]) == -1, \"test case failed: different length sequences\"\nassert accuracy_score(list(\"abcdef\"), list(\"abcxyz\")) == 0.5, \"test case failed: half correct predictions\""}
{"task_id": 143, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef leaky_relu(x, alpha: float=0.2, derivative: bool=False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x) if not isinstance(x, np.ndarray) else x\n    if not derivative:\n        return np.where(x >= 0, x, alpha * x)\n    else:\n        return np.where(x >= 0, 1, alpha)\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.1), np.array([-0.3, -0.1, 0., 1., 3.])), \"failed on mixed signs activation\"\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.05), np.array([-0.15, -0.05, 0., 1., 3.])), \"failed on custom alpha activation\"\nassert np.allclose(leaky_relu([-3, -1, 0, 1, 3], alpha=0.1, derivative=True), np.array([0.1, 0.1, 1., 1., 1.])), \"failed on gradient computation\"\nassert leaky_relu(5) == 5, \"scalar positive input failed\"\nassert leaky_relu(-5, alpha=0.3) == -1.5, \"scalar negative input failed\"\nassert leaky_relu(-5, alpha=0.3, derivative=True) == 0.3, \"scalar gradient negative failed\"\nx = np.random.randn(1000)\nassert leaky_relu(x).shape == x.shape, \"shape mismatch on random vector\"\nmat = np.random.randn(5, 4, 3)\nassert leaky_relu(mat, derivative=True).shape == mat.shape, \"shape mismatch on tensor derivative\"\nzero_arr = np.zeros((10,))\nassert np.array_equal(leaky_relu(zero_arr), zero_arr), \"zeros activation failed\""}
{"task_id": 147, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, epochs: int=10000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * X_test.shape[0]\n    X_train_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test_with_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    n_features = X_train_with_bias.shape[1]\n    w = np.zeros(n_features)\n    for _ in range(epochs):\n        z = X_train_with_bias.dot(w)\n        h = 1 / (1 + np.exp(-z))\n        error = h - y_train\n        gradient = X_train_with_bias.T.dot(error) / X_train_with_bias.shape[0]\n        w = w - learning_rate * gradient\n    z_test = X_test_with_bias.dot(w)\n    h_test = 1 / (1 + np.exp(-z_test))\n    predictions = (h_test >= 0.5).astype(int)\n    return predictions.tolist()\nassert logistic_regression(np.array([[-2,-1],[-3,-2],[-1,-2],[-2,-3],[2,1],[3,2],[1,2],[2,3]]), np.array([0,0,0,0,1,1,1,1]), np.array([[-2,-2],[2,2]])) == [0,1], \"test case failed: basic 2D separation\"\nassert logistic_regression(np.array([[-2],[-1.5],[-1],[5],[6],[7]]), np.array([0,0,0,1,1,1]), np.array([[-1.25],[6]])) == [0,1], \"test case failed: 1D simple\"\nassert logistic_regression(np.array([[-3,1],[-2,2],[-1,1],[4,1],[5,2],[6,2]]), np.array([0,0,0,1,1,1]), np.array([[-2,1],[5,1]])) == [0,1], \"test case failed: 2D x-axis split\"\nassert logistic_regression(np.array([[-4,0,1],[-3,1,0],[-2,1,1],[3,0,0],[4,1,1],[5,0,1]]), np.array([0,0,0,1,1,1]), np.array([[-3,0,1],[4,0,0]])) == [0,1], \"test case failed: 3D features\"\nassert logistic_regression(np.array([[-1,-1],[-2,-1],[-1,-2],[1,1],[2,1],[1,2]]), np.array([0,0,0,1,1,1]), np.array([[-1.5,-1.5],[1.5,1.5]])) == [0,1], \"test case failed: negative vs positive quadrant\"\nassert logistic_regression(np.array([[-5],[-4],[-3],[3],[4],[5]]), np.array([0,0,0,1,1,1]), np.array([[-4.5],[4.5]])) == [0,1], \"test case failed: symmetric 1D\"\nassert logistic_regression(np.array([[-3,0],[-2,-2],[-1,0],[1,1],[2,2],[3,1]]), np.array([0,0,0,1,1,1]), np.array([[-2,-1],[2,1]])) == [0,1], \"test case failed: diagonal separation\"\nassert logistic_regression(np.array([[-1,10],[-2,9],[-3,11],[4,10],[5,9],[6,11]]), np.array([0,0,0,1,1,1]), np.array([[-2,10],[5,10]])) == [0,1], \"test case failed: irrelevant second feature\"\nassert logistic_regression(np.array([[-20],[-15],[-10],[10],[15],[20]]), np.array([0,0,0,1,1,1]), np.array([[-12],[12]])) == [0,1], \"test case failed: large magnitude 1D\"\nassert logistic_regression(np.array([[-3,-2,0],[-2,-1,-1],[-4,-1,-2],[3,2,1],[2,1,1],[4,2,2]]), np.array([0,0,0,1,1,1]), np.array([[-3,-1,-1],[3,1,1]])) == [0,1], \"test case failed: sum of features\""}
{"task_id": 148, "completion_id": 0, "passed": false, "result": "failed: name 'exp_smoothing' is not defined", "solution": "from typing import List\n\nassert exp_smoothing([10, 20, 30, 25, 15], 0.1) == [10.0, 11.0, 12.9, 14.11, 14.199], \"failed on basic example\"\nassert exp_smoothing([5, 4, 3, 2, 1], 0.3) == [5.0, 4.7, 4.19, 3.533, 2.7731], \"failed on decreasing list\"\nassert exp_smoothing([1, 1, 1, 1], 0.25) == [1.0, 1.0, 1.0, 1.0], \"failed on constant list\"\nassert exp_smoothing([3, 6, 9], 0.5) == [3.0, 4.5, 6.75], \"failed on alpha 0.5\"\nassert exp_smoothing([7, 8, 9], 0) == [7.0, 7.0, 7.0], \"failed on alpha 0\"\nassert exp_smoothing([7, 8, 9], 1) == [7.0, 8.0, 9.0], \"failed on alpha 1\"\nassert exp_smoothing([], 0.3) == -1, \"failed on empty data list\"\nassert exp_smoothing([1, 2, 3], -0.1) == -1, \"failed on negative alpha\"\nassert exp_smoothing([1, 2, 3], 1.1) == -1, \"failed on alpha greater than 1\"\nassert exp_smoothing([100], 0.6) == [100.0], \"failed on single element list\""}
{"task_id": 152, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\nassert accuracy_score([1, 0, 2, 2, 1], [1, 0, 1, 2, 0]) == 0.6, \"Test case failed: basic mixed correctness\"\nassert accuracy_score([0, 1, 2], [0, 1, 2]) == 1.0, \"Test case failed: perfect accuracy\"\nassert accuracy_score([1, 2, 3], [1, 2]) == -1, \"Test case failed: unequal length should return -1\"\nassert accuracy_score([0, 1, 1, 1], [1, 1, 1, 1]) == 0.75, \"Test case failed: one misclassification\"\nassert accuracy_score([1, 2, 3, 4], [4, 3, 2, 1]) == 0.0, \"Test case failed: zero accuracy\"\nassert accuracy_score([1], [1]) == 1.0, \"Test case failed: single element correct\"\nassert accuracy_score([0, 0, 0, 0, 0], [0, 1, 0, 1, 0]) == 0.6, \"Test case failed: majority correct\"\nimport numpy as np\nassert accuracy_score(np.array([1, 2, 1, 2]), np.array([1, 2, 2, 2])) == 0.75, \"Test case failed: numpy array input\"\nassert accuracy_score([], []) == -1, \"Test case failed: empty input\"\nassert accuracy_score([1, 1, 1, 0, 0], [1, 0, 1, 0, 0]) == 0.8, \"Test case failed: 80 percent accuracy\""}
{"task_id": 154, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float=0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n    X_norms = np.sum(X ** 2, axis=1)\n    Y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X.dot(Y.T)\n    squared_distances = X_norms[:, np.newaxis] + Y_norms[np.newaxis, :] - 2 * dot_product\n    K = np.exp(-gamma * squared_distances)\n    K_rounded = np.round(K, 6)\n    return K_rounded.tolist()\nassert rbf_kernel(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [5, 5]]), 0.1) == [[1.0, 0.082085], [0.449329, 0.606531]], \"test case failed: example\"\nassert rbf_kernel(np.array([[1, 2, 3]]), np.array([[1, 2, 4]]), 0.5) == [[0.606531]], \"test case failed: single samples\"\nassert rbf_kernel(np.array([[1]]), np.array([[3]]), 0.0) == [[1.0]], \"test case failed: gamma 0\"\nassert rbf_kernel(np.array([[0, 0], [1, 1], [2, 2]]), np.array([[0, 0], [2, 2]]), 1.0) == [[1.0, 0.000335], [0.135335, 0.135335], [0.000335, 1.0]], \"test case failed: multiple samples\"\nassert rbf_kernel(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), 0.5) == [[1.0, 0.367879], [0.367879, 1.0]], \"test case failed: symmetric matrix\"\nassert rbf_kernel(np.array([[1, 2]]), np.array([[1, 2]]), 10.0) == [[1.0]], \"test case failed: large gamma\"\nassert rbf_kernel(np.array([[0, 0]]), np.array([[100, 100]]), 0.01) == [[0.0]], \"test case failed: distant points\"\nassert rbf_kernel(np.zeros((2, 2)), np.ones((2, 2)), 0.2) == [[0.67032, 0.67032], [0.67032, 0.67032]], \"test case failed: zeros vs ones\"\nassert rbf_kernel(np.array([[0], [1], [2]]), np.array([[1], [2]]), 0.5) == [[0.606531, 0.135335], [1.0, 0.606531], [0.606531, 1.0]], \"test case failed: 1-D features\"\nassert rbf_kernel(np.array([[1, 2, 3]]), np.array([[1, 2, 3]])) == [[1.0]], \"test case failed: default gamma\""}
{"task_id": 155, "completion_id": 0, "passed": false, "result": "failed: name 'weights_simple' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    return [round(x, 4) for x in action]\nassert actor_forward([0.5, -0.5], weights_simple) == [0.4621], \"test case failed: actor_forward([0.5,-0.5],weights_simple)\"\nassert actor_forward([2.0, 3.0], weights_simple) == [0.9999], \"test case failed: actor_forward([2,3],weights_simple)\"\nassert actor_forward([-2.0, 3.0], weights_simple) == [0.9951], \"test case failed: actor_forward([-2,3],weights_simple)\"\nassert actor_forward([1.0, 1.0], weights_simple) == [0.964], \"test case failed: actor_forward([1,1],weights_simple)\"\nassert actor_forward([0.0, 0.0], weights_simple) == [0.0], \"test case failed: actor_forward([0,0],weights_simple)\"\nassert actor_forward([-1.0, -1.0], weights_simple) == [0.0], \"test case failed: actor_forward([-1,-1],weights_simple)\"\nassert actor_forward([100.0, -100.0], weights_simple) == [1.0], \"test case failed: actor_forward([100,-100],weights_simple)\"\nassert actor_forward([-0.1, 0.1], weights_simple) == [0.0997], \"test case failed: actor_forward([-0.1,0.1],weights_simple)\"\nassert actor_forward([0.3, 0.3], weights_simple) == [0.537], \"test case failed: actor_forward([0.3,0.3],weights_simple)\"\nassert actor_forward([-0.3, 0.7], weights_simple) == [0.6044], \"test case failed: actor_forward([-0.3,0.7],weights_simple)\""}
{"task_id": 157, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return 0\n    matches = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    total = len(y_true)\n    accuracy = matches / total\n    return round(accuracy, 4)\nassert accuracy_score([1, 2, 3], [1, 2, 3]) == 1.0, \"test case failed: identical labels\"\nassert accuracy_score([1, 2, 3], [3, 2, 1]) == 0.3333, \"test case failed: one correct out of three\"\nassert accuracy_score([], []) == 0, \"test case failed: empty inputs\"\nassert accuracy_score([1], []) == -1, \"test case failed: length mismatch\"\nassert accuracy_score([0, 1, 2, 2, 1], [0, 0, 2, 2, 1]) == 0.8, \"test case failed: mixed correctness\"\nassert accuracy_score([\"cat\", \"dog\", \"cat\"], [\"cat\", \"cat\", \"cat\"]) == 0.6667, \"test case failed: string labels\"\nassert accuracy_score([True, False, True, False], [True, True, True, False]) == 0.75, \"test case failed: boolean labels\"\nassert accuracy_score(list(range(100)), list(range(100))) == 1.0, \"test case failed: large perfect match\"\nassert accuracy_score(list(range(100)), list(range(99)) + [101]) == 0.99, \"test case failed: one error in 100\"\nassert accuracy_score((1, 1, 1, 1), (1, 0, 1, 0)) == 0.5, \"test case failed: tuple input\""}
{"task_id": 158, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_regression(X: list[list[int | float]], y: list[int | float], alpha: float=1.0, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    if fit_intercept:\n        intercept = np.ones((X_np.shape[0], 1))\n        X_np = np.hstack((intercept, X_np))\n    XtX = np.dot(X_np.T, X_np)\n    n_features = X_np.shape[1]\n    A = alpha * np.eye(n_features)\n    if fit_intercept:\n        A[0, 0] = 0\n    inv_matrix = np.linalg.inv(XtX + A)\n    Xty = np.dot(X_np.T, y_np)\n    beta = np.dot(inv_matrix, Xty)\n    beta_rounded = [round(x, 4) for x in beta]\n    return beta_rounded\nassert ridge_regression([[1, 2], [3, 4], [5, 6]], [1, 2, 3], 1.0, True) == [0.3529, 0.2353, 0.2353], \"failed: example case\"\nassert ridge_regression([[1], [2], [3]], [1, 2, 3], 0.0, False) == [1.0], \"failed: simple OLS without intercept\"\nassert ridge_regression([[1], [2], [3]], [1, 2, 3], 1.0, False) == [0.9333], \"failed: ridge without intercept, \u03b1=1\"\nassert ridge_regression([[1, 0], [0, 1]], [1, 1], 1.0, True) == [1.0, 0.0, 0.0], \"failed: tiny matrix with intercept\"\nassert ridge_regression([[1, 2], [3, 4]], [5, 6], 0.0, False) == [-4.0, 4.5], \"failed: 2\u00d72 OLS\"\nassert ridge_regression([[1, 2], [3, 4]], [5, 6], 1.0, False) == [0.2, 1.4857], \"failed: 2\u00d72 ridge \u03b1=1\"\nassert ridge_regression([[1], [2]], [1, 1], 1.0, True) == [1.0, 0.0], \"failed: intercept only fit\"\nassert ridge_regression([[0], [0], [0]], [1, 1, 1], 1.0, True) == [1.0, 0.0], \"failed: zero feature with intercept\"\nassert ridge_regression([[1], [1]], [2, 4], 1.0, False) == [2.0], \"failed: single feature duplicates\"\nassert ridge_regression([[2, 0], [0, 2]], [1, 2], 0.0, False) == [0.5, 1.0], \"failed: diagonal X matrix\""}
{"task_id": 159, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = x_i.shape[0]\n    term1 = d * np.log(2 * np.pi)\n    (sign, logdet) = np.linalg.slogdet(sigma)\n    term2 = logdet\n    delta = x_i - mu\n    y = np.linalg.solve(sigma, delta)\n    term3 = np.dot(delta, y)\n    total = term1 + term2 + term3\n    result = -0.5 * total\n    return round(result, 4)\nassert log_gaussian_pdf(np.array([0]), np.array([0]), np.array([[1]])) == -0.9189, \"failed on 1D standard normal at mean\"\nassert log_gaussian_pdf(np.array([1]), np.array([0]), np.array([[1]])) == -1.4189, \"failed on 1D standard normal at x=1\"\nassert log_gaussian_pdf(np.array([0,0]), np.array([0,0]), np.eye(2)) == -1.8379, \"failed on 2D standard normal at mean\"\nassert log_gaussian_pdf(np.array([1,1]), np.array([0,0]), np.eye(2)) == -2.8379, \"failed on 2D standard normal at [1,1]\"\nassert log_gaussian_pdf(np.array([2,0]), np.array([1,0]), np.eye(2)) == -2.3379, \"failed on 2D standard normal shifted point\"\nassert log_gaussian_pdf(np.array([0,0]), np.array([0,0]), np.diag([2,2])) == -2.5310, \"failed on 2D diagonal covariance det>1\"\nassert log_gaussian_pdf(np.array([1,0]), np.array([0,0]), np.diag([2,2])) == -2.7810, \"failed on 2D diagonal covariance at [1,0]\"\nassert log_gaussian_pdf(np.array([0,0,0]), np.array([0,0,0]), np.eye(3)) == -2.7568, \"failed on 3D standard normal at mean\"\nassert log_gaussian_pdf(np.array([0]), np.array([0]), np.array([[0.5]])) == -0.5724, \"failed on 1D variance 0.5 at mean\"\nassert log_gaussian_pdf(np.array([1]), np.array([0]), np.array([[0.5]])) == -1.5724, \"failed on 1D variance 0.5 at x=1\""}
{"task_id": 161, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary class labels using a Factorization Machine model.\"\"\"\n    linear_part = w0 + np.dot(X, w)\n    X_V = np.dot(X, V)\n    sum_X_V_sq = np.sum(X_V ** 2, axis=1)\n    V_sq_sum = np.sum(V ** 2, axis=1)\n    X_squared = X ** 2\n    sum_X_squared_V_sq = np.dot(X_squared, V_sq_sum)\n    pairwise = 0.5 * (sum_X_V_sq - sum_X_squared_V_sq)\n    total_score = linear_part + pairwise\n    labels = np.where(total_score >= 0, 1, -1)\n    return labels.tolist()\nassert factorization_machine_predict(np.array([[1, 0], [0, 1]]), 0.0, np.array([-0.5, 0.5]), np.zeros((2, 1))) == [-1, 1], \"test case 1 failed\"\nassert factorization_machine_predict(np.array([[1, 1]]), 0.0, np.array([0.2, 0.2]), np.zeros((2, 1))) == [1], \"test case 2 failed\"\nassert factorization_machine_predict(np.array([[1, 0], [1, 1]]), -1.0, np.array([1.0, 0.0]), np.zeros((2, 1))) == [1, 1], \"test case 3 failed\"\nassert factorization_machine_predict(np.array([[1, 0, 1], [0, 1, 1]]), 0.5, np.array([0.2, -0.3, 0.8]), np.array([[0.1, 0.2], [0.0, -0.1], [0.5, 0.6]])) == [1, 1], \"test case 4 failed\"\nassert factorization_machine_predict(np.array([[1, 0, 1], [0, 1, 1]]), 0.0, np.zeros(3), np.array([[0.1, 0.2], [0.0, -0.1], [0.5, 0.6]])) == [1, -1], \"test case 5 failed\"\nassert factorization_machine_predict(np.array([[1, 1]]), 0.0, np.zeros(2), np.array([[1.0], [1.0]])) == [1], \"test case 6 failed\"\nassert factorization_machine_predict(np.array([[3, 4]]), 0.0, np.zeros(2), np.zeros((2, 1))) == [1], \"test case 7 failed\"\nassert factorization_machine_predict(np.array([[1, 2]]), -0.2, np.array([0.2, 0.1]), np.array([[0.1, 0.2], [0.3, 0.4]])) == [1], \"test case 8 failed\"\nassert factorization_machine_predict(np.array([[2, 1]]), 0.0, np.array([-0.4, -0.3]), np.zeros((2, 1))) == [-1], \"test case 9 failed\"\nassert factorization_machine_predict(np.array([[0, 0], [1, 0], [0, 1]]), 0.2, np.array([0.1, 0.1]), np.zeros((2, 1))) == [1, 1, 1], \"test case 10 failed\""}
{"task_id": 162, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float=1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, 'Shapes of Y_hat and Y do not match.'\n    m = Y_hat.shape[0]\n    loss = -(1 / m) * np.sum(Y * np.log(Y_hat + epsilon))\n    gradient = (Y_hat - Y) / m\n    loss = round(loss, 4)\n    gradient = np.round(gradient, 4)\n    gradient = gradient.tolist()\n    return (loss, gradient)\nassert cross_entropy(np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1]]), np.array([[1, 0, 0], [0, 1, 0]])) == (0.2899, [[-0.15, 0.1, 0.05], [0.05, -0.1, 0.05]]), \"test case failed: example 1\"\nassert cross_entropy(np.array([[0.9, 0.1], [0.2, 0.8]]), np.array([[1, 0], [0, 1]])) == (0.1643, [[-0.05, 0.05], [0.1, -0.1]]), \"test case failed: example 2\"\nassert cross_entropy(np.array([[0.5, 0.5]]), np.array([[1, 0]])) == (0.6931, [[-0.5, 0.5]]), \"test case failed: example 3\"\nassert cross_entropy(np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]), np.array([[1, 0], [0, 1], [1, 0]])) == (0.0, [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]), \"test case failed: example 4\"\nassert cross_entropy(np.array([[0.2, 0.3, 0.5]]), np.array([[0, 0, 1]])) == (0.6931, [[0.2, 0.3, -0.5]]), \"test case failed: example 5\"\nassert cross_entropy(np.array([[0.0, 1.0]]), np.array([[1, 0]])) == (46.0517, [[-1.0, 1.0]]), \"test case failed: example 6\"\nassert cross_entropy(np.array([[0.25, 0.25, 0.25, 0.25], [0.1, 0.2, 0.3, 0.4]]), np.array([[0, 0, 0, 1], [0, 0, 1, 0]])) == (1.2951, [[0.125, 0.125, 0.125, -0.375], [0.05, 0.1, -0.35, 0.2]]), \"test case failed: example 7\"\nassert cross_entropy(np.array([[0.6, 0.4], [0.3, 0.7], [0.5, 0.5], [0.9, 0.1]]), np.array([[1, 0], [0, 1], [1, 0], [0, 1]])) == (0.9658, [[-0.1, 0.1], [0.075, -0.075], [-0.125, 0.125], [0.225, -0.225]]), \"test case failed: example 8\"\nassert cross_entropy(np.array([[1.0, 0.0]]), np.array([[1, 0]])) == (0.0, [[0.0, 0.0]]), \"test case failed: example 9\"\nassert cross_entropy(np.array([[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.2, 0.2, 0.6]]), np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])) == (0.646, [[0.0333, -0.0667, 0.0333], [-0.2333, 0.1333, 0.1], [0.0667, 0.0667, -0.1333]]), \"test case failed: example 10\""}
{"task_id": 163, "completion_id": 0, "passed": false, "result": "failed: name 'elastic_net_regression' is not defined", "solution": "import numpy as np\nimport numpy as np\nassert elastic_net_regression([0, 1, 2, 3], [3, 5, 7, 9], 1, 0.0, 0.0, 0.01, 1000) == [3.0, 5.0, 7.0, 9.0], \"Test case 1 failed\"\nassert elastic_net_regression([0, 1, 2], [1, 3, 7], 2, 0.0, 0.0, 0.01, 1000) == [1.0, 3.0, 7.0], \"Test case 2 failed\"\nassert elastic_net_regression([-1, 0, 1], [1, 0, 1], 2, 0.0, 0.0, 0.01, 1000) == [1.0, 0.0, 1.0], \"Test case 3 failed\"\nassert elastic_net_regression([1, 2, 3], [6, 11, 18], 2, 0.0, 0.0, 0.01, 1000) == [6.0, 11.0, 18.0], \"Test case 4 failed\"\nassert elastic_net_regression([0.5, 1.5, 2.5], [1, 3, 5], 1, 0.0, 0.0, 0.01, 1000) == [1.0, 3.0, 5.0], \"Test case 5 failed\"\nassert elastic_net_regression([-1, 0, 1], [-1, 0, 1], 1, 0.0, 0.0, 0.01, 1000) == [-1.0, 0.0, 1.0], \"Test case 6 failed\"\nassert elastic_net_regression([0, 1, 2, 3], [5, 6, 7, 8], 1, 1.0, 0.0, 0.01, 1000) == [5.25, 6.0833, 6.9167, 7.75], \"Test case 7 failed\"\nassert elastic_net_regression([0, 1, 2], [1, 2, 3], 1, 0.5, 0.0, 0.01, 1000) == [1.2, 2.0, 2.8], \"Test case 8 failed\"\nassert elastic_net_regression([1, 3, 5], [2, 4, 6], 1, 2.0, 0.0, 0.01, 1000) == [2.4, 4.0, 5.6], \"Test case 9 failed\"\nassert elastic_net_regression([0, 1, 2], [5, 5, 5], 1, 0.0, 0.0, 0.01, 1000) == [5.0, 5.0, 5.0], \"Test case 10 failed\""}
{"task_id": 166, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int=1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(len(X_train)):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n    predictions = []\n    for x in X_test:\n        s = np.dot(w, x) + b\n        if s >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions\nassert perceptron_predict(np.array([[1], [2], [-2], [-1]]), np.array([1, 1, -1, -1]), np.array([[0.5], [-1.5]])) == [1, -1], \"test case 1 failed\"\nassert perceptron_predict(np.array([[2, 2], [1, 1], [-2, -2], [-1, -1]]), np.array([1, 1, -1, -1]), np.array([[3, 3], [-3, -3]])) == [1, -1], \"test case 2 failed\"\nassert perceptron_predict(np.array([[2, 1], [2, 2], [-1, 1], [-1, -1]]), np.array([1, 1, -1, -1]), np.array([[1, 0], [-3, 2]])) == [1, -1], \"test case 3 failed\"\nassert perceptron_predict(np.array([[3, 1], [2, 1], [-3, -1], [-2, -1]]), np.array([1, 1, -1, -1]), np.array([[5, 2], [-4, -2]])) == [1, -1], \"test case 4 failed\"\nassert perceptron_predict(np.array([[1, 2, 0], [1, 1, 0], [-1, -2, 0], [-1, -1, 0]]), np.array([1, 1, -1, -1]), np.array([[2, 3, 0], [-2, -3, 0]])) == [1, -1], \"test case 5 failed\"\nassert perceptron_predict(np.array([[1, 0], [1, 1], [-1, 0], [-1, -1]]), np.array([1, 1, -1, -1]), np.array([[0.5, 0.5], [-0.5, -0.1]])) == [1, -1], \"test case 6 failed\"\nassert perceptron_predict(np.array([[2], [3], [-1], [-2]]), np.array([1, 1, -1, -1]), np.array([[1], [-1]])) == [1, -1], \"test case 7 failed\"\nassert perceptron_predict(np.array([[4, 4], [1, 2], [-3, -3], [-2, -4]]), np.array([1, 1, -1, -1]), np.array([[6, 5], [-3, -2]])) == [1, -1], \"test case 8 failed\"\nassert perceptron_predict(np.array([[2, 2, 2], [1, 1, 1], [-2, -2, -2], [-1, -1, -1]]), np.array([1, 1, -1, -1]), np.array([[3, 3, 3], [-2, -2, -2]])) == [1, -1], \"test case 9 failed\"\nassert perceptron_predict(np.array([[0, 2], [0, 1], [0, -2], [0, -1]]), np.array([1, 1, -1, -1]), np.array([[0, 3], [0, -3]])) == [1, -1], \"test case 10 failed\""}
{"task_id": 172, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    z_np = np.asarray(z)\n    grad = 1 - np.tanh(z_np) ** 2\n    grad_rounded = np.round(grad, 4)\n    if z_np.ndim == 0:\n        return grad_rounded.item()\n    else:\n        return grad_rounded.tolist()\nassert tanh_grad(0) == 1.0, \"test case failed: tanh_grad(0)\"\nassert tanh_grad(1) == 0.42, \"test case failed: tanh_grad(1)\"\nassert tanh_grad(-1) == 0.42, \"test case failed: tanh_grad(-1)\"\nassert tanh_grad(2) == 0.0707, \"test case failed: tanh_grad(2)\"\nassert tanh_grad(-2) == 0.0707, \"test case failed: tanh_grad(-2)\"\nassert tanh_grad(0.5) == 0.7864, \"test case failed: tanh_grad(0.5)\"\nassert tanh_grad([0, 1, -1, 2]) == [1.0, 0.42, 0.42, 0.0707], \"test case failed: tanh_grad([0, 1, -1, 2])\"\nassert tanh_grad(np.array([4])) == [0.0013], \"test case failed: tanh_grad(np.array([4]))\"\nassert tanh_grad(np.array([-4])) == [0.0013], \"test case failed: tanh_grad(np.array([-4]))\"\nassert tanh_grad(np.array([0.123])) == [0.985], \"test case failed: tanh_grad(np.array([0.123]))\""}
{"task_id": 173, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for (c, t) in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\nassert alphabetical_distance(\"HELLO\", \"WORLD\") == 42, \"Test 1 failed: alphabetical_distance('HELLO', 'WORLD')\"\nassert alphabetical_distance(\"ABC\", \"ABC\") == 0, \"Test 2 failed: alphabetical_distance('ABC', 'ABC')\"\nassert alphabetical_distance(\"aaaa\", \"bbbb\") == 4, \"Test 3 failed: alphabetical_distance('aaaa', 'bbbb')\"\nassert alphabetical_distance(\"abcd\", \"dcba\") == 8, \"Test 4 failed: alphabetical_distance('abcd', 'dcba')\"\nassert alphabetical_distance(\"HI\", \"HI\") == 0, \"Test 5 failed: alphabetical_distance('HI', 'HI')\"\nassert alphabetical_distance(\"AZ\", \"ZA\") == 50, \"Test 6 failed: alphabetical_distance('AZ', 'ZA')\"\nassert alphabetical_distance(\"\", \"\") == 0, \"Test 7 failed: alphabetical_distance('', '')\"\nassert alphabetical_distance(\"Genetic\", \"Algorithm\") == -1, \"Test 8 failed: alphabetical_distance('Genetic', 'Algorithm')\"\nassert alphabetical_distance(\"2023\", \"2024\") == 1, \"Test 9 failed: alphabetical_distance('2023', '2024')\"\nassert alphabetical_distance(\"Python3\", \"Python3\") == 0, \"Test 10 failed: alphabetical_distance('Python3', 'Python3')\""}
{"task_id": 175, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to the nearest 4th decimal place.\n    \"\"\"\n    weights_array = np.array(weights)\n    sum_squares = np.sum(weights_array ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)\nassert l2_penalty([1, 2, 3], 1) == 14.0, \"failed: l2_penalty([1, 2, 3], 1)\"\nassert l2_penalty(np.array([1, -2, 3]), 0.5) == 7.0, \"failed: l2_penalty(np.array([1,-2,3]),0.5)\"\nassert l2_penalty([], 2) == 0.0, \"failed: l2_penalty([],2)\"\nassert l2_penalty([0, 0, 0], 1.5) == 0.0, \"failed: l2_penalty([0,0,0],1.5)\"\nassert l2_penalty([1.1111, 2.2222], 0.3) == round(0.3 * ((1.1111**2) + (2.2222**2)), 4), \"failed: l2_penalty([1.1111,2.2222],0.3)\"\nassert l2_penalty(np.array([5]), 0.1) == 2.5, \"failed: l2_penalty([5],0.1)\"\nassert l2_penalty(np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]), 0.05) == round(0.05 * np.sum(np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4]) ** 2), 4), \"failed: mixed range\"\nassert l2_penalty([10, 20, 30], 0.0) == 0.0, \"failed: zero C\"\nassert l2_penalty([1e-3, -1e-3], 1) == round(((1e-3)**2 + (1e-3)**2), 4), \"failed: small weights\"\nassert l2_penalty([3.1415, 2.7182], 0.25) == round(0.25 * ((3.1415**2) + (2.7182**2)), 4), \"failed: l2_penalty([pi,e],0.25)\""}
{"task_id": 179, "completion_id": 0, "passed": true, "result": "passed", "solution": "import re\nimport string\nimport re\nimport string\ndef tokenize_chars(line: str, lowercase: bool=True, filter_punctuation: bool=True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    if lowercase:\n        line = line.lower()\n    if filter_punctuation:\n        pattern = '[{}]'.format(re.escape(string.punctuation))\n        line = re.sub(pattern, '', line)\n    line = re.sub(' {2,}', ' ', line)\n    line = line.strip()\n    if not line:\n        return []\n    return list(line)\nassert tokenize_chars(\"Hello, World!\") == ['h','e','l','l','o',' ','w','o','r','l','d'], \"failed: default params\"\nassert tokenize_chars(\"Hello, World!\", lowercase=False, filter_punctuation=True) == ['H','e','l','l','o',' ','W','o','r','l','d'], \"failed: keep case\"\nassert tokenize_chars(\"Hello, World!\", lowercase=False, filter_punctuation=False) == ['H','e','l','l','o',',',' ','W','o','r','l','d','!'], \"failed: keep case & punctuation\"\nassert tokenize_chars(\"  Hi   there  \") == ['h','i',' ','t','h','e','r','e'], \"failed: extra spaces\"\nassert tokenize_chars(\"!!!\") == [], \"failed: only punctuation removed\"\nassert tokenize_chars(\"!!!\", filter_punctuation=False) == ['!','!','!'], \"failed: keep punctuation only\"\nassert tokenize_chars(\"\") == [], \"failed: empty string\"\nassert tokenize_chars(None) == [], \"failed: None input\"\nassert tokenize_chars(\"A.B,C\", filter_punctuation=True) == ['a','b','c'], \"failed: punctuation removal with dots and commas\"\nassert tokenize_chars(\"MixED CaSe\") == ['m','i','x','e','d',' ','c','a','s','e'], \"failed: lowercasing mixed case\""}
{"task_id": 183, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError('Weight shape must be 2D or 4D.')\n    limit = np.sqrt(6.0 / fan_in)\n    weights = np.random.rand(*weight_shape) * (2 * limit) - limit\n    return weights\nw = he_uniform((2, 3)); assert w.shape == (2, 3) and np.all(w >= -np.sqrt(6/2)) and np.all(w <= np.sqrt(6/2)), \"test case failed: he_uniform((2, 3))\"\nw = he_uniform((5, 10)); assert w.shape == (5, 10) and np.all(w >= -np.sqrt(6/5)) and np.all(w <= np.sqrt(6/5)), \"test case failed: he_uniform((5, 10))\"\nw = he_uniform((3, 3, 1, 32)); assert w.shape == (3, 3, 1, 32) and np.all(w >= -np.sqrt(6/9)) and np.all(w <= np.sqrt(6/9)), \"test case failed: he_uniform((3, 3, 1, 32))\"\nw = he_uniform((5, 5, 16, 32)); assert w.shape == (5, 5, 16, 32) and np.all(w >= -np.sqrt(6/400)) and np.all(w <= np.sqrt(6/400)), \"test case failed: he_uniform((5, 5, 16, 32))\"\nw = he_uniform((1, 1, 64, 128)); assert w.shape == (1, 1, 64, 128) and np.all(w >= -np.sqrt(6/64)) and np.all(w <= np.sqrt(6/64)), \"test case failed: he_uniform((1, 1, 64, 128))\"\nw = he_uniform((10, 1)); assert w.shape == (10, 1) and np.all(w >= -np.sqrt(6/10)) and np.all(w <= np.sqrt(6/10)), \"test case failed: he_uniform((10, 1))\"\nw = he_uniform((7, 7, 3, 64)); assert w.shape == (7, 7, 3, 64) and np.all(w >= -np.sqrt(6/147)) and np.all(w <= np.sqrt(6/147)), \"test case failed: he_uniform((7, 7, 3, 64))\"\nw = he_uniform((128, 256)); assert w.shape == (128, 256) and np.all(w >= -np.sqrt(6/128)) and np.all(w <= np.sqrt(6/128)), \"test case failed: he_uniform((128, 256))\"\nw = he_uniform((4, 4)); assert w.shape == (4, 4) and np.all(w >= -np.sqrt(6/4)) and np.all(w <= np.sqrt(6/4)), \"test case failed: he_uniform((4, 4))\"\nw = he_uniform((3, 3, 3, 3)); assert w.shape == (3, 3, 3, 3) and np.all(w >= -np.sqrt(6/27)) and np.all(w <= np.sqrt(6/27)), \"test case failed: he_uniform((3, 3, 3, 3))\""}
{"task_id": 185, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError('Got an empty matrix.')\n    if X.ndim == 0:\n        return (1, 1)\n    elif X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, n_features)\nassert dataset_shape([[1, 2], [3, 4]]) == (2, 2), \"test case failed: dataset_shape([[1, 2], [3, 4]])\"\nassert dataset_shape([1, 2, 3, 4]) == (1, 4), \"test case failed: dataset_shape([1, 2, 3, 4])\"\nassert dataset_shape(np.array([[1, 2, 3], [4, 5, 6]])) == (2, 3), \"test case failed: dataset_shape(np.array([[1, 2, 3], [4, 5, 6]]))\"\nassert dataset_shape(np.zeros((3, 4, 5))) == (3, 20), \"test case failed: dataset_shape(np.zeros((3, 4, 5)))\"\nassert dataset_shape([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) == (2, 4), \"test case failed: nested 3-D list\"\nassert dataset_shape(np.ones((1,))) == (1, 1), \"test case failed: dataset_shape(np.ones((1,)))\"\nassert dataset_shape(np.arange(12).reshape(3, 2, 2)) == (3, 4), \"test case failed: dataset_shape(np.arange(12).reshape(3, 2, 2))\"\nassert dataset_shape([[7]]) == (1, 1), \"test case failed: dataset_shape([[7]])\"\nassert dataset_shape(np.array([[0, 1]])) == (1, 2), \"test case failed: dataset_shape(np.array([[0, 1]]))\"\nassert dataset_shape(np.random.rand(5, 3)) == (5, 3), \"test case failed: dataset_shape(np.random.rand(5, 3))\""}
{"task_id": 186, "completion_id": 0, "passed": false, "result": "failed: name 'baseline_mean_predict' is not defined", "solution": "import numpy as np\n\nassert baseline_mean_predict(np.array([[1],[2],[3]]), np.array([2, 4, 6]), np.array([[4],[5]])) == [4.0, 4.0], \"test case failed: basic integer example\"\nassert baseline_mean_predict([[1],[2],[3],[4]], [1, 1, 1, 1], [[3],[4]]) == [1.0, 1.0], \"test case failed: identical targets\"\nassert baseline_mean_predict([[1],[2],[3]], [1.5, 2.5, 4.0], [[0],[1],[2]]) == [2.6667, 2.6667, 2.6667], \"test case failed: mean needs rounding\"\nassert baseline_mean_predict([], [1.1111, 2.2222, 3.3333], [[1],[2]]) == [2.2222, 2.2222], \"test case failed: 4-decimal precision targets\"\nassert baseline_mean_predict([[1],[2],[3]], [-1, -2, -3], [[10]]) == [-2.0], \"test case failed: negative targets\"\nassert baseline_mean_predict([[1],[2],[3]], [5, 6, 7], np.empty((0, 1))) == [], \"test case failed: empty X_test\"\nassert baseline_mean_predict([[42]], [10], [[4],[5],[6]]) == [10.0, 10.0, 10.0], \"test case failed: single training sample\"\nassert baseline_mean_predict([[1,2,3],[4,5,6]], [0, 10], [[7,8,9],[9,8,7]]) == [5.0, 5.0], \"test case failed: multi-feature X_test\"\nassert baseline_mean_predict([[1],[2]], [3, 5], [[9],[9],[9]]) == [4.0, 4.0, 4.0], \"test case failed: pure python lists\"\nassert baseline_mean_predict(np.random.rand(1000, 5), list(range(1,1001)), np.zeros((5,5))) == [500.5]*5, \"test case failed: large dataset\""}
{"task_id": 188, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    if min_epsilon >= max_epsilon or min_epsilon < 0 or decay_rate <= 0 or (step < 0):\n        return -1\n    exponent = -decay_rate * step\n    exp_term = math.exp(exponent)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * exp_term\n    epsilon = max(epsilon, min_epsilon)\n    return round(epsilon, 4)\nassert epsilon_decay(0.9, 0.1, 0.005, 0) == 0.9, \"failed: step 0 should return max_epsilon\"\nassert epsilon_decay(0.9, 0.1, 0.005, 100) == 0.5852, \"failed: step 100, \u03b5 should be 0.5852\"\nassert epsilon_decay(0.9, 0.1, 0.005, 200) == 0.3943, \"failed: step 200, \u03b5 should be 0.3943\"\nassert epsilon_decay(0.9, 0.1, 0.005, 500) == 0.1657, \"failed: step 500, \u03b5 should be 0.1657\"\nassert epsilon_decay(0.9, 0.1, 0.005, 1000) == 0.1054, \"failed: step 1000, \u03b5 should be 0.1054\"\nassert epsilon_decay(0.9, 0.1, 0.005, 10000) == 0.1, \"failed: \u03b5 must never drop below \u03b5_min\"\nassert epsilon_decay(0.5, 0.5, 0.01, 10) == -1, \"failed: \u03b5_max equal to \u03b5_min should be invalid\"\nassert epsilon_decay(0.5, 0.6, 0.01, 10) == -1, \"failed: \u03b5_max smaller than \u03b5_min should be invalid\"\nassert epsilon_decay(0.9, 0.1, -0.01, 10) == -1, \"failed: negative decay_rate should be invalid\"\nassert epsilon_decay(0.9, 0.1, 0.005, -5) == -1, \"failed: negative step should be invalid\""}
{"task_id": 193, "completion_id": 0, "passed": false, "result": "failed: name 'softsign' is not defined", "solution": "import numpy as np\n\nassert softsign(3) == 0.75, \"test case failed: softsign(3)\"\nassert softsign(-4) == -0.8, \"test case failed: softsign(-4)\"\nassert softsign(0) == 0.0, \"test case failed: softsign(0)\"\nassert softsign([1, -2, 0]) == [0.5, -0.6667, 0.0], \"test case failed: softsign([1, -2, 0])\"\nassert softsign(np.array([10, -10])) == [0.9091, -0.9091], \"test case failed: softsign(np.array([10, -10]))\"\nassert softsign([]) == [], \"test case failed: softsign([])\"\nassert softsign(np.array([])).__eq__([]), \"test case failed: softsign(np.array([]))\"\nassert softsign([0.5]) == [0.3333], \"test case failed: softsign([0.5])\"\nassert softsign(np.array([1000])) == [0.999], \"test case failed: softsign(np.array([1000]))\"\nassert softsign([-1, -2, -3]) == [-0.5, -0.6667, -0.75], \"test case failed: softsign([-1, -2, -3])\""}
{"task_id": 199, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\nassert split(np.array([2,3,1,5,4]), np.array([0,1,0,1,0]), 3) == ([0,0],[1,1,0]), \"failed: basic split\"\nassert split(np.array([1,2,3,4]), np.array([10,20,30,40]), 4) == ([10,20,30],[40]), \"failed: threshold at max element\"\nassert split(np.array([1,2,3,4]), np.array([10,20,30,40]), 0) == ([],[10,20,30,40]), \"failed: threshold below min\"\nassert split(np.array([1,1,1,1]), np.array([7,8,9,10]), 1) == ([],[7,8,9,10]), \"failed: all equal to threshold\"\nassert split(np.array([1,1,1,1]), np.array([7,8,9,10]), 2) == ([7,8,9,10],[]), \"failed: all below threshold\"\nassert split(np.array([-5,-2,0,3]), np.array([5,4,3,2]), 0) == ([5,4],[3,2]), \"failed: negative values\"\nassert split(np.array([0.1,0.2,0.3]), np.array([1,2,3]), 0.25) == ([1,2],[3]), \"failed: float threshold\"\nassert split(np.array([5]), np.array([42]), 5) == ([],[42]), \"failed: single element equal threshold\"\nassert split(np.array([5]), np.array([42]), 10) == ([42],[]), \"failed: single element below threshold\"\nassert split(np.array([5]), np.array([42]), 0) == ([],[42]), \"failed: single element above threshold\""}
{"task_id": 200, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    weights_array = np.array(weights)\n    if weights_array.size == 0:\n        return 0.0\n    sum_abs = np.sum(np.abs(weights_array))\n    l1_penalty = C * sum_abs\n    return round(l1_penalty, 4)\nassert compute_l1_penalty([1.5, -2.0, 0.0, 3.2], 0.1) == 0.67, \"failed on simple 1-D list\"\nassert compute_l1_penalty([[1, -2], [3, -4]], 0.25) == 2.5, \"failed on 2-D list\"\nassert compute_l1_penalty([], 1.0) == 0.0, \"failed on empty list\"\nassert compute_l1_penalty([0, 0, 0], 10.0) == 0.0, \"failed on all zeros\"\nassert compute_l1_penalty([-3.3, 4.4], 1.0) == 7.7, \"failed on negative values\"\nassert compute_l1_penalty(np.array([0.3333, -0.6666]), 3) == 2.9997, \"failed on numpy 1-D array\"\nassert compute_l1_penalty(np.array([[1.1, -1.1], [2.2, -2.2]]), 0.5) == 3.3, \"failed on numpy 2-D array\"\nassert compute_l1_penalty([1]*1000, 0.001) == 1.0, \"failed on large list\"\nassert compute_l1_penalty([[[]]], 5) == 0.0, \"failed on deeply nested empty list\"\nassert compute_l1_penalty([[0.12345]], 2) == 0.2469, \"failed on rounding rule\""}
{"task_id": 205, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen = set()\n    result = []\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                result.append(env_id)\n    return result\nassert get_gym_environs([]) == [], \"test case failed: empty list\"\nassert get_gym_environs([{\"id\": \"CartPole-v1\"}]) == [\"CartPole-v1\"], \"test case failed: single element\"\nassert get_gym_environs([{\"id\": \"A\"}, {\"id\": \"B\"}, {\"id\": \"A\"}]) == [\"A\", \"B\"], \"test case failed: duplicates must be removed\"\nassert get_gym_environs([{\"name\": \"NoId\"}, {\"id\": \"X\"}]) == [\"X\"], \"test case failed: items without 'id' key are ignored\"\nassert get_gym_environs([{\"id\": \"X\"}, 42, \"string\", (1, 2), {\"id\": \"Y\"}]) == [\"X\", \"Y\"], \"test case failed: non-dict elements should be ignored\"\nassert get_gym_environs([{\"id\": \"D\"}, {\"id\": \"C\"}, {\"id\": \"B\"}, {\"id\": \"A\"}, {\"id\": \"C\"}, {\"id\": \"D\"}]) == [\"D\", \"C\", \"B\", \"A\"], \"test case failed: preserve order of first occurrence\"\nassert get_gym_environs([{\"id\": 1}, {\"id\": 2}, {\"id\": 1}]) == [1, 2], \"test case failed: ids can be non-string\"\nassert get_gym_environs([{\"id\": \"same\"}, {\"id\": \"same\"}, {\"id\": \"same\"}]) == [\"same\"], \"test case failed: all ids identical\"\nassert get_gym_environs([{\"id\": \"X\"}, {\"name\": \"n/a\"}, {\"id\": \"Y\"}, {\"foo\": \"bar\"}, {\"id\": \"X\"}]) == [\"X\", \"Y\"], \"test case failed: mixed valid and invalid elements\"\nassert get_gym_environs([{\"id\": \"first\"}, {\"ignore\": \"me\"}, {\"id\": \"second\"}, {\"id\": \"third\"}, {\"id\": \"second\"}]) == [\"first\", \"second\", \"third\"], \"test case failed: general behaviour\""}
{"task_id": 206, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    try:\n        actual_arr = np.array(actual)\n        predicted_arr = np.array(predicted)\n    except:\n        return -1\n    if actual_arr.ndim != 1 or predicted_arr.ndim != 1:\n        return -1\n    if len(actual_arr) != len(predicted_arr):\n        return -1\n    if len(actual_arr) == 0:\n        return -1\n    try:\n        mae = np.mean(np.abs(actual_arr - predicted_arr))\n    except:\n        return -1\n    return round(mae, 4)\nassert absolute_error([1, 2, 3], [1, 2, 3]) == 0.0, \"Test case failed: perfect prediction\"\nassert absolute_error([3, -0.5, 2, 7], [2.5, 0.0, 2, 8]) == 0.5, \"Test case failed: sample data\"\nassert absolute_error([1, 2, 3], [2, 2, 2]) == 0.6667, \"Test case failed: rounding check\"\nassert absolute_error([1, 2], [1]) == -1, \"Test case failed: length mismatch\"\nassert absolute_error([-1, -2, -3], [-1.5, -2.5, -2.5]) == 0.5, \"Test case failed: negative values\"\nassert absolute_error(list(range(10)), [0] * 10) == 4.5, \"Test case failed: larger range\"\nassert absolute_error([5], [3]) == 2.0, \"Test case failed: single element\"\nassert absolute_error(np.array([1, 2, 3, 4]), [2, 3, 4, 5]) == 1.0, \"Test case failed: mixed input types\"\nassert absolute_error([1, 1, 1], [0.9999, 1.0001, 1.0002]) == 0.0001, \"Test case failed: small decimals\"\nassert absolute_error([10, 20, 30, 40], [8, 25, 28, 41]) == 2.5, \"Test case failed: general case\""}
{"task_id": 211, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1.0 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    total = len(actual)\n    mismatches = sum((1 for (a, p) in zip(actual, predicted) if a != p))\n    error = mismatches / total\n    acc = 1 - error\n    return round(acc, 4)\nassert accuracy([1, 0, 1, 1], [1, 1, 1, 0]) == 0.5, \"failed: accuracy([1, 0, 1, 1], [1, 1, 1, 0])\"\nassert accuracy(['cat', 'dog', 'bird'], ['cat', 'dog', 'bird']) == 1.0, \"failed: perfect prediction\"\nassert accuracy([1, 1, 1], [0, 0, 0]) == 0.0, \"failed: completely wrong prediction\"\nassert accuracy([1, 2], [1]) == -1, \"failed: different lengths should give -1\"\nassert accuracy([], []) == -1, \"failed: empty lists should give -1\"\nassert accuracy([True, False, True, False], [True, False, False, False]) == 0.75, \"failed: boolean labels\"\nassert accuracy(['a', 'b', 'c', 'd'], ['a', 'x', 'c', 'y']) == 0.5, \"failed: string labels\"\nassert accuracy([0, 1, 2, 3, 4], [0, 2, 2, 3, 5]) == 0.6, \"failed: mixed correct/incorrect\"\nassert accuracy([1, 2, 3, 4, 5], [5, 4, 3, 2, 1]) == 0.2, \"failed: reversed list\"\nassert accuracy([0], [1]) == 0.0, \"failed: single element wrong\""}
{"task_id": 214, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function must return -1 if the two input sequences do not share the\n    same length. Otherwise, it should return the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays have to be supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    a = np.array(actual)\n    p = np.array(predicted)\n    squared_diff = (a - p) ** 2\n    sum_squared = np.sum(squared_diff)\n    mse = sum_squared / n\n    return np.round(mse, 4)\nassert mean_squared_error([1, 2, 3], [1, 2, 3]) == 0.0, \"failed: identical lists should return 0.0\"\nassert mean_squared_error([1, 2, 3], [1, 2, 5]) == 1.3333, \"failed: mean_squared_error([1,2,3],[1,2,5])\"\nassert mean_squared_error(np.array([1, 2]), np.array([2, 3])) == 1.0, \"failed: numpy array input\"\nassert mean_squared_error([-1, -2, -3], [1, 2, 3]) == 18.6667, \"failed: negative values\"\nassert mean_squared_error([2.5, 0.0, 2.1], [3.0, -0.5, 2.0]) == 0.17, \"failed: float inputs\"\nassert mean_squared_error([7], [5]) == 4.0, \"failed: single element input\"\nassert mean_squared_error([1, 2], [1]) == -1, \"failed: unequal length should return -1\"\nassert mean_squared_error([100, 200], [110, 190]) == 100.0, \"failed: large numbers\"\nassert mean_squared_error(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0.0, \"failed: zeros input\"\nassert mean_squared_error([3, -3, 5, -5], [0, 0, 0, 0]) == 17.0, \"failed: mixed signs input\""}
{"task_id": 217, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    return (gradient_list, hessian_list, probability_list)\nassert logistic_components([1, -1], [0.5, -0.5]) == ([0.377541, -0.377541], [0.235004, 0.235004], [0.622459, 0.377541]), \"failed on ([1, -1], [0.5, -0.5])\"\nassert logistic_components([1, -1], [10, -10]) == ([0.000045, -0.000045], [0.000045, 0.000045], [0.999955, 0.000045]), \"failed on large magnitude scores\"\nassert logistic_components([1, 1, 1], [0, 0, 0]) == ([0.5, 0.5, 0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with positive labels\"\nassert logistic_components([-1, -1, -1], [0, 0, 0]) == ([-0.5, -0.5, -0.5], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]), \"failed on zeros with negative labels\"\nassert logistic_components([1], [-2]) == ([0.880797], [0.104994], [0.119203]), \"failed on single sample (1, -2)\"\nassert logistic_components([-1], [2]) == ([-0.880797], [0.104994], [0.880797]), \"failed on single sample (-1, 2)\"\nassert logistic_components([1, -1, 1, -1], [1, 1, -1, -1]) == ([0.268941, -0.731059, 0.731059, -0.268941], [0.196612, 0.196612, 0.196612, 0.196612], [0.731059, 0.731059, 0.268941, 0.268941]), \"failed on mixed signs\"\nassert logistic_components([1], [0]) == ([0.5], [0.25], [0.5]), \"failed on ([1], [0])\"\nassert logistic_components([1, -1], [5, -5]) == ([0.006693, -0.006693], [0.006648, 0.006648], [0.993307, 0.006693]), \"failed on moderate magnitude scores\"\nassert logistic_components([-1], [3]) == ([-0.952574], [0.045177], [0.952574]), \"failed on (-1, 3)\""}
{"task_id": 220, "completion_id": 0, "passed": false, "result": "failed: name 'estimate_lda_params' is not defined", "solution": "import numpy as np\n\nassert estimate_lda_params(np.array([[2, 1], [3, 4], [5, 0]]), np.array([[3, 2], [1, 4]]), 0.1, 0.5) == ([[0.2174, 0.2308], [0.3043, 0.6923], [0.4783, 0.0769]], [[0.5962, 0.4038], [0.2115, 0.7885]]), \"test case failed: asymmetric example\"\nassert estimate_lda_params(np.zeros((2,2)), np.zeros((1,2)), 1, 1) == ([[0.5,0.5],[0.5,0.5]], [[0.5,0.5]]), \"test case failed: all zero counts\"\nassert estimate_lda_params(np.array([[3],[7]]), np.array([[10],[0]]), 0.1, 0.1) == ([[0.3039],[0.6961]], [[1.0],[1.0]]), \"test case failed: single topic\"\nassert estimate_lda_params(np.ones((2,2)), 2*np.ones((2,2)), 0.1, 0.1) == ([[0.5,0.5],[0.5,0.5]], [[0.5,0.5],[0.5,0.5]]), \"test case failed: symmetric counts\"\nassert estimate_lda_params(2*np.ones((3,3)), 3*np.ones((1,3)), 0.5, 0.5) == ([[0.3333,0.3333,0.3333],[0.3333,0.3333,0.3333],[0.3333,0.3333,0.3333]], [[0.3333,0.3333,0.3333]]), \"test case failed: three topics uniform\"\nassert estimate_lda_params(np.array([[4,0],[1,3]]), np.array([[2,1],[0,3]]), 0.2, 0.2) == ([[0.7778,0.0588],[0.2222,0.9412]], [[0.6471,0.3529],[0.0588,0.9412]]), \"test case failed: skewed word counts\"\nassert estimate_lda_params(np.array([[0,1],[2,3],[4,5]]), np.array([[6,9]]), 1.0, 1.0) == ([[0.1111,0.1667],[0.3333,0.3333],[0.5556,0.5]], [[0.4118,0.5882]]), \"test case failed: single document varying topics\"\nassert estimate_lda_params(np.array([[10,0],[0,10]]), np.array([[4,6],[7,3]]), 10, 10) == ([[0.6667,0.3333],[0.3333,0.6667]], [[0.4667,0.5333],[0.5667,0.4333]]), \"test case failed: large priors\"\nassert estimate_lda_params(np.array([[5,2,0]]), np.array([[1,2,3],[4,5,6]]), 0.3, 0.3) == ([[1.0,1.0,1.0]], [[0.1884,0.3333,0.4783],[0.2704,0.3333,0.3962]]), \"test case failed: single vocabulary word\"\nassert estimate_lda_params(np.zeros((2,4)), np.zeros((1,4)), 0.5, 0.5) == ([[0.5,0.5,0.5,0.5],[0.5,0.5,0.5,0.5]], [[0.25,0.25,0.25,0.25]]), \"test case failed: four topics zero counts\""}
{"task_id": 224, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z\nassert leaky_relu([-2, -1, 0, 1, 2]) == [-0.02, -0.01, 0.0, 1.0, 2.0], \"failed on default slope, 1-D list\"\nassert leaky_relu(np.array([-5.0, 5.0]), 0.2) == [-1.0, 5.0], \"failed on numpy input, custom slope\"\nassert leaky_relu(np.array([[[-1]]]), 0.5) == [[[-0.5]]], \"failed on 3-D array\"\nassert leaky_relu(0) == 0.0, \"failed on scalar zero\"\nassert leaky_relu(-4, 0.25) == -1.0, \"failed on scalar negative\"\nassert leaky_relu(3.3) == 3.3, \"failed on scalar positive\"\nassert leaky_relu([[0]]) == [[0.0]], \"failed on zero inside nested list\""}
{"task_id": 225, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef hz2mel(hz, formula: str='htk') -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    if formula not in ['htk', 'slaney']:\n        raise AssertionError(\"formula must be 'htk' or 'slaney'\")\n    hz_array = np.asarray(hz)\n    if formula == 'htk':\n        mel = 2595 * np.log10(1 + hz_array / 700)\n    else:\n        mel = 1127 * np.log(1 + hz_array / 700)\n    mel_rounded = np.round(mel, decimals=4)\n    if hz_array.shape == ():\n        return [mel_rounded.item()]\n    else:\n        return mel_rounded.tolist()\nassert hz2mel(np.array([0.0]), \"htk\") == [0.0], \"failed: hz2mel([0], 'htk')\"\nassert hz2mel([6300], \"htk\") == [2595.0], \"failed: hz2mel([6300], 'htk')\"\nassert hz2mel([69300], \"htk\") == [5190.0], \"failed: hz2mel([69300], 'htk')\"\nassert hz2mel([0.0, 6300, 69300], \"htk\") == [0.0, 2595.0, 5190.0], \"failed: hz2mel([...], 'htk')\"\nassert hz2mel(np.array([0.0]), \"slaney\") == [0.0], \"failed: hz2mel([0], 'slaney')\"\nassert hz2mel([1202.7972799213314], \"slaney\") == [1127.0], \"failed: hz2mel([1202.7972799], 'slaney')\"\nassert hz2mel([4472.339268], \"slaney\") == [2254.0], \"failed: hz2mel([4472.339268], 'slaney')\"\nassert hz2mel([0.0, 1202.7972799213314, 4472.339268], \"slaney\") == [0.0, 1127.0, 2254.0], \"failed: hz2mel([...], 'slaney')\"\nassert hz2mel([[0, 6300],[6300, 0]], \"htk\") == [[0.0, 2595.0],[2595.0, 0.0]], \"failed: hz2mel(2D, 'htk')\"\nassert hz2mel([[0, 1202.7972799213314],[4472.339268, 0]], \"slaney\") == [[0.0, 1127.0],[2254.0, 0.0]], \"failed: hz2mel(2D, 'slaney')\""}
{"task_id": 228, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None=None) -> list[list[float]]:\n    \"\"\"Compute the pairwise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If *Y* is ``None`` the function should compute the distances inside *X* itself.\n    All distances must be rounded to four decimal places and the function must\n    return a plain Python nested list (use ``tolist()`` if needed).\n\n    Args:\n        X: NumPy array with shape (N, C).\n        Y: NumPy array with shape (M, C) or ``None``.  Default is ``None``.\n\n    Returns:\n        Nested list representing the N \u00d7 M matrix of Euclidean distances.\n    \"\"\"\n    if Y is None:\n        Y = X\n    x_norms = np.sum(X ** 2, axis=1)\n    y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X @ Y.T\n    D_sq = x_norms[:, np.newaxis] + y_norms[np.newaxis, :] - 2 * dot_product\n    D_sq = np.maximum(D_sq, 0)\n    D = np.sqrt(D_sq)\n    D_rounded = np.round(D, 4)\n    return D_rounded.tolist()\nassert pairwise_l2_distances(np.array([[0, 0], [1, 0]]), np.array([[1, 0], [2, 0], [0, 0]])) == [[1.0, 2.0, 0.0], [0.0, 1.0, 1.0]], \"failed on X1, Y1\"\nassert pairwise_l2_distances(np.array([[0, 0]]), np.array([[0, 0]])) == [[0.0]], \"failed on zero-distance case\"\nassert pairwise_l2_distances(np.array([[0, 0], [3, 4], [6, 8]])) == [[0.0, 5.0, 10.0], [5.0, 0.0, 5.0], [10.0, 5.0, 0.0]], \"failed when Y is None\"\nassert pairwise_l2_distances(np.array([[1, 2, 3], [4, 5, 6]]), np.array([[7, 8, 9]])) == [[10.3923], [5.1962]], \"failed on 3-D vectors\"\nassert pairwise_l2_distances(np.array([[1, 2]])) == [[0.0]], \"failed on single vector (Y None)\"\nassert pairwise_l2_distances(np.ones((1, 100)), np.zeros((1, 100))) == [[10.0]], \"failed on 100-D vectors\"\nassert pairwise_l2_distances(np.array([[-1, -1]]), np.array([[1, 1]])) == [[2.8284]], \"failed on negative coordinates\"\nassert pairwise_l2_distances(np.array([[0, 1], [1, 1]]), np.array([[1, 0]])) == [[1.4142], [1.0]], \"failed on mixed distances\"\nassert pairwise_l2_distances(np.array([[1, 2], [2, 1], [3, 3]])) == [[0.0, 1.4142, 2.2361], [1.4142, 0.0, 2.2361], [2.2361, 2.2361, 0.0]], \"failed on 3\u00d73 symmetry\"\nassert pairwise_l2_distances(np.array([[2]]), np.array([[5]])) == [[3.0]], \"failed on 1-D feature space\""}
{"task_id": 231, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack.\n    \n    A natural blackjack is a hand with exactly two cards: one Ace (1) and one ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1\nassert is_natural([1, 10]) is True, \"failed on: [1, 10]\"\nassert is_natural([10, 1]) is True, \"failed on: [10, 1]\"\nassert is_natural([1, 5]) is False, \"failed on: [1, 5]\"\nassert is_natural([10, 10]) is False, \"failed on: [10, 10]\"\nassert is_natural([1, 1]) is False, \"failed on: [1, 1]\"\nassert is_natural([5, 10]) is False, \"failed on: [5, 10]\"\nassert is_natural([1, 9]) is False, \"failed on: [1, 9]\"\nassert is_natural([7, 1]) is False, \"failed on: [7, 1]\"\nassert is_natural([10, 11]) is False, \"failed on: [10, 11] (11 is not allowed, no Ace)\"\nassert is_natural([1, 10, 5]) is False, \"failed on: [1, 10, 5] (more than two cards)\""}
{"task_id": 232, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if x.ndim != 2:\n        return False\n    if not np.all(np.isin(x, [0, 1])):\n        return False\n    row_sums = x.sum(axis=1)\n    if not np.all(row_sums == 1):\n        return False\n    return True\nassert is_one_hot(np.array([[1, 0, 0], [0, 1, 0]])) == True, \"failed: basic valid matrix\"\nassert is_one_hot(np.array([[0, 1], [1, 0], [0, 1]])) == True, \"failed: rectangular valid matrix\"\nassert is_one_hot(np.array([[0, 1], [1, 1]])) == False, \"failed: row with two ones\"\nassert is_one_hot(np.array([[0, 2], [1, 0]])) == False, \"failed: value other than 0/1 present\"\nassert is_one_hot(np.array([[0, 0, 0], [0, 1, 0]])) == False, \"failed: row with no ones\"\nassert is_one_hot(np.array([1, 0, 0])) == False, \"failed: not a 2-D array\"\nassert is_one_hot(np.array([[True, False], [False, True]])) == True, \"failed: boolean dtype matrix\"\nassert is_one_hot(np.array([[False, False], [False, True]])) == False, \"failed: boolean matrix with invalid row\"\nassert is_one_hot(np.eye(5)) == True, \"failed: identity matrix\"\nassert is_one_hot(np.array([[0, 0], [0, 0]])) == False, \"failed: all zeros matrix\""}
{"task_id": 234, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef thompson_sampling_beta_binomial(successes: list[int], failures: list[int], prior_alpha: float=1.0, prior_beta: float=1.0, seed: int | None=None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\"\"\"\n    if len(successes) != len(failures) or len(successes) == 0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    K = len(successes)\n    samples = []\n    for i in range(K):\n        alpha = prior_alpha + successes[i]\n        beta = prior_beta + failures[i]\n        sample = np.random.beta(alpha, beta)\n        samples.append(sample)\n    selected_arm = np.argmax(samples)\n    return selected_arm\nassert thompson_sampling_beta_binomial([10,0],[0,10],seed=42)==0, \"test case failed: ([10,0],[0,10],seed=42)\"\nassert thompson_sampling_beta_binomial([0,10],[10,0],seed=42)==1, \"test case failed: ([0,10],[10,0],seed=42)\"\nassert thompson_sampling_beta_binomial([100,0,50],[0,100,30])==0, \"test case failed: ([100,0,50],[0,100,30])\"\nassert thompson_sampling_beta_binomial([5],[3])==0, \"test case failed: single arm ([5],[3])\"\nassert thompson_sampling_beta_binomial([],[])==-1, \"test case failed: empty input\"\nassert thompson_sampling_beta_binomial([1,2,3],[3,2,1],seed=7)==2, \"test case failed: ([1,2,3],[3,2,1],seed=7)\"\nassert thompson_sampling_beta_binomial([20,5],[5,20])==0, \"test case failed: ([20,5],[5,20])\"\nassert thompson_sampling_beta_binomial([0,1,0,1],[1,0,1,0],seed=123)==3, \"test case failed: ([0,1,0,1],[1,0,1,0],seed=123)\"\n"}
{"task_id": 235, "completion_id": 0, "passed": false, "result": "failed: name 'to_one_hot' is not defined", "solution": "import numpy as np\n\nassert to_one_hot(np.array([0, 1, 2])).tolist() == [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], \"Test case 1 failed: labels=[0,1,2]\"\nassert to_one_hot(np.array([2, 0, 1]), 4).tolist() == [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]], \"Test case 2 failed: labels=[2,0,1], n_classes=4\"\nassert to_one_hot(np.array([1, 1, 1, 1])).tolist() == [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]], \"Test case 3 failed: repeated label 1\"\nassert to_one_hot(np.array([3, 2])).tolist() == [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0]], \"Test case 4 failed: labels=[3,2]\"\nassert to_one_hot(np.array([0]), 1).tolist() == [[1.0]], \"Test case 5 failed: single label 0 with n_classes=1\"\nassert to_one_hot(np.array([0]), 3).tolist() == [[1.0, 0.0, 0.0]], \"Test case 6 failed: single label 0 with n_classes=3\"\nassert to_one_hot(np.array([2, 2, 0]), 3).tolist() == [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]], \"Test case 7 failed: labels=[2,2,0]\"\nassert to_one_hot(np.array([4, 1, 3]), 6).tolist() == [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]], \"Test case 8 failed: labels=[4,1,3], n_classes=6\"\nassert to_one_hot(np.array([5]), 8).tolist() == [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]], \"Test case 9 failed: labels=[5], n_classes=8\"\nassert to_one_hot(np.array([1, 0, 4, 2]), 5).tolist() == [[0.0, 1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0]], \"Test case 10 failed: mixed labels with n_classes=5\""}
{"task_id": 236, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef minibatch(X: np.ndarray, batchsize: int=256, shuffle: bool=True):\n    \"\"\"Create a generator that yields index mini-batches.\"\"\"\n    N = X.shape[0]\n    idx = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(idx)\n    n_batches = (N + batchsize - 1) // batchsize\n\n    def gen():\n        for i in range(0, N, batchsize):\n            yield idx[i:i + batchsize]\n    return (gen(), n_batches)\n# 1  --- basic sequential batching\\nX = np.arange(10).reshape(10, 1)\\ngen, n = minibatch(X, batchsize=4, shuffle=False)\\nassert n == 3, \"failed: n_batches should be 3 for 10 samples and batchsize 4\"\\nassert [b.tolist() for b in gen] == [[0,1,2,3],[4,5,6,7],[8,9]], \"failed: wrong indices for sequential batching\"\n# 2 --- full batch (batchsize == N)\\nX = np.arange(7).reshape(7,1)\\ngen, n = minibatch(X, batchsize=7, shuffle=False)\\nassert n == 1, \"failed: there should be 1 batch\"\\nassert [b.tolist() for b in gen] == [[0,1,2,3,4,5,6]], \"failed: indices do not cover full data set\"\n# 3 --- batchsize larger than N\\nX = np.arange(5).reshape(5,1)\\ngen, n = minibatch(X, batchsize=8, shuffle=False)\\nassert n == 1, \"failed: ceil(5/8) == 1\"\\nassert [b.tolist() for b in gen] == [[0,1,2,3,4]], \"failed: should return all indices in single batch\"\n# 4 --- batchsize = 1 (all singles)\\nX = np.arange(4).reshape(4,1)\\ngen, n = minibatch(X, batchsize=1, shuffle=False)\\nassert n == 4, \"failed: there should be 4 single-element batches\"\\nassert [int(b) for b in gen] == [0,1,2,3], \"failed: sequential single indices incorrect\"\n# 5 --- uneven last batch\\nX = np.arange(9).reshape(9,1)\\ngen, n = minibatch(X, batchsize=4, shuffle=False)\\nassert n == 3, \"failed: ceil(9/4) == 3\"\\nassert [b.tolist() for b in gen] == [[0,1,2,3],[4,5,6,7],[8]], \"failed: last batch should contain index 8 only\"\n# 6 --- empty data set\\nX = np.empty((0,2))\\ngen, n = minibatch(X, batchsize=3, shuffle=False)\\nassert n == 0, \"failed: zero samples must give zero batches\"\\nassert list(gen) == [], \"failed: generator should yield nothing for empty data set\"\n# 7 --- reproducible shuffle\\nnp.random.seed(123)\\nX = np.arange(10).reshape(10,1)\\ngen, n = minibatch(X, batchsize=3, shuffle=True)\\nshuffled = np.concatenate(list(gen))\\nnp.random.seed(123)\\nexpected = np.arange(10)\\nnp.random.shuffle(expected)\\nassert np.array_equal(shuffled, expected), \"failed: shuffle does not match NumPy's shuffle with same seed\"\n# 8 --- shuffled batches preserve sizes\\nnp.random.seed(0)\\nX = np.arange(12).reshape(12,1)\\ngen, n = minibatch(X, batchsize=5, shuffle=True)\\nlengths = [len(b) for b in gen]\\nassert lengths == [5,5,2], \"failed: batch sizes after shuffle should be 5,5,2\"\n# 9 --- generator is single-pass\\nX = np.arange(6).reshape(6,1)\\ngen, _ = minibatch(X, batchsize=2, shuffle=False)\\n_ = list(gen)\\nassert list(gen) == [], \"failed: generator should be exhausted after first iteration\"\n# 10 --- large dataset quick check\\nX = np.arange(1000).reshape(1000,1)\\nbs = 128\\n_, n = minibatch(X, batchsize=bs, shuffle=False)\\nassert n == int(np.ceil(1000/bs)), \"failed: incorrect number of batches for large data set\""}
{"task_id": 237, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\"\"\"\n    total = 0\n    aces = 0\n    for card in hand:\n        if card in ['J', 'Q', 'K']:\n            total += 10\n        elif card == 'A':\n            total += 11\n            aces += 1\n        else:\n            total += int(card)\n    while total > 21 and aces > 0:\n        total -= 10\n        aces -= 1\n    return total if total <= 21 else 0\nassert score(['A','K']) == 21, \"Test case 1 failed: ['A','K']\"\nassert score(['10','8','5']) == 0, \"Test case 2 failed: ['10','8','5']\"\nassert score(['A','9','A']) == 21, \"Test case 3 failed: ['A','9','A']\"\nassert score(['5','3','7','6']) == 21, \"Test case 4 failed: ['5','3','7','6']\"\nassert score(['J','Q']) == 20, \"Test case 5 failed: ['J','Q']\"\nassert score(['A','A','9']) == 21, \"Test case 6 failed: ['A','A','9']\"\nassert score(['A','A','A','7']) == 20, \"Test case 7 failed: ['A','A','A','7']\"\nassert score(['2','3','4','5','6']) == 20, \"Test case 8 failed: ['2','3','4','5','6']\"\nassert score(['K','Q','2']) == 0, \"Test case 9 failed: ['K','Q','2']\"\nassert score(['A']) == 11, \"Test case 10 failed: ['A']\""}
{"task_id": 239, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str='same') -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\"\"\"\n    if output_shape == 'valid':\n        return ((0, 0), (0, 0))\n    else:\n        (f_height, f_width) = filter_shape\n        total_pad_height = f_height - 1\n        pad_top = total_pad_height // 2\n        pad_bottom = total_pad_height - pad_top\n        total_pad_width = f_width - 1\n        pad_left = total_pad_width // 2\n        pad_right = total_pad_width - pad_left\n        return ((pad_top, pad_bottom), (pad_left, pad_right))\nassert determine_padding((3, 3), \"same\") == ((1, 1), (1, 1)), \"failed: (3,3) same\"\nassert determine_padding((1, 1), \"same\") == ((0, 0), (0, 0)), \"failed: (1,1) same\"\nassert determine_padding((5, 5), \"same\") == ((2, 2), (2, 2)), \"failed: (5,5) same\"\nassert determine_padding((4, 4), \"same\") == ((1, 2), (1, 2)), \"failed: (4,4) same\"\nassert determine_padding((4, 2), \"same\") == ((1, 2), (0, 1)), \"failed: (4,2) same\"\nassert determine_padding((2, 4), \"same\") == ((0, 1), (1, 2)), \"failed: (2,4) same\"\nassert determine_padding((3, 3), \"valid\") == ((0, 0), (0, 0)), \"failed: (3,3) valid\"\nassert determine_padding((2, 2), \"valid\") == ((0, 0), (0, 0)), \"failed: (2,2) valid\"\nassert determine_padding((2, 2), \"same\") == ((0, 1), (0, 1)), \"failed: (2,2) same\"\nassert determine_padding((7, 3), \"same\") == ((3, 3), (1, 1)), \"failed: (7,3) same\""}
{"task_id": 240, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef create_encoding_dicts(obs_ids: list, action_ids: list, obs_encoder: dict | None=None, act_encoder: dict | None=None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    if obs_encoder is not None:\n        obs2num = obs_encoder.copy()\n    elif not obs_ids:\n        obs2num = {}\n    else:\n        obs2num = {id: idx for (idx, id) in enumerate(obs_ids)}\n    num2obs = {v: k for (k, v) in obs2num.items()}\n    if act_encoder is not None:\n        act2num = act_encoder.copy()\n    elif not action_ids:\n        act2num = {}\n    else:\n        act2num = {id: idx for (idx, id) in enumerate(action_ids)}\n    num2act = {v: k for (k, v) in act2num.items()}\n    return {'obs2num': obs2num, 'num2obs': num2obs, 'act2num': act2num, 'num2act': num2act}\nassert create_encoding_dicts(['a', 'b'], ['x', 'y']) == {'obs2num': {'a': 0, 'b': 1}, 'num2obs': {0: 'a', 1: 'b'}, 'act2num': {'x': 0, 'y': 1}, 'num2act': {0: 'x', 1: 'y'}}, 'test case failed: basic enumeration'\nres = create_encoding_dicts(['left', 'right', 'front', 'back'], ['move', 'turn']); assert res['obs2num']['front'] == 2 and res['num2obs'][2] == 'front', 'test case failed: larger lists (obs)'\nobs_enc = {'cold': 5, 'hot': 7}; act_enc = {'go': 9, 'stop': 3}; res = create_encoding_dicts(['cold', 'hot'], ['go', 'stop'], obs_enc, act_enc); assert res['obs2num'] == obs_enc and res['num2obs'] == {5: 'cold', 7: 'hot'}, 'test case failed: explicit obs encoder'\nassert create_encoding_dicts([], []) == {'obs2num': {}, 'num2obs': {}, 'act2num': {}, 'num2act': {}}, 'test case failed: empty lists'\nres = create_encoding_dicts(['sun', 'rain'], ['walk', 'run'], {'sun': 2, 'rain': 4}); assert res['act2num']['walk'] == 0 and res['num2act'][1] == 'run', 'test case failed: mixed encoders (act)'\nres = create_encoding_dicts([], ['jump'], None, {'jump': 42}); assert res['act2num']['jump'] == 42 and res['num2act'][42] == 'jump', 'test case failed: encoder with empty obs list'\nres = create_encoding_dicts(['x1', 'x2', 'x3'], ['y']); assert all(res['num2obs'][res['obs2num'][o]] == o for o in ['x1', 'x2', 'x3']), 'test case failed: consistency (obs)'\nres = create_encoding_dicts([10, 20], [30, 40]); assert res['obs2num'][10] == 0 and res['num2act'][0] == 30, 'test case failed: numerical identifiers'\nres = create_encoding_dicts(['only_obs'], ['only_act']); assert res['obs2num']['only_obs'] == 0 and res['act2num']['only_act'] == 0, 'test case failed: single element lists'\nres = create_encoding_dicts(['A','B','C'], ['L']); assert res['num2act'][res['act2num']['L']] == 'L', 'test case failed: consistency (act)'"}
{"task_id": 242, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float=1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    for i in range(len(pull_counts)):\n        if pull_counts[i] == 0:\n            return i\n    T = current_step + 1\n    ln_T = math.log(T)\n    scores = []\n    for a in range(len(ev_estimates)):\n        score = ev_estimates[a] + C * math.sqrt(2 * ln_T / pull_counts[a])\n        scores.append(score)\n    max_score = max(scores)\n    selected_arm = scores.index(max_score)\n    return selected_arm\nassert ucb1_select([0.5, 0.7], [1, 1], 2) == 1, \"failed on basic two-arm example\"\nassert ucb1_select([0.2, 0.9, 0.1], [0, 5, 3], 8) == 0, \"failed when an arm was never pulled\"\nassert ucb1_select([0.6, 0.6], [10, 1], 11) == 1, \"failed when counts differ greatly\"\nassert ucb1_select([0.3, 0.4, 0.5, 0.6], [5, 5, 5, 5], 20) == 3, \"failed on equal counts different EVs\"\nassert ucb1_select([0.5, 0.5], [1, 1], 2) == 0, \"failed tie-breaking rule\"\nassert ucb1_select([0.2], [10], 10) == 0, \"failed single-arm bandit\"\nassert ucb1_select([0.4, 0.4], [3, 3], 6, C=0.5) == 0, \"failed tie with custom C\"\nassert ucb1_select([0.1, 0.2, 0.3], [2, 4, 6], 12) == 0, \"failed three-arm varying counts\"\nassert ucb1_select([0.9, 0.1], [100, 1], 101) == 1, \"failed exploration against high EV arm\"\nassert ucb1_select([0.0, 0.0, 0.0], [1, 0, 1], 2) == 1, \"failed when exactly one arm untried\""}
{"task_id": 243, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    z1 = np.maximum(0, state.dot(W1) + b1)\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    z2 = np.maximum(0, z1.dot(W2) + b2)\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    z3 = np.maximum(0, z2.dot(W3) + b3)\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    logits = z3.dot(W_a) + b_a\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    state_value = z3.dot(W_c) + b_c\n    state_value = state_value[0]\n    action_probs = [round(p, 4) for p in action_probs]\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)\nassert actor_critic_forward([1.0, -1.0, 0.5]) == ([0.3333, 0.3333, 0.3333], 32.0), \"failed on state [1.0, -1.0, 0.5]\"\nassert actor_critic_forward([2.0, 3.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 384.0), \"failed on state [2.0, 3.0, 1.0]\"\nassert actor_critic_forward([-2.0, 0.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-2.0, 0.0, 1.0]\"\nassert actor_critic_forward([0.0, 0.0, 0.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [0.0, 0.0, 0.0]\"\nassert actor_critic_forward([1.0, 1.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [1.0, 1.0, 1.0]\"\nassert actor_critic_forward([-1.0, -1.0, -1.0]) == ([0.3333, 0.3333, 0.3333], 0.0), \"failed on state [-1.0, -1.0, -1.0]\"\nassert actor_critic_forward([4.0, -2.0, 1.0]) == ([0.3333, 0.3333, 0.3333], 192.0), \"failed on state [4.0, -2.0, 1.0]\"\nassert actor_critic_forward([10.0, -10.0, 5.0]) == ([0.3333, 0.3333, 0.3333], 320.0), \"failed on state [10.0, -10.0, 5.0]\"\nassert actor_critic_forward([0.25, 0.25, 0.25]) == ([0.3333, 0.3333, 0.3333], 48.0), \"failed on state [0.25, 0.25, 0.25]\"\nassert actor_critic_forward([0.1, -0.2, 0.3]) == ([0.3333, 0.3333, 0.3333], 12.8), \"failed on state [0.1, -0.2, 0.3]\""}
{"task_id": 244, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef random_binary_tensor(shape: tuple, sparsity: float=0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    if not isinstance(shape, tuple):\n        return -1\n    if not shape:\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    if isinstance(sparsity, bool):\n        return -1\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if not 0 <= sparsity <= 1:\n        return -1\n    tensor = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (tensor >= threshold).astype(float)\n    return binary_tensor.tolist()\nnp.random.seed(0)\nassert random_binary_tensor((2, 3), 0.5) == [[1.0, 1.0, 1.0], [1.0, 0.0, 1.0]], \"test case failed: seed=0, shape=(2,3), sparsity=0.5\"\nnp.random.seed(1)\nassert random_binary_tensor((5,), 0.3) == [0.0, 1.0, 0.0, 0.0, 0.0], \"test case failed: seed=1, shape=(5,), sparsity=0.3\"\nnp.random.seed(2)\nassert random_binary_tensor((1, 4), 0.0) == [[0.0, 0.0, 0.0, 0.0]], \"test case failed: seed=2, shape=(1,4), sparsity=0.0\"\nnp.random.seed(3)\nassert random_binary_tensor((2, 2), 1.0) == [[1.0, 1.0], [1.0, 1.0]], \"test case failed: seed=3, shape=(2,2), sparsity=1.0\"\nnp.random.seed(4)\nassert random_binary_tensor((3, 2), 0.7) == [[1.0, 1.0], [1.0, 1.0], [1.0, 0.0]], \"test case failed: seed=4, shape=(3,2), sparsity=0.7\"\nassert random_binary_tensor((2, 2), 1.2) == -1, \"test case failed: sparsity > 1\"\nassert random_binary_tensor((2, 2), -0.1) == -1, \"test case failed: sparsity < 0\"\nassert random_binary_tensor([2, 3], 0.5) == -1, \"test case failed: shape is not tuple\"\nassert random_binary_tensor((0, 3), 0.5) == -1, \"test case failed: zero dimension in shape\"\nassert random_binary_tensor((2.5, 3), 0.5) == -1, \"test case failed: non-integer dimension\""}
{"task_id": 247, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]], payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Determine the arm with the highest expected reward in a Gaussian bandit.\n\n    The function must compute the expected reward for each arm (p_k * \u03bc_k) and\n    return a tuple (best_ev, best_arm), where best_ev is rounded to 4 decimal\n    places and best_arm is the index of the arm with that expected reward.  If\n    the input is invalid (see task description) the function should return -1.\n    \"\"\"\n    if len(payoff_dists) != len(payoff_probs) or len(payoff_dists) == 0:\n        return -1\n    for i in range(len(payoff_dists)):\n        p = payoff_probs[i]\n        if p < 0 or p > 1:\n            return -1\n        (mu, sigma_sq) = payoff_dists[i]\n        if sigma_sq <= 0:\n            return -1\n    mu = np.array([d[0] for d in payoff_dists])\n    p = np.array(payoff_probs)\n    expected_rewards = p * mu\n    max_ev = np.max(expected_rewards)\n    best_arm = np.argmax(expected_rewards)\n    best_ev = round(max_ev, 4)\n    return (best_ev, best_arm)\nassert gaussian_bandit_oracle([(5, 2), (3, 1), (10, 4)], [1.0, 0.5, 0.2]) == (5.0, 0), \"failed on basic example\"\nassert gaussian_bandit_oracle([(0, 1), (7, 2)], [1, 1]) == (7.0, 1), \"failed when best arm is last\"\nassert gaussian_bandit_oracle([(4, 1), (4, 1)], [1, 1]) == (4.0, 0), \"failed on tie breaking\"\nassert gaussian_bandit_oracle([(4, 1), (1, 1)], [0, 1]) == (1.0, 1), \"failed with zero probability on an arm\"\nassert gaussian_bandit_oracle([(3.3333, 0.5)], [0.3]) == (1.0, 0), \"failed on rounding check\"\nassert gaussian_bandit_oracle([], []) == -1, \"failed on empty input\"\nassert gaussian_bandit_oracle([(1, -1)], [0.5]) == -1, \"failed to detect non-positive variance\"\nassert gaussian_bandit_oracle([(1, 1)], [1.2]) == -1, \"failed to detect probability > 1\"\nassert gaussian_bandit_oracle([(1, 1)], [-0.1]) == -1, \"failed to detect probability < 0\"\nassert gaussian_bandit_oracle([(1, 1), (2, 2), (3, 3), (4, 4)], [0.1, 0.2, 0.3, 0.4]) == (1.6, 3), \"failed on larger input\""}
{"task_id": 248, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\"\"\"\n    p = np.sum(y_train) / len(y_train)\n    n_test = X_test.shape[0]\n    if n_test == 0:\n        return []\n    predictions = np.full(n_test, p)\n    predictions = np.round(predictions, 4)\n    return predictions.tolist()\nassert baseline_prob_predict(np.array([[1, 2], [3, 4], [5, 6]]), np.array([0, 1, 1]), np.array([[7, 8], [9, 10]])) == [0.6667, 0.6667], \"test case failed: p = 2/3\"\nassert baseline_prob_predict(np.array([[0], [1]]), np.array([0, 0]), np.array([[5], [6], [7]])) == [0.0, 0.0, 0.0], \"test case failed: all negatives\"\nassert baseline_prob_predict(np.array([[0], [1], [2]]), np.array([1, 1, 1]), np.array([[5], [6]])) == [1.0, 1.0], \"test case failed: all positives\"\nassert baseline_prob_predict(np.array([[0], [1], [2], [3]]), np.array([1, 0, 0, 0]), np.array([[9], [9], [9], [9]])) == [0.25, 0.25, 0.25, 0.25], \"test case failed: p = 0.25\"\nassert baseline_prob_predict(np.array([[0], [1], [2], [3], [4]]), np.array([1, 0, 1, 0, 0]), np.array([[0], [0], [0], [0], [0]])) == [0.4, 0.4, 0.4, 0.4, 0.4], \"test case failed: p = 0.4\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([1]), np.empty((0, 2))) == [], \"test case failed: empty X_test\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([0]), np.array([[3, 4]])) == [0.0], \"test case failed: single negative\"\nassert baseline_prob_predict(np.array([[1, 2]]), np.array([1]), np.array([[3, 4], [5, 6]])) == [1.0, 1.0], \"test case failed: single positive\"\nassert baseline_prob_predict(np.array([[0], [1], [2]]), np.array([1, 0, 0]), np.array([[8], [8], [8]])) == [0.3333, 0.3333, 0.3333], \"test case failed: rounding 1/3\"\nassert baseline_prob_predict(np.arange(100).reshape(100, 1), np.array([1] * 55 + [0] * 45), np.array([[0], [0], [0], [0]])) == [0.55, 0.55, 0.55, 0.55], \"test case failed: p = 0.55\""}
{"task_id": 252, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)\nassert least_squares_loss(np.array([3.0, -0.5, 2.0, 7.0]), np.array([2.5, 0.0, 2.0, 8.0])) == ([0.5, -0.5, 0.0, -1.0], [1.0, 1.0, 1.0, 1.0]), \"failed on mixed positive/negative values\"\nassert least_squares_loss(np.array([0.0]), np.array([0.0])) == ([0.0], [1.0]), \"failed on single zero element\"\nassert least_squares_loss(np.array([1.0]), np.array([-1.0])) == ([2.0], [1.0]), \"failed on single element opposite sign\"\nassert least_squares_loss(np.array([5.0, 5.0]), np.array([5.0, 5.0])) == ([0.0, 0.0], [1.0, 1.0]), \"failed on identical vectors\"\nassert least_squares_loss(np.array([-2.0, -4.0]), np.array([-3.0, -1.0])) == ([1.0, -3.0], [1.0, 1.0]), \"failed on negative values\"\nassert least_squares_loss(np.array([1e6, -1e6]), np.array([0.0, 0.0])) == ([1000000.0, -1000000.0], [1.0, 1.0]), \"failed on large magnitude values\"\nassert least_squares_loss(np.array([0.123, 0.456]), np.array([0.321, 0.654])) == ([-0.198, -0.198], [1.0, 1.0]), \"failed on decimal values\"\nassert least_squares_loss(np.array([10.0, 20.0, 30.0]), np.array([9.0, 18.0, 33.0])) == ([1.0, 2.0, -3.0], [1.0, 1.0, 1.0]), \"failed on three-element vector\"\nassert least_squares_loss(np.array([7.0, 8.0, 9.0, 10.0]), np.array([7.0, 8.0, 9.0, 10.0])) == ([0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0]), \"failed on perfect prediction\""}
{"task_id": 253, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    if gradient:\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        gradient = [round(x, 4) for x in gradient]\n        return gradient\n    else:\n        l2_norm = np.linalg.norm(w, 2)\n        sum_w_sq = np.dot(w, w)\n        l1_part = l1_ratio * l2_norm\n        l2_part = (1 - l1_ratio) * 0.5 * sum_w_sq\n        penalty = alpha * (l1_part + l2_part)\n        return round(penalty, 4)\nassert elastic_net_regularization([1, -2, 3], 0.1, 0.5) == 0.5371, \"failed on value, case 1\"\nassert elastic_net_regularization([1, -2, 3], 0.1, 0.5, True) == [0.1, -0.15, 0.2], \"failed on grad, case 2\"\nassert elastic_net_regularization([0, 0, 0], 0.3, 0.7) == 0.0, \"failed on value, zero vector\"\nassert elastic_net_regularization([0, 0, 0], 0.3, 0.7, True) == [0.0, 0.0, 0.0], \"failed on grad, zero vector\"\nassert elastic_net_regularization([5], 1.0, 1.0) == 5.0, \"failed on value, pure L1\"\nassert elastic_net_regularization([5], 1.0, 1.0, True) == [1.0], \"failed on grad, pure L1\"\nassert elastic_net_regularization([3, 4], 0.2, 0.0) == 2.5, \"failed on value, pure L2\"\nassert elastic_net_regularization([3, 4], 0.2, 0.0, True) == [0.6, 0.8], \"failed on grad, pure L2\"\nassert elastic_net_regularization([-1, -1, -1, -1], 0.05, 0.3) == 0.1, \"failed on value, neg weights\"\nassert elastic_net_regularization([-1, -1, -1, -1], 0.05, 0.3, True) == [-0.05, -0.05, -0.05, -0.05], \"failed on grad, neg weights\""}
{"task_id": 255, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 without causing the\n    hand's total value to exceed 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    aces = hand.count(1)\n    if aces == 0:\n        return False\n    sum_aces_1 = sum(hand)\n    return sum_aces_1 + 10 <= 21\nassert usable_ace([1, 7]) is True, \"failed on [1, 7]\"\nassert usable_ace([1, 10]) is True, \"failed on [1, 10]\"\nassert usable_ace([1, 10, 10]) is False, \"failed on [1, 10, 10]\"\nassert usable_ace([2, 3, 4]) is False, \"failed on [2, 3, 4]\"\nassert usable_ace([1]) is True, \"failed on [1]\"\nassert usable_ace([1, 1, 9]) is True, \"failed on [1, 1, 9]\"\nassert usable_ace([1, 1, 9, 2]) is False, \"failed on [1, 1, 9, 2]\"\nassert usable_ace([1, 2, 3, 4, 5, 6]) is False, \"failed on [1, 2, 3, 4, 5, 6]\"\nassert usable_ace([1, 1, 1, 1, 8]) is False, \"failed on [1, 1, 1, 1, 8]\"\nassert usable_ace([1, 8, 1]) is True, \"failed on [1, 8, 1]\""}
{"task_id": 258, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    product = actual * predicted\n    per_sample_loss = np.maximum(0, 1 - product)\n    mean_loss = np.mean(per_sample_loss)\n    return round(mean_loss, 4)\nassert hinge_loss([1, -1, 1, -1], [2, -0.5, 0.3, -3]) == 0.3, \"failed: basic mixed example\"\nassert hinge_loss([1, 1, 1], [3, 2, 4]) == 0.0, \"failed: all correct positive class\"\nassert hinge_loss([-1, -1, -1], [-2, -2, -1]) == 0.0, \"failed: all correct negative class\"\nassert hinge_loss([1, -1], [-1, -1]) == 1.0, \"failed: one mis-classified sample\"\nassert hinge_loss([-1, 1], [0, 0]) == 1.0, \"failed: zero predictions\"\nassert hinge_loss([1], [1]) == 0.0, \"failed: single perfect prediction\"\nassert hinge_loss([-1], [1]) == 2.0, \"failed: single worst-case prediction\"\nassert hinge_loss([1, -1, 1, -1], [0.5, 0.5, -0.5, -0.5]) == 1.0, \"failed: margin violations\"\nassert hinge_loss([1, -1, 1, -1], [1, -1, 1, -1]) == 0.0, \"failed: predictions on decision boundary\"\nassert hinge_loss([1, -1, 1, -1, 1], [2, -2, 0, -3, 0.8]) == 0.24, \"failed: larger array mixed case\""}
{"task_id": 259, "completion_id": 0, "passed": true, "result": "passed", "solution": "import random\nimport random\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Randomly samples *batch_size* transitions from the replay buffer.\n\n    The selection must be deterministic (use random.seed(42)).  If\n    *batch_size* exceeds the buffer length, return -1.\n    \"\"\"\n    if batch_size > len(memory):\n        return -1\n    random.seed(42)\n    return random.sample(memory, batch_size)\nassert sample_replay_memory([1, 2, 3, 4, 5], 3) == [1, 5, 3], \"failed: sample_replay_memory([1,2,3,4,5],3)\"\nassert sample_replay_memory(['a', 'b', 'c', 'd'], 2) == ['a', 'd'], \"failed: sample_replay_memory(['a','b','c','d'],2)\"\nassert sample_replay_memory([10], 1) == [10], \"failed: sample_replay_memory([10],1)\"\nassert sample_replay_memory([0, 1, 2], 4) == -1, \"failed: sample_replay_memory request larger than buffer\"\nassert sample_replay_memory(['x', 'y'], 1) == ['x'], \"failed: sample_replay_memory(['x','y'],1)\"\nassert sample_replay_memory(['x', 'y'], 2) == ['x', 'y'], \"failed: sample_replay_memory(['x','y'],2)\""}
{"task_id": 261, "completion_id": 0, "passed": false, "result": "failed: name 'glorot_normal' is not defined", "solution": "import numpy as np\n\nnp.random.seed(1)\nshape=(64,32)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (64,32)\"\nnp.random.seed(2)\nshape=(16,3,3,3)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (16,3,3,3)\"\nnp.random.seed(3)\nshape=(5,5)\nW=glorot_normal(shape)\nassert abs(W.mean())<0.2,\"mean far from 0 for shape (5,5)\"\nnp.random.seed(4)\nshape=(8,8,1,1)\nW=glorot_normal(shape)\nfan_in=8*1*1\nfan_out=8*1*1\nexp_std=np.sqrt(2/(fan_in+fan_out))\nassert abs(W.std()-exp_std)<0.1,\"std incorrect for shape (8,8,1,1)\"\nnp.random.seed(5)\nshape=(1,1,3,3)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (1,1,3,3)\"\nnp.random.seed(6)\nshape=(10,)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (10,)\"\nnp.random.seed(7)\nshape=(12,6,2,2)\nW=glorot_normal(shape)\nassert abs(W.mean())<0.1,\"mean far from 0 for shape (12,6,2,2)\"\nnp.random.seed(8)\nshape=(4,4,4)\nW=glorot_normal(shape)\nassert W.shape==shape,\"failed shape (4,4,4)\"\nnp.random.seed(9)\nshape=(128,256)\nW=glorot_normal(shape)\nassert abs(W.std()-np.sqrt(2/(128+256)))<0.05,\"std incorrect for shape (128,256)\""}
{"task_id": 266, "completion_id": 0, "passed": false, "result": "failed: failed: self loop ignored", "solution": "\nfrom typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for (i, v) in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    for (u, v) in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    return G\nassert build_adj_list([], []) == [], \"failed: empty graph\"\nassert build_adj_list([0], []) == [[]], \"failed: single isolated vertex\"\nassert build_adj_list([0,1], [(0,1)]) == [[1],[0]], \"failed: simple two-vertex edge\"\nassert build_adj_list([0,1,2], [(0,1),(1,2)]) == [[1],[0,2],[1]], \"failed: 3-line path\"\nassert build_adj_list([\"A\",\"B\",\"C\",\"D\"], [(\"A\",\"B\"),(\"B\",\"C\"),(\"C\",\"A\"),(\"B\",\"A\")]) == [[\"B\",\"C\"],[\"A\",\"C\"],[\"A\",\"B\"],[]], \"failed: example with duplicate/reversed edges\"\nassert build_adj_list([\"x\",\"y\",\"z\"], [(\"x\",\"x\"),(\"x\",\"y\")]) == [[\"y\"],[\"x\"],[]], \"failed: self loop ignored\"\nassert build_adj_list([0,1,2], [(0,3),(3,4)]) == [[ ], [ ], [ ]], \"failed: edges with unknown vertices ignored\"\nassert build_adj_list([\"A\",\"B\",\"C\"], []) == [[],[],[]], \"failed: all isolated vertices\""}
{"task_id": 269, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\"\"\"\n    layer1 = state_dimensions * 164 + 164\n    layer2 = 164 * action_dimensions + action_dimensions\n    return layer1 + layer2\nassert count_trainable_params(4, 2) == 1150, \"failed for (4,2)\"\nassert count_trainable_params(1, 1) == 493, \"failed for (1,1)\"\nassert count_trainable_params(10, 5) == 2629, \"failed for (10,5)\"\nassert count_trainable_params(3, 7) == 1811, \"failed for (3,7)\"\nassert count_trainable_params(20, 20) == 6744, \"failed for (20,20)\"\nassert count_trainable_params(100, 100) == 33064, \"failed for (100,100)\""}
{"task_id": 277, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env['action_space'], (tuple, dict))\n    tuple_obs = isinstance(env['observation_space'], (tuple, dict))\n    return (tuple_action, tuple_obs)\nassert is_tuple({\"action_space\": (0, 1), \"observation_space\": {\"x\": 0}}) == (True, True), \"test case failed: both composite\"\nassert is_tuple({\"action_space\": [0, 1], \"observation_space\": (0, 1)}) == (False, True), \"test case failed: obs composite (tuple)\"\nassert is_tuple({\"action_space\": {\"a\": 1}, \"observation_space\": 42}) == (True, False), \"test case failed: action composite (dict)\"\nassert is_tuple({\"action_space\": 3.14, \"observation_space\": \"state\"}) == (False, False), \"test case failed: none composite\"\nassert is_tuple({\"action_space\": ((1, 2), (3, 4)), \"observation_space\": {\"a\": {\"b\": 2}}}) == (True, True), \"test case failed: nested composites\"\nassert is_tuple({\"action_space\": None, \"observation_space\": None}) == (False, False), \"test case failed: None spaces\"\nassert is_tuple({\"action_space\": [1, 2, 3], \"observation_space\": {}}) == (False, True), \"test case failed: observation dict\"\nassert is_tuple({\"action_space\": (\"left\", \"right\"), \"observation_space\": -1}) == (True, False), \"test case failed: action tuple\"\nassert is_tuple({\"action_space\": tuple(range(100)), \"observation_space\": {i: i*i for i in range(10)}}) == (True, True), \"test case failed: large composites\"\nassert is_tuple({\"action_space\": (1, {\"a\": 2}), \"observation_space\": {\"tuple\": (1, 2)}}) == (True, True), \"test case failed: mixed inner types\""}
{"task_id": 278, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        else:\n            feature_i = current_node['feature_i']\n            threshold = current_node['threshold']\n            if sample[feature_i] <= threshold:\n                current_node = current_node['true_branch']\n            else:\n                current_node = current_node['false_branch']\nassert evaluate_decision_tree({'feature_i': 0, 'threshold': 2.5, 'true_branch': {'value': 1}, 'false_branch': {'value': 0}}, [3.0]) == 0, \"test case failed: simple_tree, sample [3.0]\"\nassert evaluate_decision_tree({'feature_i': 0, 'threshold': 2.5, 'true_branch': {'value': 1}, 'false_branch': {'value': 0}}, [2.0]) == 1, \"test case failed: simple_tree, sample [2.0]\"\nassert evaluate_decision_tree({'feature_i': 1, 'threshold': 1.0, 'true_branch': {'value': -1}, 'false_branch': {'feature_i': 0, 'threshold': 0.5, 'true_branch': {'value': 2}, 'false_branch': {'value': 3}}}, [0.4, 1.2]) == 2, \"test case failed: deep_tree, sample [0.4,1.2]\"\nassert evaluate_decision_tree({'feature_i': 1, 'threshold': 1.0, 'true_branch': {'value': -1}, 'false_branch': {'feature_i': 0, 'threshold': 0.5, 'true_branch': {'value': 2}, 'false_branch': {'value': 3}}}, [0.6, 1.2]) == 3, \"test case failed: deep_tree, sample [0.6,1.2]\"\nassert evaluate_decision_tree({'feature_i': 1, 'threshold': 1.0, 'true_branch': {'value': -1}, 'false_branch': {'feature_i': 0, 'threshold': 0.5, 'true_branch': {'value': 2}, 'false_branch': {'value': 3}}}, [0.1, 0.5]) == -1, \"test case failed: deep_tree, sample [0.1,0.5]\"\nassert evaluate_decision_tree({'value': 42}, [10, 20, 30]) == 42, \"test case failed: leaf_tree should always return 42\"\nassert evaluate_decision_tree({'feature_i': 0, 'threshold': -1.5, 'true_branch': {'value': 'inside'}, 'false_branch': {'feature_i': 1, 'threshold': 0, 'true_branch': {'value': 'edge'}, 'false_branch': {'value': 'outside'}}}, [-2, 3]) == 'inside', \"test case failed: neg_tree, sample [-2,3]\"\nassert evaluate_decision_tree({'feature_i': 0, 'threshold': -1.5, 'true_branch': {'value': 'inside'}, 'false_branch': {'feature_i': 1, 'threshold': 0, 'true_branch': {'value': 'edge'}, 'false_branch': {'value': 'outside'}}}, [0, 0]) == 'edge', \"test case failed: neg_tree, sample [0,0]\"\nassert evaluate_decision_tree({'feature_i': 0, 'threshold': -1.5, 'true_branch': {'value': 'inside'}, 'false_branch': {'feature_i': 1, 'threshold': 0, 'true_branch': {'value': 'edge'}, 'false_branch': {'value': 'outside'}}}, [1, 1]) == 'outside', \"test case failed: neg_tree, sample [1,1]\""}
{"task_id": 280, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool=False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector or matrix.\n    alpha : float\n        Regularization strength (must be non-negative).\n    return_grad : bool, optional (default=False)\n        If True, return the gradient of the regularization term instead of the\n        regularization value itself.\n\n    Returns\n    -------\n    float or np.ndarray\n        \u2022 When *return_grad* is False: the scalar L2 regularization value.\n        \u2022 When *return_grad* is True:  the gradient with the same shape as *w*.\n    \"\"\"\n    if return_grad:\n        grad = alpha * w\n        grad = np.round(grad, decimals=4)\n        return grad\n    else:\n        loss = 0.5 * alpha * np.sum(w ** 2)\n        loss = np.round(loss, decimals=4)\n        return float(loss)\nassert l2_regularization(np.array([1.0, -2.0, 3.0]), 0.1) == 0.7, \"Failed on scalar value with vector input\"\nassert np.allclose(l2_regularization(np.array([1.0, -2.0, 3.0]), 0.1, True), np.array([0.1, -0.2, 0.3])), \"Failed on gradient with vector input\"\nassert l2_regularization(np.array([0.0]), 0.3) == 0.0, \"Failed on zero vector value\"\nassert np.allclose(l2_regularization(np.zeros(5), 0.3, True), np.zeros(5)), \"Failed on zero vector gradient\"\nassert l2_regularization(np.array([4.0]), 2.0) == 16.0, \"Failed on single weight value\"\nassert np.allclose(l2_regularization(np.array([4.0]), 2.0, True), np.array([8.0])), \"Failed on single weight gradient\"\nassert l2_regularization(np.array([2.0, 2.0, 2.0, 2.0]), 0.5) == 4.0, \"Failed on equal elements value\"\nassert np.allclose(l2_regularization(np.array([2.0, 2.0, 2.0, 2.0]), 0.5, True), np.array([1.0, 1.0, 1.0, 1.0])), \"Failed on equal elements gradient\"\nassert l2_regularization(np.array([[1.0, 2.0], [3.0, 4.0]]), 0.2) == 3.0, \"Failed on matrix value\"\nassert np.allclose(l2_regularization(np.array([[1.0, 2.0], [3.0, 4.0]]), 0.2, True), np.array([[0.2, 0.4], [0.6, 0.8]])), \"Failed on matrix gradient\""}
{"task_id": 281, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef L1Regularization(alpha: float):\n    \"\"\"Create a callable object that implements L1 regularisation.\n\n    The returned object must support two operations:\n    1. call with a NumPy array to obtain the L1 penalty value\n    2. call its .grad(w) method to obtain the gradient of the penalty\n\n    Args:\n        alpha: Non-negative float controlling the strength of the regularisation.\n\n    Returns:\n        An instance that fulfils the described interface.\n    \"\"\"\n\n    class L1Regularizer:\n\n        def __init__(self, alpha):\n            self.alpha = alpha\n\n        def __call__(self, w):\n            if self.alpha == 0:\n                return 0.0\n            return self.alpha * np.sum(np.abs(w))\n\n        def grad(self, w):\n            if self.alpha == 0:\n                return np.zeros_like(w)\n            return self.alpha * np.sign(w)\n    return L1Regularizer(alpha)\nimport numpy as np\nreg1 = L1Regularization(0.1)\nreg2 = L1Regularization(0.0)\nw2 = np.random.randn(5,3)\nreg3 = L1Regularization(2.5)\nw3 = np.array([[0.0, 4.0], [-3.0, 1.0]])\nreg4 = L1Regularization(1.0)\nw4 = np.array([0.0, 0.0, 0.0])"}
{"task_id": 285, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    if x.ndim == 1:\n        x_reshaped = x.reshape(1, -1)\n    else:\n        x_reshaped = x\n    if y.ndim == 1:\n        y_reshaped = y.reshape(1, -1)\n    else:\n        y_reshaped = y\n    if x_reshaped.shape[1] != y_reshaped.shape[1]:\n        return -1\n    kernel = x_reshaped @ y_reshaped.T\n    return kernel.tolist()\nimport numpy as np\nassert linear_kernel(np.array([1,2,3]), np.array([4,5,6])) == [[32]], \"failed on 1-D vs 1-D\"\nassert linear_kernel(np.array([[1,2],[3,4]]), np.array([[5,6],[7,8]])) == [[17,23],[39,53]], \"failed on 2\u00d72 vs 2\u00d72\"\nassert linear_kernel(np.array([[1,2,3],[4,5,6]]), np.array([[7,8,9],[1,0,1]])) == [[50,4],[122,10]], \"failed on 2\u00d73 vs 2\u00d73\"\nassert linear_kernel(np.array([[1,2]]), np.array([[3,4],[5,6]])) == [[11,17]], \"failed on 1\u00d7d vs 2\u00d7d\"\nassert linear_kernel(np.array([[3,4],[5,6]]), np.array([1,2])) == [[11],[17]], \"failed on 2\u00d7d vs 1\u00d7d\"\nassert linear_kernel(np.array([[1,2]]), np.array([[1,2,3]])) == -1, \"failed on mismatched features (2 vs 3)\"\nassert linear_kernel(np.array([1,2,3]), np.array([1,2])) == -1, \"failed on mismatched 1-D lengths\"\nassert linear_kernel(np.array([[1,2,3]]), np.array([[4,5,6],[7,8,9],[1,1,1]])) == [[32,50,6]], \"failed on 1\u00d73 vs 3\u00d73\"\nassert linear_kernel(np.array([[2,0],[0,2]]), np.array([[1,1]])) == [[2],[2]], \"failed on 2\u00d72 vs 1\u00d72\""}
{"task_id": 286, "completion_id": 0, "passed": false, "result": "failed: name 'get_initializer' is not defined", "solution": "import random\n\nassert get_initializer('zeros_init')((2, 2)) == [[0.0, 0.0], [0.0, 0.0]], \"test case failed: zeros_init((2,2))\"\nassert get_initializer('ones_init')((1, 3)) == [[1.0, 1.0, 1.0]], \"test case failed: ones_init((1,3))\"\nassert get_initializer('zeros_init')((3, 1)) == [[0.0], [0.0], [0.0]], \"test case failed: zeros_init((3,1))\"\nassert get_initializer('ones_init')((2, 4)) == [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]], \"test case failed: ones_init((2,4))\"\n_matrix = get_initializer('random_uniform_init')((2, 3)); assert len(_matrix) == 2 and all(len(r) == 3 for r in _matrix), \"test case failed: random_uniform_init shape (2,3)\"\n_matrix = get_initializer('random_uniform_init')((2, 3)); assert all(0.0 <= v <= 1.0 for r in _matrix for v in r), \"test case failed: random_uniform_init value range\"\n_matrix2 = get_initializer('random_uniform_init')((3, 1)); assert len(_matrix2) == 3 and len(_matrix2[0]) == 1, \"test case failed: random_uniform_init shape (3,1)\"\nassert get_initializer('zeros_init') is zeros_init, \"test case failed: object identity for zeros_init\"\nassert get_initializer('ones_init') is ones_init, \"test case failed: object identity for ones_init\"\nassert get_initializer('random_uniform_init') is random_uniform_init, \"test case failed: object identity for random_uniform_init\""}
{"task_id": 289, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return ''\n    formatted = []\n    for (field, value) in entries:\n        field_str = str(field)\n        value_str = str(value)\n        formatted_line = f'{field_str}: \"{value_str}\"'\n        formatted.append(formatted_line)\n    return '\\n'.join(formatted)\nassert format_entries([(\"username\", \"john_doe\"), (\"password\", \"s3cr3t\")]) == \"username: \\\"john_doe\\\"\\npassword: \\\"s3cr3t\\\"\", \"Test case 1 failed: two simple pairs\"\nassert format_entries([]) == \"\", \"Test case 2 failed: empty list should return empty string\"\nassert format_entries([(\"Quote\", \"He said \\\"Hello\\\"\")]) == \"Quote: \\\"He said \\\"Hello\\\"\\\"\", \"Test case 3 failed: value containing quotes\"\nassert format_entries([(\"a\", \"\"), (\"b\", \"\")]) == \"a: \\\"\\\"\\nb: \\\"\\\"\", \"Test case 4 failed: empty strings as values\"\nassert format_entries([(\"single\", \"one\")]) == \"single: \\\"one\\\"\", \"Test case 5 failed: single pair\"\nassert format_entries([(\"x\", \"1\"), (\"y\", \"2\"), (\"z\", \"3\")]) == \"x: \\\"1\\\"\\ny: \\\"2\\\"\\nz: \\\"3\\\"\", \"Test case 6 failed: three numeric-string pairs\"\nassert format_entries([(\"\", \"blank field\"), (\"space\", \"with space\")]) == \": \\\"blank field\\\"\\nspace: \\\"with space\\\"\", \"Test case 7 failed: empty field name\"\nassert format_entries([(\"UPPER\", \"lower\"), (\"MiXeD\", \"CaSe\")]) == \"UPPER: \\\"lower\\\"\\nMiXeD: \\\"CaSe\\\"\", \"Test case 8 failed: case sensitivity\"\nassert format_entries([(\"special\", \"!@#$%^&*()\")]) == \"special: \\\"!@#$%^&*()\\\"\", \"Test case 9 failed: special characters\"\nassert format_entries([(\"newline\", \"line1\\\\nline2\")]) == \"newline: \\\"line1\\\\nline2\\\"\", \"Test case 10 failed: value containing newline\""}
{"task_id": 290, "completion_id": 0, "passed": false, "result": "failed: name 'Node' is not defined", "solution": "import numpy as np\nfrom dataclasses import dataclass\nimport numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if type(tree_a) != type(tree_b):\n        return False\n    if isinstance(tree_a, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-08)\n    else:\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-08):\n            return False\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        return True\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0, Leaf(0), Leaf(1))) is True, \"failed on identical shallow tree\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(1, 5.0, Leaf(0), Leaf(1))) is False, \"failed on feature mismatch\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0 + 1e-9, Leaf(0), Leaf(1))) is True, \"failed on near-equal thresholds\"\nassert compare_trees(Node(0, 5.0, Leaf(0), Leaf(1)), Node(0, 5.0 + 1e-4, Leaf(0), Leaf(1))) is False, \"failed on threshold mismatch\"\nassert compare_trees(Node(0, 1.0, Leaf(0), Leaf(1)), Node(0, 1.0, Node(1, 2.0, Leaf(0), Leaf(1)), Leaf(1))) is False, \"failed on structure mismatch\"\nassert compare_trees(Leaf(42), Leaf(42)) is True, \"failed on identical scalar leaves\"\nassert compare_trees(Leaf(42), Leaf(43)) is False, \"failed on unequal scalar leaves\"\nassert compare_trees(Leaf(np.array([1.0, 0.0])), Leaf(np.array([1.0, 0.0]))) is True, \"failed on identical array leaves\"\nassert compare_trees(Leaf(np.array([1.0, 0.0])), Leaf(np.array([0.0, 1.0]))) is False, \"failed on unequal array leaves\"\nassert compare_trees(Node(0, 0.0, Node(1, 2.2, Leaf(0), Leaf(1)), Node(2, -1.3, Leaf(1), Leaf(0))), Node(0, 0.0, Node(1, 2.2, Leaf(0), Leaf(1)), Node(2, -1.3, Leaf(1), Leaf(0)))) is True, \"failed on identical deep tree\""}
{"task_id": 293, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    processed_name = agent_name.strip().lower()\n    agent_map = {'dqn': ('CartPole-v0', 'unwrapped'), 'ddpg': ('Pendulum-v0', 'env'), 'a3c': ('Pendulum-v0', 'unwrapped'), 'a2c': ('CartPole-v0', 'env'), 'a2c_multi': ('CartPole-v0', 'raw'), 'trpo': ('Pendulum-v0', 'unwrapped')}\n    return agent_map.get(processed_name, -1)\nassert map_agent_environment(\"dqn\") == (\"CartPole-v0\", \"unwrapped\"), \"failed on dqn\"\nassert map_agent_environment(\"Ddpg\") == (\"Pendulum-v0\", \"env\"), \"failed on Ddpg (case-insensitive)\"\nassert map_agent_environment(\"A3C\") == (\"Pendulum-v0\", \"unwrapped\"), \"failed on A3C\"\nassert map_agent_environment(\"a2c\") == (\"CartPole-v0\", \"env\"), \"failed on a2c\"\nassert map_agent_environment(\"A2C_MULTI\") == (\"CartPole-v0\", \"raw\"), \"failed on A2C_MULTI\"\nassert map_agent_environment(\"trpo\") == (\"Pendulum-v0\", \"unwrapped\"), \"failed on trpo\"\nassert map_agent_environment(\"random_agent\") == -1, \"failed on unknown agent\"\nassert map_agent_environment(\"  dqn  \") == (\"CartPole-v0\", \"unwrapped\"), \"failed on whitespace handling\"\nassert map_agent_environment(\"\") == -1, \"failed on empty string\"\nassert map_agent_environment(\"A2c\") == (\"CartPole-v0\", \"env\"), \"failed on mixed-case A2c\""}
{"task_id": 328, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nfrom typing import List\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []\nassert first_capitalized_word([\"hello\", \"World\", \"again\"]) == [\"World\"], \"failed: ['hello','World','again']\"\nassert first_capitalized_word([\"Python\", \"java\", \"c++\"]) == [\"Python\"], \"failed: ['Python','java','c++']\"\nassert first_capitalized_word([\"lower\", \"case\", \"only\"]) == [], \"failed: ['lower','case','only']\"\nassert first_capitalized_word([\"CONSTANT\", \"CamelCase\"]) == [\"CONSTANT\"], \"failed: ['CONSTANT','CamelCase']\"\nassert first_capitalized_word([]) == [], \"failed: []\"\nassert first_capitalized_word([\"hello\", \"world\", \"123\", \"!\"]) == [], \"failed: ['hello','world','123','!']\"\nassert first_capitalized_word([\"test\", \"StackOverflow\", \"github\"]) == [\"StackOverflow\"], \"failed: ['test','StackOverflow','github']\"\nassert first_capitalized_word([\"python3\", \"Ruby\", \"perl\"]) == [\"Ruby\"], \"failed: ['python3','Ruby','perl']\"\nassert first_capitalized_word([\"A\", \"b\", \"C\"]) == [\"A\"], \"failed: ['A','b','C']\"\nassert first_capitalized_word([\"lower\", \"UPPER\"]) == [\"UPPER\"], \"failed: ['lower','UPPER']\""}
{"task_id": 364, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\nassert sign(10) == 1, \"test case failed: sign(10)\"\nassert sign(-5) == -1, \"test case failed: sign(-5)\"\nassert sign(0) == 0, \"test case failed: sign(0)\"\nassert sign(3.14) == 1, \"test case failed: sign(3.14)\"\nassert sign(-2.71) == -1, \"test case failed: sign(-2.71)\"\nassert sign(1e-12) == 1, \"test case failed: sign(1e-12)\"\nassert sign(-1e-8) == -1, \"test case failed: sign(-1e-8)\"\nassert sign(-0.0) == 0, \"test case failed: sign(-0.0)\"\nassert sign(999999999) == 1, \"test case failed: sign(999999999)\"\nassert sign(-999999999) == -1, \"test case failed: sign(-999999999)\""}
{"task_id": 378, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\nassert indicator(1) == 1, \"test case failed: indicator(1)\"\nassert indicator(0) == 0, \"test case failed: indicator(0)\"\nassert indicator(-1) == 0, \"test case failed: indicator(-1)\"\nassert indicator(2) == 0, \"test case failed: indicator(2)\"\nassert indicator(999) == 0, \"test case failed: indicator(999)\"\nassert indicator(-999) == 0, \"test case failed: indicator(-999)\"\nassert indicator(int(False)) == 0, \"test case failed: indicator(int(False))\"\nassert indicator(1+0) == 1, \"test case failed: indicator(1+0)\"\nassert indicator(1*1) == 1, \"test case failed: indicator(1*1)\""}
{"task_id": 379, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    return [node['key'] for node in sorted_nodes]\n# Test Case 1: Example from the description\nassert sort_priority_nodes([\n    {\"key\": \"A\", \"val\": 10, \"priority\": 3, \"entry_id\": 2},\n    {\"key\": \"B\", \"val\": 20, \"priority\": 2, \"entry_id\": 1},\n    {\"key\": \"C\", \"val\": 30, \"priority\": 3, \"entry_id\": 1}\n]) == ['B', 'C', 'A'], \"failed on example input\"\n# Test Case 2: already ordered input\nassert sort_priority_nodes([\n    {\"key\": \"X\", \"val\": 1, \"priority\": 1, \"entry_id\": 1},\n    {\"key\": \"Y\", \"val\": 2, \"priority\": 2, \"entry_id\": 2},\n    {\"key\": \"Z\", \"val\": 3, \"priority\": 3, \"entry_id\": 3}\n]) == ['X', 'Y', 'Z'], \"failed on already ordered input\"\n# Test Case 3: reverse ordered input\nassert sort_priority_nodes([\n    {\"key\": \"Z\", \"val\": 3, \"priority\": 3, \"entry_id\": 3},\n    {\"key\": \"Y\", \"val\": 2, \"priority\": 2, \"entry_id\": 2},\n    {\"key\": \"X\", \"val\": 1, \"priority\": 1, \"entry_id\": 1}\n]) == ['X', 'Y', 'Z'], \"failed on reverse ordered input\"\n# Test Case 4: duplicate priorities, different entry_ids\nassert sort_priority_nodes([\n    {\"key\": \"A\", \"val\": 0, \"priority\": 5, \"entry_id\": 2},\n    {\"key\": \"B\", \"val\": 0, \"priority\": 5, \"entry_id\": 1},\n    {\"key\": \"C\", \"val\": 0, \"priority\": 5, \"entry_id\": 3}\n]) == ['B', 'A', 'C'], \"failed on identical priorities\"\n# Test Case 5: single element list\nassert sort_priority_nodes([\n    {\"key\": \"only\", \"val\": 42, \"priority\": 7, \"entry_id\": 0}\n]) == ['only'], \"failed on single element\"\n# Test Case 6: empty input\nassert sort_priority_nodes([]) == [], \"failed on empty input\"\n# Test Case 7: negative priorities\nassert sort_priority_nodes([\n    {\"key\": \"neg\", \"val\": -1, \"priority\": -10, \"entry_id\": 5},\n    {\"key\": \"zero\", \"val\": 0, \"priority\": 0, \"entry_id\": 1}\n]) == ['neg', 'zero'], \"failed on negative priorities\"\n# Test Case 8: priorities with floats\nassert sort_priority_nodes([\n    {\"key\": \"low\", \"val\": 1, \"priority\": 1.5, \"entry_id\": 1},\n    {\"key\": \"high\", \"val\": 2, \"priority\": 0.5, \"entry_id\": 2}\n]) == ['high', 'low'], \"failed on float priorities\"\n# Test Case 9: mixed identical priorities and entry_ids (order stable for equal tuples)\nassert sort_priority_nodes([\n    {\"key\": \"first\", \"val\": 0, \"priority\": 4, \"entry_id\": 4},\n    {\"key\": \"second\", \"val\": 0, \"priority\": 4, \"entry_id\": 4}\n]) == ['first', 'second'], \"failed on identical priority and entry_id\"\n# Test Case 10: non-consecutive entry_ids\nassert sort_priority_nodes([\n    {\"key\": \"k1\", \"val\": 0, \"priority\": 2, \"entry_id\": 10},\n    {\"key\": \"k2\", \"val\": 0, \"priority\": 2, \"entry_id\": 3},\n    {\"key\": \"k3\", \"val\": 0, \"priority\": 1, \"entry_id\": 7}\n]) == ['k3', 'k2', 'k1'], \"failed on non-consecutive entry_ids\""}
{"task_id": 399, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    X_squared = np.sum(X ** 2, axis=1)\n    Y_squared = np.sum(Y ** 2, axis=1)\n    X_dot_YT = X.dot(Y.T)\n    squared_dist = X_squared[:, np.newaxis] + Y_squared - 2 * X_dot_YT\n    dist = np.sqrt(squared_dist)\n    rounded_dist = np.round(dist, 4)\n    return rounded_dist.tolist()\nassert pairwise_distances(np.array([[0, 0], [3, 4]]), np.array([[3, 0], [0, 4]])) == [[3.0, 4.0], [4.0, 3.0]], \"Test-1 failed\"\nassert pairwise_distances(np.array([[0, 0]]), np.array([[0, 0]])) == [[0.0]], \"Test-2 failed\"\nassert pairwise_distances(np.array([[1, 2], [3, 4]]), np.array([[5, 6]])) == [[5.6569], [2.8284]], \"Test-3 failed\"\nassert pairwise_distances(np.array([[0, 0, 0]]), np.array([[1, 2, 2], [2, 2, 1]])) == [[3.0, 3.0]], \"Test-4 failed\"\nassert pairwise_distances(np.array([[1, 2]]), np.array([[1, 2, 3]])) == -1, \"Test-5 failed (shape mismatch)\"\nassert pairwise_distances(np.array([[1, 1]]), np.array([[2, 2]])) == [[1.4142]], \"Test-6 failed\"\nassert pairwise_distances(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]])) == [[0.0, 1.4142], [1.4142, 0.0]], \"Test-7 failed\"\nassert pairwise_distances(np.array([[2, -3]]), np.array([[-2, 3]])) == [[7.2111]], \"Test-8 failed\"\nassert pairwise_distances(np.empty((0, 2)), np.array([[1, 1]])) == [], \"Test-9 failed (empty X)\"\nassert pairwise_distances(np.array([[1, 2]]), np.empty((0, 2))) == [[]], \"Test-10 failed (empty Y)\""}
{"task_id": 420, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    col = X[:, column]\n    left_mask = (col < value).tolist()\n    right_mask = (col >= value).tolist()\n    return (left_mask, right_mask)\nassert get_split_mask(np.array([[1,2,3],[4,5,6],[7,8,9]]),1,5) == ([True, False, False],[False, True, True]), \"failed: column 1, value 5\"\nassert get_split_mask(np.array([[0],[0],[1]]),0,0) == ([False, False, False],[True, True, True]), \"failed: threshold equal to 0\"\nassert get_split_mask(np.array([[0],[0],[1]]),0,2) == ([True, True, True],[False, False, False]), \"failed: threshold greater than max\"\nassert get_split_mask(np.array([[-3],[-2],[5]]),0,0) == ([True, True, False],[False, False, True]), \"failed: negative values\"\nassert get_split_mask(np.array([[1,2],[3,4]]),1,3) == ([True, False],[False, True]), \"failed: last column\"\nassert get_split_mask(np.array([[1,2],[3,4]]),-1,0) == ([],[]), \"failed: negative column index\"\nassert get_split_mask(np.array([[1,2],[3,4]]),5,0) == ([],[]), \"failed: column index out of range\"\nassert get_split_mask(np.array([[5,5,5],[5,5,5]]),2,5) == ([False, False],[True, True]), \"failed: values equal to threshold\"\nassert get_split_mask(np.array([[10,20],[30,40],[50,60]]),0,25) == ([True, False, False],[False, True, True]), \"failed: mix split on first column\"\nassert get_split_mask(np.array([[1.1,2.2],[3.3,4.4],[5.5,6.6]]),0,3.3) == ([True, False, False],[False, True, True]), \"failed: float threshold\""}
{"task_id": 424, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    (unique_labels, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    sum_terms = np.sum(probabilities * np.log2(probabilities))\n    H = -sum_terms\n    return round(H, 4)\nassert entropy([0, 0, 1, 1]) == 1.0, \"test case failed: entropy([0, 0, 1, 1])\"\nassert entropy([0, 0, 0, 0]) == 0.0, \"test case failed: entropy([0, 0, 0, 0])\"\nassert entropy([0, 1, 2, 3]) == 2.0, \"test case failed: entropy([0, 1, 2, 3])\"\nassert entropy([0, 1]) == 1.0, \"test case failed: entropy([0, 1])\"\nassert entropy([0, 1, 1, 1]) == 0.8113, \"test case failed: entropy([0, 1, 1, 1])\"\nassert entropy([i % 3 for i in range(300)]) == 1.585, \"test case failed: entropy([i % 3 for i in range(300)])\"\nassert entropy([0, 0, 0, 1]) == 0.8113, \"test case failed: entropy([0, 0, 0, 1])\"\nassert entropy([0, 1, 2, 2]) == 1.5, \"test case failed: entropy([0, 1, 2, 2])\"\nassert entropy([0, 1, 2, 3, 4]) == 2.3219, \"test case failed: entropy([0, 1, 2, 3, 4])\""}
{"task_id": 429, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, -1).\n    \"\"\"\n    if context.ndim != 1:\n        return (-1, -1)\n    D = context_probs.shape[0]\n    if context.size != D:\n        return (-1, -1)\n    if np.sum(context) != 1:\n        return (-1, -1)\n    if not np.all(np.isin(context, [0, 1])):\n        return (-1, -1)\n    current_context = np.argmax(context)\n    row = context_probs[current_context]\n    optimal_arm = np.argmax(row)\n    optimal_rwd = row[optimal_arm]\n    optimal_rwd = np.round(optimal_rwd, 4)\n    return (optimal_rwd, optimal_arm)\nassert oracle_payoff(np.array([[0.1,0.8],[0.6,0.4]]), np.array([1,0])) == (0.8,1), \"failed on basic 2x2 (context 0)\"\nassert oracle_payoff(np.array([[0.1,0.8],[0.6,0.4]]), np.array([0,1])) == (0.6,0), \"failed on basic 2x2 (context 1)\"\nassert oracle_payoff(np.array([[0.5,0.5],[0.9,0.9]]), np.array([0,1])) == (0.9,0), \"failed when best arms tie (choose smallest)\"\nassert oracle_payoff(np.array([[0.3,0.2,0.7]]), np.array([1])) == (0.7,2), \"failed single-context multi-arm\"\nassert oracle_payoff(np.array([[0.4,0.2,0.6],[0.1,0.5,0.3],[0.7,0.4,0.9]]), np.array([0,0,1])) == (0.9,2), \"failed on example from task description\"\nassert oracle_payoff(np.array([[0.123456,0.654321]]), np.array([1])) == (0.6543,1), \"failed on rounding to 4 decimals\"\nassert oracle_payoff(np.array([[0.2,0.8],[0.3,0.4]]), np.array([0,0])) == (-1,-1), \"failed on invalid context (no 1 present)\"\nassert oracle_payoff(np.array([[0.2,0.8],[0.3,0.4]]), np.array([0,1,0])) == (-1,-1), \"failed on length mismatch\"\nassert oracle_payoff(np.array([[0.2],[0.7],[0.6],[0.1]]), np.array([0,1,0,0])) == (0.7,0), \"failed on K=1\""}
