{"task_id": 2, "completion_id": 0, "solution": "import numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {'identity': {'link': lambda mu: mu, 'inv_link': lambda eta: eta, 'derivative': lambda mu: np.ones_like(mu)}, 'log': {'link': np.log, 'inv_link': np.exp, 'derivative': lambda mu: 1 / mu}, 'logit': {'link': lambda mu: np.log(mu / (1 - mu)), 'inv_link': lambda eta: 1 / (1 + np.exp(-eta)), 'derivative': lambda mu: 1 / (mu * (1 - mu))}}\n    return links"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n\n    class Node:\n\n        def __init__(self):\n            self.is_leaf = False\n            self.left = None\n            self.right = None\n            self.feature = None\n            self.value = None\n            self.indices = None\n\n    def build_node(indices):\n        if len(indices) < 3:\n            node = Node()\n            node.is_leaf = True\n            node.indices = indices\n            return node\n        else:\n            f = np.random.choice(data.shape[1])\n            min_val = data[indices, f].min()\n            max_val = data[indices, f].max()\n            v = np.random.uniform(min_val, max_val)\n            left_indices = []\n            right_indices = []\n            for idx in indices:\n                if data[idx, f] <= v:\n                    left_indices.append(idx)\n                else:\n                    right_indices.append(idx)\n            node = Node()\n            node.feature = f\n            node.value = v"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    row_sums = np.sum(X ** 2, axis=1)\n    outer = row_sums[:, np.newaxis] + row_sums[np.newaxis, :]\n    dot_product = X.dot(X.T)\n    D = outer - 2 * dot_product\n    D_rounded = np.round(D, decimals=4)\n    return D_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, n_iterations: int=4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\"\"\"\n    m = X_train.shape[0]\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(n_iterations):\n        z = np.dot(X_train, w) + b\n        sigmoid_z = 1 / (1 + np.exp(-z))\n        error = sigmoid_z - y_train\n        grad_w = X_train.T.dot(error) / m\n        grad_b = np.sum(error) / m\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    z_test = np.dot(X_test, w) + b\n    sigmoid_test = 1 / (1 + np.exp(-z_test))\n    predictions = [int(round(p)) for p in sigmoid_test]\n    return predictions"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef power_spectrum(frames: np.ndarray, scale: bool=False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True the resulting power spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        The power spectrum of each frame (only positive frequencies), rounded\n        to four decimal places and returned as a regular Python list.\n    \"\"\"\n    fft_frames = np.fft.rfft(frames, axis=1)\n    power = np.abs(fft_frames) ** 2\n    if scale:\n        n = frames.shape[1]\n        scale_factor = n // 2 + 1\n        power = power / scale_factor\n    power = np.round(power, 4)\n    return power.tolist()"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimals.\n    \"\"\"\n    p = 1 / (1 + np.exp(-y_pred))\n    p = np.clip(p, 1e-15, 1 - 1e-15)\n    loss = -(y * np.log(p) + (1 - y) * np.log(1 - p))\n    mean_loss = np.mean(loss)\n    gradient = p - y\n    hessian = p * (1 - p)\n    mean_loss = np.round(mean_loss, 4)\n    gradient = [np.round(g, 4) for g in gradient]\n    hessian = [np.round(h, 4) for h in hessian]\n    return (mean_loss, gradient, hessian)"}
{"task_id": 16, "completion_id": 0, "solution": "from typing import Any, Iterable, List, Tuple\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i + N]) for i in range(L - N + 1)]"}
{"task_id": 17, "completion_id": 0, "solution": "def build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\"\"\"\n    if not items:\n        return {}\n    root_index = None\n    for (i, p) in enumerate(parents):\n        if p == -1:\n            root_index = i\n            break\n    children_map = {}\n    for (i, p) in enumerate(parents):\n        if p != -1:\n            if p not in children_map:\n                children_map[p] = []\n            children_map[p].append(i)\n\n    def build_node(index):\n        node = {'item': items[index], 'count': counts[index], 'children': []}\n        for child in children_map.get(index, []):\n            node['children'].append(build_node(child))\n        return node\n    return build_node(root_index)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)"}
{"task_id": 20, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        sigmoid = 1.0 / (1.0 + np.exp(-x))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        if isinstance(x, list):\n            x = np.array(x)\n        sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n        gradient_x = sigmoid_x * (1 - sigmoid_x)\n        sigmoid_rounded = np.round(sigmoid_x, 4)\n        gradient_rounded = np.round(gradient_x, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)"}
{"task_id": 22, "completion_id": 0, "solution": "import numpy as np\ndef random_tensor(shape: tuple[int, ...], standardize: bool=False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    offset = np.random.randint(low=-300, high=300, size=shape)\n    u = np.random.rand(*shape)\n    tensor = offset + u\n    if standardize:\n        mean = tensor.mean(axis=0)\n        std = tensor.std(axis=0)\n        epsilon = np.finfo(float).eps\n        tensor = (tensor - mean) / (std + epsilon)\n    return tensor"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    w_np = np.array(w)\n    if len(velocity) == 0:\n        velocity_np = np.zeros_like(w_np)\n    else:\n        velocity_np = np.array(velocity)\n    tilde_w = w_np - momentum * velocity_np\n    g = grad_func(tilde_w)\n    g = np.array(g)\n    g_clipped = np.clip(g, -1, 1)\n    new_velocity = momentum * velocity_np + learning_rate * g_clipped\n    new_w = w_np - new_velocity\n    new_w_list = np.round(new_w, 4).tolist()\n    new_velocity_list = np.round(new_velocity, 4).tolist()\n    return (new_w_list, new_velocity_list)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    dot_products = X_train.dot(X_test.T)\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    K = np.exp(-gamma * dist_sq)\n    alpha_y = alpha * y_train\n    g = alpha_y.dot(K) + b\n    signs = np.where(g >= 0, 1, -1)\n    return signs.tolist()"}
{"task_id": 26, "completion_id": 0, "solution": "import numpy as np\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(0, x_array)\n    gradient = np.where(x_array >= 0, 1, 0)\n    return (activation.tolist(), gradient.tolist())"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not 0 <= sparsity <= 1:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array"}
{"task_id": 35, "completion_id": 0, "solution": "from typing import Any\nclass node:\n    \"\"\"A minimal tree node for decision-tree-like structures.\"\"\"\n\n    def __init__(self, fea: int=-1, res: Any | None=None, child: dict | None=None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\"\"\"\n    current = root\n    while current.fea != -1:\n        if current.fea >= len(sample):\n            return -1\n        val = sample[current.fea]\n        if val not in current.child:\n            return -1\n        current = current.child[val]\n    return current.res"}
{"task_id": 36, "completion_id": 0, "solution": ""}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    p_list = []\n    for a in range(n_arms):\n        A_a = np.array(A[a])\n        A_a_inv = np.linalg.inv(A_a)\n        b_a = np.array(b[a])\n        theta_hat_a = A_a_inv.dot(b_a)\n        c_a = context[:, a]\n        term1 = theta_hat_a.dot(c_a)\n        term2 = alpha * np.sqrt(c_a.dot(A_a_inv.dot(c_a)))\n        p_a = term1 + term2\n        p_list.append(p_a)\n    selected_arm = np.argmax(p_list)\n    return selected_arm"}
{"task_id": 42, "completion_id": 0, "solution": "import numpy as np\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1.0\n    n = len(actual)\n    if n == 0:\n        return -1.0\n    for a in actual:\n        if a < 0:\n            return -1.0\n    for p in predicted:\n        if p < 0:\n            return -1.0\n    log_actual = np.log(np.array(actual) + 1)\n    log_predicted = np.log(np.array(predicted) + 1)\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    mean_squared = np.mean(squared_diff)\n    rmsle_value = np.sqrt(mean_squared)\n    return round(rmsle_value, 4)"}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        current_sum = 0\n        for n in range(N - k):\n            current_sum += x[n] * x[n + k]\n        result.append(current_sum)\n    return result"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float=1e-06, max_iter: int=1000) -> list[list[float]]:\n    \"\"\"Implement Newton's method for solving a two-variable linear least-squares problem.\"\"\"\n    n = X.shape[0]\n    w = np.array([[1.0], [1.0]])\n    for _ in range(max_iter):\n        res = X.dot(w) - y\n        grad = 2.0 / n * X.T.dot(res)\n        H = 2.0 / n * X.T.dot(X)\n        H_inv = np.linalg.pinv(H)\n        delta_w = H_inv.dot(grad)\n        w = w - delta_w\n        if np.linalg.norm(grad) < epsilon:\n            break\n    rounded_w = np.round(w, 4)\n    return rounded_w.tolist()"}
{"task_id": 65, "completion_id": 0, "solution": "def backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    N = len(pi)\n    T = len(obs)\n    if T == 0:\n        return 0.0\n    beta = [1.0 for _ in range(N)]\n    for t in range(T - 2, -1, -1):\n        new_beta = [0.0] * N\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                sum_val += A[i][j] * B[j][obs[t + 1]] * beta[j]\n            new_beta[i] = sum_val\n        beta = new_beta\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    return round(prob, 6)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float=0.01, momentum: float=0.0, prev_update: np.ndarray | None=None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    w_new = w - learning_rate * new_update\n    rounded_w = np.round(w_new, 4).tolist()\n    rounded_update = np.round(new_update, 4).tolist()\n    return (rounded_w, rounded_update)"}
{"task_id": 69, "completion_id": 0, "solution": "def forward_algorithm(S: list[float], A: list[list[float]], B: list[list[float]], observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    if not S or not A or (not B) or (not observations):\n        return -1\n    n = len(S)\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    if len(B) != n:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    T = len(observations)\n    f = [S[i] * B[i][observations[0]] for i in range(n)]\n    for t in range(1, T):\n        next_f = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += f[j] * A[j][i]\n            next_f[i] = sum_val * B[i][observations[t]]\n        f = next_f\n    result = sum(f)\n    return round(result, 4)"}
{"task_id": 71, "completion_id": 0, "solution": "def label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    for sample in data:\n        label = sample[-1]\n        label_counts[label] = label_counts.get(label, 0) + 1\n    return label_counts"}
{"task_id": 72, "completion_id": 0, "solution": ""}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef magnitude_spectrum(frames: 'np.ndarray') -> 'list[list[float]]':\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    if frames.ndim == 1:\n        frames = frames.reshape(1, -1)\n    fft = np.fft.rfft(frames, axis=1)\n    magnitude = np.abs(fft)\n    rounded_magnitude = np.round(magnitude, 4)\n    return rounded_magnitude.tolist()"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)"}
{"task_id": 78, "completion_id": 0, "solution": "def adamax_step(params: list[float], grads: list[float], m: list[float], u: list[float], t: int, learning_rate: float=0.002, beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-08) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\"\"\"\n    beta1_power_t = beta1 ** t\n    scaling = learning_rate / (1 - beta1_power_t)\n    new_params = []\n    new_m = []\n    new_u = []\n    for i in range(len(params)):\n        new_m_i = beta1 * m[i] + (1 - beta1) * grads[i]\n        new_u_i = max(beta2 * u[i], abs(grads[i]))\n        step_i = scaling * new_m_i / (new_u_i + epsilon)\n        new_params_i = params[i] - step_i\n        new_params.append(new_params_i)\n        new_m.append(new_m_i)\n        new_u.append(new_u_i)\n    new_params = [round(x, 6) for x in new_params]\n    new_m = [round(x, 6) for x in new_m]\n    new_u = [round(x, 6) for x in new_u]\n    new_t = t + 1\n    return (new_params, new_m, new_u, new_t)"}
{"task_id": 80, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = -1 / m * np.sum(sum_terms)\n    return round(cost, 6)"}
{"task_id": 84, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": "from collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample_predictions = [tree[i] for tree in predictions]\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        max_labels = [label for (label, cnt) in counts.items() if cnt == max_count]\n        max_labels.sort()\n        result.append(max_labels[0])\n    return result"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        s = np.logaddexp(0.0, z)\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()"}
{"task_id": 91, "completion_id": 0, "solution": "import numpy as np\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    if dA_np.shape != Z_np.shape:\n        return -1\n    dZ = dA_np * (Z_np > 0)\n    return dZ.tolist()"}
{"task_id": 92, "completion_id": 0, "solution": "import numbers\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and (not isinstance(a, bool))"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_np = np.array(Z)\n    if Z_np.ndim == 0:\n        Z_np = Z_np.reshape(1)\n    A_np = np.maximum(0, Z_np)\n    A = A_np.tolist()\n    cache = Z_np.tolist()\n    return (A, cache)"}
{"task_id": 94, "completion_id": 0, "solution": "from collections import Counter\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for (label, count) in counts.items() if count == max_count]\n    return min(max_labels)"}
{"task_id": 97, "completion_id": 0, "solution": "import numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        return (Z > 0).astype(float).tolist()\n    elif isinstance(Z, list):\n        return [drelu(element) for element in Z]\n    else:\n        raise ValueError('Unsupported input type')"}
{"task_id": 99, "completion_id": 0, "solution": "import numpy as np\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The function keeps the container type of the input:\n    scalar \u2192 scalar, list \u2192 list, NumPy array \u2192 NumPy array.\n\n    Args:\n        Z (int | float | list | numpy.ndarray): Input data \u2013 scalar, 1-D or 2-D.\n\n    Returns:\n        Same type as *Z* with all negative values clipped to 0.\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n    elif isinstance(Z, list):\n        arr = np.array(Z)\n        relu_arr = np.maximum(0, arr)\n        return relu_arr.tolist()\n    elif isinstance(Z, np.ndarray):\n        return np.maximum(0, Z)\n    else:\n        raise TypeError('Unsupported input type')"}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x: np.ndarray, axis: int=1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax = exp_x / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    x_arr = np.asarray(x)\n    x_arr = x_arr.flatten()\n    if x_arr.size == 0:\n        return []\n    x_arr = np.atleast_1d(x_arr)\n    exponent = -(x_arr - mean) ** 2 / (2 * sigma ** 2)\n    denominator = np.sqrt(2 * np.pi) * sigma\n    pdf = 1 / denominator * np.exp(exponent)\n    pdf_rounded = np.round(pdf, 5)\n    return pdf_rounded.tolist()"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef adam_step(theta: np.ndarray, grad: np.ndarray, m_prev: np.ndarray, v_prev: np.ndarray, t: int, alpha: float=0.01, beta1: float=0.9, beta2: float=0.99, epsilon: float=1e-09) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\"\"\"\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * grad ** 2\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    m_t_hat = m_t / (1 - beta1_power)\n    v_t_hat = v_t / (1 - beta2_power)\n    theta_new = theta - alpha * (m_t_hat / (np.sqrt(v_t_hat) + epsilon))\n    theta_new_rounded = np.round(theta_new, 6).tolist()\n    m_t_rounded = np.round(m_t, 6).tolist()\n    v_t_rounded = np.round(v_t, 6).tolist()\n    return (theta_new_rounded, m_t_rounded, v_t_rounded, t + 1)"}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\nTIME_STEPS = 20\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a string into a fixed-length list of integer token IDs.\"\"\"\n    substring = text[:time_steps]\n    encoded = []\n    for c in substring:\n        encoded.append(vocabulary.get(c, 0))\n    while len(encoded) < time_steps:\n        encoded.append(0)\n    return encoded\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token IDs back to a string, ignoring padding.\"\"\"\n    s = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        if idx in inverse_vocab:\n            s.append(inverse_vocab[idx])\n    return ''.join(s)\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes text, runs through model, and decodes the prediction.\"\"\"\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    encoded_batch = np.array([encoded])\n    predictions = model.predict(encoded_batch)\n    predicted_ids = np.argmax(predictions, axis=2)\n    predicted_ids = predicted_ids[0]\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded"}
{"task_id": 112, "completion_id": 0, "solution": "import re\nfrom collections import Counter\nclass Token:\n\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    words = re.findall('[a-zA-Z]+', text)\n    lower_words = [word.lower() for word in words]\n    counts = Counter(lower_words)\n    tokens = []\n    for (word, count) in counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    return tokens"}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef selu(x: np.ndarray, derivative: bool=False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    lambda_val = 1.0507009873554805\n    if derivative:\n        result = np.where(x >= 0, lambda_val, lambda_val * alpha * np.exp(x))\n    else:\n        result = np.where(x >= 0, lambda_val * x, lambda_val * alpha * (np.exp(x) - 1))\n    result = np.round(result, decimals=6)\n    return result.tolist()"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    y = y.ravel()\n    w = w.ravel()\n    z = X.dot(w)\n    p = 1 / (1 + np.exp(-z))\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n    loss = -(y * np.log(p) + (1 - y) * np.log(1 - p)).sum() / m\n    loss = round(loss, 4)\n    gradient = X.T.dot(p - y) / m\n    gradient = np.round(gradient, 4)\n    gradient = gradient.reshape(-1, 1).tolist()\n    return (loss, gradient)"}
{"task_id": 116, "completion_id": 0, "solution": "import numpy as np\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    N = len(y)\n    if N == 0:\n        return 0.0\n    mean_y = np.mean(y)\n    parent_mse = np.mean((y - mean_y) ** 2)\n    mse_children = 0.0\n    for split in splits:\n        len_split = len(split)\n        if len_split == 0:\n            continue\n        mean_split = np.mean(split)\n        split_mse = np.mean((split - mean_split) ** 2)\n        weight = len_split / N\n        mse_children += weight * split_mse\n    delta_mse = parent_mse - mse_children\n    return round(delta_mse, 4)"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_augmented = np.insert(X_np, 0, 1, axis=1)\n    X_pseudo = np.linalg.pinv(X_augmented)\n    weights = X_pseudo @ y_np\n    weights_rounded = np.round(weights, 4).tolist()\n    return weights_rounded"}
{"task_id": 123, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique = np.unique(y)\n    result = []\n    for element in y:\n        row = []\n        for cat in unique:\n            row.append(1 if element == cat else 0)\n        result.append(row)\n    return result"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef polynomial_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, d: int=3, gamma: float | None=None, c0: float=1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    dot_product = X_np.dot(Y_np.T)\n    C = X_np.shape[1]\n    if gamma is None:\n        gamma_val = 1.0 / C\n    else:\n        gamma_val = gamma\n    kernel = (gamma_val * dot_product + c0) ** d\n    kernel_rounded = np.round(kernel, decimals=4)\n    result = kernel_rounded.tolist()\n    return result"}
{"task_id": 127, "completion_id": 0, "solution": "def confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    unique_true = set(y_true)\n    unique_pred = set(y_pred)\n    all_classes = unique_true.union(unique_pred)\n    classes = sorted(all_classes)\n    class_to_idx = {c: i for (i, c) in enumerate(classes)}\n    num_classes = len(classes)\n    matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n    for (yt, yp) in zip(y_true, y_pred):\n        i = class_to_idx[yt]\n        j = class_to_idx[yp]\n        matrix[i][j] += 1\n    return matrix"}
{"task_id": 129, "completion_id": 0, "solution": "from itertools import islice, cycle\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    c = cycle(sequence)\n    return list(islice(c, samples))"}
{"task_id": 130, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Any, List, Tuple\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]], state: Any, action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    filtered = [t for t in transitions if t[0] == state and t[1] == action]\n    if not filtered:\n        return []\n    counts = Counter((t[3] for t in filtered))\n    total = len(filtered)\n    probs = []\n    for (next_state, count) in counts.items():\n        prob = count / total\n        rounded_prob = round(prob, 4)\n        probs.append([next_state, rounded_prob])\n    probs.sort(key=lambda x: x[0])\n    return probs"}
{"task_id": 131, "completion_id": 0, "solution": "def get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    seq = _SHARED_SEQUENCES[uid]\n    if not seq:\n        return None\n    if i >= 0:\n        effective_i = i\n    else:\n        effective_i = len(seq) + i\n    if effective_i < 0 or effective_i >= len(seq):\n        return None\n    return seq[effective_i]"}
{"task_id": 133, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    for seq in X_train:\n        for token in seq:\n            if token < 0 or token >= vocab_length:\n                return -1\n    Y_train = []\n    for seq in X_train:\n        one_hot_seq = []\n        for token in seq:\n            vector = np.zeros(vocab_length, dtype=int)\n            vector[token] = 1\n            one_hot_seq.append(vector.tolist())\n        Y_train.append(one_hot_seq)\n    return Y_train"}
{"task_id": 134, "completion_id": 0, "solution": "def best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    index = payoff_probs.index(max_p)\n    return (max_p, index)"}
{"task_id": 136, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z: np.ndarray, axis: int=-1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Divide by the sum of exponentials along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: -1).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    max_z = np.max(z, axis=axis, keepdims=True)\n    shifted_z = z - max_z\n    exp_z = np.exp(shifted_z)\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    softmax = exp_z / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()"}
{"task_id": 138, "completion_id": 0, "solution": "def accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    count = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    n = len(y_true)\n    accuracy = count / n\n    return round(accuracy, 4)"}
{"task_id": 143, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(x, alpha: float=0.2, derivative: bool=False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x) if not isinstance(x, np.ndarray) else x\n    if not derivative:\n        return np.where(x >= 0, x, alpha * x)\n    else:\n        return np.where(x >= 0, 1, alpha)"}
{"task_id": 147, "completion_id": 0, "solution": "import numpy as np\ndef logistic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, learning_rate: float=0.1, epochs: int=10000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * X_test.shape[0]\n    X_train_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test_with_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    n_features = X_train_with_bias.shape[1]\n    w = np.zeros(n_features)\n    for _ in range(epochs):\n        z = X_train_with_bias.dot(w)\n        h = 1 / (1 + np.exp(-z))\n        error = h - y_train\n        gradient = X_train_with_bias.T.dot(error) / X_train_with_bias.shape[0]\n        w = w - learning_rate * gradient\n    z_test = X_test_with_bias.dot(w)\n    h_test = 1 / (1 + np.exp(-z_test))\n    predictions = (h_test >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 148, "completion_id": 0, "solution": ""}
{"task_id": 152, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 154, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float=0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n    X_norms = np.sum(X ** 2, axis=1)\n    Y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X.dot(Y.T)\n    squared_distances = X_norms[:, np.newaxis] + Y_norms[np.newaxis, :] - 2 * dot_product\n    K = np.exp(-gamma * squared_distances)\n    K_rounded = np.round(K, 6)\n    return K_rounded.tolist()"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    return [round(x, 4) for x in action]"}
{"task_id": 157, "completion_id": 0, "solution": "def accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return 0\n    matches = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    total = len(y_true)\n    accuracy = matches / total\n    return round(accuracy, 4)"}
{"task_id": 158, "completion_id": 0, "solution": "import numpy as np\ndef ridge_regression(X: list[list[int | float]], y: list[int | float], alpha: float=1.0, fit_intercept: bool=True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    if fit_intercept:\n        intercept = np.ones((X_np.shape[0], 1))\n        X_np = np.hstack((intercept, X_np))\n    XtX = np.dot(X_np.T, X_np)\n    n_features = X_np.shape[1]\n    A = alpha * np.eye(n_features)\n    if fit_intercept:\n        A[0, 0] = 0\n    inv_matrix = np.linalg.inv(XtX + A)\n    Xty = np.dot(X_np.T, y_np)\n    beta = np.dot(inv_matrix, Xty)\n    beta_rounded = [round(x, 4) for x in beta]\n    return beta_rounded"}
{"task_id": 159, "completion_id": 0, "solution": "import numpy as np\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = x_i.shape[0]\n    term1 = d * np.log(2 * np.pi)\n    (sign, logdet) = np.linalg.slogdet(sigma)\n    term2 = logdet\n    delta = x_i - mu\n    y = np.linalg.solve(sigma, delta)\n    term3 = np.dot(delta, y)\n    total = term1 + term2 + term3\n    result = -0.5 * total\n    return round(result, 4)"}
{"task_id": 161, "completion_id": 0, "solution": "import numpy as np\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary class labels using a Factorization Machine model.\"\"\"\n    linear_part = w0 + np.dot(X, w)\n    X_V = np.dot(X, V)\n    sum_X_V_sq = np.sum(X_V ** 2, axis=1)\n    V_sq_sum = np.sum(V ** 2, axis=1)\n    X_squared = X ** 2\n    sum_X_squared_V_sq = np.dot(X_squared, V_sq_sum)\n    pairwise = 0.5 * (sum_X_V_sq - sum_X_squared_V_sq)\n    total_score = linear_part + pairwise\n    labels = np.where(total_score >= 0, 1, -1)\n    return labels.tolist()"}
{"task_id": 162, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float=1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, 'Shapes of Y_hat and Y do not match.'\n    m = Y_hat.shape[0]\n    loss = -(1 / m) * np.sum(Y * np.log(Y_hat + epsilon))\n    gradient = (Y_hat - Y) / m\n    loss = round(loss, 4)\n    gradient = np.round(gradient, 4)\n    gradient = gradient.tolist()\n    return (loss, gradient)"}
{"task_id": 163, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 166, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int=1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(len(X_train)):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n    predictions = []\n    for x in X_test:\n        s = np.dot(w, x) + b\n        if s >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions"}
{"task_id": 172, "completion_id": 0, "solution": "import numpy as np\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    z_np = np.asarray(z)\n    grad = 1 - np.tanh(z_np) ** 2\n    grad_rounded = np.round(grad, 4)\n    if z_np.ndim == 0:\n        return grad_rounded.item()\n    else:\n        return grad_rounded.tolist()"}
{"task_id": 173, "completion_id": 0, "solution": "def alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for (c, t) in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total"}
{"task_id": 175, "completion_id": 0, "solution": "import numpy as np\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to the nearest 4th decimal place.\n    \"\"\"\n    weights_array = np.array(weights)\n    sum_squares = np.sum(weights_array ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)"}
{"task_id": 179, "completion_id": 0, "solution": "import re\nimport string\ndef tokenize_chars(line: str, lowercase: bool=True, filter_punctuation: bool=True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    if lowercase:\n        line = line.lower()\n    if filter_punctuation:\n        pattern = '[{}]'.format(re.escape(string.punctuation))\n        line = re.sub(pattern, '', line)\n    line = re.sub(' {2,}', ' ', line)\n    line = line.strip()\n    if not line:\n        return []\n    return list(line)"}
{"task_id": 183, "completion_id": 0, "solution": "import numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError('Weight shape must be 2D or 4D.')\n    limit = np.sqrt(6.0 / fan_in)\n    weights = np.random.rand(*weight_shape) * (2 * limit) - limit\n    return weights"}
{"task_id": 185, "completion_id": 0, "solution": "import numpy as np\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError('Got an empty matrix.')\n    if X.ndim == 0:\n        return (1, 1)\n    elif X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, n_features)"}
{"task_id": 186, "completion_id": 0, "solution": ""}
{"task_id": 188, "completion_id": 0, "solution": "import math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    if min_epsilon >= max_epsilon or min_epsilon < 0 or decay_rate <= 0 or (step < 0):\n        return -1\n    exponent = -decay_rate * step\n    exp_term = math.exp(exponent)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * exp_term\n    epsilon = max(epsilon, min_epsilon)\n    return round(epsilon, 4)"}
{"task_id": 193, "completion_id": 0, "solution": ""}
{"task_id": 199, "completion_id": 0, "solution": "import numpy as np\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)"}
{"task_id": 200, "completion_id": 0, "solution": "import numpy as np\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    weights_array = np.array(weights)\n    if weights_array.size == 0:\n        return 0.0\n    sum_abs = np.sum(np.abs(weights_array))\n    l1_penalty = C * sum_abs\n    return round(l1_penalty, 4)"}
{"task_id": 205, "completion_id": 0, "solution": "def get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen = set()\n    result = []\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                result.append(env_id)\n    return result"}
{"task_id": 206, "completion_id": 0, "solution": "import numpy as np\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    try:\n        actual_arr = np.array(actual)\n        predicted_arr = np.array(predicted)\n    except:\n        return -1\n    if actual_arr.ndim != 1 or predicted_arr.ndim != 1:\n        return -1\n    if len(actual_arr) != len(predicted_arr):\n        return -1\n    if len(actual_arr) == 0:\n        return -1\n    try:\n        mae = np.mean(np.abs(actual_arr - predicted_arr))\n    except:\n        return -1\n    return round(mae, 4)"}
{"task_id": 211, "completion_id": 0, "solution": "def accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1.0 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    total = len(actual)\n    mismatches = sum((1 for (a, p) in zip(actual, predicted) if a != p))\n    error = mismatches / total\n    acc = 1 - error\n    return round(acc, 4)"}
{"task_id": 214, "completion_id": 0, "solution": "import numpy as np\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function must return -1 if the two input sequences do not share the\n    same length. Otherwise, it should return the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays have to be supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    a = np.array(actual)\n    p = np.array(predicted)\n    squared_diff = (a - p) ** 2\n    sum_squared = np.sum(squared_diff)\n    mse = sum_squared / n\n    return np.round(mse, 4)"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    return (gradient_list, hessian_list, probability_list)"}
{"task_id": 220, "completion_id": 0, "solution": ""}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z"}
{"task_id": 225, "completion_id": 0, "solution": "import numpy as np\ndef hz2mel(hz, formula: str='htk') -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    if formula not in ['htk', 'slaney']:\n        raise AssertionError(\"formula must be 'htk' or 'slaney'\")\n    hz_array = np.asarray(hz)\n    if formula == 'htk':\n        mel = 2595 * np.log10(1 + hz_array / 700)\n    else:\n        mel = 1127 * np.log(1 + hz_array / 700)\n    mel_rounded = np.round(mel, decimals=4)\n    if hz_array.shape == ():\n        return [mel_rounded.item()]\n    else:\n        return mel_rounded.tolist()"}
{"task_id": 228, "completion_id": 0, "solution": "import numpy as np\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None=None) -> list[list[float]]:\n    \"\"\"Compute the pairwise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If *Y* is ``None`` the function should compute the distances inside *X* itself.\n    All distances must be rounded to four decimal places and the function must\n    return a plain Python nested list (use ``tolist()`` if needed).\n\n    Args:\n        X: NumPy array with shape (N, C).\n        Y: NumPy array with shape (M, C) or ``None``.  Default is ``None``.\n\n    Returns:\n        Nested list representing the N \u00d7 M matrix of Euclidean distances.\n    \"\"\"\n    if Y is None:\n        Y = X\n    x_norms = np.sum(X ** 2, axis=1)\n    y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X @ Y.T\n    D_sq = x_norms[:, np.newaxis] + y_norms[np.newaxis, :] - 2 * dot_product\n    D_sq = np.maximum(D_sq, 0)\n    D = np.sqrt(D_sq)\n    D_rounded = np.round(D, 4)\n    return D_rounded.tolist()"}
{"task_id": 231, "completion_id": 0, "solution": "def is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack.\n    \n    A natural blackjack is a hand with exactly two cards: one Ace (1) and one ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1"}
{"task_id": 232, "completion_id": 0, "solution": "import numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    if x.ndim != 2:\n        return False\n    if not np.all(np.isin(x, [0, 1])):\n        return False\n    row_sums = x.sum(axis=1)\n    if not np.all(row_sums == 1):\n        return False\n    return True"}
{"task_id": 234, "completion_id": 0, "solution": "import numpy as np\ndef thompson_sampling_beta_binomial(successes: list[int], failures: list[int], prior_alpha: float=1.0, prior_beta: float=1.0, seed: int | None=None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\"\"\"\n    if len(successes) != len(failures) or len(successes) == 0:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    K = len(successes)\n    samples = []\n    for i in range(K):\n        alpha = prior_alpha + successes[i]\n        beta = prior_beta + failures[i]\n        sample = np.random.beta(alpha, beta)\n        samples.append(sample)\n    selected_arm = np.argmax(samples)\n    return selected_arm"}
{"task_id": 235, "completion_id": 0, "solution": ""}
{"task_id": 236, "completion_id": 0, "solution": "import numpy as np\ndef minibatch(X: np.ndarray, batchsize: int=256, shuffle: bool=True):\n    \"\"\"Create a generator that yields index mini-batches.\"\"\"\n    N = X.shape[0]\n    idx = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(idx)\n    n_batches = (N + batchsize - 1) // batchsize\n\n    def gen():\n        for i in range(0, N, batchsize):\n            yield idx[i:i + batchsize]\n    return (gen(), n_batches)"}
{"task_id": 237, "completion_id": 0, "solution": "from typing import List\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\"\"\"\n    total = 0\n    aces = 0\n    for card in hand:\n        if card in ['J', 'Q', 'K']:\n            total += 10\n        elif card == 'A':\n            total += 11\n            aces += 1\n        else:\n            total += int(card)\n    while total > 21 and aces > 0:\n        total -= 10\n        aces -= 1\n    return total if total <= 21 else 0"}
{"task_id": 239, "completion_id": 0, "solution": "def determine_padding(filter_shape: tuple[int, int], output_shape: str='same') -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\"\"\"\n    if output_shape == 'valid':\n        return ((0, 0), (0, 0))\n    else:\n        (f_height, f_width) = filter_shape\n        total_pad_height = f_height - 1\n        pad_top = total_pad_height // 2\n        pad_bottom = total_pad_height - pad_top\n        total_pad_width = f_width - 1\n        pad_left = total_pad_width // 2\n        pad_right = total_pad_width - pad_left\n        return ((pad_top, pad_bottom), (pad_left, pad_right))"}
{"task_id": 240, "completion_id": 0, "solution": "def create_encoding_dicts(obs_ids: list, action_ids: list, obs_encoder: dict | None=None, act_encoder: dict | None=None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    if obs_encoder is not None:\n        obs2num = obs_encoder.copy()\n    elif not obs_ids:\n        obs2num = {}\n    else:\n        obs2num = {id: idx for (idx, id) in enumerate(obs_ids)}\n    num2obs = {v: k for (k, v) in obs2num.items()}\n    if act_encoder is not None:\n        act2num = act_encoder.copy()\n    elif not action_ids:\n        act2num = {}\n    else:\n        act2num = {id: idx for (idx, id) in enumerate(action_ids)}\n    num2act = {v: k for (k, v) in act2num.items()}\n    return {'obs2num': obs2num, 'num2obs': num2obs, 'act2num': act2num, 'num2act': num2act}"}
{"task_id": 242, "completion_id": 0, "solution": "import math\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float=1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    for i in range(len(pull_counts)):\n        if pull_counts[i] == 0:\n            return i\n    T = current_step + 1\n    ln_T = math.log(T)\n    scores = []\n    for a in range(len(ev_estimates)):\n        score = ev_estimates[a] + C * math.sqrt(2 * ln_T / pull_counts[a])\n        scores.append(score)\n    max_score = max(scores)\n    selected_arm = scores.index(max_score)\n    return selected_arm"}
{"task_id": 243, "completion_id": 0, "solution": "import numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    z1 = np.maximum(0, state.dot(W1) + b1)\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    z2 = np.maximum(0, z1.dot(W2) + b2)\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    z3 = np.maximum(0, z2.dot(W3) + b3)\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    logits = z3.dot(W_a) + b_a\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    state_value = z3.dot(W_c) + b_c\n    state_value = state_value[0]\n    action_probs = [round(p, 4) for p in action_probs]\n    state_value = round(state_value, 4)\n    return (action_probs, state_value)"}
{"task_id": 244, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple, sparsity: float=0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    if not isinstance(shape, tuple):\n        return -1\n    if not shape:\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    if isinstance(sparsity, bool):\n        return -1\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if not 0 <= sparsity <= 1:\n        return -1\n    tensor = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (tensor >= threshold).astype(float)\n    return binary_tensor.tolist()"}
{"task_id": 247, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]], payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Determine the arm with the highest expected reward in a Gaussian bandit.\n\n    The function must compute the expected reward for each arm (p_k * \u03bc_k) and\n    return a tuple (best_ev, best_arm), where best_ev is rounded to 4 decimal\n    places and best_arm is the index of the arm with that expected reward.  If\n    the input is invalid (see task description) the function should return -1.\n    \"\"\"\n    if len(payoff_dists) != len(payoff_probs) or len(payoff_dists) == 0:\n        return -1\n    for i in range(len(payoff_dists)):\n        p = payoff_probs[i]\n        if p < 0 or p > 1:\n            return -1\n        (mu, sigma_sq) = payoff_dists[i]\n        if sigma_sq <= 0:\n            return -1\n    mu = np.array([d[0] for d in payoff_dists])\n    p = np.array(payoff_probs)\n    expected_rewards = p * mu\n    max_ev = np.max(expected_rewards)\n    best_arm = np.argmax(expected_rewards)\n    best_ev = round(max_ev, 4)\n    return (best_ev, best_arm)"}
{"task_id": 248, "completion_id": 0, "solution": "import numpy as np\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\"\"\"\n    p = np.sum(y_train) / len(y_train)\n    n_test = X_test.shape[0]\n    if n_test == 0:\n        return []\n    predictions = np.full(n_test, p)\n    predictions = np.round(predictions, 4)\n    return predictions.tolist()"}
{"task_id": 252, "completion_id": 0, "solution": "import numpy as np\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)"}
{"task_id": 253, "completion_id": 0, "solution": "import numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    if gradient:\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        gradient = [round(x, 4) for x in gradient]\n        return gradient\n    else:\n        l2_norm = np.linalg.norm(w, 2)\n        sum_w_sq = np.dot(w, w)\n        l1_part = l1_ratio * l2_norm\n        l2_part = (1 - l1_ratio) * 0.5 * sum_w_sq\n        penalty = alpha * (l1_part + l2_part)\n        return round(penalty, 4)"}
{"task_id": 255, "completion_id": 0, "solution": "def usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 without causing the\n    hand's total value to exceed 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    aces = hand.count(1)\n    if aces == 0:\n        return False\n    sum_aces_1 = sum(hand)\n    return sum_aces_1 + 10 <= 21"}
{"task_id": 258, "completion_id": 0, "solution": "import numpy as np\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    product = actual * predicted\n    per_sample_loss = np.maximum(0, 1 - product)\n    mean_loss = np.mean(per_sample_loss)\n    return round(mean_loss, 4)"}
{"task_id": 259, "completion_id": 0, "solution": "import random\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Randomly samples *batch_size* transitions from the replay buffer.\n\n    The selection must be deterministic (use random.seed(42)).  If\n    *batch_size* exceeds the buffer length, return -1.\n    \"\"\"\n    if batch_size > len(memory):\n        return -1\n    random.seed(42)\n    return random.sample(memory, batch_size)"}
{"task_id": 261, "completion_id": 0, "solution": ""}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for (i, v) in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    for (u, v) in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    return G"}
{"task_id": 269, "completion_id": 0, "solution": "def count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\"\"\"\n    layer1 = state_dimensions * 164 + 164\n    layer2 = 164 * action_dimensions + action_dimensions\n    return layer1 + layer2"}
{"task_id": 277, "completion_id": 0, "solution": "def is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env['action_space'], (tuple, dict))\n    tuple_obs = isinstance(env['observation_space'], (tuple, dict))\n    return (tuple_action, tuple_obs)"}
{"task_id": 278, "completion_id": 0, "solution": "def evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        else:\n            feature_i = current_node['feature_i']\n            threshold = current_node['threshold']\n            if sample[feature_i] <= threshold:\n                current_node = current_node['true_branch']\n            else:\n                current_node = current_node['false_branch']"}
{"task_id": 280, "completion_id": 0, "solution": "import numpy as np\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool=False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector or matrix.\n    alpha : float\n        Regularization strength (must be non-negative).\n    return_grad : bool, optional (default=False)\n        If True, return the gradient of the regularization term instead of the\n        regularization value itself.\n\n    Returns\n    -------\n    float or np.ndarray\n        \u2022 When *return_grad* is False: the scalar L2 regularization value.\n        \u2022 When *return_grad* is True:  the gradient with the same shape as *w*.\n    \"\"\"\n    if return_grad:\n        grad = alpha * w\n        grad = np.round(grad, decimals=4)\n        return grad\n    else:\n        loss = 0.5 * alpha * np.sum(w ** 2)\n        loss = np.round(loss, decimals=4)\n        return float(loss)"}
{"task_id": 281, "completion_id": 0, "solution": "import numpy as np\ndef L1Regularization(alpha: float):\n    \"\"\"Create a callable object that implements L1 regularisation.\n\n    The returned object must support two operations:\n    1. call with a NumPy array to obtain the L1 penalty value\n    2. call its .grad(w) method to obtain the gradient of the penalty\n\n    Args:\n        alpha: Non-negative float controlling the strength of the regularisation.\n\n    Returns:\n        An instance that fulfils the described interface.\n    \"\"\"\n\n    class L1Regularizer:\n\n        def __init__(self, alpha):\n            self.alpha = alpha\n\n        def __call__(self, w):\n            if self.alpha == 0:\n                return 0.0\n            return self.alpha * np.sum(np.abs(w))\n\n        def grad(self, w):\n            if self.alpha == 0:\n                return np.zeros_like(w)\n            return self.alpha * np.sign(w)\n    return L1Regularizer(alpha)"}
{"task_id": 285, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    if x.ndim == 1:\n        x_reshaped = x.reshape(1, -1)\n    else:\n        x_reshaped = x\n    if y.ndim == 1:\n        y_reshaped = y.reshape(1, -1)\n    else:\n        y_reshaped = y\n    if x_reshaped.shape[1] != y_reshaped.shape[1]:\n        return -1\n    kernel = x_reshaped @ y_reshaped.T\n    return kernel.tolist()"}
{"task_id": 286, "completion_id": 0, "solution": ""}
{"task_id": 289, "completion_id": 0, "solution": "def format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return ''\n    formatted = []\n    for (field, value) in entries:\n        field_str = str(field)\n        value_str = str(value)\n        formatted_line = f'{field_str}: \"{value_str}\"'\n        formatted.append(formatted_line)\n    return '\\n'.join(formatted)"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if type(tree_a) != type(tree_b):\n        return False\n    if isinstance(tree_a, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-08)\n    else:\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-08):\n            return False\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        return True"}
{"task_id": 293, "completion_id": 0, "solution": "def map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    processed_name = agent_name.strip().lower()\n    agent_map = {'dqn': ('CartPole-v0', 'unwrapped'), 'ddpg': ('Pendulum-v0', 'env'), 'a3c': ('Pendulum-v0', 'unwrapped'), 'a2c': ('CartPole-v0', 'env'), 'a2c_multi': ('CartPole-v0', 'raw'), 'trpo': ('Pendulum-v0', 'unwrapped')}\n    return agent_map.get(processed_name, -1)"}
{"task_id": 328, "completion_id": 0, "solution": "from typing import List\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []"}
{"task_id": 364, "completion_id": 0, "solution": "def sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0"}
{"task_id": 378, "completion_id": 0, "solution": "def indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0"}
{"task_id": 379, "completion_id": 0, "solution": "def sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    return [node['key'] for node in sorted_nodes]"}
{"task_id": 399, "completion_id": 0, "solution": "import numpy as np\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    X_squared = np.sum(X ** 2, axis=1)\n    Y_squared = np.sum(Y ** 2, axis=1)\n    X_dot_YT = X.dot(Y.T)\n    squared_dist = X_squared[:, np.newaxis] + Y_squared - 2 * X_dot_YT\n    dist = np.sqrt(squared_dist)\n    rounded_dist = np.round(dist, 4)\n    return rounded_dist.tolist()"}
{"task_id": 420, "completion_id": 0, "solution": "import numpy as np\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    col = X[:, column]\n    left_mask = (col < value).tolist()\n    right_mask = (col >= value).tolist()\n    return (left_mask, right_mask)"}
{"task_id": 424, "completion_id": 0, "solution": "import numpy as np\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    (unique_labels, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    sum_terms = np.sum(probabilities * np.log2(probabilities))\n    H = -sum_terms\n    return round(H, 4)"}
{"task_id": 429, "completion_id": 0, "solution": "import numpy as np\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, -1).\n    \"\"\"\n    if context.ndim != 1:\n        return (-1, -1)\n    D = context_probs.shape[0]\n    if context.size != D:\n        return (-1, -1)\n    if np.sum(context) != 1:\n        return (-1, -1)\n    if not np.all(np.isin(context, [0, 1])):\n        return (-1, -1)\n    current_context = np.argmax(context)\n    row = context_probs[current_context]\n    optimal_arm = np.argmax(row)\n    optimal_rwd = row[optimal_arm]\n    optimal_rwd = np.round(optimal_rwd, 4)\n    return (optimal_rwd, optimal_arm)"}
