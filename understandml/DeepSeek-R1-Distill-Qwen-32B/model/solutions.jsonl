{"task_id": 16, "completion_id": 0, "solution": "from typing import Any, Iterable, List, Tuple\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i + N]) for i in range(L - N + 1)]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_svm_predict(X_train: np.ndarray, y_train: np.ndarray, alpha: np.ndarray, b: float, gamma: float, X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    dot_products = X_train.dot(X_test.T)\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    K = np.exp(-gamma * dist_sq)\n    alpha_y = alpha * y_train\n    g = alpha_y.dot(K) + b\n    signs = np.where(g >= 0, 1, -1)\n    return signs.tolist()"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef random_binary_tensor(shape: tuple[int, ...], sparsity: float=0.5, seed: int | None=None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not 0 <= sparsity <= 1:\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array"}
{"task_id": 36, "completion_id": 0, "solution": ""}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 71, "completion_id": 0, "solution": "def label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    for sample in data:\n        label = sample[-1]\n        label_counts[label] = label_counts.get(label, 0) + 1\n    return label_counts"}
{"task_id": 72, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)"}
{"task_id": 80, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 81, "completion_id": 0, "solution": "import numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = -1 / m * np.sum(sum_terms)\n    return round(cost, 6)"}
{"task_id": 84, "completion_id": 0, "solution": ""}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)"}
{"task_id": 91, "completion_id": 0, "solution": "import numpy as np\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    if dA_np.shape != Z_np.shape:\n        return -1\n    dZ = dA_np * (Z_np > 0)\n    return dZ.tolist()"}
{"task_id": 92, "completion_id": 0, "solution": "import numbers\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and (not isinstance(a, bool))"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_np = np.array(Z)\n    if Z_np.ndim == 0:\n        Z_np = Z_np.reshape(1)\n    A_np = np.maximum(0, Z_np)\n    A = A_np.tolist()\n    cache = Z_np.tolist()\n    return (A, cache)"}
{"task_id": 94, "completion_id": 0, "solution": "from collections import Counter\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for (label, count) in counts.items() if count == max_count]\n    return min(max_labels)"}
{"task_id": 97, "completion_id": 0, "solution": "import numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        return (Z > 0).astype(float).tolist()\n    elif isinstance(Z, list):\n        return [drelu(element) for element in Z]\n    else:\n        raise ValueError('Unsupported input type')"}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x: np.ndarray, axis: int=1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax = exp_x / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()"}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f'input:  {example}')\n        print(f'output: {predicted_str}')\n        predictions.append(predicted_str)\n    return predictions"}
{"task_id": 123, "completion_id": 0, "solution": "import numpy as np\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique = np.unique(y)\n    result = []\n    for element in y:\n        row = []\n        for cat in unique:\n            row.append(1 if element == cat else 0)\n        result.append(row)\n    return result"}
{"task_id": 129, "completion_id": 0, "solution": "from itertools import islice, cycle\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    c = cycle(sequence)\n    return list(islice(c, samples))"}
{"task_id": 131, "completion_id": 0, "solution": "def get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    seq = _SHARED_SEQUENCES[uid]\n    if not seq:\n        return None\n    if i >= 0:\n        effective_i = i\n    else:\n        effective_i = len(seq) + i\n    if effective_i < 0 or effective_i >= len(seq):\n        return None\n    return seq[effective_i]"}
{"task_id": 134, "completion_id": 0, "solution": "def best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    index = payoff_probs.index(max_p)\n    return (max_p, index)"}
{"task_id": 138, "completion_id": 0, "solution": "def accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    count = sum((1 for (a, b) in zip(y_true, y_pred) if a == b))\n    n = len(y_true)\n    accuracy = count / n\n    return round(accuracy, 4)"}
{"task_id": 143, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(x, alpha: float=0.2, derivative: bool=False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x) if not isinstance(x, np.ndarray) else x\n    if not derivative:\n        return np.where(x >= 0, x, alpha * x)\n    else:\n        return np.where(x >= 0, 1, alpha)"}
{"task_id": 148, "completion_id": 0, "solution": ""}
{"task_id": 154, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float=0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n    X_norms = np.sum(X ** 2, axis=1)\n    Y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X.dot(Y.T)\n    squared_distances = X_norms[:, np.newaxis] + Y_norms[np.newaxis, :] - 2 * dot_product\n    K = np.exp(-gamma * squared_distances)\n    K_rounded = np.round(K, 6)\n    return K_rounded.tolist()"}
{"task_id": 155, "completion_id": 0, "solution": "import numpy as np\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    return [round(x, 4) for x in action]"}
{"task_id": 166, "completion_id": 0, "solution": "import numpy as np\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int=1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(len(X_train)):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n    predictions = []\n    for x in X_test:\n        s = np.dot(w, x) + b\n        if s >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions"}
{"task_id": 173, "completion_id": 0, "solution": "def alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for (c, t) in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total"}
{"task_id": 175, "completion_id": 0, "solution": "import numpy as np\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to the nearest 4th decimal place.\n    \"\"\"\n    weights_array = np.array(weights)\n    sum_squares = np.sum(weights_array ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)"}
{"task_id": 183, "completion_id": 0, "solution": "import numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    if len(weight_shape) == 2:\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        raise ValueError('Weight shape must be 2D or 4D.')\n    limit = np.sqrt(6.0 / fan_in)\n    weights = np.random.rand(*weight_shape) * (2 * limit) - limit\n    return weights"}
{"task_id": 185, "completion_id": 0, "solution": "import numpy as np\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError('Got an empty matrix.')\n    if X.ndim == 0:\n        return (1, 1)\n    elif X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, n_features)"}
{"task_id": 186, "completion_id": 0, "solution": ""}
{"task_id": 193, "completion_id": 0, "solution": ""}
{"task_id": 199, "completion_id": 0, "solution": "import numpy as np\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)"}
{"task_id": 205, "completion_id": 0, "solution": "def get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen = set()\n    result = []\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                result.append(env_id)\n    return result"}
{"task_id": 217, "completion_id": 0, "solution": "import numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    return (gradient_list, hessian_list, probability_list)"}
{"task_id": 220, "completion_id": 0, "solution": ""}
{"task_id": 224, "completion_id": 0, "solution": "import numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z"}
{"task_id": 231, "completion_id": 0, "solution": "def is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack.\n    \n    A natural blackjack is a hand with exactly two cards: one Ace (1) and one ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1"}
{"task_id": 235, "completion_id": 0, "solution": ""}
{"task_id": 252, "completion_id": 0, "solution": "import numpy as np\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)"}
{"task_id": 261, "completion_id": 0, "solution": ""}
{"task_id": 266, "completion_id": 0, "solution": "from typing import Any, List, Tuple\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for (i, v) in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    for (u, v) in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    return G"}
{"task_id": 269, "completion_id": 0, "solution": "def count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\"\"\"\n    layer1 = state_dimensions * 164 + 164\n    layer2 = 164 * action_dimensions + action_dimensions\n    return layer1 + layer2"}
{"task_id": 277, "completion_id": 0, "solution": "def is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env['action_space'], (tuple, dict))\n    tuple_obs = isinstance(env['observation_space'], (tuple, dict))\n    return (tuple_action, tuple_obs)"}
{"task_id": 278, "completion_id": 0, "solution": "def evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        else:\n            feature_i = current_node['feature_i']\n            threshold = current_node['threshold']\n            if sample[feature_i] <= threshold:\n                current_node = current_node['true_branch']\n            else:\n                current_node = current_node['false_branch']"}
{"task_id": 285, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    if x.ndim == 1:\n        x_reshaped = x.reshape(1, -1)\n    else:\n        x_reshaped = x\n    if y.ndim == 1:\n        y_reshaped = y.reshape(1, -1)\n    else:\n        y_reshaped = y\n    if x_reshaped.shape[1] != y_reshaped.shape[1]:\n        return -1\n    kernel = x_reshaped @ y_reshaped.T\n    return kernel.tolist()"}
{"task_id": 286, "completion_id": 0, "solution": ""}
{"task_id": 289, "completion_id": 0, "solution": "def format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return ''\n    formatted = []\n    for (field, value) in entries:\n        field_str = str(field)\n        value_str = str(value)\n        formatted_line = f'{field_str}: \"{value_str}\"'\n        formatted.append(formatted_line)\n    return '\\n'.join(formatted)"}
{"task_id": 290, "completion_id": 0, "solution": "import numpy as np\nfrom dataclasses import dataclass\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    if type(tree_a) != type(tree_b):\n        return False\n    if isinstance(tree_a, Leaf):\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-08)\n    else:\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-08):\n            return False\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        return True"}
{"task_id": 328, "completion_id": 0, "solution": "from typing import List\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []"}
{"task_id": 364, "completion_id": 0, "solution": "def sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0"}
{"task_id": 378, "completion_id": 0, "solution": "def indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0"}
{"task_id": 379, "completion_id": 0, "solution": "def sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    return [node['key'] for node in sorted_nodes]"}
{"task_id": 399, "completion_id": 0, "solution": "import numpy as np\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    X_squared = np.sum(X ** 2, axis=1)\n    Y_squared = np.sum(Y ** 2, axis=1)\n    X_dot_YT = X.dot(Y.T)\n    squared_dist = X_squared[:, np.newaxis] + Y_squared - 2 * X_dot_YT\n    dist = np.sqrt(squared_dist)\n    rounded_dist = np.round(dist, 4)\n    return rounded_dist.tolist()"}
{"task_id": 420, "completion_id": 0, "solution": "import numpy as np\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    col = X[:, column]\n    left_mask = (col < value).tolist()\n    right_mask = (col >= value).tolist()\n    return (left_mask, right_mask)"}
{"task_id": 424, "completion_id": 0, "solution": "import numpy as np\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    (unique_labels, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    sum_terms = np.sum(probabilities * np.log2(probabilities))\n    H = -sum_terms\n    return round(H, 4)"}
{"task_id": 429, "completion_id": 0, "solution": "import numpy as np\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, -1).\n    \"\"\"\n    if context.ndim != 1:\n        return (-1, -1)\n    D = context_probs.shape[0]\n    if context.size != D:\n        return (-1, -1)\n    if np.sum(context) != 1:\n        return (-1, -1)\n    if not np.all(np.isin(context, [0, 1])):\n        return (-1, -1)\n    current_context = np.argmax(context)\n    row = context_probs[current_context]\n    optimal_arm = np.argmax(row)\n    optimal_rwd = row[optimal_arm]\n    optimal_rwd = np.round(optimal_rwd, 4)\n    return (optimal_rwd, optimal_arm)"}
