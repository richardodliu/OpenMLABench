{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate all contiguous N-grams of a given sequence. An N-gram is a sliding window of length N taken from consecutive elements of the sequence. The function should handle various types of sequences, including lists, tuples, and strings, and return the N-grams as a list of tuples.\n\n### Approach\n1. **Check Sequence Length**: First, determine the length of the sequence. If the length of the sequence is less than N, return an empty list since it's impossible to form any N-gram.\n2. **Generate N-grams**: For each possible starting index in the sequence, extract a sub-sequence of length N and convert it into a tuple. Collect all these tuples into a list and return it.\n\nThe approach leverages the ability of sequences to be sliced, which allows us to efficiently generate each N-gram by iterating through valid starting indices.\n\n### Solution Code\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i+N]) for i in range(L - N + 1)]\n```\n\n### Explanation\n1. **Check Sequence Length**: The function first calculates the length of the sequence. If N is greater than this length, it immediately returns an empty list.\n2. **Generate N-grams**: Using a list comprehension, the function iterates over all valid starting indices (from 0 to `len(sequence) - N`). For each index, it slices the sequence to get a sub-sequence of length N, converts this sub-sequence into a tuple, and collects all such tuples into a list.\n\nThis approach ensures that all possible contiguous N-grams are generated efficiently and correctly, handling various types of input sequences uniformly."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy array filled with zeros, similar to NumPy's `zeros` function. The function should accept a shape parameter and an optional dtype parameter to specify the data type of the resulting array.\n\n### Approach\n1. **Shape Handling**: The shape parameter can be an integer, a tuple, or a list. If it's an integer, the function will create a 1-D array of that length. If it's a tuple or list, it will create a multi-dimensional array with the specified dimensions.\n2. **Data Type Handling**: The function accepts an optional keyword argument `dtype` to specify the data type of the array. If `dtype` is not provided, the default data type will be `float`, which corresponds to `np.float64` in NumPy.\n3. **Return Type**: The function returns a NumPy ndarray filled with zeros, constructed using NumPy's `zeros` function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)\n```\n\n### Explanation\n- **Shape Parameter**: The function handles both integer and iterable (tuple/list) shapes. If an integer is provided, it creates a 1-D array of that length. If a tuple or list is provided, it creates a multi-dimensional array with the specified dimensions.\n- **Data Type Handling**: The function checks for the presence of the `dtype` keyword argument. If provided, it uses the specified data type; otherwise, it defaults to `float`.\n- **Return Value**: The function uses NumPy's `zeros` function to create and return the array, ensuring the result is a NumPy ndarray filled with zeros.\n\nThis approach ensures that the function is lightweight and leverages NumPy's built-in capabilities to handle array creation efficiently."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function - RBF) kernel. The decision function for each test sample is computed using the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\n1. **Compute Squared Norms**: Calculate the squared Euclidean norms of the training and test samples. This helps in efficiently computing the squared distances between each training sample and each test sample.\n2. **Dot Products**: Compute the dot products between each training sample and each test sample. This is used to efficiently calculate the squared distances.\n3. **Squared Distances**: Using the squared norms and dot products, compute the squared Euclidean distances between each training sample and each test sample.\n4. **Kernel Matrix**: Apply the Gaussian kernel function to the squared distances to get the kernel matrix, which measures the similarity between samples.\n5. **Decision Function**: Compute the decision function for each test sample using the Lagrange multipliers, class labels, kernel matrix, and bias term.\n6. **Predict Labels**: Determine the predicted class labels based on the sign of the decision function values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    # Compute squared norms of training and test samples\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    \n    # Compute dot products between training and test samples\n    dot_products = X_train.dot(X_test.T)\n    \n    # Calculate squared distances using the expanded form\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * dist_sq)\n    \n    # Multiply alpha with corresponding y_train values\n    alpha_y = alpha * y_train\n    \n    # Compute the decision function for each test sample\n    g = alpha_y.dot(K) + b\n    \n    # Determine the predicted labels based on the sign of g\n    signs = np.where(g >= 0, 1, -1)\n    \n    return signs.tolist()\n```\n\n### Explanation\n1. **Squared Norms**: The squared norms of the training and test samples are computed to facilitate the efficient calculation of squared distances.\n2. **Dot Products**: The dot products between training and test samples are computed to avoid redundant calculations when determining distances.\n3. **Squared Distances**: Using the squared norms and dot products, the squared distances between each training and test sample are computed efficiently.\n4. **Kernel Matrix**: The Gaussian kernel function transforms these squared distances into similarity scores, which are stored in a matrix.\n5. **Decision Function**: The decision function for each test sample is computed by taking the weighted sum of the kernel values (using Lagrange multipliers and class labels) and adding the bias term.\n6. **Predict Labels**: The sign of the decision function values determines the predicted class labels, ensuring each label is either 1 or -1.\n\nThis approach ensures that the solution is both efficient and scalable, leveraging vectorized operations to handle arbitrary numbers of training and test samples."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy tensor filled with 0.0s and 1.0s based on a specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and return the appropriate output.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Setting**: If a seed is provided, set the random seed to ensure reproducible results.\n3. **Random Array Generation**: Generate a random array of the specified shape using a uniform distribution between 0 and 1.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity`. This threshold determines the probability of an element being 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where each element is 1.0 if it is greater than or equal to the threshold, otherwise 0.0. This is done using a boolean mask and casting to float to ensure the correct data type.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns -1 immediately.\n- **Seed Setting**: If a seed is provided, it sets the random seed to ensure that the same sequence of random numbers is generated each time the function is called with the same seed.\n- **Random Array Generation**: Using `np.random.rand`, the function generates a random array of the specified shape with values uniformly distributed between 0 and 1.\n- **Threshold Calculation**: The threshold is calculated as `1 - sparsity`, which determines the cutoff for converting random values to 1.0.\n- **Binary Conversion**: The random values are compared against the threshold. Values greater than or equal to the threshold are converted to 1.0, and others to 0.0. The result is cast to a float array to ensure the correct data type.\n\nThis approach ensures that the function efficiently generates the required binary tensor while adhering to the specified constraints and providing reproducible results when a seed is provided."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a binary decision tree for each sample in a given dataset and predict the outcome based on the tree's structure. The tree is represented using a `Node` class where each node can either be a leaf node (storing a prediction) or a non-leaf node (storing a feature index and a threshold for splitting).\n\n### Approach\n1. **Check for an Empty Tree**: If the root node is `None`, return an empty list as there are no predictions to make.\n2. **Traverse the Tree for Each Sample**: For each sample in the dataset, start from the root node and traverse down the tree according to the decision rules:\n   - If the current node is a leaf (both left and right children are"}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the hyperbolic tangent (tanh) activation function and its derivative. The function should handle both cases where the input is a"}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to count the occurrences of each class label in a given tabular dataset. Each sample in the dataset is represented as a list, where the last element is the class label. The goal is to return a dictionary where the keys are the unique class labels and the values are their respective counts.\n\n### Approach\n1. **Check for Empty Data**: If the input list is empty, return an empty dictionary immediately.\n2. **Initialize a Dictionary**: Create an empty dictionary to keep track of the counts of each class label.\n3. **Iterate Through Samples**: For each sample in the dataset, extract the class label (the last element of the sample list).\n4. **Update Counts**: Use the dictionary's `get` method to update the count for each class label. If the label is already a key in the dictionary, increment its count by 1. If it is not, add the label as a new key with an initial count of 1.\n5. **Return the Result**: After processing all samples, return the dictionary containing the counts of each class label.\n\nThis approach ensures that we efficiently count the occurrences of each class label with a single pass through the dataset, making the solution both time and space efficient.\n\n### Solution Code\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    for sample in data:\n        label = sample[-1]\n        label_counts[label] = label_counts.get(label, 0) + 1\n    return label_counts\n```\n\n### Explanation\n- **Empty Data Check**: The function first checks if the input list is empty and returns an empty dictionary if true.\n- **Dictionary Initialization**: An empty dictionary `label_counts` is initialized to store the counts of each class label.\n- **Sample Iteration**: The function iterates over each sample in the dataset. For each sample, it extracts the class label using `sample[-1]`.\n- **Count Update**: Using the `get` method of dictionaries, the function updates the count for each class label. This method efficiently handles both existing and new labels.\n- **Return Result**: After processing all samples, the function returns the dictionary with the counts of each class label.\n\nThis solution efficiently handles the task with a time complexity of O(n), where n is the number of samples in the dataset, ensuring optimal performance even for large datasets."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the variance reduction achieved by splitting a node into left and right children in a regression tree. The variance reduction is a measure of how much the split reduces the variance of the target values.\n\n###"}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the unnormalized categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in machine learning, especially in classification tasks where the output is a probability distribution over classes.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Inputs to NumPy Arrays**: Ensure that both the true labels `y` and the predicted probabilities `y_pred` are converted to NumPy arrays for efficient element-wise operations.\n\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to each element of `y_pred`. This ensures that the logarithm operation is always applied to a positive number.\n\n3. **Compute Logarithms**: Calculate the natural logarithm of each element in the modified `y_pred` array.\n\n4. **Element-wise Multiplication**: Multiply each element of `y` with the corresponding element in the logarithm array. This step effectively zeros out all terms where the true label is not the correct class, as `y` is one-hot encoded.\n\n5. **Sum and Negate**: Sum all the resulting values and negate the sum to get the final loss value. This is because the cross-entropy loss is defined as the negative sum of the log probabilities.\n\n6. **Round the Result**: Finally, round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can leverage NumPy's efficient array operations, which are crucial for handling potentially large datasets efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in negative infinity and cause numerical instability in the loss calculation.\n- **Logarithm Calculation**: The natural logarithm of each predicted probability is computed to transform the multiplicative probabilities into additive terms, which is a standard step in cross-entropy calculations.\n- **Element-wise Multiplication**: This step ensures that only the log probability of the correct class contributes to the loss for each sample, as all other terms are multiplied by zero.\n- **Sum and Negate**: The sum of these contributions is taken and negated to align with the definition of cross-entropy loss, which measures the difference between the predicted and true distributions.\n- **Rounding**: The final loss value is rounded to four decimal places to provide a precise and readable result.\n\nThis approach efficiently computes the categorical cross-entropy loss while ensuring numerical stability and correctness."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single update step of the Adadelta optimization algorithm. Adadelta is an adaptive learning rate method that adjusts the learning rate for each parameter based on the history of gradients and parameter updates. This method helps in avoiding the need for manually setting the learning rate, which can be a tedious task.\n\n### Approach\nThe Adadelta algorithm maintains two running averages for each parameter:\n1. `E_grad` (running average of the squared gradients)\n2. `E_delta` (running average of the squared parameter updates)\n\nFor the first update step, both `E_grad` and `E_delta` are initialized to zero. The update rules for each parameter are as follows:\n1. Compute the running average of the squared gradients.\n2. Compute the root mean square (RMS) of the gradients.\n3. Compute the RMS of the parameter updates, which starts as the square root of a small constant `\u03b5` since `E_delta` is zero initially.\n4. Calculate the adaptive learning rate as the ratio of the RMS of the parameter updates to the RMS of the gradients.\n5. Compute the parameter update using the adaptive learning rate.\n6. Update the parameter using the computed update.\n\nThe steps are implemented element-wise for any input shape, ensuring that the function works efficiently for both 1-D and n-D inputs.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform one Adadelta optimisation step.\"\"\"\n    w_np = np.array(w)\n    grad_np = np.array(grad_w)\n    \n    e_grad = (1 - rho) * (grad_np ** 2)\n    rms_grad = np.sqrt(e_grad + eps)\n    rms_delta = np.sqrt(eps)\n    adaptive_lr = rms_delta / rms_grad\n    delta_w = adaptive_lr * grad_np\n    \n    new_w = w_np - delta_w\n"}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost, also known as log-loss, between the predicted probabilities and the ground-truth binary labels. This cost function is commonly used in binary classification problems to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clip Values**: To avoid taking the logarithm of zero or one, which is undefined, we clip the predicted probabilities to a small value \u03b5 (1e-15) and 1 - \u03b5. This ensures that all values are within the valid range for logarithmic operations.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements.\n3. **Element-wise Multiplication**: Multiply the ground-truth labels with the logarithms of the predicted probabilities and the complements of the ground-truth labels with the logarithms of the complements of the predicted probabilities.\n4. **Sum Terms**: Sum the results of the element-wise multiplications for each sample.\n5. **Average and Negate**: Compute the average of the summed terms and negate the result to get the final cost.\n6. **Round Result**: Return the cost rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = (-1 / m) * np.sum(sum_terms)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The predicted probabilities are clipped to ensure they are within the range [\u03b5, 1 - \u03b5], avoiding undefined logarithmic operations.\n- **Logarithmic Operations**: The natural logarithm of the clipped values and their complements are computed to handle the log terms in the binary cross-entropy formula.\n- **Element-wise Multiplication and Summation**: The terms involving the ground-truth labels and their complements are computed and summed element-wise.\n- **Averaging and Negation**: The summed terms are averaged and negated to compute the final cost, which is then rounded to six decimal places for precision.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be too close to 0 or 1."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the optimal step size (gamma) for adding a new base learner in a gradient boosting model using the mean-squared-error (MSE) objective. The step size is determined by minimizing the MSE, and it is derived using a closed-form solution.\n\n### Approach\n1. **Compute Residuals**: Calculate the residuals, which are the differences between the true target values and the current predictions.\n2. **Calculate Numerator and Denominator**: The numerator is the sum of the product of residuals and the new learner's predictions. The denominator is the sum of the squares of the new learner's predictions.\n3. **Check for Zero Denominator**: If the denominator is zero, it implies that the new learner's predictions are all zero, meaning the residuals are already perfectly fitted. In this case, the step size is set to 1.0 by convention.\n"}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the sizes of the input, hidden, and output layers in a feed-forward neural network. The input and output layer sizes are derived from the dimensions of the provided input and output matrices, while the hidden layer size is fixed at 10.\n\n### Approach\n1. **Extract Input Layer Size**: The number of input features, which is the size of the input layer, is determined by the number of rows in the input matrix `X`.\n2. **Fixed Hidden Layer Size**: The hidden layer size is fixed at 10 as per the problem statement.\n3. **Extract Output Layer Size**: The number of output units, which is the size of the output layer, is determined by the number of rows in the output matrix `Y`.\n4. **Return the Tuple**: Combine these three values into a tuple and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\n```\n\n### Explanation\n- **Input Layer Size (`n_x`)**: This is obtained by taking the number of rows in the input matrix `X`, which is `X.shape[0]`.\n- **Hidden Layer Size (`n_h`)**: This is fixed at 10 as specified in the problem.\n- **Output Layer Size (`n_y`)**: This is obtained by taking the number of rows in the output matrix `Y`, which is `Y.shape[0]`.\n\nThe function efficiently computes these values and returns them as a tuple, providing the necessary layer sizes for initializing the neural network parameters."}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the gradient of the loss with respect to the pre-activation values (z) during the backpropagation of a Rectified Linear Unit (ReLU) activation function. The ReLU function is defined as ReLU(z) = max(0, z), and its derivative is 1 when z is positive and 0 otherwise. \n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists `dA` (upstream gradient) and `activation_cache` (pre-activation values Z) into NumPy arrays for efficient vectorized operations.\n2. **Check Shapes**: Ensure that the shapes of `dA` and `activation_cache` are the same. If they are not, return -1.\n3. **Compute Gradient**: Calculate the gradient `dZ` using the rule `dZ = dA * (Z > 0)`, where `(Z > 0)` is an element-wise indicator function that returns 1 for positive values and 0 otherwise.\n4. **Convert Back to List**: Convert the resulting NumPy array `dZ` back to a list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    # Convert the input lists to NumPy arrays\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    \n    # Check if the shapes are compatible\n    if dA_np.shape != Z_np.shape:\n        return -1\n    \n    # Compute the gradient dZ\n    dZ = dA_np * (Z_np > 0)\n    \n    # Convert the result back to a list of lists\n    return dZ.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The function starts by converting the input lists `dA` and `activation_cache` into NumPy arrays. This allows for efficient element-wise operations.\n2. **Shape Check**: The function checks if the shapes of `dA` and `activation_cache` are the same. If they are not, it returns -1 to indicate an error.\n3. **Gradient Calculation**: Using the element-wise multiplication, the function computes the gradient `dZ` by multiplying `dA` with the indicator function `(Z_np > 0)`. This indicator function ensures that only the elements where `Z` is positive contribute to the gradient.\n4. **Conversion Back to List**: Finally, the resulting NumPy array `dZ` is converted back to a list of lists using the `tolist()` method and returned.\n\nThis approach efficiently handles the computation using vectorized operations provided by NumPy, ensuring that the solution is both concise and performant."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a given input value is a numeric type, excluding boolean values. Numeric types include integers, floats, complex numbers, fractions, decimals, and other instances of `numbers.Number`. The challenge is to ensure that boolean values, which are technically a subclass of integers, are excluded from being considered numeric in this context.\n\n### Approach\nThe approach to solve this problem involves two main checks:\n1. Check if the input value is an instance of `numbers.Number`. This includes all standard numeric types such as integers, floats, complex numbers, fractions, and decimals.\n2. Ensure that the input value is not a boolean. This is necessary because, in Python, boolean values (`True` and `False`) are instances of `int`, which is a subclass of `numbers.Number`.\n\nBy combining these two checks, we can accurately determine if the input value is a numeric type while excluding boolean values.\n\n### Solution Code\n```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```\n\n### Explanation\n- The function `is_number` takes a single argument `a`.\n- It uses `isinstance(a, numbers.Number)` to check if `a` is an instance of any numeric type.\n- It then uses `not isinstance(a, bool)` to ensure that `a` is not a boolean value.\n- The function returns `True` if both conditions are satisfied, indicating that `a` is a numeric type and not a boolean. Otherwise, it returns `False`.\n\nThis approach efficiently combines type checking to meet the problem requirements, ensuring that all numeric types are included while explicitly excluding boolean values."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the ReLU (Rectified Linear Unit) activation function for neural networks. The ReLU function takes an input and returns the maximum of 0 and the input value. This function must handle various input types, including scalars, 1-D arrays, 2-D arrays, and higher-dimensional arrays, and return the results as Python lists for JSON serializability.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a scalar, a list, or a nested list. We first convert this input into a NumPy array to leverage vectorized operations.\n2. **Handle Scalar Input**: If the input is a scalar (0-dimensional array), we reshape it into a 1-dimensional array to ensure consistent handling.\n3. **Compute ReLU Activation**: Using NumPy's `maximum` function, we compute the ReLU activation element-wise for the input array.\n4. **Convert to Python Lists**: Convert the resulting NumPy arrays back to Python lists using the `tolist()` method to ensure the output is JSON serializable.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_np = np.array(Z)\n    if Z_np.ndim == 0:\n        Z_np = Z_np.reshape(1)\n    A_np = np.maximum(0, Z_np)\n    A = A_np.tolist()\n    cache = Z_np.tolist()\n    return (A, cache)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input `Z` is converted to a NumPy array to"}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the majority class label among the k-nearest neighbours using a majority vote mechanism. If there is a tie in the frequency of the labels, we return the smallest label according to Python's default ordering.\n\n### Approach\n1. **Count Frequencies**: Use the `Counter` class from the `collections` module to count the frequency of each label in the list of neighbour targets.\n2. **Identify Maximum Frequency**: Determine the highest frequency of occurrence among the labels.\n3. **Collect Labels with Maximum Frequency**: Gather all labels that have this maximum frequency.\n4. **Select the Smallest Label**: From the collected labels, return the smallest one based on Python's default ordering (numerical for integers and lexicographic for strings).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for label, count in counts.items() if count == max_count]\n    return min(max_labels)\n```\n\n### Explanation\n1. **Counting Frequencies**: The `Counter` class efficiently counts the occurrences of each label in the input list.\n2. **Finding Maximum Frequency**: Using the `max` function on the values of the counter gives the highest frequency.\n3. **Filtering Labels**: We filter out the labels that have the maximum frequency.\n4. **Selecting the Smallest Label**: The `min` function is used to find the smallest label among those with the highest frequency, ensuring the correct tie-breaking as per the problem requirements.\n\nThis approach ensures that we efficiently and correctly determine the majority class label, handling ties by returning the smallest label as specified."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the ReLU activation function for a given input. The input can be a scalar, a 1-D list, a nested list, or a NumPy ndarray. The output should have the same shape as the input, with 1.0 where the input is greater than 0, and 0.0 otherwise.\n\n### Approach\nThe approach to solve this problem involves handling different types of inputs appropriately:\n\n1. **Scalar Input**: If the input is a scalar (int or float), return 1.0 if the value is greater than 0, otherwise return 0.0.\n2. **NumPy Array Input**: If the input is a NumPy array, compute the derivative using vectorized operations. Convert the result to a list using `tolist()` to maintain the same structure as the input.\n3. **List Input**: If the input is a list, process each element recursively. This handles nested lists and ensures each element is processed correctly, whether it is a scalar or another list.\n\nThe function uses recursion to handle nested lists and leverages NumPy's vectorized operations for efficient computation on arrays.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        return (Z > 0).astype(float).tolist()\n    elif isinstance(Z, list):\n        return [drelu(element) for element in Z]\n    else:\n        raise ValueError(\"Unsupported input type\")\n```\n\n### Explanation\n- **Scalar Handling**: The function first checks if the input is a scalar. If it is, it directly computes the derivative based on whether the value is greater than 0.\n- **NumPy Array Handling**: For NumPy arrays, the function uses vectorized operations to compute the derivative efficiently. The result is then converted to a list to match the expected output format.\n- **List Handling**: For lists, the function processes each element recursively. This ensures that nested lists are handled correctly, and each element is processed to compute the derivative.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification error rate given two sequences of true class labels and predicted class labels. The classification error rate is defined as the proportion of incorrectly predicted samples. \n\n### Approach\n1. **Input Validation**: First, we check if the lengths of the actual and predicted labels are the same. If they are not, we return -1.\n2. **Conversion to NumPy Arrays**: Convert the input sequences to NumPy arrays to leverage vectorized operations for efficient computation.\n3. **Mismatch Calculation**: Compute the number of mismatches between the actual and predicted labels using element-wise comparison.\n4. **Error Rate Calculation**: Calculate the error rate as the ratio of mismatches to the total number of samples.\n5. **Rounding**: Round the computed error rate to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple"}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies the softmax activation function to a given NumPy array along a specified axis. The softmax function is commonly used in neural networks to convert raw scores into normalized probabilities. The function must handle arrays with at least two dimensions and ensure numerical stability during computation.\n\n### Approach\n1. **Check for 1D Array**: If the input array is 1-dimensional, raise a `ValueError` since softmax cannot be applied to a 1D tensor.\n2. **Numerical Stability**: To avoid numerical instability, we subtract the maximum value along the specified axis from each element before computing the exponential. This helps in preventing overflow issues.\n3. **Compute Exponential**: Calculate the exponential of the shifted values.\n4. **Sum Exponentials**: Sum the exponentials along the specified axis to get the denominator for normalization.\n5. **Normalization**: Divide each element by the sum of exponentials to get the softmax probabilities.\n6. **Rounding and Conversion**: Round the resulting probabilities to 4 decimal places and convert the NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n    \n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax = exp_x / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Check for 1D Array**: The function first checks if the input array is 1-dimensional and raises an error if true.\n2. **Numerical Stability**: By subtracting the maximum value along the specified axis, we ensure that the largest value is zero, which helps in preventing overflow when computing the exponential.\n3. **Exponential Calculation**: The shifted values are exponentiated to get positive values.\n4. **Summing Exponentials**: The sum of these exponentials along the specified axis is computed to form the denominator for normalization.\n5. **Normalization**: Each element is divided by the sum to get the probabilities, which are then rounded to 4 decimal places.\n6. **Conversion to List**: The resulting NumPy array is converted to a Python list to meet the output requirements.\n\nThis approach ensures that the function is both numerically stable and efficient, handling arrays of any dimensionality greater than or equal to 2."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nB"}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the execution of the model on each input string and the formatting of the output for easy review.\n\n### Approach\n1. **Check for Examples**: The function first checks if the `examples` argument is provided. If not, it falls back to a global constant `EXAMPLES`.\n2. **Iterate Through Examples**: For each input string in the examples, the function calls another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Each prediction is converted from a list of characters into a string. The input and output strings are then printed in a specified format.\n4. **Collect Predictions**: All predicted strings are collected into a list, which is returned at the end.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n        predictions.append(predicted_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Examples**: The function checks if `examples` is `None` and uses the global `EXAMPLES` if necessary.\n- **Running the Model**: For each input string, the `run_example` function is called to get the model's prediction as a list of characters.\n- **Formatting Output**: Each prediction is converted into a string and printed alongside the input string for easy review.\n- **Collecting Results**: All predictions are collected into a list and returned, maintaining the order of the input examples.\n\nThis approach ensures that the function is both efficient and user-friendly, providing clear output for quick review while maintaining the necessary structure for further processing."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array of categorical values into its one-hot encoded representation. The goal is to detect the unique categories automatically, sort them, and then represent each sample as a binary row vector where each position is 1 if the sample belongs to the corresponding category and 0 otherwise.\n\n### Approach\n1. **Extract Unique Categories**: Use NumPy's `np.unique` function to extract the unique categories from the input array. This function also sorts the unique values by default, which simplifies our task.\n2. **Generate One-Hot Encoded Vectors**: For each element in the input array, create a binary vector where each position corresponds to a unique category. The position is set to 1 if the element matches the category and 0 otherwise.\n3. **Construct the Result**: Collect all these binary vectors into a list of lists, which is the required one-hot encoded matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique = np.unique(y)\n    result = []\n    for element in y:\n        row = []\n        for cat in unique:\n            row.append(1 if element == cat else 0)\n        result.append(row)\n    return result\n```\n\n### Explanation\n1. **Extract Unique Categories**: The `np.unique` function is used to get the unique values from the input array `y`. These values are sorted in ascending order for integers and alphabetically for strings.\n2. **Generate Binary Vectors**: For each element in `y`, we iterate over each unique category. If the element matches the category, we append 1 to the current row; otherwise, we append 0.\n3. **Construct Result**: Each binary vector (row) is added to the result list, which is returned as the final one-hot encoded matrix.\n\nThis approach ensures that we efficiently convert the input array into the required one-hot encoded format without using any external libraries beyond NumPy for handling the input array."}
{"task_id": 129, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that returns the first `samples` elements from an infinite cycle over a given sequence. This function should handle both lists and tuples as input sequences and return an empty list if the sequence is empty or if `samples` is not a positive integer.\n\n### Approach\n1. **Check for Edge Cases**: If the input sequence is empty or the number of samples requested is not positive, return an empty list immediately.\n2. **Create an Infinite Cycle**: Use the `itertools.cycle` function to create an infinite iterator that repeatedly cycles through the elements of the input sequence.\n3. **Extract the First `samples` Elements**: Use the `itertools.islice` function to extract the first `samples` elements from the infinite iterator.\n4. **Convert to List**: Convert the sliced iterator into a list and return it.\n\nThis approach ensures that we efficiently handle the infinite cycling and slicing operations without raising exceptions for edge cases.\n\n### Solution Code\n```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    c = cycle(sequence)\n    return list(islice(c, samples))\n```\n\n### Explanation\n- **Edge Case Handling**: The function first checks if the sequence is empty or if `samples` is non-positive. If either condition is met, it returns an empty list immediately.\n- **Infinite Cycle Creation**: The `itertools.cycle` function is used to create an infinite iterator that loops over the input sequence repeatedly.\n- **Slicing the Iterator**: The `itertools.islice` function is used to take the first `samples` elements from the infinite iterator. This is efficient as it does not require storing the entire infinite sequence in memory.\n- **Conversion to List**: The sliced iterator is converted into a list to match the required return type.\n\nThis solution efficiently handles the problem requirements and constraints, ensuring that it works correctly for both lists and tuples, and gracefully handles edge cases."}
{"task_id": 131, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that retrieves an element from a specific sequence within a shared container. The function must handle cases where the sequence identifier or the index is invalid by returning `None` instead of raising an exception.\n\n### Approach\n1. **Check Validity of Sequence Identifier (uid):** The function first checks if the given `uid` is a valid index for the shared container `_SHARED_SEQUENCES`. If `uid` is negative or exceeds the number of sequences in the container, the function returns `None`.\n\n2. **Retrieve the Sequence:** If the `uid` is valid, the function retrieves the corresponding sequence from the shared container.\n\n3. **Check Sequence Validity:** The function then checks if the retrieved sequence is empty. If it is, the function returns `None` since there are no elements to access.\n\n4. **Compute Effective Index:** The function computes the effective index by considering both positive and negative indices. Negative indices are converted to their positive counterparts by adding the length of the sequence to the index.\n\n5. **Check Index Validity:** The function checks if the computed effective index is within the valid range of the sequence. If it is not, the function returns `None`.\n\n6. **Return the Element:** If all checks are passed, the function returns the element at the computed effective index.\n\n### Solution Code\n```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # Check if uid is a valid index for _SHARED_SEQUENCES\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    # Retrieve the sequence\n    seq = _SHARED_SEQUENCES[uid]\n    # Check if the sequence is empty\n    if not seq:\n        return None\n    # Compute the effective index\n    if i >= 0:\n        effective_i = i\n    else:\n        effective_i = len(seq) + i\n    # Check if the effective index is within bounds\n    if effective_i < 0 or effective_i >= len(seq):\n        return None\n    return seq[effective_i]\n```\n\n### Explanation\n- **Validity Check for uid:** The function ensures that `uid` is within the valid range of indices for the shared container. This prevents accessing non-existent sequences.\n- **Sequence Retrieval and Check:** After retrieving the sequence, the function checks if it is empty to avoid index errors.\n- **Effective Index Calculation:** The function handles both positive and negative indices by converting negative indices to their positive counterparts, ensuring correct element retrieval.\n- **Index Bounds Check:** The function ensures that the computed index is within the valid range of the sequence, preventing out-of-bounds errors.\n\nThis approach ensures that the function handles all edge cases gracefully, returning `None` for invalid inputs and correctly retrieving elements when possible."}
{"task_id": 134, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to identify the arm with the highest expected reward in a Bernoulli bandit setup. Each arm provides a binary reward (0 or 1) with a given probability. The goal is to determine the arm with the highest success probability and return both this probability and the corresponding arm index.\n\n### Approach\n1. **Input Validation**: First, check if the input list is empty. If it is, return the tuple (-1.0, -1). Next, verify that all probabilities in the list are within the valid range [0, 1]. If any probability is outside this range, return the same error tuple.\n2. **Find Maximum Probability**: If the input is valid, determine the maximum probability value in the list.\n3. **Determine the Best Arm**: Identify the smallest index where this maximum probability occurs. This ensures that in case of ties, the smallest index is chosen.\n\n### Solution Code\n```python\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    index = payoff_probs.index(max_p)\n    return (max_p, index)\n```\n\n### Explanation\n1. **Input Validation**: The function starts by checking if the input list is empty. If it is, the function immediately returns (-1.0, -1). It then iterates through each probability to ensure they are within the valid range [0, 1]. If any probability is found to be outside this range, the function returns the error tuple.\n2. **Finding Maximum Probability**: Using Python's built-in `max` function, the function identifies the highest probability in the list.\n3. **Identifying the Best Arm**: The `index` method is used to find the first occurrence of the maximum probability, ensuring the smallest index is returned in case of ties.\n\nThis approach efficiently handles all edge cases and ensures the correct arm is identified based on the given constraints."}
{"task_id": 138, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification accuracy between two lists of labels. The classification accuracy is the proportion of elements that are identical in the same positions of both sequences. The function should handle various types of labels, including numerical values, strings, and booleans, and return the accuracy as a floating-point number rounded to four decimal places. If the input lists are of different lengths or are empty, the function should return -1.0.\n\n### Approach\n1. **Input Validation**: Check if the lengths of the two input lists are equal and non-zero. If not, return -1.0.\n2. **Count Matches**: Iterate through the elements of both lists simultaneously and count how many times the elements at the same positions are equal.\n3. **Compute Accuracy**: Calculate the accuracy by dividing the count of matches by the total number of elements.\n4. **Round the Result**: Round the computed accuracy to four decimal places and return it as a float.\n\n### Solution Code\n```python\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    count = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    n = len(y_true)\n    accuracy = count / n\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal and non-zero. If not, it returns -1.0 immediately.\n2. **Counting Matches**: Using a generator expression with `zip`, the function iterates through pairs of elements from `y_true` and `y_pred`, counting how many times the elements are equal.\n3. **Computing Accuracy**: The count of matches is divided by the total number of elements to get the accuracy.\n4. **Rounding**: The computed accuracy is rounded to four decimal places using Python's `round` function and returned as a float.\n\nThis approach ensures that the function efficiently computes the classification accuracy while handling various edge cases, such as mismatched list lengths and empty lists."}
{"task_id": 143, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function and its derivative using NumPy. The Leaky ReLU function is a variant of the ReLU function that allows a small negative slope, which helps mitigate the \"dying ReLU\" problem where neurons can become inactive during training.\n\n### Approach\n1. **Input Handling**: The function should accept inputs in the form of NumPy arrays, lists, or tuples. If the input is a list or tuple, it is converted to a NumPy array.\n2. **Activation Function**: For each element in the input array, the Leaky ReLU function returns the element itself if it is non-negative, and a scaled version of the element (using a small positive slope `alpha`) if it is negative.\n3. **Derivative Function**: The derivative of the Leaky ReLU function is 1 for non-negative inputs and `alpha` for negative inputs. This derivative is used during back-propagation to compute gradients efficiently.\n4. **Vectorized Operations**: The implementation uses NumPy's vectorized operations to ensure efficiency and avoid explicit loops, which is crucial for handling large arrays common in deep learning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x) if not isinstance(x, np.ndarray) else x\n    if not derivative:\n        return np.where(x >= 0, x, alpha * x)\n    else:\n        return np.where(x >= 0, 1, alpha)\n```\n\n### Explanation\n- **Input Conversion**: The function first checks if the input `x` is a NumPy array. If not, it converts `x` to a NumPy array using `np.asarray`.\n- **Activation Calculation**: Using `np.where`, the function checks each element of `x`. If an element is non-negative, it is returned as is. If negative, it is scaled by `alpha`.\n- **Derivative Calculation**: Similarly, the derivative is computed using `np.where`. For non-negative elements, the derivative is 1, and for negative elements, it is `alpha`.\n\nThis approach ensures that the function is efficient, handles arbitrary input shapes, and adheres to the requirements of deep learning frameworks that need both the activation and its derivative for back-propagation."}
{"task_id": 148, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the exponential moving average (EMA) of a given sequence of numerical observations using a specified smoothing factor. The function should handle various edge cases and return the results in a specific format.\n\n### Approach\n1. **Input Validation**: Check if the input data list is empty or if the smoothing factor `alpha` is outside the valid range [0, 1]. If either condition is met, return -1.\n2. **Special Cases Handling**:\n   - If `alpha` is 0, the output list will consist of the first element of the input list repeated for all positions.\n   - If `alpha` is 1, the output list will be identical to the input list.\n3. **General Case Handling**: For valid `alpha` values between 0 and 1, compute the EMA using the recursive formula. The first element of the result is the same as the first element of the input list. Each subsequent element is computed as a weighted average of the previous result and the current input element.\n4. **Rounding**: Each element of the result list is rounded to four decimal places before returning.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n   "}
{"task_id": 154, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the Radial Basis Function (RBF) kernel matrix for two given sets of vectors. The RBF kernel matrix is a measure of similarity between pairs of vectors, which is widely used in machine learning algorithms such as Support Vector Machines (SVMs).\n\n### Approach\nThe RBF kernel matrix is defined such that each element \\( K_{ij} \\) is computed using the formula:\n\\[ K_{ij} = \\exp(-\\gamma \\cdot ||x_i - y_j||_2^2) \\]\nwhere \\( \\gamma \\) is a positive scalar that controls the decay rate of the similarity with distance.\n\nThe approach to compute this matrix efficiently involves the following steps:\n1. **Reshape Input Arrays**: Ensure that both input arrays are 2-dimensional. If they are 1-dimensional, reshape them to treat each as a single sample.\n2. **Compute Norms**: Calculate the squared Euclidean norms for each row in both input arrays.\n3. **Dot Product Calculation**: Compute the dot product matrix between the two input arrays.\n4. **Squared Distances**: Use the norms and dot product to compute the squared Euclidean distances between all pairs of vectors.\n5. **Kernel Matrix**: Apply the RBF formula to the squared distances to get the kernel matrix.\n6. **Rounding and Conversion**: Round each element of the kernel matrix to six decimal places and convert it to a nested list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # Reshape 1D arrays to 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n    \n    # Compute squared norms\n    X_norms = np.sum(X ** 2, axis=1)\n    Y_norms = np.sum(Y ** 2, axis=1)\n    \n    # Compute dot product\n    dot_product = X.dot(Y.T)\n    \n    # Compute squared distances\n    squared_distances = X_norms[:, np.newaxis] + Y_norms[np.newaxis, :] - 2 * dot_product\n    \n    # Compute RBF kernel\n    K = np.exp(-gamma * squared_distances)\n    \n    # Round to 6 decimal places and convert to list\n    K_rounded = np.round(K, 6)\n    return K_rounded.tolist()\n```\n\n### Explanation\n1. **Reshape Input Arrays**: The function first checks if the input arrays are 1-dimensional and reshapes them to 2-dimensional arrays if necessary. This ensures that the subsequent operations work correctly.\n2. **Compute Norms**: The squared norms of each row in the input arrays are computed using `np.sum(X ** 2, axis=1)` and `np.sum(Y ** 2, axis=1)`.\n3. **Dot Product Calculation**: The dot product between the two input arrays is computed using `X.dot(Y.T)`, which results in a matrix where each element (i, j) is the dot product of the i-th row of X and the j-th row of Y.\n4. **Squared Distances**: Using the formula for squared Euclidean distance, the distances between all pairs of vectors are computed efficiently using broadcasting.\n5. **Kernel Matrix**: The RBF kernel values are computed by applying the exponential function to the negative product of gamma and the squared distances.\n6. **Rounding and Conversion**: The resulting kernel matrix is rounded to six decimal places and converted to a nested list format using `tolist()` for the final output.\n\nThis approach ensures that the RBF kernel matrix is computed efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an actor network in a reinforcement learning context. The network consists of three linear layers with ReLU activations after the first two layers and a tanh activation at the output layer. The goal is to convert a given state vector into an action vector using the provided weights and biases.\n\n### Approach\n1. **Convert State to Numpy Array**: Ensure the input state is a numpy array for consistent operations.\n2. **Linear Transformation and ReLU Activation**: Compute the output of each linear layer followed by a ReLU activation. This involves matrix multiplication with the weights and addition of biases, followed by applying the ReLU function.\n3. **Final Linear Transformation and Tanh Activation**: The output of the second ReLU layer is passed through the final linear layer, followed by a tanh activation to ensure the action values are within the range [-1, 1].\n4. **Rounding**: Convert the resulting action vector to a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a numpy array if it's not already\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    \n    # First linear layer followed by ReLU\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    \n    # Second linear layer followed by ReLU\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    \n    # Third linear layer followed by tanh\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    \n    # Round each element to four decimal places and convert to list\n    return [round(x, 4) for x in action]\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted to a numpy array to facilitate matrix operations.\n2. **First Layer**: The state vector is multiplied by the first weight matrix `W1`, and the bias `b1` is added. The result is passed through the ReLU activation function.\n3. **Second Layer**: The output from the first layer is multiplied by the second weight matrix `W2`, and the bias `b2` is added. Another ReLU activation is applied.\n4. **Output Layer**: The output from the second layer is multiplied by the third weight matrix `W3`, and the bias `b3` is added. The result is passed through the tanh activation function to produce the action vector.\n5. **Rounding**: Each element of the action vector is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the network correctly processes the input state through the defined architecture, producing an action vector suitable for reinforcement learning applications."}
{"task_id": 166, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Perceptron algorithm from scratch to train a binary classifier and make predictions on test data. The Perceptron algorithm is a simple yet effective method for learning a linear decision boundary between two classes. The goal is to find a separating hyper-plane using the Perceptron update rule with a learning rate of 1.\n\n### Approach\n1. **Initialization**: Start with a weight vector `w` and a bias term `b`, both initialized to zero. The weight vector has the same number of features as the input data.\n2. **Training Loop**: Iterate over the training data for a maximum of `max_iter` times. In each iteration, process each training sample sequentially.\n3. **Margin Calculation**: For each sample, compute the signed margin `m` which is given by `y_i * (w\u00b7x_i + b)`. If the margin is less than or equal to zero, the sample is misclassified.\n4. **Update Rule**: If a sample is misclassified, update the weight vector and bias term using the Perceptron update rule: `w = w + y_i * x_i` and `b = b + y_i`.\n5. **Early Termination**: If an entire pass over the training data results in zero misclassifications, the algorithm has converged, and we terminate early.\n6. **Prediction**: After training, compute the raw scores for each test sample using the learned weights and bias. The prediction is the sign of this score, treating zero as +1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int = 1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(len(X_train)):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n\n    predictions = []\n    for x in X_test:\n        s = np.dot(w, x) + b\n        if s >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions\n```\n\n### Explanation\n- **Initialization**: The weight vector `w` and bias `b` are initialized to zero to start the learning process.\n- **Training Loop**: The outer loop runs for up to `max_iter` iterations. Each iteration processes all training samples sequentially.\n- **Margin Calculation**: For each sample, the margin is computed to determine if the sample is correctly classified. If the margin is non-positive, the sample is misclassified.\n- **Update Rule**: The weight vector and bias are updated using the Perceptron rule to correct misclassifications.\n- **Early Termination**: If no misclassifications occur in an entire pass through the training data, the algorithm stops early as it has converged.\n- **Prediction**: After training, the model makes predictions on the test data by computing the dot product of each test sample with the learned weights and adding the bias. The sign of this result determines the predicted label, with zero treated as +1.\n\nThis approach ensures that the Perceptron algorithm is implemented efficiently and correctly, providing a clear and interpretable solution for binary classification tasks."}
{"task_id": 173, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the alphabetical distance between two strings. The alphabetical distance is defined as the sum of the absolute differences of the ASCII values of corresponding characters in the two strings. If the strings are of different lengths, we return -1.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input strings are equal. If they are not, we return -1 immediately.\n2. **Calculate Distance**: If the lengths are equal, we initialize a variable to keep track of the total distance. We then iterate through each pair of corresponding characters from the two strings, compute the absolute difference of their ASCII values, and accumulate this difference into the total distance.\n3. **Return Result**: Finally, we return the total accumulated distance.\n\n### Solution Code\n```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for c, t in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the candidate and target strings are the same. If not, it returns -1.\n- **Iterate Through Characters**: Using the `zip` function, we pair each character from the candidate string with the corresponding character from the target string. For each pair, we compute the absolute difference of their ASCII values using the `ord` function and accumulate this value into the total distance.\n- **Return Total Distance**: After processing all character pairs, the total distance is returned.\n\nThis approach ensures that we efficiently compute the alphabetical distance while handling edge cases such as different string lengths and empty strings correctly."}
{"task_id": 175, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the L2 regularization penalty for a given set of weights. The L2 penalty is a common technique used in machine learning to prevent overfitting by adding a penalty term to the loss function, which discourages overly complex models.\n\n### Approach\nThe L2 regularization penalty is defined as the product of a non-negative regularization strength \\( C \\) and the sum of the squares of the weights. The steps to compute this penalty are as follows:\n\n1. **Convert Weights to NumPy Array**: The input weights can be either a Python list or a 1-D NumPy array. Converting them to a NumPy array allows us to use efficient array operations.\n2. **Compute Sum of Squares**: Calculate the sum of the squares of each weight using NumPy's array operations, which are optimized for performance.\n3. **Calculate Penalty**: Multiply the sum of squares by the regularization strength \\( C \\) to get the penalty.\n4. **Round the Result**: Return the penalty value rounded to four decimal places to meet the specified precision requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to the nearest 4th decimal place.\n    \"\"\"\n    weights_array = np.array(weights)\n    sum_squares = np.sum(weights_array ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The function first converts the input weights into a NumPy array. This allows us to leverage NumPy's efficient operations for computing the sum of squares.\n- **Sum of Squares Calculation**: Using NumPy's element-wise exponentiation and summation, we efficiently compute the sum of the squares of the weights.\n- **Penalty Calculation**: The sum of squares is then multiplied by the regularization strength \\( C \\) to get the penalty.\n- **Rounding**: The result is rounded to four decimal places to ensure the required precision.\n\nThis approach ensures that the function handles both lists and NumPy arrays efficiently, computes the penalty accurately, and returns the result in the required format."}
{"task_id": 183, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a weight tensor using the He uniform initialization method, which is particularly effective for layers followed by ReLU activation. The initialization method ensures that the weights are scaled appropriately to maintain consistent gradient magnitudes throughout the network.\n\n### Approach\n1. **Determine Layer Type**: The function first checks the length of the weight shape to determine if it is a dense layer (2D) or a convolutional layer (4D).\n2. **Compute Fan-In**: For a dense layer, the fan-in is the first dimension of the weight shape. For a convolutional layer, the fan-in is the product of the first three dimensions (spatial size and input depth).\n3. **Calculate Limit**: The limit for the uniform distribution is computed as the square root of 6 divided by the fan-in.\n4. **Generate Weights**: Using the calculated limit, random values are generated from a uniform distribution scaled to the interval [-limit, limit].\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    # Determine the type of layer based on the shape length\n    if len(weight_shape) == 2:\n        # Dense layer: fan_in is the first dimension\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        # Convolutional layer: fan_in is product of first three dimensions\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        # According to the problem statement, this case won't be tested\n        raise ValueError(\"Weight shape must be 2D or 4D.\")\n    \n    # Calculate the limit for uniform distribution\n    limit = np.sqrt(6.0 / fan_in)\n    \n    # Generate random values from uniform distribution in [-limit, limit]\n    weights = np.random.rand(*weight_shape) * (2 * limit) - limit\n    \n    return weights\n```\n\n### Explanation\n- **Layer Type Determination**: The function checks if the weight shape is 2D or 4D to decide if it's a dense or convolutional layer.\n- **Fan-In Calculation**: For dense layers, fan-in is the number of input features. For convolutional layers, it's the product of the filter dimensions and the number of input channels.\n- **Limit Calculation**: The limit is derived from the He uniform initialization formula, ensuring that the weights are scaled appropriately.\n- **Weight Generation**: Using NumPy's random number generator, the function creates an array of the specified shape with values uniformly distributed between -limit and limit.\n\nThis approach ensures that the weights are initialized in a way that facilitates effective gradient flow during training, especially when using ReLU activation functions."}
{"task_id": 185, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the shape of a given feature matrix `X` and return the number of samples and features as a tuple. The solution must handle various cases, including converting array-like objects to NumPy arrays, checking for empty matrices, and correctly interpreting the dimensions of the input data.\n\n### Approach\n1. **Convert to NumPy Array**: Convert the input `X` to a NumPy array if it is not already one. This ensures consistent handling of different array-like objects.\n2. **Check for Empty Matrix**: If the resulting array is empty (i.e., has zero elements), raise a `ValueError`.\n3. **Handle Different Dimensions**:\n   - **0-Dimensional Array**: Treat a 0-dimensional array as a single sample with one feature.\n   - **1-Dimensional Array**: Treat a 1-dimensional array as a single sample with the number of features equal to the length of the array.\n   - **2-Dimensional and Higher Arrays**: For arrays with two or more dimensions, the first dimension represents the number of samples, and the product of the remaining dimensions gives the number of features.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError(\"Got an empty matrix.\")\n    if X.ndim == 0:\n        return (1, 1)\n    elif X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, n_features)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The function starts by converting the input `X` into a NumPy array using `np.asarray(X)`. This ensures that the input is handled consistently regardless of its original type (e.g., list, pandas DataFrame).\n2. **Empty Matrix Check**: The function checks if the array is empty by verifying if `X.size` is zero. If it is, a `ValueError` is raised with the message \"Got an empty matrix.\"\n3. **Dimension Handling**:\n   - For a 0-dimensional array, the function treats it as a single sample with one feature, returning `(1, 1)`.\n   - For a 1-dimensional array, the function treats it as a single sample with the number of features equal to the length of the array, returning `(1, len(X))`.\n   - For arrays with two or more dimensions, the function extracts the number of samples from the first dimension and computes the number of features as the product of the remaining dimensions, returning `(n_samples, n_features)`.\n\nThis approach ensures that the function correctly handles various input types and dimensions, providing the required shape information efficiently."}
{"task_id": 186, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a baseline regressor that always predicts the mean of the training targets for each test sample. This provides a simple reference score that any reasonable model should beat.\n\n### Approach\nThe approach involves the following steps:\n1. **Compute the Mean of Training Targets**: Calculate the arithmetic mean of the training target vector `y_train`.\n2. **Handle Empty Test Data**: If the test feature matrix `X_test` is empty, return an empty list immediately.\n3. **Generate Predictions**: For each row in `X_test`, predict the mean value computed from `y_train`.\n4. **Round Predictions**: Round each predicted value to four decimal places.\n5. **Return as List**: Convert the predictions into a list of floats and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Your task is to complete this function so that it returns the arithmetic\n    mean of *y_train* for every sample in *X_test*.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n"}
{"task_id": 193, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth alternative to the hyperbolic tangent function. The Softsign function is defined as Softsign(z) = z / (1 + |z|). The function should handle both scalar inputs and array-like inputs (lists or NumPy arrays) and return the results rounded to four decimal places.\n\n### Approach\n1. **Scalar Input Handling**: If the input is a single real number (int or float), compute the Softsign value and return it as a float rounded to four decimal places.\n2. **Array-like Input Handling**: If the input is a list or a NumPy array, compute the Softsign value for each element. For NumPy arrays, use vectorized operations for efficiency, then convert the result to a list of floats, each rounded to four decimal places.\n3. **Edge Cases**: Handle empty inputs by returning an empty list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softsign(z):\n"}
{"task_id": 199, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a label vector `y` into two parts based on a threshold value applied to a feature vector `X`. The goal is to separate the labels into two groups: those corresponding to features in `X` that are strictly smaller than the threshold, and those that are greater than or equal to the threshold.\n\n### Approach\n1. **Problem Analysis**: We are given two NumPy arrays, `X` (feature values) and `y` (labels), of the same length. We need to split `y` into two lists: `y_left` for labels where the corresponding feature in `X` is less than a given threshold, and `y_right` for labels where the feature is greater than or equal to the threshold.\n\n2. **Intuition**: Using boolean indexing in NumPy, we can efficiently create a mask where each element is `True` if the corresponding feature in `X` is less than the threshold. This mask can then be used to extract the corresponding labels from `y` for `y_left`. The inverse of this mask will give us the labels for `y_right`.\n\n3. **Algorithm Selection**: The use of boolean indexing is optimal here because it allows us to directly index the arrays based on the condition, making the solution both efficient and concise.\n\n4. **Complexity Analysis**: The solution involves creating a boolean mask and using it to index the arrays, both of which are O(n) operations where n is the length of the arrays. Thus, the overall time complexity is O(n).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\n```\n\n### Explanation\n1. **Boolean Mask Creation**: The mask is created using `X < value`, which results in a boolean array where each element is `True` if the corresponding element in `X` is less than `value`.\n\n2. **Extracting Labels**: Using the boolean mask, `y_left` is extracted from `y` where the mask is `True`. The inverse of the mask (`~mask`) is used to extract `y_right` from `y`.\n\n3. **Conversion to Lists**: The resulting NumPy arrays from the boolean indexing are converted to Python lists using the `tolist()` method to meet the problem's requirement of returning lists.\n\nThis approach efficiently splits the labels based on the threshold and ensures the original order of labels is preserved."}
{"task_id": 205, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to extract unique environment IDs from a given registry. The registry is represented as a list, where each item may or may not be a dictionary containing an \"id\" key. Our task is to collect these IDs, ensuring each ID appears only once and in the order of their first occurrence.\n\n### Approach\n1. **Initialization**: We start by initializing an empty list to store the unique IDs in the order they first appear and an empty set to keep track of the IDs we have already encountered.\n2. **Iteration**: We iterate through each item in the registry. For each item, we check if it is a dictionary and if it contains the key \"id\".\n3. **Extraction and Tracking**: If an item meets the criteria (being a dictionary with an \"id\" key), we extract the ID. We then check if this ID has already been encountered using our set. If it hasn't, we add it to both our result list and the set.\n4. **Return Result**: After processing all items in the registry, we return the list of unique IDs.\n\nThis approach ensures that we efficiently track and collect unique IDs while maintaining their order of first appearance.\n\n### Solution Code\n```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen = set()\n    result = []\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                result.append(env_id)\n    return result\n```\n\n### Explanation\n- **Initialization**: `seen` is a set to track encountered IDs, and `result` is a list to store the unique IDs in order.\n- **Iteration**: For each item in the registry, we check if it is a dictionary and contains the \"id\" key.\n- **Extraction and Tracking**: If the ID is new (not in `seen`), it is added to both `seen` and `result`.\n- **Efficiency**: Using a set for tracking ensures that checking for existence is average O(1), making the overall algorithm efficient with a time complexity of O(n), where n is the number of items in the registry.\n\nThis solution effectively handles the extraction of unique IDs while preserving their order of first appearance, adhering to the problem constraints and requirements."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability obtained by applying the sigmoid transformation. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input lists, tuples, or NumPy arrays into NumPy arrays for efficient vectorized computations.\n2. **Compute the Gradient**: The gradient for each observation is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the term `-actual * predicted`, applying the sigmoid function to this term, and then multiplying by the actual values.\n3. **Compute the Probability and Hessian**: The probability is obtained by applying the sigmoid function to the predicted values. The Hessian is then computed as the product of the probability and its complement (1 - probability).\n4. **Rounding**: Each computed value (gradient, Hessian, and probability) is rounded to exactly six decimal places to meet the precision requirement.\n5. **Convert Results to Lists**: The NumPy arrays are converted back to Python lists before returning the results.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute the term for the gradient\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    \n    # Compute the probability and Hessian\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    \n    # Round each value to 6 decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert NumPy arrays to Python lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the application of the sigmoid function.\n- **Probability and Hessian**: The probability is derived from the sigmoid function applied to the predicted values. The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Efficiency**: Using NumPy arrays allows for efficient vectorized operations, which is crucial for handling large datasets.\n- **Precision**: Each result is rounded to six decimal places to ensure the required precision, and the results are converted back to Python lists for compatibility with the expected output format."}
{"task_id": 220, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to estimate the word-topic (\u03d5) and document-topic (\u03b8) distributions for a Latent Dirichlet Allocation (LDA) model using symmetric Dirichlet smoothing. The goal is to compute these distributions from given count matrices and hyper-parameters, and return them as rounded nested lists.\n\n### Approach\n1. **Problem Analysis**:\n   - We are given two count matrices: `C_wt` (word-topic counts) and `C_dt` (document-topic counts).\n   - We need to compute the probability distributions"}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is commonly used in neural networks to avoid the issue of \"dying\" neurons. The function should handle various input types, including scalars, nested lists, and numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For any input value \\( x \\geq 0 \\), the output is \\( x \\).\n- For any input value \\( x < 0 \\), the output is \\( a \\times x \\), where \\( a \\) is a parameter in the range [0, 1).\n\nThe function must handle different input types:\n1. **Scalars**: Directly apply the Leaky ReLU transformation.\n2. **Lists (possibly nested)**: Recursively process each element to apply the transformation.\n3. **Numpy Arrays**: Use vectorized operations for efficient computation and then convert the result to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n1. **Handling Numpy Arrays**: If the input `z` is a numpy array, we use `np.where` to apply the Leaky ReLU transformation element-wise. This is efficient and leverages numpy's vectorized operations. The result is then converted to a Python list using `tolist()` to maintain the required output format.\n\n2. **Handling Lists**: If the input `z` is a list, we recursively process each element. This ensures that nested lists are handled correctly, maintaining the structure of the input.\n\n3. **Handling Scalars**: If the input `z` is a scalar, we directly apply the Leaky ReLU transformation. If `z` is non-negative, it remains unchanged; otherwise, it is multiplied by the parameter `a`.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 231, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given hand of cards in the game of Blackjack is a natural blackjack. A natural blackjack is defined as a hand containing exactly two cards: one Ace (represented as 1) and one ten-valued card (represented as 10).\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Length of the Hand**: The hand must contain exactly two cards. If the length of the hand is not two, it cannot be a natural blackjack.\n2. **Count the Occurrences of Ace and Ten-Valued Cards**: The hand must contain exactly one Ace (1) and one ten-valued card (10). We can achieve this by counting the occurrences of 1 and 10 in the hand.\n\nBy following these steps, we can efficiently determine if the hand is a natural blackjack.\n\n### Solution Code\n```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack.\n    \n    A natural blackjack is a hand with exactly two cards: one Ace (1) and one ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1\n```\n\n### Explanation\n1. **Check the Length**: The function first checks if the length of the hand is exactly two. If not, it immediately returns `False`.\n2. **Count Occurrences**: The function then counts how many times 1 (Ace) and 10 (ten-valued card) appear in the hand. If both counts are exactly one, the function returns `True`, indicating a natural blackjack. Otherwise, it returns `False`.\n\nThis approach ensures that we efficiently and accurately determine if the hand meets the criteria for a natural blackjack."}
{"task_id": 235, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a 1-dimensional NumPy array of integer class labels into a one-hot encoded 2-dimensional NumPy array. This transformation is essential in machine learning tasks where categorical data needs to be represented in a binary format suitable for various algorithms.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Input Validation**: Ensure the input array is 1-dimensional. If not, raise a `ValueError`.\n2. **Handle Empty Labels**: If the input array is empty, check if `n_classes` is provided. If not, raise a `ValueError`. If `n_classes` is provided, return an empty array with the specified number of classes.\n3. **Determine Number of Classes**: If `n_classes` is not provided, compute it as the maximum value in the labels plus one. If `n_classes` is provided, ensure it is at least the maximum value in the labels plus one; otherwise, raise a `ValueError`.\n4. **Create One-Hot Encoded Matrix**: Use NumPy's `eye` function to create an identity matrix of size `n_classes` and index it with the labels to form the one-hot encoded matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as `labels.max() + 1`.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n"}
{"task_id": 252, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute both the gradient and the diagonal Hessian of the least-squares loss function. The least-squares loss function is defined as \\( L(actual, predicted) = 0.5 \\times ||actual - predicted||^2 \\). The gradient and Hessian are essential for optimization algorithms, such as gradient descent, to adjust model parameters and minimize the loss.\n\n### Approach\n1. **Gradient Calculation**: The gradient of the least-squares loss with respect to the predicted values is given by \\( \\nabla L = actual - predicted \\). This can be computed by subtracting the predicted values from the actual values element-wise.\n2. **Hessian Calculation**: The Hessian matrix for the least-squares loss is a diagonal matrix where each diagonal element is 1. Since we only need the diagonal elements, we can represent this as a list of ones with the same length as the input arrays.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed by subtracting each element of the predicted array from the corresponding element of the actual array. This result is then converted to a Python list.\n- **Hessian Calculation**: The Hessian is a diagonal matrix where each diagonal element is 1. Since we only need the diagonal elements, we create a list of ones with the same length as the input arrays.\n\nThis approach efficiently computes the required gradient and Hessian using basic array operations, ensuring that the results are returned as Python lists as specified."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes weights in a way that helps in maintaining consistent variance of activations across layers, which is crucial for effective training of deep networks.\n\n### Approach\nThe approach to solve this problem involves three main steps:\n\n1. **Compute Fan In and Fan Out**: \n   - For a 2D shape `(fan_in, fan_out)`, the values are directly taken from the dimensions.\n   - For shapes with more than two dimensions (common in convolutional layers), the fan in and fan out are computed by considering the product of all dimensions beyond the first two (receptive field size).\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation `s` is computed using the formula `s = sqrt(2 / (fan_in + fan_out))`.\n\n3. **Generate Random Array**: \n   - Using the computed standard deviation, generate a NumPy array with elements drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n"}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should be a list of lists where each inner list contains the neighbors of the corresponding vertex in `V`, sorted by their order of appearance in `V`.\n\n### Approach\n1. **Create a Vertex Index Mapping**: First, we create a dictionary to map each vertex to its index in the list `V`. This helps in quickly determining the order of vertices.\n2. **Initialize Adjacency Dictionary**: We use a dictionary where each key is a vertex and the value is a set of its neighbors. This helps in efficiently managing and avoiding duplicate edges.\n3. **Process Edges**: For each edge in `E`, we add each vertex to the other's neighbor set. This ensures that both directions of the undirected edge are accounted for.\n4. **Build the Adjacency List**: For each vertex in `V`, we collect its neighbors, sort them based on their indices in `V`, and construct the adjacency list.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for i, v in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    \n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    \n    return G\n```\n\n### Explanation\n1. **Vertex Index Mapping**: The dictionary `vertex_index` maps each vertex to its position in `V`, which is used to sort neighbors later.\n2. **Adjacency Dictionary**: `adj_dict` is initialized with each vertex having an empty set of neighbors. This helps in efficiently adding and managing neighbors without duplicates.\n3. **Edge Processing**: For each edge `(u, v)`, both `u` is added to `v`'s neighbors and `v` is added to `u`'s neighbors, ensuring the undirected nature of the graph.\n4. **Building the Adjacency List**: For each vertex in `V`, its neighbors are collected, sorted by their indices using `vertex_index`, and added to the result list `G`.\n\nThis approach ensures that the adjacency list is constructed efficiently and correctly, with neighbors sorted as required."}
{"task_id": 269, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the total number of trainable parameters in a two-layer Deep Q-Network (DQN). The DQN consists of two fully-connected layers with specific configurations, and we need to sum the parameters from both layers to get the total count.\n\n### Approach\nThe DQN architecture is as follows:\n1. The first layer is a fully-connected layer that maps the input state vector (of length `state_dimensions`) to 164 hidden neurons.\n2. The second layer is another fully-connected layer that maps these 164 hidden neurons to the number of possible actions (of length `action_dimensions`).\n\nFor each fully-connected layer, the number of trainable parameters is calculated as the sum of the weights and biases. The formula for the number of parameters in a fully-connected layer is:\n- Weights: `in_features \u00d7 out_features`\n- Biases: `out_features`\n\nThus, the total parameters for each layer are given by `(in_features \u00d7 out_features) + out_features`.\n\n### Solution Code\n```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\"\"\"\n    layer1 = state_dimensions * 164 + 164\n    layer2 = 164 * action_dimensions + action_dimensions\n    return layer1 + layer2\n```\n\n### Explanation\n1. **First Layer Calculation**:\n   - The input dimension is `state_dimensions` and the output dimension is 164.\n   - The number of weights is `state_dimensions * 164`.\n   - The number of biases is 164.\n   - Total parameters for the first layer: `state_dimensions * 164 + 164`.\n\n2. **Second Layer Calculation**:\n   - The input dimension is 164 and the output dimension is `action_dimensions`.\n   - The number of weights is `164 * action_dimensions`.\n   - The number of biases is `action_dimensions`.\n   - Total parameters for the second layer: `164 * action_dimensions + action_dimensions`.\n\n3. **Total Parameters**:\n   - Sum the parameters from both layers to get the total number of trainable parameters.\n\nThis approach efficiently computes the total number of parameters by breaking down the problem into manageable parts and summing the results."}
{"task_id": 277, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether the action and observation spaces in a given reinforcement learning environment are composite. A composite space is defined as either a tuple or a dictionary. \n\n### Approach\nThe approach involves checking the type of the action and observation spaces in the environment dictionary. Specifically, we will:\n1. Check if the value associated with the key `\"action_space\"` is either a tuple or a dictionary.\n2. Similarly, check if the value associated with the key `\"observation_space\"` is either a tuple or a dictionary.\n3. Return the results as a tuple of two boolean values indicating whether each space is composite.\n\nThis approach leverages Python's `isinstance` function to check the type of each space, ensuring that we correctly identify composite spaces.\n\n### Solution Code\n```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env[\"action_space\"], (tuple, dict))\n    tuple_obs = isinstance(env[\"observation_space\"], (tuple, dict))\n    return (tuple_action, tuple_obs)\n```\n\n### Explanation\n- The function `is_tuple` takes a dictionary `env` as input, which is guaranteed to contain the keys `\"action_space\"` and `\"observation_space\"`.\n- It uses the `isinstance` function to check if the value of `\"action_space\"` is either a tuple or a dictionary. This result is stored in `tuple_action`.\n- Similarly, it checks the type of the value associated with `\"observation_space\"` and stores the result in `tuple_obs`.\n- The function returns a tuple containing these two boolean values, indicating whether each space is composite.\n\nThis solution efficiently determines the composite nature of the spaces using straightforward type checking, ensuring clarity and correctness."}
{"task_id": 278, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a binary decision tree represented by nested dictionaries and predict the value for a given input sample. The decision tree consists of internal nodes and leaf nodes. Internal nodes contain rules for branching, while leaf nodes contain the predicted values.\n\n### Approach\n1. **Initialization**: Start at the root of the decision tree.\n2. **Traversal**: For each internal node, check the feature value of the sample against the threshold. If the sample's feature value is less than or equal to the threshold, follow the true branch; otherwise, follow the false branch.\n3. **Leaf Node Check**: Continue traversing until a leaf node is reached. The leaf node contains the predicted value, which is then returned.\n\n### Solution Code\n```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        else:\n            feature_i = current_node['feature_i']\n            threshold = current_node['threshold']\n            if sample[feature_i] <= threshold:\n                current_node = current_node['true_branch']\n            else:\n                current_node = current_node['false_branch']\n```\n\n### Explanation\n- **Initialization**: The function starts by setting the current node to the root of the tree.\n- **Loop Until Leaf Node**: The loop continues until a leaf node is found. For each internal node, the function checks the feature value of the sample against the threshold.\n- **Branching**: Depending on whether the sample's feature value is less than or equal to the threshold, the function follows the true or false branch to the next node.\n- **Return Leaf Value**: Once a leaf node is reached, the function returns the value stored in that leaf node, which is the prediction for the given sample.\n\nThis approach ensures that the decision tree is traversed efficiently, and the correct prediction is made based on the given sample."}
{"task_id": 285, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel (Gram matrix) between two datasets. The linear kernel is defined as the matrix product of the first dataset and the transpose of the second dataset. The function should handle both one-dimensional and two-dimensional input arrays and return the result as a nested list or -1 if the feature dimensions do not match.\n\n### Approach\n1. **Reshape Input Arrays**: If the input arrays are one-dimensional, reshape them into two-dimensional row vectors. This ensures that both inputs are in a consistent format for further processing.\n2. **Check Feature Dimensions**: Verify that the number of features (columns) in both datasets is the same. If they differ, return -1.\n3. **Compute Kernel Matrix**: If the feature dimensions match, compute the matrix product of the first dataset and the transpose of the second dataset.\n4. **Convert to List**: Convert the resulting matrix from NumPy to a nested list format using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    # Reshape x and y if they are 1D\n    if x.ndim == 1:\n        x_reshaped = x.reshape(1, -1)\n    else:\n        x_reshaped = x\n\n    if y.ndim == 1:\n        y_reshaped = y.reshape(1, -1)\n    else:\n        y_reshaped = y\n\n    # Check if feature dimensions match\n    if x_reshaped.shape[1] != y_reshaped.shape[1]:\n        return -1\n\n    # Compute the kernel matrix\n    kernel = x_reshaped @ y_reshaped.T\n\n    # Convert to a list of lists\n    return kernel.tolist()\n```\n\n### Explanation\n1. **Reshaping**: The function first checks if the input arrays are one-dimensional. If they are, they are reshaped into two-dimensional row vectors to ensure consistency.\n2. **Dimension Check**: The function then checks if the number of features (columns) in both datasets is the same. If not, it returns -1.\n3. **Matrix Multiplication**: If the feature dimensions match, the function computes the matrix product of the first dataset and the transpose of the second dataset.\n4. **Conversion**: The resulting matrix is converted into a nested list format using the `tolist()` method, which is then returned as the result.\n\nThis approach ensures that the function handles various input formats correctly and efficiently computes the linear kernel matrix."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function `get_initializer` that retrieves a weight-initialization function based on a given name. The function should return the corresponding callable initializer if it exists, or raise an error if it does not.\n\n### Approach\n1. **Lookup in"}
{"task_id": 289, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to format a list of tuples into a specific string format. Each tuple consists of a field name and its corresponding value, both of which are strings. The goal is to convert each tuple into a string of the form \"field: \"value\"\" and then join all these strings with newline characters.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return an empty string immediately.\n2. **Iterate Over Tuples**: For each tuple in the list, convert the field and value to strings (to handle any non-string inputs gracefully).\n3. **Format Each Tuple**: Convert each tuple into the required string format \"field: \"value\"\".\n4. **Join Formatted Strings**: Collect all formatted strings into a list and join them with newline characters to form the final result string.\n\n### Solution Code\n```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return \"\"\n    formatted = []\n    for field, value in entries:\n        field_str = str(field)\n        value_str = str(value)\n        formatted_line = f\"{field_str}: \\\"{value_str}\\\"\"\n        formatted.append(formatted_line)\n    return '\\n'.join(formatted)\n```\n\n### Explanation\n- **Empty List Check**: The function first checks if the input list is empty. If it is, it returns an empty string immediately.\n- **String Conversion**: Each field and value is converted to a string to handle any non-string inputs, ensuring robustness.\n- **Formatting**: Each tuple is formatted into the string \"field: \"value\"\" using an f-string for clarity and efficiency.\n- **Joining Strings**: All formatted strings are collected into a list and then joined with newline characters to produce the final result string, ensuring no extra newline is added at the end.\n\nThis approach efficiently handles the conversion and formatting, ensuring the output meets the specified requirements."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same features and thresholds at each internal node, and produce the same predictions at each leaf node, within a specified tolerance.\n\n### Approach\n1. **Type Check**: First, we check if both trees are of the same type (both are either `Node` or `Leaf`). If they are not, the trees are not equivalent.\n2. **Leaf Node Comparison**: If both nodes are `Leaf` nodes, we compare their values using `numpy.allclose` to handle numerical comparisons with a tolerance of \\(1 \\times 10^{-8}\\).\n3. **Internal Node Comparison**: If both nodes are `Node` nodes, we check:\n   - The feature index used for splitting.\n   - The threshold value, ensuring it is the same within the specified tolerance.\n   - Recursively compare the left and right subtrees to ensure they are equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both nodes are of the same type\n    if type(tree_a) != type(tree_b):\n        return False\n    \n    if isinstance(tree_a, Leaf):\n        # Compare leaf values with tolerance\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    else:\n        # Both are Node instances; check feature and threshold\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        \n        # Recursively compare left and right subtrees\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        \n        return True\n```\n\n### Explanation\n- **Type Check**: The function first ensures both nodes are of the same type. If one is a `Node` and the other is a `Leaf`, they are not equivalent.\n- **Leaf Node Comparison**: For `Leaf` nodes, the values are compared using `numpy.allclose` to handle both scalar and array comparisons with a tolerance.\n- **Internal Node Comparison**: For `Node` nodes, the function checks if the feature index and threshold are the same within the tolerance. It then recursively checks the left and right subtrees to ensure they are structurally and functionally equivalent.\n\nThis approach ensures that the trees are compared thoroughly, checking both structure and values at each node, ensuring equivalence within the specified tolerance."}
{"task_id": 328, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that scans through a list of words (corpus) and returns the first word whose first character is an uppercase letter. If no such word exists, the function should return an empty list.\n\n### Approach\n1. **Iterate through each word in the corpus**: We will loop through each word in the given list of words.\n2. **Check for non-empty words**: For each word, we first check if it is non-empty to avoid any index errors when accessing the first character.\n3. **Check the first character**: For each non-empty word, we check if the first character is an uppercase letter using the `isupper()` method.\n4. **Return the first valid word**: If a word meets the criteria, we return it in a list. If no such word is found after checking all words, we return an empty list.\n\nThis approach ensures that we efficiently find the first capitalized word or determine that none exists.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []\n```\n\n### Explanation\n- **Iteration**: The function iterates through each word in the input list `corpus`.\n- **Non-empty Check**: For each word, it checks if the word is non-empty to avoid index errors.\n- **Uppercase Check**: It then checks if the first character of the word is uppercase using `isupper()`.\n- **Return**: If a word meets the criteria, it is returned in a list. If no such word is found, an empty list is returned.\n\nThis solution efficiently handles all edge cases, including empty words and words with leading whitespace, ensuring robust performance across various inputs."}
{"task_id": 364, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the mathematical sign (or signum) function. This function takes a real number as input and returns 1 if the number is positive, -1 if the number is negative, and 0 if the number is zero. The solution must handle both integers and floating-point numbers efficiently.\n\n### Approach\nThe approach to solve this problem involves using conditional statements to check the value of the input number. Here's a step-by-step breakdown of the approach:\n\n1. **Check if the number is positive**: If the input number is greater than zero, the function returns 1.\n2. **Check if the number is negative**: If the input number is less than zero, the function returns -1.\n3. **Handle zero**: If the number is neither positive nor negative, it must be zero, so the function returns 0.\n\nThis approach ensures that all possible cases for the input number are covered efficiently and correctly.\n\n### Solution Code\n```python\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\nThe function `sign` takes a single argument `x`, which can be either an integer or a floating-point number. The function uses a series of conditional checks to determine the sign of `x`:\n\n1. **Positive Check**: The first condition checks if `x` is greater than zero. If true, the function returns 1.\n2. **Negative Check**: The second condition checks if `x` is less than zero. If true, the function returns -1.\n3. **Zero Check**: If neither of the above conditions is met, it means `x` is zero, and the function returns 0.\n\nThis approach ensures that the function handles all possible values of `x` correctly and efficiently, providing the correct signum value for any real number input."}
{"task_id": 378, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that acts as an indicator function, returning 1 if the input flag is exactly 1, and 0 for any other value. This function is straightforward and can be efficiently implemented using a simple conditional check.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Input Value**: The function takes an integer as input. We need to check if this integer is exactly 1.\n2. **Return the Appropriate Value**: If the input integer is 1, the function returns 1. For any other value, it returns 0.\n\nThis can be efficiently implemented using a one-liner with a conditional expression, which makes the code concise and easy to read.\n\n### Solution Code\n```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```\n\n### Explanation\n- **Function Definition**: The function `indicator` is defined to take a single parameter `flag` of type integer.\n- **Return Statement**: The return statement uses a conditional expression to check if `flag` is 1. If true, it returns 1; otherwise, it returns 0.\n- **Efficiency**: This approach is efficient with a time complexity of O(1) since it involves a single comparison operation.\n\nThis solution is optimal for the given problem as it directly addresses the requirement with minimal computational overhead."}
{"task_id": 379, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to sort a list of nodes based on their priority and entry_id, and then return a list of their keys in the correct order. Each node is represented as a dictionary with specific keys, and the sorting must follow a specific set of rules.\n\n### Approach\n1. **Problem Analysis**: Each node has a 'priority' and an 'entry_id'. The nodes must be sorted such that:\n   - Nodes with smaller priority values come first.\n   - If two nodes have the same priority, the node with the smaller entry_id comes first.\n\n2. **Sorting Strategy**: We can use Python's built-in `sorted()` function, which allows us to specify a custom key for sorting. The key will be a tuple consisting of the node's priority and entry_id. This ensures that the nodes are sorted first by priority and then by entry_id if priorities are equal.\n\n3. **Extracting Keys**: After sorting the nodes, we extract the 'key' from each node and return them in a list.\n\n### Solution Code\n```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    return [node['key'] for node in sorted_nodes]\n```\n\n### Explanation\n- **Sorting**: The `sorted()` function is used with a lambda function as the key. This lambda function returns a tuple `(priority, entry_id)` for each node, ensuring that nodes are sorted by priority first and then by entry_id if priorities are equal.\n- **Extracting Keys**: After sorting, we create a list comprehension that extracts the 'key' from each node in the sorted list and returns this list.\n\nThis approach efficiently sorts the nodes according to the given rules and handles all edge cases, including an empty input list."}
{"task_id": 399, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Euclidean distance matrix between two sets of vectors. The goal is to create a matrix where each element represents the Euclidean distance between corresponding points from the two sets. If the input arrays are not valid (i.e., not 2D or have different numbers of columns), we return -1.\n\n### Approach\n1. **Input Validation**: Check if both input arrays are 2D and have the same number of columns. If not, return -1.\n2. **Compute Squared Norms**: Calculate the squared Euclidean norms for each row in both arrays. This helps in efficiently computing the distances using vectorized operations.\n3. **Dot Product Matrix**: Compute the dot product matrix between the two arrays to facilitate the efficient calculation of pairwise distances.\n4. **Squared Distances**: Use the formula for squared Euclidean distance, which is derived from the norms and the dot product matrix.\n5. **Square Root and Rounding**: Take the square root of the squared distances to get the actual Euclidean distances and round each value to four decimal places.\n6. **Conversion to List**: Convert the resulting NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # Check if X and Y are 2-dimensional\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    \n    # Check if the number of columns (d) is the same for both X and Y\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    # Compute squared norms for each row in X and Y\n    X_squared = np.sum(X ** 2, axis=1)\n    Y_squared = np.sum(Y ** 2, axis=1)\n    \n    # Compute the dot product matrix\n    X_dot_YT = X.dot(Y.T)\n    \n    # Compute squared distances using the formula\n    squared_dist = X_squared[:, np.newaxis] + Y_squared - 2 * X_dot_YT\n    \n    # Compute the square root to get the Euclidean distances\n    dist = np.sqrt(squared_dist)\n    \n    # Round to four decimal places\n    rounded_dist = np.round(dist, 4)\n    \n    # Convert to a list of lists\n    return rounded_dist.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if both input arrays are 2D and have the same number of columns. If either condition is not met, it returns -1.\n2. **Squared Norms**: The squared norms of each row in both arrays are computed to avoid recalculating these values multiple times.\n3. **Dot Product Matrix**: The dot product of each row in X with each row in Y is computed efficiently using matrix multiplication.\n4. **Squared Distances**: Using the formula for squared Euclidean distance, the distances are computed in a vectorized manner, which is efficient and avoids explicit loops.\n5. **Square Root and Rounding**: The square root of the squared distances gives the actual Euclidean distances, which are then rounded to four decimal places for precision.\n6. **Conversion**: The resulting NumPy array is converted to a list of lists to meet the output requirements.\n\nThis approach ensures that the solution is both efficient and easy to understand, leveraging NumPy's capabilities for vectorized operations to handle potentially large input sizes effectively."}
{"task_id": 420, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate two Boolean masks from a given 2-D NumPy array based on a specified column and split value. These masks will help in splitting the data for tree-based machine learning algorithms.\n\n### Approach\n1. **Input Validation**: Check if the provided column index is within the valid range of the array's columns. If not, return two empty lists.\n2. **Extract Column Data**: If the column index is valid, extract the specified column from the array.\n3. **Generate Masks**: Create two Boolean masks:\n   - `left_mask`: True for rows where the column value is strictly less than the split value.\n   - `right_mask`: True for rows where the column value is greater than or equal to the split value.\n4. **Return Masks**: Convert the Boolean arrays to Python lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    \n    col = X[:, column]\n    left_mask = (col < value).tolist()\n    right_mask = (col >= value).tolist()\n    \n    return (left_mask, right_mask)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the column index is within the valid range. If not, it returns empty lists to indicate an invalid column.\n- **Column Extraction**: Using NumPy's slicing, the specified column is extracted from the array.\n- **Mask Generation**: Using element-wise comparisons, two Boolean arrays are created. These arrays are then converted to Python lists using the `tolist()` method to ensure compatibility with non-NumPy code.\n- **Return**: The function returns a tuple containing the two Boolean masks, which can be used to split the data as required by tree-based algorithms."}
{"task_id": 424, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of uncertainty or impurity in a set of data, which is particularly useful in machine learning for decision tree algorithms.\n\n### Approach\nThe Shannon entropy \\(H\\) is defined as:\n\\[\nH = -\\sum_{i=1}^{k} p_i \\log_2 p_i\n\\]\nwhere \\(p_i\\) is the relative frequency of each class label \\(i\\).\n\nThe steps to compute the Shannon entropy are as follows:\n1. **Count Frequencies**: Use numpy's `unique` function to get the unique class labels and their respective counts.\n2. **Compute Probabilities**: Convert these counts into probabilities by dividing each count by the total number of elements in the list.\n3. **Calculate Entropy**: For each probability, compute the term \\(p_i \\log_2 p_i\\), sum these terms, and then take the negative of this sum to get the entropy.\n4. **Round the Result**: Round the computed entropy to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    unique_labels, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    sum_terms = np.sum(probabilities * np.log2(probabilities))\n    H = -sum_terms\n    return round(H, 4)\n```\n\n### Explanation\n1. **Counting Frequencies**: The `np.unique` function is used to find the unique class labels and their counts. This helps in determining how many times each label appears in the list.\n2. **Probabilities Calculation**: Each count is divided by the total number of elements to get the probability of each label. This step converts the counts into a probability distribution.\n3. **Entropy Calculation**: Using the probabilities, we compute each term \\(p_i \\log_2 p_i\\), sum these terms, and then take the negative of the sum to get the entropy. This step follows the mathematical definition of Shannon entropy.\n4. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements, ensuring precision and consistency in the output.\n\nThis approach efficiently computes the Shannon entropy using numpy for numerical operations, ensuring accuracy and performance."}
{"task_id": 429, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a helper function that acts as an oracle for a contextual Bernoulli bandit problem. The function will determine the optimal expected reward and the corresponding arm index based on a given context. If the context is invalid, the function will return a specific error tuple.\n\n### Approach\n1. **Validation of Context**: The function first checks if the provided context is a valid one-hot vector. A valid one-hot vector must be one-dimensional, have the same length as the number of contexts in `context_probs`, sum to exactly 1, and contain only 0s and 1s.\n2. **Determine Current Context**: If the context is valid, the function identifies the current context by finding the index of the 1 in the one-hot vector.\n3. **Find Optimal Arm**: Using the identified context, the function retrieves the corresponding row from `context_probs` and determines the arm with the highest success probability. If multiple arms have the same maximum probability, the smallest index is chosen.\n4. **Return Result**: The function returns the optimal expected reward (rounded to four decimal places) and the index of the optimal arm.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, -1).\n    \"\"\"\n    # Check if context is a 1D array\n    if context.ndim != 1:\n        return (-1, -1)\n    \n    D = context_probs.shape[0]\n    \n    # Check if context has the correct length\n    if context.size != D:\n        return (-1, -1)\n    \n    # Check if context is a valid one-hot vector\n    if np.sum(context) != 1:\n        return (-1, -1)\n    \n    if not np.all(np.isin(context, [0, 1])):\n        return (-1, -1)\n    \n    # Determine the current context index\n    current_context = np.argmax(context)\n    \n    # Extract the corresponding row from context_probs\n    row = context_probs[current_context]\n    \n    # Find the optimal arm (smallest index if there are ties)\n    optimal_arm = np.argmax(row)\n    optimal_rwd = row[optimal_arm]\n    \n    # Round the reward to four decimal places\n    optimal_rwd = np.round(optimal_rwd, 4)\n    \n    return (optimal_rwd, optimal_arm)\n```\n\n### Explanation\n1. **Validation Checks**: The function starts by validating the context to ensure it meets the criteria of a one-hot vector. This includes checking the dimensionality, length, sum, and element values.\n2. **Context Identification**: The index of the 1 in the context vector is found using `np.argmax`, which gives the current context.\n3. **Optimal Arm Calculation**: The corresponding row in `context_probs` for the current context is retrieved. The arm with the highest success probability is identified using `np.argmax`, which also handles ties by selecting the smallest index.\n4. **Result Formatting**: The optimal reward is rounded to four decimal places and returned along with the optimal arm index.\n\nThis approach ensures that the function efficiently and correctly identifies the optimal action in a contextual bandit setting, adhering to the constraints and requirements specified."}
