{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of Generalized Linear Model (GLM) link functions. Each link function includes the link function itself, its inverse, and its first derivative. These functions must work with both numpy scalars and numpy arrays, performing element-wise operations.\n\n### Approach\nThe problem requires implementing three specific link functions: identity, log, and logit. Each of these functions must be able to handle numpy arrays and scalars. The approach involves defining each link function, its inverse, and its derivative using numpy operations to ensure element-wise processing.\n\n1. **Identity Link**: \n   - The link function is the identity function, which returns the input as is.\n   - The inverse function is also the identity function.\n   - The derivative of the identity function is 1 for all inputs.\n\n2. **Log Link**:\n   - The link function is the natural logarithm.\n   - The inverse function is the exponential function.\n   - The derivative of the log function is the reciprocal of the input.\n\n3. **Logit Link**:\n   - The link function is the logit function, which is the logarithm of the odds.\n   - The inverse function is the logistic function.\n   - The derivative of the logit function is the product of the input and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: np.ones_like(mu)\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\nThe function `glm_links` constructs a dictionary with three keys: 'identity', 'log', and 'logit'. Each key maps to another dictionary containing the link function, its inverse, and its derivative.\n\n- **Identity Link**: \n  - `link`: Returns the input as is.\n  - `inv_link`: Returns the input as is.\n  - `derivative`: Returns an array of ones with the same shape as the input.\n\n- **Log Link**:\n  - `link`: Applies the natural logarithm to the input.\n  - `inv_link`: Applies the exponential function to the input.\n  - `derivative`: Returns the reciprocal of the input.\n\n- **Logit Link**:\n  - `link`: Computes the logit of the input, which is the logarithm of the odds.\n  - `inv_link`: Computes the logistic function of the input.\n  - `derivative`: Computes the derivative of the logit function, which is the product of the input and its complement.\n\nThis implementation ensures that all functions work efficiently with numpy arrays and scalars, providing the necessary element-wise operations for GLM computations."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to build an isolation tree for a given data matrix and compute the depth of each sample's leaf node. The depth is defined as the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Tree Construction**: The tree is built recursively. Each node receives a set of row indices. If fewer than three samples reach a node, it becomes a leaf. Otherwise, the node splits the data based on a randomly chosen feature and a randomly chosen split value within the range of that feature's values in the current node.\n2. **Traversal and Depth Calculation**: After constructing the tree, each sample is traversed from the root to the leaf node, counting the number of edges (depth) taken to reach the leaf.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    \n    class Node:\n        def __init__(self):\n            self.is_leaf = False\n            self.left = None\n            self.right = None\n            self.feature = None\n            self.value = None\n            self.indices = None\n    \n    def build_node(indices):\n        if len(indices) < 3:\n            node = Node()\n            node.is_leaf = True\n            node.indices = indices\n            return node\n        else:\n            f = np.random.choice(data.shape[1])\n            min_val = data[indices, f].min()\n            max_val = data[indices, f].max()\n            v = np.random.uniform(min_val, max_val)\n            left_indices = []\n            right_indices = []\n            for idx in indices:\n                if data[idx, f] <= v:\n                    left_indices.append(idx)\n                else:\n                    right_indices.append(idx)\n            node = Node()\n            node.feature = f\n            node.value = v\n            node.left ="}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation function and its gradient for a given input. The function should handle different input types, including scalars, lists, and NumPy arrays, and return the results in the appropriate format.\n\n### Approach\n1. **Sigmoid Function**: The sigmoid function maps any real-valued input to a value between 0 and 1. It is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Gradient Calculation**: The gradient of the sigmoid function is given by \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n3. **Input Handling**: The function should handle different input types:\n   - **Scalars**: If the input is an integer or float, the function returns the sigmoid and gradient as floats.\n   - **Arrays and Lists**: If the input is a list or a NumPy array, the function processes it as an array, computes the sigmoid and gradient element-wise, and returns the results as lists.\n4. **Rounding**: All results are rounded to 4 decimal places. For array-like inputs, the results are converted to Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Scalar case\n        sigmoid = 1.0 / (1.0 + np.exp(-x))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Array-like case\n        if isinstance(x, list):\n            x = np.array(x)\n        sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n        gradient_x = sigmoid_x * (1 - sigmoid_x)\n        sigmoid_rounded = np.round(sigmoid_x, 4)\n        gradient_rounded = np.round(gradient_x, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar (int, float, or 0-dimensional NumPy array), the function computes the sigmoid and gradient directly and returns them as floats rounded to 4 decimal places.\n- **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array if necessary, computes the sigmoid and gradient element-wise, rounds each element to 4 decimal places, and converts the results back to Python lists.\n- **Efficiency**: The use of NumPy ensures efficient computation, especially for large arrays, by leveraging vectorized operations.\n\nThis approach ensures that the function is versatile and handles various input types efficiently while providing the required output format."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function - RBF) kernel. The decision function for each test sample is computed using the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\n1. **Compute Squared Norms**: Calculate the squared Euclidean norms of the training and test samples. This helps in efficiently computing the squared distances between each training sample and each test sample.\n2. **Dot Products**: Compute the dot products between each training sample and each test sample. This is used to efficiently calculate the squared distances.\n3. **Squared Distances**: Using the squared norms and dot products, compute the squared Euclidean distances between each training sample and each test sample.\n4. **Kernel Matrix**: Apply the Gaussian kernel function to the squared distances to get the kernel matrix, which measures the similarity between samples.\n5. **Decision Function**: Compute the decision function for each test sample using the Lagrange multipliers, class labels, kernel matrix, and bias term.\n6. **Predict Labels**: Determine the predicted class labels based on the sign of the decision function values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    # Compute squared norms of training and test samples\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    \n    # Compute dot products between training and test samples\n    dot_products = X_train.dot(X_test.T)\n    \n    # Calculate squared distances using the expanded form\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * dist_sq)\n    \n    # Multiply alpha with corresponding y_train values\n    alpha_y = alpha * y_train\n    \n    # Compute the decision function for each test sample\n    g = alpha_y.dot(K) + b\n    \n    # Determine the predicted labels based on the sign of g\n    signs = np.where(g >= 0, 1, -1)\n    \n    return signs.tolist()\n```\n\n### Explanation\n1. **Squared Norms**: The squared norms of the training and test samples are computed to facilitate the efficient calculation of squared distances.\n2. **Dot Products**: The dot products between training and test samples are computed to avoid redundant calculations when determining distances.\n3. **Squared Distances**: Using the squared norms and dot products, the squared distances between each training and test sample are computed efficiently.\n4. **Kernel Matrix**: The Gaussian kernel function transforms these squared distances into similarity scores, which are stored in a matrix.\n5. **Decision Function**: The decision function for each test sample is computed by taking the weighted sum of the kernel values (using Lagrange multipliers and class labels) and adding the bias term.\n6. **Predict Labels**: The sign of the decision function values determines the predicted class labels, ensuring each label is either 1 or -1.\n\nThis approach ensures that the solution is both efficient and scalable, leveraging vectorized operations to handle arbitrary numbers of training and test samples."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy tensor filled with 0.0s and 1.0s based on a specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and return the appropriate output.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Setting**: If a seed is provided, set the random seed to ensure reproducible results.\n3. **Random Array Generation**: Generate a random array of the specified shape using a uniform distribution between 0 and 1.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity`. This threshold determines the probability of an element being 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where each element is 1.0 if it is greater than or equal to the threshold, otherwise 0.0. This is done using a boolean mask and casting to float to ensure the correct data type.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns -1 immediately.\n- **Seed Setting**: If a seed is provided, it sets the random seed to ensure that the same sequence of random numbers is generated each time the function is called with the same seed.\n- **Random Array Generation**: Using `np.random.rand`, the function generates a random array of the specified shape with values uniformly distributed between 0 and 1.\n- **Threshold Calculation**: The threshold is calculated as `1 - sparsity`, which determines the cutoff for converting random values to 1.0.\n- **Binary Conversion**: The random values are compared against the threshold. Values greater than or equal to the threshold are converted to 1.0, and others to 0.0. The result is cast to a float array to ensure the correct data type.\n\nThis approach ensures that the function efficiently generates the required binary tensor while adhering to the specified constraints and providing reproducible results when a seed is provided."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by recursively computing the probabilities of observing the remaining sequence from each state at each time step.\n\n### Approach\n1. **Check for Empty Inputs**: If any of the input lists (transition matrix `A`, emission matrix `B`, initial distribution `pi`, or observation sequence `obs`) are empty, return 0.0 immediately as no valid probability can be computed.\n\n2. **Initialize Parameters**: Determine the number of states `N` from the length of the initial distribution `pi` and the length of the observation sequence `T` from the observation list `obs`.\n\n3. **Backward Algorithm Initialization**: Initialize the backward variables `beta` for the last time step `T-1` to 1.0 for all states, as there are no future observations beyond this point.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to 0, compute the backward variables `beta` for each state by summing over all possible next states, weighted by the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Compute Final Probability**: After computing the backward variables for all time steps, calculate the final probability by summing the contributions from each state, weighted by the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\n6. **Return Result**: Return the computed probability rounded to six decimal places.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    N = len(pi)\n    T = len(obs)\n    \n    if T == 0:\n        return 0.0\n    \n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n    \n    # Iterate from t = T-2 down to 0\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0] * N\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                # A[i][j] is the transition from i to j\n                # B[j][obs[t+1]] is the emission probability for the next observation\n                sum_val += A[i][j] * B[j][obs[t+1]] * beta[j]\n            new_beta[i] = sum_val\n        beta = new_beta\n    \n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    \n    # Round to six decimal places\n    return round(prob, 6)\n```\n\n### Explanation\n- **Initialization**: The backward variables `beta` are initialized to 1.0 for the last time step because there are no future observations to consider.\n- **Recursive Calculation**: For each time step, the backward variables are updated by considering all possible transitions from the current state to the next state, weighted by the transition probabilities and emission probabilities.\n- **Final Probability**: The final probability is computed by summing the contributions from each state, considering the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\nThis approach efficiently computes the probability of the observation sequence using the backward algorithm, ensuring that the result is accurate and rounded to six decimal places."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward algorithm for Hidden Markov Models (HMMs) to compute the likelihood of an observation sequence given the model parameters. The model is specified by the initial state probabilities, state transition matrix, and emission matrix. The goal is to validate the input and compute the sequence likelihood efficiently.\n\n### Approach\n1. **Input Validation**: Ensure that the input dimensions are correct and that all observations are within the valid range. If any input is invalid, return -1.\n2. **Forward Algorithm**: Use dynamic programming to compute the probability of each observation sequence step-by-step. This involves initializing the forward probabilities for the first observation and iteratively updating them for each subsequent observation.\n3. **Result Calculation**: Sum the final forward probabilities to get the total likelihood of the observation sequence and return it rounded to four decimal places.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check for empty inputs\n    if not S or not A or not B or not observations:\n        return -1\n    \n    n = len(S)\n    # Check if A is n x n\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check if B is n x m\n    if len(B) != n:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check observations are within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Initialize forward variables\n    T = len(observations)\n    f = [S[i] * B[i][observations[0]] for i in range(n)]\n    \n    # Iterate through the rest of the observations\n    for t in range(1, T):\n        next_f = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += f[j] * A[j][i]\n            next_f[i] = sum_val * B[i][observations[t]]\n        f = next_f\n    \n    # Sum the final probabilities\n    result = sum(f)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if any of the input lists are empty. It then verifies that the dimensions of the state transition matrix `A` and emission matrix `B` match the initial state probabilities `S`. Each observation is checked to ensure it falls within the valid range of emission symbols.\n2. **Forward Initialization**: The forward probabilities are initialized using the initial state probabilities and the emission probabilities for the first observation.\n3. **Forward Iteration**: For each subsequent observation, the forward probabilities are updated by considering all possible transitions from the previous states. This is done using nested loops to compute the sum of probabilities from all previous states multiplied by the transition probabilities.\n4. **Result Calculation**: The final result is the sum of the forward probabilities after processing all observations, rounded to four decimal places.\n\nThis approach ensures that the function efficiently computes the likelihood of the observation sequence while handling invalid inputs appropriately."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the unnormalized categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in machine learning, especially in classification tasks where the output is a probability distribution over classes.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Inputs to NumPy Arrays**: Ensure that both the true labels `y` and the predicted probabilities `y_pred` are converted to NumPy arrays for efficient element-wise operations.\n\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to each element of `y_pred`. This ensures that the logarithm operation is always applied to a positive number.\n\n3. **Compute Logarithms**: Calculate the natural logarithm of each element in the modified `y_pred` array.\n\n4. **Element-wise Multiplication**: Multiply each element of `y` with the corresponding element in the logarithm array. This step effectively zeros out all terms where the true label is not the correct class, as `y` is one-hot encoded.\n\n5. **Sum and Negate**: Sum all the resulting values and negate the sum to get the final loss value. This is because the cross-entropy loss is defined as the negative sum of the log probabilities.\n\n6. **Round the Result**: Finally, round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can leverage NumPy's efficient array operations, which are crucial for handling potentially large datasets efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in negative infinity and cause numerical instability in the loss calculation.\n- **Logarithm Calculation**: The natural logarithm of each predicted probability is computed to transform the multiplicative probabilities into additive terms, which is a standard step in cross-entropy calculations.\n- **Element-wise Multiplication**: This step ensures that only the log probability of the correct class contributes to the loss for each sample, as all other terms are multiplied by zero.\n- **Sum and Negate**: The sum of these contributions is taken and negated to align with the definition of cross-entropy loss, which measures the difference between the predicted and true distributions.\n- **Rounding**: The final loss value is rounded to four decimal places to provide a precise and readable result.\n\nThis approach efficiently computes the categorical cross-entropy loss while ensuring numerical stability and correctness."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost, also known as log-loss, between the predicted probabilities and the ground-truth binary labels. This cost function is commonly used in binary classification problems to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clip Values**: To avoid taking the logarithm of zero or one, which is undefined, we clip the predicted probabilities to a small value \u03b5 (1e-15) and 1 - \u03b5. This ensures that all values are within the valid range for logarithmic operations.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements.\n3. **Element-wise Multiplication**: Multiply the ground-truth labels with the logarithms of the predicted probabilities and the complements of the ground-truth labels with the logarithms of the complements of the predicted probabilities.\n4. **Sum Terms**: Sum the results of the element-wise multiplications for each sample.\n5. **Average and Negate**: Compute the average of the summed terms and negate the result to get the final cost.\n6. **Round Result**: Return the cost rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = (-1 / m) * np.sum(sum_terms)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The predicted probabilities are clipped to ensure they are within the range [\u03b5, 1 - \u03b5], avoiding undefined logarithmic operations.\n- **Logarithmic Operations**: The natural logarithm of the clipped values and their complements are computed to handle the log terms in the binary cross-entropy formula.\n- **Element-wise Multiplication and Summation**: The terms involving the ground-truth labels and their complements are computed and summed element-wise.\n- **Averaging and Negation**: The summed terms are averaged and negated to compute the final cost, which is then rounded to six decimal places for precision.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be too close to 0 or 1."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate individual predictions from multiple decision trees in a Random Forest classifier using a majority vote mechanism. The goal is to determine the final prediction for each sample by considering the most frequent prediction across all trees. In case of a tie, the smallest label (numerically or lexicographically) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions into a structure where each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions independently.\n2. **Count Votes**: For each sample, count the occurrences of each prediction using a counter.\n3. **Determine Majority Vote**: Identify the prediction(s) with the highest count. If there is a tie, select the smallest label based on numerical or lexicographical order.\n4. **Construct Result**: Collect the final predictions for all samples and return them as a list.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample_predictions = [tree[i] for tree in predictions]\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        max_labels = [label for label, cnt in counts.items() if cnt == max_count]\n        max_labels.sort()\n        result.append(max_labels[0])\n    return result\n```\n\n### Explanation\n1. **Check for Empty Input**: If the input list is empty, return an empty list immediately.\n2. **Determine Number of Samples**: The number of samples is derived from the length of the first inner list, assuming all inner lists are of the same length.\n3. **Process Each Sample**: For each sample, gather all predictions from each tree, count the occurrences of each prediction, and determine the most frequent prediction(s).\n4. **Handle Ties**: If multiple predictions have the same highest count, sort them and select the smallest one.\n5. **Return Result**: Collect the final predictions for all samples and return them as a list.\n\nThis approach ensures that each sample's prediction is determined efficiently and correctly, adhering to the majority vote mechanism with tie-breaking by selecting the smallest label."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function in a numerically stable manner. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation using \\( \\exp \\) can lead to overflow for large positive values and underflow for very small negative values. To avoid these issues, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept scalar values (int or float), lists, or NumPy arrays. The output should be a float for scalar inputs and a Python list for array-like inputs.\n2. **Numerical Stability**: Use `np.logaddexp(0.0, z)` to compute the softplus function. This avoids overflow and underflow issues by leveraging the numerically stable implementation provided by NumPy.\n3. **Rounding**: Each computed value is rounded to 4 decimal places.\n4. **Output Conversion**: For array-like inputs, the result is converted to a pure Python list using `tolist()`. For scalar inputs, the result is returned as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Determine if z is a scalar\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        s = np.logaddexp(0.0, z)\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        # Treat as array-like\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\n```\n\n### Explanation\n1. **Input Check**: The function first checks if the input `z` is a scalar (int, float, or 0-dimensional array). If so, it computes the softplus value directly and returns it as a float after rounding.\n2. **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array, computes the softplus values using `logaddexp`, rounds each value to 4 decimal places, and converts the result to a Python list.\n3. **Numerical Stability**: By using `np.logaddexp(0.0, z)`, the function ensures that the computation is numerically stable, avoiding overflow for large positive values and underflow for very small negative values.\n\nThis approach ensures that the softplus function is computed efficiently and accurately for various input types, providing the correct output format as specified."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nB"}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions for character-level sequence encoding and decoding. These functions will be used in a neural model to process text data, convert it into a format suitable for model input, and then convert the model's predictions back into human-readable text.\n\n### Approach\n\n1. **string_to_int**: This function converts a raw text string into a fixed-length list of integer token IDs. The fixed length is determined by the constant `TIME_STEPS` (set to 20). If the input string is shorter than `TIME_STEPS`, it is padded with zeros. If it is longer, it is truncated to fit the fixed length. Characters not found in the provided vocabulary are also converted to zeros.\n\n2. **int_to_string**: This function converts a list of integer token IDs back into a text string using an inverse vocabulary. Padding tokens (zeros) are ignored during the conversion.\n\n3. **run_example**: This function ties everything together. It encodes the input text, feeds it to a model for prediction, processes the model's output to get the predicted token IDs, and then decodes these IDs back into a text string.\n\n### Solution Code\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a string into a fixed-length list of integer token IDs.\"\"\"\n    # Take the first 'time_steps' characters\n    substring = text[:time_steps]\n    encoded = []\n    for c in substring:\n        # Use the vocabulary, default to 0 if not found\n        encoded.append(vocabulary.get(c, 0))\n    # Pad with zeros if necessary\n    while len(encoded) < time_steps:\n        encoded.append(0)\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token IDs back to a string, ignoring padding.\"\"\"\n    s = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        if idx in inverse_vocab:\n            s.append(inverse_vocab[idx])\n    return ''.join(s)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes text, runs through model, and decodes the prediction.\"\"\"\n    # Encode the input text\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Prepare the input for the model (batch size 1)\n    encoded_batch = np.array([encoded])\n    # Get model predictions\n    predictions = model.predict(encoded_batch)\n    # Apply argmax to get the predicted class indices\n    predicted_ids = np.argmax(predictions, axis=2)\n    # Extract the first (and only) sequence from the batch\n    predicted_ids = predicted_ids[0]\n    # Decode the predicted IDs to a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n\n1. **string_to_int**:\n   - This function processes each character of the input string up to the specified `time_steps` length.\n   - Each character is converted to its corresponding integer ID using the provided vocabulary. If a character is not found in the vocabulary, it is converted to 0.\n   - If the resulting list of integer IDs is shorter than `time_steps`, it is padded with zeros to ensure the fixed length.\n\n2. **int_to_string**:\n   - This function converts a list of integer IDs back into a string using an inverse vocabulary.\n   - Padding tokens (zeros) are ignored during the conversion, ensuring they do not appear in the final output string.\n\n3. **run_example**:\n   - This function first encodes the input text into a fixed-length list of integer IDs.\n   - The encoded list is converted into a numpy array and fed into the model for prediction.\n   - The model's output is processed to get the predicted token IDs using argmax.\n   - These predicted IDs are then decoded back into a human-readable string, ignoring any padding tokens.\n\nThis approach ensures that the text is properly formatted for model input and that the model's predictions are accurately converted back into readable text, making it suitable for character-level sequence modeling tasks."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the execution of the model on each input string and the formatting of the output for easy review.\n\n### Approach\n1. **Check for Examples**: The function first checks if the `examples` argument is provided. If not, it falls back to a global constant `EXAMPLES`.\n2. **Iterate Through Examples**: For each input string in the examples, the function calls another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Each prediction is converted from a list of characters into a string. The input and output strings are then printed in a specified format.\n4. **Collect Predictions**: All predicted strings are collected into a list, which is returned at the end.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n        predictions.append(predicted_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Examples**: The function checks if `examples` is `None` and uses the global `EXAMPLES` if necessary.\n- **Running the Model**: For each input string, the `run_example` function is called to get the model's prediction as a list of characters.\n- **Formatting Output**: Each prediction is converted into a string and printed alongside the input string for easy review.\n- **Collecting Results**: All predictions are collected into a list and returned, maintaining the order of the input examples.\n\nThis approach ensures that the function is both efficient and user-friendly, providing clear output for quick review while maintaining the necessary structure for further processing."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for logistic regression. This function will be used to evaluate the performance of a logistic regression model and to update the model's weights during training.\n\n### Approach\n1. **Compute the Linear Combination**: For each sample, compute the linear combination of features and weights, denoted as \\( z = X \\cdot w \\).\n2. **Apply Sigmoid Function**: Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities**: To avoid numerical issues when taking logarithms, clip the probabilities to a small constant \\( \\epsilon \\) to ensure they are within the range \\([ \\epsilon, 1 - \\epsilon ]\\).\n4. **Compute Loss**: Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [ y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p) ] \\).\n5. **Compute Gradient**: Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Round Results**: Round the loss to 4 decimal places and the gradient to 4 decimal places, converting the gradient into a nested list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    \n    # Reshape y and w to 1D arrays\n    y = y.ravel()\n    w = w.ravel()\n    \n    # Compute z = Xw\n    z = X.dot(w)\n    \n    # Compute p = sigmoid(z)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip p to avoid numerical issues\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute loss\n    loss = - (y * np.log(p) + (1 - y) * np.log(1 - p)).sum() / m\n    loss = round(loss, 4)\n    \n    # Compute gradient\n    gradient = (X.T.dot(p - y)) / m\n    gradient = np.round(gradient, 4)\n    gradient = gradient.reshape(-1, 1).tolist()\n    \n    return (loss, gradient)\n```\n\n### Explanation\n1. **Linear Combination**: The linear combination \\( z \\) is computed using matrix multiplication of the feature matrix \\( X \\) and the weight vector \\( w \\).\n2. **Sigmoid Function**: The sigmoid function transforms the linear combination into probabilities, which are then clipped to avoid numerical issues when taking logarithms.\n3. **Loss Calculation**: The binary cross-entropy loss is computed by averaging the negative log-likelihood across all samples.\n4. **Gradient Calculation**: The gradient is computed using the transpose of the feature matrix and the difference between the predicted probabilities and the actual targets.\n5. **Rounding and Formatting**: The loss is rounded to 4 decimal places, and the gradient is rounded and converted into a nested list format for the final output.\n\nThis approach ensures that the function efficiently computes both the loss and its gradient, handling numerical stability issues and providing results in the required format."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an actor network in a reinforcement learning context. The network consists of three linear layers with ReLU activations after the first two layers and a tanh activation at the output layer. The goal is to convert a given state vector into an action vector using the provided weights and biases.\n\n### Approach\n1. **Convert State to Numpy Array**: Ensure the input state is a numpy array for consistent operations.\n2. **Linear Transformation and ReLU Activation**: Compute the output of each linear layer followed by a ReLU activation. This involves matrix multiplication with the weights and addition of biases, followed by applying the ReLU function.\n3. **Final Linear Transformation and Tanh Activation**: The output of the second ReLU layer is passed through the final linear layer, followed by a tanh activation to ensure the action values are within the range [-1, 1].\n4. **Rounding**: Convert the resulting action vector to a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a numpy array if it's not already\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    \n    # First linear layer followed by ReLU\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    \n    # Second linear layer followed by ReLU\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    \n    # Third linear layer followed by tanh\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    \n    # Round each element to four decimal places and convert to list\n    return [round(x, 4) for x in action]\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted to a numpy array to facilitate matrix operations.\n2. **First Layer**: The state vector is multiplied by the first weight matrix `W1`, and the bias `b1` is added. The result is passed through the ReLU activation function.\n3. **Second Layer**: The output from the first layer is multiplied by the second weight matrix `W2`, and the bias `b2` is added. Another ReLU activation is applied.\n4. **Output Layer**: The output from the second layer is multiplied by the third weight matrix `W3`, and the bias `b3` is added. The result is passed through the tanh activation function to produce the action vector.\n5. **Rounding**: Each element of the action vector is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the network correctly processes the input state through the defined architecture, producing an action vector suitable for reinforcement learning applications."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability obtained by applying the sigmoid transformation. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input lists, tuples, or NumPy arrays into NumPy arrays for efficient vectorized computations.\n2. **Compute the Gradient**: The gradient for each observation is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the term `-actual * predicted`, applying the sigmoid function to this term, and then multiplying by the actual values.\n3. **Compute the Probability and Hessian**: The probability is obtained by applying the sigmoid function to the predicted values. The Hessian is then computed as the product of the probability and its complement (1 - probability).\n4. **Rounding**: Each computed value (gradient, Hessian, and probability) is rounded to exactly six decimal places to meet the precision requirement.\n5. **Convert Results to Lists**: The NumPy arrays are converted back to Python lists before returning the results.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute the term for the gradient\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    \n    # Compute the probability and Hessian\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    \n    # Round each value to 6 decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert NumPy arrays to Python lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the application of the sigmoid function.\n- **Probability and Hessian**: The probability is derived from the sigmoid function applied to the predicted values. The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Efficiency**: Using NumPy arrays allows for efficient vectorized operations, which is crucial for handling large datasets.\n- **Precision**: Each result is rounded to six decimal places to ensure the required precision, and the results are converted back to Python lists for compatibility with the expected output format."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is commonly used in neural networks to avoid the issue of \"dying\" neurons. The function should handle various input types, including scalars, nested lists, and numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For any input value \\( x \\geq 0 \\), the output is \\( x \\).\n- For any input value \\( x < 0 \\), the output is \\( a \\times x \\), where \\( a \\) is a parameter in the range [0, 1).\n\nThe function must handle different input types:\n1. **Scalars**: Directly apply the Leaky ReLU transformation.\n2. **Lists (possibly nested)**: Recursively process each element to apply the transformation.\n3. **Numpy Arrays**: Use vectorized operations for efficient computation and then convert the result to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n1. **Handling Numpy Arrays**: If the input `z` is a numpy array, we use `np.where` to apply the Leaky ReLU transformation element-wise. This is efficient and leverages numpy's vectorized operations. The result is then converted to a Python list using `tolist()` to maintain the required output format.\n\n2. **Handling Lists**: If the input `z` is a list, we recursively process each element. This ensures that nested lists are handled correctly, maintaining the structure of the input.\n\n3. **Handling Scalars**: If the input `z` is a scalar, we directly apply the Leaky ReLU transformation. If `z` is non-negative, it remains unchanged; otherwise, it is multiplied by the parameter `a`.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using NumPy. The network consists of three dense layers followed by two separate output heads for the actor and critic. The weights and biases are fixed, which allows us to compute the network's output deterministically.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers with ReLU activation, followed by an actor head with a softmax activation and a critic head with no activation.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0, simplifying the computations.\n3. **Forward Pass Steps**:\n   - **Dense-1**: Input (3 neurons) \u2192 4 neurons with ReLU activation.\n   - **Dense-2**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Dense-3**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Actor Head**: 4 neurons \u2192 3 neurons with softmax activation to get action probabilities.\n   - **Critic Head**: 4 neurons \u2192 1 neuron to get the state value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    \n    # Dense-1: input (3) -> 4 neurons, ReLU\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    z1 = np.maximum(0, state.dot(W1) + b1)\n    \n    # Dense-2: 4 -> 4 neurons, ReLU\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    z2 = np.maximum(0, z1.dot(W2) + b2)\n    \n    # Dense-3: 4 -> 4 neurons, ReLU\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    z3 = np.maximum(0, z2.dot(W3) + b3)\n    \n    # Actor head: 4 -> 3 neurons, Softmax\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    logits = z3.dot(W_a) + b_a\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    \n    # Critic head: 4 -> 1 neuron\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    state_value = z3.dot(W_c) + b_c\n    state_value = state_value[0]  # Extract the scalar value\n    \n    # Rounding to four decimal places\n    action_probs = [round(p, 4) for p in action_probs]\n    state_value = round(state_value, 4)\n    \n    return action_probs, state_value\n```\n\n### Explanation\n1. **Dense Layers**: Each dense layer transforms the input using matrix multiplication with weights and biases, followed by ReLU activation. Given the fixed weights and biases, each transformation simplifies to a sum of the input elements.\n2. **Actor Head**: The actor head computes logits for each action, which are then converted to probabilities using the softmax function. Due to identical logits, each action has an equal probability of 1/3.\n3. **Critic Head**: The critic head computes the state value as a linear transformation of the output from the last dense layer, resulting in a scalar value.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, leveraging the fixed weights and biases for simplicity."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Elastic-Net penalty and its gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularization, which helps in reducing model complexity and preventing over-fitting. The function should return either the penalty value or the gradient based on the input flag.\n\n### Approach\n1. **Problem Analysis**: The Elastic-Net penalty is defined as a convex combination of L1 and L2 regularization. The penalty function is given by:\n   \\[\n   R(w) = \\alpha \\cdot [\\lambda \\cdot ||w||_2 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w]\n   \\]\n   where \\( ||w||_2 \\) is the L2 norm of the weight vector \\( w \\), and \\( w^T w \\) is the squared L2 norm. The gradient of this penalty function is given by:\n   \\[\n   \\nabla R(w) = \\alpha \\cdot [\\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w]\n   \\]\n   where \\( \\text{sign}(w) \\) is the sign function applied element-wise to \\( w \\).\n\n2. **Algorithm Design**:\n   - Convert the input weight vector to a NumPy array for efficient computation.\n   - Compute the penalty value if the gradient flag is False.\n   - Compute the gradient if the gradient flag is True.\n\n3. **Penalty Calculation**:\n   - Compute the L2 norm of the weight vector.\n   - Compute the sum of the squares of the weight vector elements.\n   - Calculate the L1 and L2 parts of the penalty using the given formula.\n   - Combine these parts and multiply by the regularization strength \\( \\alpha \\).\n\n4. **Gradient Calculation**:\n   - Compute the sign of each element in the weight vector.\n   - Calculate the gradient using the given formula, combining the contributions from both L1 and L2 regularization.\n\n5. **Edge Cases**: Handle cases where the weight vector is empty or contains zeros appropriately.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    if gradient:\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        gradient = [round(x, 4) for x in gradient]\n        return gradient\n    else:\n        l2_norm = np.linalg.norm(w, 2)\n        sum_w_sq = np.dot(w, w)\n        l1_part = l1_ratio * l2_norm\n        l2_part = (1 - l1_ratio) * 0.5 * sum_w_sq\n        penalty = alpha * (l1_part + l2_part)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**: The function first converts the input weight vector into a NumPy array for efficient computation. It then calculates the L2 norm and the sum of squares of the weight vector elements. Using these values, it computes the L1 and L2 parts of the penalty and combines them, scaling by the regularization strength \\( \\alpha \\).\n\n- **Gradient Calculation**: If the gradient flag is True, the function computes the sign of each element in the weight vector. It then calculates the gradient by combining the contributions from both L1 and L2 regularization, scaling by \\( \\alpha \\) and rounding each element to 4 decimal places.\n\nThis approach ensures that the function efficiently computes either the penalty or the gradient based on the input parameters, adhering to the problem constraints and requirements."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes weights in a way that helps in maintaining consistent variance of activations across layers, which is crucial for effective training of deep networks.\n\n### Approach\nThe approach to solve this problem involves three main steps:\n\n1. **Compute Fan In and Fan Out**: \n   - For a 2D shape `(fan_in, fan_out)`, the values are directly taken from the dimensions.\n   - For shapes with more than two dimensions (common in convolutional layers), the fan in and fan out are computed by considering the product of all dimensions beyond the first two (receptive field size).\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation `s` is computed using the formula `s = sqrt(2 / (fan_in + fan_out))`.\n\n3. **Generate Random Array**: \n   - Using the computed standard deviation, generate a NumPy array with elements drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n"}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should be a list of lists where each inner list contains the neighbors of the corresponding vertex in `V`, sorted by their order of appearance in `V`.\n\n### Approach\n1. **Create a Vertex Index Mapping**: First, we create a dictionary to map each vertex to its index in the list `V`. This helps in quickly determining the order of vertices.\n2. **Initialize Adjacency Dictionary**: We use a dictionary where each key is a vertex and the value is a set of its neighbors. This helps in efficiently managing and avoiding duplicate edges.\n3. **Process Edges**: For each edge in `E`, we add each vertex to the other's neighbor set. This ensures that both directions of the undirected edge are accounted for.\n4. **Build the Adjacency List**: For each vertex in `V`, we collect its neighbors, sort them based on their indices in `V`, and construct the adjacency list.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for i, v in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    \n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    \n    return G\n```\n\n### Explanation\n1. **Vertex Index Mapping**: The dictionary `vertex_index` maps each vertex to its position in `V`, which is used to sort neighbors later.\n2. **Adjacency Dictionary**: `adj_dict` is initialized with each vertex having an empty set of neighbors. This helps in efficiently adding and managing neighbors without duplicates.\n3. **Edge Processing**: For each edge `(u, v)`, both `u` is added to `v`'s neighbors and `v` is added to `u`'s neighbors, ensuring the undirected nature of the graph.\n4. **Building the Adjacency List**: For each vertex in `V`, its neighbors are collected, sorted by their indices using `vertex_index`, and added to the result list `G`.\n\nThis approach ensures that the adjacency list is constructed efficiently and correctly, with neighbors sorted as required."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function `get_initializer` that retrieves a weight-initialization function based on a given name. The function should return the corresponding callable initializer if it exists, or raise an error if it does not.\n\n### Approach\n1. **Lookup in"}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same features and thresholds at each internal node, and produce the same predictions at each leaf node, within a specified tolerance.\n\n### Approach\n1. **Type Check**: First, we check if both trees are of the same type (both are either `Node` or `Leaf`). If they are not, the trees are not equivalent.\n2. **Leaf Node Comparison**: If both nodes are `Leaf` nodes, we compare their values using `numpy.allclose` to handle numerical comparisons with a tolerance of \\(1 \\times 10^{-8}\\).\n3. **Internal Node Comparison**: If both nodes are `Node` nodes, we check:\n   - The feature index used for splitting.\n   - The threshold value, ensuring it is the same within the specified tolerance.\n   - Recursively compare the left and right subtrees to ensure they are equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both nodes are of the same type\n    if type(tree_a) != type(tree_b):\n        return False\n    \n    if isinstance(tree_a, Leaf):\n        # Compare leaf values with tolerance\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    else:\n        # Both are Node instances; check feature and threshold\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        \n        # Recursively compare left and right subtrees\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        \n        return True\n```\n\n### Explanation\n- **Type Check**: The function first ensures both nodes are of the same type. If one is a `Node` and the other is a `Leaf`, they are not equivalent.\n- **Leaf Node Comparison**: For `Leaf` nodes, the values are compared using `numpy.allclose` to handle both scalar and array comparisons with a tolerance.\n- **Internal Node Comparison**: For `Node` nodes, the function checks if the feature index and threshold are the same within the tolerance. It then recursively checks the left and right subtrees to ensure they are structurally and functionally equivalent.\n\nThis approach ensures that the trees are compared thoroughly, checking both structure and values at each node, ensuring equivalence within the specified tolerance."}
