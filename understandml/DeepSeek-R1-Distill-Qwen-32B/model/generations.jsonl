{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of Generalized Linear Model (GLM) link functions. Each link function includes the link function itself, its inverse, and its first derivative. These functions must work with both numpy scalars and numpy arrays, performing element-wise operations.\n\n### Approach\nThe problem requires implementing three specific link functions: identity, log, and logit. Each of these functions must be able to handle numpy arrays and scalars. The approach involves defining each link function, its inverse, and its derivative using numpy operations to ensure element-wise processing.\n\n1. **Identity Link**: \n   - The link function is the identity function, which returns the input as is.\n   - The inverse function is also the identity function.\n   - The derivative of the identity function is 1 for all inputs.\n\n2. **Log Link**:\n   - The link function is the natural logarithm.\n   - The inverse function is the exponential function.\n   - The derivative of the log function is the reciprocal of the input.\n\n3. **Logit Link**:\n   - The link function is the logit function, which is the logarithm of the odds.\n   - The inverse function is the logistic function.\n   - The derivative of the logit function is the product of the input and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: np.ones_like(mu)\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\nThe function `glm_links` constructs a dictionary with three keys: 'identity', 'log', and 'logit'. Each key maps to another dictionary containing the link function, its inverse, and its derivative.\n\n- **Identity Link**: \n  - `link`: Returns the input as is.\n  - `inv_link`: Returns the input as is.\n  - `derivative`: Returns an array of ones with the same shape as the input.\n\n- **Log Link**:\n  - `link`: Applies the natural logarithm to the input.\n  - `inv_link`: Applies the exponential function to the input.\n  - `derivative`: Returns the reciprocal of the input.\n\n- **Logit Link**:\n  - `link`: Computes the logit of the input, which is the logarithm of the odds.\n  - `inv_link`: Computes the logistic function of the input.\n  - `derivative`: Computes the derivative of the logit function, which is the product of the input and its complement.\n\nThis implementation ensures that all functions work efficiently with numpy arrays and scalars, providing the necessary element-wise operations for GLM computations."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to build an isolation tree for a given data matrix and compute the depth of each sample's leaf node. The depth is defined as the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Tree Construction**: The tree is built recursively. Each node receives a set of row indices. If fewer than three samples reach a node, it becomes a leaf. Otherwise, the node splits the data based on a randomly chosen feature and a randomly chosen split value within the range of that feature's values in the current node.\n2. **Traversal and Depth Calculation**: After constructing the tree, each sample is traversed from the root to the leaf node, counting the number of edges (depth) taken to reach the leaf.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    \n    class Node:\n        def __init__(self):\n            self.is_leaf = False\n            self.left = None\n            self.right = None\n            self.feature = None\n            self.value = None\n            self.indices = None\n    \n    def build_node(indices):\n        if len(indices) < 3:\n            node = Node()\n            node.is_leaf = True\n            node.indices = indices\n            return node\n        else:\n            f = np.random.choice(data.shape[1])\n            min_val = data[indices, f].min()\n            max_val = data[indices, f].max()\n            v = np.random.uniform(min_val, max_val)\n            left_indices = []\n            right_indices = []\n            for idx in indices:\n                if data[idx, f] <= v:\n                    left_indices.append(idx)\n                else:\n                    right_indices.append(idx)\n            node = Node()\n            node.feature = f\n            node.value = v\n            node.left ="}
{"task_id": 4, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the pair-wise squared Euclidean distance matrix for a given set of samples represented as a 2D NumPy array. The goal is to efficiently compute this matrix using vectorized operations to ensure optimal performance.\n\n### Approach\nThe approach leverages the mathematical identity for the squared Euclidean distance between two points, which allows us to avoid explicit loops and compute the result in an efficient manner. The key steps are as follows:\n\n1. **Sum of Squares Calculation**: Compute the sum of squares for each row in the input matrix. This helps in efficiently calculating the squared distances using the identity.\n2. **Outer Sum Matrix**: Construct a matrix where each element (i, j) is the sum of the squares of the i-th and j-th rows. This is done using broadcasting in NumPy.\n3. **Dot Product Matrix**: Compute the dot product of the input matrix with its transpose. This gives the pairwise dot products of all rows.\n4. **Distance Matrix Calculation**: Using the identity, the distance matrix is derived by subtracting twice the dot product matrix from the outer sum matrix.\n5. **Rounding and Conversion**: The resulting matrix is rounded to 4 decimal places and converted to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A Python list of lists where each element [i][j] holds the squared\n        Euclidean distance between the *i*-th and *j*-th samples in *X*,\n        rounded to 4 decimal places.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n    \n    row_sums = np.sum(X ** 2, axis=1)\n    outer = row_sums[:, np.newaxis] + row_sums[np.newaxis, :]\n    dot_product = X.dot(X.T)\n    D = outer - 2 * dot_product\n    D_rounded = np.round(D, decimals=4)\n    return D_rounded.tolist()\n```\n\n### Explanation\n1. **Sum of Squares Calculation**: The sum of squares for each row is computed using `np.sum(X ** 2, axis=1)`, resulting in a 1D array where each element represents the sum of squares of the corresponding row in the input matrix.\n2. **Outer Sum Matrix**: Using broadcasting, we create a matrix where each element (i, j) is the sum of the i-th and j-th elements of the row sums array. This is achieved by reshaping the row sums array into a column vector and a row vector and adding them together.\n3. **Dot Product Matrix**: The dot product of the input matrix with its transpose is computed using `X.dot(X.T)`, resulting in a matrix where each element (i, j) is the dot product of the i-th and j-th rows of the input matrix.\n4. **Distance Matrix Calculation**: The distance matrix is computed using the identity, which involves subtracting twice the dot product matrix from the outer sum matrix.\n5. **Rounding and Conversion**: The resulting distance matrix is rounded to 4 decimal places using `np.round` and converted to a list of lists using `tolist()` for the final output.\n\nThis approach ensures that the solution is efficient and handles edge cases, such as empty input matrices, appropriately."}
{"task_id": 8, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression model from scratch using batch gradient descent. The goal is to train the model on a given training dataset and then use it to predict class labels for a test dataset.\n\n### Approach\n1. **Initialization**: Start by initializing the model parameters (weights `w` and bias `b`) to zero.\n2. **Training Loop**: For a specified number of iterations, compute the linear score, apply the sigmoid function to get probabilities, and then update the model parameters using the gradients derived from the loss function.\n3. **Gradient Computation**: The gradients for the weights and bias are computed using the error between the predicted probabilities and the true labels.\n4. **Parameter Update**: Update the weights and bias using the computed gradients and the given learning rate.\n5. **Prediction**: After training, use the learned parameters to predict class labels for the test dataset by applying the sigmoid function and rounding the results.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\"\"\"\n    m = X_train.shape[0]\n    n_features = X_train.shape[1]\n    \n    # Initialize parameters\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(n_iterations):\n        # Compute linear combination\n        z = np.dot(X_train, w) + b\n        # Apply sigmoid function\n        sigmoid_z = 1 / (1 + np.exp(-z))\n        # Compute error\n        error = sigmoid_z - y_train\n        # Compute gradients\n        grad_w = (X_train.T.dot(error)) / m\n        grad_b = np.sum(error) / m\n        # Update parameters\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    \n    # Make predictions on test set\n    z_test = np.dot(X_test, w) + b\n    sigmoid_test = 1 / (1 + np.exp(-z_test))\n    predictions = [int(round(p)) for p in sigmoid_test]\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights `w` are initialized to a zero vector of size equal to the number of features, and the bias `b` is initialized to zero.\n2. **Training Loop**: For each iteration, the linear score `z` is computed as the dot product of the training features and weights plus the bias. The sigmoid function is applied to `z` to get the predicted probabilities.\n3. **Error Calculation**: The error is the difference between the predicted probabilities and the true labels.\n4. **Gradient Calculation**: The gradients for the weights and bias are computed using the error. The gradient for the weights is the transpose of the feature matrix dotted with the error, divided by the number of samples. The gradient for the bias is the sum of the error divided by the number of samples.\n5. **Parameter Update**: The weights and bias are updated using the computed gradients and the learning rate.\n6. **Prediction**: After training, the model is used to predict probabilities for the test set. These probabilities are rounded to the nearest integer to get the class labels.\n\nThis approach ensures that the model is trained efficiently using batch gradient descent and can accurately predict class labels for new data."}
{"task_id": 12, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the power spectrum for each frame of a real-valued signal. The power spectrum is derived from the squared magnitude of the Discrete Fourier Transform (DFT) of each frame. The result can optionally be scaled by the number of DFT bins.\n\n### Approach\n1. **Compute Real FFT**: For each frame, compute the real-valued Fast Fourier Transform (FFT) using `numpy.fft.rfft`. This function efficiently computes the FFT for real-valued signals and returns only the non-redundant components.\n2. **Power Spectrum Calculation**: The power spectrum is obtained by taking the squared magnitude of the FFT results. This is done using `np.abs(fft_result) ** 2`.\n3. **Scaling (Optional)**: If the `scale` parameter is set to `True`, each element of the power spectrum is divided by the number of DFT bins, which is `N//2 + 1` for a frame of length `N`.\n4. **Rounding**: Each value in the power spectrum is rounded to four decimal places.\n5. **Conversion to List**: The resulting NumPy array is converted to a list of lists of floats using the `.tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef power_spectrum(frames: np.ndarray, scale: bool = False) -> list[list[float]]:\n    \"\"\"Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray of shape (M, N)\n        Collection of M frames, each containing N real-valued samples.\n    scale : bool, optional (default=False)\n        If True the resulting power spectrum is divided by ``N//2 + 1``.\n\n    Returns\n    -------\n    list[list[float]]\n        The power spectrum of each frame (only positive frequencies), rounded\n        to four decimal places and returned as a regular Python list.\n    \"\"\"\n    # Compute the real FFT for each frame\n    fft_frames = np.fft.rfft(frames, axis=1)\n    \n    # Calculate the power spectrum\n    power = np.abs(fft_frames) ** 2\n    \n    # Apply scaling if required\n    if scale:\n        n = frames.shape[1]\n        scale_factor = n // 2 + 1\n        power = power / scale_factor\n    \n    # Round to four decimal places\n    power = np.round(power, 4)\n    \n    # Convert to a list of lists and return\n    return power.tolist()\n```\n\n### Explanation\n- **Real FFT Calculation**: The `np.fft.rfft` function is used to compute the FFT for each frame, which returns the non-redundant frequency components.\n- **Power Spectrum**: The squared magnitude of the FFT results is computed to get the power spectrum.\n- **Scaling**: If scaling is enabled, each element of the power spectrum is divided by the number of DFT bins to normalize the values.\n- **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach efficiently computes the power spectrum for each frame, ensuring the results are accurate and formatted as required."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the binary logistic loss function along with its first and second derivatives. The logistic loss is commonly used in binary classification problems and is derived from the cross-entropy loss. The task involves computing the loss, its gradient, and the Hessian diagonal for each sample.\n\n### Approach\n1. **Compute the Sigmoid Function**: The predicted probabilities are obtained using the sigmoid function applied to the predicted logits. This function transforms the logits into probabilities between 0 and 1.\n2. **Clip Probabilities**: To avoid numerical instability when taking the logarithm of very small or very large values, we clip the probabilities to a small value \u03b5 (1e-15) and 1 - \u03b5.\n3. **Calculate Loss**: The loss for each sample is computed using the cross-entropy formula. The mean loss is then obtained by averaging these individual losses.\n4. **Compute Gradient and Hessian**: The gradient of the loss with respect to the logits is given by the difference between the predicted probabilities and the true labels. The Hessian diagonal for each sample is the product of the predicted probability and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n    \"\"\"Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss across all samples\n                               rounded to 4 decimal places.\n            gradient (list[float]): First derivative for every sample,\n                                    each value rounded to 4 decimals.\n            hessian (list[float]): Second derivative (diagonal of the\n                                   Hessian) for every sample, rounded to\n                                   4 decimals.\n    \"\"\"\n    # Compute the sigmoid of y_pred\n    p = 1 / (1 + np.exp(-y_pred))\n    # Clip p to avoid log(0) issues\n    p = np.clip(p, 1e-15, 1 - 1e-15)\n    \n    # Calculate the loss for each sample\n    loss = - (y * np.log(p) + (1 - y) * np.log(1 - p))\n    mean_loss = np.mean(loss)\n    \n    # Compute gradient and Hessian\n    gradient = p - y\n    hessian = p * (1 - p)\n    \n    # Round all values to 4 decimal places\n    mean_loss = np.round(mean_loss, 4)\n    gradient = [np.round(g, 4) for g in gradient]\n    hessian = [np.round(h, 4) for h in hessian]\n    \n    return (mean_loss, gradient, hessian)\n```\n\n### Explanation\n1. **Sigmoid Function**: The sigmoid function transforms the logits into probabilities, ensuring values are between 0 and 1.\n2. **Clipping**: By clipping the probabilities to a small value \u03b5, we avoid taking the logarithm of zero, which would result in undefined values.\n3. **Loss Calculation**: The loss for each sample is computed using the cross-entropy formula, which measures the difference between the predicted probabilities and the true labels.\n4. **Gradient and Hessian**: The gradient is the first derivative of the loss with respect to the logits, and the Hessian diagonal is the second derivative. These are essential for optimization algorithms like gradient descent.\n\nThis approach ensures numerical stability and provides the necessary components for training binary classification models using logistic regression."}
{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate all contiguous N-grams of a given sequence. An N-gram is a sliding window of length N taken from consecutive elements of the sequence. The function should handle various types of sequences, including lists, tuples, and strings, and return the N-grams as a list of tuples.\n\n### Approach\n1. **Check Sequence Length**: First, determine the length of the sequence. If the length of the sequence is less than N, return an empty list since it's impossible to form any N-gram.\n2. **Generate N-grams**: For each possible starting index in the sequence, extract a sub-sequence of length N and convert it into a tuple. Collect all these tuples into a list and return it.\n\nThe approach leverages the ability of sequences to be sliced, which allows us to efficiently generate each N-gram by iterating through valid starting indices.\n\n### Solution Code\n```python\nfrom typing import Any, Iterable, List, Tuple\n\ndef ngrams(sequence: Iterable[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    L = len(sequence)\n    if N > L:\n        return []\n    return [tuple(sequence[i:i+N]) for i in range(L - N + 1)]\n```\n\n### Explanation\n1. **Check Sequence Length**: The function first calculates the length of the sequence. If N is greater than this length, it immediately returns an empty list.\n2. **Generate N-grams**: Using a list comprehension, the function iterates over all valid starting indices (from 0 to `len(sequence) - N`). For each index, it slices the sequence to get a sub-sequence of length N, converts this sub-sequence into a tuple, and collects all such tuples into a list.\n\nThis approach ensures that all possible contiguous N-grams are generated efficiently and correctly, handling various types of input sequences uniformly."}
{"task_id": 17, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a tree from three parallel lists: `items`, `counts`, and `parents`. Each list describes the nodes of a rooted tree, where the i-th entry in each list corresponds to the same node. The goal is to return the tree as a nested dictionary structure.\n\n### Approach\n1. **Identify the Root**: The root of the tree is the node whose parent index is -1. We loop through the `parents` list to find this root node.\n2. **Build the Children Map**: We create a dictionary where each key is a parent node index, and the value is a list of its child node indices. This helps in efficiently constructing the tree structure.\n3. **Recursive Tree Construction**: Using the children map, we recursively build the tree starting from the root. Each node is represented as a dictionary with keys 'item', 'count', and 'children'.\n\n### Solution Code\n```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\"\"\"\n    if not items:\n        return {}\n    \n    # Find the root index\n    root_index = None\n    for i, p in enumerate(parents):\n        if p == -1:\n            root_index = i\n            break\n    \n    # Build the children map\n    children_map = {}\n    for i, p in enumerate(parents):\n        if p != -1:\n            if p not in children_map:\n                children_map[p] = []\n            children_map[p].append(i)\n    \n    # Recursive function to build the tree\n    def build_node(index):\n        node = {\n            'item': items[index],\n            'count': counts[index],\n            'children': []\n        }\n        for child in children_map.get(index, []):\n            node['children'].append(build_node(child))\n        return node\n    \n    return build_node(root_index)\n```\n\n### Explanation\n1. **Empty Input Check**: If the input lists are empty, we immediately return an empty dictionary.\n2. **Root Identification**: We loop through the `parents` list to find the index where the value is -1, which identifies the root node.\n3. **Children Map Construction**: For each node, we map it to its parent in the `children_map` dictionary. This helps in quickly accessing the children of any node during tree construction.\n4. **Tree Construction**: Using a recursive helper function, we build the tree starting from the root. Each node is constructed with its item, count, and a list of its children, which are built recursively.\n\nThis approach ensures that the tree is built efficiently and correctly, with children appearing in the order they first appear in the input lists."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy array filled with zeros, similar to NumPy's `zeros` function. The function should accept a shape parameter and an optional dtype parameter to specify the data type of the resulting array.\n\n### Approach\n1. **Shape Handling**: The shape parameter can be an integer, a tuple, or a list. If it's an integer, the function will create a 1-D array of that length. If it's a tuple or list, it will create a multi-dimensional array with the specified dimensions.\n2. **Data Type Handling**: The function accepts an optional keyword argument `dtype` to specify the data type of the array. If `dtype` is not provided, the default data type will be `float`, which corresponds to `np.float64` in NumPy.\n3. **Return Type**: The function returns a NumPy ndarray filled with zeros, constructed using NumPy's `zeros` function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape.  If an int is provided, a 1-D array of that\n        length is created.  If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs : dict, optional\n        Additional keyword arguments.  Supported argument:\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n    \"\"\"\n    dtype = kwargs.get('dtype', float)\n    return np.zeros(shape, dtype=dtype)\n```\n\n### Explanation\n- **Shape Parameter**: The function handles both integer and iterable (tuple/list) shapes. If an integer is provided, it creates a 1-D array of that length. If a tuple or list is provided, it creates a multi-dimensional array with the specified dimensions.\n- **Data Type Handling**: The function checks for the presence of the `dtype` keyword argument. If provided, it uses the specified data type; otherwise, it defaults to `float`.\n- **Return Value**: The function uses NumPy's `zeros` function to create and return the array, ensuring the result is a NumPy ndarray filled with zeros.\n\nThis approach ensures that the function is lightweight and leverages NumPy's built-in capabilities to handle array creation efficiently."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation function and its gradient for a given input. The function should handle different input types, including scalars, lists, and NumPy arrays, and return the results in the appropriate format.\n\n### Approach\n1. **Sigmoid Function**: The sigmoid function maps any real-valued input to a value between 0 and 1. It is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Gradient Calculation**: The gradient of the sigmoid function is given by \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n3. **Input Handling**: The function should handle different input types:\n   - **Scalars**: If the input is an integer or float, the function returns the sigmoid and gradient as floats.\n   - **Arrays and Lists**: If the input is a list or a NumPy array, the function processes it as an array, computes the sigmoid and gradient element-wise, and returns the results as lists.\n4. **Rounding**: All results are rounded to 4 decimal places. For array-like inputs, the results are converted to Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Scalar case\n        sigmoid = 1.0 / (1.0 + np.exp(-x))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Array-like case\n        if isinstance(x, list):\n            x = np.array(x)\n        sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n        gradient_x = sigmoid_x * (1 - sigmoid_x)\n        sigmoid_rounded = np.round(sigmoid_x, 4)\n        gradient_rounded = np.round(gradient_x, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar (int, float, or 0-dimensional NumPy array), the function computes the sigmoid and gradient directly and returns them as floats rounded to 4 decimal places.\n- **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array if necessary, computes the sigmoid and gradient element-wise, rounds each element to 4 decimal places, and converts the results back to Python lists.\n- **Efficiency**: The use of NumPy ensures efficient computation, especially for large arrays, by leveraging vectorized operations.\n\nThis approach ensures that the function is versatile and handles various input types efficiently while providing the required output format."}
{"task_id": 22, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a random real-valued tensor (NumPy array) of a given shape. Each element of the tensor is generated as the sum of an offset and a uniform random number. The offset is an integer drawn from a discrete uniform distribution, and the uniform random number is drawn from a continuous uniform distribution. Additionally, if specified, each column of the tensor can be standardized to have a mean of 0 and a standard deviation of 1.\n\n### Approach\n1. **Generate Offset**: The offset is an integer drawn from a discrete uniform distribution over the range [-300, 299]. This is achieved using `np.random.randint`.\n2. **Generate Uniform Noise**: The uniform noise is a float drawn from a continuous uniform distribution over the interval [0, 1). This is generated using `np.random.rand`.\n3. **Create Tensor**: The tensor is created by summing the offset and the uniform noise.\n4. **Standardization (Optional)**: If the `standardize` flag is set to `True`, each column of the tensor is standardized. This involves subtracting the mean of each column and dividing by the standard deviation of each column, with a tiny constant added to the denominator to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is produced as an integer drawn uniformly from the inclusive\n    range [\u2212300, 299] plus a real number drawn uniformly from the half-open\n    interval [0, 1).\n\n    Args:\n        shape: Desired shape of the output tensor.\n        standardize: If True, each column is shifted to mean 0 and scaled to\n            unit variance.\n\n    Returns:\n        A NumPy ndarray containing the generated data.\n    \"\"\"\n    offset = np.random.randint(low=-300, high=300, size=shape)\n    u = np.random.rand(*shape)\n    tensor = offset + u\n    if standardize:\n        mean = tensor.mean(axis=0)\n        std = tensor.std(axis=0)\n        epsilon = np.finfo(float).eps\n        tensor = (tensor - mean) / (std + epsilon)\n    return tensor\n```\n\n### Explanation\n1. **Offset Generation**: The offset is generated using `np.random.randint` with `low=-300` and `high=300`, ensuring that the integers are within the range [-300, 299].\n2. **Uniform Noise Generation**: The uniform noise is generated using `np.random.rand`, which produces values in the interval [0, 1).\n3. **Tensor Creation**: The tensor is created by adding the offset and the uniform noise element-wise.\n4. **Standardization**: If `standardize` is `True`, the mean and standard deviation of each column are computed. Each element in a column is then adjusted by subtracting the column's mean and dividing by the column's standard deviation plus a tiny constant (`epsilon`) to avoid division by zero. This ensures that each column has a mean of 0 and a standard deviation of 1, up to numerical precision."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single update step of the Nesterov Accelerated Gradient (NAG) optimization algorithm. This algorithm is an extension of gradient descent that incorporates momentum to accelerate convergence. The key steps involve computing a look-ahead point, evaluating the gradient at this point, updating the velocity, and then updating the parameters.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the current parameter vector `w` and velocity vector `v` into numpy arrays for easier vector operations. If the velocity vector is empty, initialize it as a zero vector with the same shape as `w`.\n2. **Compute Look-Ahead Point**: Calculate the look-ahead point using the current parameters and velocity. This point is used to evaluate the gradient.\n3. **Evaluate and Clip Gradient**: Compute the gradient at the look-ahead point using the provided gradient function. Clip each element of the gradient to ensure values are within the range [-1, 1].\n4. **Update Velocity**: Update the velocity using the momentum coefficient and the clipped gradient.\n5. **Update Parameters**: Update the parameters using the new velocity.\n6. **Convert and Round Results**: Convert the updated parameters and velocity back to Python lists, rounding each element to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\n\n    Parameters\n    ----------\n    w : list | np.ndarray\n        Current parameter vector.\n    velocity : list | np.ndarray\n        Current velocity (momentum term). Supply an empty list for the initial\n        call.\n    grad_func : callable\n        Function that returns the gradient when given a parameter vector.\n    learning_rate : float, default 0.001\n        Step size (\u03b7) for the update.\n    momentum : float, default 0.9\n        Momentum coefficient (\u03bc).\n\n    Returns\n    -------\n    tuple[list, list]\n        The updated parameter vector and the updated velocity, both as Python\n        lists rounded to 4 decimal places.\n    \"\"\"\n    # Convert w to a numpy array\n    w_np = np.array(w)\n    \n    # Handle the case where velocity is empty\n    if len(velocity) == 0:\n        velocity_np = np.zeros_like(w_np)\n    else:\n        velocity_np = np.array(velocity)\n    \n    # Compute the look-ahead point\n    tilde_w = w_np - momentum * velocity_np\n    \n    # Compute the gradient and clip it\n    g = grad_func(tilde_w)\n    g = np.array(g)  # Ensure it's a numpy array\n    g_clipped = np.clip(g, -1, 1)\n    \n    # Update the velocity\n    new_velocity = momentum * velocity_np + learning_rate * g_clipped\n    \n    # Update the parameters\n    new_w = w_np - new_velocity\n    \n    # Round and convert to lists\n    new_w_list = np.round(new_w, 4).tolist()\n    new_velocity_list = np.round(new_velocity, 4).tolist()\n    \n    return (new_w_list, new_velocity_list)\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The function starts by converting the input parameters `w` and `velocity` into numpy arrays. If `velocity` is empty, it is initialized as a zero vector of the same shape as `w`.\n2. **Look-Ahead Point Calculation**: The look-ahead point `tilde_w` is computed by adjusting the current parameters `w` using the current velocity scaled by the momentum coefficient.\n3. **Gradient Evaluation and Clipping**: The gradient at the look-ahead point is evaluated using the provided gradient function. Each element of this gradient is clipped to ensure it lies within the range [-1, 1].\n4. **Velocity Update**: The velocity is updated using the momentum coefficient and the clipped gradient, scaled by the learning rate.\n5. **Parameter Update**: The parameters are updated by subtracting the new velocity from the current parameters.\n6. **Rounding and Conversion**: The updated parameters and velocity are rounded to 4 decimal places and converted back to Python lists before being returned as a tuple.\n\nThis approach ensures that the Nesterov Accelerated Gradient update is efficiently computed, leveraging numpy for vector operations and handling edge cases such as the initial call with an empty velocity vector."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function - RBF) kernel. The decision function for each test sample is computed using the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\n1. **Compute Squared Norms**: Calculate the squared Euclidean norms of the training and test samples. This helps in efficiently computing the squared distances between each training sample and each test sample.\n2. **Dot Products**: Compute the dot products between each training sample and each test sample. This is used to efficiently calculate the squared distances.\n3. **Squared Distances**: Using the squared norms and dot products, compute the squared Euclidean distances between each training sample and each test sample.\n4. **Kernel Matrix**: Apply the Gaussian kernel function to the squared distances to get the kernel matrix, which measures the similarity between samples.\n5. **Decision Function**: Compute the decision function for each test sample using the Lagrange multipliers, class labels, kernel matrix, and bias term.\n6. **Predict Labels**: Determine the predicted class labels based on the sign of the decision function values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    # Compute squared norms of training and test samples\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    \n    # Compute dot products between training and test samples\n    dot_products = X_train.dot(X_test.T)\n    \n    # Calculate squared distances using the expanded form\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * dist_sq)\n    \n    # Multiply alpha with corresponding y_train values\n    alpha_y = alpha * y_train\n    \n    # Compute the decision function for each test sample\n    g = alpha_y.dot(K) + b\n    \n    # Determine the predicted labels based on the sign of g\n    signs = np.where(g >= 0, 1, -1)\n    \n    return signs.tolist()\n```\n\n### Explanation\n1. **Squared Norms**: The squared norms of the training and test samples are computed to facilitate the efficient calculation of squared distances.\n2. **Dot Products**: The dot products between training and test samples are computed to avoid redundant calculations when determining distances.\n3. **Squared Distances**: Using the squared norms and dot products, the squared distances between each training and test sample are computed efficiently.\n4. **Kernel Matrix**: The Gaussian kernel function transforms these squared distances into similarity scores, which are stored in a matrix.\n5. **Decision Function**: The decision function for each test sample is computed by taking the weighted sum of the kernel values (using Lagrange multipliers and class labels) and adding the bias term.\n6. **Predict Labels**: The sign of the decision function values determines the predicted class labels, ensuring each label is either 1 or -1.\n\nThis approach ensures that the solution is both efficient and scalable, leveraging vectorized operations to handle arbitrary numbers of training and test samples."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function and its corresponding gradient. The ReLU function is applied element-wise to an input array, and the gradient is computed based on whether each element is non-negative or not.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is a possibly nested list, which we convert into a NumPy array for efficient element-wise operations.\n2. **Compute ReLU Activation**: Using NumPy's `maximum` function, we apply the ReLU transformation, which sets all negative values to zero.\n3. **Compute Gradient**: Using NumPy's `where` function, we create a gradient mask where each element is 1 if the corresponding input element is non-negative, and 0 otherwise.\n4. **Convert Back to Python Lists**: After computing the activation and gradient, we convert the NumPy arrays back to Python lists to maintain the original structure of the input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n    \"\"\"Compute the element-wise ReLU activation and its gradient.\n\n    Parameters\n    ----------\n    x : list or nested list\n        Input data. Can be a 1-D list of numbers or a nested list representing\n        higher-dimensional data (e.g., a matrix).\n\n    Returns\n    -------\n    tuple of lists\n        A tuple `(activation, gradient)` where both items are Python lists in\n        the same shape as `x`.\n    \"\"\"\n    x_array = np.array(x)\n    activation = np.maximum(0, x_array)\n    gradient = np.where(x_array >= 0, 1, 0)\n    return activation.tolist(), gradient.tolist()\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input list is converted to a NumPy array to leverage efficient element-wise operations.\n- **ReLU Activation**: The `np.maximum(0, x_array)` function is used to compute the ReLU activation, which ensures all negative values are set to zero.\n- **Gradient Calculation**: The gradient is computed using `np.where(x_array >= 0, 1, 0)`, which creates a mask where each element is 1 if the corresponding input element is non-negative, and 0 otherwise.\n- **Conversion Back to Lists**: The results are converted back to Python lists using the `tolist()` method to preserve the original structure of the input.\n\nThis approach efficiently handles both 1-D and nested lists, ensuring the output maintains the same shape as the input."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy tensor filled with 0.0s and 1.0s based on a specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and return the appropriate output.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Setting**: If a seed is provided, set the random seed to ensure reproducible results.\n3. **Random Array Generation**: Generate a random array of the specified shape using a uniform distribution between 0 and 1.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity`. This threshold determines the probability of an element being 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where each element is 1.0 if it is greater than or equal to the threshold, otherwise 0.0. This is done using a boolean mask and casting to float to ensure the correct data type.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns -1 immediately.\n- **Seed Setting**: If a seed is provided, it sets the random seed to ensure that the same sequence of random numbers is generated each time the function is called with the same seed.\n- **Random Array Generation**: Using `np.random.rand`, the function generates a random array of the specified shape with values uniformly distributed between 0 and 1.\n- **Threshold Calculation**: The threshold is calculated as `1 - sparsity`, which determines the cutoff for converting random values to 1.0.\n- **Binary Conversion**: The random values are compared against the threshold. Values greater than or equal to the threshold are converted to 1.0, and others to 0.0. The result is cast to a float array to ensure the correct data type.\n\nThis approach ensures that the function efficiently generates the required binary tensor while adhering to the specified constraints and providing reproducible results when a seed is provided."}
{"task_id": 35, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that traverses a decision tree and returns the prediction for a given sample. The decision tree is represented using a node class, where each node can either be an internal node (splitting on a feature) or a leaf node (containing a prediction).\n\n### Approach\nThe approach to solve this problem involves traversing the decision tree starting from the root node and following the rules specified until we reach a leaf node or encounter a failure condition. Here are the key steps:\n\n1. **Start at the Root Node**: Initialize the traversal from the root node of the decision tree.\n2. **Check for Leaf Node**: If the current node is a leaf node (indicated by `fea = -1`), return the prediction stored in the `res` attribute.\n3. **Feature Index Check**: For internal nodes, check if the feature index specified by the node is within the bounds of the sample. If not, return -1.\n4. **Feature Value Lookup**: Retrieve the feature value from the sample using the current node's feature index. If this value is not present in the node's children, return -1.\n5. **Traverse to Child Node**: Move to the child node corresponding to the feature value and repeat the process.\n\nThis approach ensures that we correctly follow the decision tree structure and handle any failure conditions gracefully by returning -1.\n\n### Solution Code\n```python\nfrom typing import Any\n\nclass node:  # noqa: N801\n    \"\"\"A minimal tree node for decision-tree-like structures.\"\"\"\n    def __init__(self, fea: int = -1, res: Any | None = None, child: dict | None = None) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\n\ndef classify(root: node, sample: list[Any]) -> Any:\n    \"\"\"Return the prediction obtained by traversing a *node* tree.\"\"\"\n    current = root\n    while current.fea != -1:\n        if current.fea >= len(sample):\n            return -1\n        val = sample[current.fea]\n        if val not in current.child:\n            return -1\n        current = current.child[val]\n    return current.res\n```\n\n### Explanation\n- **Initialization**: The function starts by initializing the traversal at the root node.\n- **Loop Through Internal Nodes**: The loop continues as long as the current node is an internal node (i.e., `fea != -1`).\n- **Feature Index Check**: For each internal node, it checks if the feature index is within the bounds of the sample. If not, it returns -1.\n- **Feature Value Lookup**: It retrieves the feature value from the sample and checks if this value exists in the node's children. If not, it returns -1.\n- **Traverse to Child Node**: If the feature value exists, the traversal moves to the corresponding child node.\n- **Return Prediction**: Once a leaf node is reached, the function returns the prediction stored in the `res` attribute of the leaf node.\n\nThis approach ensures that the function efficiently traverses the decision tree and handles all specified failure conditions correctly."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a binary decision tree for each sample in a given dataset and predict the outcome based on the tree's structure. The tree is represented using a `Node` class where each node can either be a leaf node (storing a prediction) or a non-leaf node (storing a feature index and a threshold for splitting).\n\n### Approach\n1. **Check for an Empty Tree**: If the root node is `None`, return an empty list as there are no predictions to make.\n2. **Traverse the Tree for Each Sample**: For each sample in the dataset, start from the root node and traverse down the tree according to the decision rules:\n   - If the current node is a leaf (both left and right children are"}
{"task_id": 38, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the arm-selection phase of the LinUCB (Linear Upper Confidence Bound) algorithm for a contextual linear bandit problem. The goal is to select the arm that maximizes the LinUCB score, which balances both the expected reward and the exploration potential.\n\n### Approach\nThe LinUCB algorithm is designed to balance exploration and exploitation in contextual bandit problems. The key steps to implement the arm selection are as follows:\n\n1. **Context Matrix and Parameters**: The context matrix contains the feature vectors for each arm. The parameters `A` and `b` are lists where each element corresponds to the Gram matrix and the accumulated reward-context vector for each arm, respectively. The exploration coefficient `alpha` determines the balance between exploration and exploitation.\n\n2. **Compute LinUCB Score for Each Arm**: For each arm, compute the LinUCB score using the formula:\n   \\[\n   p_a = \\hat{\\theta}_a^\\top \\mathbf{c}_a + \\alpha \\sqrt{\\mathbf{c}_a^\\top A_a^{-1} \\mathbf{c}_a}\n   \\]\n   where:\n   - \\(\\hat{\\theta}_a = A_a^{-1} \\mathbf{b}_a\\) is the estimated parameter vector for arm \\(a\\).\n   - \\(\\mathbf{c}_a\\) is the context vector for arm \\(a\\).\n   - \\(A_a^{-1}\\) is the inverse of the Gram matrix for arm \\(a\\).\n\n3. **Select the Arm with the Highest Score**: After computing the LinUCB scores for all arms, select the arm with the highest score. In case of a tie, the smallest index is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linucb_select_arm(context: np.ndarray, A: list[list[list[float]]], b: list[list[float]], alpha: float) -> int:\n    \"\"\"Select an arm according to the LinUCB rule.\"\"\"\n    n_arms = context.shape[1]\n    p_list = []\n    for a in range(n_arms):\n        A_a = np.array(A[a])\n        A_a_inv = np.linalg.inv(A_a)\n        b_a = np.array(b[a])\n        theta_hat_a = A_a_inv.dot(b_a)\n        c_a = context[:, a]\n        term1 = theta_hat_a.dot(c_a)\n        term2 = alpha * np.sqrt(c_a.dot(A_a_inv.dot(c_a)))\n        p_a = term1 + term2\n        p_list.append(p_a)\n    selected_arm = np.argmax(p_list)\n    return selected_arm\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of arms from the context matrix. It initializes an empty list to store the LinUCB scores for each arm.\n\n2. **Loop Through Each Arm**: For each arm, the Gram matrix \\(A_a\\) is converted to a numpy array and inverted. The parameter vector \\(\\hat{\\theta}_a\\) is computed by multiplying the inverse of \\(A_a\\) with the accumulated reward vector \\(b_a\\).\n\n3. **Compute Terms**: The first term of the LinUCB score is the dot product of \\(\\hat{\\theta}_a\\) and the context vector \\(\\mathbf{c}_a\\). The second term involves the exploration coefficient \\(\\alpha\\) and the square root of the quadratic form of the context vector with the inverse Gram matrix.\n\n4. **Select the Arm**: After computing the LinUCB scores for all arms, the function uses `np.argmax` to find the index of the arm with the highest score, ensuring the smallest index is chosen in case of ties.\n\nThis approach efficiently balances exploration and exploitation, making it suitable for contextual bandit problems where the goal is to maximize cumulative reward over time."}
{"task_id": 42, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) for given actual and predicted values. RMSLE is a metric used to evaluate regression models, especially when the target values span several orders of magnitude and are non-negative.\n\n### Approach\n1. **Input Validation**: Ensure both input lists are of the same non-zero length and all values are non-negative. If any validation fails, return -1.0.\n2. **Compute Logarithms**: For each value in the actual and predicted lists, compute the natural logarithm of (value + 1).\n3. **Calculate Differences**: Compute the difference between the logarithms of predicted and actual values.\n4. **Square Differences**: Square each of these differences.\n5. **Mean of Squared Differences**: Compute the mean of these squared differences.\n6. **Square Root**: Take the square root of the mean to get the RMSLE.\n7. **Round Result**: Round the result to four decimal places and return it as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n    \"\"\"\n    # Validate input lengths\n    if len(actual) != len(predicted):\n        return -1.0\n    n = len(actual)\n    if n == 0:\n        return -1.0\n    \n    # Check for non-negative values\n    for a in actual:\n        if a < 0:\n            return -1.0\n    for p in predicted:\n        if p < 0:\n            return -1.0\n    \n    # Compute the logarithms\n    log_actual = np.log(np.array(actual) + 1)\n    log_predicted = np.log(np.array(predicted) + 1)\n    \n    # Calculate the differences and squared differences\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    \n    # Compute mean and square root\n    mean_squared = np.mean(squared_diff)\n    rmsle_value = np.sqrt(mean_squared)\n    \n    # Round to four decimal places\n    return round(rmsle_value, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the actual and predicted lists are equal and non-zero. It then ensures all values in both lists are non-negative.\n2. **Logarithm Calculation**: Using NumPy, the function computes the natural logarithm of each value incremented by 1 to handle zero values gracefully.\n3. **Difference and Squaring**: The differences between the logarithms of predicted and actual values are computed and squared.\n4. **Mean and Square Root**: The mean of the squared differences is calculated, and the square root of this mean gives the RMSLE.\n5. **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement.\n\nThis approach ensures that the function handles edge cases and invalid inputs appropriately while efficiently computing the RMSLE using vectorized operations for performance."}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the hyperbolic tangent (tanh) activation function and its derivative. The function should handle both cases where the input is a"}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the one-dimensional autocorrelation of a given signal. The autocorrelation is a measure of the similarity between a signal and its delayed version. This problem requires us to implement a function that computes the autocorrelation coefficients for non-negative lags using a direct double-sum approach.\n\n### Approach\n1. **Problem Analysis**: The autocorrelation for a signal `x` of length `N` is defined for each lag `k` (from 0 to N-1) as the sum of the products of elements of `x` separated by `k` positions. Specifically, for each lag `k`, the autocorrelation coefficient `a_k` is computed as the sum of `x[n] * x[n + k]` for all valid `n`.\n\n2. **Algorithm Selection**: We use a direct double-sum approach to compute the autocorrelation. This involves iterating over each possible lag `k` and, for each lag, summing the products of elements separated by `k` positions. This approach ensures that we handle each element correctly and efficiently, resulting in a time complexity of O(N\u00b2), where N is the length of the signal.\n\n3. **Edge Cases**: The function should handle the case where the input signal is empty by returning an empty list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n    \"\"\"Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Args:\n        x: A one-dimensional sequence of numbers. It can be a Python list or a\n           NumPy array with length *N* (N \u2265 0).\n\n    Returns:\n        A list of length *N* where the *k*-th element is the autocorrelation\n        coefficient a_k.\n    \"\"\"\n    N = len(x)\n    if N == 0:\n        return []\n    result = []\n    for k in range(N):\n        current_sum = 0\n        for n in range(N - k):\n            current_sum += x[n] * x[n + k]\n        result.append(current_sum)\n    return result\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the length `N` of the input signal `x`. If `N` is 0, it immediately returns an empty list.\n\n2. **Iterate Over Lags**: For each lag `k` from 0 to N-1, the function initializes a sum to accumulate the products of elements separated by `k` positions.\n\n3. **Compute Sum for Each Lag**: For each valid index `n` (from 0 to N-k-1), the function computes the product of `x[n]` and `x[n + k]` and adds this product to the current sum.\n\n4. **Store Result**: After computing the sum for the current lag `k`, the result is appended to the output list.\n\n5. **Return Result**: Finally, the function returns the list of autocorrelation coefficients.\n\nThis approach ensures that we correctly compute the autocorrelation for each lag using a straightforward and efficient method, adhering to the problem constraints and requirements."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Newton's method to find the weight vector that minimizes the mean squared error (MSE) for a two-variable linear regression problem. The goal is to iteratively update the weights using the gradient and the Hessian of the MSE until the gradient's norm is below a specified threshold or the maximum number of iterations is reached.\n\n### Approach\n1. **Problem Analysis**: We are given a data matrix `X` and a target vector `y`, and we need to find the weight vector `w` that minimizes the MSE. The MSE is defined as \\( \\text{MSE}(w) = \\frac{1}{n} \\|Xw - y\\|^2 \\).\n\n2. **Newton's Method**: Newton's method uses the gradient and the Hessian of the objective function to update the weights iteratively. The update rule is \\( w \\leftarrow w - H^{-1} \\nabla \\), where \\( \\nabla \\) is the gradient and \\( H \\) is the Hessian.\n\n3. **Gradient and Hessian**: For the MSE, the gradient \\( \\nabla \\) is given by \\( \\frac{2}{n} X^T (Xw - y) \\) and the Hessian \\( H \\) is \\( \\frac{2}{n} X^T X \\).\n\n4. **Handling Singular Hessian**: If the Hessian is singular, we use its Moore-Penrose pseudo-inverse to ensure the algorithm works for rank-deficient matrices.\n\n5. **Iteration**: Starting from an initial guess \\( w = [1, 1]^T \\), we iteratively update `w` until the norm of the gradient is below a specified threshold `epsilon` or the maximum number of iterations `max_iter` is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n    \"\"\"Implement Newton's method for solving a two-variable linear least-squares problem.\"\"\"\n    n = X.shape[0]\n    w = np.array([[1.0], [1.0]])  # Initial guess\n    \n    for _ in range(max_iter):\n        res = X.dot(w) - y\n        grad = (2.0 / n) * X.T.dot(res)\n        H = (2.0 / n) * X.T.dot(X)\n        H_inv = np.linalg.pinv(H)\n        delta_w = H_inv.dot(grad)\n        w = w - delta_w\n        \n        if np.linalg.norm(grad) < epsilon:\n            break\n    \n    rounded_w = np.round(w, 4)\n    return rounded_w.tolist()\n```\n\n### Explanation\n1. **Initialization**: The weight vector `w` is initialized to `[1.0, 1.0]`.\n\n2. **Residual Calculation**: For each iteration, compute the residual `res` as \\( Xw - y \\).\n\n3. **Gradient and Hessian**: Compute the gradient `grad` and the Hessian `H` using the given formulas.\n\n4. **Pseudo-Inverse of Hessian**: Use the Moore-Penrose pseudo-inverse of the Hessian to handle singular cases.\n\n5. **Weight Update**: Update the weight vector using the computed gradient and the pseudo-inverse of the Hessian.\n\n6. **Convergence Check**: If the norm of the gradient is below the threshold `epsilon`, the loop breaks early.\n\n7. **Result Formatting**: The final weight vector is rounded to four decimal places and returned as a list of lists.\n\nThis approach ensures that we efficiently find the optimal weights using Newton's method, handling both invertible and singular Hessians appropriately."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by recursively computing the probabilities of observing the remaining sequence from each state at each time step.\n\n### Approach\n1. **Check for Empty Inputs**: If any of the input lists (transition matrix `A`, emission matrix `B`, initial distribution `pi`, or observation sequence `obs`) are empty, return 0.0 immediately as no valid probability can be computed.\n\n2. **Initialize Parameters**: Determine the number of states `N` from the length of the initial distribution `pi` and the length of the observation sequence `T` from the observation list `obs`.\n\n3. **Backward Algorithm Initialization**: Initialize the backward variables `beta` for the last time step `T-1` to 1.0 for all states, as there are no future observations beyond this point.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to 0, compute the backward variables `beta` for each state by summing over all possible next states, weighted by the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Compute Final Probability**: After computing the backward variables for all time steps, calculate the final probability by summing the contributions from each state, weighted by the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\n6. **Return Result**: Return the computed probability rounded to six decimal places.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    N = len(pi)\n    T = len(obs)\n    \n    if T == 0:\n        return 0.0\n    \n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n    \n    # Iterate from t = T-2 down to 0\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0] * N\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                # A[i][j] is the transition from i to j\n                # B[j][obs[t+1]] is the emission probability for the next observation\n                sum_val += A[i][j] * B[j][obs[t+1]] * beta[j]\n            new_beta[i] = sum_val\n        beta = new_beta\n    \n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    \n    # Round to six decimal places\n    return round(prob, 6)\n```\n\n### Explanation\n- **Initialization**: The backward variables `beta` are initialized to 1.0 for the last time step because there are no future observations to consider.\n- **Recursive Calculation**: For each time step, the backward variables are updated by considering all possible transitions from the current state to the next state, weighted by the transition probabilities and emission probabilities.\n- **Final Probability**: The final probability is computed by summing the contributions from each state, considering the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\nThis approach efficiently computes the probability of the observation sequence using the backward algorithm, ensuring that the result is accurate and rounded to six decimal places."}
{"task_id": 68, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Stochastic Gradient Descent (SGD) with momentum for a single optimization step. This method helps in accelerating the convergence of the optimization process by incorporating a \"momentum\" term, which takes into account the past gradients to smooth out the update direction.\n\n### Approach\n1. **Problem Analysis**: The task is to update the parameters of a model using the SGD with momentum update rule. The update rule involves computing a velocity (or momentum) term that combines the previous velocity and the current gradient. This velocity is then used to update the parameters.\n2. **Intuition**: Momentum helps in accelerating SGD in the relevant direction and dampening oscillations. It does this by adding a fraction of the previous update to the current update, which helps in moving more confidently in the direction of the minima.\n3. **Algorithm**:\n   - If the previous update (velocity) is `None`, initialize it as a zero tensor of the same shape as the parameters.\n   - Compute the new velocity using the formula: \\( u_t = \\text{momentum} \\times u_{t-1} + (1 - \\text{momentum}) \\times \\nabla_w \\).\n   - Update the parameters using the new velocity: \\( w_{t+1} = w_t - \\text{learning\\_rate} \\times u_t \\).\n   - Round the updated parameters and the new velocity to 4 decimal places and convert them to Python lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sgd_momentum_update(w: np.ndarray, grad: np.ndarray, learning_rate: float = 0.01, momentum: float = 0.0, prev_update: np.ndarray | None = None) -> tuple[list, list]:\n    \"\"\"Performs one SGD optimisation step with momentum.\"\"\"\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    new_update = momentum * prev_update + (1 - momentum) * grad\n    w_new = w - learning_rate * new_update\n    rounded_w = np.round(w_new, 4).tolist()\n    rounded_update = np.round(new_update, 4).tolist()\n    return (rounded_w, rounded_update)\n```\n\n### Explanation\n- **Initialization**: If the previous update (`prev_update`) is `None`, it is initialized to a zero tensor of the same shape as the parameters `w`.\n- **Velocity Update**: The new velocity (`new_update`) is computed by combining the previous velocity and the current gradient, scaled by the momentum factor.\n- **Parameter Update**: The parameters are updated by subtracting the learning rate multiplied by the new velocity from the current parameters.\n- **Rounding and Conversion**: The updated parameters and the new velocity are rounded to 4 decimal places and converted to Python lists to meet the output requirements.\n\nThis approach ensures that the optimization step is efficient and incorporates the benefits of momentum to improve convergence."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward algorithm for Hidden Markov Models (HMMs) to compute the likelihood of an observation sequence given the model parameters. The model is specified by the initial state probabilities, state transition matrix, and emission matrix. The goal is to validate the input and compute the sequence likelihood efficiently.\n\n### Approach\n1. **Input Validation**: Ensure that the input dimensions are correct and that all observations are within the valid range. If any input is invalid, return -1.\n2. **Forward Algorithm**: Use dynamic programming to compute the probability of each observation sequence step-by-step. This involves initializing the forward probabilities for the first observation and iteratively updating them for each subsequent observation.\n3. **Result Calculation**: Sum the final forward probabilities to get the total likelihood of the observation sequence and return it rounded to four decimal places.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check for empty inputs\n    if not S or not A or not B or not observations:\n        return -1\n    \n    n = len(S)\n    # Check if A is n x n\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check if B is n x m\n    if len(B) != n:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check observations are within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Initialize forward variables\n    T = len(observations)\n    f = [S[i] * B[i][observations[0]] for i in range(n)]\n    \n    # Iterate through the rest of the observations\n    for t in range(1, T):\n        next_f = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += f[j] * A[j][i]\n            next_f[i] = sum_val * B[i][observations[t]]\n        f = next_f\n    \n    # Sum the final probabilities\n    result = sum(f)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if any of the input lists are empty. It then verifies that the dimensions of the state transition matrix `A` and emission matrix `B` match the initial state probabilities `S`. Each observation is checked to ensure it falls within the valid range of emission symbols.\n2. **Forward Initialization**: The forward probabilities are initialized using the initial state probabilities and the emission probabilities for the first observation.\n3. **Forward Iteration**: For each subsequent observation, the forward probabilities are updated by considering all possible transitions from the previous states. This is done using nested loops to compute the sum of probabilities from all previous states multiplied by the transition probabilities.\n4. **Result Calculation**: The final result is the sum of the forward probabilities after processing all observations, rounded to four decimal places.\n\nThis approach ensures that the function efficiently computes the likelihood of the observation sequence while handling invalid inputs appropriately."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to count the occurrences of each class label in a given tabular dataset. Each sample in the dataset is represented as a list, where the last element is the class label. The goal is to return a dictionary where the keys are the unique class labels and the values are their respective counts.\n\n### Approach\n1. **Check for Empty Data**: If the input list is empty, return an empty dictionary immediately.\n2. **Initialize a Dictionary**: Create an empty dictionary to keep track of the counts of each class label.\n3. **Iterate Through Samples**: For each sample in the dataset, extract the class label (the last element of the sample list).\n4. **Update Counts**: Use the dictionary's `get` method to update the count for each class label. If the label is already a key in the dictionary, increment its count by 1. If it is not, add the label as a new key with an initial count of 1.\n5. **Return the Result**: After processing all samples, return the dictionary containing the counts of each class label.\n\nThis approach ensures that we efficiently count the occurrences of each class label with a single pass through the dataset, making the solution both time and space efficient.\n\n### Solution Code\n```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count the occurrence of each label in a tabular data set.\n\n    Each sample in *data* is a list whose last element represents the\n    class label.  The function must return a dictionary mapping every\n    unique label to the number of times it appears in the data set.\n\n    Args:\n        data: List of samples (list of lists).  The last element of each\n              inner list is treated as the label.\n\n    Returns:\n        A dictionary where keys are unique labels and values are their\n        integer counts.  If *data* is empty an empty dictionary is\n        returned.\n    \"\"\"\n    label_counts = {}\n    for sample in data:\n        label = sample[-1]\n        label_counts[label] = label_counts.get(label, 0) + 1\n    return label_counts\n```\n\n### Explanation\n- **Empty Data Check**: The function first checks if the input list is empty and returns an empty dictionary if true.\n- **Dictionary Initialization**: An empty dictionary `label_counts` is initialized to store the counts of each class label.\n- **Sample Iteration**: The function iterates over each sample in the dataset. For each sample, it extracts the class label using `sample[-1]`.\n- **Count Update**: Using the `get` method of dictionaries, the function updates the count for each class label. This method efficiently handles both existing and new labels.\n- **Return Result**: After processing all samples, the function returns the dictionary with the counts of each class label.\n\nThis solution efficiently handles the task with a time complexity of O(n), where n is the number of samples in the dataset, ensuring optimal performance even for large datasets."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the variance reduction achieved by splitting a node into left and right children in a regression tree. The variance reduction is a measure of how much the split reduces the variance of the target values.\n\n###"}
{"task_id": 74, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the magnitude spectrum for a batch of real-valued frames. The magnitude spectrum is derived from the Discrete Fourier Transform (DFT) and only considers the non-redundant positive-frequency part of the spectrum. This approach efficiently handles both even and odd frame lengths.\n\n### Approach\n1. **Input Handling**: The input can be a 1D or 2D NumPy array. If it's a 1D array, it is treated as a single frame. If it's a 2D array, each row represents a frame.\n2. **Real FFT Computation**: Using NumPy's `rfft` function, we compute the real FFT for each frame. This function efficiently computes the FFT for real-valued inputs and returns only the non-redundant positive-frequency coefficients.\n3. **Magnitude Calculation**: The magnitude of the FFT coefficients is computed using the absolute value.\n4. **Rounding**: Each magnitude value is rounded to four decimal places.\n5. **Conversion to List**: The resulting NumPy array is converted to a list of lists to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef magnitude_spectrum(frames: \"np.ndarray\") -> \"list[list[float]]\":\n    \"\"\"Compute the positive-frequency magnitude spectrum for each frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        Either a 2-D array of shape (M, N) where each row is a frame, or a\n        1-D array treated as a single frame of length N.\n\n    Returns\n    -------\n    list[list[float]]\n        The magnitude spectrum of every frame, rounded to four decimals.\n    \"\"\"\n    # Ensure the input is treated as a 2D array\n    if frames.ndim == 1:\n        frames = frames.reshape(1, -1)\n    \n    # Compute the real FFT along the last axis\n    fft = np.fft.rfft(frames, axis=1)\n    \n    # Calculate the magnitude\n    magnitude = np.abs(fft)\n    \n    # Round to four decimal places\n    rounded_magnitude = np.round(magnitude, 4)\n    \n    # Convert to a list of lists\n    return rounded_magnitude.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a 1D array and reshapes it to a 2D array if necessary. This ensures consistent processing for both 1D and 2D inputs.\n- **Real FFT Computation**: Using `np.fft.rfft`, the function computes the FFT for each frame along the last axis. This function is optimized for real-valued inputs and returns only the necessary positive-frequency coefficients.\n- **Magnitude Calculation**: The magnitude of each FFT coefficient is computed using `np.abs`, which gives the absolute value of the complex numbers.\n- **Rounding**: The magnitudes are rounded to four decimal places using `np.round` to meet the precision requirement.\n- **Conversion to List**: The resulting NumPy array is converted to a list of lists using `tolist()`, ensuring the output format is as specified.\n\nThis approach efficiently handles both even and odd frame lengths and ensures the output is in the required format."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the unnormalized categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in machine learning, especially in classification tasks where the output is a probability distribution over classes.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Inputs to NumPy Arrays**: Ensure that both the true labels `y` and the predicted probabilities `y_pred` are converted to NumPy arrays for efficient element-wise operations.\n\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to each element of `y_pred`. This ensures that the logarithm operation is always applied to a positive number.\n\n3. **Compute Logarithms**: Calculate the natural logarithm of each element in the modified `y_pred` array.\n\n4. **Element-wise Multiplication**: Multiply each element of `y` with the corresponding element in the logarithm array. This step effectively zeros out all terms where the true label is not the correct class, as `y` is one-hot encoded.\n\n5. **Sum and Negate**: Sum all the resulting values and negate the sum to get the final loss value. This is because the cross-entropy loss is defined as the negative sum of the log probabilities.\n\n6. **Round the Result**: Finally, round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can leverage NumPy's efficient array operations, which are crucial for handling potentially large datasets efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in negative infinity and cause numerical instability in the loss calculation.\n- **Logarithm Calculation**: The natural logarithm of each predicted probability is computed to transform the multiplicative probabilities into additive terms, which is a standard step in cross-entropy calculations.\n- **Element-wise Multiplication**: This step ensures that only the log probability of the correct class contributes to the loss for each sample, as all other terms are multiplied by zero.\n- **Sum and Negate**: The sum of these contributions is taken and negated to align with the definition of cross-entropy loss, which measures the difference between the predicted and true distributions.\n- **Rounding**: The final loss value is rounded to four decimal places to provide a precise and readable result.\n\nThis approach efficiently computes the categorical cross-entropy loss while ensuring numerical stability and correctness."}
{"task_id": 78, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs one step of the Adamax optimization algorithm. Adamax is a variant of the Adam optimizer that uses an exponentially weighted infinity-norm instead of the squared gradients. The function will update the parameters, first-moment estimates, and the infinity-norm estimates based on the given gradients and hyperparameters.\n\n### Approach\n1. **Compute the Beta1 Power**: Calculate the power of beta1 raised to the current time step `t`.\n2. **Scaling Factor**: Compute the scaling factor for the learning rate, which adjusts for the bias correction in the first-moment estimate.\n3. **Update First-Moment Estimates**: For each parameter, update the first-moment estimate using the given beta1 and gradients.\n4. **Update Infinity-Norm Estimates**: For each parameter, update the infinity-norm estimate using the given beta2 and the absolute value of the gradients.\n5. **Compute the Step Size**: Calculate the step size for each parameter using the updated first-moment and infinity-norm estimates.\n6. **Update Parameters**: Adjust each parameter by subtracting the computed step size.\n7. **Increment Time Step**: Increase the time step counter by 1.\n8. **Round Values**: Round each element of the updated parameters, first-moment estimates, and infinity-norm estimates to 6 decimal places.\n\n### Solution Code\n```python\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step.\"\"\"\n    beta1_power_t = beta1 ** t\n    scaling = learning_rate / (1 - beta1_power_t)\n    \n    new_params = []\n    new_m = []\n    new_u = []\n    \n    for i in range(len(params)):\n        # Update first-moment estimate\n        new_m_i = beta1 * m[i] + (1 - beta1) * grads[i]\n        # Update infinity-norm estimate\n        new_u_i = max(beta2 * u[i], abs(grads[i]))\n        # Compute the step\n        step_i = scaling * new_m_i / (new_u_i + epsilon)\n        # Update parameters\n        new_params_i = params[i] - step_i\n        \n        new_params.append(new_params_i)\n        new_m.append(new_m_i)\n        new_u.append(new_u_i)\n    \n    # Round each element to 6 decimal places\n    new_params = [round(x, 6) for x in new_params]\n    new_m = [round(x, 6) for x in new_m]\n    new_u = [round(x, 6) for x in new_u]\n    \n    new_t = t + 1\n    \n    return (new_params, new_m, new_u, new_t)\n```\n\n### Explanation\n- **Beta1 Power Calculation**: This step adjusts the learning rate to account for the bias in the first-moment estimate.\n- **Scaling Factor**: This factor ensures that the learning rate is appropriately scaled based on the time step.\n- **First-Moment Update**: This step computes the exponentially weighted average of the gradients.\n- **Infinity-Norm Update**: This step computes the maximum of the exponentially weighted infinity-norm and the absolute gradient.\n- **Step Size Calculation**: The step size is computed using the updated estimates and the learning rate.\n- **Parameter Update**: Each parameter is updated by subtracting the computed step size.\n- **Rounding**: Each value is rounded to 6 decimal places to meet the precision requirement.\n\nThis approach ensures that the parameters are updated correctly according to the Adamax optimization algorithm, providing an efficient and precise update step."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single update step of the Adadelta optimization algorithm. Adadelta is an adaptive learning rate method that adjusts the learning rate for each parameter based on the history of gradients and parameter updates. This method helps in avoiding the need for manually setting the learning rate, which can be a tedious task.\n\n### Approach\nThe Adadelta algorithm maintains two running averages for each parameter:\n1. `E_grad` (running average of the squared gradients)\n2. `E_delta` (running average of the squared parameter updates)\n\nFor the first update step, both `E_grad` and `E_delta` are initialized to zero. The update rules for each parameter are as follows:\n1. Compute the running average of the squared gradients.\n2. Compute the root mean square (RMS) of the gradients.\n3. Compute the RMS of the parameter updates, which starts as the square root of a small constant `\u03b5` since `E_delta` is zero initially.\n4. Calculate the adaptive learning rate as the ratio of the RMS of the parameter updates to the RMS of the gradients.\n5. Compute the parameter update using the adaptive learning rate.\n6. Update the parameter using the computed update.\n\nThe steps are implemented element-wise for any input shape, ensuring that the function works efficiently for both 1-D and n-D inputs.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform one Adadelta optimisation step.\"\"\"\n    w_np = np.array(w)\n    grad_np = np.array(grad_w)\n    \n    e_grad = (1 - rho) * (grad_np ** 2)\n    rms_grad = np.sqrt(e_grad + eps)\n    rms_delta = np.sqrt(eps)\n    adaptive_lr = rms_delta / rms_grad\n    delta_w = adaptive_lr * grad_np\n    \n    new_w = w_np - delta_w\n"}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost, also known as log-loss, between the predicted probabilities and the ground-truth binary labels. This cost function is commonly used in binary classification problems to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clip Values**: To avoid taking the logarithm of zero or one, which is undefined, we clip the predicted probabilities to a small value \u03b5 (1e-15) and 1 - \u03b5. This ensures that all values are within the valid range for logarithmic operations.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements.\n3. **Element-wise Multiplication**: Multiply the ground-truth labels with the logarithms of the predicted probabilities and the complements of the ground-truth labels with the logarithms of the complements of the predicted probabilities.\n4. **Sum Terms**: Sum the results of the element-wise multiplications for each sample.\n5. **Average and Negate**: Compute the average of the summed terms and negate the result to get the final cost.\n6. **Round Result**: Return the cost rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = (-1 / m) * np.sum(sum_terms)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The predicted probabilities are clipped to ensure they are within the range [\u03b5, 1 - \u03b5], avoiding undefined logarithmic operations.\n- **Logarithmic Operations**: The natural logarithm of the clipped values and their complements are computed to handle the log terms in the binary cross-entropy formula.\n- **Element-wise Multiplication and Summation**: The terms involving the ground-truth labels and their complements are computed and summed element-wise.\n- **Averaging and Negation**: The summed terms are averaged and negated to compute the final cost, which is then rounded to six decimal places for precision.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be too close to 0 or 1."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the optimal step size (gamma) for adding a new base learner in a gradient boosting model using the mean-squared-error (MSE) objective. The step size is determined by minimizing the MSE, and it is derived using a closed-form solution.\n\n### Approach\n1. **Compute Residuals**: Calculate the residuals, which are the differences between the true target values and the current predictions.\n2. **Calculate Numerator and Denominator**: The numerator is the sum of the product of residuals and the new learner's predictions. The denominator is the sum of the squares of the new learner's predictions.\n3. **Check for Zero Denominator**: If the denominator is zero, it implies that the new learner's predictions are all zero, meaning the residuals are already perfectly fitted. In this case, the step size is set to 1.0 by convention.\n"}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate individual predictions from multiple decision trees in a Random Forest classifier using a majority vote mechanism. The goal is to determine the final prediction for each sample by considering the most frequent prediction across all trees. In case of a tie, the smallest label (numerically or lexicographically) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions into a structure where each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions independently.\n2. **Count Votes**: For each sample, count the occurrences of each prediction using a counter.\n3. **Determine Majority Vote**: Identify the prediction(s) with the highest count. If there is a tie, select the smallest label based on numerical or lexicographical order.\n4. **Construct Result**: Collect the final predictions for all samples and return them as a list.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample_predictions = [tree[i] for tree in predictions]\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        max_labels = [label for label, cnt in counts.items() if cnt == max_count]\n        max_labels.sort()\n        result.append(max_labels[0])\n    return result\n```\n\n### Explanation\n1. **Check for Empty Input**: If the input list is empty, return an empty list immediately.\n2. **Determine Number of Samples**: The number of samples is derived from the length of the first inner list, assuming all inner lists are of the same length.\n3. **Process Each Sample**: For each sample, gather all predictions from each tree, count the occurrences of each prediction, and determine the most frequent prediction(s).\n4. **Handle Ties**: If multiple predictions have the same highest count, sort them and select the smallest one.\n5. **Return Result**: Collect the final predictions for all samples and return them as a list.\n\nThis approach ensures that each sample's prediction is determined efficiently and correctly, adhering to the majority vote mechanism with tie-breaking by selecting the smallest label."}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the sizes of the input, hidden, and output layers in a feed-forward neural network. The input and output layer sizes are derived from the dimensions of the provided input and output matrices, while the hidden layer size is fixed at 10.\n\n### Approach\n1. **Extract Input Layer Size**: The number of input features, which is the size of the input layer, is determined by the number of rows in the input matrix `X`.\n2. **Fixed Hidden Layer Size**: The hidden layer size is fixed at 10 as per the problem statement.\n3. **Extract Output Layer Size**: The number of output units, which is the size of the output layer, is determined by the number of rows in the output matrix `Y`.\n4. **Return the Tuple**: Combine these three values into a tuple and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training input.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]\n    n_h = 10\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)\n```\n\n### Explanation\n- **Input Layer Size (`n_x`)**: This is obtained by taking the number of rows in the input matrix `X`, which is `X.shape[0]`.\n- **Hidden Layer Size (`n_h`)**: This is fixed at 10 as specified in the problem.\n- **Output Layer Size (`n_y`)**: This is obtained by taking the number of rows in the output matrix `Y`, which is `Y.shape[0]`.\n\nThe function efficiently computes these values and returns them as a tuple, providing the necessary layer sizes for initializing the neural network parameters."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function in a numerically stable manner. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation using \\( \\exp \\) can lead to overflow for large positive values and underflow for very small negative values. To avoid these issues, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept scalar values (int or float), lists, or NumPy arrays. The output should be a float for scalar inputs and a Python list for array-like inputs.\n2. **Numerical Stability**: Use `np.logaddexp(0.0, z)` to compute the softplus function. This avoids overflow and underflow issues by leveraging the numerically stable implementation provided by NumPy.\n3. **Rounding**: Each computed value is rounded to 4 decimal places.\n4. **Output Conversion**: For array-like inputs, the result is converted to a pure Python list using `tolist()`. For scalar inputs, the result is returned as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Determine if z is a scalar\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        s = np.logaddexp(0.0, z)\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        # Treat as array-like\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\n```\n\n### Explanation\n1. **Input Check**: The function first checks if the input `z` is a scalar (int, float, or 0-dimensional array). If so, it computes the softplus value directly and returns it as a float after rounding.\n2. **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array, computes the softplus values using `logaddexp`, rounds each value to 4 decimal places, and converts the result to a Python list.\n3. **Numerical Stability**: By using `np.logaddexp(0.0, z)`, the function ensures that the computation is numerically stable, avoiding overflow for large positive values and underflow for very small negative values.\n\nThis approach ensures that the softplus function is computed efficiently and accurately for various input types, providing the correct output format as specified."}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the gradient of the loss with respect to the pre-activation values (z) during the backpropagation of a Rectified Linear Unit (ReLU) activation function. The ReLU function is defined as ReLU(z) = max(0, z), and its derivative is 1 when z is positive and 0 otherwise. \n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists `dA` (upstream gradient) and `activation_cache` (pre-activation values Z) into NumPy arrays for efficient vectorized operations.\n2. **Check Shapes**: Ensure that the shapes of `dA` and `activation_cache` are the same. If they are not, return -1.\n3. **Compute Gradient**: Calculate the gradient `dZ` using the rule `dZ = dA * (Z > 0)`, where `(Z > 0)` is an element-wise indicator function that returns 1 for positive values and 0 otherwise.\n4. **Convert Back to List**: Convert the resulting NumPy array `dZ` back to a list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_backward(dA: list[list[int | float]], activation_cache: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer in the network.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z) from the forward pass.\n\n    Returns\n    -------\n    list[list[int | float]]\n        Gradient with respect to Z, having the same shape as *dA*.\n        Returns -1 if *dA* and *activation_cache* do not share the same shape.\n    \"\"\"\n    # Convert the input lists to NumPy arrays\n    dA_np = np.array(dA)\n    Z_np = np.array(activation_cache)\n    \n    # Check if the shapes are compatible\n    if dA_np.shape != Z_np.shape:\n        return -1\n    \n    # Compute the gradient dZ\n    dZ = dA_np * (Z_np > 0)\n    \n    # Convert the result back to a list of lists\n    return dZ.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The function starts by converting the input lists `dA` and `activation_cache` into NumPy arrays. This allows for efficient element-wise operations.\n2. **Shape Check**: The function checks if the shapes of `dA` and `activation_cache` are the same. If they are not, it returns -1 to indicate an error.\n3. **Gradient Calculation**: Using the element-wise multiplication, the function computes the gradient `dZ` by multiplying `dA` with the indicator function `(Z_np > 0)`. This indicator function ensures that only the elements where `Z` is positive contribute to the gradient.\n4. **Conversion Back to List**: Finally, the resulting NumPy array `dZ` is converted back to a list of lists using the `tolist()` method and returned.\n\nThis approach efficiently handles the computation using vectorized operations provided by NumPy, ensuring that the solution is both concise and performant."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a given input value is a numeric type, excluding boolean values. Numeric types include integers, floats, complex numbers, fractions, decimals, and other instances of `numbers.Number`. The challenge is to ensure that boolean values, which are technically a subclass of integers, are excluded from being considered numeric in this context.\n\n### Approach\nThe approach to solve this problem involves two main checks:\n1. Check if the input value is an instance of `numbers.Number`. This includes all standard numeric types such as integers, floats, complex numbers, fractions, and decimals.\n2. Ensure that the input value is not a boolean. This is necessary because, in Python, boolean values (`True` and `False`) are instances of `int`, which is a subclass of `numbers.Number`.\n\nBy combining these two checks, we can accurately determine if the input value is a numeric type while excluding boolean values.\n\n### Solution Code\n```python\nimport numbers\n\ndef is_number(a) -> bool:\n    \"\"\"Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n    \"\"\"\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```\n\n### Explanation\n- The function `is_number` takes a single argument `a`.\n- It uses `isinstance(a, numbers.Number)` to check if `a` is an instance of any numeric type.\n- It then uses `not isinstance(a, bool)` to ensure that `a` is not a boolean value.\n- The function returns `True` if both conditions are satisfied, indicating that `a` is a numeric type and not a boolean. Otherwise, it returns `False`.\n\nThis approach efficiently combines type checking to meet the problem requirements, ensuring that all numeric types are included while explicitly excluding boolean values."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the ReLU (Rectified Linear Unit) activation function for neural networks. The ReLU function takes an input and returns the maximum of 0 and the input value. This function must handle various input types, including scalars, 1-D arrays, 2-D arrays, and higher-dimensional arrays, and return the results as Python lists for JSON serializability.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input can be a scalar, a list, or a nested list. We first convert this input into a NumPy array to leverage vectorized operations.\n2. **Handle Scalar Input**: If the input is a scalar (0-dimensional array), we reshape it into a 1-dimensional array to ensure consistent handling.\n3. **Compute ReLU Activation**: Using NumPy's `maximum` function, we compute the ReLU activation element-wise for the input array.\n4. **Convert to Python Lists**: Convert the resulting NumPy arrays back to Python lists using the `tolist()` method to ensure the output is JSON serializable.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z* and return\n    both the activation and the original input.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple `(A, cache)` where `A` is the ReLU of `Z`\n            and `cache` is `Z` itself, both converted to Python lists.\n    \"\"\"\n    Z_np = np.array(Z)\n    if Z_np.ndim == 0:\n        Z_np = Z_np.reshape(1)\n    A_np = np.maximum(0, Z_np)\n    A = A_np.tolist()\n    cache = Z_np.tolist()\n    return (A, cache)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input `Z` is converted to a NumPy array to"}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the majority class label among the k-nearest neighbours using a majority vote mechanism. If there is a tie in the frequency of the labels, we return the smallest label according to Python's default ordering.\n\n### Approach\n1. **Count Frequencies**: Use the `Counter` class from the `collections` module to count the frequency of each label in the list of neighbour targets.\n2. **Identify Maximum Frequency**: Determine the highest frequency of occurrence among the labels.\n3. **Collect Labels with Maximum Frequency**: Gather all labels that have this maximum frequency.\n4. **Select the Smallest Label**: From the collected labels, return the smallest one based on Python's default ordering (numerical for integers and lexicographic for strings).\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef knn_majority_vote(neighbors_targets: list[str | int]) -> str | int:\n    \"\"\"Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A list containing the class labels of the k nearest\n            neighbours.\n\n    Returns:\n        The label that appears most frequently. In case of a tie, the smallest\n        label according to Python\u2019s default ordering is returned.\n    \"\"\"\n    counts = Counter(neighbors_targets)\n    max_count = max(counts.values())\n    max_labels = [label for label, count in counts.items() if count == max_count]\n    return min(max_labels)\n```\n\n### Explanation\n1. **Counting Frequencies**: The `Counter` class efficiently counts the occurrences of each label in the input list.\n2. **Finding Maximum Frequency**: Using the `max` function on the values of the counter gives the highest frequency.\n3. **Filtering Labels**: We filter out the labels that have the maximum frequency.\n4. **Selecting the Smallest Label**: The `min` function is used to find the smallest label among those with the highest frequency, ensuring the correct tie-breaking as per the problem requirements.\n\nThis approach ensures that we efficiently and correctly determine the majority class label, handling ties by returning the smallest label as specified."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the ReLU activation function for a given input. The input can be a scalar, a 1-D list, a nested list, or a NumPy ndarray. The output should have the same shape as the input, with 1.0 where the input is greater than 0, and 0.0 otherwise.\n\n### Approach\nThe approach to solve this problem involves handling different types of inputs appropriately:\n\n1. **Scalar Input**: If the input is a scalar (int or float), return 1.0 if the value is greater than 0, otherwise return 0.0.\n2. **NumPy Array Input**: If the input is a NumPy array, compute the derivative using vectorized operations. Convert the result to a list using `tolist()` to maintain the same structure as the input.\n3. **List Input**: If the input is a list, process each element recursively. This handles nested lists and ensures each element is processed correctly, whether it is a scalar or another list.\n\nThe function uses recursion to handle nested lists and leverages NumPy's vectorized operations for efficient computation on arrays.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input data that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        Derivative of ReLU with the same shape layout as *Z* (float if *Z* is a scalar).\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return 1.0 if Z > 0 else 0.0\n    elif isinstance(Z, np.ndarray):\n        return (Z > 0).astype(float).tolist()\n    elif isinstance(Z, list):\n        return [drelu(element) for element in Z]\n    else:\n        raise ValueError(\"Unsupported input type\")\n```\n\n### Explanation\n- **Scalar Handling**: The function first checks if the input is a scalar. If it is, it directly computes the derivative based on whether the value is greater than 0.\n- **NumPy Array Handling**: For NumPy arrays, the function uses vectorized operations to compute the derivative efficiently. The result is then converted to a list to match the expected output format.\n- **List Handling**: For lists, the function processes each element recursively. This ensures that nested lists are handled correctly, and each element is processed to compute the derivative.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 99, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function, which is a fundamental component in neural networks. The ReLU function is defined as ReLU(x) = max(0, x), meaning it returns the input if it is positive, otherwise, it returns zero. The challenge is to handle different input types (scalar, list, NumPy array) and return the transformed data in the same container type.\n\n### Approach\n1. **Identify Input Type**: The function first checks the type of the input to determine how to process it.\n2. **Scalar Handling**: If the input is a scalar (int or float), the function directly applies the ReLU transformation.\n3. **List Handling**: If the input is a list (either 1D or 2D), it is converted to a NumPy array for efficient element-wise operations. After applying ReLU, the result is converted back to a list to maintain the original container type.\n4. **NumPy Array Handling**: If the input is already a NumPy array, the function applies the ReLU transformation directly using NumPy's vectorized operations for efficiency.\n5. **Type Conversion**: The function ensures that the output type matches the input type, whether it's a scalar, list, or NumPy array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The function keeps the container type of the input:\n    scalar \u2192 scalar, list \u2192 list, NumPy array \u2192 NumPy array.\n\n    Args:\n        Z (int | float | list | numpy.ndarray): Input data \u2013 scalar, 1-D or 2-D.\n\n    Returns:\n        Same type as *Z* with all negative values clipped to 0.\n    \"\"\"\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n    elif isinstance(Z, list):\n        arr = np.array(Z)\n        relu_arr = np.maximum(0, arr)\n        return relu_arr.tolist()\n    elif isinstance(Z, np.ndarray):\n        return np.maximum(0, Z)\n    else:\n        raise TypeError(\"Unsupported input type\")\n```\n\n### Explanation\n- **Scalar Input**: For a scalar input, the function simply returns the maximum of 0 and the input value.\n- **List Input**: The function converts the list to a NumPy array to leverage vectorized operations. After applying the ReLU transformation, it converts the result back to a list to maintain the original structure.\n- **NumPy Array Input**: The function directly applies the ReLU transformation using NumPy's `maximum` function, which efficiently handles element-wise operations on arrays.\n\nThis approach ensures that the function is both efficient and flexible, handling different input types seamlessly while maintaining the original container type for the output."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification error rate given two sequences of true class labels and predicted class labels. The classification error rate is defined as the proportion of incorrectly predicted samples. \n\n### Approach\n1. **Input Validation**: First, we check if the lengths of the actual and predicted labels are the same. If they are not, we return -1.\n2. **Conversion to NumPy Arrays**: Convert the input sequences to NumPy arrays to leverage vectorized operations for efficient computation.\n3. **Mismatch Calculation**: Compute the number of mismatches between the actual and predicted labels using element-wise comparison.\n4. **Error Rate Calculation**: Calculate the error rate as the ratio of mismatches to the total number of samples.\n5. **Rounding**: Round the computed error rate to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef classification_error(actual, predicted):\n    \"\"\"Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple"}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies the softmax activation function to a given NumPy array along a specified axis. The softmax function is commonly used in neural networks to convert raw scores into normalized probabilities. The function must handle arrays with at least two dimensions and ensure numerical stability during computation.\n\n### Approach\n1. **Check for 1D Array**: If the input array is 1-dimensional, raise a `ValueError` since softmax cannot be applied to a 1D tensor.\n2. **Numerical Stability**: To avoid numerical instability, we subtract the maximum value along the specified axis from each element before computing the exponential. This helps in preventing overflow issues.\n3. **Compute Exponential**: Calculate the exponential of the shifted values.\n4. **Sum Exponentials**: Sum the exponentials along the specified axis to get the denominator for normalization.\n5. **Normalization**: Divide each element by the sum of exponentials to get the softmax probabilities.\n6. **Rounding and Conversion**: Round the resulting probabilities to 4 decimal places and convert the NumPy array to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    This function must reproduce the behaviour of Keras' backend version shown\n    in the prompt while working solely with NumPy.  The output should be a\n    Python list and every probability must be rounded to four decimal places.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Integer axis along which to apply the softmax.  Negative indices\n               follow NumPy\u2019s convention (e.g. ``axis=-1`` refers to the last\n               axis).\n\n    Returns:\n        Nested Python lists containing the softmax probabilities (rounded to\n        4 decimals).\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n    \n    max_x = np.max(x, axis=axis, keepdims=True)\n    x_shifted = x - max_x\n    exp_x = np.exp(x_shifted)\n    sum_exp = np.sum(exp_x, axis=axis, keepdims=True)\n    softmax = exp_x / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Check for 1D Array**: The function first checks if the input array is 1-dimensional and raises an error if true.\n2. **Numerical Stability**: By subtracting the maximum value along the specified axis, we ensure that the largest value is zero, which helps in preventing overflow when computing the exponential.\n3. **Exponential Calculation**: The shifted values are exponentiated to get positive values.\n4. **Summing Exponentials**: The sum of these exponentials along the specified axis is computed to form the denominator for normalization.\n5. **Normalization**: Each element is divided by the sum to get the probabilities, which are then rounded to 4 decimal places.\n6. **Conversion to List**: The resulting NumPy array is converted to a Python list to meet the output requirements.\n\nThis approach ensures that the function is both numerically stable and efficient, handling arrays of any dimensionality greater than or equal to 2."}
{"task_id": 105, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the Gaussian (Normal) probability density function (PDF) for a given set of observations. The function must handle various input types, validate the standard deviation, and return the results in a specified format.\n\n### Approach\n1. **Validation of Standard Deviation**: The function first checks if the standard deviation (\u03c3) is positive. If \u03c3 is less than or equal to zero, the function returns -1.\n2. **Input Handling**: The function converts the input observations (x) into a NumPy array to handle different input types uniformly. If x is a scalar, it is treated as a single-element array. If x is empty, the function returns an empty list.\n3. **PDF Computation**: Using the Gaussian PDF formula, the function computes the PDF for each element in the input array. The formula used is:\n   \\[\n   f(x; \u03bc, \u03c3) = \\frac{1}{\\sqrt{2\u03c0}\u03c3} \\cdot \\exp\\left(-\\frac{(x-\u03bc)^2}{2\u03c3^2}\\right)\n   \\]\n4. **Rounding and Conversion**: The computed PDF values are rounded to five decimal places and converted into a list of floats before being returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    if sigma <= 0:\n        return -1\n    \n    x_arr = np.asarray(x)\n    x_arr = x_arr.flatten()\n    \n    if x_arr.size == 0:\n        return []\n    \n    x_arr = np.atleast_1d(x_arr)\n    \n    exponent = -( (x_arr - mean) ** 2 ) / (2 * sigma ** 2)\n    denominator = np.sqrt(2 * np.pi) * sigma\n    pdf = (1 / denominator) * np.exp(exponent)\n    pdf_rounded = np.round(pdf, 5)\n    \n    return pdf_rounded.tolist()\n```\n\n### Explanation\n- **Validation**: The function starts by checking if the standard deviation (\u03c3) is positive. If not, it returns -1 immediately.\n- **Input Conversion**: The input observations (x) are converted into a NumPy array and flattened to ensure it is a 1D array. This handles various input types uniformly.\n- **Empty Input Check**: If the input array is empty, the function returns an empty list.\n- **PDF Calculation**: The function computes the exponent and denominator parts of the Gaussian PDF formula separately for clarity and efficiency. It then combines these parts to compute the PDF values.\n- **Rounding and Conversion**: The computed PDF values are rounded to five decimal places and converted into a list of floats before being returned.\n\nThis approach ensures that the function handles different input types gracefully, validates the standard deviation, and efficiently computes the Gaussian PDF for each observation."}
{"task_id": 107, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single optimization step of the Adam algorithm, which is a popular method for training machine learning models. The Adam algorithm uses adaptive learning rates for each parameter, which are computed based on estimates of the first and second moments of the gradients.\n\n### Approach\nThe Adam optimization algorithm involves several steps to update the parameters of a model. Here's a detailed breakdown of the approach:\n\n1. **Compute the First and Second Moments**: \n   - The first moment estimate \\( m_t \\) is an exponential moving average of the gradient \\( g \\).\n   - The second moment estimate \\( v_t \\) is an exponential moving average of the squared gradient \\( g^2 \\).\n\n2. **Bias Correction**:\n   - The first and second moment estimates are biased towards zero in the initial steps. To correct this bias, we divide the estimates by \\( 1 - \\beta_1^t \\) and \\( 1 - \\beta_2^t \\) respectively, where \\( t \\) is the current time step.\n\n3. **Parameter Update**:\n   - Using the bias-corrected estimates, we compute the parameter update. The update is scaled by the learning rate \\( \\alpha \\) and normalized by the square root of the second moment estimate plus a small constant \\( \\epsilon \\) for numerical stability.\n\n4. **Return Values**:\n   - The function returns the updated parameters, the new first and second moment estimates, and the incremented time step, all rounded to 6 decimal places and converted to lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm.\"\"\"\n    # Compute the new first and second moments\n    m_t = beta1 * m_prev + (1 - beta1) * grad\n    v_t = beta2 * v_prev + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias correction factors\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    \n    # Bias-corrected moments\n    m_t_hat = m_t / (1 - beta1_power)\n    v_t_hat = v_t / (1 - beta2_power)\n    \n    # Update parameters\n    theta_new = theta - alpha * (m_t_hat / (np.sqrt(v_t_hat) + epsilon))\n    \n    # Round to 6 decimal places and convert to lists\n    theta_new_rounded = np.round(theta_new, 6).tolist()\n    m_t_rounded = np.round(m_t, 6).tolist()\n    v_t_rounded = np.round(v_t, 6).tolist()\n    \n    return (theta_new_rounded, m_t_rounded, v_t_rounded, t + 1)\n```\n\n### Explanation\n- **First and Second Moments**: The first moment \\( m_t \\) and second moment \\( v_t \\) are updated using exponential decay rates \\( \\beta_1 \\) and \\( \\beta_2 \\) respectively. These moments are computed as weighted averages of the current gradient and the previous moments.\n- **Bias Correction**: The bias-corrected moments \\( \\hat{m}_t \\) and \\( \\hat{v}_t \\) are obtained by dividing the raw moments by the decay factors raised to the power of the current time step \\( t \\).\n- **Parameter Update**: The parameters are updated using the bias-corrected moments, scaled by the learning rate and normalized to ensure stability.\n- **Return Values**: The function returns the updated parameters, moments, and the next time step, all formatted as lists with values rounded to 6 decimal places.\n\nThis approach ensures that the Adam optimization step is correctly implemented, providing efficient and stable parameter updates for training machine learning models."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nB"}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions for character-level sequence encoding and decoding. These functions will be used in a neural model to process text data, convert it into a format suitable for model input, and then convert the model's predictions back into human-readable text.\n\n### Approach\n\n1. **string_to_int**: This function converts a raw text string into a fixed-length list of integer token IDs. The fixed length is determined by the constant `TIME_STEPS` (set to 20). If the input string is shorter than `TIME_STEPS`, it is padded with zeros. If it is longer, it is truncated to fit the fixed length. Characters not found in the provided vocabulary are also converted to zeros.\n\n2. **int_to_string**: This function converts a list of integer token IDs back into a text string using an inverse vocabulary. Padding tokens (zeros) are ignored during the conversion.\n\n3. **run_example**: This function ties everything together. It encodes the input text, feeds it to a model for prediction, processes the model's output to get the predicted token IDs, and then decodes these IDs back into a text string.\n\n### Solution Code\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a string into a fixed-length list of integer token IDs.\"\"\"\n    # Take the first 'time_steps' characters\n    substring = text[:time_steps]\n    encoded = []\n    for c in substring:\n        # Use the vocabulary, default to 0 if not found\n        encoded.append(vocabulary.get(c, 0))\n    # Pad with zeros if necessary\n    while len(encoded) < time_steps:\n        encoded.append(0)\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token IDs back to a string, ignoring padding.\"\"\"\n    s = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        if idx in inverse_vocab:\n            s.append(inverse_vocab[idx])\n    return ''.join(s)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes text, runs through model, and decodes the prediction.\"\"\"\n    # Encode the input text\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Prepare the input for the model (batch size 1)\n    encoded_batch = np.array([encoded])\n    # Get model predictions\n    predictions = model.predict(encoded_batch)\n    # Apply argmax to get the predicted class indices\n    predicted_ids = np.argmax(predictions, axis=2)\n    # Extract the first (and only) sequence from the batch\n    predicted_ids = predicted_ids[0]\n    # Decode the predicted IDs to a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n\n1. **string_to_int**:\n   - This function processes each character of the input string up to the specified `time_steps` length.\n   - Each character is converted to its corresponding integer ID using the provided vocabulary. If a character is not found in the vocabulary, it is converted to 0.\n   - If the resulting list of integer IDs is shorter than `time_steps`, it is padded with zeros to ensure the fixed length.\n\n2. **int_to_string**:\n   - This function converts a list of integer IDs back into a string using an inverse vocabulary.\n   - Padding tokens (zeros) are ignored during the conversion, ensuring they do not appear in the final output string.\n\n3. **run_example**:\n   - This function first encodes the input text into a fixed-length list of integer IDs.\n   - The encoded list is converted into a numpy array and fed into the model for prediction.\n   - The model's output is processed to get the predicted token IDs using argmax.\n   - These predicted IDs are then decoded back into a human-readable string, ignoring any padding tokens.\n\nThis approach ensures that the text is properly formatted for model input and that the model's predictions are accurately converted back into readable text, making it suitable for character-level sequence modeling tasks."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a raw text string into a list of Token objects, where each Token represents a word and its frequency. The list must be sorted by descending frequency and lexicographically for words with the same frequency.\n\n### Approach\n1. **Extract Words**: Use regular expressions to find all sequences of alphabetic characters in the input text. This ensures that only valid words are considered.\n2. **Normalize Case**: Convert all extracted words to lowercase to ensure case-insensitive comparison.\n3. **Count Frequencies**: Use a Counter to count the frequency of each word.\n4. **Create Tokens**: For each unique word, create a Token object and set its count to the frequency from the Counter.\n5. **Sort Tokens**: Sort the list of Token objects first by descending frequency and then lexicographically for words with the same frequency.\n\n### Solution Code\n```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\"\"\"\n    # Extract all words as sequences of alphabetic characters\n    words = re.findall(r'[a-zA-Z]+', text)\n    # Convert all words to lowercase for case-insensitive comparison\n    lower_words = [word.lower() for word in words]\n    # Count the frequency of each word\n    counts = Counter(lower_words)\n    # Create Token objects for each word with its count\n    tokens = []\n    for word, count in counts.items():\n        token = Token(word)\n        token.count = count\n        tokens.append(token)\n    # Sort tokens by descending count, then lexicographically\n    tokens.sort(key=lambda x: (-x.count, x.word))\n    return tokens\n```\n\n### Explanation\n1. **Extract Words**: The regular expression `[a-zA-Z]+` is used to find all sequences of alphabetic characters, ensuring that only valid words are considered.\n2. **Normalize Case**: Each word is converted to lowercase to handle case insensitivity.\n3. **Count Frequencies**: The `Counter` from the `collections` module efficiently counts the occurrences of each word.\n4. **Create Tokens**: For each word, a Token object is created with the word and its count.\n5. **Sort Tokens**: The tokens are sorted using a custom key that sorts primarily by descending count and secondarily by lexicographical order for words with the same count.\n\nThis approach ensures that the output is a list of Token objects sorted according to the specified rules, providing an efficient and clear solution to the problem."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the execution of the model on each input string and the formatting of the output for easy review.\n\n### Approach\n1. **Check for Examples**: The function first checks if the `examples` argument is provided. If not, it falls back to a global constant `EXAMPLES`.\n2. **Iterate Through Examples**: For each input string in the examples, the function calls another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Each prediction is converted from a list of characters into a string. The input and output strings are then printed in a specified format.\n4. **Collect Predictions**: All predicted strings are collected into a list, which is returned at the end.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n        predictions.append(predicted_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Examples**: The function checks if `examples` is `None` and uses the global `EXAMPLES` if necessary.\n- **Running the Model**: For each input string, the `run_example` function is called to get the model's prediction as a list of characters.\n- **Formatting Output**: Each prediction is converted into a string and printed alongside the input string for easy review.\n- **Collecting Results**: All predictions are collected into a list and returned, maintaining the order of the input examples.\n\nThis approach ensures that the function is both efficient and user-friendly, providing clear output for quick review while maintaining the necessary structure for further processing."}
{"task_id": 114, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function and its derivative. The function should handle both activation and derivative modes, returning the results as Python lists with the same shape as the input array, rounded to six decimal places.\n\n### Approach\n1. **Problem Analysis**: The SELU activation function is defined differently for non-negative and negative inputs. Similarly, its derivative also has different forms for non-negative and negative inputs. The function must handle these cases efficiently using vectorized operations for performance.\n2. **Mathematical Definitions**:\n   - For activation: \n     \\[\n     \\operatorname{selu}(x) = \\lambda \\begin{cases} \n     x, & x \\ge 0 \\\\ \n     \\alpha (e^x - 1), & x < 0 \n     \\end{cases}\n     \\]\n   - For derivative:\n     \\[\n     \\operatorname{selu}'(x) = \\lambda \\begin{cases} \n     1, & x \\ge 0 \\\\ \n     \\alpha e^x, & x < 0 \n     \\end{cases}\n     \\]\n3. **Constants**: The predefined constants are \\(\\alpha = 1.6732632423543772848170429916717\\) and \\(\\lambda = 1.0507009873554804934193349852946\\).\n4. **Implementation Strategy**:\n   - Use NumPy for efficient element-wise operations.\n   - Compute the result using `np.where` to handle the piecewise definition.\n   - Round the results to six decimal places using `np.round`.\n   - Convert the NumPy array to a Python list using `tolist()` to preserve the nested structure.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n    \"\"\"Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray containing any real values.\n        derivative: If ``False`` (default) the function returns the SELU\n            activation values. If ``True`` the function returns the analytical\n            gradient of SELU with respect to *x*.\n\n    Returns:\n        A Python nested list with the same shape as *x* containing the SELU\n        activation (or derivative) values rounded to **six** decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772848170429916717\n    lambda_val = 1.0507009873554804934193349852946\n\n    if derivative:\n        result = np.where(x >= 0, lambda_val, lambda_val * alpha * np.exp(x))\n    else:\n        result = np.where(x >= 0, lambda_val * x, lambda_val * alpha * (np.exp(x) - 1))\n\n    result = np.round(result, decimals=6)\n    return result.tolist()\n```\n\n### Explanation\n- **Constants Initialization**: The constants \\(\\alpha\\) and \\(\\lambda\\) are defined at the beginning of the function.\n- **Condition Check**: Using `np.where`, the function efficiently computes the result based on whether each element of the input array is non-negative or negative.\n- **Rounding and Conversion**: The results are rounded to six decimal places and converted to a Python list to match the required output format.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's capabilities for vectorized operations to handle large arrays quickly."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for logistic regression. This function will be used to evaluate the performance of a logistic regression model and to update the model's weights during training.\n\n### Approach\n1. **Compute the Linear Combination**: For each sample, compute the linear combination of features and weights, denoted as \\( z = X \\cdot w \\).\n2. **Apply Sigmoid Function**: Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities**: To avoid numerical issues when taking logarithms, clip the probabilities to a small constant \\( \\epsilon \\) to ensure they are within the range \\([ \\epsilon, 1 - \\epsilon ]\\).\n4. **Compute Loss**: Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [ y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p) ] \\).\n5. **Compute Gradient**: Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Round Results**: Round the loss to 4 decimal places and the gradient to 4 decimal places, converting the gradient into a nested list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    \n    # Reshape y and w to 1D arrays\n    y = y.ravel()\n    w = w.ravel()\n    \n    # Compute z = Xw\n    z = X.dot(w)\n    \n    # Compute p = sigmoid(z)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip p to avoid numerical issues\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute loss\n    loss = - (y * np.log(p) + (1 - y) * np.log(1 - p)).sum() / m\n    loss = round(loss, 4)\n    \n    # Compute gradient\n    gradient = (X.T.dot(p - y)) / m\n    gradient = np.round(gradient, 4)\n    gradient = gradient.reshape(-1, 1).tolist()\n    \n    return (loss, gradient)\n```\n\n### Explanation\n1. **Linear Combination**: The linear combination \\( z \\) is computed using matrix multiplication of the feature matrix \\( X \\) and the weight vector \\( w \\).\n2. **Sigmoid Function**: The sigmoid function transforms the linear combination into probabilities, which are then clipped to avoid numerical issues when taking logarithms.\n3. **Loss Calculation**: The binary cross-entropy loss is computed by averaging the negative log-likelihood across all samples.\n4. **Gradient Calculation**: The gradient is computed using the transpose of the feature matrix and the difference between the predicted probabilities and the actual targets.\n5. **Rounding and Formatting**: The loss is rounded to 4 decimal places, and the gradient is rounded and converted into a nested list format for the final output.\n\nThis approach ensures that the function efficiently computes both the loss and its gradient, handling numerical stability issues and providing results in the required format."}
{"task_id": 116, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the reduction in mean-squared error (MSE) achieved by a split in a regression decision tree. The goal is to determine how much the MSE is reduced when the target values are split into child nodes.\n\n### Approach\n1. **Calculate Parent MSE**: The parent node's MSE is computed by taking the mean of the squared differences between each target value and the mean of all target values.\n2. **Calculate Child MSE**: For each child node resulting from the split, compute the MSE by taking the mean of the squared differences between each target value in the child node and the mean of the target values in that child node. Each child node's MSE is then weighted by the proportion of samples it contains relative to the total number of samples in the parent node.\n3. **Compute MSE Reduction**: The reduction in MSE (\u0394MSE) is the difference between the parent MSE and the weighted sum of the child MSEs. This value is then rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n    \"\"\"Calculate the reduction in mean-squared error achieved by a split.\n\n    Args:\n        y: A 1-D NumPy array containing the original target values.\n        splits: A list where each element is a 1-D NumPy array holding the\n            target values that fall into one child node after the split.\n\n    Returns:\n        A float \u2013 the parent MSE minus the weighted child MSE \u2013 rounded to\n        four decimal places.  A higher value indicates a better split.\n    \"\"\"\n    N = len(y)\n    if N == 0:\n        return 0.0  # Edge case, though unlikely given problem constraints\n    \n    mean_y = np.mean(y)\n    parent_mse = np.mean((y - mean_y) ** 2)\n    \n    mse_children = 0.0\n    for split in splits:\n        len_split = len(split)\n        if len_split == 0:\n            continue  # Skip empty splits, though problem implies splits are non-empty\n        mean_split = np.mean(split)\n        split_mse = np.mean((split - mean_split) ** 2)\n        weight = len_split / N\n        mse_children += weight * split_mse\n    \n    delta_mse = parent_mse - mse_children\n    return round(delta_mse, 4)\n```\n\n### Explanation\n1. **Parent MSE Calculation**: The mean of the target values is computed, and each value is squared and averaged after subtracting this mean. This gives the parent MSE.\n2. **Child MSE Calculation**: For each child node, the mean of its target values is computed. Each value in the child node is squared after subtracting this mean, and the average of these squared differences gives the child MSE. This MSE is then weighted by the proportion of samples in the child node relative to the parent node.\n3. **MSE Reduction**: The reduction in MSE is the difference between the parent MSE and the weighted sum of the child MSEs. This value is rounded to four decimal places to provide the final result.\n\nThis approach ensures that we accurately measure how much the MSE is reduced by the split, providing a clear indication of the quality of the split in a regression decision tree."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the optimal weights for an ordinary least-squares linear regression model. The function will take a design matrix `X` and a target vector `y`, and return the weight vector that minimizes the square error.\n\n### Approach\n1. **Convert Lists to NumPy Arrays**: Convert the input lists `X` and `y` into NumPy arrays for efficient numerical computations.\n2. **Augment the Design Matrix**: Add a column of ones to the design matrix `X` to account for the intercept term in the linear regression model.\n3. **Compute the Pseudo-Inverse**: Use the Moore-Penrose pseudo-inverse of the augmented matrix to solve for the weights. This method is robust even when the matrix is not invertible.\n4. **Calculate Weights**: Multiply the pseudo-inverse of the augmented matrix by the target vector `y` to obtain the optimal weights.\n5. **Round and Return**: Round each weight to four decimal places and return them as a flat Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_weights(X: list[list[int | float]], y: list[int | float]) -> list[float]:\n    \"\"\"Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector `[w0, w1, ..., wd]` rounded to 4 decimal places, where\n        `w0` is the intercept term.\n    \"\"\"\n    # Convert the input lists to NumPy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Augment X with a column of ones for the intercept term\n    X_augmented = np.insert(X_np, 0, 1, axis=1)\n    \n    # Compute the Moore-Penrose pseudo-inverse of the augmented matrix\n    X_pseudo = np.linalg.pinv(X_augmented)\n    \n    # Calculate the optimal weights\n    weights = X_pseudo @ y_np\n    \n    # Round the weights to 4 decimal places and convert to a Python list\n    weights_rounded = np.round(weights, 4).tolist()\n    \n    return weights_rounded\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The function starts by converting the input lists `X` and `y` into NumPy arrays to leverage efficient numerical operations.\n2. **Augmenting the Matrix**: A column of ones is added to the design matrix `X` to include the intercept term in the linear model. This augmented matrix is stored in `X_augmented`.\n3. **Pseudo-Inverse Calculation**: The Moore-Penrose pseudo-inverse of the augmented matrix is computed using `np.linalg.pinv()`. This method is chosen because it handles cases where the matrix might not be invertible.\n4. **Weight Calculation**: The optimal weights are obtained by multiplying the pseudo-inverse of the augmented matrix with the target vector `y`.\n5. **Rounding and Conversion**: The computed weights are rounded to four decimal places and converted into a Python list to meet the output requirements.\n\nThis approach ensures that the solution is both efficient and robust, providing the correct weights for the linear regression model even in cases where the design matrix is not invertible."}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array of categorical values into its one-hot encoded representation. The goal is to detect the unique categories automatically, sort them, and then represent each sample as a binary row vector where each position is 1 if the sample belongs to the corresponding category and 0 otherwise.\n\n### Approach\n1. **Extract Unique Categories**: Use NumPy's `np.unique` function to extract the unique categories from the input array. This function also sorts the unique values by default, which simplifies our task.\n2. **Generate One-Hot Encoded Vectors**: For each element in the input array, create a binary vector where each position corresponds to a unique category. The position is set to 1 if the element matches the category and 0 otherwise.\n3. **Construct the Result**: Collect all these binary vectors into a list of lists, which is the required one-hot encoded matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to one-hot encoded format.\"\"\"\n    unique = np.unique(y)\n    result = []\n    for element in y:\n        row = []\n        for cat in unique:\n            row.append(1 if element == cat else 0)\n        result.append(row)\n    return result\n```\n\n### Explanation\n1. **Extract Unique Categories**: The `np.unique` function is used to get the unique values from the input array `y`. These values are sorted in ascending order for integers and alphabetically for strings.\n2. **Generate Binary Vectors**: For each element in `y`, we iterate over each unique category. If the element matches the category, we append 1 to the current row; otherwise, we append 0.\n3. **Construct Result**: Each binary vector (row) is added to the result list, which is returned as the final one-hot encoded matrix.\n\nThis approach ensures that we efficiently convert the input array into the required one-hot encoded format without using any external libraries beyond NumPy for handling the input array."}
{"task_id": 126, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the polynomial kernel Gram matrix for given data matrices. The polynomial kernel is a commonly used kernel in kernel methods such as Support Vector Machines (SVMs). The function should handle various edge cases and efficiently compute the kernel using NumPy for speed and clarity.\n\n### Approach\n1. **Convert Input Data to NumPy Arrays**: Convert the input lists `X` and `Y` into NumPy arrays for efficient matrix operations.\n2. **Handle Default Values**: If `Y` is not provided, use `X` as the default value for `Y`. If `gamma` is not provided, compute it as `1/C`, where `C` is the number of features in `X`.\n3. **Compute Dot Product**: Calculate the dot product of `X` and the transpose of `Y` to get the matrix of inner products.\n4. **Apply Kernel Formula**: Use the polynomial kernel formula `(gamma * dot_product + c0)^d` to compute the kernel matrix.\n5. **Round Results**: Round each element of the resulting kernel matrix to 4 decimal places for precision.\n6. **Convert to Python List**: Convert the NumPy array result back to a nested Python list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n    \"\"\"Compute the degree-d polynomial kernel between all rows of *X* and *Y*.\n\n    Args:\n        X: First data matrix as a list-of-lists, shape (N, C).\n        Y: Optional second data matrix; if *None* defaults to *X*.\n        d: Degree of the polynomial.\n        gamma: Scaling factor.  Uses 1/C when *None*.\n        c0: Bias term.\n\n    Returns:\n        Gram matrix as a (nested) Python list rounded to 4 decimals.\n    \"\"\"\n    # Convert X and Y to NumPy arrays\n    X_np = np.array(X)\n    if Y is None:\n        Y_np = X_np\n    else:\n        Y_np = np.array(Y)\n    \n    # Compute the dot product\n    dot_product = X_np.dot(Y_np.T)\n    \n    # Determine gamma\n    C = X_np.shape[1]\n    if gamma is None:\n        gamma_val = 1.0 / C\n    else:\n        gamma_val = gamma\n    \n    # Compute the kernel\n    kernel = (gamma_val * dot_product + c0) ** d\n    \n    # Round to 4 decimals\n    kernel_rounded = np.round(kernel, decimals=4)\n    \n    # Convert to Python list\n    result = kernel_rounded.tolist()\n    \n    return result\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can leverage NumPy's efficient matrix operations for faster computation.\n- **Handling Defaults**: By setting `Y` to `X` if it is not provided and computing `gamma` as `1/C` if it is not given, we handle the default cases appropriately.\n- **Dot Product Calculation**: The dot product of `X` and `Y.T` gives the matrix of inner products, which is a crucial step in computing the polynomial kernel.\n- **Kernel Formula Application**: Using the polynomial kernel formula, we transform the inner products into the kernel matrix.\n- **Rounding and Conversion**: Rounding ensures the results are precise to four decimal places, and converting back to a Python list provides the required output format.\n\nThis approach efficiently computes the polynomial kernel Gram matrix while handling various edge cases and ensuring numerical precision."}
{"task_id": 127, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a confusion matrix from scratch. A confusion matrix is a tool used to evaluate the performance of a classification model. It provides a detailed summary of correct and incorrect predictions made by the model, broken down by each class.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the true labels (`y_true`) and predicted labels (`y_pred`) are the same. If they are not, we return -1 as the evaluation cannot be performed.\n2. **Identify Unique Classes**: We gather all unique classes from both `y_true` and `y_pred`, sort them, and create a mapping from each class to its index in the confusion matrix.\n3. **Initialize Matrix**: We initialize a square matrix of size C x C, where C is the number of unique classes. Each element in the matrix starts at 0.\n4. **Populate Matrix**: We iterate through each pair of true and predicted labels, updating the corresponding cell in the matrix to reflect the count of each true-predicted label pair.\n\n### Solution Code\n```python\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n    \"\"\"Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with true class equal to\n        the *i-th* sorted unique label and predicted class equal to the\n        *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    \n    # Get all unique classes from both y_true and y_pred\n    unique_true = set(y_true)\n    unique_pred = set(y_pred)\n    all_classes = unique_true.union(unique_pred)\n    classes = sorted(all_classes)\n    \n    # Create a mapping from class to its index\n    class_to_idx = {c: i for i, c in enumerate(classes)}\n    \n    # Initialize the confusion matrix with zeros\n    num_classes = len(classes)\n    matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n    \n    # Populate the matrix\n    for yt, yp in zip(y_true, y_pred):\n        i = class_to_idx[yt]\n        j = class_to_idx[yp]\n        matrix[i][j] += 1\n    \n    return matrix\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of `y_true` and `y_pred` are equal. If not, it returns -1.\n2. **Unique Classes**: The unique classes from both `y_true` and `y_pred` are combined into a set, sorted, and stored in a list. This list determines the rows and columns of the confusion matrix.\n3. **Mapping Classes**: Each class is mapped to an index to facilitate easy access and updating of the matrix.\n4. **Matrix Initialization**: A matrix of size C x C (where C is the number of unique classes) is initialized with zeros.\n5. **Matrix Population**: For each pair of true and predicted labels, the corresponding cell in the matrix is incremented to reflect the count of that true-predicted pair.\n\nThis approach ensures that the confusion matrix accurately reflects the performance of the classification model, providing insights into the model's accuracy, precision, recall, and other metrics."}
{"task_id": 129, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that returns the first `samples` elements from an infinite cycle over a given sequence. This function should handle both lists and tuples as input sequences and return an empty list if the sequence is empty or if `samples` is not a positive integer.\n\n### Approach\n1. **Check for Edge Cases**: If the input sequence is empty or the number of samples requested is not positive, return an empty list immediately.\n2. **Create an Infinite Cycle**: Use the `itertools.cycle` function to create an infinite iterator that repeatedly cycles through the elements of the input sequence.\n3. **Extract the First `samples` Elements**: Use the `itertools.islice` function to extract the first `samples` elements from the infinite iterator.\n4. **Convert to List**: Convert the sliced iterator into a list and return it.\n\nThis approach ensures that we efficiently handle the infinite cycling and slicing operations without raising exceptions for edge cases.\n\n### Solution Code\n```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    if not sequence or samples <= 0:\n        return []\n    c = cycle(sequence)\n    return list(islice(c, samples))\n```\n\n### Explanation\n- **Edge Case Handling**: The function first checks if the sequence is empty or if `samples` is non-positive. If either condition is met, it returns an empty list immediately.\n- **Infinite Cycle Creation**: The `itertools.cycle` function is used to create an infinite iterator that loops over the input sequence repeatedly.\n- **Slicing the Iterator**: The `itertools.islice` function is used to take the first `samples` elements from the infinite iterator. This is efficient as it does not require storing the entire infinite sequence in memory.\n- **Conversion to List**: The sliced iterator is converted into a list to match the required return type.\n\nThis solution efficiently handles the problem requirements and constraints, ensuring that it works correctly for both lists and tuples, and gracefully handles edge cases."}
{"task_id": 130, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the empirical probability distribution over the next states for a given (state, action) pair in a tabular Markov Decision Process. This involves analyzing a list of experience tuples to count occurrences of each next state and then computing the probabilities.\n\n### Approach\n1. **Filter Transitions**: Extract all transitions from the given list where the state and action match the queried state and action.\n2. **Count Next States**: Use a counter to tally the occurrences of each next state from the filtered transitions.\n3. **Calculate Probabilities**: For each next state, compute its probability by dividing its count by the total number of filtered transitions. Round these probabilities to four decimal places.\n4. **Sort and Return**: Sort the resulting list of [next_state, probability] pairs by next_state in ascending order and return this list.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities must be rounded to 4 decimal places.  If the\n    pair never occurred, return an empty list.\n    \"\"\"\n    # Filter transitions to get only those with the given state and action\n    filtered = [t for t in transitions if t[0] == state and t[1] == action]\n    \n    # If no transitions found, return an empty list\n    if not filtered:\n        return []\n    \n    # Count occurrences of each next_state\n    counts = Counter(t[3] for t in filtered)\n    total = len(filtered)\n    \n    # Calculate probabilities and round to 4 decimal places\n    probs = []\n    for next_state, count in counts.items():\n        prob = count / total\n        rounded_prob = round(prob, 4)\n        probs.append([next_state, rounded_prob])\n    \n    # Sort the probabilities by next_state\n    probs.sort(key=lambda x: x[0])\n    \n    return probs\n```\n\n### Explanation\n1. **Filtering Transitions**: The code first filters the list of transitions to include only those that match the given state and action. This is done using a list comprehension.\n2. **Counting Next States**: Using the `Counter` class from the `collections` module, the code counts how many times each next state appears in the filtered transitions.\n3. **Calculating Probabilities**: For each unique next state, the code calculates its probability by dividing its count by the total number of filtered transitions. Each probability is then rounded to four decimal places.\n4. **Sorting and Returning**: The resulting list of [next_state, probability] pairs is sorted by next_state in ascending order and returned.\n\nThis approach ensures that we efficiently compute the empirical probabilities and handle edge cases such as when no transitions match the given state and action."}
{"task_id": 131, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that retrieves an element from a specific sequence within a shared container. The function must handle cases where the sequence identifier or the index is invalid by returning `None` instead of raising an exception.\n\n### Approach\n1. **Check Validity of Sequence Identifier (uid):** The function first checks if the given `uid` is a valid index for the shared container `_SHARED_SEQUENCES`. If `uid` is negative or exceeds the number of sequences in the container, the function returns `None`.\n\n2. **Retrieve the Sequence:** If the `uid` is valid, the function retrieves the corresponding sequence from the shared container.\n\n3. **Check Sequence Validity:** The function then checks if the retrieved sequence is empty. If it is, the function returns `None` since there are no elements to access.\n\n4. **Compute Effective Index:** The function computes the effective index by considering both positive and negative indices. Negative indices are converted to their positive counterparts by adding the length of the sequence to the index.\n\n5. **Check Index Validity:** The function checks if the computed effective index is within the valid range of the sequence. If it is not, the function returns `None`.\n\n6. **Return the Element:** If all checks are passed, the function returns the element at the computed effective index.\n\n### Solution Code\n```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` contains every available sequence so\n    that several parts of a program can work on different sequences at the\n    same time.\n\n    If *uid* or *i* is invalid the function must return *None* instead of\n    raising an exception.\n\n    Args:\n        uid: Integer identifier of the desired sequence.\n        i:   Position inside the selected sequence (supports negative indices).\n\n    Returns:\n        The requested element, or None if the access is invalid.\n    \"\"\"\n    # Check if uid is a valid index for _SHARED_SEQUENCES\n    if uid < 0 or uid >= len(_SHARED_SEQUENCES):\n        return None\n    # Retrieve the sequence\n    seq = _SHARED_SEQUENCES[uid]\n    # Check if the sequence is empty\n    if not seq:\n        return None\n    # Compute the effective index\n    if i >= 0:\n        effective_i = i\n    else:\n        effective_i = len(seq) + i\n    # Check if the effective index is within bounds\n    if effective_i < 0 or effective_i >= len(seq):\n        return None\n    return seq[effective_i]\n```\n\n### Explanation\n- **Validity Check for uid:** The function ensures that `uid` is within the valid range of indices for the shared container. This prevents accessing non-existent sequences.\n- **Sequence Retrieval and Check:** After retrieving the sequence, the function checks if it is empty to avoid index errors.\n- **Effective Index Calculation:** The function handles both positive and negative indices by converting negative indices to their positive counterparts, ensuring correct element retrieval.\n- **Index Bounds Check:** The function ensures that the computed index is within the valid range of the sequence, preventing out-of-bounds errors.\n\nThis approach ensures that the function handles all edge cases gracefully, returning `None` for invalid inputs and correctly retrieving elements when possible."}
{"task_id": 133, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a batch of integer-encoded sequences into a 3-D one-hot encoded format. This is a common requirement in sequence-to-sequence or language modeling tasks where the targets need to be in a specific format for training deep learning models.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Validation Check**: First, we check each token in the input sequences to ensure they are within the valid range. A token is considered invalid if it is negative or greater than or equal to the vocabulary length. If any invalid token is found, the function returns -1.\n\n2. **One-Hot Encoding**: If all tokens are valid, we proceed to create the one-hot encoded vectors. For each token, we generate a vector of zeros with a length equal to the vocabulary size. The position corresponding to the token's index is set to 1, creating a one-hot vector.\n\n3. **Efficient Vector Creation**: Using NumPy, we efficiently create these one-hot vectors. NumPy's `zeros` function is used to create the zero vectors, and the appropriate index is set to 1 for each token. The resulting vectors are then converted to lists to ensure the output is a pure Python nested list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary.\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested list with shape (m, time_steps, vocab_length) representing the\n        one-hot encoded targets, or -1 if the input contains invalid indices.\n    \"\"\"\n    # Check for invalid tokens\n    for seq in X_train:\n        for token in seq:\n            if token < 0 or token >= vocab_length:\n                return -1\n    \n    # Create one-hot encoded sequences\n    Y_train = []\n    for seq in X_train:\n        one_hot_seq = []\n        for token in seq:\n            vector = np.zeros(vocab_length, dtype=int)\n            vector[token] = 1\n            one_hot_seq.append(vector.tolist())\n        Y_train.append(one_hot_seq)\n    \n    return Y_train\n```\n\n### Explanation\n- **Validation Check**: The function iterates through each token in the input sequences. If any token is found to be outside the valid range (0 to vocab_length - 1), the function immediately returns -1.\n- **One-Hot Encoding**: For each valid token, a zero vector of length `vocab_length` is created. The position corresponding to the token's index is set to 1, converting the token into a one-hot vector.\n- **Efficiency**: Using NumPy for vector creation ensures that the process is efficient, especially for larger vocabularies and sequences. The resulting vectors are converted to lists to meet the requirement of returning a pure Python nested list.\n\nThis approach ensures that the function handles both valid and invalid inputs correctly and efficiently, providing the required one-hot encoded targets for deep learning models."}
{"task_id": 134, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to identify the arm with the highest expected reward in a Bernoulli bandit setup. Each arm provides a binary reward (0 or 1) with a given probability. The goal is to determine the arm with the highest success probability and return both this probability and the corresponding arm index.\n\n### Approach\n1. **Input Validation**: First, check if the input list is empty. If it is, return the tuple (-1.0, -1). Next, verify that all probabilities in the list are within the valid range [0, 1]. If any probability is outside this range, return the same error tuple.\n2. **Find Maximum Probability**: If the input is valid, determine the maximum probability value in the list.\n3. **Determine the Best Arm**: Identify the smallest index where this maximum probability occurs. This ensures that in case of ties, the smallest index is chosen.\n\n### Solution Code\n```python\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n    \"\"\"Find the arm with the highest expected reward in a Bernoulli bandit.\"\"\"\n    if not payoff_probs:\n        return (-1.0, -1)\n    for p in payoff_probs:\n        if p < 0 or p > 1:\n            return (-1.0, -1)\n    max_p = max(payoff_probs)\n    index = payoff_probs.index(max_p)\n    return (max_p, index)\n```\n\n### Explanation\n1. **Input Validation**: The function starts by checking if the input list is empty. If it is, the function immediately returns (-1.0, -1). It then iterates through each probability to ensure they are within the valid range [0, 1]. If any probability is found to be outside this range, the function returns the error tuple.\n2. **Finding Maximum Probability**: Using Python's built-in `max` function, the function identifies the highest probability in the list.\n3. **Identifying the Best Arm**: The `index` method is used to find the first occurrence of the maximum probability, ensuring the smallest index is returned in case of ties.\n\nThis approach efficiently handles all edge cases and ensures the correct arm is identified based on the given constraints."}
{"task_id": 136, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a numerically-stable softmax activation function using NumPy. The softmax function converts raw scores into a probability distribution, but directly exponentiating large or very small numbers can lead to overflow or underflow issues. The stable version of the softmax function addresses this by subtracting the maximum value from each element before exponentiation.\n\n### Approach\n1. **Subtract the Maximum Value**: Compute the maximum value along the specified axis and subtract this value from each element in the input array. This step ensures that the largest exponent is zero, reducing the range of values and preventing overflow or underflow.\n2. **Exponentiate the Shifted Values**: Apply the exponential function to each element of the shifted array.\n3. **Sum the Exponentials**: Compute the sum of these exponentials along the specified axis.\n4. **Normalize**: Divide each element of the exponentiated array by the computed sum to get the probabilities.\n5. **Round the Results**: Round each probability to four decimal places.\n6. **Convert to List**: Convert the resulting NumPy array to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n    \"\"\"Compute the numerically-stable softmax of *z* along *axis*.\n\n    The function should:\n    1. Subtract the maximum value along *axis* from every element of *z*.\n    2. Exponentiate the shifted values.\n    3. Divide by the sum of exponentials along the same axis.\n    4. Round the result to four decimal places.\n    5. Convert the NumPy array to a Python list and return it.\n\n    Args:\n        z: Input NumPy array containing raw scores.\n        axis: Axis along which softmax is computed (default: -1).\n\n    Returns:\n        A Python list with the same shape as *z* containing the softmax\n        probabilities rounded to four decimals.\n    \"\"\"\n    max_z = np.max(z, axis=axis, keepdims=True)\n    shifted_z = z - max_z\n    exp_z = np.exp(shifted_z)\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n    softmax = exp_z / sum_exp\n    rounded = np.round(softmax, decimals=4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Subtract the Maximum Value**: The maximum value along the specified axis is computed and subtracted from each element to ensure numerical stability.\n2. **Exponentiate**: Each element of the shifted array is exponentiated to convert the scores into positive values.\n3. **Sum and Normalize**: The sum of these exponentials is computed along the same axis, and each element is divided by this sum to produce probabilities.\n4. **Rounding and Conversion**: The resulting probabilities are rounded to four decimal places and converted to a Python list for the final output.\n\nThis approach ensures that the softmax function is computed in a numerically stable manner, avoiding potential issues with overflow or underflow."}
{"task_id": 138, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the classification accuracy between two lists of labels. The classification accuracy is the proportion of elements that are identical in the same positions of both sequences. The function should handle various types of labels, including numerical values, strings, and booleans, and return the accuracy as a floating-point number rounded to four decimal places. If the input lists are of different lengths or are empty, the function should return -1.0.\n\n### Approach\n1. **Input Validation**: Check if the lengths of the two input lists are equal and non-zero. If not, return -1.0.\n2. **Count Matches**: Iterate through the elements of both lists simultaneously and count how many times the elements at the same positions are equal.\n3. **Compute Accuracy**: Calculate the accuracy by dividing the count of matches by the total number of elements.\n4. **Round the Result**: Round the computed accuracy to four decimal places and return it as a float.\n\n### Solution Code\n```python\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1.0\n    count = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    n = len(y_true)\n    accuracy = count / n\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are equal and non-zero. If not, it returns -1.0 immediately.\n2. **Counting Matches**: Using a generator expression with `zip`, the function iterates through pairs of elements from `y_true` and `y_pred`, counting how many times the elements are equal.\n3. **Computing Accuracy**: The count of matches is divided by the total number of elements to get the accuracy.\n4. **Rounding**: The computed accuracy is rounded to four decimal places using Python's `round` function and returned as a float.\n\nThis approach ensures that the function efficiently computes the classification accuracy while handling various edge cases, such as mismatched list lengths and empty lists."}
{"task_id": 143, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function and its derivative using NumPy. The Leaky ReLU function is a variant of the ReLU function that allows a small negative slope, which helps mitigate the \"dying ReLU\" problem where neurons can become inactive during training.\n\n### Approach\n1. **Input Handling**: The function should accept inputs in the form of NumPy arrays, lists, or tuples. If the input is a list or tuple, it is converted to a NumPy array.\n2. **Activation Function**: For each element in the input array, the Leaky ReLU function returns the element itself if it is non-negative, and a scaled version of the element (using a small positive slope `alpha`) if it is negative.\n3. **Derivative Function**: The derivative of the Leaky ReLU function is 1 for non-negative inputs and `alpha` for negative inputs. This derivative is used during back-propagation to compute gradients efficiently.\n4. **Vectorized Operations**: The implementation uses NumPy's vectorized operations to ensure efficiency and avoid explicit loops, which is crucial for handling large arrays common in deep learning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n    \"\"\"Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative slope coefficient. Default is 0.2.\n    derivative : bool, optional\n        If False (default), compute the Leaky ReLU activation.\n        If True, compute the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with the same shape as *x* containing the computed values.\n    \"\"\"\n    x = np.asarray(x) if not isinstance(x, np.ndarray) else x\n    if not derivative:\n        return np.where(x >= 0, x, alpha * x)\n    else:\n        return np.where(x >= 0, 1, alpha)\n```\n\n### Explanation\n- **Input Conversion**: The function first checks if the input `x` is a NumPy array. If not, it converts `x` to a NumPy array using `np.asarray`.\n- **Activation Calculation**: Using `np.where`, the function checks each element of `x`. If an element is non-negative, it is returned as is. If negative, it is scaled by `alpha`.\n- **Derivative Calculation**: Similarly, the derivative is computed using `np.where`. For non-negative elements, the derivative is 1, and for negative elements, it is `alpha`.\n\nThis approach ensures that the function is efficient, handles arbitrary input shapes, and adheres to the requirements of deep learning frameworks that need both the activation and its derivative for back-propagation."}
{"task_id": 147, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression model from scratch using NumPy. The goal is to train the model on a given training dataset and use it to predict class labels for an unseen test dataset. The model will use batch gradient descent to minimize the negative log-likelihood, which is equivalent to maximizing the likelihood of the training data.\n\n### Approach\n1. **Special Case Handling**: If all labels in the training set are the same, the model will predict that class for all test examples. This is a straightforward check to handle cases where the data is trivially separable.\n2. **Data Preprocessing**: Add a bias term (a column of ones) to both the training and test feature matrices to simplify the model's bias handling.\n3. **Weight Initialization**: Initialize the weights to zeros. The weights will be updated during training.\n4. **Training Loop**: Use batch gradient descent to update the weights for a given number of epochs. For each epoch:\n   - Compute the linear combination of features and weights.\n   - Apply the sigmoid function to get predicted probabilities.\n   - Calculate the error between predicted probabilities and actual labels.\n   - Compute the gradient of the loss with respect to the weights.\n   - Update the weights using the computed gradient and learning rate.\n5. **Prediction**: After training, use the learned weights to make predictions on the test set by applying the sigmoid function and thresholding at 0.5.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10000,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model and predict labels for `X_test`.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary target vector of shape (n_samples,). Must contain only 0 and 1.\n    X_test : np.ndarray\n        Feature matrix to classify of shape (m_samples, n_features).\n    learning_rate : float, optional (default=0.1)\n        Step size used by batch gradient descent.\n    epochs : int, optional (default=10000)\n        Number of gradient descent iterations.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels for every row in `X_test`.\n    \"\"\"\n    # Check if all labels are the same\n    if np.all(y_train == y_train[0]):\n        return [int(y_train[0])] * X_test.shape[0]\n    \n    # Add bias term to X_train and X_test\n    X_train_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test_with_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    \n    # Initialize weights\n    n_features = X_train_with_bias.shape[1]\n    w = np.zeros(n_features)\n    \n    # Training loop\n    for _ in range(epochs):\n        z = X_train_with_bias.dot(w)\n        h = 1 / (1 + np.exp(-z))\n        error = h - y_train\n        gradient = (X_train_with_bias.T.dot(error)) / X_train_with_bias.shape[0]\n        w = w - learning_rate * gradient\n    \n    # Make predictions\n    z_test = X_test_with_bias.dot(w)\n    h_test = 1 / (1 + np.exp(-z_test))\n    predictions = (h_test >= 0.5).astype(int)\n    \n    return predictions.tolist()\n```\n\n### Explanation\n- **Special Case Handling**: The function first checks if all training labels are the same. If so, it directly returns the same label for all test examples.\n- **Data Preprocessing**: A column of ones is added to both the training and test feature matrices to handle the bias term in the model.\n- **Weight Initialization**: Weights are initialized to zeros, which is a common starting point for gradient descent.\n- **Training Loop**: For each epoch, the model computes the predicted probabilities using the sigmoid function, calculates the error, computes the gradient, and updates the weights. This process continues for the specified number of epochs.\n- **Prediction**: After training, the model makes predictions on the test set by applying the sigmoid function to the linear combination of features and weights, then thresholding at 0.5 to get class labels.\n\nThis approach ensures that the model is trained efficiently using batch gradient descent and can handle both linearly separable and non-linearly separable data effectively."}
{"task_id": 148, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the exponential moving average (EMA) of a given sequence of numerical observations using a specified smoothing factor. The function should handle various edge cases and return the results in a specific format.\n\n### Approach\n1. **Input Validation**: Check if the input data list is empty or if the smoothing factor `alpha` is outside the valid range [0, 1]. If either condition is met, return -1.\n2. **Special Cases Handling**:\n   - If `alpha` is 0, the output list will consist of the first element of the input list repeated for all positions.\n   - If `alpha` is 1, the output list will be identical to the input list.\n3. **General Case Handling**: For valid `alpha` values between 0 and 1, compute the EMA using the recursive formula. The first element of the result is the same as the first element of the input list. Each subsequent element is computed as a weighted average of the previous result and the current input element.\n4. **Rounding**: Each element of the result list is rounded to four decimal places before returning.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef exp_smoothing(data: List[float], alpha: float) -> List[float] | int:\n    \"\"\"Compute the exponential moving average of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n   "}
{"task_id": 152, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the accuracy score of a classifier. The accuracy score is the proportion of correctly classified samples out of the total samples. This function should handle both Python lists and NumPy arrays as inputs and return the accuracy rounded to four decimal places. If the inputs are invalid (different lengths or empty), the function should return -1.\n\n### Approach\n1. **Input Validation**: Check if the lengths of the input arrays are the same and neither is empty. If not, return -1.\n2. **Conversion to NumPy Arrays**: Convert the input lists or arrays to NumPy arrays to leverage NumPy's efficient operations.\n3. **Compute Correct Predictions**: Use element-wise comparison to count the number of correct predictions.\n4. **Calculate Accuracy**: Divide the number of correct predictions by the total number of samples to get the accuracy.\n5. **Round the Result**: Round the computed accuracy to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n    \"\"\"\n    if len(y_true) != len(y_pred) or len(y_true) == 0:\n        return -1\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `y_true` and `y_pred` are the same and neither is empty. If either condition is not met, it returns -1.\n2. **Conversion to NumPy Arrays**: The inputs are converted to NumPy arrays to facilitate efficient element-wise operations.\n3. **Element-wise Comparison**: Using `y_true == y_pred`, we create a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n4. **Summing Correct Predictions**: The sum of the boolean array gives the count of correct predictions since `True` is treated as 1 and `False` as 0.\n5. **Calculating Accuracy**: The accuracy is computed by dividing the number of correct predictions by the total number of samples.\n6. **Rounding**: The result is rounded to four decimal places using Python's `round` function and returned.\n\nThis approach ensures that the function efficiently computes the accuracy score while handling various edge cases and input types."}
{"task_id": 154, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the Radial Basis Function (RBF) kernel matrix for two given sets of vectors. The RBF kernel matrix is a measure of similarity between pairs of vectors, which is widely used in machine learning algorithms such as Support Vector Machines (SVMs).\n\n### Approach\nThe RBF kernel matrix is defined such that each element \\( K_{ij} \\) is computed using the formula:\n\\[ K_{ij} = \\exp(-\\gamma \\cdot ||x_i - y_j||_2^2) \\]\nwhere \\( \\gamma \\) is a positive scalar that controls the decay rate of the similarity with distance.\n\nThe approach to compute this matrix efficiently involves the following steps:\n1. **Reshape Input Arrays**: Ensure that both input arrays are 2-dimensional. If they are 1-dimensional, reshape them to treat each as a single sample.\n2. **Compute Norms**: Calculate the squared Euclidean norms for each row in both input arrays.\n3. **Dot Product Calculation**: Compute the dot product matrix between the two input arrays.\n4. **Squared Distances**: Use the norms and dot product to compute the squared Euclidean distances between all pairs of vectors.\n5. **Kernel Matrix**: Apply the RBF formula to the squared distances to get the kernel matrix.\n6. **Rounding and Conversion**: Round each element of the kernel matrix to six decimal places and convert it to a nested list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel (default 0.1).\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # Reshape 1D arrays to 2D\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n    \n    # Compute squared norms\n    X_norms = np.sum(X ** 2, axis=1)\n    Y_norms = np.sum(Y ** 2, axis=1)\n    \n    # Compute dot product\n    dot_product = X.dot(Y.T)\n    \n    # Compute squared distances\n    squared_distances = X_norms[:, np.newaxis] + Y_norms[np.newaxis, :] - 2 * dot_product\n    \n    # Compute RBF kernel\n    K = np.exp(-gamma * squared_distances)\n    \n    # Round to 6 decimal places and convert to list\n    K_rounded = np.round(K, 6)\n    return K_rounded.tolist()\n```\n\n### Explanation\n1. **Reshape Input Arrays**: The function first checks if the input arrays are 1-dimensional and reshapes them to 2-dimensional arrays if necessary. This ensures that the subsequent operations work correctly.\n2. **Compute Norms**: The squared norms of each row in the input arrays are computed using `np.sum(X ** 2, axis=1)` and `np.sum(Y ** 2, axis=1)`.\n3. **Dot Product Calculation**: The dot product between the two input arrays is computed using `X.dot(Y.T)`, which results in a matrix where each element (i, j) is the dot product of the i-th row of X and the j-th row of Y.\n4. **Squared Distances**: Using the formula for squared Euclidean distance, the distances between all pairs of vectors are computed efficiently using broadcasting.\n5. **Kernel Matrix**: The RBF kernel values are computed by applying the exponential function to the negative product of gamma and the squared distances.\n6. **Rounding and Conversion**: The resulting kernel matrix is rounded to six decimal places and converted to a nested list format using `tolist()` for the final output.\n\nThis approach ensures that the RBF kernel matrix is computed efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an actor network in a reinforcement learning context. The network consists of three linear layers with ReLU activations after the first two layers and a tanh activation at the output layer. The goal is to convert a given state vector into an action vector using the provided weights and biases.\n\n### Approach\n1. **Convert State to Numpy Array**: Ensure the input state is a numpy array for consistent operations.\n2. **Linear Transformation and ReLU Activation**: Compute the output of each linear layer followed by a ReLU activation. This involves matrix multiplication with the weights and addition of biases, followed by applying the ReLU function.\n3. **Final Linear Transformation and Tanh Activation**: The output of the second ReLU layer is passed through the final linear layer, followed by a tanh activation to ensure the action values are within the range [-1, 1].\n4. **Rounding**: Convert the resulting action vector to a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a numpy array if it's not already\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    \n    # First linear layer followed by ReLU\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    \n    # Second linear layer followed by ReLU\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    \n    # Third linear layer followed by tanh\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    \n    # Round each element to four decimal places and convert to list\n    return [round(x, 4) for x in action]\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted to a numpy array to facilitate matrix operations.\n2. **First Layer**: The state vector is multiplied by the first weight matrix `W1`, and the bias `b1` is added. The result is passed through the ReLU activation function.\n3. **Second Layer**: The output from the first layer is multiplied by the second weight matrix `W2`, and the bias `b2` is added. Another ReLU activation is applied.\n4. **Output Layer**: The output from the second layer is multiplied by the third weight matrix `W3`, and the bias `b3` is added. The result is passed through the tanh activation function to produce the action vector.\n5. **Rounding**: Each element of the action vector is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the network correctly processes the input state through the defined architecture, producing an action vector suitable for reinforcement learning applications."}
{"task_id": 157, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the classification accuracy score. This score is a common metric used to evaluate the performance of classification models. The function will compare the true class labels with the predicted class labels and return the proportion of correct predictions, rounded to four decimal places.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the true labels (`y_true`) and predicted labels (`y_pred`) are different. If they are, the function returns -1.\n2. **Handle Empty Containers**: If both containers are empty, the function returns 0, as there are no samples to evaluate.\n3. **Count Matches**: If the containers are valid (same length and non-empty), we count the number of correct predictions by comparing each corresponding element in `y_true` and `y_pred`.\n4. **Calculate Accuracy**: The accuracy is calculated as the ratio of correct predictions to the total number of samples. This value is then rounded to four decimal places and returned.\n\n### Solution Code\n```python\ndef accuracy_score(y_true, y_pred):\n    \"\"\"Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy or \u20111 according to the rules above.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return -1\n    if len(y_true) == 0:\n        return 0\n    matches = sum(1 for a, b in zip(y_true, y_pred) if a == b)\n    total = len(y_true)\n    accuracy = matches / total\n    return round(accuracy, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of `y_true` and `y_pred` are different. If they are, it returns -1 immediately.\n- **Handle Empty Containers**: If both containers are empty, the function returns 0, as there are no samples to evaluate.\n- **Count Matches**: Using a generator expression, the function iterates through each pair of elements from `y_true` and `y_pred`, counting how many times they are equal.\n- **Calculate Accuracy**: The number of correct predictions is divided by the total number of samples to get the accuracy. This value is then rounded to four decimal places and returned.\n\nThis approach ensures that the function efficiently handles all edge cases and correctly computes the accuracy score for valid inputs."}
{"task_id": 158, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the closed-form solution for Ridge Regression, also known as Tikhonov regularization. This method helps in preventing overfitting by adding a regularization term to the loss function. The goal is to find the weight vector \u03b2 that minimizes the given objective function.\n\n### Approach\n1. **Convert Input Data**: Convert the input feature matrix `X` and target vector `y` into numpy arrays for efficient computation.\n2. **Add Intercept Term**: If `fit_intercept` is True, augment the feature matrix `X` with a column of ones to include an intercept term in the model. This intercept term is not regularized.\n3. **Compute Matrices**: Calculate the matrix product `X^T X` and the regularization matrix `A`. The matrix `A` is a diagonal matrix where the first element is zero (if `fit_intercept` is True) and the rest are set to the regularization strength `alpha`.\n4. **Matrix Inversion**: Compute the inverse of the matrix `(X^T X + A)`.\n5. **Solve for \u03b2**: Use the inverse matrix to solve for the weight vector \u03b2 by multiplying it with the matrix product `X^T y`.\n6. **Round Results**: Round each element of the resulting weight vector to four decimal places and return it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    # Convert to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add intercept if needed\n    if fit_intercept:\n        intercept = np.ones((X_np.shape[0], 1))\n        X_np = np.hstack((intercept, X_np))\n    \n    # Compute X^T X\n    XtX = np.dot(X_np.T, X_np)\n    \n    # Compute A\n    n_features = X_np.shape[1]\n    A = alpha * np.eye(n_features)\n    if fit_intercept:\n        A[0, 0] = 0\n    \n    # Compute inverse of (XtX + A)\n    inv_matrix = np.linalg.inv(XtX + A)\n    \n    # Compute X^T y\n    Xty = np.dot(X_np.T, y_np)\n    \n    # Compute beta\n    beta = np.dot(inv_matrix, Xty)\n    \n    # Round to four decimals\n    beta_rounded = [round(x, 4) for x in beta]\n    \n    return beta_rounded\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: This step ensures that we can leverage numpy's efficient matrix operations for computations.\n- **Adding Intercept Term**: This step is crucial for including an intercept in the model, which helps in fitting the data better without regularizing the intercept.\n- **Matrix Computations**: The matrices `X^T X` and `A` are computed to form the regularized version of the least squares problem.\n- **Matrix Inversion**: The inverse of the matrix `(X^T X + A)` is computed to solve the linear system efficiently.\n- **Solving for \u03b2**: The weight vector \u03b2 is obtained by multiplying the inverse matrix with `X^T y`, giving the coefficients that minimize the regularized loss function.\n- **Rounding**: The results are rounded to four decimal places for precision and readability.\n\nThis approach ensures that we efficiently compute the Ridge Regression coefficients while handling the regularization correctly, especially when an intercept term is included."}
{"task_id": 159, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the log probability density of a multivariate Gaussian distribution. This involves using the formula for the log-likelihood of a data point under a multivariate normal distribution, which includes terms for the dimensionality, the determinant of the covariance matrix, and the quadratic form involving the covariance matrix and the sample vector.\n\n### Approach\n1. **Dimensionality Calculation**: Determine the dimensionality `d` from the sample vector `x_i`.\n2. **Logarithm of 2\u03c0**: Compute the term `d * ln(2\u03c0)` using numpy's logarithm function.\n3. **Log Determinant**: Use `numpy.linalg.slogdet` to compute the log determinant of the covariance matrix `sigma` for numerical stability.\n4. **Delta Calculation**: Compute the difference vector `delta` between the sample vector `x_i` and the mean vector `mu`.\n5. **Solving Linear System**: Use `numpy.linalg.solve` to solve the linear system `sigma @ y = delta` to avoid explicitly computing the inverse of `sigma`.\n6. **Quadratic Term**: Compute the quadratic term using the dot product of `delta` and the solution `y`.\n7. **Summing Terms**: Sum the computed terms and multiply by -0.5 to get the log probability.\n8. **Rounding**: Return the result rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n    \"\"\"Compute the log probability density of a multivariate Gaussian.\n\n    The function implements the formula\n        log N(x | \u00b5, \u03a3) = -0.5 * [ d * ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5) ].\n\n    Args:\n        x_i (np.ndarray): 1-D array representing the sample vector (shape (d,)).\n        mu (np.ndarray): 1-D array representing the mean vector (shape (d,)).\n        sigma (np.ndarray): 2-D array representing the covariance matrix (shape (d,d)).\n\n    Returns:\n        float: The log probability, rounded to 4 decimal places.\n    \"\"\"\n    d = x_i.shape[0]\n    term1 = d * np.log(2 * np.pi)\n    sign, logdet = np.linalg.slogdet(sigma)\n    term2 = logdet\n    delta = x_i - mu\n    y = np.linalg.solve(sigma, delta)\n    term3 = np.dot(delta, y)\n    total = term1 + term2 + term3\n    result = -0.5 * total\n    return round(result, 4)\n```\n\n### Explanation\n- **Dimensionality (d)**: This is the length of the sample vector `x_i` and is used to compute the first term in the log-likelihood formula.\n- **Logarithm of 2\u03c0**: This term accounts for the normalization factor in the Gaussian distribution.\n- **Log Determinant**: Using `slogdet` ensures numerical stability when dealing with the determinant of the covariance matrix.\n- **Delta Vector**: This vector represents the deviation of the sample from the mean.\n- **Solving Linear System**: This step efficiently computes the product of the inverse covariance matrix and the delta vector without explicitly forming the inverse.\n- **Quadratic Term**: This term measures how well the sample fits the distribution, scaled by the covariance matrix.\n- **Summing Terms**: Combining all terms and applying the formula gives the log probability, which is then rounded for the final result.\n\nThis approach ensures numerical stability and efficiency, making it suitable for high-dimensional data and large covariance matrices."}
{"task_id": 161, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that predicts binary class labels using a Factorization Machine (FM) model. The FM model efficiently captures both linear and pairwise feature interactions, making it suitable for tasks like binary classification.\n\n### Approach\nThe Factorization Machine (FM) model combines linear and pairwise feature interactions to make predictions. The prediction formula for a feature vector \\(\\mathbf{x}\\) is given by:\n\n\\[ s(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j \\]\n\nTo efficiently compute the pairwise interactions, we use a computational trick that reduces the complexity from \\(O(n^2k)\\) to \\(O(nk)\\). The trick involves transforming the pairwise interaction term into a more efficient form:\n\n\\[ \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i=1}^{n} x_i v_{i,f} \\right)^2 - \\sum_{i=1}^{n} x_i^2 v_{i,f}^2 \\right) \\]\n\nThe steps to compute the prediction are as follows:\n1. Compute the linear part of the prediction.\n2. Compute the pairwise interaction part using the trick.\n3. Combine the linear and pairwise parts to get the total score.\n4. Convert the scores into binary labels (1 or -1).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"Predict binary class labels using a Factorization Machine model.\"\"\"\n    # Compute the linear part\n    linear_part = w0 + np.dot(X, w)\n    \n    # Compute the pairwise interaction part using the computational trick\n    X_V = np.dot(X, V)\n    sum_X_V_sq = np.sum(X_V ** 2, axis=1)\n    \n    V_sq_sum = np.sum(V ** 2, axis=1)\n    X_squared = X ** 2\n    sum_X_squared_V_sq = np.dot(X_squared, V_sq_sum)\n    \n    pairwise = 0.5 * (sum_X_V_sq - sum_X_squared_V_sq)\n    \n    # Combine linear and pairwise parts to get the total score\n    total_score = linear_part + pairwise\n    \n    # Determine the binary labels\n    labels = np.where(total_score >= 0, 1, -1)\n    \n    return labels.tolist()\n```\n\n### Explanation\n1. **Linear Part Calculation**: The linear part of the prediction is computed using the dot product of the feature matrix \\(X\\) with the linear weights \\(w\\), plus the global bias \\(w0\\).\n\n2. **Pairwise Interaction Calculation**: \n   - Compute the product of \\(X\\) and the latent factor matrix \\(V\\).\n   - Calculate the sum of squares of each row in the resulting matrix to get the first part of the pairwise term.\n   - Compute the sum of squares of each row in \\(V\\) and use it to transform the feature matrix \\(X\\) into squared values.\n   - Combine these results to compute the pairwise interaction term efficiently.\n\n3. **Total Score and Labels**: The total score is the sum of the linear and pairwise parts. Binary labels are determined by checking if the total score is non-negative (label 1) or negative (label -1).\n\nThis approach ensures that the prediction is computed efficiently, making it suitable for large datasets with many features."}
{"task_id": 162, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the cross-entropy loss function and its gradient for a neural network. This function is widely used in multi-class classification problems where the model's predictions are passed through a softmax layer, ensuring each row sums to one.\n\n### Approach\n1. **Check Shapes**: Ensure the shapes of the predicted probabilities (Y_hat) and the one-hot encoded labels (Y) match. If they don't, raise an `AssertionError`.\n2. **Compute Loss**: The cross-entropy loss is calculated using the formula:\n   \\[\n   \\text{loss} = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{c=1}^{C} y_{ic}\\,\\log(\\hat y_{ic} + \\varepsilon)\n   \\]\n   where \\(m\\) is the batch size, \\(C\\) is the number of classes, and \\(\\varepsilon\\) is a small constant added for numerical stability.\n3. **Compute Gradient**: The gradient of the loss with respect to the softmax output is given by:\n   \\[\n   \\nabla_{\\hat Y} = \\frac{1}{m}(\\hat Y - Y)\n   \\]\n4. **Rounding**: Both the loss and each element of the gradient are rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        A tuple containing the scalar loss and the gradient (as a nested\n        Python list), both rounded to 4 decimal places.\n    \"\"\"\n    assert Y_hat.shape == Y.shape, \"Shapes of Y_hat and Y do not match.\"\n    \n    m = Y_hat.shape[0]\n    # Compute the loss\n    loss = - (1/m) * np.sum(Y * np.log(Y_hat + epsilon))\n    # Compute the gradient\n    gradient = (Y_hat - Y) / m\n    # Round the loss and gradient to 4 decimal places\n    loss = round(loss, 4)\n    gradient = np.round(gradient, 4)\n    # Convert gradient to a list of lists\n    gradient = gradient.tolist()\n    \n    return (loss, gradient)\n```\n\n### Explanation\n1. **Shape Check**: The function starts by ensuring the input arrays Y_hat and Y have the same shape, which is crucial for correct computation.\n2. **Loss Calculation**: The loss is computed using element-wise multiplication of Y and the logarithm of Y_hat (with a small epsilon added to avoid numerical issues). The result is summed and scaled by the batch size.\n3. **Gradient Calculation**: The gradient is derived from the difference between Y_hat and Y, scaled by the batch size.\n4. **Rounding and Conversion**: Both the loss and gradient are rounded to four decimal places for precision. The gradient, initially a numpy array, is converted to a list of lists to meet the return type requirements.\n\nThis approach ensures numerical stability, correct computation of loss and gradient, and proper handling of data types and precision."}
{"task_id": 163, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic Net regression from scratch for a single real-valued feature and an arbitrary polynomial degree. The task involves expanding the input feature into a polynomial feature matrix, fitting the model using either an analytical solution or gradient descent, and returning the predictions.\n\n### Approach\n1. **Polynomial Feature Expansion**: Convert the input feature list into a polynomial feature matrix up to the specified degree, including a bias term.\n2. **Cost Function**: The cost function for Elastic Net regression includes both L1 and L2 regularization terms. The L1 term encourages sparsity, while the L2 term helps prevent overfitting.\n3. **Analytical Solution for Ridge Regression**: When the L1 ratio (alpha) is 0, the problem reduces to Ridge regression, which can be solved using the normal equations with a Tikhonov regularization term.\n4. **Gradient Descent for Elastic Net**: When the L1 ratio is non-zero, use batch gradient descent to minimize the cost function. The gradient includes sub-gradients for the L1 term and gradients for the L2 term.\n5. **Predictions**: Once the model coefficients are determined, compute the predictions using the polynomial feature matrix and return them rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: list[float], y: list[float], degree: int, reg_factor: float, l1_ratio: float, learning_rate: float, n_iterations: int) -> list[float]:\n    \"\"\"Fit an Elastic Net regression model and return the predictions for X.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    n = len(X)\n    X_poly = np.vander(X, degree + 1, increasing=True)\n    \n    if l1_ratio == 0:\n        # Analytical solution for Ridge regression\n        XtX = X_poly.T.dot(X_poly)\n        Xty = X_poly.T.dot(y)\n        d = degree + 1\n        diag = np.zeros(d)\n        diag[1:] = n * reg_factor * (1 - l1_ratio) / 2\n        XtX_reg = XtX + np.diag(diag)\n        beta = np.linalg.inv(XtX_reg).dot(Xty)\n    else:\n        # Gradient descent for Elastic Net\n        beta = np.zeros(degree + 1)\n        for _ in"}
{"task_id": 166, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Perceptron algorithm from scratch to train a binary classifier and make predictions on test data. The Perceptron algorithm is a simple yet effective method for learning a linear decision boundary between two classes. The goal is to find a separating hyper-plane using the Perceptron update rule with a learning rate of 1.\n\n### Approach\n1. **Initialization**: Start with a weight vector `w` and a bias term `b`, both initialized to zero. The weight vector has the same number of features as the input data.\n2. **Training Loop**: Iterate over the training data for a maximum of `max_iter` times. In each iteration, process each training sample sequentially.\n3. **Margin Calculation**: For each sample, compute the signed margin `m` which is given by `y_i * (w\u00b7x_i + b)`. If the margin is less than or equal to zero, the sample is misclassified.\n4. **Update Rule**: If a sample is misclassified, update the weight vector and bias term using the Perceptron update rule: `w = w + y_i * x_i` and `b = b + y_i`.\n5. **Early Termination**: If an entire pass over the training data results in zero misclassifications, the algorithm has converged, and we terminate early.\n6. **Prediction**: After training, compute the raw scores for each test sample using the learned weights and bias. The prediction is the sign of this score, treating zero as +1.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int = 1000) -> list[int]:\n    \"\"\"Train a Perceptron on (X_train, y_train) and predict labels for X_test.\"\"\"\n    n_features = X_train.shape[1]\n    w = np.zeros(n_features)\n    b = 0.0\n\n    for _ in range(max_iter):\n        errors = 0\n        for i in range(len(X_train)):\n            x_i = X_train[i]\n            y_i = y_train[i]\n            margin = y_i * (np.dot(w, x_i) + b)\n            if margin <= 0:\n                w += y_i * x_i\n                b += y_i\n                errors += 1\n        if errors == 0:\n            break\n\n    predictions = []\n    for x in X_test:\n        s = np.dot(w, x) + b\n        if s >= 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    return predictions\n```\n\n### Explanation\n- **Initialization**: The weight vector `w` and bias `b` are initialized to zero to start the learning process.\n- **Training Loop**: The outer loop runs for up to `max_iter` iterations. Each iteration processes all training samples sequentially.\n- **Margin Calculation**: For each sample, the margin is computed to determine if the sample is correctly classified. If the margin is non-positive, the sample is misclassified.\n- **Update Rule**: The weight vector and bias are updated using the Perceptron rule to correct misclassifications.\n- **Early Termination**: If no misclassifications occur in an entire pass through the training data, the algorithm stops early as it has converged.\n- **Prediction**: After training, the model makes predictions on the test data by computing the dot product of each test sample with the learned weights and adding the bias. The sign of this result determines the predicted label, with zero treated as +1.\n\nThis approach ensures that the Perceptron algorithm is implemented efficiently and correctly, providing a clear and interpretable solution for binary classification tasks."}
{"task_id": 172, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the hyperbolic tangent (tanh) activation function. The derivative of tanh(z) is given by 1 - tanh(z)^2. The function should handle inputs that are scalars, Python lists, or NumPy arrays and return the result rounded to four decimal places.\n\n### Approach\n1. **Convert Input to NumPy Array**: Convert the input `z` into a NumPy array to leverage NumPy's element-wise operations. This allows us to handle scalars, lists, and NumPy arrays uniformly.\n2. **Compute the Derivative**: Using NumPy's `tanh` function, compute the tanh values of the input array. Then, compute the derivative using the formula 1 - tanh(z)^2.\n3. **Round the Result**: Round each element of the resulting derivative array to four decimal places.\n4. **Return Appropriate Type**: Check if the original input was a scalar. If it was, return the result as a float. Otherwise, return the result as a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef tanh_grad(z):\n    \"\"\"Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 - tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of floats/ints.\n\n    Returns:\n        float | list[float]: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n    \"\"\"\n    # Convert input to numpy array\n    z_np = np.asarray(z)\n    # Compute derivative\n    grad = 1 - np.tanh(z_np) ** 2\n    # Round to four decimals\n    grad_rounded = np.round(grad, 4)\n    # Check if original input was a scalar\n    if z_np.ndim == 0:\n        return grad_rounded.item()\n    else:\n        return grad_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input `z` is converted to a NumPy array using `np.asarray(z)`. This ensures that all subsequent operations can be performed element-wise, regardless of whether `z` is a scalar, list, or array.\n2. **Derivative Calculation**: Using NumPy's `tanh` function, we compute the tanh values of the input array. The derivative is then calculated as `1 - np.tanh(z_np) ** 2`.\n3. **Rounding**: The result is rounded to four decimal places using `np.round(grad, 4)`.\n4. **Return Type Handling**: If the original input `z` was a scalar (checked by examining the dimensionality of the NumPy array), the result is returned as a float. Otherwise, the result is converted to a Python list using `tolist()`.\n\nThis approach ensures that the function handles all specified input types correctly and returns the results in the required format."}
{"task_id": 173, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the alphabetical distance between two strings. The alphabetical distance is defined as the sum of the absolute differences of the ASCII values of corresponding characters in the two strings. If the strings are of different lengths, we return -1.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input strings are equal. If they are not, we return -1 immediately.\n2. **Calculate Distance**: If the lengths are equal, we initialize a variable to keep track of the total distance. We then iterate through each pair of corresponding characters from the two strings, compute the absolute difference of their ASCII values, and accumulate this difference into the total distance.\n3. **Return Result**: Finally, we return the total accumulated distance.\n\n### Solution Code\n```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        The sum of absolute ASCII differences between corresponding\n        characters. If the strings differ in length, return -1.\n    \"\"\"\n    if len(candidate) != len(target):\n        return -1\n    total = 0\n    for c, t in zip(candidate, target):\n        total += abs(ord(c) - ord(t))\n    return total\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the candidate and target strings are the same. If not, it returns -1.\n- **Iterate Through Characters**: Using the `zip` function, we pair each character from the candidate string with the corresponding character from the target string. For each pair, we compute the absolute difference of their ASCII values using the `ord` function and accumulate this value into the total distance.\n- **Return Total Distance**: After processing all character pairs, the total distance is returned.\n\nThis approach ensures that we efficiently compute the alphabetical distance while handling edge cases such as different string lengths and empty strings correctly."}
{"task_id": 175, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the L2 regularization penalty for a given set of weights. The L2 penalty is a common technique used in machine learning to prevent overfitting by adding a penalty term to the loss function, which discourages overly complex models.\n\n### Approach\nThe L2 regularization penalty is defined as the product of a non-negative regularization strength \\( C \\) and the sum of the squares of the weights. The steps to compute this penalty are as follows:\n\n1. **Convert Weights to NumPy Array**: The input weights can be either a Python list or a 1-D NumPy array. Converting them to a NumPy array allows us to use efficient array operations.\n2. **Compute Sum of Squares**: Calculate the sum of the squares of each weight using NumPy's array operations, which are optimized for performance.\n3. **Calculate Penalty**: Multiply the sum of squares by the regularization strength \\( C \\) to get the penalty.\n4. **Round the Result**: Return the penalty value rounded to four decimal places to meet the specified precision requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n    \"\"\"Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: The penalty value rounded to the nearest 4th decimal place.\n    \"\"\"\n    weights_array = np.array(weights)\n    sum_squares = np.sum(weights_array ** 2)\n    penalty = C * sum_squares\n    return round(penalty, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The function first converts the input weights into a NumPy array. This allows us to leverage NumPy's efficient operations for computing the sum of squares.\n- **Sum of Squares Calculation**: Using NumPy's element-wise exponentiation and summation, we efficiently compute the sum of the squares of the weights.\n- **Penalty Calculation**: The sum of squares is then multiplied by the regularization strength \\( C \\) to get the penalty.\n- **Rounding**: The result is rounded to four decimal places to ensure the required precision.\n\nThis approach ensures that the function handles both lists and NumPy arrays efficiently, computes the penalty accurately, and returns the result in the required format."}
{"task_id": 179, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that tokenizes a given text string into individual characters after performing certain preprocessing steps. The function should handle optional preprocessing steps such as converting the text to lowercase and removing punctuation, followed by processing spaces to collapse consecutive spaces and remove leading/trailing spaces.\n\n### Approach\n1. **Handle None Input**: If the input string is `None`, return an empty list immediately.\n2. **Lowercase Conversion**: Convert the entire string to lowercase if the `lowercase` flag is set to `True`.\n3. **Punctuation Removal**: Remove all standard ASCII punctuation marks from the string if the `filter_punctuation` flag is set to `True`.\n4. **Space Processing**: Collapse runs of two or more consecutive spaces into a single space and remove any leading or trailing spaces.\n5. **Return Result**: If the resulting string is empty after preprocessing, return an empty list. Otherwise, split the string into individual characters and return them as a list.\n\n### Solution Code\n```python\nimport re\nimport string\n\ndef tokenize_chars(line: str, lowercase: bool = True, filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    if line is None:\n        return []\n    \n    if lowercase:\n        line = line.lower()\n    \n    if filter_punctuation:\n        pattern = r'[{}]'.format(re.escape(string.punctuation))\n        line = re.sub(pattern, '', line)\n    \n    # Collapse runs of two or more spaces into a single space\n    line = re.sub(r' {2,}', ' ', line)\n    # Strip leading and trailing spaces\n    line = line.strip()\n    \n    if not line:\n        return []\n    \n    return list(line)\n```\n\n### Explanation\n1. **None Check**: The function first checks if the input string is `None` and returns an empty list if true.\n2. **Lowercase Conversion**: If the `lowercase` flag is `True`, the string is converted to lowercase using the `lower()` method.\n3. **Punctuation Removal**: If the `filter_punctuation` flag is `True`, the function uses a regular expression to remove all characters present in `string.punctuation`.\n4. **Space Processing**: The function uses a regular expression to replace runs of two or more spaces with a single space and then strips any leading or trailing spaces from the string.\n5. **Result Handling**: If the resulting string is empty after preprocessing, an empty list is returned. Otherwise, the string is split into individual characters, and the list of these characters is returned.\n\nThis approach ensures that the text is processed efficiently and correctly according to the specified preprocessing steps, resulting in a list of individual characters as required."}
{"task_id": 183, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a weight tensor using the He uniform initialization method, which is particularly effective for layers followed by ReLU activation. The initialization method ensures that the weights are scaled appropriately to maintain consistent gradient magnitudes throughout the network.\n\n### Approach\n1. **Determine Layer Type**: The function first checks the length of the weight shape to determine if it is a dense layer (2D) or a convolutional layer (4D).\n2. **Compute Fan-In**: For a dense layer, the fan-in is the first dimension of the weight shape. For a convolutional layer, the fan-in is the product of the first three dimensions (spatial size and input depth).\n3. **Calculate Limit**: The limit for the uniform distribution is computed as the square root of 6 divided by the fan-in.\n4. **Generate Weights**: Using the calculated limit, random values are generated from a uniform distribution scaled to the interval [-limit, limit].\n\n### Solution Code\n```python\nimport numpy as np\n\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    # Determine the type of layer based on the shape length\n    if len(weight_shape) == 2:\n        # Dense layer: fan_in is the first dimension\n        fan_in = weight_shape[0]\n    elif len(weight_shape) == 4:\n        # Convolutional layer: fan_in is product of first three dimensions\n        fan_in = weight_shape[0] * weight_shape[1] * weight_shape[2]\n    else:\n        # According to the problem statement, this case won't be tested\n        raise ValueError(\"Weight shape must be 2D or 4D.\")\n    \n    # Calculate the limit for uniform distribution\n    limit = np.sqrt(6.0 / fan_in)\n    \n    # Generate random values from uniform distribution in [-limit, limit]\n    weights = np.random.rand(*weight_shape) * (2 * limit) - limit\n    \n    return weights\n```\n\n### Explanation\n- **Layer Type Determination**: The function checks if the weight shape is 2D or 4D to decide if it's a dense or convolutional layer.\n- **Fan-In Calculation**: For dense layers, fan-in is the number of input features. For convolutional layers, it's the product of the filter dimensions and the number of input channels.\n- **Limit Calculation**: The limit is derived from the He uniform initialization formula, ensuring that the weights are scaled appropriately.\n- **Weight Generation**: Using NumPy's random number generator, the function creates an array of the specified shape with values uniformly distributed between -limit and limit.\n\nThis approach ensures that the weights are initialized in a way that facilitates effective gradient flow during training, especially when using ReLU activation functions."}
{"task_id": 185, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the shape of a given feature matrix `X` and return the number of samples and features as a tuple. The solution must handle various cases, including converting array-like objects to NumPy arrays, checking for empty matrices, and correctly interpreting the dimensions of the input data.\n\n### Approach\n1. **Convert to NumPy Array**: Convert the input `X` to a NumPy array if it is not already one. This ensures consistent handling of different array-like objects.\n2. **Check for Empty Matrix**: If the resulting array is empty (i.e., has zero elements), raise a `ValueError`.\n3. **Handle Different Dimensions**:\n   - **0-Dimensional Array**: Treat a 0-dimensional array as a single sample with one feature.\n   - **1-Dimensional Array**: Treat a 1-dimensional array as a single sample with the number of features equal to the length of the array.\n   - **2-Dimensional and Higher Arrays**: For arrays with two or more dimensions, the first dimension represents the number of samples, and the product of the remaining dimensions gives the number of features.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dataset_shape(X):\n    \"\"\"Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n    \"\"\"\n    X = np.asarray(X)\n    if X.size == 0:\n        raise ValueError(\"Got an empty matrix.\")\n    if X.ndim == 0:\n        return (1, 1)\n    elif X.ndim == 1:\n        return (1, X.shape[0])\n    else:\n        n_samples = X.shape[0]\n        n_features = np.prod(X.shape[1:])\n        return (n_samples, n_features)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The function starts by converting the input `X` into a NumPy array using `np.asarray(X)`. This ensures that the input is handled consistently regardless of its original type (e.g., list, pandas DataFrame).\n2. **Empty Matrix Check**: The function checks if the array is empty by verifying if `X.size` is zero. If it is, a `ValueError` is raised with the message \"Got an empty matrix.\"\n3. **Dimension Handling**:\n   - For a 0-dimensional array, the function treats it as a single sample with one feature, returning `(1, 1)`.\n   - For a 1-dimensional array, the function treats it as a single sample with the number of features equal to the length of the array, returning `(1, len(X))`.\n   - For arrays with two or more dimensions, the function extracts the number of samples from the first dimension and computes the number of features as the product of the remaining dimensions, returning `(n_samples, n_features)`.\n\nThis approach ensures that the function correctly handles various input types and dimensions, providing the required shape information efficiently."}
{"task_id": 186, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a baseline regressor that always predicts the mean of the training targets for each test sample. This provides a simple reference score that any reasonable model should beat.\n\n### Approach\nThe approach involves the following steps:\n1. **Compute the Mean of Training Targets**: Calculate the arithmetic mean of the training target vector `y_train`.\n2. **Handle Empty Test Data**: If the test feature matrix `X_test` is empty, return an empty list immediately.\n3. **Generate Predictions**: For each row in `X_test`, predict the mean value computed from `y_train`.\n4. **Round Predictions**: Round each predicted value to four decimal places.\n5. **Return as List**: Convert the predictions into a list of floats and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n\n    Your task is to complete this function so that it returns the arithmetic\n    mean of *y_train* for every sample in *X_test*.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (not used in the computation).\n    y_train : array-like, shape (n_samples,)\n        Target values corresponding to *X_train*.\n    X_test : array-like, shape (m_samples, n_features)\n        Test samples for which predictions are required.\n\n    Returns\n    -------\n    list of float\n        A list with *m_samples* elements where every element equals the mean\n"}
{"task_id": 188, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an exponential decay scheduler for the exploration rate (epsilon) in a reinforcement learning algorithm. The goal is to ensure that the exploration rate starts at a high value and gradually decreases over time, while never falling below a specified minimum value. This scheduler is crucial for balancing exploration and exploitation in the learning process.\n\n### Approach\n1. **Input Validation**: The function first checks if the input parameters are valid. Specifically, it ensures that the minimum exploration rate is less than the maximum, both are non-negative, the decay rate is positive, and the step count is non-negative. If any of these conditions are violated, the function returns -1.\n\n2. **Exponential Decay Calculation**: Using the formula provided, the function computes the current exploration rate at a given step. The formula used is:\n   \\[\n   \\epsilon(t) = \\epsilon_{\\text{min}} + (\\epsilon_{\\text{max}} - \\epsilon_{\\text{min}}) \\cdot e^{-\\lambda \\cdot t}\n   \\]\n   where \\( \\epsilon_{\\text{max}} \\) is the initial exploration rate, \\( \\epsilon_{\\text{min}} \\) is the minimum exploration rate, \\( \\lambda \\) is the decay rate, and \\( t \\) is the current step.\n\n3. **Clipping the Result**: To ensure numerical stability, the computed exploration rate is clipped to be at least \\( \\epsilon_{\\text{min}} \\). This step is a safeguard against any potential numerical underflow issues.\n\n4. **Rounding**: The result is rounded to four decimal places to meet the precision requirement.\n\n### Solution Code\n```python\nimport math\n\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    The function must implement the formula\n        \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n    and respect the constraints described in the task description.\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: The exploration rate \u03b5(t) rounded to 4 decimal places, or \u22121 if\n               the input parameters are invalid.\n    \"\"\"\n    # Validate input parameters\n    if min_epsilon >= max_epsilon or min_epsilon < 0 or decay_rate <= 0 or step < 0:\n        return -1\n    \n    # Calculate the exponential term\n    exponent = -decay_rate * step\n    exp_term = math.exp(exponent)\n    \n    # Compute epsilon\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * exp_term\n    \n    # Ensure epsilon does not fall below min_epsilon\n    epsilon = max(epsilon, min_epsilon)\n    \n    # Round to 4 decimal places\n    return round(epsilon, 4)\n```\n\n### Explanation\n- **Input Validation**: The function checks if the inputs are within the valid range. If any input is invalid, it returns -1 immediately.\n- **Exponential Decay Calculation**: The function computes the decayed exploration rate using the given formula, ensuring that the rate decreases exponentially over time.\n- **Clipping**: The result is clipped to ensure it never falls below the minimum exploration rate, providing a numerical safeguard.\n- **Rounding**: The result is rounded to four decimal places to meet the precision requirement, ensuring the output is user-friendly and consistent.\n\nThis approach ensures that the exploration rate starts high, decreases over time, and never falls below a specified minimum, providing a balanced approach to exploration and exploitation in reinforcement learning."}
{"task_id": 193, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth alternative to the hyperbolic tangent function. The Softsign function is defined as Softsign(z) = z / (1 + |z|). The function should handle both scalar inputs and array-like inputs (lists or NumPy arrays) and return the results rounded to four decimal places.\n\n### Approach\n1. **Scalar Input Handling**: If the input is a single real number (int or float), compute the Softsign value and return it as a float rounded to four decimal places.\n2. **Array-like Input Handling**: If the input is a list or a NumPy array, compute the Softsign value for each element. For NumPy arrays, use vectorized operations for efficiency, then convert the result to a list of floats, each rounded to four decimal places.\n3. **Edge Cases**: Handle empty inputs by returning an empty list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softsign(z):\n"}
{"task_id": 199, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a label vector `y` into two parts based on a threshold value applied to a feature vector `X`. The goal is to separate the labels into two groups: those corresponding to features in `X` that are strictly smaller than the threshold, and those that are greater than or equal to the threshold.\n\n### Approach\n1. **Problem Analysis**: We are given two NumPy arrays, `X` (feature values) and `y` (labels), of the same length. We need to split `y` into two lists: `y_left` for labels where the corresponding feature in `X` is less than a given threshold, and `y_right` for labels where the feature is greater than or equal to the threshold.\n\n2. **Intuition**: Using boolean indexing in NumPy, we can efficiently create a mask where each element is `True` if the corresponding feature in `X` is less than the threshold. This mask can then be used to extract the corresponding labels from `y` for `y_left`. The inverse of this mask will give us the labels for `y_right`.\n\n3. **Algorithm Selection**: The use of boolean indexing is optimal here because it allows us to directly index the arrays based on the condition, making the solution both efficient and concise.\n\n4. **Complexity Analysis**: The solution involves creating a boolean mask and using it to index the arrays, both of which are O(n) operations where n is the length of the arrays. Thus, the overall time complexity is O(n).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n        * y_left  contains labels with `X < value`.\n        * y_right contains labels with `X >= value`.\n    \"\"\"\n    mask = X < value\n    y_left = y[mask].tolist()\n    y_right = y[~mask].tolist()\n    return (y_left, y_right)\n```\n\n### Explanation\n1. **Boolean Mask Creation**: The mask is created using `X < value`, which results in a boolean array where each element is `True` if the corresponding element in `X` is less than `value`.\n\n2. **Extracting Labels**: Using the boolean mask, `y_left` is extracted from `y` where the mask is `True`. The inverse of the mask (`~mask`) is used to extract `y_right` from `y`.\n\n3. **Conversion to Lists**: The resulting NumPy arrays from the boolean indexing are converted to Python lists using the `tolist()` method to meet the problem's requirement of returning lists.\n\nThis approach efficiently splits the labels based on the threshold and ensures the original order of labels is preserved."}
{"task_id": 200, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the L1 regularization penalty for a given set of model weights. The L1 penalty is calculated as the sum of the absolute values of all the weights, scaled by a regularization strength parameter. This helps in preventing overfitting by discouraging large weights.\n\n### Approach\n1. **Convert Weights to NumPy Array**: Convert the input weights, which can be a nested list or a NumPy array, into a NumPy array. This allows us to easily handle and manipulate the weights regardless of their initial structure.\n2. **Check for Empty Weights**: If the converted NumPy array is empty, return 0.0 immediately as there are no weights to compute the penalty for.\n3. **Compute Absolute Values Sum**: Calculate the sum of the absolute values of all elements in the weights array. This is done using NumPy's `np.abs` and `np.sum` functions.\n4. **Apply Regularization Strength**: Multiply the computed sum by the regularization strength parameter `C` to get the L1 penalty.\n5. **Round the Result**: Round the resulting penalty to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_l1_penalty(weights, C):\n    \"\"\"Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n    \"\"\"\n    weights_array = np.array(weights)\n    if weights_array.size == 0:\n        return 0.0\n    sum_abs = np.sum(np.abs(weights_array))\n    l1_penalty = C * sum_abs\n    return round(l1_penalty, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Array**: The input weights are converted to a NumPy array to handle both nested lists and NumPy arrays uniformly.\n- **Empty Check**: If the array is empty, the function returns 0.0 immediately, ensuring no unnecessary computations.\n- **Sum of Absolute Values**: Using `np.abs` and `np.sum`, we efficiently compute the sum of absolute values of all weights.\n- **Regularization Application**: The sum is scaled by the regularization strength `C` to compute the penalty.\n- **Rounding**: The result is rounded to four decimal places to meet the precision requirement.\n\nThis approach ensures that the function handles various input structures efficiently and computes the L1 penalty accurately."}
{"task_id": 205, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to extract unique environment IDs from a given registry. The registry is represented as a list, where each item may or may not be a dictionary containing an \"id\" key. Our task is to collect these IDs, ensuring each ID appears only once and in the order of their first occurrence.\n\n### Approach\n1. **Initialization**: We start by initializing an empty list to store the unique IDs in the order they first appear and an empty set to keep track of the IDs we have already encountered.\n2. **Iteration**: We iterate through each item in the registry. For each item, we check if it is a dictionary and if it contains the key \"id\".\n3. **Extraction and Tracking**: If an item meets the criteria (being a dictionary with an \"id\" key), we extract the ID. We then check if this ID has already been encountered using our set. If it hasn't, we add it to both our result list and the set.\n4. **Return Result**: After processing all items in the registry, we return the list of unique IDs.\n\nThis approach ensures that we efficiently track and collect unique IDs while maintaining their order of first appearance.\n\n### Solution Code\n```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    seen = set()\n    result = []\n    for item in registry:\n        if isinstance(item, dict) and 'id' in item:\n            env_id = item['id']\n            if env_id not in seen:\n                seen.add(env_id)\n                result.append(env_id)\n    return result\n```\n\n### Explanation\n- **Initialization**: `seen` is a set to track encountered IDs, and `result` is a list to store the unique IDs in order.\n- **Iteration**: For each item in the registry, we check if it is a dictionary and contains the \"id\" key.\n- **Extraction and Tracking**: If the ID is new (not in `seen`), it is added to both `seen` and `result`.\n- **Efficiency**: Using a set for tracking ensures that checking for existence is average O(1), making the overall algorithm efficient with a time complexity of O(n), where n is the number of items in the registry.\n\nThis solution effectively handles the extraction of unique IDs while preserving their order of first appearance, adhering to the problem constraints and requirements."}
{"task_id": 206, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the Mean Absolute Error (MAE) between two sequences of numerical values. The function should handle various input types, validate the inputs, and efficiently compute the MAE using NumPy operations.\n\n### Approach\n1. **Input Conversion**: Convert the input sequences into NumPy arrays to leverage efficient numerical operations.\n2. **Dimensionality Check**: Ensure both input arrays are one-dimensional. If not, return -1.\n3. **Length Check**: Verify that both arrays have the same length. If not, return -1.\n4. **Empty Check**: If either array is empty, return -1 as MAE cannot be computed for empty sequences.\n5. **MAE Calculation**: Compute the absolute differences between corresponding elements, then calculate the mean of these differences.\n6. **Rounding**: Round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length or are not one-dimensional, return -1.\n    \"\"\"\n    try:\n        actual_arr = np.array(actual)\n        predicted_arr = np.array(predicted)\n    except:\n        return -1\n\n    if actual_arr.ndim != 1 or predicted_arr.ndim != 1:\n        return -1\n\n    if len(actual_arr) != len(predicted_arr):\n        return -1\n\n    if len(actual_arr) == 0:\n        return -1\n\n    try:\n        mae = np.mean(np.abs(actual_arr - predicted_arr))\n    except:\n        return -1\n\n    return round(mae, 4)\n```\n\n### Explanation\n1. **Input Conversion**: The function starts by converting the input sequences into NumPy arrays. This allows the use of efficient vectorized operations for calculations.\n2. **Dimensionality Check**: The function checks if both arrays are one-dimensional using the `ndim` attribute. If either array is not one-dimensional, the function returns -1.\n3. **Length Check**: The function ensures both arrays have the same length. If they do not, it returns -1.\n4. **Empty Check**: If either array is empty, the function returns -1 as MAE cannot be computed for empty sequences.\n5. **MAE Calculation**: Using NumPy's vectorized operations, the function computes the absolute differences between corresponding elements of the two arrays. It then calculates the mean of these differences to get the MAE.\n6. **Rounding**: The computed MAE is rounded to four decimal places using Python's `round` function and returned as a float.\n\nThis approach ensures that the function is efficient, handles various input types, and validates inputs correctly before performing the MAE calculation."}
{"task_id": 211, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the classification accuracy between two sequences of labels. The classification accuracy is the proportion of correctly predicted labels, and it is closely related to the classification error, which is the proportion of misclassified samples. The task is to write a function that computes this accuracy, ensuring the inputs are valid and handling edge cases appropriately.\n\n### Approach\n1. **Input Validation**: Check if the input sequences are of the same length and not empty. If they are not, return -1.0.\n2. **Compute Mismatches**: Count the number of mismatches between the actual and predicted labels.\n3. **Calculate Classification Error**: The classification error is the number of mismatches divided by the total number of samples.\n4. **Compute Accuracy**: The accuracy is 1 minus the classification error.\n5. **Round the Result**: Return the accuracy rounded to four decimal places.\n\n### Solution Code\n```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1.0 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1.0\n    total = len(actual)\n    mismatches = sum(1 for a, p in zip(actual, predicted) if a != p)\n    error = mismatches / total\n    acc = 1 - error\n    return round(acc, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of the actual and predicted sequences are equal and that they are not empty. If either condition is not met, it returns -1.0.\n2. **Counting Mismatches**: Using a generator expression, the function counts how many times the actual and predicted labels do not match.\n3. **Classification Error**: The error is calculated by dividing the number of mismatches by the total number of samples.\n4. **Accuracy Calculation**: The accuracy is derived by subtracting the classification error from 1.\n5. **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement.\n\nThis approach ensures that the function handles edge cases efficiently and computes the classification accuracy accurately."}
{"task_id": 214, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the Mean Squared Error (MSE) between two numerical sequences. The function should handle both Python lists and NumPy arrays, and it must return -1 if the input sequences have different lengths or are empty.\n\n### Approach\n1. **Check Lengths**: First, we check if the two input sequences have the same length. If they do not, the function returns -1.\n2. **Handle Empty Sequences**: If both sequences are empty (length zero), the function also returns -1 since MSE cannot be computed for empty sequences.\n3. **Convert to NumPy Arrays**: Convert the input sequences into NumPy arrays to leverage vectorized operations for efficient computation.\n4. **Compute Squared Differences**: Calculate the squared differences between corresponding elements of the two arrays.\n5. **Sum and Average**: Sum the squared differences and divide by the number of elements to get the MSE.\n6. **Round the Result**: Round the computed MSE to 4 decimal places using NumPy's rounding function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mean_squared_error(actual, predicted):\n    \"\"\"Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function must return -1 if the two input sequences do not share the\n    same length. Otherwise, it should return the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays have to be supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    n = len(actual)\n    if n == 0:\n        return -1\n    a = np.array(actual)\n    p = np.array(predicted)\n    squared_diff = (a - p) ** 2\n    sum_squared = np.sum(squared_diff)\n    mse = sum_squared / n\n    return np.round(mse, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function starts by comparing the lengths of the two input sequences. If they differ, it returns -1 immediately.\n- **Handle Empty Sequences**: If both sequences are empty, the function returns -1 to avoid division by zero and handle the undefined case.\n- **Convert to NumPy Arrays**: Converting the inputs to NumPy arrays allows us to use efficient vectorized operations for computing the squared differences.\n- **Compute Squared Differences**: Using element-wise subtraction and squaring, we compute the squared differences between corresponding elements of the two arrays.\n- **Sum and Average**: The sum of squared differences is computed, and then divided by the number of elements to get the mean, which is the MSE.\n- **Round the Result**: The result is rounded to 4 decimal places to meet the problem's requirements.\n\nThis approach ensures that the function is efficient, handles both lists and NumPy arrays, and correctly handles edge cases such as empty sequences."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability obtained by applying the sigmoid transformation. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input lists, tuples, or NumPy arrays into NumPy arrays for efficient vectorized computations.\n2. **Compute the Gradient**: The gradient for each observation is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the term `-actual * predicted`, applying the sigmoid function to this term, and then multiplying by the actual values.\n3. **Compute the Probability and Hessian**: The probability is obtained by applying the sigmoid function to the predicted values. The Hessian is then computed as the product of the probability and its complement (1 - probability).\n4. **Rounding**: Each computed value (gradient, Hessian, and probability) is rounded to exactly six decimal places to meet the precision requirement.\n5. **Convert Results to Lists**: The NumPy arrays are converted back to Python lists before returning the results.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute the term for the gradient\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    \n    # Compute the probability and Hessian\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    \n    # Round each value to 6 decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert NumPy arrays to Python lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the application of the sigmoid function.\n- **Probability and Hessian**: The probability is derived from the sigmoid function applied to the predicted values. The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Efficiency**: Using NumPy arrays allows for efficient vectorized operations, which is crucial for handling large datasets.\n- **Precision**: Each result is rounded to six decimal places to ensure the required precision, and the results are converted back to Python lists for compatibility with the expected output format."}
{"task_id": 220, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to estimate the word-topic (\u03d5) and document-topic (\u03b8) distributions for a Latent Dirichlet Allocation (LDA) model using symmetric Dirichlet smoothing. The goal is to compute these distributions from given count matrices and hyper-parameters, and return them as rounded nested lists.\n\n### Approach\n1. **Problem Analysis**:\n   - We are given two count matrices: `C_wt` (word-topic counts) and `C_dt` (document-topic counts).\n   - We need to compute the probability distributions"}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is commonly used in neural networks to avoid the issue of \"dying\" neurons. The function should handle various input types, including scalars, nested lists, and numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For any input value \\( x \\geq 0 \\), the output is \\( x \\).\n- For any input value \\( x < 0 \\), the output is \\( a \\times x \\), where \\( a \\) is a parameter in the range [0, 1).\n\nThe function must handle different input types:\n1. **Scalars**: Directly apply the Leaky ReLU transformation.\n2. **Lists (possibly nested)**: Recursively process each element to apply the transformation.\n3. **Numpy Arrays**: Use vectorized operations for efficient computation and then convert the result to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n1. **Handling Numpy Arrays**: If the input `z` is a numpy array, we use `np.where` to apply the Leaky ReLU transformation element-wise. This is efficient and leverages numpy's vectorized operations. The result is then converted to a Python list using `tolist()` to maintain the required output format.\n\n2. **Handling Lists**: If the input `z` is a list, we recursively process each element. This ensures that nested lists are handled correctly, maintaining the structure of the input.\n\n3. **Handling Scalars**: If the input `z` is a scalar, we directly apply the Leaky ReLU transformation. If `z` is non-negative, it remains unchanged; otherwise, it is multiplied by the parameter `a`.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 225, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given set of frequencies from Hertz (Hz) to the mel scale using either the HTK or Slaney formula. The function should handle both one-dimensional and multi-dimensional inputs and return the results as a Python list, maintaining the original shape of the input.\n\n### Approach\n1. **Input Validation**: Check if the provided formula is either \"htk\" or \"slaney\". If not, raise an `AssertionError`.\n2. **Conversion to Numpy Array**: Convert the input to a numpy array to facilitate element-wise operations.\n3. **Mel Scale Conversion**: Depending on the formula, compute the mel values using the respective mathematical operations:\n   - **HTK Formula**: `mel = 2595 * log10(1 + hz / 700)`\n   - **Slaney Formula**: `mel = 1127 * ln(1 + hz / 700)`\n4. **Rounding**: Round the computed mel values to four decimal places.\n5. **Conversion to List**: Convert the resulting numpy array back to a Python list, ensuring the output maintains the original shape of the input. Handle scalar inputs by returning a single-element list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str = \"htk\") -> list:\n    \"\"\"Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula : {\"htk\", \"slaney\"}, optional\n        Conversion formula to use. \"htk\" corresponds to the formula used in\n        the Hidden Markov Model Toolkit, while \"slaney\" corresponds to the\n        formula used in Malcolm Slaney\u2019s Auditory Toolbox. Default is\n        \"htk\".\n\n    Returns\n    -------\n    list of float\n        Mel-scaled frequencies with the same shape as *hz*, rounded to four\n        decimal places.\n    \"\"\"\n    if formula not in ['htk', 'slaney']:\n        raise AssertionError(\"formula must be 'htk' or 'slaney'\")\n    \n    hz_array = np.asarray(hz)\n    \n    if formula == 'htk':\n        mel = 2595 * np.log10(1 + hz_array / 700)\n    else:\n        mel = 1127 * np.log(1 + hz_array / 700)\n    \n    mel_rounded = np.round(mel, decimals=4)\n    \n    if hz_array.shape == ():\n        return [mel_rounded.item()]\n    else:\n        return mel_rounded.tolist()\n```\n\n### Explanation\n- **Input Validation**: Ensures that only valid formulas are used for conversion.\n- **Numpy Array Conversion**: Facilitates efficient element-wise operations on the input data.\n- **Mel Scale Conversion**: Uses the appropriate mathematical formula based on the specified method.\n- **Rounding**: Ensures the results are precise to four decimal places.\n- **List Conversion**: Converts the result back to a Python list, handling both scalar and multi-dimensional inputs appropriately.\n\nThis approach ensures that the function is both efficient and versatile, capable of handling various input shapes and providing accurate mel scale conversions."}
{"task_id": 228, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the pairwise Euclidean (L2) distances between each row of two sets of vectors. If the second set of vectors (Y) is not provided, we compute the distances within the first set (X) itself. The result should be a matrix where each element represents the distance between corresponding vectors, rounded to four decimal places.\n\n### Approach\n1. **Problem Analysis**: The task involves calculating the Euclidean distance between each pair of vectors from two sets. The Euclidean distance between two vectors can be efficiently computed using vectorized operations to avoid explicit loops, which is crucial for performance with large datasets.\n\n2. **Vectorized Calculation**: The Euclidean distance formula can be optimized using the identity:\n   \\[\n   \\|x - y\\|^2 = \\|x\\|^2 + \\|y\\|^2 - 2x^Ty\n   \\]\n   This allows us to compute the squared distances efficiently using matrix operations.\n\n3. **Steps**:\n   - **Handle Default Case**: If Y is None, set Y to X to compute distances within X.\n   - **Compute Norms**: Calculate the squared norms of each row in X and Y.\n   - **Dot Product**: Compute the dot product matrix between rows of X and Y.\n   - **Squared Distances**: Use the identity to compute the squared distances matrix.\n   - **Ensure Non-Negative**: Clip any negative values in the squared distances matrix to avoid NaNs when taking square roots.\n   - **Square Root and Rounding**: Compute the square roots of the squared distances and round to four decimal places.\n   - **Convert to List**: Convert the resulting NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Compute the pairwise Euclidean (L2) distances between each row of X and each row of Y.\n\n    If *Y* is ``None`` the function should compute the distances inside *X* itself.\n    All distances must be rounded to four decimal places and the function must\n    return a plain Python nested list (use ``tolist()`` if needed).\n\n    Args:\n        X: NumPy array with shape (N, C).\n        Y: NumPy array with shape (M, C) or ``None``.  Default is ``None``.\n\n    Returns:\n        Nested list representing the N \u00d7 M matrix of Euclidean distances.\n    \"\"\"\n    if Y is None:\n        Y = X\n    x_norms = np.sum(X ** 2, axis=1)\n    y_norms = np.sum(Y ** 2, axis=1)\n    dot_product = X @ Y.T\n    D_sq = x_norms[:, np.newaxis] + y_norms[np.newaxis, :] - 2 * dot_product\n    D_sq = np.maximum(D_sq, 0)  # Ensure non-negative to avoid NaNs\n    D = np.sqrt(D_sq)\n    D_rounded = np.round(D, 4)\n    return D_rounded.tolist()\n```\n\n### Explanation\n- **Vectorized Operations**: The use of vectorized operations ensures that the solution is efficient and avoids the overhead of explicit loops, making it suitable for large datasets.\n- **Efficiency**: The approach leverages matrix multiplication and broadcasting to compute distances in an optimized manner, reducing the time complexity significantly compared to a naive implementation.\n- **Handling Edge Cases**: The solution correctly handles cases where Y is None by treating it as X, ensuring the function works for both within-set and between-set distance calculations.\n\nThis approach ensures that the solution is both efficient and correct, providing the required output in the specified format."}
{"task_id": 231, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given hand of cards in the game of Blackjack is a natural blackjack. A natural blackjack is defined as a hand containing exactly two cards: one Ace (represented as 1) and one ten-valued card (represented as 10).\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Length of the Hand**: The hand must contain exactly two cards. If the length of the hand is not two, it cannot be a natural blackjack.\n2. **Count the Occurrences of Ace and Ten-Valued Cards**: The hand must contain exactly one Ace (1) and one ten-valued card (10). We can achieve this by counting the occurrences of 1 and 10 in the hand.\n\nBy following these steps, we can efficiently determine if the hand is a natural blackjack.\n\n### Solution Code\n```python\ndef is_natural(hand: list[int]) -> bool:\n    \"\"\"Determine if the given hand is a natural blackjack.\n    \n    A natural blackjack is a hand with exactly two cards: one Ace (1) and one ten-valued card (10).\n    \"\"\"\n    if len(hand) != 2:\n        return False\n    return hand.count(1) == 1 and hand.count(10) == 1\n```\n\n### Explanation\n1. **Check the Length**: The function first checks if the length of the hand is exactly two. If not, it immediately returns `False`.\n2. **Count Occurrences**: The function then counts how many times 1 (Ace) and 10 (ten-valued card) appear in the hand. If both counts are exactly one, the function returns `True`, indicating a natural blackjack. Otherwise, it returns `False`.\n\nThis approach ensures that we efficiently and accurately determine if the hand meets the criteria for a natural blackjack."}
{"task_id": 232, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given NumPy array is a valid one-hot matrix. A valid one-hot matrix must satisfy three conditions: it must be two-dimensional, every element must be either 0 or 1, and each row must contain exactly one 1.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check Dimensionality**: Verify that the array is two-dimensional. If the number of dimensions is not 2, the function returns False.\n2. **Check Element Values**: Ensure all elements in the array are either 0 or 1. This can be done using the `np.isin` function to check if each element is in the set {0, 1}.\n3. **Check Row Sums**: For each row in the array, check if the sum of the elements is exactly 1. This ensures that each row contains exactly one 1.\n\nBy following these steps, we can determine if the given array meets all the criteria of a valid one-hot matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    # Check if the array is two-dimensional\n    if x.ndim != 2:\n        return False\n    # Check all elements are either 0 or 1\n    if not np.all(np.isin(x, [0, 1])):\n        return False\n    # Check each row sums to exactly 1\n    row_sums = x.sum(axis=1)\n    if not np.all(row_sums == 1):\n        return False\n    # All conditions are satisfied\n    return True\n```\n\n### Explanation\n- **Dimensionality Check**: The function first checks if the array is two-dimensional using `x.ndim != 2`. If this condition is not met, the function returns False.\n- **Element Value Check**: The function uses `np.isin(x, [0, 1])` to ensure all elements are either 0 or 1. If any element is outside this range, the function returns False.\n- **Row Sum Check**: The function calculates the sum of each row using `x.sum(axis=1)` and checks if each sum is exactly 1. If any row does not meet this condition, the function returns False.\n\nBy systematically checking each condition, the function ensures that the input array is a valid one-hot matrix, returning True if all conditions are satisfied and False otherwise."}
{"task_id": 234, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single-step Thompson sampling algorithm for a Bernoulli multi-armed bandit. The goal is to select the arm with the highest sampled payoff probability using a Beta-Binomial model.\n\n### Approach\n1. **Input Validation**: Check if the input lists `successes` and `failures` are valid. If they are of different lengths or empty, return `-1`.\n2. **Seed Setting**: If a seed is provided, set the NumPy random seed to ensure deterministic sampling.\n3. **Posterior Calculation**: For each arm, compute the posterior parameters using the given prior parameters and observed data.\n4. **Sampling**: Draw a sample from each arm's posterior Beta distribution.\n5. **Selection**: Determine the arm with the highest sampled value, breaking ties by selecting the smallest index.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta: float = 1.0,\n                                    seed: int | None = None) -> int:\n    \"\"\"Single-step Thompson sampling for a Bernoulli multi-armed bandit.\"\"\"\n    if len(successes) != len(failures) or len(successes) == 0:\n        return -1\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    K = len(successes)\n    samples = []\n    for i in range(K):\n        alpha = prior_alpha + successes[i]\n        beta = prior_beta + failures[i]\n        sample = np.random.beta(alpha, beta)\n        samples.append(sample)\n    \n    selected_arm = np.argmax(samples)\n    return selected_arm\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `successes` and `failures` are equal and non-zero. If not, it returns `-1`.\n2. **Seed Setting**: If a seed is provided, it sets the random seed to ensure reproducibility of the sampling process.\n3. **Posterior Calculation**: For each arm, the posterior parameters are calculated by adding the prior parameters to the observed successes and failures.\n4. **Sampling**: Using NumPy's `beta` function, a sample is drawn from each arm's posterior distribution.\n5. **Selection**: The arm with the highest sampled value is selected, with ties broken by choosing the smallest index.\n\nThis approach ensures that we efficiently and correctly implement the Thompson sampling algorithm for a Bernoulli multi-armed bandit problem."}
{"task_id": 235, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a 1-dimensional NumPy array of integer class labels into a one-hot encoded 2-dimensional NumPy array. This transformation is essential in machine learning tasks where categorical data needs to be represented in a binary format suitable for various algorithms.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Input Validation**: Ensure the input array is 1-dimensional. If not, raise a `ValueError`.\n2. **Handle Empty Labels**: If the input array is empty, check if `n_classes` is provided. If not, raise a `ValueError`. If `n_classes` is provided, return an empty array with the specified number of classes.\n3. **Determine Number of Classes**: If `n_classes` is not provided, compute it as the maximum value in the labels plus one. If `n_classes` is provided, ensure it is at least the maximum value in the labels plus one; otherwise, raise a `ValueError`.\n4. **Create One-Hot Encoded Matrix**: Use NumPy's `eye` function to create an identity matrix of size `n_classes` and index it with the labels to form the one-hot encoded matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A 1-D NumPy array containing integer class labels.\n    n_classes : int | None, optional (default=None)\n        Total number of classes.  If None, it will be inferred from the\n        data as `labels.max() + 1`.\n\n    Returns\n    -------\n    np.ndarray\n        A two-dimensional NumPy array of shape (N, C) where N is the\n"}
{"task_id": 236, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a utility function that splits a dataset into mini-batches of indices. This function will return a generator that yields these indices and the total number of mini-batches. The function should handle both shuffled and non-shuffled data.\n\n### Approach\n1. **Determine the Number of Samples**: Extract the number of samples, `N`, from the input array `X`.\n2. **Generate Indices**: Create an array of indices from `0` to `N-1`.\n3. **Shuffle Indices**: If shuffling is enabled, randomly permute the indices in-place.\n4. **Calculate Batches**: Compute the total number of mini-batches using the ceiling of `N` divided by `batchsize`.\n5. **Create Generator**: Define a generator function that yields slices of the indices array, each slice corresponding to a mini-batch.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef minibatch(X: np.ndarray, batchsize: int = 256, shuffle: bool = True):\n    \"\"\"Create a generator that yields index mini-batches.\"\"\"\n    N = X.shape[0]\n    idx = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(idx)\n    n_batches = (N + batchsize - 1) // batchsize\n    \n    def gen():\n        for i in range(0, N, batchsize):\n            yield idx[i:i+batchsize]\n    \n    return (gen(), n_batches)\n```\n\n### Explanation\n1. **Extract Number of Samples**: The number of samples `N` is determined from the shape of the input array `X`.\n2. **Generate Indices**: An array `idx` of indices from `0` to `N-1` is created using `np.arange`.\n3. **Shuffle Indices**: If `shuffle` is `True`, the indices are shuffled in-place using `np.random.shuffle`.\n4. **Calculate Batches**: The total number of mini-batches is calculated using integer division to ensure we account for any remaining samples that don't fill a full batch.\n5. **Generator Function**: The generator function `gen` iterates over the indices array in steps of `batchsize`, yielding each slice as a mini-batch. This ensures that the generator is single-pass and raises `StopIteration` when exhausted.\n\nThis approach efficiently handles both shuffled and non-shuffled data, ensuring that each mini-batch is processed in the correct order and that the generator is memory efficient."}
{"task_id": 237, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the score of a Blackjack hand. The goal is to determine the highest possible score without exceeding 21. If the score exceeds 21, the hand is considered bust, and we return 0.\n\n### Approach\n1. **Card Values**: Numeric cards (2-10) are worth their face value. Face cards (J, Q, K) are each worth 10 points. Aces (A) can be worth either 11 or 1, and we need to choose the value that maximizes the hand's total without exceeding 21.\n2. **Initial Calculation**: Start by treating each Ace as 11 and sum the values of all cards. Count the number of Aces.\n3. **Adjust for Aces**: If the initial total exceeds 21, convert Aces from 11 to 1 (subtract 10 for each Ace) until the total is 21 or less or there are no more Aces to convert.\n4. **Return Result**: If the adjusted total is 21 or less, return this total. If it exceeds 21, return 0.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n    \"\"\"Return the Blackjack score of *hand* or 0 if the hand is bust.\"\"\"\n    total = 0\n    aces = 0\n    for card in hand:\n        if card in ['J', 'Q', 'K']:\n            total += 10\n        elif card == 'A':\n            total += 11\n            aces += 1\n        else:\n            total += int(card)\n    # Adjust for aces if total exceeds 21\n    while total > 21 and aces > 0:\n        total -= 10\n        aces -= 1\n    return total if total <= 21 else 0\n```\n\n### Explanation\n1. **Initialization**: We start by initializing `total` to 0 and `aces` to 0.\n2. **Card Processing**: For each card in the hand:\n   - If the card is a face card (J, Q, K), add 10 to the total.\n   - If the card is an Ace, add 11 to the total and increment the Ace count.\n   - For numeric cards, add their integer value to the total.\n3. **Adjusting Aces**: If the total exceeds 21, we convert Aces from 11 to 1 by subtracting 10 for each Ace until the total is 21 or less or there are no more Aces to convert.\n4. **Result**: If the adjusted total is 21 or less, return this total. If it exceeds 21, return 0.\n\nThis approach ensures that we maximize the hand's score without exceeding 21, efficiently handling the special case of Aces."}
{"task_id": 239, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the amount of zero-padding required for a 2-D convolution operation based on the filter size and the desired output shape. The output shape can be either \"same\" or \"valid\". \n\n### Approach\n1. **Understanding the Problem**: \n   - For \"valid\" convolution, no padding is needed, so the result is always `((0, 0), (0, 0))`.\n   - For \"same\" convolution, the goal is to pad the input such that the output dimensions match the input dimensions. This requires calculating the necessary padding for each dimension (height and width) of the input image.\n\n2. **Padding Calculation**:\n   - For each dimension (height and width), the total padding required is `filter_size - 1`.\n   - If the total padding is even, it is split equally between the top and bottom (or left and right) sides.\n   - If the total padding is odd, the smaller part is placed on the top/left side, and the larger part on the bottom/right side.\n\n3. **Implementation**:\n   - Check the output shape. If it is \"valid\", return zero padding.\n   - For \"same\" output, compute the padding for each dimension using integer division and modulo operations to handle both even and odd cases.\n\n### Solution Code\n```python\ndef determine_padding(filter_shape: tuple[int, int], output_shape: str = \"same\") -> tuple[tuple[int, int], tuple[int, int]]:\n    \"\"\"Calculate the amount of zero-padding needed on all borders for a 2-D convolution.\"\"\"\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n    else:\n        f_height, f_width = filter_shape\n        total_pad_height = f_height - 1\n        pad_top = total_pad_height // 2\n        pad_bottom = total_pad_height - pad_top\n        \n        total_pad_width = f_width - 1\n        pad_left = total_pad_width // 2\n        pad_right = total_pad_width - pad_left\n        \n        return ((pad_top, pad_bottom), (pad_left, pad_right))\n```\n\n### Explanation\n- **Valid Output**: When the output shape is \"valid\", the function immediately returns zero padding as no padding is needed.\n- **Same Output**: For each dimension (height and width), the function calculates the total padding required as `filter_size - 1`. This total padding is then split into top/bottom and left/right padding. If the total padding is odd, the smaller part is assigned to the top/left and the larger part to the bottom/right, ensuring the output dimensions match the input dimensions.\n\nThis approach efficiently computes the necessary padding for both \"same\" and \"valid\" convolution operations, ensuring the correct spatial dimensions of the output."}
{"task_id": 240, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create four dictionaries that map observation and action identifiers to unique integers and vice versa. These dictionaries are essential for efficiently indexing tables, tensors, or neural network outputs in reinforcement learning applications.\n\n### Approach\nThe function `create_encoding_dicts` takes in two lists of identifiers (`obs_ids` for observations and `action_ids` for actions) and optionally two pre-defined encoders (`obs_encoder` for observations and `act_encoder` for actions). The function builds four dictionaries:\n1. `obs2num`: Maps each observation identifier to a unique integer.\n2. `num2obs`: The inverse of `obs2num`, mapping integers back to observation identifiers.\n3. `act2num`: Maps each action identifier to a unique integer.\n4. `num2act`: The inverse of `act2num`, mapping integers back to action identifiers.\n\nIf an encoder is provided, it is used directly. If no encoder is provided, the function creates the mappings by enumerating the identifiers in the order they appear in the respective lists. If a list is empty and no encoder is provided, the corresponding dictionaries are left empty.\n\n### Solution Code\n```python\ndef create_encoding_dicts(obs_ids: list, action_ids: list,\n                          obs_encoder: dict | None = None,\n                          act_encoder: dict | None = None) -> dict:\n    \"\"\"Generate forward and backward mappings for observations and actions.\n\n    Args:\n        obs_ids (list): All distinct observation identifiers.\n        action_ids (list): All distinct action identifiers.\n        obs_encoder (dict | None, optional): Existing obs\u2192number mapping.\n        act_encoder (dict | None, optional): Existing act\u2192number mapping.\n\n    Returns:\n        dict: A dictionary that contains the keys 'obs2num', 'num2obs',\n        'act2num', and 'num2act'.\n    \"\"\"\n    # Process observations\n    if obs_encoder is not None:\n        obs2num = obs_encoder.copy()\n    else:\n        if not obs_ids:\n            obs2num = {}\n        else:\n            obs2num = {id: idx for idx, id in enumerate(obs_ids)}\n    num2obs = {v: k for k, v in obs2num.items()}\n\n    # Process actions\n    if act_encoder is not None:\n        act2num = act_encoder.copy()\n    else:\n        if not action_ids:\n            act2num = {}\n        else:\n            act2num = {id: idx for idx, id in enumerate(action_ids)}\n    num2act = {v: k for k, v in act2num.items()}\n\n    return {\n        'obs2num': obs2num,\n        'num2obs': num2obs,\n        'act2num': act2num,\n        'num2act': num2act\n    }\n```\n\n### Explanation\n1. **Observation Handling**:\n   - If `obs_encoder` is provided, it is used directly to create `obs2num`.\n   - If no encoder is provided, `obs2num` is created by enumerating `obs_ids`.\n   - The inverse dictionary `num2obs` is created by swapping keys and values of `obs2num`.\n\n2. **Action Handling**:\n   - If `act_encoder` is provided, it is used directly to create `act2num`.\n   - If no encoder is provided, `act2num` is created by enumerating `action_ids`.\n   - The inverse dictionary `num2act` is created by swapping keys and values of `act2num`.\n\n3. **Edge Cases**:\n   - If `obs_ids` or `action_ids` is empty and no encoder is provided, the corresponding dictionaries are left empty.\n   - The function ensures that the inverse dictionaries are exact inverses of their forward counterparts, maintaining consistency.\n\nThis approach efficiently handles both provided encoders and dynamically created mappings, ensuring that all identifiers are uniquely mapped to integers and vice versa."}
{"task_id": 242, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the UCB1 (Upper-Confidence-Bound) algorithm for the stochastic multi-armed bandit problem. The goal is to select the arm that maximizes the expected reward while balancing exploration and exploitation.\n\n### Approach\nThe UCB1 algorithm follows these steps:\n1. **Check for Untried Arms**: If any arm has never been pulled (its pull count is zero), the algorithm immediately selects the first such arm. This ensures that every arm is tried at least once.\n2. **Compute UCB Scores**: If all arms have been pulled at least once, compute the UCB score for each arm. The UCB score for an arm is given by:\n   \\[\n   \\text{score}(a) = \\text{ev\\_estimates}[a] + C \\times \\sqrt{\\frac{2 \\ln(\\text{current\\_step} + 1)}{\\text{pull\\_counts}[a]}}\n   \\]\n   where \\( C \\) is a parameter that scales the exploration term.\n3. **Select the Best Arm**: The arm with the highest UCB score is selected. In case of a tie, the arm with the smallest index is chosen.\n\n### Solution Code\n```python\nimport math\n\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float = 1.0) -> int:\n    \"\"\"Return the arm index that UCB1 would pick next.\"\"\"\n    # Check if any arm has never been pulled\n    for i in range(len(pull_counts)):\n        if pull_counts[i] == 0:\n            return i\n    \n    # Calculate the UCB scores for each arm\n    T = current_step + 1\n    ln_T = math.log(T)\n    scores = []\n    for a in range(len(ev_estimates)):\n        score = ev_estimates[a] + C * math.sqrt((2 * ln_T) / pull_counts[a])\n        scores.append(score)\n    \n    # Find the index of the maximum score, preferring the smallest index in case of ties\n    max_score = max(scores)\n    selected_arm = scores.index(max_score)\n    return selected_arm\n```\n\n### Explanation\n1. **Untried Arms Check**: The function first iterates through the pull counts to find any arm that has not been pulled. If found, it returns the index of the first such arm.\n2. **UCB Score Calculation**: If all arms have been pulled, the function calculates the UCB score for each arm using the given formula. This score combines the empirical mean reward with an exploration term that decreases as the arm is pulled more often.\n3. **Arm Selection**: The function then finds the arm with the highest UCB score. If multiple arms have the same score, the smallest index is selected to ensure consistency and avoid arbitrary choices.\n\nThis approach efficiently balances exploration and exploitation, ensuring that the algorithm both explores new arms and exploits the best-known arms to maximize rewards over time."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using NumPy. The network consists of three dense layers followed by two separate output heads for the actor and critic. The weights and biases are fixed, which allows us to compute the network's output deterministically.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers with ReLU activation, followed by an actor head with a softmax activation and a critic head with no activation.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0, simplifying the computations.\n3. **Forward Pass Steps**:\n   - **Dense-1**: Input (3 neurons) \u2192 4 neurons with ReLU activation.\n   - **Dense-2**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Dense-3**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Actor Head**: 4 neurons \u2192 3 neurons with softmax activation to get action probabilities.\n   - **Critic Head**: 4 neurons \u2192 1 neuron to get the state value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    \n    # Dense-1: input (3) -> 4 neurons, ReLU\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    z1 = np.maximum(0, state.dot(W1) + b1)\n    \n    # Dense-2: 4 -> 4 neurons, ReLU\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    z2 = np.maximum(0, z1.dot(W2) + b2)\n    \n    # Dense-3: 4 -> 4 neurons, ReLU\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    z3 = np.maximum(0, z2.dot(W3) + b3)\n    \n    # Actor head: 4 -> 3 neurons, Softmax\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    logits = z3.dot(W_a) + b_a\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    \n    # Critic head: 4 -> 1 neuron\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    state_value = z3.dot(W_c) + b_c\n    state_value = state_value[0]  # Extract the scalar value\n    \n    # Rounding to four decimal places\n    action_probs = [round(p, 4) for p in action_probs]\n    state_value = round(state_value, 4)\n    \n    return action_probs, state_value\n```\n\n### Explanation\n1. **Dense Layers**: Each dense layer transforms the input using matrix multiplication with weights and biases, followed by ReLU activation. Given the fixed weights and biases, each transformation simplifies to a sum of the input elements.\n2. **Actor Head**: The actor head computes logits for each action, which are then converted to probabilities using the softmax function. Due to identical logits, each action has an equal probability of 1/3.\n3. **Critic Head**: The critic head computes the state value as a linear transformation of the output from the last dense layer, resulting in a scalar value.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, leveraging the fixed weights and biases for simplicity."}
{"task_id": 244, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a random binary tensor of a given shape where each element is independently 1.0 with a probability equal to the given sparsity parameter and 0.0 otherwise. The function must validate its arguments and return -1 if any argument is invalid.\n\n### Approach\n1. **Argument Validation**:\n   - **Shape Validation**: Ensure the shape is a non-empty tuple of positive integers. If not, return -1.\n   - **Sparsity Validation**: Ensure the sparsity is a real number within the interval [0, 1]. If not, return -1.\n\n2. **Generate Random Tensor**:\n   - Use `numpy.random.rand` to generate a tensor of the given shape filled with random floats between 0 and 1.\n\n3. **Convert to Binary Tensor**:\n   - Convert each element of the generated tensor to 1.0 if it is greater than or equal to the threshold (1 - sparsity), otherwise convert it to 0.0.\n\n4. **Return as Nested List**:\n   - Convert the resulting binary tensor to a nested Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    The function should create a tensor of the requested ``shape`` such that\n    each entry independently takes the value ``1.0`` with probability equal\n    to ``sparsity`` and ``0.0`` otherwise.\n\n    The result **must** be returned as a regular (nested) Python list obtained\n    via NumPy\u2019s ``tolist`` method.  If either ``shape`` or ``sparsity`` is\n    illegal (see task description) the function must return **-1**.\n    \"\"\"\n    # Validate shape\n    if not isinstance(shape, tuple):\n        return -1\n    if not shape:  # Check if shape is an empty tuple\n        return -1\n    for dim in shape:\n        if not isinstance(dim, int) or dim <= 0:\n            return -1\n    \n    # Validate sparsity\n    if isinstance(sparsity, bool):\n        return -1\n    if not isinstance(sparsity, (int, float)):\n        return -1\n    if not (0 <= sparsity <= 1):\n        return -1\n    \n    # Generate the random tensor\n    tensor = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_tensor = (tensor >= threshold).astype(float)\n    \n    # Convert to nested list and return\n    return binary_tensor.tolist()\n```\n\n### Explanation\n- **Argument Validation**: The function first checks if the shape is a valid tuple of positive integers and if the sparsity is a valid float within the range [0, 1]. If any check fails, the function returns -1.\n- **Random Tensor Generation**: Using `numpy.random.rand`, a tensor of the specified shape is generated with random values between 0 and 1.\n- **Binary Conversion**: Each element of the tensor is converted to 1.0 if it is greater than or equal to the threshold (1 - sparsity), otherwise it is set to 0.0.\n- **Conversion to List**: The resulting binary tensor is converted to a nested Python list using the `tolist()` method, ensuring the output is in the required format.\n\nThis approach ensures that the function efficiently generates the required binary tensor while handling all edge cases and invalid inputs appropriately."}
{"task_id": 247, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the arm with the highest expected reward in a Gaussian bandit setup. The function should return the maximum expected reward rounded to four decimal places and the index of the arm that achieves this maximum. If the input is invalid, the function should return -1.\n\n### Approach\n1. **Input Validation**: \n   - Ensure both input lists are of the same non-zero length.\n   - Check that each probability in `payoff_probs` lies within the interval [0, 1].\n   - Ensure each variance in `payoff_dists` is strictly positive.\n\n2. **Expected Reward Calculation**:\n   - For each arm, compute the expected reward using the formula \\( \\text{expected\\_reward} = p_k \\times \\mu_k \\), where \\( p_k \\) is the probability of payout and \\( \\mu_k \\) is the mean reward.\n\n3. **Determine Best Arm**:\n   - Identify the maximum expected reward and the smallest index of the arm that achieves this maximum.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]],\n                           payoff_probs: list[float]) -> tuple[float, int] | int:\n    \"\"\"Determine the arm with the highest expected reward in a Gaussian bandit.\n\n    The function must compute the expected reward for each arm (p_k * \u03bc_k) and\n    return a tuple (best_ev, best_arm), where best_ev is rounded to 4 decimal\n    places and best_arm is the index of the arm with that expected reward.  If\n    the input is invalid (see task description) the function should return -1.\n    \"\"\"\n    # Check if the lengths are equal and non-zero\n    if len(payoff_dists) != len(payoff_probs) or len(payoff_dists) == 0:\n        return -1\n    \n    # Check each probability and variance\n    for i in range(len(payoff_dists)):\n        p = payoff_probs[i]\n        if p < 0 or p > 1:\n            return -1\n        mu, sigma_sq = payoff_dists[i]\n        if sigma_sq <= 0:\n            return -1\n    \n    # Compute expected rewards using NumPy\n    mu = np.array([d[0] for d in payoff_dists])\n    p = np.array(payoff_probs)\n    expected_rewards = p * mu\n    \n    # Find the maximum expected reward and the smallest index\n    max_ev = np.max(expected_rewards)\n    best_arm = np.argmax(expected_rewards)\n    \n    # Round the best expected value to 4 decimal places\n    best_ev = round(max_ev, 4)\n    \n    return (best_ev, best_arm)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input lists are valid. If they are not, it returns -1 immediately.\n2. **Expected Reward Calculation**: Using NumPy, the function computes the expected reward for each arm by multiplying the probability of payout with the mean reward.\n3. **Determine Best Arm**: The function finds the maximum expected reward and the smallest index of the arm that achieves this maximum. The result is then formatted and returned as a tuple.\n\nThis approach ensures that the function efficiently computes the required values while adhering to the constraints and requirements specified."}
{"task_id": 248, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple baseline classifier that always predicts the same probability for every example. This probability is determined by the empirical proportion of the positive class (encoded as 1) observed in the training labels.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Compute the Positive-Class Probability**: Calculate the probability of the positive class (1) in the training labels. This is done by dividing the count of positive labels by the total number of labels in the training set.\n2. **Create Predictions Array**: Generate an array where each element is the computed probability. The length of this array is equal to the number of rows in the test feature matrix.\n3. **Round the Probabilities**: Round each probability value to four decimal places to ensure precision.\n4. **Handle Edge Cases**: If the test feature matrix is empty, return an empty list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\"\"\"\n    # Calculate the probability of the positive class\n    p = np.sum(y_train) / len(y_train)\n    \n    # Determine the number of test samples\n    n_test = X_test.shape[0]\n    \n    # Handle the case where there are no test samples\n    if n_test == 0:\n        return []\n    \n    # Create an array filled with the probability p\n    predictions = np.full(n_test, p)\n    \n    # Round the predictions to four decimal places\n    predictions = np.round(predictions, 4)\n    \n    # Convert the numpy array to a Python list and return\n    return predictions.tolist()\n```\n\n### Explanation\n1. **Compute Positive-Class Probability**: The probability `p` is calculated as the sum of all positive labels (1s) in `y_train` divided by the total number of labels in `y_train`. This gives the empirical probability of the positive class.\n2. **Create Predictions Array**: Using `np.full`, we create an array where each element is `p`, and the length of this array is the number of rows in `X_test`.\n3. **Round Probabilities**: The `np.round` function is used to round each element of the predictions array to four decimal places, ensuring the required precision.\n4. **Edge Case Handling**: If `X_test` is empty (i.e., it has zero rows), the function immediately returns an empty list to handle this edge case.\n\nThis approach ensures that the function efficiently computes the required probabilities and handles all specified cases correctly."}
{"task_id": 252, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute both the gradient and the diagonal Hessian of the least-squares loss function. The least-squares loss function is defined as \\( L(actual, predicted) = 0.5 \\times ||actual - predicted||^2 \\). The gradient and Hessian are essential for optimization algorithms, such as gradient descent, to adjust model parameters and minimize the loss.\n\n### Approach\n1. **Gradient Calculation**: The gradient of the least-squares loss with respect to the predicted values is given by \\( \\nabla L = actual - predicted \\). This can be computed by subtracting the predicted values from the actual values element-wise.\n2. **Hessian Calculation**: The Hessian matrix for the least-squares loss is a diagonal matrix where each diagonal element is 1. Since we only need the diagonal elements, we can represent this as a list of ones with the same length as the input arrays.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the gradient and Hessian of the least-squares loss.\n\n    The least-squares loss is defined as 0.5 * ||actual \u2212 predicted||\u00b2.\n\n    Args:\n        actual: 1-D NumPy array containing the true labels/targets.\n        predicted: 1-D NumPy array containing the model predictions.\n\n    Returns:\n        A tuple (grad, hess):\n            grad  \u2013 Python list representing the gradient w.r.t. each prediction.\n            hess  \u2013 Python list representing the diagonal Hessian entries (all ones).\n    \"\"\"\n    grad = (actual - predicted).tolist()\n    hess = [1.0] * len(actual)\n    return (grad, hess)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed by subtracting each element of the predicted array from the corresponding element of the actual array. This result is then converted to a Python list.\n- **Hessian Calculation**: The Hessian is a diagonal matrix where each diagonal element is 1. Since we only need the diagonal elements, we create a list of ones with the same length as the input arrays.\n\nThis approach efficiently computes the required gradient and Hessian using basic array operations, ensuring that the results are returned as Python lists as specified."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Elastic-Net penalty and its gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularization, which helps in reducing model complexity and preventing over-fitting. The function should return either the penalty value or the gradient based on the input flag.\n\n### Approach\n1. **Problem Analysis**: The Elastic-Net penalty is defined as a convex combination of L1 and L2 regularization. The penalty function is given by:\n   \\[\n   R(w) = \\alpha \\cdot [\\lambda \\cdot ||w||_2 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w]\n   \\]\n   where \\( ||w||_2 \\) is the L2 norm of the weight vector \\( w \\), and \\( w^T w \\) is the squared L2 norm. The gradient of this penalty function is given by:\n   \\[\n   \\nabla R(w) = \\alpha \\cdot [\\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w]\n   \\]\n   where \\( \\text{sign}(w) \\) is the sign function applied element-wise to \\( w \\).\n\n2. **Algorithm Design**:\n   - Convert the input weight vector to a NumPy array for efficient computation.\n   - Compute the penalty value if the gradient flag is False.\n   - Compute the gradient if the gradient flag is True.\n\n3. **Penalty Calculation**:\n   - Compute the L2 norm of the weight vector.\n   - Compute the sum of the squares of the weight vector elements.\n   - Calculate the L1 and L2 parts of the penalty using the given formula.\n   - Combine these parts and multiply by the regularization strength \\( \\alpha \\).\n\n4. **Gradient Calculation**:\n   - Compute the sign of each element in the weight vector.\n   - Calculate the gradient using the given formula, combining the contributions from both L1 and L2 regularization.\n\n5. **Edge Cases**: Handle cases where the weight vector is empty or contains zeros appropriately.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    if gradient:\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        gradient = [round(x, 4) for x in gradient]\n        return gradient\n    else:\n        l2_norm = np.linalg.norm(w, 2)\n        sum_w_sq = np.dot(w, w)\n        l1_part = l1_ratio * l2_norm\n        l2_part = (1 - l1_ratio) * 0.5 * sum_w_sq\n        penalty = alpha * (l1_part + l2_part)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**: The function first converts the input weight vector into a NumPy array for efficient computation. It then calculates the L2 norm and the sum of squares of the weight vector elements. Using these values, it computes the L1 and L2 parts of the penalty and combines them, scaling by the regularization strength \\( \\alpha \\).\n\n- **Gradient Calculation**: If the gradient flag is True, the function computes the sign of each element in the weight vector. It then calculates the gradient by combining the contributions from both L1 and L2 regularization, scaling by \\( \\alpha \\) and rounding each element to 4 decimal places.\n\nThis approach ensures that the function efficiently computes either the penalty or the gradient based on the input parameters, adhering to the problem constraints and requirements."}
{"task_id": 255, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given Blackjack hand contains a usable ace. A usable ace is defined as an ace that can be counted as 11 without causing the total hand value to exceed 21.\n\n### Approach\n1. **Identify Aces**: First, we check if the hand contains any aces. If there are no aces, the function immediately returns `False`.\n2. **Calculate Sum with Aces as 1**: Compute the total value of the hand treating all aces as 1. This gives us the minimum possible value of the hand.\n3. **Check Usable Ace Condition**: To determine if there is a usable ace, we check if adding 10 (converting one ace from 1 to 11) to the computed sum does not exceed 21. If it does not exceed 21, the hand has a usable ace.\n\n### Solution Code\n```python\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 without causing the\n    hand's total value to exceed 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    aces = hand.count(1)\n    if aces == 0:\n        return False\n    sum_aces_1 = sum(hand)\n    return (sum_aces_1 + 10) <= 21\n```\n\n### Explanation\n- **Identify Aces**: The function first counts the number of aces in the hand. If there are no aces, it returns `False` immediately.\n- **Calculate Sum with Aces as 1**: The sum of the hand is computed by treating each ace as 1. This gives the minimum possible value of the hand.\n- **Check Usable Ace Condition**: By adding 10 to the computed sum (simulating the conversion of one ace from 1 to 11), we check if the total remains within the limit of 21. If it does, the function returns `True`, indicating the presence of a usable ace. Otherwise, it returns `False`.\n\nThis approach efficiently determines the presence of a usable ace by leveraging simple arithmetic and conditional checks, ensuring optimal performance and clarity."}
{"task_id": 258, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the hinge loss function, which is commonly used in training linear Support Vector Machines (SVMs). The hinge loss function measures the performance of a classification model by calculating the loss between the predicted scores and the actual class labels.\n\n### Approach\nThe hinge loss function is defined as the arithmetic mean of the per-sample hinge losses. The per-sample hinge loss is given by the formula:\n\\[ L_i = \\max(0, 1 - y_i \\cdot \\hat{y}_i) \\]\nwhere \\( y_i \\) is the true label and \\( \\hat{y}_i \\) is the predicted score for the i-th sample.\n\nThe steps to compute the hinge loss are as follows:\n1. **Check Lengths**: Ensure the lengths of the actual and predicted arrays are the same. If not, return -1.\n2. **Convert to NumPy Arrays**: Convert the input lists or arrays to NumPy arrays for efficient vectorized operations.\n3. **Compute Element-wise Product**: Calculate the product of corresponding elements in the actual and predicted arrays.\n4. **Calculate Per-sample Loss**: For each element, compute \\( 1 - (y_i \\cdot \\hat{y}_i) \\) and take the maximum of this value and 0.\n5. **Compute Mean Loss**: Calculate the mean of the per-sample losses and round it to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hinge_loss(actual, predicted):\n    \"\"\"Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n    \"\"\"\n    if len(actual) != len(predicted):\n        return -1\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    product = actual * predicted\n    per_sample_loss = np.maximum(0, 1 - product)\n    mean_loss = np.mean(per_sample_loss)\n    return round(mean_loss, 4)\n```\n\n### Explanation\n1. **Check Lengths**: The function first checks if the lengths of the actual and predicted arrays are the same. If they are not, it returns -1.\n2. **Convert to NumPy Arrays**: The inputs are converted to NumPy arrays to leverage vectorized operations for efficiency.\n3. **Element-wise Product**: The product of corresponding elements in the actual and predicted arrays is computed. This step is crucial as it directly relates to the hinge loss formula.\n4. **Per-sample Loss Calculation**: For each element, the value \\( 1 - (y_i \\cdot \\hat{y}_i) \\) is computed. If this value is negative, it is replaced with 0 using the `np.maximum` function.\n5. **Mean Loss**: The mean of the per-sample losses is computed and rounded to four decimal places to get the final result.\n\nThis approach ensures that the hinge loss is computed efficiently and correctly, adhering to the problem constraints and requirements."}
{"task_id": 259, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that samples a specified number of transitions from a replay buffer in a deterministic manner. The function should return the sampled transitions in the order they are produced by the random number generator, which is seeded with a fixed value to ensure determinism.\n\n### Approach\n1. **Check Buffer Size**: First, we check if the requested batch size exceeds the current length of the replay buffer. If it does, the function returns -1.\n2. **Seed the Random Number Generator**: To ensure deterministic behavior, we seed the random number generator with the value 42.\n3. **Sample Transitions**: Using the `random.sample` function, we select the required number of transitions from the buffer. This function returns a list of unique elements in a specific order determined by the seed.\n4. **Return the Sample**: The sampled transitions are returned as a list.\n\n### Solution Code\n```python\nimport random\n\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n    \"\"\"Randomly samples *batch_size* transitions from the replay buffer.\n\n    The selection must be deterministic (use random.seed(42)).  If\n    *batch_size* exceeds the buffer length, return -1.\n    \"\"\"\n    if batch_size > len(memory):\n        return -1\n    random.seed(42)\n    return random.sample(memory, batch_size)\n```\n\n### Explanation\n- **Buffer Check**: The function first checks if the requested batch size is larger than the buffer's length. If it is, the function returns -1 immediately.\n- **Seeding**: By setting the seed to 42, we ensure that the random number generator produces the same sequence of numbers each time the function is called with the same inputs, making the sampling deterministic.\n- **Sampling**: The `random.sample` function is used to select the required number of transitions. This function ensures that each transition is unique and returns them in a specific order determined by the seed.\n- **Return Value**: The function returns the sampled transitions as a list, or -1 if the batch size is too large.\n\nThis approach ensures that the function behaves deterministically and efficiently handles the sampling of transitions from the replay buffer."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes weights in a way that helps in maintaining consistent variance of activations across layers, which is crucial for effective training of deep networks.\n\n### Approach\nThe approach to solve this problem involves three main steps:\n\n1. **Compute Fan In and Fan Out**: \n   - For a 2D shape `(fan_in, fan_out)`, the values are directly taken from the dimensions.\n   - For shapes with more than two dimensions (common in convolutional layers), the fan in and fan out are computed by considering the product of all dimensions beyond the first two (receptive field size).\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation `s` is computed using the formula `s = sqrt(2 / (fan_in + fan_out))`.\n\n3. **Generate Random Array**: \n   - Using the computed standard deviation, generate a NumPy array with elements drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n"}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should be a list of lists where each inner list contains the neighbors of the corresponding vertex in `V`, sorted by their order of appearance in `V`.\n\n### Approach\n1. **Create a Vertex Index Mapping**: First, we create a dictionary to map each vertex to its index in the list `V`. This helps in quickly determining the order of vertices.\n2. **Initialize Adjacency Dictionary**: We use a dictionary where each key is a vertex and the value is a set of its neighbors. This helps in efficiently managing and avoiding duplicate edges.\n3. **Process Edges**: For each edge in `E`, we add each vertex to the other's neighbor set. This ensures that both directions of the undirected edge are accounted for.\n4. **Build the Adjacency List**: For each vertex in `V`, we collect its neighbors, sort them based on their indices in `V`, and construct the adjacency list.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for i, v in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    \n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    \n    return G\n```\n\n### Explanation\n1. **Vertex Index Mapping**: The dictionary `vertex_index` maps each vertex to its position in `V`, which is used to sort neighbors later.\n2. **Adjacency Dictionary**: `adj_dict` is initialized with each vertex having an empty set of neighbors. This helps in efficiently adding and managing neighbors without duplicates.\n3. **Edge Processing**: For each edge `(u, v)`, both `u` is added to `v`'s neighbors and `v` is added to `u`'s neighbors, ensuring the undirected nature of the graph.\n4. **Building the Adjacency List**: For each vertex in `V`, its neighbors are collected, sorted by their indices using `vertex_index`, and added to the result list `G`.\n\nThis approach ensures that the adjacency list is constructed efficiently and correctly, with neighbors sorted as required."}
{"task_id": 269, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the total number of trainable parameters in a two-layer Deep Q-Network (DQN). The DQN consists of two fully-connected layers with specific configurations, and we need to sum the parameters from both layers to get the total count.\n\n### Approach\nThe DQN architecture is as follows:\n1. The first layer is a fully-connected layer that maps the input state vector (of length `state_dimensions`) to 164 hidden neurons.\n2. The second layer is another fully-connected layer that maps these 164 hidden neurons to the number of possible actions (of length `action_dimensions`).\n\nFor each fully-connected layer, the number of trainable parameters is calculated as the sum of the weights and biases. The formula for the number of parameters in a fully-connected layer is:\n- Weights: `in_features \u00d7 out_features`\n- Biases: `out_features`\n\nThus, the total parameters for each layer are given by `(in_features \u00d7 out_features) + out_features`.\n\n### Solution Code\n```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n    \"\"\"Return the total number of trainable parameters of the DQN network.\"\"\"\n    layer1 = state_dimensions * 164 + 164\n    layer2 = 164 * action_dimensions + action_dimensions\n    return layer1 + layer2\n```\n\n### Explanation\n1. **First Layer Calculation**:\n   - The input dimension is `state_dimensions` and the output dimension is 164.\n   - The number of weights is `state_dimensions * 164`.\n   - The number of biases is 164.\n   - Total parameters for the first layer: `state_dimensions * 164 + 164`.\n\n2. **Second Layer Calculation**:\n   - The input dimension is 164 and the output dimension is `action_dimensions`.\n   - The number of weights is `164 * action_dimensions`.\n   - The number of biases is `action_dimensions`.\n   - Total parameters for the second layer: `164 * action_dimensions + action_dimensions`.\n\n3. **Total Parameters**:\n   - Sum the parameters from both layers to get the total number of trainable parameters.\n\nThis approach efficiently computes the total number of parameters by breaking down the problem into manageable parts and summing the results."}
{"task_id": 277, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether the action and observation spaces in a given reinforcement learning environment are composite. A composite space is defined as either a tuple or a dictionary. \n\n### Approach\nThe approach involves checking the type of the action and observation spaces in the environment dictionary. Specifically, we will:\n1. Check if the value associated with the key `\"action_space\"` is either a tuple or a dictionary.\n2. Similarly, check if the value associated with the key `\"observation_space\"` is either a tuple or a dictionary.\n3. Return the results as a tuple of two boolean values indicating whether each space is composite.\n\nThis approach leverages Python's `isinstance` function to check the type of each space, ensuring that we correctly identify composite spaces.\n\n### Solution Code\n```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    tuple_action = isinstance(env[\"action_space\"], (tuple, dict))\n    tuple_obs = isinstance(env[\"observation_space\"], (tuple, dict))\n    return (tuple_action, tuple_obs)\n```\n\n### Explanation\n- The function `is_tuple` takes a dictionary `env` as input, which is guaranteed to contain the keys `\"action_space\"` and `\"observation_space\"`.\n- It uses the `isinstance` function to check if the value of `\"action_space\"` is either a tuple or a dictionary. This result is stored in `tuple_action`.\n- Similarly, it checks the type of the value associated with `\"observation_space\"` and stores the result in `tuple_obs`.\n- The function returns a tuple containing these two boolean values, indicating whether each space is composite.\n\nThis solution efficiently determines the composite nature of the spaces using straightforward type checking, ensuring clarity and correctness."}
{"task_id": 278, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a binary decision tree represented by nested dictionaries and predict the value for a given input sample. The decision tree consists of internal nodes and leaf nodes. Internal nodes contain rules for branching, while leaf nodes contain the predicted values.\n\n### Approach\n1. **Initialization**: Start at the root of the decision tree.\n2. **Traversal**: For each internal node, check the feature value of the sample against the threshold. If the sample's feature value is less than or equal to the threshold, follow the true branch; otherwise, follow the false branch.\n3. **Leaf Node Check**: Continue traversing until a leaf node is reached. The leaf node contains the predicted value, which is then returned.\n\n### Solution Code\n```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'. Leaf\n            nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    current_node = tree\n    while True:\n        if 'value' in current_node:\n            return current_node['value']\n        else:\n            feature_i = current_node['feature_i']\n            threshold = current_node['threshold']\n            if sample[feature_i] <= threshold:\n                current_node = current_node['true_branch']\n            else:\n                current_node = current_node['false_branch']\n```\n\n### Explanation\n- **Initialization**: The function starts by setting the current node to the root of the tree.\n- **Loop Until Leaf Node**: The loop continues until a leaf node is found. For each internal node, the function checks the feature value of the sample against the threshold.\n- **Branching**: Depending on whether the sample's feature value is less than or equal to the threshold, the function follows the true or false branch to the next node.\n- **Return Leaf Value**: Once a leaf node is reached, the function returns the value stored in that leaf node, which is the prediction for the given sample.\n\nThis approach ensures that the decision tree is traversed efficiently, and the correct prediction is made based on the given sample."}
{"task_id": 280, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the L2 regularization term and its gradient for a given weight vector or matrix. The L2 regularization term is used to penalize large model parameters, helping to prevent overfitting in machine learning models.\n\n### Approach\nThe function `l2_regularization` takes three parameters: a weight vector or matrix `w`, a non-negative regularization coefficient `alpha`, and a boolean flag `return_grad`. The function behaves differently based on the value of `return_grad`:\n\n1. **Regularization Value**: If `return_grad` is `False`, the function computes the L2 regularization value, which is given by the formula:\n   \\[\n   0.5 \\cdot \\alpha \\cdot \\sum w_i^2\n   \\]\n   where the summation runs over all elements of `w`. The result is rounded to four decimal places and returned as a float.\n\n2. **Gradient**: If `return_grad` is `True`, the function computes the gradient of the L2 regularization term with respect to `w`, which is given by:\n   \\[\n   \\alpha \\cdot w\n   \\]\n   The gradient is rounded to four decimal places and returned as a NumPy array with the same shape as `w`.\n\nThe function handles any shape of `w`, including 1D vectors, 2D matrices, and higher-dimensional arrays. If `alpha` is zero, the function correctly returns zero for both the regularization value and the gradient.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool = False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector or matrix.\n    alpha : float\n        Regularization strength (must be non-negative).\n    return_grad : bool, optional (default=False)\n        If True, return the gradient of the regularization term instead of the\n        regularization value itself.\n\n    Returns\n    -------\n    float or np.ndarray\n        \u2022 When *return_grad* is False: the scalar L2 regularization value.\n        \u2022 When *return_grad* is True:  the gradient with the same shape as *w*.\n    \"\"\"\n    if return_grad:\n        grad = alpha * w\n        grad = np.round(grad, decimals=4)\n        return grad\n    else:\n        loss = 0.5 * alpha * np.sum(w ** 2)\n        loss = np.round(loss, decimals=4)\n        return float(loss)\n```\n\n### Explanation\n- **Regularization Value Calculation**: When `return_grad` is `False`, the function calculates the sum of the squares of all elements in `w`, multiplies it by `0.5 * alpha`, and returns the result as a float rounded to four decimal places.\n- **Gradient Calculation**: When `return_grad` is `True`, the function computes the element-wise product of `alpha` and `w`, rounds each element to four decimal places, and returns the result as a NumPy array with the same shape as `w`.\n\nThis approach ensures that the function efficiently handles arrays of any shape and correctly computes both the regularization value and its gradient, adhering to the specified requirements."}
{"task_id": 281, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a class that provides L1 regularisation (Lasso penalty) for combating over-fitting in linear models. The class should be callable and provide both the penalty value and the gradient of the penalty with respect to the weight vector.\n\n### Approach\nThe L1 regularisation, also known as the Lasso penalty, is a method used to prevent over-fitting in machine learning models by adding a penalty to the loss function. This penalty is based on the absolute values of the model's coefficients.\n\nThe class `L1Regularization` is designed to be callable and should provide two main functionalities:\n1. Compute the L1 penalty value for a given weight vector.\n2. Compute the gradient of the L1 penalty with respect to the weight vector.\n\nThe constructor of the class takes a parameter `alpha`, which controls the strength of the regularisation. If `alpha` is zero, both the penalty and the gradient should be zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L1Regularization(alpha: float):\n    \"\"\"Create a callable object that implements L1 regularisation.\n\n    The returned object must support two operations:\n    1. call with a NumPy array to obtain the L1 penalty value\n    2. call its .grad(w) method to obtain the gradient of the penalty\n\n    Args:\n        alpha: Non-negative float controlling the strength of the regularisation.\n\n    Returns:\n        An instance that fulfils the described interface.\n    \"\"\"\n    class L1Regularizer:\n        def __init__(self, alpha):\n            self.alpha = alpha\n\n        def __call__(self, w):\n            if self.alpha == 0:\n                return 0.0\n            return self.alpha * np.sum(np.abs(w))\n\n        def grad(self, w):\n            if self.alpha == 0:\n                return np.zeros_like(w)\n            return self.alpha * np.sign(w)\n    \n    return L1Regularizer(alpha)\n```\n\n### Explanation\n1. **Class Definition**: The `L1Regularizer` class is defined within the `L1Regularization` function. This class is designed to be callable and provides the required functionalities.\n2. **Constructor**: The constructor initializes the `alpha` parameter, which controls the strength of the regularisation.\n3. **Callable Method (`__call__`)**: This method computes the L1 penalty value. It sums the absolute values of all elements in the weight vector `w` and multiplies the result by `alpha`. If `alpha` is zero, it returns 0.0.\n4. **Gradient Method (`grad`)**: This method computes the gradient of the L1 penalty. It uses the sign function to determine the direction of the gradient for each element in `w`, scaled by `alpha`. If `alpha` is zero, it returns an array of zeros with the same shape as `w`.\n\nThis implementation ensures that the L1 regularisation is applied correctly for any shape of the weight vector, providing both the penalty value and the gradient efficiently."}
{"task_id": 285, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel (Gram matrix) between two datasets. The linear kernel is defined as the matrix product of the first dataset and the transpose of the second dataset. The function should handle both one-dimensional and two-dimensional input arrays and return the result as a nested list or -1 if the feature dimensions do not match.\n\n### Approach\n1. **Reshape Input Arrays**: If the input arrays are one-dimensional, reshape them into two-dimensional row vectors. This ensures that both inputs are in a consistent format for further processing.\n2. **Check Feature Dimensions**: Verify that the number of features (columns) in both datasets is the same. If they differ, return -1.\n3. **Compute Kernel Matrix**: If the feature dimensions match, compute the matrix product of the first dataset and the transpose of the second dataset.\n4. **Convert to List**: Convert the resulting matrix from NumPy to a nested list format using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n    \"\"\"Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        The linear kernel matrix as a nested Python list. If *x* and *y* have\n        different feature dimensions, returns -1.\n    \"\"\"\n    # Reshape x and y if they are 1D\n    if x.ndim == 1:\n        x_reshaped = x.reshape(1, -1)\n    else:\n        x_reshaped = x\n\n    if y.ndim == 1:\n        y_reshaped = y.reshape(1, -1)\n    else:\n        y_reshaped = y\n\n    # Check if feature dimensions match\n    if x_reshaped.shape[1] != y_reshaped.shape[1]:\n        return -1\n\n    # Compute the kernel matrix\n    kernel = x_reshaped @ y_reshaped.T\n\n    # Convert to a list of lists\n    return kernel.tolist()\n```\n\n### Explanation\n1. **Reshaping**: The function first checks if the input arrays are one-dimensional. If they are, they are reshaped into two-dimensional row vectors to ensure consistency.\n2. **Dimension Check**: The function then checks if the number of features (columns) in both datasets is the same. If not, it returns -1.\n3. **Matrix Multiplication**: If the feature dimensions match, the function computes the matrix product of the first dataset and the transpose of the second dataset.\n4. **Conversion**: The resulting matrix is converted into a nested list format using the `tolist()` method, which is then returned as the result.\n\nThis approach ensures that the function handles various input formats correctly and efficiently computes the linear kernel matrix."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function `get_initializer` that retrieves a weight-initialization function based on a given name. The function should return the corresponding callable initializer if it exists, or raise an error if it does not.\n\n### Approach\n1. **Lookup in"}
{"task_id": 289, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to format a list of tuples into a specific string format. Each tuple consists of a field name and its corresponding value, both of which are strings. The goal is to convert each tuple into a string of the form \"field: \"value\"\" and then join all these strings with newline characters.\n\n### Approach\n1. **Check for Empty List**: If the input list is empty, return an empty string immediately.\n2. **Iterate Over Tuples**: For each tuple in the list, convert the field and value to strings (to handle any non-string inputs gracefully).\n3. **Format Each Tuple**: Convert each tuple into the required string format \"field: \"value\"\".\n4. **Join Formatted Strings**: Collect all formatted strings into a list and join them with newline characters to form the final result string.\n\n### Solution Code\n```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n    \"\"\"Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field: \"value\"\n    and all lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value. Both should be\n                 strings; however, any value will be converted to its string\n                 representation.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline.\n        Returns an empty string if *entries* is empty.\n    \"\"\"\n    if not entries:\n        return \"\"\n    formatted = []\n    for field, value in entries:\n        field_str = str(field)\n        value_str = str(value)\n        formatted_line = f\"{field_str}: \\\"{value_str}\\\"\"\n        formatted.append(formatted_line)\n    return '\\n'.join(formatted)\n```\n\n### Explanation\n- **Empty List Check**: The function first checks if the input list is empty. If it is, it returns an empty string immediately.\n- **String Conversion**: Each field and value is converted to a string to handle any non-string inputs, ensuring robustness.\n- **Formatting**: Each tuple is formatted into the string \"field: \"value\"\" using an f-string for clarity and efficiency.\n- **Joining Strings**: All formatted strings are collected into a list and then joined with newline characters to produce the final result string, ensuring no extra newline is added at the end.\n\nThis approach efficiently handles the conversion and formatting, ensuring the output meets the specified requirements."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same features and thresholds at each internal node, and produce the same predictions at each leaf node, within a specified tolerance.\n\n### Approach\n1. **Type Check**: First, we check if both trees are of the same type (both are either `Node` or `Leaf`). If they are not, the trees are not equivalent.\n2. **Leaf Node Comparison**: If both nodes are `Leaf` nodes, we compare their values using `numpy.allclose` to handle numerical comparisons with a tolerance of \\(1 \\times 10^{-8}\\).\n3. **Internal Node Comparison**: If both nodes are `Node` nodes, we check:\n   - The feature index used for splitting.\n   - The threshold value, ensuring it is the same within the specified tolerance.\n   - Recursively compare the left and right subtrees to ensure they are equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both nodes are of the same type\n    if type(tree_a) != type(tree_b):\n        return False\n    \n    if isinstance(tree_a, Leaf):\n        # Compare leaf values with tolerance\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    else:\n        # Both are Node instances; check feature and threshold\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        \n        # Recursively compare left and right subtrees\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        \n        return True\n```\n\n### Explanation\n- **Type Check**: The function first ensures both nodes are of the same type. If one is a `Node` and the other is a `Leaf`, they are not equivalent.\n- **Leaf Node Comparison**: For `Leaf` nodes, the values are compared using `numpy.allclose` to handle both scalar and array comparisons with a tolerance.\n- **Internal Node Comparison**: For `Node` nodes, the function checks if the feature index and threshold are the same within the tolerance. It then recursively checks the left and right subtrees to ensure they are structurally and functionally equivalent.\n\nThis approach ensures that the trees are compared thoroughly, checking both structure and values at each node, ensuring equivalence within the specified tolerance."}
{"task_id": 293, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that maps a given reinforcement learning (RL) agent name to its corresponding OpenAI-Gym environment and access type. The function should be case-insensitive and ignore leading/trailing spaces. If the agent name is not recognized, the function should return -1.\n\n### Approach\n1. **Input Processing**: The input agent name is processed to remove any leading or trailing whitespace and converted to lowercase to ensure case insensitivity.\n2. **Mapping Creation**: A dictionary is created where each key is a lowercase agent name, and the corresponding value is a tuple containing the environment name and access type.\n3. **Lookup and Return**: The processed agent name is checked against the dictionary. If it exists, the corresponding tuple is returned. If not, -1 is returned.\n\n### Solution Code\n```python\ndef map_agent_environment(agent_name: str):\n    \"\"\"Return the environment name and access type that should be used for a given RL agent.\n\n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g. \"dqn\", \"A3C\", etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n    \"\"\"\n    processed_name = agent_name.strip().lower()\n    agent_map = {\n        'dqn': ('CartPole-v0', 'unwrapped'),\n        'ddpg': ('Pendulum-v0', 'env'),\n        'a3c': ('Pendulum-v0', 'unwrapped'),\n        'a2c': ('CartPole-v0', 'env'),\n        'a2c_multi': ('CartPole-v0', 'raw'),\n        'trpo': ('Pendulum-v0', 'unwrapped')\n    }\n    return agent_map.get(processed_name, -1)\n```\n\n### Explanation\n- **Input Processing**: The function starts by stripping whitespace from the input string and converting it to lowercase to handle case insensitivity.\n- **Mapping Creation**: A dictionary `agent_map` is defined where each key is a lowercase agent name, and each value is a tuple containing the environment name and access type.\n- **Lookup and Return**: The function uses the `get` method of the dictionary to check if the processed agent name exists. If it does, the corresponding tuple is returned. If not, -1 is returned, indicating an unrecognized agent.\n\nThis approach ensures that the function efficiently maps agent names to their respective environments and access types while handling case insensitivity and ignoring extraneous whitespace."}
{"task_id": 328, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that scans through a list of words (corpus) and returns the first word whose first character is an uppercase letter. If no such word exists, the function should return an empty list.\n\n### Approach\n1. **Iterate through each word in the corpus**: We will loop through each word in the given list of words.\n2. **Check for non-empty words**: For each word, we first check if it is non-empty to avoid any index errors when accessing the first character.\n3. **Check the first character**: For each non-empty word, we check if the first character is an uppercase letter using the `isupper()` method.\n4. **Return the first valid word**: If a word meets the criteria, we return it in a list. If no such word is found after checking all words, we return an empty list.\n\nThis approach ensures that we efficiently find the first capitalized word or determine that none exists.\n\n### Solution Code\n```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n    \"\"\"Find the first capitalized word in *corpus* and return it in a list.\n\n    A *capitalized* word is one whose very first character is an uppercase\n    letter. If no word in the corpus meets this condition, return an empty\n    list instead.\n\n    Args:\n        corpus: List of candidate words.\n\n    Returns:\n        List containing the first capitalized word, or an empty list if none\n        exists.\n    \"\"\"\n    for word in corpus:\n        if word and word[0].isupper():\n            return [word]\n    return []\n```\n\n### Explanation\n- **Iteration**: The function iterates through each word in the input list `corpus`.\n- **Non-empty Check**: For each word, it checks if the word is non-empty to avoid index errors.\n- **Uppercase Check**: It then checks if the first character of the word is uppercase using `isupper()`.\n- **Return**: If a word meets the criteria, it is returned in a list. If no such word is found, an empty list is returned.\n\nThis solution efficiently handles all edge cases, including empty words and words with leading whitespace, ensuring robust performance across various inputs."}
{"task_id": 364, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the mathematical sign (or signum) function. This function takes a real number as input and returns 1 if the number is positive, -1 if the number is negative, and 0 if the number is zero. The solution must handle both integers and floating-point numbers efficiently.\n\n### Approach\nThe approach to solve this problem involves using conditional statements to check the value of the input number. Here's a step-by-step breakdown of the approach:\n\n1. **Check if the number is positive**: If the input number is greater than zero, the function returns 1.\n2. **Check if the number is negative**: If the input number is less than zero, the function returns -1.\n3. **Handle zero**: If the number is neither positive nor negative, it must be zero, so the function returns 0.\n\nThis approach ensures that all possible cases for the input number are covered efficiently and correctly.\n\n### Solution Code\n```python\ndef sign(x: int | float) -> int:\n    \"\"\"Return the sign of *x*.\n\n    The function should return 1 for positive numbers, -1 for negative numbers,\n    and 0 when the input is exactly zero.\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        -1, 0, or 1 depending on the sign of *x*.\n    \"\"\"\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\nThe function `sign` takes a single argument `x`, which can be either an integer or a floating-point number. The function uses a series of conditional checks to determine the sign of `x`:\n\n1. **Positive Check**: The first condition checks if `x` is greater than zero. If true, the function returns 1.\n2. **Negative Check**: The second condition checks if `x` is less than zero. If true, the function returns -1.\n3. **Zero Check**: If neither of the above conditions is met, it means `x` is zero, and the function returns 0.\n\nThis approach ensures that the function handles all possible values of `x` correctly and efficiently, providing the correct signum value for any real number input."}
{"task_id": 378, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that acts as an indicator function, returning 1 if the input flag is exactly 1, and 0 for any other value. This function is straightforward and can be efficiently implemented using a simple conditional check.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Check the Input Value**: The function takes an integer as input. We need to check if this integer is exactly 1.\n2. **Return the Appropriate Value**: If the input integer is 1, the function returns 1. For any other value, it returns 0.\n\nThis can be efficiently implemented using a one-liner with a conditional expression, which makes the code concise and easy to read.\n\n### Solution Code\n```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```\n\n### Explanation\n- **Function Definition**: The function `indicator` is defined to take a single parameter `flag` of type integer.\n- **Return Statement**: The return statement uses a conditional expression to check if `flag` is 1. If true, it returns 1; otherwise, it returns 0.\n- **Efficiency**: This approach is efficient with a time complexity of O(1) since it involves a single comparison operation.\n\nThis solution is optimal for the given problem as it directly addresses the requirement with minimal computational overhead."}
{"task_id": 379, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to sort a list of nodes based on their priority and entry_id, and then return a list of their keys in the correct order. Each node is represented as a dictionary with specific keys, and the sorting must follow a specific set of rules.\n\n### Approach\n1. **Problem Analysis**: Each node has a 'priority' and an 'entry_id'. The nodes must be sorted such that:\n   - Nodes with smaller priority values come first.\n   - If two nodes have the same priority, the node with the smaller entry_id comes first.\n\n2. **Sorting Strategy**: We can use Python's built-in `sorted()` function, which allows us to specify a custom key for sorting. The key will be a tuple consisting of the node's priority and entry_id. This ensures that the nodes are sorted first by priority and then by entry_id if priorities are equal.\n\n3. **Extracting Keys**: After sorting the nodes, we extract the 'key' from each node and return them in a list.\n\n### Solution Code\n```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n    \"\"\"Sort the given priority-queue nodes and return their keys.\n\n    Two nodes are ordered as follows:\n      1. The one with the smaller ``priority`` value comes first.\n      2. If priorities are equal, the node with the smaller ``entry_id``\n         comes first.\n\n    Args:\n        nodes: A list of dictionaries.  Each dictionary must contain the\n                fields ``'key'``, ``'val'``, ``'priority'`` and\n                ``'entry_id'``.\n\n    Returns:\n        A list of ``key`` values ordered from highest to lowest priority\n        according to the rule above.\n    \"\"\"\n    sorted_nodes = sorted(nodes, key=lambda x: (x['priority'], x['entry_id']))\n    return [node['key'] for node in sorted_nodes]\n```\n\n### Explanation\n- **Sorting**: The `sorted()` function is used with a lambda function as the key. This lambda function returns a tuple `(priority, entry_id)` for each node, ensuring that nodes are sorted by priority first and then by entry_id if priorities are equal.\n- **Extracting Keys**: After sorting, we create a list comprehension that extracts the 'key' from each node in the sorted list and returns this list.\n\nThis approach efficiently sorts the nodes according to the given rules and handles all edge cases, including an empty input list."}
{"task_id": 399, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Euclidean distance matrix between two sets of vectors. The goal is to create a matrix where each element represents the Euclidean distance between corresponding points from the two sets. If the input arrays are not valid (i.e., not 2D or have different numbers of columns), we return -1.\n\n### Approach\n1. **Input Validation**: Check if both input arrays are 2D and have the same number of columns. If not, return -1.\n2. **Compute Squared Norms**: Calculate the squared Euclidean norms for each row in both arrays. This helps in efficiently computing the distances using vectorized operations.\n3. **Dot Product Matrix**: Compute the dot product matrix between the two arrays to facilitate the efficient calculation of pairwise distances.\n4. **Squared Distances**: Use the formula for squared Euclidean distance, which is derived from the norms and the dot product matrix.\n5. **Square Root and Rounding**: Take the square root of the squared distances to get the actual Euclidean distances and round each value to four decimal places.\n6. **Conversion to List**: Convert the resulting NumPy array to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # Check if X and Y are 2-dimensional\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    \n    # Check if the number of columns (d) is the same for both X and Y\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    # Compute squared norms for each row in X and Y\n    X_squared = np.sum(X ** 2, axis=1)\n    Y_squared = np.sum(Y ** 2, axis=1)\n    \n    # Compute the dot product matrix\n    X_dot_YT = X.dot(Y.T)\n    \n    # Compute squared distances using the formula\n    squared_dist = X_squared[:, np.newaxis] + Y_squared - 2 * X_dot_YT\n    \n    # Compute the square root to get the Euclidean distances\n    dist = np.sqrt(squared_dist)\n    \n    # Round to four decimal places\n    rounded_dist = np.round(dist, 4)\n    \n    # Convert to a list of lists\n    return rounded_dist.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if both input arrays are 2D and have the same number of columns. If either condition is not met, it returns -1.\n2. **Squared Norms**: The squared norms of each row in both arrays are computed to avoid recalculating these values multiple times.\n3. **Dot Product Matrix**: The dot product of each row in X with each row in Y is computed efficiently using matrix multiplication.\n4. **Squared Distances**: Using the formula for squared Euclidean distance, the distances are computed in a vectorized manner, which is efficient and avoids explicit loops.\n5. **Square Root and Rounding**: The square root of the squared distances gives the actual Euclidean distances, which are then rounded to four decimal places for precision.\n6. **Conversion**: The resulting NumPy array is converted to a list of lists to meet the output requirements.\n\nThis approach ensures that the solution is both efficient and easy to understand, leveraging NumPy's capabilities for vectorized operations to handle potentially large input sizes effectively."}
{"task_id": 420, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate two Boolean masks from a given 2-D NumPy array based on a specified column and split value. These masks will help in splitting the data for tree-based machine learning algorithms.\n\n### Approach\n1. **Input Validation**: Check if the provided column index is within the valid range of the array's columns. If not, return two empty lists.\n2. **Extract Column Data**: If the column index is valid, extract the specified column from the array.\n3. **Generate Masks**: Create two Boolean masks:\n   - `left_mask`: True for rows where the column value is strictly less than the split value.\n   - `right_mask`: True for rows where the column value is greater than or equal to the split value.\n4. **Return Masks**: Convert the Boolean arrays to Python lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n    \"\"\"Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.\n    \"\"\"\n    if column < 0 or column >= X.shape[1]:\n        return ([], [])\n    \n    col = X[:, column]\n    left_mask = (col < value).tolist()\n    right_mask = (col >= value).tolist()\n    \n    return (left_mask, right_mask)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the column index is within the valid range. If not, it returns empty lists to indicate an invalid column.\n- **Column Extraction**: Using NumPy's slicing, the specified column is extracted from the array.\n- **Mask Generation**: Using element-wise comparisons, two Boolean arrays are created. These arrays are then converted to Python lists using the `tolist()` method to ensure compatibility with non-NumPy code.\n- **Return**: The function returns a tuple containing the two Boolean masks, which can be used to split the data as required by tree-based algorithms."}
{"task_id": 424, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of uncertainty or impurity in a set of data, which is particularly useful in machine learning for decision tree algorithms.\n\n### Approach\nThe Shannon entropy \\(H\\) is defined as:\n\\[\nH = -\\sum_{i=1}^{k} p_i \\log_2 p_i\n\\]\nwhere \\(p_i\\) is the relative frequency of each class label \\(i\\).\n\nThe steps to compute the Shannon entropy are as follows:\n1. **Count Frequencies**: Use numpy's `unique` function to get the unique class labels and their respective counts.\n2. **Compute Probabilities**: Convert these counts into probabilities by dividing each count by the total number of elements in the list.\n3. **Calculate Entropy**: For each probability, compute the term \\(p_i \\log_2 p_i\\), sum these terms, and then take the negative of this sum to get the entropy.\n4. **Round the Result**: Round the computed entropy to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy of the sequence, rounded to 4 decimal places.\n    \"\"\"\n    unique_labels, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    sum_terms = np.sum(probabilities * np.log2(probabilities))\n    H = -sum_terms\n    return round(H, 4)\n```\n\n### Explanation\n1. **Counting Frequencies**: The `np.unique` function is used to find the unique class labels and their counts. This helps in determining how many times each label appears in the list.\n2. **Probabilities Calculation**: Each count is divided by the total number of elements to get the probability of each label. This step converts the counts into a probability distribution.\n3. **Entropy Calculation**: Using the probabilities, we compute each term \\(p_i \\log_2 p_i\\), sum these terms, and then take the negative of the sum to get the entropy. This step follows the mathematical definition of Shannon entropy.\n4. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements, ensuring precision and consistency in the output.\n\nThis approach efficiently computes the Shannon entropy using numpy for numerical operations, ensuring accuracy and performance."}
{"task_id": 429, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a helper function that acts as an oracle for a contextual Bernoulli bandit problem. The function will determine the optimal expected reward and the corresponding arm index based on a given context. If the context is invalid, the function will return a specific error tuple.\n\n### Approach\n1. **Validation of Context**: The function first checks if the provided context is a valid one-hot vector. A valid one-hot vector must be one-dimensional, have the same length as the number of contexts in `context_probs`, sum to exactly 1, and contain only 0s and 1s.\n2. **Determine Current Context**: If the context is valid, the function identifies the current context by finding the index of the 1 in the one-hot vector.\n3. **Find Optimal Arm**: Using the identified context, the function retrieves the corresponding row from `context_probs` and determines the arm with the highest success probability. If multiple arms have the same maximum probability, the smallest index is chosen.\n4. **Return Result**: The function returns the optimal expected reward (rounded to four decimal places) and the index of the optimal arm.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n    \"\"\"Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm.  Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, -1).\n    \"\"\"\n    # Check if context is a 1D array\n    if context.ndim != 1:\n        return (-1, -1)\n    \n    D = context_probs.shape[0]\n    \n    # Check if context has the correct length\n    if context.size != D:\n        return (-1, -1)\n    \n    # Check if context is a valid one-hot vector\n    if np.sum(context) != 1:\n        return (-1, -1)\n    \n    if not np.all(np.isin(context, [0, 1])):\n        return (-1, -1)\n    \n    # Determine the current context index\n    current_context = np.argmax(context)\n    \n    # Extract the corresponding row from context_probs\n    row = context_probs[current_context]\n    \n    # Find the optimal arm (smallest index if there are ties)\n    optimal_arm = np.argmax(row)\n    optimal_rwd = row[optimal_arm]\n    \n    # Round the reward to four decimal places\n    optimal_rwd = np.round(optimal_rwd, 4)\n    \n    return (optimal_rwd, optimal_arm)\n```\n\n### Explanation\n1. **Validation Checks**: The function starts by validating the context to ensure it meets the criteria of a one-hot vector. This includes checking the dimensionality, length, sum, and element values.\n2. **Context Identification**: The index of the 1 in the context vector is found using `np.argmax`, which gives the current context.\n3. **Optimal Arm Calculation**: The corresponding row in `context_probs` for the current context is retrieved. The arm with the highest success probability is identified using `np.argmax`, which also handles ties by selecting the smallest index.\n4. **Result Formatting**: The optimal reward is rounded to four decimal places and returned along with the optimal arm index.\n\nThis approach ensures that the function efficiently and correctly identifies the optimal action in a contextual bandit setting, adhering to the constraints and requirements specified."}
