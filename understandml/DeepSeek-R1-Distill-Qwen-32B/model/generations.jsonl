{"task_id": 0, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Ridge Regression using its closed-form solution. Ridge Regression is a technique used to prevent overfitting in linear regression by adding a regularization term to the loss function. This regularization term helps in reducing the complexity of the model by penalizing large coefficients.\n\n### Approach\n1. **Split Data**: Separate the input data into features (X) and the target variable (y). The last column of the input data is the target variable, and the preceding columns are the features.\n2. **Add Bias Term**: Add a column of ones to the features matrix to account for the intercept term in the linear regression model.\n3. **Compute Matrix**: Calculate the matrix \\(\\mathbf{X}^\\top\\mathbf{X}\\) and add the regularization term \\(\\alpha\\mathbf{I}\\), where \\(\\mathbf{I}\\) is the identity matrix of appropriate size.\n4. **Check Invertibility**: Attempt to invert the matrix \\(\\mathbf{X}^\\top\\mathbf{X} + \\alpha\\mathbf{I}\\). If the matrix is not invertible, return -1.\n5. **Compute Weights**: If the matrix is invertible, compute the weight vector using the closed-form solution.\n6. **Round and Return**: Round each coefficient to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Implements Ridge Regression using the closed-form solution.\n    \n    Args:\n        data: An ndarray where the last column is the target y and the preceding columns are features X.\n        alpha: Regularization strength, default is 0.0.\n        \n    Returns:\n        A list of floats representing the weight vector rounded to 4 decimal places, or -1 if the matrix is not invertible.\n    \"\"\"\n    # Split the data into features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add a bias term (intercept) to the features\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    \n    # Compute X^T X\n    XTX = np.dot(X.T, X)\n    \n    # Create the identity matrix scaled by alpha\n    I = np.eye(XTX.shape[0])\n    matrix = XTX + alpha * I\n    \n    # Attempt to invert the matrix\n    try:\n        inv_matrix = np.linalg.inv(matrix)\n    except np.linalg.LinAlgError:\n        return -1  # Return -1 if matrix is not invertible\n    \n    # Compute X^T y\n    Xy = np.dot(X.T, y)\n    \n    # Compute the weights\n    weights = np.dot(inv_matrix, Xy)\n    \n    # Round each weight to 4 decimal places and convert to a list\n    rounded_weights = [round(w, 4) for w in weights]\n    \n    return rounded_weights\n```\n\n### Explanation\n1. **Splitting Data**: The input data is split into features (X) and the target variable (y). This is done by taking all columns except the last as features and the last column as the target.\n2. **Adding Bias Term**: A column of ones is added to the features matrix to include the intercept term in the model.\n3. **Matrix Computation**: The matrix \\(\\mathbf{X}^\\top\\mathbf{X}\\) is computed, and the regularization term \\(\\alpha\\mathbf{I}\\) is added to it. This helps in preventing overfitting by penalizing large coefficients.\n4. **Invertibility Check**: The matrix is inverted using numpy's `linalg.inv` function. If the matrix is singular (non-invertible), a `LinAlgError` is caught, and the function returns -1.\n5. **Weight Calculation**: The weight vector is computed using the closed-form solution, which involves matrix inversion and multiplication.\n6. **Rounding and Returning**: The computed weights are rounded to four decimal places and returned as a list.\n\nThis approach ensures that we efficiently compute the Ridge Regression coefficients while handling potential numerical issues related to matrix inversion."}
{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the dual-form perceptron learning algorithm. This algorithm is a variant of the classic perceptron algorithm, which is used to find a linear classifier for a given dataset. The dual-form perceptron expresses the weight vector as a linear combination of the training samples, making it a foundational step towards kernel methods.\n\n### Approach\nThe dual-form perceptron algorithm works by iteratively updating the dual parameters (\u03b1) and the bias (b) until the dataset is correctly classified or a maximum number of updates is reached. Here's a detailed breakdown of the approach:\n\n1. **Initialization**: Start with zero values for the dual parameters (\u03b1) and the bias (b).\n2. **Gram Matrix**: Compute the Gram matrix, which is the matrix of inner products of the training samples. This matrix is used to avoid recomputing inner products repeatedly.\n3. **Iterative Update**: For each sample, compute the activation and check the margin. If the margin is non-positive, update the dual parameters and bias, then restart the iteration from the beginning.\n4. **Stopping Condition**: The algorithm stops when a full pass over all samples is made without any updates or after a specified number of updates.\n5. **Compute Weight Vector**: After training, compute the weight vector from the final dual parameters.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples = X_train.shape[0]\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    K = X_train @ X_train.T  # Compute the Gram matrix\n\n    updates = 0\n    while updates < n_iter:\n        updated = False\n        for i in range(n_samples):\n            # Compute activation for the i-th sample\n            activation = np.dot(alpha * y_train, K[:, i])\n            margin = y_train[i] * (activation + b)\n            if margin <= 0:\n                # Update alpha and b\n                alpha[i] += eta\n                b += eta * y_train[i]\n                updates += 1\n                updated = True\n                # Restart the scan from the beginning\n                break\n        if not updated:\n            break  # No updates in this pass, exit\n\n    # Compute the primal weight vector w\n    w = np.sum(alpha * y_train[:, np.newaxis] * X_train, axis=0)\n    # Round to four decimal places\n    w = [round(num, 4) for num in w]\n    b = round(b, 4)\n    return (w, b)\n```\n\n### Explanation\n1. **Initialization**: The dual parameters (\u03b1) and bias (b) are initialized to zero.\n2. **Gram Matrix**: The Gram matrix is computed once at the beginning to store all inner products of the training samples, which avoids redundant calculations.\n3. **Iterative Update**: For each sample, the activation is computed using the current dual parameters and the Gram matrix. If the margin (which is the product of the label and the activation plus bias) is non-positive, the dual parameters and bias are updated, and the iteration restarts from the beginning.\n4. **Stopping Condition**: The loop continues until either the maximum number of updates is reached or a full pass over all samples is made without any updates.\n5. **Weight Vector Calculation**: After training, the weight vector is computed as a linear combination of the training samples weighted by the dual parameters and labels. The result is rounded to four decimal places for precision.\n\nThis approach ensures that the algorithm efficiently finds a linear classifier or determines that the data is not linearly separable within the given constraints."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that returns a dictionary of Generalized Linear Model (GLM) link functions. Each link function includes the link function itself, its inverse, and its first derivative. These functions must work with both numpy scalars and numpy arrays, performing element-wise operations.\n\n### Approach\nThe problem requires implementing three specific link functions: identity, log, and logit. Each of these functions must be able to handle numpy arrays and scalars. The approach involves defining each link function, its inverse, and its derivative using numpy operations to ensure element-wise processing.\n\n1. **Identity Link**: \n   - The link function is the identity function, which returns the input as is.\n   - The inverse function is also the identity function.\n   - The derivative of the identity function is 1 for all inputs.\n\n2. **Log Link**:\n   - The link function is the natural logarithm.\n   - The inverse function is the exponential function.\n   - The derivative of the log function is the reciprocal of the input.\n\n3. **Logit Link**:\n   - The link function is the logit function, which is the logarithm of the odds.\n   - The inverse function is the logistic function.\n   - The derivative of the logit function is the product of the input and its complement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    links = {\n        'identity': {\n            'link': lambda mu: mu,\n            'inv_link': lambda eta: eta,\n            'derivative': lambda mu: np.ones_like(mu)\n        },\n        'log': {\n            'link': np.log,\n            'inv_link': np.exp,\n            'derivative': lambda mu: 1 / mu\n        },\n        'logit': {\n            'link': lambda mu: np.log(mu / (1 - mu)),\n            'inv_link': lambda eta: 1 / (1 + np.exp(-eta)),\n            'derivative': lambda mu: 1 / (mu * (1 - mu))\n        }\n    }\n    return links\n```\n\n### Explanation\nThe function `glm_links` constructs a dictionary with three keys: 'identity', 'log', and 'logit'. Each key maps to another dictionary containing the link function, its inverse, and its derivative.\n\n- **Identity Link**: \n  - `link`: Returns the input as is.\n  - `inv_link`: Returns the input as is.\n  - `derivative`: Returns an array of ones with the same shape as the input.\n\n- **Log Link**:\n  - `link`: Applies the natural logarithm to the input.\n  - `inv_link`: Applies the exponential function to the input.\n  - `derivative`: Returns the reciprocal of the input.\n\n- **Logit Link**:\n  - `link`: Computes the logit of the input, which is the logarithm of the odds.\n  - `inv_link`: Computes the logistic function of the input.\n  - `derivative`: Computes the derivative of the logit function, which is the product of the input and its complement.\n\nThis implementation ensures that all functions work efficiently with numpy arrays and scalars, providing the necessary element-wise operations for GLM computations."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to build an isolation tree for a given data matrix and compute the depth of each sample's leaf node. The depth is defined as the number of edges from the root to the leaf node where the sample ends up.\n\n### Approach\n1. **Tree Construction**: The tree is built recursively. Each node receives a set of row indices. If fewer than three samples reach a node, it becomes a leaf. Otherwise, the node splits the data based on a randomly chosen feature and a randomly chosen split value within the range of that feature's values in the current node.\n2. **Traversal and Depth Calculation**: After constructing the tree, each sample is traversed from the root to the leaf node, counting the number of edges (depth) taken to reach the leaf.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n    n_samples = data.shape[0]\n    if n_samples == 0:\n        return []\n    \n    class Node:\n        def __init__(self):\n            self.is_leaf = False\n            self.left = None\n            self.right = None\n            self.feature = None\n            self.value = None\n            self.indices = None\n    \n    def build_node(indices):\n        if len(indices) < 3:\n            node = Node()\n            node.is_leaf = True\n            node.indices = indices\n            return node\n        else:\n            f = np.random.choice(data.shape[1])\n            min_val = data[indices, f].min()\n            max_val = data[indices, f].max()\n            v = np.random.uniform(min_val, max_val)\n            left_indices = []\n            right_indices = []\n            for idx in indices:\n                if data[idx, f] <= v:\n                    left_indices.append(idx)\n                else:\n                    right_indices.append(idx)\n            node = Node()\n            node.feature = f\n            node.value = v\n            node.left ="}
{"task_id": 11, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm from scratch without relying on any external machine learning libraries. The goal is to cluster the given data into `k` groups based on the K-Means algorithm, which involves iteratively choosing initial centers, assigning samples to the nearest center, recomputing the centers, and stopping when convergence is achieved or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start by choosing the first `k` samples of the input data as the initial cluster centers.\n2. **Assignment**: For each sample, compute the squared Euclidean distance to each cluster center and assign the sample to the nearest center.\n3. **Update Centers**: Recompute each cluster center as the mean of all samples assigned to it. If a cluster becomes empty, reinitialize its center with a random sample from the dataset.\n4. **Convergence Check**: Stop the iterations when the maximum change in any cluster center is less than a specified threshold (`epsilon`) or when the maximum number of iterations (`max_iter`) is reached.\n5. **Rounding and Return**: After convergence, round each coordinate of the cluster centers to 4 decimal places and return them along with the cluster labels for each sample.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n    centers = data[:k].copy()\n    \n    for _ in range(max_iter):\n        # Compute squared distances from each sample to each center\n        distances = np.sum((data[:, np.newaxis, :] - centers) ** 2, axis=2)\n        # Assign each sample to the nearest center\n        assignments = np.argmin(distances, axis=1)\n        \n        # Compute new centers\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            mask = (assignments == j)\n            if np.sum(mask) == 0:\n                # Reinitialize with a random sample if cluster is empty\n                random_idx = np.random.randint(n_samples)\n                new_centers[j] = data[random_idx]\n            else:\n                new_centers[j] = np.mean(data[mask], axis=0)\n        \n        # Check for convergence\n        max_change = np.max(np.linalg.norm(centers - new_centers, axis=1))\n        if max_change < epsilon:\n            centers = new_centers\n            break\n        else:\n            centers = new_centers.copy()\n    \n    # Round the centers to 4 decimal places\n    centers_rounded = np.round(centers, decimals=4)\n    centers_list = centers_rounded.tolist()\n    assignments_list = assignments.tolist()\n    \n    return (centers_list, assignments_list)\n```\n\n### Explanation\n1. **Initialization**: The initial cluster centers are set to the first `k` samples of the input data.\n2. **Distance Calculation**: For each sample, the squared Euclidean distance to each cluster center is computed using vectorized operations for efficiency.\n3. **Assignment**: Each sample is assigned to the nearest cluster center based on the computed distances.\n4. **Center Update**: New cluster centers are computed as the mean of the samples assigned to each cluster. If a cluster becomes empty, a random sample is chosen to reinitialize the center.\n5. **Convergence Check**: The algorithm stops when the maximum change in cluster centers is below the threshold or the maximum number of iterations is reached.\n6. **Result Preparation**: The final cluster centers are rounded to 4 decimal places and returned along with the cluster assignments for each sample.\n\nThis approach ensures that the K-Means algorithm is implemented efficiently and correctly, adhering to the specified constraints and requirements."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the sigmoid activation function and its gradient for a given input. The function should handle different input types, including scalars, lists, and NumPy arrays, and return the results in the appropriate format.\n\n### Approach\n1. **Sigmoid Function**: The sigmoid function maps any real-valued input to a value between 0 and 1. It is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n2. **Gradient Calculation**: The gradient of the sigmoid function is given by \\( \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\).\n3. **Input Handling**: The function should handle different input types:\n   - **Scalars**: If the input is an integer or float, the function returns the sigmoid and gradient as floats.\n   - **Arrays and Lists**: If the input is a list or a NumPy array, the function processes it as an array, computes the sigmoid and gradient element-wise, and returns the results as lists.\n4. **Rounding**: All results are rounded to 4 decimal places. For array-like inputs, the results are converted to Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    if isinstance(x, (int, float)) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        # Scalar case\n        sigmoid = 1.0 / (1.0 + np.exp(-x))\n        gradient = sigmoid * (1 - sigmoid)\n        sigmoid_rounded = round(sigmoid, 4)\n        gradient_rounded = round(gradient, 4)\n        return (sigmoid_rounded, gradient_rounded)\n    else:\n        # Array-like case\n        if isinstance(x, list):\n            x = np.array(x)\n        sigmoid_x = 1.0 / (1.0 + np.exp(-x))\n        gradient_x = sigmoid_x * (1 - sigmoid_x)\n        sigmoid_rounded = np.round(sigmoid_x, 4)\n        gradient_rounded = np.round(gradient_x, 4)\n        sigmoid_list = sigmoid_rounded.tolist()\n        gradient_list = gradient_rounded.tolist()\n        return (sigmoid_list, gradient_list)\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar (int, float, or 0-dimensional NumPy array), the function computes the sigmoid and gradient directly and returns them as floats rounded to 4 decimal places.\n- **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array if necessary, computes the sigmoid and gradient element-wise, rounds each element to 4 decimal places, and converts the results back to Python lists.\n- **Efficiency**: The use of NumPy ensures efficient computation, especially for large arrays, by leveraging vectorized operations.\n\nThis approach ensures that the function is versatile and handles various input types efficiently while providing the required output format."}
{"task_id": 21, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using one-dimensional decision stumps as weak learners. The goal is to create a strong classifier by combining multiple weak classifiers, each of which is a decision stump. The algorithm iteratively trains weak classifiers, updates sample weights, and combines them until the training error is below a specified threshold.\n\n### Approach\n1. **Sort the Training Data**: The training data is sorted based on the feature values to facilitate the generation of decision stumps.\n2. **Initialize Sample Weights**: Each sample is initially given an equal weight.\n3. **Generate Decision Stumps**: For each pair of consecutive points in the sorted training data, generate two decision stumps (one for each direction, d=0 and d=1).\n4. **Select the Best Stump**: For each generated stump, compute the weighted error. The stump with the minimum error is selected as the next weak learner.\n5. **Update Sample Weights**: Adjust the sample weights based on the performance of the selected stump. Samples that were misclassified have their weights increased, while correctly classified samples have their weights decreased.\n6. **Check Training Error**: After each iteration, compute the training error of the current ensemble. If the error is below the specified threshold, stop the training.\n7. **Make Predictions**: Use the trained ensemble to make predictions on the test set.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\"\"\"\n    # Sort the training data based on x values\n    sorted_data = sorted(zip(x_train, y_train), key=lambda x: x[0])\n    x_sorted = [x for x, y in sorted_data]\n    y_sorted = [y for x, y in sorted_data]\n    n = len(x_sorted)\n    \n    # Initialize sample weights\n    sample_weights = [1.0 / n] * n\n    \n    # Initialize the ensemble\n    ensemble = []\n    \n    while True:\n        # Generate all possible stumps\n        current_stumps = []\n        for i in range(n - 1):\n            theta = (x_sorted[i] + x_sorted[i + 1]) / 2\n            current_stumps.append((0, theta))\n            current_stumps.append((1, theta))\n        \n        # Find the best stump\n        min_error = float('inf')\n        best_stump = None\n        for (d, theta) in current_stumps:\n            error = 0.0\n            for i in range(n):\n                x = x_sorted[i]\n                y = y_sorted[i]\n                w = sample_weights[i]\n                if d == 0:\n                    h = 1 if x <= theta else -1\n                else:\n                    h = 1 if x > theta else -1\n                if h != y:\n                    error += w\n            if error < min_error:\n                min_error = error\n                best_stump = (d, theta)\n        \n        # Compute alpha\n        if min_error == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * math.log((1 - min_error) / min_error)\n        \n        # Update sample weights\n        new_weights = []\n        Z = 0.0\n        for i in range(n):\n            x = x_sorted[i]\n            y = y_sorted[i]\n            w = sample_weights[i]\n            d, theta = best_stump\n            if d == 0:\n                h = 1 if x <= theta else -1\n            else:\n                h = 1 if x > theta else -1\n            if h != y:\n                factor = math.exp(alpha)\n            else:\n                factor = math.exp(-alpha)\n            new_w = w * factor\n            new_weights.append(new_w)\n            Z += new_w\n        \n        # Normalize the weights\n        if Z == 0:\n            sample_weights = [0.0] * n\n        else:\n            sample_weights = [w / Z for w in new_weights]\n        \n        # Add the best stump to the ensemble\n        ensemble.append((best_stump[0], best_stump[1], alpha))\n        \n        # Compute current training error\n        count = 0\n        for i in range(n):\n            x = x_sorted[i]\n            y = y_sorted[i]\n            sum_h = 0.0\n            for (d_i, theta_i, alpha_i) in ensemble:\n                if d_i == 0:\n                    h = 1 if x <= theta_i else -1\n                else:\n                    h = 1 if x > theta_i else -1\n                sum_h += alpha_i * h\n            prediction = 1 if sum_h >= 0 else -1\n            if prediction != y:\n                count += 1\n        training_error = count / n\n        \n        if training_error <= epsilon:\n            break\n    \n    # Make predictions on the test set\n    predictions = []\n    for x in x_test:\n        sum_h = 0.0\n        for (d_i, theta_i, alpha_i) in ensemble:\n            if d_i == 0:\n                h = 1 if x <= theta_i else -1\n            else:\n                h = 1 if x > theta_i else -1\n            sum_h += alpha_i * h\n        prediction = 1 if sum_h >= 0 else -1\n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation\n1. **Sorting and Initialization**: The training data is sorted to facilitate the generation of decision stumps. Each sample is initialized with equal weight.\n2. **Stump Generation**: For each pair of consecutive points in the sorted data, two stumps are generated, one for each direction (d=0 and d=1).\n3. **Error Calculation**: For each stump, the weighted error is calculated. The stump with the minimum error is selected.\n4. **Weight Update**: Sample weights are updated based on the performance of the selected stump. Weights are normalized to ensure they sum to 1.\n5. **Training Error Check**: After each iteration, the training error is computed. If it falls below the specified threshold, the training stops.\n6. **Prediction**: The trained ensemble is used to make predictions on the test set by combining the outputs of all weak classifiers.\n\nThis approach ensures that the AdaBoost algorithm efficiently combines weak classifiers to form a strong classifier, achieving the desired training error threshold."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the class labels for test samples using a pre-trained Support Vector Machine (SVM) with a Gaussian (Radial Basis Function - RBF) kernel. The decision function for each test sample is computed using the training data, Lagrange multipliers, bias term, and kernel hyper-parameter.\n\n### Approach\n1. **Compute Squared Norms**: Calculate the squared Euclidean norms of the training and test samples. This helps in efficiently computing the squared distances between each training sample and each test sample.\n2. **Dot Products**: Compute the dot products between each training sample and each test sample. This is used to efficiently calculate the squared distances.\n3. **Squared Distances**: Using the squared norms and dot products, compute the squared Euclidean distances between each training sample and each test sample.\n4. **Kernel Matrix**: Apply the Gaussian kernel function to the squared distances to get the kernel matrix, which measures the similarity between samples.\n5. **Decision Function**: Compute the decision function for each test sample using the Lagrange multipliers, class labels, kernel matrix, and bias term.\n6. **Predict Labels**: Determine the predicted class labels based on the sign of the decision function values.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\"\"\"\n    # Compute squared norms of training and test samples\n    x_norms = np.sum(X_train ** 2, axis=1)\n    z_norms = np.sum(X_test ** 2, axis=1)\n    \n    # Compute dot products between training and test samples\n    dot_products = X_train.dot(X_test.T)\n    \n    # Calculate squared distances using the expanded form\n    dist_sq = x_norms[:, np.newaxis] + z_norms - 2 * dot_products\n    \n    # Compute the Gaussian kernel matrix\n    K = np.exp(-gamma * dist_sq)\n    \n    # Multiply alpha with corresponding y_train values\n    alpha_y = alpha * y_train\n    \n    # Compute the decision function for each test sample\n    g = alpha_y.dot(K) + b\n    \n    # Determine the predicted labels based on the sign of g\n    signs = np.where(g >= 0, 1, -1)\n    \n    return signs.tolist()\n```\n\n### Explanation\n1. **Squared Norms**: The squared norms of the training and test samples are computed to facilitate the efficient calculation of squared distances.\n2. **Dot Products**: The dot products between training and test samples are computed to avoid redundant calculations when determining distances.\n3. **Squared Distances**: Using the squared norms and dot products, the squared distances between each training and test sample are computed efficiently.\n4. **Kernel Matrix**: The Gaussian kernel function transforms these squared distances into similarity scores, which are stored in a matrix.\n5. **Decision Function**: The decision function for each test sample is computed by taking the weighted sum of the kernel values (using Lagrange multipliers and class labels) and adding the bias term.\n6. **Predict Labels**: The sign of the decision function values determines the predicted class labels, ensuring each label is either 1 or -1.\n\nThis approach ensures that the solution is both efficient and scalable, leveraging vectorized operations to handle arbitrary numbers of training and test samples."}
{"task_id": 28, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the optimal rank-k reconstruction of a given matrix using truncated Singular Value Decomposition (SVD). This approach is equivalent to finding the best linear autoencoder with k latent units, minimizing the mean squared error (MSE) between the original and reconstructed matrices.\n\n### Approach\n1. **Input Validation**: Check if the given integer k is within the valid range [1, min(m, n)], where m and n are the dimensions of the input matrix X. If k is outside this range, return -1.\n2. **Matrix Conversion**: Convert the input list of lists into a numpy array for efficient numerical operations.\n3. **SVD Computation**: Perform Singular Value Decomposition (SVD) on the matrix to obtain the matrices U, S (singular values), and Vh (conjugate transpose of V).\n4. **Truncated SVD**: Retain only the first k singular values and the corresponding columns of U and rows of Vh to form the truncated matrices U_k, S_k, and Vh_k.\n5. **Reconstruction**: Compute the reconstructed matrix using the truncated SVD components.\n6. **MSE Calculation**: Calculate the mean squared error between the original matrix and the reconstructed matrix.\n7. **Result Formatting**: Round the reconstructed matrix and the MSE to four decimal places and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    m = len(X)\n    if m == 0:\n        return -1\n    n = len(X[0])\n    min_dim = min(m, n)\n    if k < 1 or k > min_dim:\n        return -1\n    \n    # Convert X to numpy array\n    X_np = np.array(X)\n    \n    # Compute SVD\n    U, s, Vh = np.linalg.svd(X_np, full_matrices=True)\n    \n    # Truncate to k components\n    U_k = U[:, :k]\n    S_k = np.diag(s[:k])\n    Vh_k = Vh[:k, :]\n    \n    # Reconstruct X_hat\n    X_hat_np = U_k @ S_k @ Vh_k\n    \n    # Compute MSE\n    mse = np.mean((X_np - X_hat_np)**2)\n    \n    # Round and convert to list of lists\n    X_hat_rounded = np.round(X_hat_np, 4)\n    X_hat_list = X_hat_rounded.tolist()\n    \n    # Round MSE\n    mse_rounded = np.round(mse, 4)\n    \n    return (X_hat_list, mse_rounded)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the input matrix X is non-empty and if the integer k is within the valid range. If not, it returns -1.\n2. **Matrix Conversion**: The input list of lists is converted into a numpy array to leverage numpy's efficient numerical operations.\n3. **SVD Computation**: Using numpy's `linalg.svd`, the matrix is decomposed into U, S, and Vh. These matrices are used to form the truncated SVD components.\n4. **Truncated SVD**: The first k columns of U, the first k singular values, and the first k rows of Vh are used to form the truncated matrices.\n5. **Reconstruction**: The reconstructed matrix is computed by multiplying the truncated matrices U_k, S_k, and Vh_k.\n6. **MSE Calculation**: The mean squared error is calculated by comparing the original matrix with the reconstructed matrix.\n7. **Result Formatting**: The reconstructed matrix and MSE are rounded to four decimal places and returned as a tuple.\n\nThis approach ensures that we efficiently compute the optimal low-rank approximation of the input matrix using truncated SVD, providing both the reconstructed matrix and the corresponding MSE."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a NumPy tensor filled with 0.0s and 1.0s based on a specified shape, sparsity, and an optional seed for reproducibility. The function must adhere to specific constraints and return the appropriate output.\n\n### Approach\n1. **Input Validation**: Check if the provided sparsity value is within the valid range [0, 1]. If not, return -1.\n2. **Seed Setting**: If a seed is provided, set the random seed to ensure reproducible results.\n3. **Random Array Generation**: Generate a random array of the specified shape using a uniform distribution between 0 and 1.\n4. **Threshold Calculation**: Compute the threshold as `1 - sparsity`. This threshold determines the probability of an element being 1.0.\n5. **Binary Conversion**: Convert the random array into a binary array where each element is 1.0 if it is greater than or equal to the threshold, otherwise 0.0. This is done using a boolean mask and casting to float to ensure the correct data type.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"Generates a binary tensor with specified shape and sparsity.\n    \n    Args:\n        shape: A tuple of integers representing the desired shape of the tensor.\n        sparsity: A float in [0, 1] representing the probability of each element being 1.0.\n        seed: An optional integer seed for reproducibility.\n    \n    Returns:\n        A numpy.ndarray of shape `shape` with elements 0.0 or 1.0, or -1 if sparsity is invalid.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    if seed is not None:\n        np.random.seed(seed)\n    random_array = np.random.rand(*shape)\n    threshold = 1 - sparsity\n    binary_array = (random_array >= threshold).astype(float)\n    return binary_array\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the sparsity value is within the valid range. If not, it returns -1 immediately.\n- **Seed Setting**: If a seed is provided, it sets the random seed to ensure that the same sequence of random numbers is generated each time the function is called with the same seed.\n- **Random Array Generation**: Using `np.random.rand`, the function generates a random array of the specified shape with values uniformly distributed between 0 and 1.\n- **Threshold Calculation**: The threshold is calculated as `1 - sparsity`, which determines the cutoff for converting random values to 1.0.\n- **Binary Conversion**: The random values are compared against the threshold. Values greater than or equal to the threshold are converted to 1.0, and others to 0.0. The result is cast to a float array to ensure the correct data type.\n\nThis approach ensures that the function efficiently generates the required binary tensor while adhering to the specified constraints and providing reproducible results when a seed is provided."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs a median split on a dataset for KD-Tree construction. The function will find the median value in a specified column, determine the row index of this median value, and split the dataset into two parts: one with values strictly less than the median and the other with values strictly greater than the median.\n\n### Approach\n1. **Identify the Median**: The median value is determined using the `numpy.argpartition` function, which efficiently finds the k-th smallest element in O(n) time. For an even number of samples, the median is the element at position `n//2` in the sorted array.\n2. **Determine the Pivot**: The pivot is the row index of the median value in the original dataset.\n3. **Split the Data**: Using the median value, split the dataset into two parts. The left part contains indices of rows with values strictly less than the median, and the right part contains indices of rows with values strictly greater than the median.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    n = data.shape[0]\n    if n == 0:\n        return (0, [], [])\n    k = n // 2\n    column = data[:, d]\n    indices = np.argpartition(column, k)\n    pivot = indices[k]\n    median_value = data[pivot, d]\n    left = np.where(column < median_value)[0].tolist()\n    right = np.where(column > median_value)[0].tolist()\n    return (pivot, left, right)\n```\n\n### Explanation\n1. **Finding the Median**: The function uses `numpy.argpartition` to find the indices that would place the k-th smallest element (where k is `n//2`) at the correct position. This ensures the median is found efficiently in O(n) time.\n2. **Pivot Index**: The pivot is the index of the median value in the original dataset, obtained from the result of `argpartition`.\n3. **Splitting the Data**: Using boolean indexing, the function identifies rows with values less than and greater than the median. These indices are converted to lists and returned.\n\nThis approach ensures that the function operates efficiently and correctly, adhering to the constraints and requirements specified."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a one-dimensional signal into overlapping frames using NumPy's low-level stride manipulation. The function should return a view on the original signal, not a copy, and handle various edge cases and pre-conditions.\n\n### Approach\n1. **Pre-Condition Checks**: Ensure the input signal is one-dimensional, the stride is at least 1, and the signal length is at least the frame width.\n2. **Calculate Number of Frames**: Determine the number of frames using the formula `(len(x) - frame_width) // stride + 1`.\n3. **Stride Calculation**: Compute the new shape and strides for the resulting view. The new shape is `(n_frames, frame_width)`, and the strides are calculated based on the hop length (stride) and the item size of the original array.\n4. **Create View**: Use `numpy.lib.stride_tricks.as_strided` to create the view with the calculated shape and strides.\n5. **Set Writeable Flag**: Adjust the writeable flag of the resulting array based on the input parameter and the original array's writeable status.\n\n### Solution Code\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    assert x.ndim == 1, \"x must be one-dimensional\"\n    assert stride >= 1, \"stride must be at least 1\"\n    assert len(x) >= frame_width, \"length of x must be at least frame_width\"\n\n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n\n    # Calculate the new shape and strides\n    shape = (n_frames, frame_width)\n    strides = (stride * x.itemsize, x.itemsize)\n\n    # Create the view using as_strided\n    result = as_strided(x, shape=shape, strides=strides)\n\n    # Set the writeable flag based on the parameters\n    result.flags.writeable = writeable and x.flags.writeable\n\n    return result\n```\n\n### Explanation\n- **Pre-Condition Checks**: The function starts by verifying that the input signal is one-dimensional, the stride is valid, and the signal length is sufficient to form at least one frame.\n- **Number of Frames Calculation**: The number of frames is determined by the formula `(len(x) - frame_width) // stride + 1`, ensuring that only complete frames are included.\n- **Stride Calculation**: The new shape for the result is `(n_frames, frame_width)`, and the strides are calculated to ensure each frame starts `stride` samples apart and each element within a frame is consecutive.\n- **View Creation**: Using `as_strided`, the function creates a view of the original array with the calculated shape and strides, ensuring efficient memory usage.\n- **Writeable Flag**: The writeable flag of the resulting array is set based on the input parameter and the original array's writeable status, ensuring the correct permissions are applied.\n\nThis approach efficiently handles the conversion of a 1-D signal into overlapping frames using low-level NumPy operations, ensuring optimal performance and memory usage."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given matrix is a row-stochastic matrix. A row-stochastic matrix is a two-dimensional array where each element is a probability (i.e., between 0 and 1 inclusive) and each row sums to 1 within a small numerical tolerance.\n\n### Approach\nThe approach to solve this problem involves several steps:\n\n1. **Convert the Input to a NumPy Array**: The input can be either a list of lists or a NumPy array. We first convert the input into a NumPy array to simplify the subsequent checks.\n\n2. **Check if the Array is 2D**: Ensure the array is two-dimensional. If not, the matrix is not row-stochastic.\n\n3. **Check for Numeric Type**: Ensure the array contains numeric values. Non-numeric values would make the matrix invalid.\n\n4. **Check for Non-Empty Matrix**: Ensure the matrix is non-empty, meaning it has at least one row and one column.\n\n5. **Check Element Validity**: Ensure all elements are within the range [0, 1].\n\n6. **Check Row Sums**: Ensure each row sums to 1 within a tolerance of 1e-8.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Convert X to a numpy array if it's a list of lists\n    if isinstance(X, list):\n        # Check if all elements are lists\n        if not all(isinstance(row, list) for row in X):\n            return False\n        # Attempt to convert to a numpy array of floats\n        try:\n            X_np = np.array(X, dtype=np.float64)\n        except:\n            return False\n    else:\n        X_np = X\n\n    # Check if the array is two-dimensional\n    if X_np.ndim != 2:\n        return False\n\n    # Check if the array contains numeric data\n    if not np.issubdtype(X_np.dtype, np.number):\n        return False\n\n    # Check if the array is non-empty\n    if X_np.size == 0:\n        return False\n\n    # Check all elements are between 0 and 1 inclusive\n    if not np.all((X_np >= 0) & (X_np <= 1)):\n        return False\n\n    # Check each row sums to 1 within the tolerance\n    row_sums = X_np.sum(axis=1)\n    if not np.allclose(row_sums, 1, atol=1e-8):\n        return False\n\n    # All checks passed\n    return True\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage NumPy's efficient operations for checking matrix properties.\n\n2. **2D Check**: The matrix must be two-dimensional. This is verified using `ndim`.\n\n3. **Numeric Check**: The matrix must contain numeric values. This is checked using `np.issubdtype`.\n\n4. **Non-Empty Check**: The matrix must have at least one element. This is checked by ensuring the size is not zero.\n\n5. **Element Validity**: Each element must be between 0 and 1. This is checked using element-wise comparisons.\n\n6. **Row Sum Check**: Each row must sum to 1 within a tolerance. This is checked using `np.allclose` with a tolerance of 1e-8.\n\nBy following these steps, the function ensures that all conditions for a row-stochastic matrix are met, returning `True` if they are and `False` otherwise."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimization algorithm for ordinary least-squares (OLS) linear regression. The goal is to find the weight vector that minimizes the mean squared error using the RMSprop update rule, which adapts the learning rate for each parameter based on the running average of squared gradients.\n\n### Approach\n1. **Initialization**: Start with the weight vector `w` initialized to zeros and the running average of squared gradients `s` also initialized to zeros.\n2. **Batch Processing**: Handle mini-batches of data. If the number of samples is less than the batch size, use the entire dataset as a single batch. Otherwise, process successive blocks of rows, wrapping around when the end of the matrix is reached.\n3. **Gradient Computation**: For each batch, compute the gradient of the loss function with respect to the weight vector.\n4. **Update Rules**: Update the running average of squared gradients and the weight vector using the RMSprop update rule.\n5. **Early Stopping**: Stop the optimization early if the L2 norm of the gradient becomes smaller than a specified threshold or if the maximum number of iterations is reached.\n6. **Return Result**: Return the final weight vector rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)\n    current_position = 0\n\n    for iteration in range(max_iter):\n        # Determine the current batch\n        if n < batch_size:\n            X_batch = X\n            y_batch = y\n            m = n\n        else:\n            end = current_position + batch_size\n            if end <= n:\n                X_batch = X[current_position:end]\n                y_batch = y[current_position:end]\n                current_position = end\n            else:\n                # Wrap around to the beginning\n                X_batch = np.concatenate((X[current_position:], X[:end - n]))\n                y_batch = np.concatenate((y[current_position:], y[:end - n]))\n                current_position = end - n\n            m = batch_size\n\n        # Compute the gradient for the current batch\n        residuals = X_batch @ w - y_batch\n        g = (X_batch.T @ residuals) / m\n\n        # Check for early stopping\n        grad_norm = np.linalg.norm(g)\n        if grad_norm < epsilon:\n            break\n\n        # Update the running average of squared gradients\n        s = rho * s + (1 - rho) * (g ** 2)\n\n        # Update the weights\n        denominator = np.sqrt(s) + eps_station\n        w = w - eta * (g / denominator)\n\n    # Round the weights to four decimal places and convert to a list\n    w_rounded = [round(x, 4) for x in w]\n    return w_rounded\n```\n\n### Explanation\n- **Initialization**: The weight vector `w` and the running average `s` are initialized to zeros.\n- **Batch Processing**: The code handles both full-batch and mini-batch scenarios. If the number of samples is less than the batch size, the entire dataset is used as a single batch. Otherwise, successive blocks of rows are processed, wrapping around when the end is reached.\n- **Gradient Computation**: For each batch, the gradient of the loss function is computed using the formula `(1/m) * X_batch.T @ (X_batch @ w - y_batch)`, where `m` is the batch size.\n- **Update Rules**: The running average of squared gradients `s` is updated using the RMSprop formula. The weight vector `w` is then updated using the adapted learning rate.\n- **Early Stopping**: The optimization stops early if the L2 norm of the gradient falls below the specified threshold, ensuring efficient computation.\n- **Return Result**: The final weight vector is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the optimization process is both efficient and effective, leveraging the RMSprop algorithm to adaptively adjust the learning rate for each parameter."}
{"task_id": 56, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the SoftPlus activation function and its derivative, also known as the sigmoid function. The function should handle various input types, including scalars, lists, and NumPy arrays, and return the results in a numerically stable manner to avoid overflow issues.\n\n### Approach\n1. **Problem Analysis**: The SoftPlus function is defined as \\( \\text{SoftPlus}(x) = \\log(1 + e^x) \\) and its derivative is the sigmoid function \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). The challenge is to compute these functions efficiently and accurately, especially for very large or very small values of \\( x \\) to avoid numerical overflow.\n\n2. **Numerical Stability**: For very large positive values of \\( x \\), \\( e^x \\) can cause overflow. To handle this, we use a piece-wise approach where for \\( x > 20 \\), \\( \\text{SoftPlus}(x) \\) is approximated as \\( x \\) and the derivative is approximated as 1. For very negative values of \\( x \\) (i.e., \\( x < -20 \\)), \\( \\text{SoftPlus}(x) \\) is approximated as 0 and the derivative is approximated as 0.\n\n3. **Implementation Details**:\n   - Convert the input to a NumPy array for efficient element-wise operations.\n   - Use masks to handle different ranges of \\( x \\) values separately.\n   - Use `np.log1p` for better numerical accuracy when computing \\( \\log(1 + e^x) \\) for small values of \\( x \\).\n\n4. **Rounding and Output**: After computing the results, round each value to the nearest 4th decimal place. Return the result as a scalar if the input was a scalar, or as a list if the input was a list or array.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Compute the SoftPlus activation function or its derivative.\"\"\"\n    is_scalar = np.isscalar(x)\n    x = np.asarray(x)\n    \n    if deriv:\n        # Compute derivative (sigmoid)\n        mask_high = x > 20\n        mask_low = x < -20\n        mask_mid = ~mask_high & ~mask_low\n        \n        result = np.zeros_like(x, dtype=np.float64)\n        result[mask_high] = 1.0\n        result[mask_low] = 0.0\n        result[mask_mid] = 1.0 / (1.0 + np.exp(-x[mask_mid]))\n    else:\n        # Compute SoftPlus\n        mask_high = x > 20\n        mask_low = x < -20\n        mask_mid = ~mask_high & ~mask_low\n        \n        result = np.zeros_like(x, dtype=np.float64)\n        result[mask_high] = x[mask_high]\n        result[mask_low] = 0.0\n        result[mask_mid] = np.log1p(np.exp(x[mask_mid]))\n    \n    # Round to 4 decimal places\n    rounded = np.round(result, decimals=4)\n    \n    # Return as scalar or list\n    if is_scalar:\n        return float(rounded[0])\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input is a scalar, list, or NumPy array and converts it to a NumPy array for uniform processing.\n- **Derivative Handling**: If the derivative flag is set, the function computes the sigmoid function using a piece-wise approach to handle large and small values of \\( x \\) efficiently.\n- **SoftPlus Computation**: For the SoftPlus function, the function uses a similar piece-wise approach to avoid overflow and ensure numerical stability.\n- **Rounding and Output**: The results are rounded to four decimal places and returned as a scalar or list based on the input type.\n\nThis approach ensures that the function is both efficient and numerically stable, handling a wide range of input values accurately."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an unsmoothed Maximum-Likelihood Estimation (MLE) N-gram language model. The goal is to compute the log-probability of a given sequence under the N-gram model trained on a provided corpus.\n\n### Approach\n1. **Problem Analysis**:\n   - For a given order \\( N \\), the probability of an N-gram is estimated using the counts of the N-gram and its prefix (N-1)-gram.\n   - If the sequence is shorter than \\( N \\), the log-probability is zero because there are no valid N-grams to consider.\n   - If any required count (either the N-gram or its prefix) is zero, the function should return negative infinity.\n\n2. **Intuitions and Insights**:\n   - For \\( N = 1 \\) (unigram model), the probability of a word is its count divided by the total number of words in the corpus.\n   - For \\( N > 1 \\), the probability of an N-gram is the count of the N-gram divided by the count of its prefix (N-1)-gram.\n   - The log-probability of the sequence is the sum of the log-probabilities of each valid N-gram in the sequence.\n\n3. **Algorithm Selection**:\n   - Use the `Counter` class from the `collections` module to count occurrences of N-grams and (N-1)-grams efficiently.\n   - For each N-gram in the sequence, check if it exists in the corpus and if its prefix exists. If not, return negative infinity.\n\n4. **Complexity Analysis**:\n   - The algorithm processes the corpus to build N-gram and (N-1)-gram counts, which takes \\( O(M) \\) time where \\( M \\) is the length of the corpus.\n   - Processing the sequence takes \\( O(K) \\) time where \\( K \\) is the length of the sequence, making the overall complexity \\( O(M + K) \\).\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\"\"\"\n    if len(sequence) < N:\n        return 0.0\n    \n    if N == 1:\n        # For N=1, ngram is a single word, represented as a tuple\n        ngram_counts = Counter()\n        for word in corpus:\n            ngram = (word,)\n            ngram_counts[ngram] += 1\n        prev_ngram_count = len(corpus)\n    else:\n        # Generate N-grams and count them\n        ngram_counts = Counter()\n        for i in range(len(corpus) - N + 1):\n            ngram = tuple(corpus[i:i+N])\n            ngram_counts[ngram] += 1\n        # Generate (N-1)-grams and count them\n        prev_ngram_counts = Counter()\n        for i in range(len(corpus) - (N-1) + 1):\n            prev_ngram = tuple(corpus[i:i + (N-1)])\n            prev_ngram_counts[prev_ngram] += 1\n    \n    total_log_prob = 0.0\n    for i in range(len(sequence) - N + 1):\n        current_ngram = tuple(sequence[i:i+N])\n        \n        # Check if current N-gram exists\n        if current_ngram not in ngram_counts:\n            return float('-inf')\n        \n        # For N > 1, check if the prefix exists\n        if N > 1:\n            prefix = current_ngram[:-1]\n            if prefix not in prev_ngram_counts:\n                return float('-inf')\n        \n        # Calculate numerator and denominator\n        if N == 1:\n            numerator = ngram_counts[current_ngram]\n            denominator = prev_ngram_count\n        else:\n            numerator = ngram_counts[current_ngram]\n            denominator = prev_ngram_counts[prefix]\n        \n        # Check for division by zero or zero numerator\n        if denominator == 0 or numerator == 0:\n            return float('-inf')\n        \n        # Compute log probability\n        prob = numerator / denominator\n        log_prob = np.log(prob)\n        total_log_prob += log_prob\n    \n    # Round to 4 decimal places\n    return round(total_log_prob, 4)\n```\n\n### Explanation\n- **Building N-gram Counts**: The function first processes the corpus to build counts of N-grams and (N-1)-grams using the `Counter` class.\n- **Handling Edge Cases**: If the sequence is shorter than \\( N \\), the function returns 0.0. For \\( N = 1 \\), it handles single-word counts and the total word count separately.\n- **Log-probability Calculation**: For each valid N-gram in the sequence, the function checks if the N-gram and its prefix exist in the counts. If any are missing, it returns negative infinity. Otherwise, it computes the log-probability and sums them up.\n- **Efficiency**: The approach efficiently counts N-grams and processes the sequence in linear time relative to the size of the corpus and sequence."}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the backward probability vector for a given time index in a Hidden Markov Model (HMM). The backward probability vector, denoted as \u03b2_t(i), represents the probability of seeing the remaining observations from time t+1 onward given that the system is in state i at time t.\n\n### Approach\n1. **Initialization**: Start by initializing the backward probability vector for the last time step (T-1) to 1 for all states, as there are no future observations beyond this point.\n2. **Iterative Calculation**: Compute the backward probabilities iteratively from the second last time step (T-2) down to the given time index t. For each state at time t, the backward probability is calculated by summing over all possible transitions from the current state to any other state, multiplied by the emission probability of the next observation and the corresponding backward probability from the next time step.\n3. **Rounding**: After computing the backward probabilities for the given time index, round each value to four decimal places for the final result.\n\n### Solution Code\n```python\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"Compute the backward probability vector for a given time index t in a Hidden Markov Model.\"\"\"\n    N = len(A)\n    T = len(obs)\n    beta_next = [1.0 for _ in range(N)]\n    \n    for s in range(T-2, t-1, -1):\n        beta_current = []\n        for i in range(N):\n            total = 0.0\n            for j in range(N):\n                a = A[i][j]\n                b = B[j][obs[s+1]]\n                total += a * b * beta_next[j]\n            beta_current.append(total)\n        beta_next = beta_current\n    \n    return [round(x, 4) for x in beta_next]\n```\n\n### Explanation\n1. **Initialization**: The backward probability vector for the last time step (T-1) is initialized to 1 for all states because there are no future observations beyond this point.\n2. **Iterative Calculation**: For each time step from T-2 down to t, the backward probability for each state is computed by considering all possible transitions to other states, the emission probability of the next observation, and the backward probability from the next time step. This is done using nested loops to iterate over each state and possible transitions.\n3. **Rounding**: The computed backward probabilities are rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently computes the backward probabilities using dynamic programming, ensuring that each step builds on the results of the previous step, starting from the end of the observation sequence and moving backward to the specified time index."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the probability that a given observation sequence is generated by a Hidden Markov Model (HMM) using the backward algorithm. The backward algorithm efficiently calculates this probability by recursively computing the probabilities of observing the remaining sequence from each state at each time step.\n\n### Approach\n1. **Check for Empty Inputs**: If any of the input lists (transition matrix `A`, emission matrix `B`, initial distribution `pi`, or observation sequence `obs`) are empty, return 0.0 immediately as no valid probability can be computed.\n\n2. **Initialize Parameters**: Determine the number of states `N` from the length of the initial distribution `pi` and the length of the observation sequence `T` from the observation list `obs`.\n\n3. **Backward Algorithm Initialization**: Initialize the backward variables `beta` for the last time step `T-1` to 1.0 for all states, as there are no future observations beyond this point.\n\n4. **Recursive Calculation**: For each time step from `T-2` down to 0, compute the backward variables `beta` for each state by summing over all possible next states, weighted by the transition probabilities, emission probabilities, and the previously computed backward variables.\n\n5. **Compute Final Probability**: After computing the backward variables for all time steps, calculate the final probability by summing the contributions from each state, weighted by the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\n6. **Return Result**: Return the computed probability rounded to six decimal places.\n\n### Solution Code\n```python\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    # Check for empty inputs\n    if not A or not B or not pi or not obs:\n        return 0.0\n    \n    N = len(pi)\n    T = len(obs)\n    \n    if T == 0:\n        return 0.0\n    \n    # Initialize beta for t = T-1\n    beta = [1.0 for _ in range(N)]\n    \n    # Iterate from t = T-2 down to 0\n    for t in range(T-2, -1, -1):\n        new_beta = [0.0] * N\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                # A[i][j] is the transition from i to j\n                # B[j][obs[t+1]] is the emission probability for the next observation\n                sum_val += A[i][j] * B[j][obs[t+1]] * beta[j]\n            new_beta[i] = sum_val\n        beta = new_beta\n    \n    # Compute the final probability\n    prob = 0.0\n    for i in range(N):\n        prob += pi[i] * B[i][obs[0]] * beta[i]\n    \n    # Round to six decimal places\n    return round(prob, 6)\n```\n\n### Explanation\n- **Initialization**: The backward variables `beta` are initialized to 1.0 for the last time step because there are no future observations to consider.\n- **Recursive Calculation**: For each time step, the backward variables are updated by considering all possible transitions from the current state to the next state, weighted by the transition probabilities and emission probabilities.\n- **Final Probability**: The final probability is computed by summing the contributions from each state, considering the initial distribution, emission probabilities, and the backward variables at the initial time step.\n\nThis approach efficiently computes the probability of the observation sequence using the backward algorithm, ensuring that the result is accurate and rounded to six decimal places."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward algorithm for Hidden Markov Models (HMMs) to compute the likelihood of an observation sequence given the model parameters. The model is specified by the initial state probabilities, state transition matrix, and emission matrix. The goal is to validate the input and compute the sequence likelihood efficiently.\n\n### Approach\n1. **Input Validation**: Ensure that the input dimensions are correct and that all observations are within the valid range. If any input is invalid, return -1.\n2. **Forward Algorithm**: Use dynamic programming to compute the probability of each observation sequence step-by-step. This involves initializing the forward probabilities for the first observation and iteratively updating them for each subsequent observation.\n3. **Result Calculation**: Sum the final forward probabilities to get the total likelihood of the observation sequence and return it rounded to four decimal places.\n\n### Solution Code\n```python\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Check for empty inputs\n    if not S or not A or not B or not observations:\n        return -1\n    \n    n = len(S)\n    # Check if A is n x n\n    if len(A) != n:\n        return -1\n    for row in A:\n        if len(row) != n:\n            return -1\n    \n    # Check if B is n x m\n    if len(B) != n:\n        return -1\n    m = len(B[0])\n    for row in B:\n        if len(row) != m:\n            return -1\n    \n    # Check observations are within valid range\n    for obs in observations:\n        if obs < 0 or obs >= m:\n            return -1\n    \n    # Initialize forward variables\n    T = len(observations)\n    f = [S[i] * B[i][observations[0]] for i in range(n)]\n    \n    # Iterate through the rest of the observations\n    for t in range(1, T):\n        next_f = [0.0] * n\n        for i in range(n):\n            sum_val = 0.0\n            for j in range(n):\n                sum_val += f[j] * A[j][i]\n            next_f[i] = sum_val * B[i][observations[t]]\n        f = next_f\n    \n    # Sum the final probabilities\n    result = sum(f)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if any of the input lists are empty. It then verifies that the dimensions of the state transition matrix `A` and emission matrix `B` match the initial state probabilities `S`. Each observation is checked to ensure it falls within the valid range of emission symbols.\n2. **Forward Initialization**: The forward probabilities are initialized using the initial state probabilities and the emission probabilities for the first observation.\n3. **Forward Iteration**: For each subsequent observation, the forward probabilities are updated by considering all possible transitions from the previous states. This is done using nested loops to compute the sum of probabilities from all previous states multiplied by the transition probabilities.\n4. **Result Calculation**: The final result is the sum of the forward probabilities after processing all observations, rounded to four decimal places.\n\nThis approach ensures that the function efficiently computes the likelihood of the observation sequence while handling invalid inputs appropriately."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbour (k-NN) classifier. The goal is to predict the class labels for a given test set using the training data. The k-NN algorithm works by finding the k closest training samples to each test sample and using majority voting to determine the class label.\n\n### Approach\n1. **Edge Case Handling**: First, we check if the value of k is valid. If k is less than 1 or greater than the number of training samples, we return a list of -1s for each test sample.\n2. **Distance Calculation**: For each test sample, compute the squared Euclidean distance to each training sample. This avoids the computational cost of taking square roots while maintaining the order of distances.\n3. **Sorting and Selection**: Sort the training samples based on their distances to the test sample and select the top k samples.\n4. **Majority Voting**: Collect the class labels of the k nearest samples. Use these labels to determine the most frequent class. In case of a tie, the smallest class label is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid, returns a list of -1s with the same length as `X_test`.\n    \"\"\"\n    if k < 1 or k > len(X_train):\n        return [-1] * len(X_test)\n    \n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    predictions = []\n    for x in X_test_np:\n        dist_sq = np.sum((X_train_np - x) ** 2, axis=1)\n        indices = np.argsort(dist_sq)\n        k_indices = indices[:k]\n        y_labels = y_train_np[k_indices]\n        \n        unique, counts = np.unique(y_labels, return_counts=True)\n        max_count = np.max(counts)\n        candidates = unique[counts == max_count]\n        predicted = np.min(candidates)\n        predictions.append(int(predicted))\n    \n    return predictions\n```\n\n### Explanation\n1. **Edge Case Handling**: The function first checks if k is valid. If not, it returns a list of -1s with the same length as the test set.\n2. **Distance Calculation**: Using NumPy, the squared Euclidean distances between each test sample and all training samples are computed efficiently.\n3. **Sorting and Selection**: The distances are sorted, and the indices of the k nearest training samples are selected.\n4. **Majority Voting**: The class labels of the k nearest samples are collected. Using NumPy's `unique` function, the frequencies of each class are determined. The class with the highest frequency is selected, with ties broken by choosing the smallest class label.\n\nThis approach ensures that the k-NN algorithm is implemented efficiently and correctly, handling all edge cases and providing accurate predictions for the test set."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the unnormalized categorical cross-entropy loss for a batch of one-hot encoded targets. This loss function is commonly used in machine learning, especially in classification tasks where the output is a probability distribution over classes.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Inputs to NumPy Arrays**: Ensure that both the true labels `y` and the predicted probabilities `y_pred` are converted to NumPy arrays for efficient element-wise operations.\n\n2. **Add Epsilon for Numerical Stability**: To avoid taking the logarithm of zero, which is undefined, we add a small value (epsilon) to each element of `y_pred`. This ensures that the logarithm operation is always applied to a positive number.\n\n3. **Compute Logarithms**: Calculate the natural logarithm of each element in the modified `y_pred` array.\n\n4. **Element-wise Multiplication**: Multiply each element of `y` with the corresponding element in the logarithm array. This step effectively zeros out all terms where the true label is not the correct class, as `y` is one-hot encoded.\n\n5. **Sum and Negate**: Sum all the resulting values and negate the sum to get the final loss value. This is because the cross-entropy loss is defined as the negative sum of the log probabilities.\n\n6. **Round the Result**: Finally, round the computed loss to four decimal places to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    y = np.asarray(y)\n    y_pred = np.asarray(y_pred)\n    epsilon = 1e-15\n    loss = -np.sum(y * np.log(y_pred + epsilon))\n    return round(loss, 4)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: This step ensures that we can leverage NumPy's efficient array operations, which are crucial for handling potentially large datasets efficiently.\n- **Adding Epsilon**: This prevents taking the logarithm of zero, which would result in negative infinity and cause numerical instability in the loss calculation.\n- **Logarithm Calculation**: The natural logarithm of each predicted probability is computed to transform the multiplicative probabilities into additive terms, which is a standard step in cross-entropy calculations.\n- **Element-wise Multiplication**: This step ensures that only the log probability of the correct class contributes to the loss for each sample, as all other terms are multiplied by zero.\n- **Sum and Negate**: The sum of these contributions is taken and negated to align with the definition of cross-entropy loss, which measures the difference between the predicted and true distributions.\n- **Rounding**: The final loss value is rounded to four decimal places to provide a precise and readable result.\n\nThis approach efficiently computes the categorical cross-entropy loss while ensuring numerical stability and correctness."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement forward propagation for an L-layer neural network. The network uses ReLU activation in all hidden layers and sigmoid activation in the output layer. The goal is to compute the output activation and store intermediate values for potential backpropagation.\n\n### Approach\n1. **Extract the Number of Layers (L):** Determine the number of layers in the network by examining the keys in the parameters dictionary. The keys for weights are of the form 'W1', 'W2', etc., so we extract the maximum integer value from these keys to get L.\n2. **Initialize Caches:** Create an empty list to store the intermediate values (caches) for each layer.\n3. **Iterate Through Layers:** For each layer from 1 to L:\n   - Compute the linear transformation using the current layer's weights and biases.\n   - Apply the ReLU activation function for hidden layers and the sigmoid function for the output layer.\n   - Store the intermediate values (input activation, weights, biases, and linear transformation) in the caches list.\n4. **Compute Final Activation:** After processing all layers, the final activation (AL) is obtained from the output of the last layer.\n5. **Format the Output:** Round the final activation to four decimal places and convert it to a list of lists for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\"\"\"\n    # Extract the number of layers L\n    W_keys = [k for k in parameters.keys() if k.startswith('W')]\n    if not W_keys:\n        L = 0\n    else:\n        layer_numbers = [int(k[1:]) for k in W_keys]\n        L = max(layer_numbers)\n    \n    caches = []\n    A_prev = X\n    \n    for l in range(1, L + 1):\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A_prev) + b\n        \n        if l < L:\n            A = np.maximum(0, Z)  # ReLU activation\n        else:\n            A = 1 / (1 + np.exp(-Z))  # Sigmoid activation\n        \n        caches.append((A_prev, W, b, Z))\n        A_prev = A\n    \n    AL = A_prev\n    AL = np.round(AL, 4)\n    return AL.tolist()\n```\n\n### Explanation\n- **Extracting L:** The number of layers is determined by finding the maximum integer value from the keys of the weights in the parameters dictionary.\n- **Caches Initialization:** An empty list is initialized to store the intermediate values for each layer, which are necessary for backpropagation.\n- **Layer Processing:** For each layer, the linear transformation (Z) is computed using the current weights and biases. The activation function (ReLU for hidden layers and sigmoid for the output layer) is applied to Z to get the activation values (A). These values are stored in the caches list.\n- **Final Activation:** The output of the last layer is rounded to four decimal places and converted to a list of lists to meet the output requirements.\n\nThis approach ensures that the forward propagation is efficiently computed and the necessary intermediate values are stored for potential backpropagation, while the final output is formatted as required."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost, also known as log-loss, between the predicted probabilities and the ground-truth binary labels. This cost function is commonly used in binary classification problems to measure the difference between the predicted probabilities and the actual labels.\n\n### Approach\n1. **Clip Values**: To avoid taking the logarithm of zero or one, which is undefined, we clip the predicted probabilities to a small value \u03b5 (1e-15) and 1 - \u03b5. This ensures that all values are within the valid range for logarithmic operations.\n2. **Compute Logarithms**: Calculate the natural logarithm of the clipped predicted probabilities and their complements.\n3. **Element-wise Multiplication**: Multiply the ground-truth labels with the logarithms of the predicted probabilities and the complements of the ground-truth labels with the logarithms of the complements of the predicted probabilities.\n4. **Sum Terms**: Sum the results of the element-wise multiplications for each sample.\n5. **Average and Negate**: Compute the average of the summed terms and negate the result to get the final cost.\n6. **Round Result**: Return the cost rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    epsilon = 1e-15\n    m = A2.size\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    log_a = np.log(A2_clipped)\n    log_1_minus_a = np.log(1 - A2_clipped)\n    term1 = Y * log_a\n    term2 = (1 - Y) * log_1_minus_a\n    sum_terms = term1 + term2\n    cost = (-1 / m) * np.sum(sum_terms)\n    return round(cost, 6)\n```\n\n### Explanation\n- **Clipping Values**: The predicted probabilities are clipped to ensure they are within the range [\u03b5, 1 - \u03b5], avoiding undefined logarithmic operations.\n- **Logarithmic Operations**: The natural logarithm of the clipped values and their complements are computed to handle the log terms in the binary cross-entropy formula.\n- **Element-wise Multiplication and Summation**: The terms involving the ground-truth labels and their complements are computed and summed element-wise.\n- **Averaging and Negation**: The summed terms are averaged and negated to compute the final cost, which is then rounded to six decimal places for precision.\n\nThis approach ensures that the binary cross-entropy cost is computed accurately and efficiently, handling edge cases where predicted probabilities might be too close to 0 or 1."}
{"task_id": 82, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an \u03b5-soft exploration policy in reinforcement learning. This policy ensures that every action has a non-zero probability of being selected while still favoring the greedy action (the action with the highest Q-value). The goal is to compute the probabilities for each action under this policy.\n\n### Approach\n1. **Identify the Number of Actions**: Determine the number of actions, `n`, which is simply the length of the Q-values list.\n2. **Find the Greedy Action**: Identify the action with the highest Q-value. If there are multiple actions with the same maximum Q-value, select the first occurrence (smallest index).\n3. **Calculate Base Probability**: Compute the base probability for non-greedy actions, which is `epsilon / n`.\n4. **Compute Probabilities**: The greedy action's probability is `(1 - epsilon) + (epsilon / n)`, while all other actions have a probability of `epsilon / n`.\n5. **Round Probabilities**: Round each probability to 4 decimal places. Ensure the sum of the rounded probabilities is exactly 1 by adjusting the last element if necessary.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    n = len(Q)\n    if n == 0:\n        return []\n    \n    a_star = np.argmax(Q)\n    base = epsilon / n\n    probs = [base] * n\n    probs[a_star] = (1 - epsilon) + base\n    \n    # Round each probability to 4 decimal places\n    rounded_probs = [round(p, 4) for p in probs]\n    sum_rounded = sum(rounded_probs)\n    \n    # Adjust the last element to ensure the sum is exactly 1.0 within tolerance\n    if not np.isclose(sum_rounded, 1.0, atol=1e-12):\n        diff = 1.0 - sum_rounded\n        rounded_probs[-1] += diff\n    \n    return rounded_probs\n```\n\n### Explanation\n1. **Identify the Number of Actions**: The length of the Q-values list gives the number of actions, `n`.\n2. **Find the Greedy Action**: Using `np.argmax`, we find the index of the first occurrence of the maximum Q-value.\n3. **Calculate Base Probability**: The base probability for non-greedy actions is computed as `epsilon / n`.\n4. **Compute Probabilities**: The greedy action's probability is adjusted to be higher than the base probability, ensuring it is favored while still allowing exploration.\n5. **Round and Adjust Probabilities**: Each probability is rounded to 4 decimal places. If the sum of these rounded probabilities is not exactly 1, the last element is adjusted to ensure the sum is 1, maintaining the required precision.\n\nThis approach ensures that the \u03b5-soft policy is correctly implemented, balancing exploration and exploitation while adhering to the constraints of the problem."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate individual predictions from multiple decision trees in a Random Forest classifier using a majority vote mechanism. The goal is to determine the final prediction for each sample by considering the most frequent prediction across all trees. In case of a tie, the smallest label (numerically or lexicographically) is chosen.\n\n### Approach\n1. **Transpose the Predictions**: Convert the given 2D list of predictions into a structure where each column represents the predictions for a single sample across all trees. This allows us to process each sample's predictions independently.\n2. **Count Votes**: For each sample, count the occurrences of each prediction using a counter.\n3. **Determine Majority Vote**: Identify the prediction(s) with the highest count. If there is a tie, select the smallest label based on numerical or lexicographical order.\n4. **Construct Result**: Collect the final predictions for all samples and return them as a list.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    if not predictions:\n        return []\n    num_samples = len(predictions[0])\n    result = []\n    for i in range(num_samples):\n        sample_predictions = [tree[i] for tree in predictions]\n        counts = Counter(sample_predictions)\n        max_count = max(counts.values())\n        max_labels = [label for label, cnt in counts.items() if cnt == max_count]\n        max_labels.sort()\n        result.append(max_labels[0])\n    return result\n```\n\n### Explanation\n1. **Check for Empty Input**: If the input list is empty, return an empty list immediately.\n2. **Determine Number of Samples**: The number of samples is derived from the length of the first inner list, assuming all inner lists are of the same length.\n3. **Process Each Sample**: For each sample, gather all predictions from each tree, count the occurrences of each prediction, and determine the most frequent prediction(s).\n4. **Handle Ties**: If multiple predictions have the same highest count, sort them and select the smallest one.\n5. **Return Result**: Collect the final predictions for all samples and return them as a list.\n\nThis approach ensures that each sample's prediction is determined efficiently and correctly, adhering to the majority vote mechanism with tie-breaking by selecting the smallest label."}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the softplus activation function in a numerically stable manner. The softplus function is defined as \\( \\text{softplus}(z) = \\ln(1 + e^z) \\). Direct computation using \\( \\exp \\) can lead to overflow for large positive values and underflow for very small negative values. To avoid these issues, we use NumPy's `logaddexp` function, which computes \\( \\ln(e^a + e^b) \\) in a numerically stable way.\n\n### Approach\n1. **Input Handling**: The function should accept scalar values (int or float), lists, or NumPy arrays. The output should be a float for scalar inputs and a Python list for array-like inputs.\n2. **Numerical Stability**: Use `np.logaddexp(0.0, z)` to compute the softplus function. This avoids overflow and underflow issues by leveraging the numerically stable implementation provided by NumPy.\n3. **Rounding**: Each computed value is rounded to 4 decimal places.\n4. **Output Conversion**: For array-like inputs, the result is converted to a pure Python list using `tolist()`. For scalar inputs, the result is returned as a float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Determine if z is a scalar\n    if isinstance(z, (int, float)) or (isinstance(z, np.ndarray) and z.ndim == 0):\n        s = np.logaddexp(0.0, z)\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        # Treat as array-like\n        z_array = np.asarray(z)\n        s = np.logaddexp(0.0, z_array)\n        s_rounded = np.round(s, 4)\n        return s_rounded.tolist()\n```\n\n### Explanation\n1. **Input Check**: The function first checks if the input `z` is a scalar (int, float, or 0-dimensional array). If so, it computes the softplus value directly and returns it as a float after rounding.\n2. **Array Handling**: For array-like inputs (lists or NumPy arrays), the function converts the input to a NumPy array, computes the softplus values using `logaddexp`, rounds each value to 4 decimal places, and converts the result to a Python list.\n3. **Numerical Stability**: By using `np.logaddexp(0.0, z)`, the function ensures that the computation is numerically stable, avoiding overflow for large positive values and underflow for very small negative values.\n\nThis approach ensures that the softplus function is computed efficiently and accurately for various input types, providing the correct output format as specified."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate how good a policy\u2019s current estimates are by computing the mean-squared error (MSE) between the true expected payouts of every arm (provided by the bandit) and the policy\u2019s estimates of those expectations.\n\n### Approach\n1. **Check for Valid Estimates**: First, we check if the policy has any estimates. If the policy does not have the `ev_estimates` attribute or if it is empty, we return `numpy.nan`.\n2. **Retrieve True Values**: We retrieve the true expected values from the bandit. If the bandit does not have the `arm_evs` attribute, we return `numpy.nan`.\n3. **Check for Valid Arm Indices**: We ensure that the policy's estimates include all the required arm indices corresponding to the bandit's arms. If any arm index is missing, we return `numpy.nan`.\n4. **Extract Estimates in Order**: We extract the policy's estimates in the order of the arm indices (0, 1, 2, ...) to match the true values.\n5. **Compute Squared Errors**: For each arm, we compute the squared error between the true value and the policy's estimate.\n6. **Calculate Mean Squared Error**: We compute the mean of these squared errors and round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Check if policy has ev_estimates and it's not empty\n    if not hasattr(policy, 'ev_estimates') or not policy.ev_estimates:\n        return np.nan\n    estimates_dict = policy.ev_estimates\n\n    # Get true_values from bandit\n    try:\n        true_values = bandit.arm_evs\n    except AttributeError:\n        return np.nan\n\n    # Check if true_values is empty\n    if len(true_values) == 0:\n        return np.nan\n\n    # Check if all arm indices are present in estimates_dict\n    arm_indices = list(range(len(true_values)))\n    for i in arm_indices:\n        if i not in estimates_dict:\n            return np.nan\n\n    # Extract estimates in order\n    estimates_list = [estimates_dict[i] for i in arm_indices]\n\n    # Compute squared errors\n    squared_errors = [(est - true)**2 for est, true in zip(estimates_list, true_values)]\n\n    # Compute mean\n    mean_squared_error = np.mean(squared_errors)\n\n    # Round to 4 decimals\n    rounded_mse = round(mean_squared_error, 4)\n\n    return rounded_mse\n```\n\n### Explanation\n- **Check for Valid Estimates**: The function first checks if the policy has any estimates. If not, it returns `numpy.nan`.\n- **Retrieve True Values**: The true expected values are retrieved from the bandit. If the bandit does not provide these values, the function returns `numpy.nan`.\n- **Check for Valid Arm Indices**: The function ensures that the policy's estimates include all the required arm indices. If any index is missing, it returns `numpy.nan`.\n- **Extract Estimates in Order**: The estimates are extracted in the order of the arm indices to match the true values.\n- **Compute Squared Errors**: For each arm, the squared error between the true value and the estimate is computed.\n- **Calculate Mean Squared Error**: The mean of these squared errors is computed and rounded to four decimal places before being returned.\n\nThis approach ensures that the function accurately computes the MSE while handling edge cases such as missing estimates or arm indices."}
{"task_id": 96, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the additive-smoothed log-probability of a given sentence using an N-gram language model. The function will build N-gram frequency tables from a training corpus, handle unknown words using an explicit \"<UNK>\" token, and compute the probability using additive smoothing.\n\n### Approach\n1. **Build N-gram Frequency Tables**: For each sentence in the training corpus, generate all possible N-grams (from 1-gram to N-gram) and count their occurrences. This helps in estimating the probabilities of different N-grams.\n2. **Vocabulary Construction**: Collect all unique words from the training corpus and add an explicit \"<UNK>\" token to handle unknown words in the target sentence.\n3. **Process Target Sentence**: Replace any word in the target sentence that is not present in the training corpus with \"<UNK>\".\n4. **Compute Log Probability**: For each N-gram in the processed target sentence, compute the probability using additive smoothing. Sum the natural logarithms of these probabilities to get the final result.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    # Build n-gram frequency tables\n    ngram_counts = defaultdict(Counter)\n    for sentence in corpus:\n        words = sentence.split()\n        for n in range(1, N + 1):\n            for i in range(len(words) - n + 1):\n                ngram = tuple(words[i:i + n])\n                ngram_counts[n][ngram] += 1\n\n    # Build vocabulary\n    corpus_words = set()\n    for sentence in corpus:\n        words = sentence.split()\n        for word in words:\n            corpus_words.add(word)\n    vocabulary = corpus_words.union({\"<UNK>\"})\n    vocabulary_size = len(vocabulary)\n\n    # Process the target sequence\n    target_words = sequence.split()\n    processed_target = []\n    for word in target_words:\n        if word in corpus_words:\n            processed_target.append(word)\n        else:\n            processed_target.append(\"<UNK>\")\n\n    # Check if the sentence is shorter than N\n    if len(processed_target) < N:\n        return 0.0\n\n    # Calculate the log probability\n    log_prob = 0.0\n    for i in range(len(processed_target) - N + 1):\n        ngram = tuple(processed_target[i:i + N])\n        context = tuple(processed_target[i:i + N - 1])\n\n        count_ngram = ngram_counts[N].get(ngram, 0)\n        count_context = ngram_counts[N - 1].get(context, 0)\n\n        numerator = count_ngram + K\n        denominator = count_context + K * vocabulary_size\n\n        prob = numerator / denominator\n        log_prob += math.log(prob)\n\n    return round(log_prob, 4)\n```\n\n### Explanation\n1. **Building N-gram Frequency Tables**: The function iterates through each sentence in the corpus, splits it into words, and generates all possible N-grams for each sentence. These N-grams are counted and stored in a dictionary of Counters.\n2. **Vocabulary Construction**: The vocabulary is created by collecting all unique words from the corpus and adding the \"<UNK>\" token. This helps in handling unknown words in the target sentence.\n3. **Processing Target Sentence**: Each word in the target sentence is checked against the vocabulary. If a word is not found, it is replaced with \"<UNK>\".\n4. **Log Probability Calculation**: For each N-gram in the processed target sentence, the function computes the probability using additive smoothing. The natural logarithm of each probability is summed up, and the result is rounded to four decimal places before returning.\n\nThis approach ensures that the function efficiently handles the construction of N-gram models, deals with unknown words, and computes the required probabilities accurately."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the regularized Alternating Least Squares (ALS) algorithm for matrix factorization. The goal is to factorize a given matrix \\( X \\) into two low-rank matrices \\( W \\) and \\( H \\) such that the regularized Frobenius reconstruction loss is minimized. The algorithm alternates between updating \\( W \\) and \\( H \\) until the loss drops below a specified tolerance or the maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Start by initializing the matrices \\( W \\) and \\( H \\) with random values using a fixed seed for deterministic results.\n2. **Iterative Updates**: For each iteration, update \\( W \\) while keeping \\( H \\) fixed, and then update \\( H \\) while keeping \\( W \\) fixed. Each update is performed by solving a regularized least squares problem.\n3. **Regularized Least Squares**: When updating \\( W \\), solve the system \\((H^T H + \\alpha I) W = H^T X\\). Similarly, when updating \\( H \\), solve the system \\((W^T W + \\alpha I) H = W^T X\\). This ensures that the regularization term is incorporated into the updates.\n4. **Loss Calculation**: After each full iteration (updating both \\( W \\) and \\( H \\)), compute the regularized Frobenius reconstruction loss. If the loss drops below the specified tolerance, stop the iterations early.\n5. **Reconstruction**: Once the algorithm converges or reaches the maximum number of iterations, compute the reconstructed matrix \\( X_{hat} = WH \\), round it to four decimal places, and return it as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    N, M = X.shape\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n    \n    for _ in range(max_iter):\n        # Update W\n        HtH = H.T @ H\n        HtH += alpha * np.eye(K)\n        HtX = H.T @ X\n        W = np.linalg.solve(HtH, HtX)\n        \n        # Update H\n        WtW = W.T @ W\n        WtW += alpha * np.eye(K)\n        WtX = W.T @ X\n        H = np.linalg.solve(WtW, WtX)\n        \n        # Compute loss\n        X_hat = W @ H\n        loss = np.linalg.norm(X - X_hat, 'fro') ** 2\n        loss += alpha * (np.linalg.norm(W, 'fro') ** 2 + np.linalg.norm(H, 'fro') ** 2)\n        \n        if loss < tol:\n            break\n    \n    X_hat = W @ H\n    X_hat_rounded = np.round(X_hat, 4)\n    return X_hat_rounded.tolist()\n```\n\n### Explanation\n- **Initialization**: The matrices \\( W \\) and \\( H \\) are initialized with random values using a fixed seed to ensure deterministic results.\n- **Updating \\( W \\) and \\( H \\)**: Each update step involves solving a regularized least squares problem using matrix inversion and multiplication. This ensures that the regularization term is effectively incorporated into the updates.\n- **Loss Calculation**: The loss is computed after each full iteration to check for convergence. If the loss drops below the specified tolerance, the algorithm stops early.\n- **Reconstruction**: The final reconstructed matrix is computed, rounded to four decimal places, and returned as a list of lists.\n\nThis approach efficiently factorizes the matrix while incorporating regularization to prevent overfitting, ensuring that the solution is both accurate and stable."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nB"}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create three helper functions for character-level sequence encoding and decoding. These functions will be used in a neural model to process text data, convert it into a format suitable for model input, and then convert the model's predictions back into human-readable text.\n\n### Approach\n\n1. **string_to_int**: This function converts a raw text string into a fixed-length list of integer token IDs. The fixed length is determined by the constant `TIME_STEPS` (set to 20). If the input string is shorter than `TIME_STEPS`, it is padded with zeros. If it is longer, it is truncated to fit the fixed length. Characters not found in the provided vocabulary are also converted to zeros.\n\n2. **int_to_string**: This function converts a list of integer token IDs back into a text string using an inverse vocabulary. Padding tokens (zeros) are ignored during the conversion.\n\n3. **run_example**: This function ties everything together. It encodes the input text, feeds it to a model for prediction, processes the model's output to get the predicted token IDs, and then decodes these IDs back into a text string.\n\n### Solution Code\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Converts a string into a fixed-length list of integer token IDs.\"\"\"\n    # Take the first 'time_steps' characters\n    substring = text[:time_steps]\n    encoded = []\n    for c in substring:\n        # Use the vocabulary, default to 0 if not found\n        encoded.append(vocabulary.get(c, 0))\n    # Pad with zeros if necessary\n    while len(encoded) < time_steps:\n        encoded.append(0)\n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list of integer token IDs back to a string, ignoring padding.\"\"\"\n    s = []\n    for idx in indices:\n        if idx == 0:\n            continue\n        if idx in inverse_vocab:\n            s.append(inverse_vocab[idx])\n    return ''.join(s)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes text, runs through model, and decodes the prediction.\"\"\"\n    # Encode the input text\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n    # Prepare the input for the model (batch size 1)\n    encoded_batch = np.array([encoded])\n    # Get model predictions\n    predictions = model.predict(encoded_batch)\n    # Apply argmax to get the predicted class indices\n    predicted_ids = np.argmax(predictions, axis=2)\n    # Extract the first (and only) sequence from the batch\n    predicted_ids = predicted_ids[0]\n    # Decode the predicted IDs to a string\n    decoded = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded\n```\n\n### Explanation\n\n1. **string_to_int**:\n   - This function processes each character of the input string up to the specified `time_steps` length.\n   - Each character is converted to its corresponding integer ID using the provided vocabulary. If a character is not found in the vocabulary, it is converted to 0.\n   - If the resulting list of integer IDs is shorter than `time_steps`, it is padded with zeros to ensure the fixed length.\n\n2. **int_to_string**:\n   - This function converts a list of integer IDs back into a string using an inverse vocabulary.\n   - Padding tokens (zeros) are ignored during the conversion, ensuring they do not appear in the final output string.\n\n3. **run_example**:\n   - This function first encodes the input text into a fixed-length list of integer IDs.\n   - The encoded list is converted into a numpy array and fed into the model for prediction.\n   - The model's output is processed to get the predicted token IDs using argmax.\n   - These predicted IDs are then decoded back into a human-readable string, ignoring any padding tokens.\n\nThis approach ensures that the text is properly formatted for model input and that the model's predictions are accurately converted back into readable text, making it suitable for character-level sequence modeling tasks."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `run_examples` that automates the process of running a trained model on multiple input strings and collecting the predictions. This function will handle both the execution of the model on each input string and the formatting of the output for easy review.\n\n### Approach\n1. **Check for Examples**: The function first checks if the `examples` argument is provided. If not, it falls back to a global constant `EXAMPLES`.\n2. **Iterate Through Examples**: For each input string in the examples, the function calls another helper function `run_example` to get the model's prediction as a list of characters.\n3. **Format and Print Output**: Each prediction is converted from a list of characters into a string. The input and output strings are then printed in a specified format.\n4. **Collect Predictions**: All predicted strings are collected into a list, which is returned at the end.\n\n### Solution Code\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str], optional\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same order.\n    \"\"\"\n    if examples is None:\n        examples = EXAMPLES\n    predictions = []\n    for example in examples:\n        output_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        predicted_str = ''.join(output_chars)\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n        predictions.append(predicted_str)\n    return predictions\n```\n\n### Explanation\n- **Handling Examples**: The function checks if `examples` is `None` and uses the global `EXAMPLES` if necessary.\n- **Running the Model**: For each input string, the `run_example` function is called to get the model's prediction as a list of characters.\n- **Formatting Output**: Each prediction is converted into a string and printed alongside the input string for easy review.\n- **Collecting Results**: All predictions are collected into a list and returned, maintaining the order of the input examples.\n\nThis approach ensures that the function is both efficient and user-friendly, providing clear output for quick review while maintaining the necessary structure for further processing."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes both the average binary cross-entropy loss and its gradient with respect to the weights for logistic regression. This function will be used to evaluate the performance of a logistic regression model and to update the model's weights during training.\n\n### Approach\n1. **Compute the Linear Combination**: For each sample, compute the linear combination of features and weights, denoted as \\( z = X \\cdot w \\).\n2. **Apply Sigmoid Function**: Convert the linear combination into probabilities using the sigmoid function, \\( p = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n3. **Clip Probabilities**: To avoid numerical issues when taking logarithms, clip the probabilities to a small constant \\( \\epsilon \\) to ensure they are within the range \\([ \\epsilon, 1 - \\epsilon ]\\).\n4. **Compute Loss**: Calculate the average binary cross-entropy loss using the formula \\( J(w) = -\\frac{1}{m} \\sum [ y \\cdot \\ln(p) + (1 - y) \\cdot \\ln(1 - p) ] \\).\n5. **Compute Gradient**: Calculate the gradient of the loss with respect to the weights using the formula \\( \\nabla J(w) = \\frac{1}{m} X^T (p - y) \\).\n6. **Round Results**: Round the loss to 4 decimal places and the gradient to 4 decimal places, converting the gradient into a nested list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    \n    # Reshape y and w to 1D arrays\n    y = y.ravel()\n    w = w.ravel()\n    \n    # Compute z = Xw\n    z = X.dot(w)\n    \n    # Compute p = sigmoid(z)\n    p = 1 / (1 + np.exp(-z))\n    \n    # Clip p to avoid numerical issues\n    epsilon = 1e-20\n    p = np.clip(p, epsilon, 1 - epsilon)\n    \n    # Compute loss\n    loss = - (y * np.log(p) + (1 - y) * np.log(1 - p)).sum() / m\n    loss = round(loss, 4)\n    \n    # Compute gradient\n    gradient = (X.T.dot(p - y)) / m\n    gradient = np.round(gradient, 4)\n    gradient = gradient.reshape(-1, 1).tolist()\n    \n    return (loss, gradient)\n```\n\n### Explanation\n1. **Linear Combination**: The linear combination \\( z \\) is computed using matrix multiplication of the feature matrix \\( X \\) and the weight vector \\( w \\).\n2. **Sigmoid Function**: The sigmoid function transforms the linear combination into probabilities, which are then clipped to avoid numerical issues when taking logarithms.\n3. **Loss Calculation**: The binary cross-entropy loss is computed by averaging the negative log-likelihood across all samples.\n4. **Gradient Calculation**: The gradient is computed using the transpose of the feature matrix and the difference between the predicted probabilities and the actual targets.\n5. **Rounding and Formatting**: The loss is rounded to 4 decimal places, and the gradient is rounded and converted into a nested list format for the final output.\n\nThis approach ensures that the function efficiently computes both the loss and its gradient, handling numerical stability issues and providing results in the required format."}
{"task_id": 128, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the multi-class Linear Discriminant Analysis (LDA) transformation. The goal is to find a linear projection that maximizes the between-class scatter while minimizing the within-class scatter. This involves computing the within-class and between-class scatter matrices, solving the generalized eigenvalue problem, and projecting the data onto the resulting discriminant directions.\n\n### Approach\n1. **Compute the Within-class Scatter Matrix (S_W)**: This matrix is calculated by summing the outer products of the centered data for each class.\n2. **Compute the Between-class Scatter Matrix (S_B)**: This matrix is calculated by summing the outer products of the class means centered around the global mean, scaled by the number of samples in each class.\n3. **Form the Matrix A**: Using the Moore-Penrose pseudo-inverse of S_W, compute the matrix A as the product of the pseudo-inverse of S_W and S_B.\n4. **Eigen-decomposition of A**: Compute the eigenvalues and eigenvectors of A. Sort these eigenvalues in descending order and reorder the eigenvectors accordingly.\n5. **Sign Fix**: Ensure the eigenvectors have a consistent sign by flipping the sign of each eigenvector if the first non-zero element is negative.\n6. **Project the Data**: Project the input data onto the top `n_components` eigenvectors and round the results to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components == 0:\n        return []\n    \n    unique_classes = np.unique(y)\n    n_classes = len(unique_classes)\n    \n    # Compute global mean\n    mu = np.mean(X, axis=0)\n    \n    d = X.shape[1]\n    S_W = np.zeros((d, d))\n    S_B = np.zeros((d, d))\n    \n    for c in unique_classes:\n        indices = np.where(y == c)[0]\n        X_c = X[indices]\n        N_c = len(X_c)\n        mu_c = np.mean(X_c, axis=0)\n        \n        # Compute within-class scatter\n        centered = X_c - mu_c\n        S_c = np.dot(centered.T, centered)\n        S_W += S_c\n        \n        # Compute between-class scatter\n        diff = mu_c - mu\n        S_B += N_c * np.outer(diff, diff)\n    \n    # Compute A = S_W^{-1} S_B using pseudo-inverse\n    A = np.linalg.pinv(S_W) @ S_B\n    \n    # Eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Fix the sign of eigenvectors\n    for i in range(eigenvectors.shape[1]):\n        vec = eigenvectors[:, i]\n        # Find the first non-zero element\n        for j in range(vec.shape[0]):\n            if vec[j] != 0:\n                if vec[j] < 0:\n                    eigenvectors[:, i] *= -1\n                break\n    \n    # Select top n_components eigenvectors\n    W = eigenvectors[:, :n_components]\n    \n    # Project X onto W\n    projection = X @ W\n    \n    # Round to four decimal places\n    projection = np.round(projection, 4)\n    \n    # Convert to list of lists\n    return projection.tolist()\n```\n\n### Explanation\n1. **Within-class Scatter Matrix (S_W)**: This matrix captures the variability within each class. For each class, the data is centered around the class mean, and the outer products of these centered data points are summed to form S_W.\n2. **Between-class Scatter Matrix (S_B)**: This matrix captures the variability between classes. For each class, the difference between the class mean and the global mean is computed, and the outer product of this difference is scaled by the number of samples in the class and summed to form S_B.\n3. **Matrix A**: The matrix A is formed by multiplying the pseudo-inverse of S_W with S_B. This step is crucial for solving the generalized eigenvalue problem.\n4. **Eigen-decomposition**: The eigenvalues and eigenvectors of A are computed. The eigenvectors corresponding to the largest eigenvalues are selected as they represent the directions of maximum discriminant information.\n5. **Sign Fix**: To ensure consistency, each eigenvector is adjusted so that the first non-zero element is positive, removing any sign ambiguity.\n6. **Projection**: The input data is projected onto the selected eigenvectors, and the results are rounded to four decimal places for precision.\n\nThis approach ensures that the data is transformed into a lower-dimensional space that maximizes class separability, making it suitable for classification tasks."}
{"task_id": 141, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbors (k-NN) classifier. The k-NN algorithm is a simple and effective method for classification tasks. The algorithm works by finding the k nearest training samples to each test sample and predicting the class based on the majority vote among those k neighbors.\n\n### Approach\n1. **Distance Calculation**: Compute the distance between each test sample and every training sample using one of three metrics: Euclidean, Manhattan, or Cosine. If an unknown metric is provided, default to Euclidean.\n2. **Nearest Neighbors**: For each test sample, identify the k training samples with the smallest distances.\n3. **Majority Vote**: Predict the class of each test sample by majority vote among the k nearest neighbors. In case of a tie, the smallest label is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement your code here.\"\"\"\n    # Determine the metric\n    if metric not in ['euclidean', 'manhattan', 'cosine']:\n        metric = 'euclidean'\n    \n    # Compute the distance matrix\n    if metric == 'euclidean':\n        dist_sq = np.sum((X_test[:, np.newaxis] - X_train)**2, axis=2)\n        distances = np.sqrt(dist_sq)\n    elif metric == 'manhattan':\n        distances = np.sum(np.abs(X_test[:, np.newaxis] - X_train), axis=2)\n    elif metric == 'cosine':\n        dot_products = X_test.dot(X_train.T)\n        norms_test = np.linalg.norm(X_test, axis=1)\n        norms_train = np.linalg.norm(X_train, axis=1)\n        denominator = norms_test[:, np.newaxis] * norms_train\n        mask = denominator != 0\n        cosine_sim = np.zeros_like(dot_products)\n        cosine_sim[mask] = dot_products[mask] / denominator[mask]\n        cosine_sim = np.clip(cosine_sim, -1, 1)\n        distances = 1 - cosine_sim\n    else:\n        # This should not happen due to earlier check\n        pass\n    \n    # For each test sample, find the k nearest neighbors\n    n_test = X_test.shape[0]\n    k_indices = np.argpartition(distances, k, axis=1)[:, :k]\n    \n    # Prepare the predictions array\n    predictions = np.zeros(n_test, dtype=y_train.dtype)\n    \n    for i in range(n_test):\n        # Get the labels of the k nearest neighbors\n        labels = y_train[k_indices[i]]\n        # Compute unique labels and their counts\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        max_count = np.max(counts)\n        # Find all labels with max_count\n        mask = counts == max_count\n        candidates = unique_labels[mask]\n        # Choose the smallest label\n        predictions[i] = np.min(candidates)\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: The function first checks the metric and computes the distance matrix accordingly. For Euclidean, it calculates the squared distances and then takes the square root. For Manhattan, it sums the absolute differences. For Cosine, it computes the cosine similarity and converts it to a distance.\n2. **Nearest Neighbors**: Using `np.argpartition`, the function efficiently finds the indices of the k smallest distances for each test sample.\n3. **Majority Vote**: For each test sample, the function extracts the labels of the k nearest neighbors, computes the unique labels and their counts, and selects the label with the highest count. In case of a tie, the smallest label is chosen.\n\nThis approach ensures that the k-NN classifier is implemented efficiently and correctly, handling various edge cases and different distance metrics."}
{"task_id": 146, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-Nearest Neighbors (k-NN) classifier from scratch. The k-NN algorithm is a simple and effective method for classification tasks. It works by finding the k closest training samples to a given test sample and then using majority voting to predict the label of the test sample.\n\n### Approach\nThe approach to implement the k-NN classifier involves the following steps:\n\n1. **Distance Calculation**: For each test sample, compute the distance to every training sample using one of the three specified distance metrics: Euclidean, Manhattan, or Cosine.\n2. **Sorting and Selection**: For each test sample, sort the training samples based on the computed distances and select the top k closest samples.\n3. **Majority Voting**: Determine the most frequent label among the k closest samples. In case of a tie, the smallest label is chosen.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    predictions = []\n    for x in X_test:\n        if metric == 'euclidean':\n            distances = np.sqrt(np.sum((X - x) ** 2, axis=1))\n        elif metric == 'manhattan':\n            distances = np.sum(np.abs(X - x), axis=1)\n        elif metric == 'cosine':\n            dot = np.dot(X, x)\n            norm_x = np.linalg.norm(x)\n            norms = np.linalg.norm(X, axis=1)\n            denominator = norm_x * norms + 1e-12\n            cosine_sim = dot / denominator\n            distances = 1 - cosine_sim\n        else:\n            raise ValueError(\"Invalid metric\")\n        \n        dist_label = list(zip(distances, y))\n        dist_label.sort(key=lambda x: x[0])\n        \n        k_neighbors = dist_label[:k]\n        neighbor_labels = [label for (d, label) in k_neighbors]\n        \n        counts = defaultdict(int)\n        for label in neighbor_labels:\n            counts[label] += 1\n        max_count = max(counts.values())\n        candidates = [label for label, cnt in counts.items() if cnt == max_count]\n        predicted_label = min(candidates)\n        \n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation\n1. **Distance Calculation**: The code computes the distance from each test sample to every training sample using the specified metric. For Euclidean and Manhattan distances, it uses vectorized operations for efficiency. For Cosine distance, it computes the dot product and norms to avoid division by zero by adding a small epsilon.\n2. **Sorting and Selection**: After computing the distances, the code creates a list of tuples containing each distance and its corresponding label. This list is sorted by distance, and the top k elements are selected.\n3. **Majority Voting**: The labels of the selected k neighbors are counted. The label with the highest count is chosen as the prediction. In case of a tie, the smallest label is selected.\n\nThis approach ensures that the k-NN algorithm is implemented efficiently and correctly, handling various edge cases such as ties in label counts and different distance metrics."}
{"task_id": 155, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of an actor network in a reinforcement learning context. The network consists of three linear layers with ReLU activations after the first two layers and a tanh activation at the output layer. The goal is to convert a given state vector into an action vector using the provided weights and biases.\n\n### Approach\n1. **Convert State to Numpy Array**: Ensure the input state is a numpy array for consistent operations.\n2. **Linear Transformation and ReLU Activation**: Compute the output of each linear layer followed by a ReLU activation. This involves matrix multiplication with the weights and addition of biases, followed by applying the ReLU function.\n3. **Final Linear Transformation and Tanh Activation**: The output of the second ReLU layer is passed through the final linear layer, followed by a tanh activation to ensure the action values are within the range [-1, 1].\n4. **Rounding**: Convert the resulting action vector to a list of floats, each rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a numpy array if it's not already\n    if not isinstance(state, np.ndarray):\n        state = np.array(state)\n    \n    # First linear layer followed by ReLU\n    layer1 = state.dot(weights['W1']) + weights['b1']\n    layer1 = np.maximum(0, layer1)\n    \n    # Second linear layer followed by ReLU\n    layer2 = layer1.dot(weights['W2']) + weights['b2']\n    layer2 = np.maximum(0, layer2)\n    \n    # Third linear layer followed by tanh\n    action = layer2.dot(weights['W3']) + weights['b3']\n    action = np.tanh(action)\n    \n    # Round each element to four decimal places and convert to list\n    return [round(x, 4) for x in action]\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted to a numpy array to facilitate matrix operations.\n2. **First Layer**: The state vector is multiplied by the first weight matrix `W1`, and the bias `b1` is added. The result is passed through the ReLU activation function.\n3. **Second Layer**: The output from the first layer is multiplied by the second weight matrix `W2`, and the bias `b2` is added. Another ReLU activation is applied.\n4. **Output Layer**: The output from the second layer is multiplied by the third weight matrix `W3`, and the bias `b3` is added. The result is passed through the tanh activation function to produce the action vector.\n5. **Rounding**: Each element of the action vector is rounded to four decimal places and returned as a list.\n\nThis approach ensures that the network correctly processes the input state through the defined architecture, producing an action vector suitable for reinforcement learning applications."}
{"task_id": 160, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Lasso regression using the coordinate descent optimization strategy. Lasso regression is a type of linear regression that includes an L1 regularization term, which helps in feature selection by shrinking some coefficients to zero. The coordinate descent method is an efficient way to optimize the Lasso objective function by updating each parameter one at a time.\n\n### Approach\n1. **Preprocess the Data**: If an intercept is to be fitted, add a column of ones to the feature matrix. Otherwise, add a column of zeros to ensure the intercept is treated separately and not included in the L1 penalty.\n2. **Initialize Parameters**: Start with all parameters (weights and intercept) set to zero.\n3. **Iterative Optimization**: For a specified number of iterations, update each weight using the coordinate descent method:\n   - Compute the residual after accounting for the current weights.\n   - For each weight, compute the partial residual, then update the weight using the soft-thresholding operator.\n   - If an intercept is fitted, recompute it as the mean of the residuals after each iteration.\n4. **Soft-Thresholding**: This operator is used to update each weight, ensuring that the L1 penalty is applied correctly. It shrinks the weight towards zero, potentially setting it to zero if the threshold is exceeded.\n5. **Return Results**: After completing the iterations, return the optimized weights and intercept, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\"\"\"\n    m, n = X.shape\n    if fit_intercept:\n        intercept_column = np.ones((m, 1))\n    else:\n        intercept_column = np.zeros((m, 1))\n    X = np.hstack([intercept_column, X])\n    w = np.zeros(X.shape[1])\n    \n    for _ in range(max_iters):\n        if fit_intercept:\n            residual = y - X.dot(w)\n            intercept = residual.mean()\n            w[0] = intercept\n            residual = y - X.dot(w)\n        else:\n            residual = y - X.dot(w)\n        \n        for j in range(1, X.shape[1]):\n            r_j = residual + X[:, j] * w[j]\n            a = np.dot(X[:, j], r_j)\n            tau = lambda_param * m\n            denom = X[:, j].dot(X[:, j])\n            \n            if denom == 0:\n                w_j = 0.0\n            else:\n                if a > tau:\n                    w_j = (a - tau) / denom\n                elif a < -tau:\n                    w_j = (a + tau) / denom\n                else:\n                    w_j = 0.0\n            \n            delta = w_j - w[j]\n            w[j] = w_j\n            residual -= X[:, j] * delta\n    \n    if fit_intercept:\n        bias = w[0]\n        weights = w[1:]\n    else:\n        bias = 0.0\n        weights = w\n    \n    weights = [round(weight, 4) for weight in weights]\n    bias = round(bias, 4)\n    \n    return (weights, bias)\n```\n\n### Explanation\n- **Data Preprocessing**: The feature matrix is augmented with an intercept column if required. This ensures that the intercept is handled separately from the other weights.\n- **Initialization**: All weights, including the intercept, are initialized to zero.\n- **Residual Calculation**: For each iteration, the residual is computed as the difference between the target values and the current predictions.\n- **Weight Update**: Each weight is updated using the soft-thresholding operator, which applies the L1 penalty. The intercept is updated as the mean of the residuals if it is being fitted.\n- **Efficiency**: The coordinate descent method efficiently updates each weight individually, ensuring that the optimization process is both effective and computationally manageable.\n\nThis approach ensures that the Lasso regression model is optimized correctly, providing a balance between minimizing the prediction error and applying the L1 regularization to encourage sparsity in the model coefficients."}
{"task_id": 165, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the normalized Hamming distance between two 1-D integer vectors. The Hamming distance is a measure of the number of positions at which the corresponding elements of two vectors differ. The normalized Hamming distance is this count divided by the total length of the vectors, providing a proportion rather than a raw count.\n\n### Approach\n1. **Check Lengths**: First, we check if the two vectors have the same length. If they do not, we return -1 immediately.\n2. **Handle Edge Case for Empty Vectors**: If both vectors are empty, we return 0.0 since there are no elements to compare, and thus the distance is zero.\n3. **Convert to NumPy Arrays**: Convert the input vectors to NumPy arrays to leverage efficient element-wise operations.\n4. **Compute Differences**: Use element-wise comparison to identify positions where the elements of the two vectors differ.\n5. **Count Differences**: Sum the boolean array resulting from the comparison to get the count of differing elements.\n6. **Calculate Normalized Distance**: Divide the count of differing elements by the total number of elements to get the normalized Hamming distance.\n7. **Round the Result**: Round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return -1.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or -1 if the inputs have different lengths.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    n = len(x)\n    if n == 0:\n        return 0.0\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    diff = x_arr != y_arr\n    count = np.sum(diff)\n    distance = count / n\n    return round(distance, 4)\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the two vectors are different. If they are, it returns -1.\n- **Handle Empty Vectors**: If both vectors are empty, the function returns 0.0 since there are no differing elements.\n- **Convert to NumPy Arrays**: Converting the input vectors to NumPy arrays allows efficient element-wise operations, which are crucial for performance, especially with large vectors.\n- **Compute Differences**: Using the element-wise inequality operator (`!=`), we create a boolean array where each element is `True` if the corresponding elements in the vectors differ.\n- **Count Differences**: Summing the boolean array gives the total count of differing elements.\n- **Calculate Normalized Distance**: The count of differing elements is divided by the total number of elements to get the normalized distance.\n- **Round the Result**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function efficiently computes the normalized Hamming distance while handling edge cases and different input types appropriately."}
{"task_id": 169, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate coefficients for various symmetric window functions commonly used in digital signal processing. The function should handle different window types and return the coefficients as a list of floats, each rounded to 4 decimal places.\n\n### Approach\n1. **Input Validation**: Check if the number of coefficients `N` is a positive integer. If not, raise a `ValueError`. If `N` is 1, return a list containing a single value `1.0`.\n2. **Window Type Check**: Ensure the specified window type is valid. If not, raise a `ValueError`.\n3. **Window Generation**:\n   - **Hamming Window**: Compute coefficients using the formula `0.54 - 0.46 * cos(2\u03c0n/(N-1))`.\n   - **Hann Window**: Compute coefficients using the formula `0.5 - 0.5 * cos(2\u03c0n/(N-1))`.\n   - **Blackman-Harris Window**: Compute coefficients using the formula with predefined coefficients.\n   - **Generalized Cosine Window**: Compute coefficients using a provided list of coefficients, summing the contributions from each cosine term.\n4. **Rounding**: Round each coefficient to 4 decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\"\"\"\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    if N == 1:\n        return [1.0]\n    if window not in [\"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\"]:\n        raise ValueError(\"Unknown window type.\")\n    if window == \"generalized_cosine\" and coefficients is None:\n        raise ValueError(\"Coefficients must be provided for generalized_cosine window.\")\n    \n    if window == \"hamming\":\n        n = np.arange(N)\n        w = 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"hann\":\n        n = np.arange(N)\n        w = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n    elif window == \"blackman_harris\":\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        n = np.arange(N)\n        term1 = a0\n        term2 = a1 * np.cos(2 * np.pi * n / (N - 1))\n        term3 = a2 * np.cos(4 * np.pi * n / (N - 1))\n        term4 = a3 * np.cos(6 * np.pi * n / (N - 1))\n        w = term1 - term2 + term3 - term4\n    elif window == \"generalized_cosine\":\n        n = np.arange(N)\n        w = np.zeros(N)\n        for k, a in enumerate(coefficients):\n            w += a * np.cos(2 * np.pi * k * n / (N - 1))\n    \n    rounded_w = [round(x, 4) for x in w]\n    return rounded_w\n```\n\n### Explanation\n- **Input Validation**: The function first checks if `N` is a positive integer and handles the special case where `N` is 1.\n- **Window Type Handling**: Depending on the specified window type, the function computes the coefficients using the respective mathematical formula.\n- **Generalized Cosine Window**: This window type uses a provided list of coefficients to compute the window values by summing the contributions from each cosine term.\n- **Rounding**: Each computed coefficient is rounded to 4 decimal places to meet the problem's requirements.\n\nThis approach ensures that the function efficiently generates the required window coefficients for various types, adhering to the specified constraints and requirements."}
{"task_id": 171, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a binary logistic regression classifier from scratch using mini-batch gradient descent. The goal is to train the model on a given training set and use it to predict labels for a test set. The solution must handle various edge cases, such as when all training labels are identical, and must map predictions back to the original label set.\n\n### Approach\n1. **Data Preprocessing**: Convert the input lists into numpy arrays for efficient computation. Add an intercept term (bias) to both the training and test feature matrices to handle the bias term in the model.\n2. **Label Handling**: Check if all training labels are identical. If so, return this label for all test samples. Otherwise, map the two distinct labels to 0 and 1 for easier computation.\n3. **Model Initialization**: Initialize the weight vector (theta) with zeros.\n4. **Training Loop**: Use mini-batch gradient descent to update the weights. For each epoch, shuffle the data and process it in mini-batches. Compute the hypothesis (sigmoid function), the gradient, and update the weights.\n5. **Prediction**: After training, compute the predictions for the test set using the learned weights. Map the binary predictions back to the original label set.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    # Check if all labels are the same\n    unique_labels = np.unique(y_train_np)\n    if len(unique_labels) == 1:\n        return [unique_labels[0]] * len(X_test)\n    \n    # Add intercept term to X_train and X_test\n    X_train_np = np.insert(X_train_np, 0, 1, axis=1)\n    X_test_np = np.insert(X_test_np, 0, 1, axis=1)\n    \n    # Create label mapping to 0 and 1\n    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n    y_train_bin = np.array([label_map[y] for y in y_train_np])\n    \n    # Initialize theta with zeros\n    theta = np.zeros(X_train_np.shape[1])\n    \n    # Training loop\n    for _ in range(epochs):\n        # Shuffle the data\n        indices = np.random.permutation(len(X_train_np))\n        X_shuffled = X_train_np[indices]\n        y_shuffled = y_train_bin[indices]\n        \n        # Split into mini-batches\n        for i in range(0, len(X_shuffled), batch_size):\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n            \n            # Compute the hypothesis\n            z = X_batch.dot(theta)\n            h = 1 / (1 + np.exp(-z))\n            \n            # Compute the gradient\n            gradient = (X_batch.T.dot(h - y_batch)) / len(X_batch)\n            \n            # Update theta\n            theta -= learning_rate * gradient\n    \n    # Make predictions on the test set\n    z_test = X_test_np.dot(theta)\n    h_test = 1 / (1 + np.exp(-z_test))\n    y_pred_bin = (h_test >= 0.5).astype(int)\n    \n    # Map back to original labels\n    y_pred = unique_labels[y_pred_bin].tolist()\n    \n    return y_pred\n```\n\n### Explanation\n1. **Data Preprocessing**: The input feature matrices are converted to numpy arrays for efficient computation. An intercept term is added to both the training and test sets to handle the bias in the model.\n2. **Label Handling**: The function checks if all training labels are the same. If so, it returns this label for all test samples. Otherwise, it maps the two distinct labels to 0 and 1.\n3. **Model Initialization**: The weight vector (theta) is initialized with zeros.\n4. **Training Loop**: The model is trained using mini-batch gradient descent. For each epoch, the data is shuffled, and processed in mini-batches. The hypothesis is computed using the sigmoid function, and the gradient is calculated to update the weights.\n5. **Prediction**: After training, the model makes predictions on the test set using the learned weights. The binary predictions are mapped back to the original label set and returned as a list."}
{"task_id": 178, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Particle Swarm Optimization (PSO) from scratch to minimize the Sphere function. The Sphere function is defined as \\( f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2 \\) where each \\( x_i \\) is within the range \\([-1, 1]\\). The goal is to find the minimum value of this function using PSO.\n\n### Approach\n1. **Input Validation**: Check if any of the input parameters (n_dims, num_particles, num_iterations) are non-positive. If so, return -1.\n2. **Initialization**: \n   - Generate initial positions for each particle within the range \\([-1, 1]\\).\n   - Initialize velocities to zero.\n   - Compute the initial values of the Sphere function for each particle and set these as their personal best values.\n   - Determine the global best value from the initial personal best values.\n3. **PSO Algorithm**:\n   - For each iteration, update the velocity and position of each particle.\n   - The velocity update rule combines inertia, cognitive, and social components.\n   - Clip the particle's position to ensure it remains within the bounds \\([-1, 1]\\).\n   - Update the personal best values if the new position yields a better result.\n   - Update the global best value after processing all particles in the current iteration.\n4. **Return Result**: After completing all iterations, return the best value found, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    \n    rng = np.random.default_rng(seed)\n    \n    # Initialize positions and velocities\n    positions = rng.uniform(low=-1, high=1, size=(num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n    \n    # Compute initial personal bests\n    personal_best_values = np.sum(positions ** 2, axis=1)\n    personal_best_positions = positions.copy()\n    \n    # Initialize global best\n    global_best_value = np.min(personal_best_values)\n    global_best_position = personal_best_positions[np.argmin(personal_best_values)].copy()\n    \n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    \n    for _ in range(num_iterations):\n        for p in range(num_particles):\n            r1 = rng.random()\n            r2 = rng.random()\n            \n            # Update velocity\n            velocities[p] = w * velocities[p] + c1 * r1 * (personal_best_positions[p] - positions[p]) + c2 * r2 * (global_best_position - positions[p])\n            \n            # Update position\n            positions[p] += velocities[p]\n            positions[p] = np.clip(positions[p], -1, 1)\n            \n            # Compute new value\n            new_value = np.sum(positions[p] ** 2)\n            \n            # Update personal best\n            if new_value < personal_best_values[p]:\n                personal_best_positions[p] = positions[p].copy()\n                personal_best_values[p] = new_value\n        \n        # Update global best\n        current_min = np.min(personal_best_values)\n        if current_min < global_best_value:\n            global_best_value = current_min\n            idx = np.argmin(personal_best_values)\n            global_best_position = personal_best_positions[idx].copy()\n    \n    return round(global_best_value, 4)\n```\n\n### Explanation\n- **Input Validation**: Ensures that the function handles invalid inputs gracefully by returning -1.\n- **Initialization**: Sets up the initial positions and velocities for each particle, and computes the initial best values.\n- **Velocity and Position Update**: Uses the PSO update rules to adjust each particle's velocity and position, ensuring the position remains within bounds.\n- **Personal and Global Best Update**: Continuously updates the best values found by each particle and the swarm as a whole.\n- **Result**: After completing all iterations, the best value found is returned, providing the minimum value of the Sphere function achieved by the swarm."}
{"task_id": 180, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a k-Nearest Neighbors (k-NN) algorithm that can handle both classification and regression tasks. The algorithm should predict the target values for a given test dataset based on the training data, using either majority voting for classification or averaging for regression.\n\n### Approach\n1. **Calculate Distances**: For each test sample, compute the Euclidean distance to every training sample. This helps in determining the nearest neighbors.\n2. **Sort and Select Neighbors**: Sort the training samples based on the computed distances and select the top k neighbors. If k is 0 or larger than the number of training samples, use all training samples.\n3. **Predict Based on Task**: Depending on whether the task is classification or regression, predict the target value:\n   - **Classification**: Use majority voting among the k nearest neighbors. In case of a tie, return the smallest label.\n   - **Regression**: Compute the arithmetic mean of the target values of the k nearest neighbors and round it to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\"\"\"\n    n_train = len(X_train)\n    if k <= 0:\n        effective_k = n_train\n    else:\n        effective_k = min(k, n_train)\n    \n    X_train_np = np.array(X_train)\n    X_test_np = np.array(X_test)\n    \n    predictions = []\n    \n    for x_test in X_test_np:\n        distances = np.sqrt(np.sum((X_train_np - x_test) ** 2, axis=1))\n        indices = np.argsort(distances)\n        k_indices = indices[:effective_k]\n        k_y = [y_train[i] for i in k_indices]\n        \n        if task == \"classification\":\n            counts = Counter(k_y)\n            max_count = max(counts.values())\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            prediction = min(candidates)\n        elif task == \"regression\":\n            prediction = round(np.mean(k_y), 4)\n        else:\n            raise ValueError(\"Invalid task type. Must be 'classification' or 'regression'.\")\n        \n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation\n1. **Effective k Calculation**: The effective number of neighbors (k) is determined based on the input k. If k is 0 or larger than the number of training samples, all training samples are used.\n2. **Distance Calculation**: Using numpy for efficient computation, the Euclidean distance between each test sample and all training samples is calculated.\n3. **Sorting and Selection**: The indices of the training samples are sorted based on their distances to the test sample, and the top k indices are selected.\n4. **Prediction**: Depending on the task, either the most frequent label (for classification) or the mean of the target values (for regression) is computed and added to the predictions list.\n\nThis approach ensures that the k-NN algorithm is efficiently implemented and handles both classification and regression tasks as required."}
{"task_id": 191, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a factory function that generates distance metric functions based on a given name. The generated function must validate the input vectors and compute the distance according to the specified metric. If the input vectors fail validation or the metric name is unsupported, the function returns -1.\n\n### Approach\n1. **Factory Function**: The `metric` function acts as a factory that returns a callable function based on the provided metric name. If the name is unsupported, it returns a function that always returns -1.\n2. **Validation**: The generated function validates the input vectors to ensure they are one-dimensional, of the same length, non-empty, and contain numeric values. For the cosine metric, it also checks that the vectors have non-zero norms.\n3. **Distance Calculation**: Depending on the metric name, the function computes the distance using the appropriate formula:\n   - **Euclidean**: Sum of squared differences, square-rooted.\n   - **Manhattan**: Sum of absolute differences.\n   - **Chebyshev**: Maximum absolute difference.\n   - **Cosine**: 1 minus the dot product divided by the product of the norms.\n4. **Rounding**: The result is rounded to four decimal places before returning.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\"\"\"\n    if name not in ['euclidean', 'manhattan', 'chebyshev', 'cosine']:\n        def d(x, y):\n            return -1\n        return d\n    \n    def d(x, y):\n        # Check if x and y are of correct types\n        if not (isinstance(x, (list, tuple, np.ndarray)) and isinstance(y, (list, tuple, np.ndarray))):\n            return -1\n        \n        # Try to convert to numpy arrays of float\n        try:\n            x = np.array(x, dtype=np.float64)\n            y = np.array(y, dtype=np.float64)\n        except ValueError:\n            return -1\n        \n        # Check if they are one-dimensional\n        if x.ndim != 1 or y.ndim != 1:\n            return -1\n        \n        # Check lengths\n        if len(x) != len(y) or len(x) == 0:\n            return -1\n        \n        # For cosine, check norms\n        if name == 'cosine':\n            norm_x = np.linalg.norm(x)\n            norm_y = np.linalg.norm(y)\n            if norm_x == 0 or norm_y == 0:\n                return -1\n        \n        # Compute the distance based on name\n        if name == 'euclidean':\n            diff = x - y\n            squared = diff ** 2\n            sum_sq = np.sum(squared)\n            distance = np.sqrt(sum_sq)\n        elif name == 'manhattan':\n            diff = x - y\n            abs_diff = np.abs(diff)\n            distance = np.sum(abs_diff)\n        elif name == 'chebyshev':\n            diff = x - y\n            abs_diff = np.abs(diff)\n            distance = np.max(abs_diff)\n        elif name == 'cosine':\n            dot = np.dot(x, y)\n            distance = 1 - (dot / (norm_x * norm_y))\n        else:\n            return -1\n        \n        # Round to four decimal places\n        return round(distance, 4)\n    \n    return d\n```\n\n### Explanation\n- **Factory Function**: The `metric` function checks if the provided name is valid. If not, it returns a function that always returns -1. Otherwise, it returns a function `d` that computes the specified distance metric.\n- **Validation**: The function `d` first checks if the inputs are valid vectors. It converts them to NumPy arrays and ensures they are one-dimensional, of the same length, and non-empty. For the cosine metric, it checks that both vectors have non-zero norms.\n- **Distance Calculation**: Depending on the metric name, the function computes the distance using the appropriate formula. The result is rounded to four decimal places before returning.\n\nThis approach ensures that the function handles various edge cases and invalid inputs gracefully, providing accurate distance calculations when the inputs are valid."}
{"task_id": 197, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function `action_stats` that provides statistics about the action space of a given reinforcement learning environment. The function should handle both continuous and discrete action spaces, as well as single and multi-dimensional cases.\n\n### Approach\n1. **Determine the Action Space Structure**: The function first checks if the action space is multi-dimensional using the `md_action` flag. If `md_action` is `True`, the action space is treated as a collection of sub-spaces stored in `env.action_space.spaces`. If `md_action` is `False`, the action space is treated as a single space.\n2. **Identify Action Types**: For each sub-space (or the single space if not multi-dimensional), the function checks if the space is discrete or continuous. Discrete spaces have an `n` attribute indicating the number of actions, while continuous spaces are identified by the presence of a `shape` attribute.\n3. **Collect Action Statistics**: The function collects the number of actions per dimension, using `numpy.inf` for continuous dimensions. It also determines the total number of action dimensions.\n4. **Generate Action IDs**: If any dimension is continuous, the function sets `action_ids` to `None` because continuous spaces have infinitely many actions. If all dimensions are discrete, it computes the Cartesian product of all dimensions to generate all possible action tuples.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    # Determine the sub-spaces based on whether the action space is multi-dimensional\n    if md_action:\n        sub_spaces = env.action_space.spaces\n    else:\n        sub_spaces = [env.action_space]\n    \n    # Collect the number of actions per dimension\n    n_actions_per_dim = []\n    for sub_space in sub_spaces:\n        if hasattr(sub_space, 'n'):\n            n_actions_per_dim.append(sub_space.n)\n        else:\n            n_actions_per_dim.append(np.inf)\n    \n    # Determine the action dimensionality\n    action_dim = len(n_actions_per_dim)\n    \n    # Determine the action IDs\n    if any(x == np.inf for x in n_actions_per_dim):\n        action_ids = None\n    else:\n        ranges = [range(n) for n in n_actions_per_dim]\n        action_ids = list(product(*ranges))\n    \n    return (n_actions_per_dim, action_ids, action_dim)\n```\n\n### Explanation\n- **Determine Sub-Spaces**: The function checks if the action space is multi-dimensional and retrieves the appropriate sub-spaces.\n- **Identify Action Types**: Each sub-space is checked for discrete (`n` attribute) or continuous (`shape` attribute) nature.\n- **Collect Statistics**: The number of actions per dimension is collected, with `numpy.inf` for continuous dimensions.\n- **Generate Action IDs**: If all dimensions are discrete, the Cartesian product of all dimensions is computed to generate all possible action tuples. If any dimension is continuous, `action_ids` is set to `None`.\n\nThis approach ensures that the function correctly handles both single and multi-dimensional action spaces, as well as discrete and continuous action types, providing the required statistics efficiently."}
{"task_id": 198, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to update the word\u2013topic distribution matrix \u03b2 in Latent Dirichlet Allocation (LDA). The matrix \u03b2 stores the probability of each word being generated by each topic. The update is done using the variational parameter \u03c6 and the corpus of documents.\n\n### Approach\n1. **Initialization**: Create an accumulator matrix `beta_accum` of size V x T (where V is the vocabulary size and T is the number of topics) initialized to zeros. This matrix will store the sum of \u03c6 values for each word-topic pair.\n2. **Accumulation**: For each document, iterate through each token. For each token, add the corresponding \u03c6 values to the accumulator matrix based on the word index of the token.\n3. **Normalization**: After accumulating the sums, normalize each column of the accumulator matrix so that each column sums to 1. This ensures that the probabilities for each topic across all words are valid.\n4. **Rounding**: Round each element of the normalized matrix to 4 decimal places.\n5. **Conversion**: Convert the resulting matrix into a list of lists format as required by the problem.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    if not phi:\n        T = 0\n    else:\n        T = phi[0].shape[1]\n    beta_accum = np.zeros((V, T), dtype=np.float64)\n    \n    for d in range(len(corpus)):\n        words = corpus[d]\n        phi_d = phi[d]\n        for n in range(len(words)):\n            word = words[n]\n            beta_accum[word] += phi_d[n]\n    \n    for t in range(T):\n        sum_t = beta_accum[:, t].sum()\n        if sum_t == 0:\n            continue\n        beta_accum[:, t] /= sum_t\n    \n    beta = beta_accum.round(4)\n    beta_list = beta.tolist()\n    \n    return beta_list\n```\n\n### Explanation\n1. **Initialization**: The `beta_accum` matrix is initialized to zeros to store the accumulated \u03c6 values for each word-topic pair.\n2. **Accumulation**: For each document and each token within the document, the corresponding \u03c6 values are added to the accumulator matrix based on the word index. This step efficiently aggregates the contributions of each word to each topic.\n3. **Normalization**: Each column of the accumulator matrix is normalized to ensure that the probabilities for each topic sum to 1. This step is crucial for maintaining valid probability distributions.\n4. **Rounding**: The normalized values are rounded to 4 decimal places to meet the problem's precision requirements.\n5. **Conversion**: The resulting matrix is converted into a list of lists format, ensuring the output matches the required structure.\n\nThis approach ensures that the word\u2013topic distribution matrix \u03b2 is updated correctly and efficiently, adhering to the constraints and requirements of the problem."}
{"task_id": 202, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Partitioning Around Medoids (PAM) clustering algorithm. The goal is to group samples into k clusters by iteratively improving a set of representative points called medoids. The algorithm is deterministic, ensuring that the results can be tested reliably.\n\n### Approach\n1. **Initialization**: Start with the first k samples of the data matrix as the initial medoids.\n2. **Iterative Improvement**: Repeatedly attempt to reduce the total clustering cost by swapping any current medoid with any non-medoid sample. The swap that gives the largest cost reduction is accepted if it strictly lowers the cost. This process continues until no further improvement can be made.\n3. **Label Assignment**: After convergence, each sample is assigned to the closest medoid. In case of ties, the medoid that appears first in the list is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\"\"\"\n    n = X.shape[0]\n    if k == 0:\n        return []\n    if k == 1:\n        return [0] * n\n    \n    # Initialize medoid indices as the first k samples\n    medoid_indices = list(range(k))\n    current_cost = compute_cost(X, medoid_indices)\n    improvement = True\n    \n    while improvement:\n        max_delta = 0\n        best_new_medoid_indices = None\n        medoid_indices_set = set(medoid_indices)\n        \n        for m in medoid_indices:\n            for s in range(n):\n                if s not in medoid_indices_set:\n                    # Create new medoid indices by replacing m with s\n                    new_medoid_indices = medoid_indices.copy()\n                    idx = new_medoid_indices.index(m)\n                    new_medoid_indices[idx] = s\n                    new_cost = compute_cost(X, new_medoid_indices)\n                    delta = current_cost - new_cost\n                    if delta > max_delta:\n                        max_delta = delta\n                        best_new_medoid_indices = new_medoid_indices.copy()\n        \n        if max_delta > 0:\n            medoid_indices = best_new_medoid_indices\n            current_cost = compute_cost(X, medoid_indices)\n        else:\n            improvement = False\n    \n    # Assign labels based on the closest medoid\n    labels = []\n    for x in X:\n        min_dist = np.inf\n        best_idx = 0\n        for idx, m_idx in enumerate(medoid_indices):\n            m = X[m_idx]\n            dist = np.linalg.norm(x - m)\n            if dist < min_dist:\n                min_dist = dist\n                best_idx = idx\n            elif dist == min_dist:\n                if idx < best_idx:\n                    best_idx = idx\n        labels.append(best_idx)\n    \n    return labels\n\ndef compute_cost(X, medoid_indices):\n    medoids = X[medoid_indices]\n    cost = 0.0\n    for x in X:\n        min_dist = np.inf\n        for m in medoids:\n            dist = np.linalg.norm(x - m)\n            if dist < min_dist:\n                min_dist = dist\n        cost += min_dist\n    return cost\n```\n\n### Explanation\n1. **Initialization**: The medoids are initialized as the first k samples of the input data matrix X.\n2. **Cost Calculation**: The total clustering cost is computed as the sum of the Euclidean distances from each sample to its closest medoid.\n3. **Swapping Process**: For each medoid, every possible swap with a non-medoid sample is considered. The swap that results in the maximum reduction of the total cost is performed if it strictly reduces the cost.\n4. **Convergence Check**: The process continues until no further swaps can reduce the cost, indicating convergence.\n5. **Label Assignment**: Each sample is assigned to the closest medoid, with ties broken by choosing the medoid that appears first in the list.\n\nThis approach ensures that the clustering is done deterministically and efficiently, adhering to the problem constraints and requirements."}
{"task_id": 216, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic Net regression from scratch using batch gradient descent. The model must be able to generate polynomial features, standardize the features, learn the weight vector by minimizing the mean-squared error with an Elastic Net penalty, and make predictions on unseen data.\n\n### Approach\n1. **Input Validation**: Check if the input matrices `x_train` and `x_test` have the same number of features and if the number of iterations is valid.\n2. **Generate Polynomial Features**: For each feature in the input data, generate polynomial features up to the specified degree and include a bias term.\n3. **Standardize Features**: Standardize the non-bias features to have zero mean and unit variance using the training data's statistics.\n4. **Initialize Weights**: Start with a zero-initialized weight vector.\n5. **Batch Gradient Descent**: Update the weights iteratively using the gradient of the cost function, which includes both L1 and L2 regularization terms.\n6. **Make Predictions**: Use the trained weights to predict the target values for the test data and return the results rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\"\"\"\n    # Convert inputs to numpy arrays\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Check for invalid inputs\n    if x_train.shape[1] != x_test.shape[1] or n_iterations < 1:\n        return -1\n    \n    # Generate polynomial features for training and test sets\n    def generate_polynomial_features(matrix, degree):\n        poly = []\n        for sample in matrix:\n            features = []\n            for x_i in sample:\n                for d in range(1, degree + 1):\n                    features.append(x_i ** d)\n            poly.append([1.0] + features)\n        return np.array(poly)\n    \n    x_train_poly = generate_polynomial_features(x_train, degree)\n    x_test_poly = generate_polynomial_features(x_test, degree)\n    \n    # Standardize non-bias features\n    means = np.mean(x_train_poly[:, 1:], axis=0)\n    stds = np.std(x_train_poly[:, 1:], axis=0)\n    \n    # Avoid division by zero by adding a small epsilon to stds\n    eps = 1e-8\n    stds = np.maximum(stds, eps)\n    \n    x_train_poly[:, 1:] = (x_train_poly[:, 1:] - means) / stds\n    x_test_poly[:, 1:] = (x_test_poly[:, 1:] - means) / stds\n    \n    # Initialize weights\n    m, n_features = x_train_poly.shape\n    w = np.zeros(n_features)\n    \n    # Perform gradient descent\n    for _ in range(n_iterations):\n        y_pred = x_train_poly @ w\n        error = y_pred - y_train\n        \n        # Compute gradient\n        grad = np.zeros_like(w)\n        grad[0] = (1.0 / m) * np.sum(error * x_train_poly[:, 0])\n        \n        x_without_bias = x_train_poly[:, 1:]\n        reg_term = reg_factor * (l1_ratio * np.sign(w[1:]) + (1 - l1_ratio) * w[1:])\n        grad[1:] = (1.0 / m) * (x_without_bias.T @ error) + reg_term\n        \n        # Update weights\n        w -= learning_rate * grad\n    \n    # Make predictions on test set\n    y_pred_test = x_test_poly @ w\n    predictions = np.round(y_pred_test, 4).tolist()\n    \n    return predictions\n```\n\n### Explanation\n1. **Input Validation**: The function first converts the input lists to numpy arrays and checks if the number of features in `x_train` and `x_test` are the same and if the number of iterations is valid.\n2. **Polynomial Features**: The `generate_polynomial_features` function creates polynomial features for each sample up to the specified degree and includes a bias term.\n3. **Standardization**: The non-bias features are standardized using the mean and standard deviation from the training data to ensure zero mean and unit variance.\n4. **Weight Initialization**: The weight vector is initialized to zeros.\n5. **Gradient Descent**: The weights are updated iteratively using the gradient of the cost function, which includes both L1 and L2 regularization terms. The L1 term uses the sub-gradient, and the L2 term uses the standard gradient.\n6. **Predictions**: After training, the model makes predictions on the test data, which are then rounded to four decimal places and returned as a list."}
{"task_id": 217, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute three core components of the logistic loss function: the gradient, the Hessian, and the probability obtained by applying the sigmoid transformation. These components are essential for binary classification algorithms such as Gradient Boosting and Newton-based optimization.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: The function starts by converting the input lists, tuples, or NumPy arrays into NumPy arrays for efficient vectorized computations.\n2. **Compute the Gradient**: The gradient for each observation is calculated using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves computing the term `-actual * predicted`, applying the sigmoid function to this term, and then multiplying by the actual values.\n3. **Compute the Probability and Hessian**: The probability is obtained by applying the sigmoid function to the predicted values. The Hessian is then computed as the product of the probability and its complement (1 - probability).\n4. **Rounding**: Each computed value (gradient, Hessian, and probability) is rounded to exactly six decimal places to meet the precision requirement.\n5. **Convert Results to Lists**: The NumPy arrays are converted back to Python lists before returning the results.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized operations\n    actual_np = np.array(actual)\n    predicted_np = np.array(predicted)\n    \n    # Compute the term for the gradient\n    term = -actual_np * predicted_np\n    sigmoid_term = 1 / (1 + np.exp(-term))\n    gradient = actual_np * sigmoid_term\n    \n    # Compute the probability and Hessian\n    prob = 1 / (1 + np.exp(-predicted_np))\n    hessian = prob * (1 - prob)\n    \n    # Round each value to 6 decimal places\n    gradient_rounded = np.round(gradient, 6)\n    hessian_rounded = np.round(hessian, 6)\n    prob_rounded = np.round(prob, 6)\n    \n    # Convert NumPy arrays to Python lists\n    gradient_list = gradient_rounded.tolist()\n    hessian_list = hessian_rounded.tolist()\n    probability_list = prob_rounded.tolist()\n    \n    return (gradient_list, hessian_list, probability_list)\n```\n\n### Explanation\n- **Gradient Calculation**: The gradient is computed using the formula `actual_i * sigmoid(-actual_i * predicted_i)`. This involves element-wise multiplication and the application of the sigmoid function.\n- **Probability and Hessian**: The probability is derived from the sigmoid function applied to the predicted values. The Hessian is the product of the probability and its complement, which gives the second derivative of the logistic loss.\n- **Efficiency**: Using NumPy arrays allows for efficient vectorized operations, which is crucial for handling large datasets.\n- **Precision**: Each result is rounded to six decimal places to ensure the required precision, and the results are converted back to Python lists for compatibility with the expected output format."}
{"task_id": 218, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the outcome of a Blackjack game based on the final hands of the player and the dealer. The goal is to determine the reward for the player according to the given rules.\n\n### Approach\n1. **Calculate Hand Value**: The value of a hand is the sum of its cards, with aces (1s) potentially being counted as 11 if it helps the hand not bust. The helper function `calculate_hand` computes the score and checks if the hand is a natural blackjack.\n2. **Determine Natural Blackjack**: A natural blackjack is a two-card hand consisting of an Ace (1) and a 10-value card (10). The helper function checks for this condition.\n3. **Evaluate Outcome**: Based on the scores and whether either hand is a natural blackjack, determine the reward:\n   - If the player busts, the reward is -1.\n   - If the dealer busts and the player does not, the reward is +1.\n   - Compare scores if neither busts. If the player's score is higher, they win; if the dealer's score is higher, the player loses; if scores are equal, it's a draw.\n   - If the player wins with a natural blackjack, the reward is +1.5.\n\n### Solution Code\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    def calculate_hand(hand):\n        sum_base = sum(hand)\n        num_aces = hand.count(1)\n        if num_aces > 0 and sum_base <= 11:\n            possible_score = sum_base + 10\n            if possible_score <= 21:\n                score = possible_score\n            else:\n                score = sum_base\n        else:\n            score = sum_base\n        is_natural = (len(hand) == 2) and (1 in hand) and (10 in hand)\n        return score, is_natural\n\n    player_score, player_natural = calculate_hand(player)\n    dealer_score, dealer_natural = calculate_hand(dealer)\n\n    if player_score > 21:\n        return -1.0\n    if dealer_score > 21:\n        return 1.0\n    if player_score > dealer_score:\n        if player_natural and not dealer_natural:\n            return 1.5\n        else:\n            return 1.0\n    elif dealer_score > player_score:\n        return -1.0\n    else:\n        return 0.0\n```\n\n### Explanation\n1. **Helper Function**: `calculate_hand` computes the score of a hand and checks if it is a natural blackjack. It treats aces as 1 by default but adjusts to 11 if beneficial without busting.\n2. **Score Calculation**: The score is determined by summing the card values, adjusting for aces if necessary.\n3. **Outcome Evaluation**: The main function evaluates the outcome based on the scores and natural blackjack status of both hands, applying the rules to determine the reward.\n\nThis approach ensures that all possible outcomes are handled correctly, providing the correct reward based on the game rules."}
{"task_id": 221, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Mean Squared Logarithmic Error (MSLE) metric for evaluating regression models. The MSLE is a measure of the difference between the actual and predicted values, and it is particularly useful when the target values are positive and the model's predictions are also non-negative.\n\n### Approach\nThe approach to solving this problem involves two main functions: `squared_log_error` and `mean_squared_log_error`. The `squared_log_error` function computes the element-wise squared logarithmic errors, while the `mean_squared_log_error` function calculates the mean of these squared errors after performing input validation.\n\n1. **Input Validation**: The main function first converts the input sequences into NumPy arrays. It then checks if the shapes of the actual and predicted arrays are the same, if the arrays are non-empty, and if all values are non-negative. If any of these checks fail, the function returns -1.\n\n2. **Squared Logarithmic Error Calculation**: The helper function computes the squared difference between the logarithms of (1 + actual values) and (1 + predicted values).\n\n3. **Mean Calculation**: The main function computes the mean of the squared logarithmic errors and rounds the result to four decimal places before returning it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Compute element-wise squared logarithmic errors.\"\"\"\n    log_actual = np.log(1 + actual)\n    log_predicted = np.log(1 + predicted)\n    diff = log_actual - log_predicted\n    return diff ** 2\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values. All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values. Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals. If the inputs are invalid, the\n        function returns -1.\n    \"\"\"\n    # Convert inputs to NumPy arrays\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Check if shapes are the same\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Check if arrays are empty\n    if actual.size == 0:\n        return -1\n    \n    # Check for non-negative values\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    \n    # Compute squared log errors\n    sle = squared_log_error(actual, predicted)\n    \n    # Compute mean and round\n    msle = np.mean(sle)\n    return round(msle, 4)\n```\n\n### Explanation\n1. **Input Conversion and Validation**: The main function converts the input sequences into NumPy arrays to facilitate element-wise operations. It then checks if the arrays have the same shape, are non-empty, and contain only non-negative values. If any of these conditions are not met, the function returns -1.\n\n2. **Squared Logarithmic Error Calculation**: The helper function computes the logarithm of (1 + actual values) and (1 + predicted values), finds the difference between these logarithms, squares the difference, and returns the result as an array.\n\n3. **Mean Calculation**: The main function computes the mean of the squared logarithmic errors using NumPy's `mean` function, rounds the result to four decimal places, and returns it as a float.\n\nThis approach ensures that the MSLE is computed efficiently and correctly, with proper handling of input validation to ensure the function behaves as expected under various input conditions."}
{"task_id": 222, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the sigmoid (logistic) activation function, which can handle scalar values, lists, and NumPy arrays. The function must be numerically stable for very large positive or negative inputs and return the results rounded to four decimal places.\n\n### Approach\nThe sigmoid function is defined as:\n\\[ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} \\]\n\nFor numerical stability, especially when dealing with very large values of \\( z \\), we use a different computation method based on whether \\( z \\) is non-negative or negative:\n- For \\( z \\geq 0 \\), compute \\( \\frac{1}{1 + e^{-z}} \\).\n- For \\( z < 0 \\), compute \\( \\frac{e^{z}}{1 + e^{z}} \\).\n\nThe function handles both scalar and array-like inputs. For scalar inputs, it directly computes the sigmoid value. For array-like inputs, it converts the input to a NumPy array, computes the sigmoid element-wise, and then converts the result back to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        if z >= 0:\n            s = 1.0 / (1.0 + np.exp(-z))\n        else:\n            s = np.exp(z) / (1.0 + np.exp(z))\n        return round(s, 4)\n    else:\n        arr = np.asarray(z)\n        sigmoid = np.where(arr >= 0, 1/(1 + np.exp(-arr)), np.exp(arr)/(1 + np.exp(arr)))\n        rounded = np.round(sigmoid, 4)\n        return rounded.tolist()\n```\n\n### Explanation\n1. **Scalar Input Handling**: If the input \\( z \\) is a scalar (int or float), the function checks if \\( z \\) is non-negative or negative and computes the sigmoid using the appropriate formula. The result is then rounded to four decimal places and returned as a float.\n\n2. **Array-like Input Handling**: If the input \\( z \\) is a list or a NumPy array, it is converted to a NumPy array. The function uses NumPy's vectorized operations to compute the sigmoid for each element. The `np.where` function is used to apply the correct formula based on whether each element is non-negative or negative. The results are rounded to four decimal places and converted back to a Python list before being returned.\n\nThis approach ensures that the function is efficient, numerically stable, and handles various input types correctly."}
{"task_id": 224, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky ReLU activation function, which is commonly used in neural networks to avoid the issue of \"dying\" neurons. The function should handle various input types, including scalars, nested lists, and numpy arrays, and return the activated values in the same structure as the input.\n\n### Approach\nThe Leaky ReLU activation function is defined as follows:\n- For any input value \\( x \\geq 0 \\), the output is \\( x \\).\n- For any input value \\( x < 0 \\), the output is \\( a \\times x \\), where \\( a \\) is a parameter in the range [0, 1).\n\nThe function must handle different input types:\n1. **Scalars**: Directly apply the Leaky ReLU transformation.\n2. **Lists (possibly nested)**: Recursively process each element to apply the transformation.\n3. **Numpy Arrays**: Use vectorized operations for efficient computation and then convert the result to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    if isinstance(z, np.ndarray):\n        activated = np.where(z >= 0, z, a * z)\n        return activated.tolist()\n    elif isinstance(z, list):\n        return [leaky_relu(elem, a) for elem in z]\n    else:\n        return z if z >= 0 else a * z\n```\n\n### Explanation\n1. **Handling Numpy Arrays**: If the input `z` is a numpy array, we use `np.where` to apply the Leaky ReLU transformation element-wise. This is efficient and leverages numpy's vectorized operations. The result is then converted to a Python list using `tolist()` to maintain the required output format.\n\n2. **Handling Lists**: If the input `z` is a list, we recursively process each element. This ensures that nested lists are handled correctly, maintaining the structure of the input.\n\n3. **Handling Scalars**: If the input `z` is a scalar, we directly apply the Leaky ReLU transformation. If `z` is non-negative, it remains unchanged; otherwise, it is multiplied by the parameter `a`.\n\nThis approach ensures that the function works efficiently for all specified input types and maintains the structure of the input in the output."}
{"task_id": 241, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to initialize a tensor with an orthogonal matrix using the algorithm popularized by Saxe et al. (2014). This method ensures that the weights of a layer in a deep learning model are initialized in a way that prevents the activations from shrinking or exploding at the beginning of training.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Check Tensor Dimensions**: If the input shape has fewer than two dimensions, return -1 as the problem requires at least two dimensions for an orthogonal matrix.\n\n2. **Generate Random Matrix**: Create a 2-D matrix filled with samples from the standard normal distribution. The shape of this matrix is determined by the first dimension of the input shape and the product of the remaining dimensions.\n\n3. **Singular Value Decomposition (SVD)**: Compute the SVD of the generated matrix. This decomposes the matrix into three components: U, \u03a3, and V^T.\n\n4. **Select Orthogonal Matrix**: Depending on the dimensions of the original matrix, select either U or V^T as the orthogonal matrix. If U has the same shape as the original matrix, use U; otherwise, use V^T.\n\n5. **Reshape and Scale**: Reshape the selected orthogonal matrix back to the original input shape and scale it by the specified factor.\n\n6. **Return Result**: Convert the resulting NumPy array to a list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    \n    d0 = shape[0]\n    product_of_rest = 1\n    for dim in shape[1:]:\n        product_of_rest *= dim\n    \n    A = np.random.randn(d0, product_of_rest)\n    U, s, Vh = np.linalg.svd(A, full_matrices=False)\n    \n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vh\n    \n    Q_reshaped = Q.reshape(shape)\n    result = Q_reshaped * scale\n    return result.tolist()\n```\n\n### Explanation\n1. **Check Tensor Dimensions**: The function first checks if the input shape has fewer than two dimensions. If so, it returns -1 as specified.\n\n2. **Generate Random Matrix**: Using `np.random.randn`, a matrix `A` is created with the shape `(d0, product_of_rest)`, where `d0` is the first dimension of the input shape and `product_of_rest` is the product of the remaining dimensions.\n\n3. **Singular Value Decomposition (SVD)**: The matrix `A` is decomposed into U, \u03a3, and V^T using `np.linalg.svd` with `full_matrices=False` to ensure the correct dimensions for U and V^T.\n\n4. **Select Orthogonal Matrix**: The orthogonal matrix Q is chosen based on the shape of U. If U matches the shape of A, Q is set to U; otherwise, Q is set to V^T.\n\n5. **Reshape and Scale**: The matrix Q is reshaped to the original input shape and scaled by the specified factor.\n\n6. **Return Result**: The resulting matrix is converted to a list of lists using `tolist()` and returned.\n\nThis approach ensures that the resulting tensor maintains the orthogonality condition, which is crucial for stable training in deep learning models."}
{"task_id": 243, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the forward pass of a small fully-connected Actor-Critic network using NumPy. The network consists of three dense layers followed by two separate output heads for the actor and critic. The weights and biases are fixed, which allows us to compute the network's output deterministically.\n\n### Approach\n1. **Network Architecture**: The network has three dense layers with ReLU activation, followed by an actor head with a softmax activation and a critic head with no activation.\n2. **Fixed Weights and Biases**: All weights are set to 1.0 and biases to 0.0, simplifying the computations.\n3. **Forward Pass Steps**:\n   - **Dense-1**: Input (3 neurons) \u2192 4 neurons with ReLU activation.\n   - **Dense-2**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Dense-3**: 4 neurons \u2192 4 neurons with ReLU activation.\n   - **Actor Head**: 4 neurons \u2192 3 neurons with softmax activation to get action probabilities.\n   - **Critic Head**: 4 neurons \u2192 1 neuron to get the state value.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    state = np.array(state)\n    \n    # Dense-1: input (3) -> 4 neurons, ReLU\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    z1 = np.maximum(0, state.dot(W1) + b1)\n    \n    # Dense-2: 4 -> 4 neurons, ReLU\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    z2 = np.maximum(0, z1.dot(W2) + b2)\n    \n    # Dense-3: 4 -> 4 neurons, ReLU\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    z3 = np.maximum(0, z2.dot(W3) + b3)\n    \n    # Actor head: 4 -> 3 neurons, Softmax\n    W_a = np.ones((4, 3))\n    b_a = np.zeros(3)\n    logits = z3.dot(W_a) + b_a\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    \n    # Critic head: 4 -> 1 neuron\n    W_c = np.ones((4, 1))\n    b_c = np.zeros(1)\n    state_value = z3.dot(W_c) + b_c\n    state_value = state_value[0]  # Extract the scalar value\n    \n    # Rounding to four decimal places\n    action_probs = [round(p, 4) for p in action_probs]\n    state_value = round(state_value, 4)\n    \n    return action_probs, state_value\n```\n\n### Explanation\n1. **Dense Layers**: Each dense layer transforms the input using matrix multiplication with weights and biases, followed by ReLU activation. Given the fixed weights and biases, each transformation simplifies to a sum of the input elements.\n2. **Actor Head**: The actor head computes logits for each action, which are then converted to probabilities using the softmax function. Due to identical logits, each action has an equal probability of 1/3.\n3. **Critic Head**: The critic head computes the state value as a linear transformation of the output from the last dense layer, resulting in a scalar value.\n\nThis approach ensures that the network's output is computed efficiently and deterministically, leveraging the fixed weights and biases for simplicity."}
{"task_id": 249, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs a forward pass through a three-layer Actor-Critic neural network using NumPy. The network consists of three shared layers followed by two independent heads: an actor head that outputs action probabilities and a critic head that estimates the state value.\n\n### Approach\n1. **Convert Input State**: Convert the input state list into a NumPy array for easier manipulation.\n2. **Shared Layers**: Process the state through three fully connected layers with ReLU activation. Each layer involves an affine transformation (matrix multiplication followed by addition of a bias) and applying the ReLU activation function.\n3. **Actor Head**: After the shared layers, the output is fed into the actor head, which consists of a dense layer followed by the softmax function to convert the output into a probability distribution.\n4. **Critic Head**: The same output from the shared layers is fed into the critic head, which is a dense layer that outputs a scalar value representing the state value.\n5. **Rounding**: Both the action probabilities and the state value are rounded to four decimal places before being returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\"\"\"\n    state = np.array(state)\n    \n    # Shared layers\n    W1 = np.array(params['W1'])\n    b1 = np.array(params['b1'])\n    h1 = np.dot(state, W1) + b1\n    h1 = np.maximum(0, h1)\n    \n    W2 = np.array(params['W2'])\n    b2 = np.array(params['b2'])\n    h2 = np.dot(h1, W2) + b2\n    h2 = np.maximum(0, h2)\n    \n    W3 = np.array(params['W3'])\n    b3 = np.array(params['b3'])\n    h3 = np.dot(h2, W3) + b3\n    h3 = np.maximum(0, h3)\n    \n    # Actor head\n    Wa = np.array(params['Wa'])\n    ba = np.array(params['ba'])\n    actor_output = np.dot(h3, Wa) + ba\n    actor_output_exp = np.exp(actor_output)\n    probabilities = actor_output_exp / np.sum(actor_output_exp)\n    probabilities = np.round(probabilities, 4).tolist()\n    \n    # Critic head\n    Wc = np.array(params['Wc'])\n    bc = np.array(params['bc'])\n    value = np.dot(h3, Wc) + bc\n    value = np.round(value, 4).item()\n    \n    return probabilities, value\n```\n\n### Explanation\n1. **State Conversion**: The input state is converted into a NumPy array to facilitate matrix operations.\n2. **Shared Layers Processing**: The state is passed through three layers, each involving an affine transformation followed by ReLU activation. This results in a hidden representation that is used by both the actor and critic heads.\n3. **Actor Head Processing**: The hidden representation is transformed into action scores using the actor head's weights and biases. The softmax function is applied to these scores to obtain a probability distribution over possible actions.\n4. **Critic Head Processing**: The same hidden representation is used to estimate the state value, which is a scalar output from the critic head.\n5. **Rounding**: The action probabilities and state value are rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the network processes the input state efficiently and produces the required outputs for both the actor and critic components."}
{"task_id": 253, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Elastic-Net penalty and its gradient for a given weight vector. The Elastic-Net penalty is a combination of L1 and L2 regularization, which helps in reducing model complexity and preventing over-fitting. The function should return either the penalty value or the gradient based on the input flag.\n\n### Approach\n1. **Problem Analysis**: The Elastic-Net penalty is defined as a convex combination of L1 and L2 regularization. The penalty function is given by:\n   \\[\n   R(w) = \\alpha \\cdot [\\lambda \\cdot ||w||_2 + (1 - \\lambda) \\cdot 0.5 \\cdot w^T w]\n   \\]\n   where \\( ||w||_2 \\) is the L2 norm of the weight vector \\( w \\), and \\( w^T w \\) is the squared L2 norm. The gradient of this penalty function is given by:\n   \\[\n   \\nabla R(w) = \\alpha \\cdot [\\lambda \\cdot \\text{sign}(w) + (1 - \\lambda) \\cdot w]\n   \\]\n   where \\( \\text{sign}(w) \\) is the sign function applied element-wise to \\( w \\).\n\n2. **Algorithm Design**:\n   - Convert the input weight vector to a NumPy array for efficient computation.\n   - Compute the penalty value if the gradient flag is False.\n   - Compute the gradient if the gradient flag is True.\n\n3. **Penalty Calculation**:\n   - Compute the L2 norm of the weight vector.\n   - Compute the sum of the squares of the weight vector elements.\n   - Calculate the L1 and L2 parts of the penalty using the given formula.\n   - Combine these parts and multiply by the regularization strength \\( \\alpha \\).\n\n4. **Gradient Calculation**:\n   - Compute the sign of each element in the weight vector.\n   - Calculate the gradient using the given formula, combining the contributions from both L1 and L2 regularization.\n\n5. **Edge Cases**: Handle cases where the weight vector is empty or contains zeros appropriately.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.asarray(w)\n    if gradient:\n        sign_w = np.sign(w)\n        gradient = alpha * (l1_ratio * sign_w + (1 - l1_ratio) * w)\n        gradient = [round(x, 4) for x in gradient]\n        return gradient\n    else:\n        l2_norm = np.linalg.norm(w, 2)\n        sum_w_sq = np.dot(w, w)\n        l1_part = l1_ratio * l2_norm\n        l2_part = (1 - l1_ratio) * 0.5 * sum_w_sq\n        penalty = alpha * (l1_part + l2_part)\n        return round(penalty, 4)\n```\n\n### Explanation\n- **Penalty Calculation**: The function first converts the input weight vector into a NumPy array for efficient computation. It then calculates the L2 norm and the sum of squares of the weight vector elements. Using these values, it computes the L1 and L2 parts of the penalty and combines them, scaling by the regularization strength \\( \\alpha \\).\n\n- **Gradient Calculation**: If the gradient flag is True, the function computes the sign of each element in the weight vector. It then calculates the gradient by combining the contributions from both L1 and L2 regularization, scaling by \\( \\alpha \\) and rounding each element to 4 decimal places.\n\nThis approach ensures that the function efficiently computes either the penalty or the gradient based on the input parameters, adhering to the problem constraints and requirements."}
{"task_id": 256, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a numerical gradient checker for a vanilla Recurrent Neural Network (RNN) parameter using the centered finite-difference formula. This technique helps in verifying the correctness of the analytically computed gradients during back-propagation.\n\n### Approach\n1. **Special Cases Handling**: If the parameter name is \"X\" or \"y\", the function should return `None` immediately as these are not trainable parameters. If the parameter name is \"Ba\" or \"Bx\", it should be mapped to their respective lower-case keys \"ba\" or \"bx\".\n\n2. **Parameter Extraction**: Extract the parameter from the model's parameters dictionary using the determined key.\n\n3. **Gradient Calculation**: For each element in the parameter matrix:\n   - Perturb the element by +\u03b5 and compute the loss after running the forward pass for all time steps.\n   - Perturb the element by -\u03b5 and compute the loss similarly.\n   - Use the centered finite-difference formula to approximate the partial derivative for the element.\n\n4. **Internal State Management**: Before each forward pass, reset the model's internal state using `flush_gradients()` to ensure no residual state affects the computation.\n\n5. **Result Transposition**: The computed gradient matrix is transposed before returning to match the required output format.\n\n### Solution Code\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\"\"\"\n    if param_name in [\"X\", \"y\"]:\n        return None\n    elif param_name in [\"Ba\", \"Bx\"]:\n        param_key = param_name.lower()\n    else:\n        param_key = param_name\n    \n    theta = model.parameters.get(param_key, None)\n    if theta is None:\n        return None  # Handle case where parameter doesn't exist\n    \n    original_theta = theta.copy().astype(np.float64)\n    grads = np.zeros_like(original_theta, dtype=np.float64)\n    \n    for i in range(original_theta.shape[0]):\n        for j in range(original_theta.shape[1]):\n            # Compute loss for theta + epsilon\n            theta_plus = original_theta.copy()\n            theta_plus[i, j] += epsilon\n            model.parameters[param_key] = theta_plus\n            model.flush_gradients()\n            predictions_plus = []\n            for t in range(n_t):\n                x_t = X[:, :, t]\n                y_t = model.forward(x_t)\n                predictions_plus.append(y_t)\n            loss_plus = loss_func(predictions_plus)\n            \n            # Compute loss for theta - epsilon\n            theta_minus = original_theta.copy()\n            theta_minus[i, j] -= epsilon\n            model.parameters[param_key] = theta_minus\n            model.flush_gradients()\n            predictions_minus = []\n            for t in range(n_t):\n                x_t = X[:, :, t]\n                y_t = model.forward(x_t)\n                predictions_minus.append(y_t)\n            loss_minus = loss_func(predictions_minus)\n            \n            # Calculate the gradient\n            grads[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n            \n            # Restore the original parameter\n            model.parameters[param_key] = original_theta\n    \n    return grads.T\n```\n\n### Explanation\n- **Special Cases Handling**: The function immediately returns `None` for non-trainable parameters and adjusts the parameter key for \"Ba\" and \"Bx\".\n- **Parameter Extraction**: The parameter is extracted from the model's dictionary and copied to ensure no side effects during perturbation.\n- **Gradient Calculation**: For each element, the function perturbs the parameter, runs the forward pass, computes the loss, and uses the centered finite-difference formula to estimate the gradient.\n- **Internal State Management**: The model's internal state is reset before each forward pass to ensure accurate loss computation.\n- **Result Transposition**: The gradient matrix is transposed to match the required output format, providing the numerical gradient for each parameter element."}
{"task_id": 257, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost algorithm using decision stumps as weak learners. AdaBoost works by iteratively reweighting training examples and combining weak classifiers to form a strong classifier. Each weak classifier is a decision stump, which is a one-level decision tree that makes decisions based on a single feature and a threshold.\n\n### Approach\n1. **Initialization**: Start by initializing the weights for each training example. Initially, each weight is set to 1/m, where m is the number of training examples.\n2. **Iterative Training**: For each iteration, find the best decision stump that minimizes the weighted classification error. This involves:\n   - Considering each feature and generating possible thresholds.\n   - For each threshold, compute the error for both possible polarities (which side of the threshold is considered positive).\n   - Selecting the threshold and polarity that yield the minimum error.\n3. **Update Weights**: After selecting the best stump, compute its weight (alpha) and update the weights of the training examples based on whether they were misclassified by the stump.\n4. **Store Stumps**: Store each stump's parameters (feature, threshold, polarity, and alpha) for later use in prediction.\n5. **Prediction**: For each test example, aggregate the predictions from all stumps by computing the weighted sum of their votes and taking the sign to determine the final prediction.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    m = X_train.shape[0]\n    n = X_train.shape[1]\n    n_clf = max(n_clf, 1)\n    \n    # Initialize weights\n    w = np.ones(m) / m\n    \n    stumps = []  # Each element is (feature, threshold, polarity, alpha)\n    \n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        \n        for j in range(n):\n            feature_values = np.sort(np.unique(X_train[:, j]))\n            if len(feature_values) == 0:\n                continue\n            elif len(feature_values) == 1:\n                thresholds = [feature_values[0]]\n            else:\n                thresholds = []\n                for i in range(len(feature_values) - 1):\n                    thresholds.append((feature_values[i] + feature_values[i+1]) / 2)\n            \n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    if polarity == 1:\n                        predictions = np.where(X_train[:, j] <= threshold, 1, -1)\n                    else:\n                        predictions = np.where(X_train[:, j] > threshold, 1, -1)\n                    error = np.sum(w * (y_train != predictions))\n                    if error < best_error:\n                        best_error = error\n                        best_feature = j\n                        best_threshold = threshold\n                        best_polarity = polarity\n        \n        if best_error == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * np.log((1 - best_error) / best_error)\n        \n        if best_polarity == 1:\n            h = np.where(X_train[:, best_feature] <= best_threshold, 1, -1)\n        else:\n            h = np.where(X_train[:, best_feature] > best_threshold, 1, -1)\n        \n        exponent = -alpha * y_train * h\n        w = w * np.exp(exponent)\n        Z = np.sum(w)\n        if Z == 0:\n            Z = 1e-10\n        w = w / Z\n        \n        stumps.append((best_feature, best_threshold, best_polarity, alpha))\n        \n        if best_error == 0:\n            break\n    \n    predictions = []\n    for x in X_test:\n        total = 0.0\n        for (feature, threshold, polarity, alpha) in stumps:\n            if polarity == 1:\n                if x[feature] <= threshold:\n                    h = 1\n                else:\n                    h = -1\n            else:\n                if x[feature] > threshold:\n                    h = 1\n                else:\n                    h = -1\n            total += alpha * h\n        if total > 0:\n            predictions.append(1)\n        else:\n            predictions.append(-1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The weights for each training example are initialized to be equal, ensuring each example starts with equal influence.\n2. **Iterative Training**: For each iteration, the algorithm evaluates all possible decision stumps (each defined by a feature, threshold, and polarity) to find the one with the minimum weighted error. This involves checking all possible thresholds derived from the feature values and both possible polarities.\n3. **Update Weights**: After selecting the best stump, the weights of the training examples are updated to give more weight to misclassified examples, ensuring subsequent stumps focus more on these examples.\n4. **Store Stumps**: Each stump's parameters are stored along with its computed weight (alpha), which determines its influence in the final prediction.\n5. **Prediction**: For each test example, the algorithm aggregates the predictions from all stumps, weighted by their respective alphas, and takes the sign of the sum to determine the final prediction.\n\nThis approach ensures that the AdaBoost algorithm effectively combines weak classifiers to form a strong classifier, improving the overall accuracy by iteratively focusing on difficult examples."}
{"task_id": 261, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Glorot (Xavier) normal weight-initialization function for neural networks. This function initializes weights in a way that helps in maintaining consistent variance of activations across layers, which is crucial for effective training of deep networks.\n\n### Approach\nThe approach to solve this problem involves three main steps:\n\n1. **Compute Fan In and Fan Out**: \n   - For a 2D shape `(fan_in, fan_out)`, the values are directly taken from the dimensions.\n   - For shapes with more than two dimensions (common in convolutional layers), the fan in and fan out are computed by considering the product of all dimensions beyond the first two (receptive field size).\n\n2. **Calculate Standard Deviation**: \n   - The standard deviation `s` is computed using the formula `s = sqrt(2 / (fan_in + fan_out))`.\n\n3. **Generate Random Array**: \n   - Using the computed standard deviation, generate a NumPy array with elements drawn from a normal distribution with mean 0 and the computed standard deviation.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n"}
{"task_id": 266, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency list representation. The adjacency list should be a list of lists where each inner list contains the neighbors of the corresponding vertex in `V`, sorted by their order of appearance in `V`.\n\n### Approach\n1. **Create a Vertex Index Mapping**: First, we create a dictionary to map each vertex to its index in the list `V`. This helps in quickly determining the order of vertices.\n2. **Initialize Adjacency Dictionary**: We use a dictionary where each key is a vertex and the value is a set of its neighbors. This helps in efficiently managing and avoiding duplicate edges.\n3. **Process Edges**: For each edge in `E`, we add each vertex to the other's neighbor set. This ensures that both directions of the undirected edge are accounted for.\n4. **Build the Adjacency List**: For each vertex in `V`, we collect its neighbors, sort them based on their indices in `V`, and construct the adjacency list.\n\n### Solution Code\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\"\"\"\n    vertex_index = {v: i for i, v in enumerate(V)}\n    adj_dict = {v: set() for v in V}\n    \n    for u, v in E:\n        adj_dict[u].add(v)\n        adj_dict[v].add(u)\n    \n    G = []\n    for v in V:\n        neighbors = list(adj_dict[v])\n        sorted_neighbors = sorted(neighbors, key=lambda x: vertex_index[x])\n        G.append(sorted_neighbors)\n    \n    return G\n```\n\n### Explanation\n1. **Vertex Index Mapping**: The dictionary `vertex_index` maps each vertex to its position in `V`, which is used to sort neighbors later.\n2. **Adjacency Dictionary**: `adj_dict` is initialized with each vertex having an empty set of neighbors. This helps in efficiently adding and managing neighbors without duplicates.\n3. **Edge Processing**: For each edge `(u, v)`, both `u` is added to `v`'s neighbors and `v` is added to `u`'s neighbors, ensuring the undirected nature of the graph.\n4. **Building the Adjacency List**: For each vertex in `V`, its neighbors are collected, sorted by their indices using `vertex_index`, and added to the result list `G`.\n\nThis approach ensures that the adjacency list is constructed efficiently and correctly, with neighbors sorted as required."}
{"task_id": 267, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the optimal weighted decision stump for a binary classification task. A decision stump is a simple decision tree that makes predictions based on a single feature and a threshold. The goal is to minimize the weighted classification error, considering the weights of each sample.\n\n### Approach\n1. **Extract and Sort Features**: For each feature, extract the corresponding values and sort them along with their labels and weights.\n2. **Compute Prefix Sums**: For the sorted data, compute prefix sums for the weights where the labels are 1 and -1. This helps in efficiently calculating the classification error for different thresholds.\n3. **Evaluate Thresholds**: For each unique value of the feature (considered as a potential threshold), compute the classification error for both polarities (1 and -1). The polarity determines how the threshold splits the data into two groups for prediction.\n4. **Track Minimum Error**: Keep track of the feature, threshold, and polarity that result in the minimum weighted classification error.\n\n### Solution Code\n```python\nimport numpy as np\nimport bisect\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_error = float('inf')\n    best_feature = 0\n    best_threshold = 0.0\n    best_polarity = 1\n\n    for j in range(n_features):\n        x_j = X[:, j]\n        data = list(zip(x_j, y, sample_weights))\n        data.sort(key=lambda x: x[0])\n        x_sorted = [d[0] for d in data]\n        y_sorted = [d[1] for d in data]\n        w_sorted = [d[2] for d in data]\n\n        prefix_ones = [0.0]\n        prefix_minus_ones = [0.0]\n        for i in range(len(data)):\n            if y_sorted[i] == 1:\n                prefix_ones.append(prefix_ones[-1] + w_sorted[i])\n                prefix_minus_ones.append(prefix_minus_ones[-1])\n            else:\n                prefix_minus_ones.append(prefix_minus_ones[-1] + w_sorted[i])\n                prefix_ones.append(prefix_ones[-1])\n\n        total_ones = prefix_ones[-1]\n        total_minus_ones = prefix_minus_ones[-1]\n\n        unique_thresholds = sorted(list(set(x_sorted)))\n\n        for theta in unique_thresholds:\n            k = bisect.bisect_left(x_sorted, theta)\n            error_p1 = prefix_minus_ones[k] + (total_ones - prefix_ones[k])\n            error_p_neg1 = prefix_ones[k] + (total_minus_ones - prefix_minus_ones[k])\n            current_error = min(error_p1, error_p_neg1)\n            p = 1 if error_p1 < error_p_neg1 else -1\n\n            if current_error < best_error:\n                best_error = current_error\n                best_feature = j\n                best_threshold = theta\n                best_polarity = p\n\n    best_threshold = round(best_threshold, 4)\n    best_error = round(best_error, 4)\n    return {\n        \"feature_index\": best_feature,\n        \"threshold\": best_threshold,\n        \"polarity\": best_polarity,\n        \"weighted_error\": best_error\n    }\n```\n\n### Explanation\n1. **Extract and Sort Features**: For each feature, we extract the values and sort them along with their corresponding labels and weights. This helps in efficiently computing the prefix sums.\n2. **Compute Prefix Sums**: We compute prefix sums for the weights where the labels are 1 and -1. These sums allow us to quickly calculate the classification error for any given threshold.\n3. **Evaluate Thresholds**: For each unique value of the feature, we determine the split point and compute the classification error for both polarities. The error is calculated by summing the weights of misclassified samples in both groups (left and right of the threshold).\n4. **Track Minimum Error**: We keep track of the feature, threshold, and polarity that result in the minimum weighted classification error. The results are rounded to four decimal places for the final output.\n\nThis approach ensures that we efficiently find the optimal decision stump by considering all possible features, thresholds, and polarities, while minimizing the weighted classification error."}
{"task_id": 273, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine the optimal numerical threshold for splitting a dataset in a decision tree. The goal is to find the threshold that maximizes the information gain, which is a measure of how well a given attribute separates the training examples according to their target classification.\n\n### Approach\n1. **Check for Uniform Feature Values**: If all feature values are the same, no split is possible, so we return `(None, 0.0)`.\n2. **Sort Data**: Pair each feature value with its corresponding target label and sort these pairs by the feature values.\n3. **Generate Candidate Thresholds**: For each pair of consecutive distinct feature values, compute the midpoint as a candidate threshold.\n4. **Compute Entropy**: Calculate the Shannon entropy of the target labels for the entire dataset (parent entropy).\n5. **Evaluate Each Threshold**: For each candidate threshold, split the dataset into left and right subsets, compute their entropies, and determine the information gain.\n6. **Select Optimal Threshold**: Track the threshold that provides the maximum information gain. If multiple thresholds yield the same gain, choose the smallest one. If no positive gain is found, return `(None, 0.0)`.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport bisect\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\"\"\"\n    # Check if all feature values are the same\n    if len(set(feature)) == 1:\n        return (None, 0.0)\n    \n    # Pair feature and target, then sort by feature\n    data = sorted(zip(feature, target), key=lambda x: x[0])\n    sorted_feature = [x[0] for x in data]\n    sorted_target = [x[1] for x in data]\n    n = len(sorted_feature)\n    \n    # Generate candidate thresholds\n    thresholds = []\n    for i in range(len(sorted_feature) - 1):\n        if sorted_feature[i] != sorted_feature[i+1]:\n            t = (sorted_feature[i] + sorted_feature[i+1]) / 2\n            thresholds.append(t)\n    \n    if not thresholds:\n        return (None, 0.0)\n    \n    # Helper function to compute entropy\n    def entropy(labels):\n        if not labels:\n            return 0.0\n        counts = Counter(labels)\n        total = len(labels)\n        h = 0.0\n        for count in counts.values():\n            p = count / total\n            if p == 0:\n                continue\n            h -= p * math.log2(p)\n        return h\n    \n    H_parent = entropy(sorted_target)\n    max_ig = -1\n    best_t = None\n    \n    for t in thresholds:\n        # Find the split index\n        index = bisect.bisect_left(sorted_feature, t)\n        left = sorted_target[:index]\n        right = sorted_target[index:]\n        \n        # Compute entropies\n        H_left = entropy(left)\n        H_right = entropy(right)\n        \n        # Calculate information gain\n        ig = H_parent - (len(left)/n)*H_left - (len(right)/n)*H_right\n        \n        # Update the best threshold and max IG\n        if ig > max_ig:\n            max_ig = ig\n            best_t = t\n        elif ig == max_ig:\n            if t < best_t:\n                best_t = t\n    \n    # Determine the result based on max_ig\n    if max_ig > 0:\n        best_t_rounded = round(best_t, 4)\n        max_ig_rounded = round(max_ig, 4)\n        return (best_t_rounded, max_ig_rounded)\n    else:\n        return (None, 0.0)\n```\n\n### Explanation\n1. **Uniform Feature Check**: The function first checks if all feature values are identical, which would make splitting impossible.\n2. **Sorting**: The feature and target values are paired and sorted to facilitate threshold generation and splitting.\n3. **Threshold Generation**: Midpoints between consecutive distinct feature values are computed as potential thresholds.\n4. **Entropy Calculation**: The entropy of the target labels is computed for the entire dataset and for each subset created by a threshold.\n5. **Information Gain**: For each threshold, the information gain is calculated. The threshold with the highest gain is selected, with ties broken by choosing the smallest threshold.\n6. **Result**: The optimal threshold and corresponding information gain are returned, rounded to four decimal places. If no positive gain is found, `(None, 0.0)` is returned."}
{"task_id": 286, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function `get_initializer` that retrieves a weight-initialization function based on a given name. The function should return the corresponding callable initializer if it exists, or raise an error if it does not.\n\n### Approach\n1. **Lookup in"}
{"task_id": 287, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement both the forward and backward passes of a 2-D average-pooling layer in a convolutional neural network. The forward pass downsamples the input feature map by sliding a fixed-size window over it and replacing each window with the arithmetic mean of its elements. The backward pass propagates the gradient from the output back to the input, distributing the gradient equally to each element in the window.\n\n### Approach\n1. **Forward Pass**:\n   - Compute the output dimensions based on the input dimensions, pool size, and stride.\n   - Initialize an output array with zeros.\n   - For each element in the output, extract the corresponding window from the input, compute the mean, and store it in the output.\n\n2. **Backward Pass**:\n   - Initialize a gradient input array with zeros.\n   - For each element in the output, distribute the gradient from the output back to the corresponding window in the input, dividing the gradient by the number of elements in the window to ensure equal distribution.\n\n3. **Rounding and Conversion**:\n   - Round both the output and gradient input arrays to the nearest 4th decimal place.\n   - Convert the rounded arrays to Python lists for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n\n    output_height = (H - p_h) // s_h + 1\n    output_width = (W - p_w) // s_w + 1\n\n    output = np.zeros((N, C, output_height, output_width))\n\n    # Forward pass\n    for n in range(N):\n        for c in range(C):\n            for i in range(output_height):\n                for j in range(output_width):\n                    x_start = i * s_h\n                    y_start = j * s_w\n                    x_end = x_start + p_h\n                    y_end = y_start + p_w\n                    window = X[n, c, x_start:x_end, y_start:y_end]\n                    output[n, c, i, j] = np.mean(window)\n\n    # Backward pass\n    grad_input = np.zeros_like(X)\n    for n in range(N):\n        for c in range(C):\n            for i in range(output_height):\n                for j in range(output_width):\n                    x_start = i * s_h\n                    y_start = j * s_w\n                    x_end = x_start + p_h\n                    y_end = y_start + p_w\n                    grad = accum_grad[n, c, i, j] / (p_h * p_w)\n                    grad_input[n, c, x_start:x_end, y_start:y_end] += grad\n\n    # Rounding and conversion to lists\n    output_rounded = np.round(output, 4)\n    grad_input_rounded = np.round(grad_input, 4)\n\n    return (output_rounded.tolist(), grad_input_rounded.tolist())\n```\n\n### Explanation\n- **Forward Pass**: The function iterates over each element in the output, extracts the corresponding window from the input, computes the mean, and stores it in the output array.\n- **Backward Pass**: The function initializes a gradient input array and distributes the gradient from the output back to the input, ensuring each element in the window receives an equal share of the gradient.\n- **Rounding and Conversion**: The results are rounded to four decimal places and converted to Python lists for the final output.\n\nThis approach ensures that both the forward and backward passes are efficiently computed, adhering to the constraints and requirements of the problem."}
{"task_id": 290, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if two binary decision trees are equivalent. Two trees are considered equivalent if they have the same structure, use the same features and thresholds at each internal node, and produce the same predictions at each leaf node, within a specified tolerance.\n\n### Approach\n1. **Type Check**: First, we check if both trees are of the same type (both are either `Node` or `Leaf`). If they are not, the trees are not equivalent.\n2. **Leaf Node Comparison**: If both nodes are `Leaf` nodes, we compare their values using `numpy.allclose` to handle numerical comparisons with a tolerance of \\(1 \\times 10^{-8}\\).\n3. **Internal Node Comparison**: If both nodes are `Node` nodes, we check:\n   - The feature index used for splitting.\n   - The threshold value, ensuring it is the same within the specified tolerance.\n   - Recursively compare the left and right subtrees to ensure they are equivalent.\n\n### Solution Code\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both nodes are of the same type\n    if type(tree_a) != type(tree_b):\n        return False\n    \n    if isinstance(tree_a, Leaf):\n        # Compare leaf values with tolerance\n        return np.allclose(tree_a.value, tree_b.value, atol=1e-8)\n    else:\n        # Both are Node instances; check feature and threshold\n        if tree_a.feature != tree_b.feature:\n            return False\n        if not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        \n        # Recursively compare left and right subtrees\n        if not compare_trees(tree_a.left, tree_b.left):\n            return False\n        if not compare_trees(tree_a.right, tree_b.right):\n            return False\n        \n        return True\n```\n\n### Explanation\n- **Type Check**: The function first ensures both nodes are of the same type. If one is a `Node` and the other is a `Leaf`, they are not equivalent.\n- **Leaf Node Comparison**: For `Leaf` nodes, the values are compared using `numpy.allclose` to handle both scalar and array comparisons with a tolerance.\n- **Internal Node Comparison**: For `Node` nodes, the function checks if the feature index and threshold are the same within the tolerance. It then recursively checks the left and right subtrees to ensure they are structurally and functionally equivalent.\n\nThis approach ensures that the trees are compared thoroughly, checking both structure and values at each node, ensuring equivalence within the specified tolerance."}
{"task_id": 292, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a single-point column crossover on two parent weight matrices to generate two new offspring matrices. This operation is commonly used in evolutionary algorithms to combine genetic information from two parents.\n\n### Approach\n1. **Convert to NumPy Arrays**: Convert the input parent matrices into NumPy arrays for efficient slicing and manipulation. This also ensures that all elements are treated as floats.\n2. **Shape Validation**: Check if the two parent matrices have the same shape. If they do not, return -1.\n3. **Crossover Operation**: Perform the crossover by slicing the columns of the parent matrices at the given cutoff point. The columns before the cutoff are taken from the respective parent, and the columns from the cutoff onwards are taken from the other parent.\n4. **Concatenate and Convert**: Concatenate the sliced parts to form the new offspring matrices and convert them back to Python lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert parents to numpy arrays with float dtype\n    np_parent1 = np.array(parent1, dtype=float)\n    np_parent2 = np.array(parent2, dtype=float)\n    \n    # Check if the shapes are the same\n    if np_parent1.shape != np_parent2.shape:\n        return -1\n    \n    # Perform the crossover\n    # Child 1: left from parent1, right from parent2\n    left_child1 = np_parent1[:, :cutoff]\n    right_child1 = np_parent2[:, cutoff:]\n    child1 = np.concatenate((left_child1, right_child1), axis=1)\n    \n    # Child 2: left from parent2, right from parent1\n    left_child2 = np_parent2[:, :cutoff]\n    right_child2 = np_parent1[:, cutoff:]\n    child2 = np.concatenate((left_child2, right_child2), axis=1)\n    \n    # Convert to lists\n    child1_list = child1.tolist()\n    child2_list = child2.tolist()\n    \n    return (child1_list, child2_list)\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input matrices are converted to NumPy arrays to leverage efficient array operations. This also ensures that all elements are treated as floats, which is necessary for the output format.\n2. **Shape Validation**: The function checks if the two parent matrices have the same dimensions. If they do not, the function returns -1 immediately.\n3. **Crossover Operation**: The crossover is performed by slicing each parent matrix at the specified cutoff column. The left part (before the cutoff) of each child is taken from the respective parent, while the right part (from the cutoff onwards) is taken from the other parent.\n4. **Concatenation and Conversion**: The sliced parts are concatenated to form the new offspring matrices. These matrices are then converted back to Python lists to match the required output format.\n\nThis approach ensures that the crossover operation is performed efficiently and correctly, producing the desired offspring matrices."}
{"task_id": 296, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if all elements in a given NumPy array are binary, meaning each element is either 0 or 1. This includes handling arrays of integer, float, and boolean types, as well as arrays of any shape.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Convert Input to NumPy Array**: Ensure the input is converted to a NumPy array if it is not already one. This allows us to handle any array-like input uniformly.\n2. **Create a Mask**: Generate a boolean mask where each element is `True` if it is either 0 or 1, and `False` otherwise. This is done using vectorized operations for efficiency.\n3. **Check All Elements**: Use the `np.all` function to check if all elements in the mask are `True`. If they are, the array is binary; otherwise, it is not.\n\nThis approach efficiently handles arrays of any shape and type, including boolean arrays where `True` and `False` correspond to 1 and 0, respectively.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    x = np.asarray(x)\n    mask = (x == 0) | (x == 1)\n    return np.all(mask)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The function starts by converting the input `x` into a NumPy array using `np.asarray(x)`. This ensures that the input is handled uniformly regardless of its initial form.\n2. **Mask Creation**: The mask is created using the condition `(x == 0) | (x == 1)`. This condition checks each element to see if it is either 0 or 1. For boolean arrays, `True` is treated as 1 and `False` as 0, so this condition correctly identifies valid binary values.\n3. **Check All Elements**: The `np.all(mask)` function is used to verify that every element in the mask is `True`. If all elements are `True`, the function returns `True`, indicating the array is binary. Otherwise, it returns `False`.\n\nThis solution efficiently handles all specified cases, including edge cases such as empty arrays and arrays of different data types."}
{"task_id": 298, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the total log-probability of a query sentence using an N-gram Maximum Likelihood Estimation (MLE) model trained on a given corpus. The function should handle both unigrams and higher-order N-grams, including padding for sentences to account for the start and end of sequences.\n\n### Approach\n1. **Tokenize the Corpus**: Split each sentence in the corpus into individual tokens.\n2. **Pad Sentences**: For N > 1, pad each sentence with `<bol>` tokens at the beginning and a `<eol>` token at the end to handle the start and end of sequences.\n3. **Count N-grams and Prefixes**: For each sentence, count the occurrences of each N-gram and their (N-1)-gram prefixes.\n4. **Process the Query**: Split the query into tokens and pad it similarly if N > 1. Check if the padded query is long enough to form N-grams.\n5. **Calculate Log-Probability**: For each N-gram in the padded query, compute the log-probability using the counts from the corpus. If any N-gram is unseen, return negative infinity.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nfrom typing import List\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\"\"\"\n    count_ngram = Counter()\n    count_prefix = Counter()\n\n    for sentence in corpus:\n        tokens = sentence.split()\n        if N > 1:\n            tokens = ['<bol>'] * (N-1) + tokens + ['<eol>']\n        for i in range(len(tokens) - N + 1):\n            ngram = tuple(tokens[i:i+N])\n            prefix = tuple(tokens[i:i+N-1])\n            count_ngram[ngram] += 1\n            count_prefix[prefix] += 1\n\n    query_tokens = query.split()\n    if N > 1:\n        padded_query = ['<bol>'] * (N-1) + query_tokens + ['<eol>']\n    else:\n        padded_query = query_tokens\n\n    if len(padded_query) < N:\n        return float('-inf')\n\n    total_log = 0.0\n\n    for i in range(len(padded_query) - N + 1):\n        ngram = tuple(padded_query[i:i+N])\n        if ngram not in count_ngram:\n            return float('-inf')\n        prefix = tuple(padded_query[i:i+N-1])\n        if N > 1:\n            if prefix not in count_prefix:\n                return float('-inf')\n        denominator = count_prefix[prefix] if N > 1 else count_prefix[()]\n        if denominator == 0:\n            return float('-inf')\n        prob = count_ngram[ngram] / denominator\n        total_log += math.log(prob)\n\n    return round(total_log, 4)\n```\n\n### Explanation\n1. **Tokenization and Padding**: Each sentence in the corpus is split into tokens. For N > 1, each sentence is padded with `<bol>` tokens at the beginning and a `<eol>` token at the end to handle the start and end of sequences.\n2. **Counting N-grams**: For each sentence, we count the occurrences of each N-gram and their (N-1)-gram prefixes. This helps in calculating the conditional probabilities later.\n3. **Query Processing**: The query is split into tokens and padded similarly if N > 1. We check if the padded query is long enough to form N-grams.\n4. **Log-Probability Calculation**: For each N-gram in the padded query, we compute the log-probability using the counts from the corpus. If any N-gram is unseen, we return negative infinity. The total log-probability is the sum of the log-probabilities of all N-grams in the query, rounded to four decimal places.\n\nThis approach ensures that we efficiently build the N-gram model and accurately compute the log-probability for the query, handling edge cases such as unseen N-grams and insufficient query length."}
{"task_id": 302, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Spectral Clustering algorithm from scratch. The algorithm involves several steps, including constructing a weighted adjacency matrix, computing the graph Laplacian, finding the eigenvectors, and performing k-means clustering on the eigenvectors. \n\n### Approach\n1. **Construct the Weighted Adjacency Matrix (W):** \n   - Compute the pairwise Euclidean distances between all data points.\n   - Use these distances to construct the adjacency matrix where each element is defined as \\( w_{ij} = \\frac{1}{1 + \\|x_i - x_j\\|_2} \\). The diagonal elements are set to zero since a point is not connected to itself.\n\n2. **Compute the Graph Laplacian (L):**\n   - The graph Laplacian is given by \\( L = D - W \\), where \\( D \\) is the degree matrix. The degree matrix \\( D \\) is a diagonal matrix where each diagonal element \\( D_{ii} \\) is the sum of the weights in the corresponding row of \\( W \\).\n\n3. **Find the Eigenvectors:**\n   - Compute the eigenvalues and eigenvectors of the Laplacian matrix \\( L \\). The first \\( k \\) eigenvectors corresponding to the smallest eigenvalues are used to form the projection matrix \\( E \\).\n\n4. **Perform k-means Clustering:**\n   - Use the rows of the projection matrix \\( E \\) as data points for k-means clustering. Initialize the centroids with the first \\( k \\) rows of \\( E \\). If any cluster becomes empty during the iterations, reinitialize its centroid with a random row from \\( E \\).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n, d = X.shape\n\n    # Step 1: Build the weighted adjacency matrix W\n    # Compute pairwise distances\n    D = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n    W = 1.0 / (1.0 + D)\n    np.fill_diagonal(W, 0)  # Ensure diagonal is zero\n\n    # Step 2: Compute the unnormalized graph Laplacian L = D - W\n    D_deg = np.sum(W, axis=1)\n    D_mat = np.diag(D_deg)\n    L = D_mat - W\n\n    # Step 3: Compute the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n\n    # Step 4: Run k-means on the rows of E\n    centroids = E[:k, :].copy()\n    prev_labels = None\n\n    for _ in range(100):\n        # Compute distance matrix\n        E_norms = np.sum(E ** 2, axis=1)\n        c_norms = np.sum(centroids ** 2, axis=1)\n        dot_product = E.dot(centroids.T)\n        dist_sq = E_norms[:, None] + c_norms[None, :] - 2 * dot_product\n        dist = np.sqrt(dist_sq)\n        labels = np.argmin(dist, axis=1)\n\n        if np.array_equal(labels, prev_labels):\n            break\n        prev_labels = labels.copy()\n\n        # Update centroids\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            mask = (labels == i)\n            if np.any(mask):\n                new_centroids[i] = np.mean(E[mask], axis=0)\n            else:\n                # Re-initialize with a random row from E\n                random_idx = np.random.choice(n)\n                new_centroids[i] = E[random_idx]\n        centroids = new_centroids\n\n    return labels.tolist()\n```\n\n### Explanation\n1. **Weighted Adjacency Matrix (W):** This matrix is constructed using the pairwise distances between data points, ensuring that each point is connected to others with weights based on their proximity.\n\n2. **Graph Laplacian (L):** The Laplacian matrix is derived from the adjacency matrix and the degree matrix. It helps in capturing the structure of the graph formed by the data points.\n\n3. **Eigenvectors:** The first \\( k \\) eigenvectors of the Laplacian matrix are used to project the data into a lower-dimensional space, which simplifies the clustering process.\n\n4. **k-means Clustering:** This step clusters the projected data points into \\( k \\) clusters. The initial centroids are chosen deterministically, and the algorithm iterates until convergence or a maximum number of iterations is reached, ensuring robust and consistent results."}
{"task_id": 303, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates a formatted debugging string comparing the current output of a program with the expected (gold-standard) output. The function should handle both the current and previous test cases, and optionally include a warning message.\n\n### Approach\n1. **Extract Previous and Current Elements**: Determine the previous and current elements from the given list of tuples. The previous element is the one at the index `max(ix - 1, 0)`, ensuring it doesn't go out of bounds.\n2. **Retrieve Gold Outputs**: Fetch the expected outputs (gold-standard) for both the previous and current labels from the provided dictionary.\n3. **Construct the Debugging String**: Format the debugging string according to the specified structure, including the previous and current outputs, their respective gold-standard outputs, and an optional warning message.\n\n### Solution Code\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n    gold_prev = golds[prev_label]\n    gold_curr = golds[curr_label]\n    \n    lines = [\n        '-' * 25 + 'DEBUG' + '-' * 25,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_mine,\n        f\"Theirs (prev) [{prev_label}]:\",\n        gold_prev,\n        f\"Mine [{curr_label}]:\",\n        curr_mine,\n        f\"Theirs [{curr_label}]:\",\n        gold_curr + warn_str,\n    ]\n    return '\\n'.join(lines)\n```\n\n### Explanation\n1. **Extract Previous and Current Elements**: The function calculates the index of the previous element using `max(ix - 1, 0)` to ensure it doesn't go below zero. It then retrieves the mine output and label for both the previous and current elements from the `params` list.\n2. **Retrieve Gold Outputs**: Using the labels from the previous and current elements, the function fetches the corresponding gold-standard outputs from the `golds` dictionary.\n3. **Construct the Debugging String**: The function constructs each line of the debugging string as specified. The first line is formatted with 25 dashes on each side of \"DEBUG\". Each subsequent line is formatted to include the mine and gold outputs for both the previous and current elements, with the gold output for the current element followed by the optional warning message.\n\nThis approach ensures that the debugging string is formatted exactly as required, providing a clear and concise comparison between the program's output and the expected results."}
{"task_id": 304, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the posterior mean (MAP estimate) of the coefficients in a Bayesian linear regression model with an unknown noise variance. The model uses a normal-inverse-gamma conjugate prior on the coefficients and the noise variance. The goal is to derive the closed-form solution for the posterior mean of the regression coefficients.\n\n### Approach\n1. **Preprocess the Design Matrix**: If an intercept is required, prepend a column of ones to the design matrix `X`.\n2. **Handle Prior Mean (\u03bc)**: If \u03bc is a scalar, broadcast it to a vector of appropriate length. If it's a vector, ensure it matches the dimensionality of the design matrix.\n3. **Handle Prior Covariance (V)**: Depending on whether V is a scalar, list, tuple, or a 2D array, compute its inverse. If V is None, treat it as an identity matrix.\n4. **Compute Key Matrices**: Calculate the matrix product of the transpose of `X` with itself (`XTX`) and the product of the transpose of `X` with the target vector `y` (`XTy`).\n5. **Form and Invert the Matrix**: Form the matrix `A` by adding the inverse of the prior covariance matrix (V\u207b\u00b9) to `XTX`. Invert this matrix to get the posterior covariance matrix.\n6. **Compute the Posterior Mean**: Use the posterior covariance matrix and the sum of the prior mean adjusted by the data to compute the posterior mean of the coefficients.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu=0.0,\n    V=None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # Add intercept column if needed\n    if fit_intercept:\n        intercept = np.ones((X.shape[0], 1))\n        X = np.hstack([intercept, X])\n    \n    M = X.shape[1]\n    \n    # Process mu\n    if isinstance(mu, (int, float)):\n        mu = np.full(M, mu)\n    else:\n        mu = np.array(mu)\n        if mu.size != M:\n            raise ValueError(\"mu must be a scalar or a vector of length M\")\n    \n    # Process V to get V_inv\n    if V is None:\n        V_inv = np.eye(M)\n    elif isinstance(V, (int, float)):\n        V_inv = np.eye(M) / V\n    elif isinstance(V, (list, tuple)):\n        V = np.array(V)\n        if V.ndim != 1 or V.size != M:\n            raise ValueError(\"V must be a scalar, 1D array of length M, or 2D array of size MxM\")\n        V_inv = np.diag(1.0 / V)\n    elif isinstance(V, np.ndarray):\n        if V.ndim != 2 or V.shape[0] != M or V.shape[1] != M:\n            raise ValueError(\"V must be a 2D array of size MxM\")\n        V_inv = np.linalg.inv(V)\n    else:\n        raise ValueError(\"V must be None, scalar, list/tuple, or 2D array\")\n    \n    # Compute X^T X and X^T y\n    XTX = X.T.dot(X)\n    XTy = X.T.dot(y)\n    \n    # Compute A = V_inv + X^T X\n    A = V_inv + XTX\n    \n    # Compute the inverse of A (posterior covariance)\n    try:\n        Sigma_b = np.linalg.inv(A)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"Matrix A is singular; cannot compute inverse\")\n    \n    # Compute B = V_inv @ mu + X^T y\n    B = V_inv.dot(mu) + XTy\n    \n    # Compute the posterior mean\n    mu_b = Sigma_b.dot(B)\n    \n    # Round to 4 decimal places and convert to list of floats\n    return [round(float(x), 4) for x in mu_b]\n```\n\n### Explanation\n1. **Preprocessing**: The design matrix `X` is modified to include an intercept column if required. This ensures that the model can account for a bias term.\n2. **Prior Handling**: The prior mean `mu` is broadcast to the correct dimension if it's a scalar. The prior covariance `V` is converted to its inverse, handling different forms such as scalars, lists, tuples, and 2D arrays.\n3. **Matrix Computations**: Key matrices `XTX` and `XTy` are computed to form the matrix `A`, which is then inverted to get the posterior covariance matrix.\n4. **Posterior Mean Calculation**: Using the posterior covariance matrix and the adjusted prior mean, the posterior mean of the coefficients is computed and returned after rounding to four decimal places.\n\nThis approach efficiently computes the MAP estimate for the regression coefficients in a Bayesian linear regression model, leveraging the properties of conjugate priors and closed-form solutions."}
{"task_id": 308, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Mel filterbank matrix that transforms a power spectrum into Mel band energies. The Mel filterbank is a set of triangular filters laid out on the Mel scale, which provides high resolution for low frequencies and low resolution for high frequencies.\n\n### Approach\n1. **Convert Frequency Limits to Mel Scale**: Convert the given minimum and maximum frequencies from Hertz to the Mel scale using the formula `mel = 2595 * log10(1 + f/700)`.\n2. **Generate Mel Points**: Create `n_filters + 2` equally spaced points on the Mel scale between the converted minimum and maximum Mel values.\n3. **Convert Back to Hertz**: Convert these Mel points back to Hertz to get the corner frequencies of the triangular filters using the formula `f = 700 * (10^(mel/2595) - 1)`.\n4. **Compute DFT Bin Frequencies**: Calculate the center frequencies for each DFT bin.\n5. **Construct Filterbank Matrix**: For each filter, determine the weights for each DFT bin by evaluating the triangular filter's response. If normalization is enabled, scale each filter to ensure its area equals 1.\n6. **Return the Matrix**: Convert the matrix to a list of lists, rounding each element to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\"\"\"\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # Convert min and max frequencies to Mel scale\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n\n    # Generate n_filters + 2 points on the Mel scale\n    n_points = n_filters + 2\n    mel_points = np.linspace(min_mel, max_mel, n_points)\n\n    # Convert Mel points back to Hertz\n    f = 700 * (10 ** (mel_points / 2595) - 1)\n\n    # Compute all DFT bin frequencies\n    freqs = np.arange(0, N // 2 + 1) * fs / N\n\n    F = []\n    for i in range(n_filters):\n        f_i = f[i]\n        f_i1 = f[i + 1]\n        f_i2 = f[i + 2]\n\n        # Determine the regions for the triangular filter\n        mask_left = (freqs >= f_i) & (freqs <= f_i1)\n        mask_right = (freqs > f_i1) & (freqs <= f_i2)\n        mask_out = ~ (mask_left | mask_right)\n\n        # Calculate the weights for each region\n        left_weights = (freqs - f_i) / (f_i1 - f_i)\n        right_weights = (f_i2 - freqs) / (f_i2 - f_i1)\n\n        weights = np.zeros_like(freqs)\n        weights[mask_left] = left_weights[mask_left]\n        weights[mask_right] = right_weights[mask_right]\n\n        # Apply normalization if required\n        if normalize:\n            weights *= 2.0 / (f_i2 - f_i)\n\n        # Round to 4 decimal places and convert to list\n        row = np.round(weights, 4).tolist()\n        F.append(row)\n\n    return F\n```\n\n### Explanation\n1. **Mel Scale Conversion**: The function starts by converting the given minimum and maximum frequencies to the Mel scale. This is done using the formula provided.\n2. **Mel Points Generation**: It then generates `n_filters + 2` points on the Mel scale, ensuring the filters are equally spaced in the Mel domain.\n3. **Hertz Conversion**: These Mel points are converted back to Hertz to determine the corner frequencies of the triangular filters.\n4. **DFT Bin Frequencies**: The center frequencies for each DFT bin are computed.\n5. **Filter Construction**: For each filter, the function calculates the weights for each DFT bin by evaluating the triangular filter's response. The weights are determined based on the position of the bin's frequency relative to the filter's corners.\n6. **Normalization**: If normalization is enabled, each filter is scaled to ensure its area equals 1.\n7. **Result Conversion**: The resulting matrix is converted to a list of lists, with each element rounded to 4 decimal places for precision.\n\nThis approach ensures that the Mel filterbank matrix is constructed accurately and efficiently, providing the correct transformation from the power spectrum to Mel band energies."}
{"task_id": 312, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Blackman-Harris window of a specified length, which can be either symmetric or periodic. The Blackman-Harris window is a type of cosine-sum window used to reduce spectral leakage when analyzing signals. The window can be used in two forms: symmetric for FIR filter design and periodic for FFT-based spectral analysis.\n\n### Approach\n1. **Special Cases Handling**: \n   - If the window length is less than or equal to zero, return an empty list.\n   - If the window length is exactly 1, return a list containing a single element, 1.0, regardless of the window type.\n\n2. **Window Generation**:\n   - For the symmetric window, compute the window for `n` values from 0 to `window_len - 1` using the formula provided.\n   - For the periodic window, compute the window for `n` values from 0 to `window_len` (inclusive) and then discard the last value to ensure the first and last values are the same, making it suitable for seamless tiling in FFT analysis.\n\n3. **Blackman-Harris Formula**:\n   - The formula involves computing the window values using cosine terms with specific coefficients. The formula is applied for each `n` value in the range determined by the window type.\n\n4. **Rounding**:\n   - Each computed window value is rounded to four decimal places before being returned as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    \n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    if symmetric:\n        N = window_len - 1\n        n = np.arange(window_len)\n    else:\n        N = window_len\n        n = np.arange(window_len + 1)\n    \n    bh = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    \n    if not symmetric:\n        bh = bh[:window_len]\n    \n    bh_rounded = [round(x, 4) for x in bh]\n    return bh_rounded\n```\n\n### Explanation\n- **Special Cases Handling**: The function immediately handles edge cases where the window length is non-positive or exactly 1, ensuring correct output without further computation.\n- **Window Type Handling**: Depending on whether the window is symmetric or periodic, the function computes the necessary range of `n` values and applies the Blackman-Harris formula.\n- **Blackman-Harris Formula Application**: The formula is applied using numpy for efficient computation of the cosine terms and their weighted sum.\n- **Rounding**: Each computed window value is rounded to four decimal places to meet the output constraints, ensuring precision and consistency in the results.\n\nThis approach ensures that the generated window is suitable for its intended use, whether for filter design or spectral analysis, by correctly handling both symmetric and periodic window types."}
{"task_id": 313, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the expected logarithm of a component from a Dirichlet distribution. The Dirichlet distribution is commonly used in Bayesian models, and the expectation of the logarithm of its components is a key quantity in such models. The challenge is to compute this expectation without using external scientific libraries, which requires implementing the digamma function ourselves.\n\n### Approach\n1. **Understanding the Dirichlet Distribution**: The Dirichlet distribution is parameterized by a vector of positive values. For a Dirichlet random variable \\( X \\sim \\text{Dir}(\\gamma) \\), the expectation of the logarithm of its \\( t \\)-th component is given by \\( \\mathbb{E}[\\log X_t] = \\psi(\\gamma[t]) - \\psi(\\sum \\gamma) \\), where \\( \\psi \\) is the digamma function.\n\n2. **Implementing the Digamma Function**: The digamma function, which is the first derivative of the log-gamma function, can be challenging to compute directly. We use a combination of recursion and an asymptotic expansion to approximate it:\n   - For small values of \\( x \\) (less than 6), we use the recursion \\( \\psi(x) = \\psi(x+1) - 1/x \\) to shift \\( x \\) to a larger value.\n   - For larger values of \\( x \\) (6 or greater), we use the asymptotic expansion \\( \\psi(x) \\approx \\ln(x) - 1/(2x) - 1/(12x^2) + 1/(120x^4) - 1/(252x^6) \\).\n\n3. **Computing the Expectation**: Using the implemented digamma function, we compute the expectation by evaluating the digamma function at the specified component and the sum of the parameters, then taking their difference.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    def asymptotic_digamma(x):\n        ln_x = math.log(x)\n        term1 = 1.0 / (2 * x)\n        term2 = 1.0 / (12 * x ** 2)\n        term3 = 1.0 / (120 * x ** 4)\n        term4 = 1.0 / (252 * x ** 6)\n        return ln_x - term1 - term2 + term3 - term4\n\n    def digamma(x):\n        if x < 6:\n            sum_reciprocal = 0.0\n            while x < 6:\n                sum_reciprocal += 1.0 / x\n                x += 1\n            return asymptotic_digamma(x) - sum_reciprocal\n        else:\n            return asymptotic_digamma(x)\n\n    gamma_row = gamma[d]\n    sum_gamma = sum(gamma_row)\n    gamma_dt = gamma_row[t]\n\n    psi_dt = digamma(gamma_dt)\n    psi_sum = digamma(sum_gamma)\n\n    result = psi_dt - psi_sum\n    return round(result, 4)\n```\n\n### Explanation\n1. **Asymptotic Expansion**: The `asymptotic_digamma` function computes the digamma function for values of \\( x \\) that are 6 or greater using an asymptotic expansion. This provides a good approximation for large values of \\( x \\).\n\n2. **Recursive Adjustment**: The `digamma` function handles values of \\( x \\) less than 6 by recursively adjusting \\( x \\) up to 6 and then using the asymptotic expansion. This ensures that we can compute the digamma function for all positive values of \\( x \\).\n\n3. **Main Function**: The main function `expected_log_dirichlet` extracts the necessary parameters from the input, computes the digamma values for the specified component and the sum of the parameters, and returns the result rounded to four decimal places.\n\nThis approach efficiently computes the required expectation using a combination of recursion and asymptotic expansion, ensuring accuracy without relying on external libraries."}
{"task_id": 317, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a formatted debug error report that compares predictions made by a program with the expected (gold) texts. The report should include both the current and previous predictions and their corresponding gold texts, along with an optional warning message.\n\n### Approach\n1. **Extract Previous and Current Items**: Identify the previous and current items from the given list of predictions using the provided index. The previous item is determined by the index `max(ix - 1, 0)`.\n2. **Retrieve Gold Texts**: Fetch the gold texts corresponding to the labels of the previous and current items from the provided dictionary.\n3. **Construct the Report**: Build the report string by concatenating the header, previous item details, current item details, and the footer. The report includes:\n   - A header line with dashes and the word \"DEBUG\".\n   - Details of the previous prediction and its gold text.\n   - Two blank lines separating the previous and current sections.\n   - Details of the current prediction and its gold text, with an optional warning appended to the gold text.\n   - A footer line with dashes and the word \"END DEBUG\".\n\n### Solution Code\n```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\"\"\"\n    prev_ix = max(ix - 1, 0)\n    prev_prediction, prev_label = params[prev_ix]\n    curr_prediction, curr_label = params[ix]\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n    \n    header = '-' * 25 + ' DEBUG ' + '-' * 25 + '\\n'\n    mine_prev = f\"Mine (prev) [{prev_label}]:\\n{prev_prediction}\\n\"\n    theirs_prev = f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\"\n    current_mine = f\"Mine [{curr_label}]:\\n{curr_prediction}\\n\"\n    current_theirs = f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\"\n    footer = '-' * 23 + ' END DEBUG ' + '-' * 23 + '\\n'\n    \n    result = (\n        header +\n        mine_prev +\n        theirs_prev +\n        '\\n\\n' +\n        current_mine +\n        current_theirs +\n        '\\n' +\n        footer\n    )\n    return result\n```\n\n### Explanation\n- **Header Construction**: The header is created with 25 dashes, the word \"DEBUG\", and another 25 dashes, followed by a newline.\n- **Previous Item Details**: The previous prediction and its corresponding gold text are formatted into two lines each, followed by a newline.\n- **Current Item Details**: The current prediction and its corresponding gold text (with an optional warning) are formatted similarly.\n- **Footer Construction**: The footer is created with 23 dashes, the word \"END DEBUG\", and another 23 dashes, followed by a newline.\n- **String Concatenation**: The header, previous details, two blank lines, current details, and footer are concatenated to form the final report string.\n\nThis approach ensures that the report is formatted exactly as specified, making it suitable for logging or debugging purposes."}
{"task_id": 318, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak"}
{"task_id": 329, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the center frequencies of each DFT bin for a given number of coefficients and sampling frequency. The function should handle two modes: returning only non-negative frequencies or all frequencies arranged as per NumPy's `fftfreq` function.\n\n### Approach\n1. **Edge Case Handling**: If either the number of coefficients (N) or the sampling frequency (fs) is non-positive, return an empty list.\n2. **Positive Only Mode**: When `positive_only` is True, compute the non-negative frequencies. These frequencies are calculated for indices from 0 to floor(N/2), inclusive.\n3. **Full Frequency Mode**: When `positive_only` is False, use NumPy's `fftfreq` function to compute the full list of frequencies, which includes both positive and negative frequencies arranged correctly.\n4. **Rounding**: Each computed frequency is rounded to 4 decimal places before returning the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    if N <= 0 or fs <= 0:\n        return []\n    \n    if positive_only:\n        num_bins = (N // 2) + 1\n        frequencies = [k * (fs / N) for k in range(num_bins)]\n    else:\n        frequencies = np.fft.fftfreq(N, 1 / fs)\n    \n    rounded = [round(f, 4) for f in frequencies]\n    return rounded\n```\n\n### Explanation\n1. **Edge Case Handling**: The function first checks if either N or fs is non-positive. If so, it returns an empty list immediately.\n2. **Positive Only Mode**: For non-negative frequencies, the function calculates the frequencies for indices from 0 to floor(N/2). This is done using a list comprehension that iterates over the required range and computes each frequency as `k * (fs / N)`.\n3. **Full Frequency Mode**: When all frequencies are required, the function uses NumPy's `fftfreq` function, which efficiently computes the frequencies in the correct order, including both positive and negative values.\n4. **Rounding**: Each frequency value is rounded to 4 decimal places to meet the problem's precision requirements before being returned as a list.\n\nThis approach ensures that the function efficiently computes the required frequencies and handles both modes correctly, providing accurate results as specified."}
{"task_id": 331, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the out-of-bag mean squared error (OOB-MSE) for a Random Forest regressor. The OOB-MSE is an unbiased performance estimate obtained by averaging the predictions of trees that were not trained on a particular sample (out-of-bag trees).\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the input lists to numpy arrays for efficient computation.\n2. **Sum of OOB Predictions and Counts**: For each sample, compute the sum of predictions from out-of-bag trees and the count of such trees.\n3. **Identify Valid Samples**: Determine which samples have at least one out-of-bag prediction.\n4. **Compute Average Predictions**: For valid samples, compute the average prediction using the sum of OOB predictions and the count of OOB trees.\n5. **Calculate Squared Errors**: Compute the squared error between the average predictions and the true values for valid samples.\n6. **Compute OOB-MSE**: Calculate the mean of the squared errors. If no valid samples exist, return -1.0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1.0 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    y_true = np.array(y_true)\n    predictions = np.array(predictions)\n    oob_mask = np.array(oob_mask)\n    \n    sum_pred = np.sum(predictions * oob_mask, axis=1)\n    sum_oob = np.sum(oob_mask, axis=1)\n    \n    valid = sum_oob > 0\n    \n    if not np.any(valid):\n        return -1.0\n    \n    avg_preds = sum_pred[valid] / sum_oob[valid]\n    squared_errors = (avg_preds - y_true[valid]) ** 2\n    mse = np.mean(squared_errors)\n    \n    return round(mse, 4)\n```\n\n### Explanation\n1. **Convert Inputs**: The function starts by converting the input lists to numpy arrays to facilitate efficient array operations.\n2. **Sum Calculations**: It calculates the sum of predictions and the count of out-of-bag trees for each sample using element-wise multiplication and summation.\n3. **Valid Samples**: It identifies samples that have at least one out-of-bag prediction using a boolean mask.\n4. **Average Predictions**: For valid samples, it computes the average prediction by dividing the sum of OOB predictions by the count of OOB trees.\n5. **Squared Errors**: It calculates the squared error between the average predictions and the true values.\n6. **Compute MSE**: The mean of these squared errors is computed to get the OOB-MSE. If no valid samples are found, it returns -1.0.\n\nThis approach ensures that the computation is efficient and handles edge cases where no out-of-bag predictions are available."}
{"task_id": 332, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to construct a Frequent-Pattern Tree (FP-Tree) from a given set of transactions and a minimum support threshold. The FP-Tree is represented as a nested dictionary where each node stores the support count and its children nodes.\n\n### Approach\n1. **Compute Item Frequencies**: First, we count how many transactions each item appears in. This helps us determine which items are frequent based on the given minimum support threshold.\n2. **Filter Frequent Items**: Items that do not meet the minimum support threshold are discarded.\n3. **Global Ordering**: The remaining frequent items are sorted by their support count in descending order. If two items have the same support count, they are sorted alphabetically.\n4. **Construct the FP-Tree**: For each transaction, we process it by removing duplicates, keeping only frequent items, and reordering them according to the global ordering. We then traverse the tree from the root, updating the support counts and creating new nodes as needed.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\"\"\"\n    # Compute the frequency of each item\n    item_counts = Counter()\n    for transaction in transactions:\n        unique_items = set(transaction)\n        for item in unique_items:\n            item_counts[item] += 1\n\n    # Filter out items that do not meet the minimum support\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n\n    # If there are no frequent items, return the root with empty children\n    if not frequent_items:\n        return {'support': 0, 'children': {}}\n\n    # Create the global ordering of items\n    sorted_items = sorted(frequent_items.keys(), key=lambda x: (-frequent_items[x], x))\n    item_order = {item: idx for idx, item in enumerate(sorted_items)}\n\n    # Initialize the root of the FP-Tree\n    root = {'support': 0, 'children': {}}\n\n    # Process each transaction to build the FP-Tree\n    for transaction in transactions:\n        # Remove duplicates and keep only frequent items\n        unique_transaction = list(set(transaction))\n        frequent_transaction = [item for item in unique_transaction if item in frequent_items]\n\n        # Sort the transaction items according to the global ordering\n        frequent_transaction.sort(key=lambda x: item_order[x])\n\n        # Traverse the tree and update counts\n        current_node = root\n        for item in frequent_transaction:\n            if item in current_node['children']:\n                current_node = current_node['children'][item]\n                current_node['support'] += 1\n            else:\n                # Create a new node\n                new_node = {'support': 1, 'children': {}}\n                current_node['children'][item] = new_node\n                current_node = new_node\n\n    return root\n```\n\n### Explanation\n1. **Compute Item Frequencies**: We use a `Counter` to count how many transactions each item appears in.\n2. **Filter Frequent Items**: Items with support less than the minimum threshold are removed.\n3. **Global Ordering**: Items are sorted by their support count in descending order, and alphabetically for ties.\n4. **Construct the FP-Tree**: Each transaction is processed to remove duplicates, keep frequent items, and reorder them. The tree is then built by traversing from the root and updating node supports or creating new nodes as needed.\n\nThis approach ensures that the FP-Tree is constructed efficiently and correctly, adhering to the specified rules and constraints."}
{"task_id": 336, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the kernel matrix using the linear kernel function for given input vectors. The linear kernel function is defined as \\( k(x, y) = x \\cdot y + c_0 \\), where \\( x \\cdot y \\) is the dot product of vectors \\( x \\) and \\( y \\), and \\( c_0 \\) is an optional constant. The function should handle both list and NumPy array inputs and return the kernel matrix as a list of lists of floats, rounded to 4 decimal places.\n\n### Approach\n1. **Handle Default Case for Y**: If the input `Y` is `None`, set `Y` to be the same as `X`.\n2. **Convert Inputs to NumPy Arrays**: Convert both `X` and `Y` to NumPy arrays to facilitate matrix operations.\n3. **Check for Empty Inputs**: If `X` is empty, return an empty list. If `Y` is empty, return `-1` since the kernel matrix cannot be computed.\n4. **Validate Dimensions**: Ensure that both `X` and `Y` have the same number of columns. If not, return `-1`.\n5. **Compute Kernel Matrix**: Use NumPy's dot product to compute the kernel matrix and add the constant \\( c_0 \\) to each element.\n6. **Round and Convert to List**: Round each element of the kernel matrix to 4 decimal places and convert the result to a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Convert inputs to numpy arrays\n    X_np = np.array(X)\n    Y_np = np.array(Y)\n    \n    # Check if X is empty\n    if X_np.shape[0] == 0:\n        return []\n    \n    # Check if Y is empty\n    if Y_np.shape[0] == 0:\n        return -1\n    \n    # Check if the number of columns is the same\n    if X_np.shape[1] != Y_np.shape[1]:\n        return -1\n    \n    # Compute the kernel matrix\n    K = X_np.dot(Y_np.T) + c0\n    \n    # Round to 4 decimal places\n    K_rounded = np.round(K, decimals=4)\n    \n    # Convert to list of lists\n    return K_rounded.tolist()\n```\n\n### Explanation\n1. **Handling Default Case for Y**: If `Y` is not provided, it is set to `X`, ensuring that the function can compute a square kernel matrix when needed.\n2. **Conversion to NumPy Arrays**: This step ensures that both inputs are in a consistent format, allowing for efficient matrix operations.\n3. **Empty Input Check**: If `X` is empty, the function immediately returns an empty list. If `Y` is empty, it returns `-1` as the kernel matrix cannot be computed.\n4. **Dimension Validation**: The function checks that both `X` and `Y` have the same number of columns, returning `-1` if they do not.\n5. **Kernel Matrix Computation**: Using NumPy's dot product, the function efficiently computes the kernel matrix and adds the constant \\( c_0 \\) to each element.\n6. **Rounding and Conversion**: The result is rounded to 4 decimal places and converted to a list of lists to meet the output requirements."}
{"task_id": 340, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single step of the Adam optimization algorithm. Adam is an optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. It computes adaptive learning rates for each parameter and is efficient in terms of both memory and computation.\n\n### Approach\nThe Adam optimization algorithm involves several steps to update the parameters of a model. Here's a detailed breakdown of the approach:\n\n1. **Update the First Moment Estimate (m_t)**: This is calculated using the formula \\( m_t = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot g \\), where \\( m \\) is the previous first moment estimate, \\( g \\) is the gradient, and \\( \\beta_1 \\) is the decay rate for the first moment.\n\n2. **Update the Second Moment Estimate (v_t)**: This is calculated using the formula \\( v_t = \\beta_2 \\cdot v + (1 - \\beta_2) \\cdot g^2 \\), where \\( v \\) is the previous second moment estimate, \\( g^2 \\) is the element-wise square of the gradient, and \\( \\beta_2 \\) is the decay rate for the second moment.\n\n3. **Bias Correction**: The first and second moment estimates are biased towards zero, especially in the initial steps. To correct this, we compute bias-corrected estimates:\n   - \\( \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\)\n   - \\( \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\)\n\n4. **Parameter Update**: Using the bias-corrected estimates, the parameter update is computed as \\( \\theta' = \\theta - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\), where \\( \\alpha \\) is the learning rate and \\( \\epsilon \\) is a small constant to prevent division by zero.\n\n5. **Rounding and Conversion**: The updated parameter and moment estimates are rounded to 8 decimal places and converted to lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\"\"\"\n    # Compute the new moment estimates\n    m_t = beta1 * m + (1 - beta1) * grad\n    v_t = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias correction terms\n    beta1_power = beta1 ** t\n    beta2_power = beta2 ** t\n    \n    m_hat = m_t / (1 - beta1_power)\n    v_hat = v_t / (1 - beta2_power)\n    \n    # Compute the update\n    update = lr * m_hat / (np.sqrt(v_hat) + eps)\n    new_param = param - update\n    \n    # Round and convert to lists\n    new_param_rounded = np.round(new_param, 8).tolist()\n    new_m_rounded = np.round(m_t, 8).tolist()\n    new_v_rounded = np.round(v_t, 8).tolist()\n    \n    return (new_param_rounded, new_m_rounded, new_v_rounded)\n```\n\n### Explanation\nThe provided code implements the Adam optimization algorithm for a single step. Here's a detailed explanation of each part:\n\n1. **Moment Estimates**: The first and second moment estimates are updated using the given gradients and decay rates.\n2. **Bias Correction**: The bias-corrected estimates are computed to adjust for the initial bias in the moment estimates.\n3. **Parameter Update**: The new parameter value is computed using the bias-corrected estimates and the learning rate.\n4. **Rounding and Conversion**: The results are rounded to 8 decimal places and converted to lists to meet the output requirements.\n\nThis approach ensures that the optimization step is efficient and handles multi-dimensional parameters correctly using vectorized operations."}
{"task_id": 343, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core numerical step of the Cross-Entropy Method (CEM) used in Reinforcement Learning to search for good policy parameters. The goal is to update the sampling distribution by retaining the top-performing parameter vectors based on their rewards.\n\n### Approach\n1. **Convert Rewards to Numpy Array**: Ensure the rewards are in a numpy array for efficient operations.\n2. **Determine the Number of Retained Samples**: Calculate the number of top samples to retain using the given fraction `retain_prcnt`.\n3. **Sort and Select Top Samples**: Sort the parameter vectors based on their rewards in descending order and select the top `K` samples.\n4. **Compute Mean and Variance**: Calculate the mean and per-dimension variance of the selected top samples.\n5. **Round Results**: Round the computed mean and variance values to four decimal places and return them as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    rewards = np.asarray(rewards)\n    n = theta_samples.shape[0]\n    k = int(retain_prcnt * n)\n    \n    # Sort indices based on descending rewards\n    sorted_indices = np.argsort(-rewards)\n    selected_indices = sorted_indices[:k]\n    selected_thetas = theta_samples[selected_indices]\n    \n    # Compute mean and variance\n    mean = np.mean(selected_thetas, axis=0)\n    variance = np.var(selected_thetas, axis=0, ddof=0)\n    \n    # Round to four decimal places and convert to lists\n    mean_rounded = [round(x, 4) for x in mean]\n    variance_rounded = [round(x, 4) for x in variance]\n    \n    return (mean_rounded, variance_rounded)\n```\n\n### Explanation\n1. **Convert Rewards**: The rewards are converted to a numpy array to facilitate efficient sorting and indexing.\n2. **Calculate Retained Samples**: The number of top samples to retain is determined by taking the integer part of the product of `retain_prcnt` and the total number of samples.\n3. **Sort and Select**: The indices of the parameter vectors are sorted based on their rewards in descending order. The top `K` indices are selected, and the corresponding parameter vectors are chosen.\n4. **Compute Statistics**: The mean and variance of the selected parameter vectors are computed. The mean is the average along each dimension, and the variance is computed using the population variance (divided by `K`).\n5. **Round Results**: The computed mean and variance values are rounded to four decimal places and returned as lists to meet the output requirements.\n\nThis approach efficiently narrows down the best-performing parameter vectors and updates the sampling distribution, which is crucial for the Cross-Entropy Method in optimizing policy parameters."}
{"task_id": 353, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Shannon entropy of a given sequence of class labels. Shannon entropy is a measure of impurity or disorder in a set of class labels, which is used in decision-tree learning algorithms. The entropy is calculated using the probabilities of each class label in the given sequence.\n\n### Approach\n1. **Check for Empty Input**: If the input sequence is empty, the entropy is 0.0.\n2. **Count Frequencies**: Use a counter to determine the frequency of each class label in the input sequence.\n3. **Calculate Probabilities**: For each class label, compute its probability by dividing its count by the total number of labels.\n4. **Compute Entropy**: Using the probabilities, compute the entropy using the formula \\( H = -\\sum p_i \\log_2 p_i \\). If a label's probability is zero, its contribution to the entropy is considered zero.\n5. **Return Result**: Round the computed entropy to five decimal places and return it.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    counts = Counter(labels)\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    entropy_val = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy_val += p * math.log2(p)\n    entropy_val = -entropy_val\n    return round(entropy_val, 5)\n```\n\n### Explanation\n1. **Empty Input Handling**: The function first checks if the input sequence is empty by summing the values of the counter. If the total is zero, it returns 0.0 immediately.\n2. **Frequency Counting**: The `Counter` from the `collections` module is used to count the occurrences of each class label efficiently.\n3. **Probability Calculation**: For each unique class label, the probability is calculated by dividing its count by the total number of labels.\n4. **Entropy Calculation**: The entropy is computed by summing the product of each probability and its logarithm base 2, then taking the negative of this sum to get the final entropy value.\n5. **Rounding**: The result is rounded to five decimal places to meet the problem's requirements.\n\nThis approach ensures that the function handles various types of input iterables, including lists, NumPy arrays, and generators, efficiently and correctly computes the Shannon entropy."}
{"task_id": 355, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Principal Component Analysis (PCA) with two solvers: singular value decomposition (SVD) and eigen-decomposition of the covariance matrix. The goal is to standardize the data, compute the principal components, adjust their signs deterministically, project the data onto these components, and return the scores and explained variance ratios.\n\n### Approach\n1. **Standardize the Data**: Subtract the mean of each feature from the data to center it.\n2. **Compute Principal Components**:\n   - **SVD Solver**: Use SVD on the centered data. The right singular vectors from SVD are the principal components.\n   - **Eigen Solver**: Compute the covariance matrix of the centered data and use eigen-decomposition to find the principal components.\n3. **Sort Components**: Sort the principal components by the variance they explain in descending order.\n4. **Adjust Signs**: Ensure the sign of each principal component is deterministic by flipping the sign if the first non-zero element is negative.\n5. **Project Data**: Project the centered data onto the selected principal components to get the scores.\n6. **Compute Explained Variance**: Calculate the fraction of variance explained by each selected component.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Center the data\n    data_centered = data - data.mean(axis=0)\n    n_samples, n_features = data_centered.shape\n    n_components = min(n_components, n_features)\n    \n    if solver == 'svd':\n        # Compute SVD\n        U, S, Vh = np.linalg.svd(data_centered, full_matrices=False)\n        eigenvalues = (S ** 2) / (n_samples - 1)\n        V = Vh.T\n        selected_eigenvectors = V[:, :n_components]\n    elif solver == 'eigen':\n        # Compute covariance matrix and eigen-decomposition\n        cov_matrix = np.cov(data_centered, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(-eigenvalues)\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        selected_eigenvectors = eigenvectors[:, :n_components]\n    else:\n        raise ValueError(\"Invalid solver\")\n    \n    # Adjust the sign of each selected eigenvector\n    for i in range(n_components):\n        vec = selected_eigenvectors[:, i]\n        first_non_zero = 0\n        while first_non_zero < len(vec) and vec[first_non_zero] == 0:\n            first_non_zero += 1\n        if first_non_zero < len(vec):\n            if vec[first_non_zero] < 0:\n                selected_eigenvectors[:, i] *= -1\n    \n    # Project the centered data onto the selected eigenvectors\n    scores = data_centered @ selected_eigenvectors\n    scores_rounded = np.round(scores, 4).tolist()\n    \n    # Compute explained variance ratio\n    total_variance = np.sum(eigenvalues)\n    explained_variance = eigenvalues[:n_components] / total_variance\n    explained_variance_ratio = [round(r, 4) for r in explained_variance]\n    \n    return (scores_rounded, explained_variance_ratio)\n```\n\n### Explanation\n1. **Standardization**: The data is centered by subtracting the mean of each feature.\n2. **SVD Solver**: The SVD of the centered data is computed, and the right singular vectors (columns of `Vh`) are used as principal components. The eigenvalues are derived from the singular values.\n3. **Eigen Solver**: The covariance matrix of the centered data is computed, and eigen-decomposition is performed. The eigenvalues and eigenvectors are sorted in descending order.\n4. **Sign Adjustment**: Each principal component is adjusted to ensure the first non-zero element is positive, removing sign ambiguity.\n5. **Projection**: The centered data is projected onto the selected principal components to obtain the scores.\n6. **Variance Calculation**: The explained variance ratio for each component is computed and rounded to four decimal places.\n\nThis approach ensures that the PCA is performed accurately and deterministically, providing consistent results across runs."}
{"task_id": 356, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a helper function `leaf_predict` that extracts the correct prediction from a `Leaf` instance in a decision tree. The function should handle both classification and regression tasks based on the provided boolean flag `classifier`.\n\n### Approach\n1. **Problem Analysis**:\n   - For a classification tree, each leaf node stores an array of class probabilities. The prediction is the index of the highest probability, with ties resolved in favor of the smaller index.\n   - For a regression tree, each leaf node stores a scalar value representing the mean of the target values in that region.\n\n2. **Intuition**:\n   - If `classifier` is `True`, the function should return the index of the maximum value in the array of probabilities. This is done by finding the maximum value and then determining its first occurrence in the array.\n   - If `classifier` is `False`, the function should return the scalar value directly.\n\n3. **Algorithm Selection**:\n   - Use the `max` function to find the highest probability in the classification case.\n   - Use the `index` method to find the first occurrence of this maximum value.\n   - For regression, simply return the scalar value stored in the leaf.\n\n### Solution Code\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        max_val = max(leaf.value)\n        return leaf.value.index(max_val)\n    else:\n        return leaf.value\n```\n\n### Explanation\n- **Classification Case**: When `classifier` is `True`, the function retrieves the maximum value from the leaf's value array. It then finds the index of this maximum value, ensuring that the smallest index is chosen in case of ties.\n- **Regression Case**: When `classifier` is `False`, the function directly returns the scalar value stored in the leaf, which represents the mean of the target values in that region.\n\nThis approach efficiently handles both classification and regression tasks by leveraging simple array operations and direct value retrieval, ensuring accurate predictions based on the structure of the decision tree."}
{"task_id": 362, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given 2D list or NumPy array into a row-stochastic matrix. A row-stochastic matrix is one where each row sums to 1. If any row in the input matrix sums to zero, making normalization impossible, the function should return -1.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input data is first converted into a NumPy array of floats to handle both list of lists and NumPy array inputs uniformly.\n2. **Compute Row Sums**: Calculate the sum of each row in the matrix.\n3. **Check for Zero Row Sums**: If any row sum is zero, return -1 as normalization is impossible for that row.\n4. **Normalize Each Row**: Divide each element in a row by the row's sum to convert the matrix into a row-stochastic matrix.\n5. **Round Values**: Round each element of the resulting matrix to four decimal places.\n6. **Convert to List of Lists**: Convert the NumPy array back into a native Python list of lists and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    matrix = np.array(data, dtype=float)\n    row_sums = np.sum(matrix, axis=1)\n    \n    if np.any(row_sums == 0):\n        return -1\n    \n    stochastic = matrix / row_sums[:, np.newaxis]\n    stochastic = np.round(stochastic, decimals=4)\n    \n    return stochastic.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input data is converted to a NumPy array to leverage efficient numerical operations.\n2. **Row Sum Calculation**: Using NumPy's `sum` function along axis=1, we compute the sum of each row.\n3. **Zero Sum Check**: We check if any row sum is zero. If so, return -1 immediately.\n4. **Row Normalization**: Each row is divided by its respective sum using broadcasting to ensure each element is scaled correctly.\n5. **Rounding**: The resulting matrix is rounded to four decimal places to meet the precision requirement.\n6. **Conversion to List**: The NumPy array is converted back to a list of lists for the final output.\n\nThis approach ensures that the function efficiently handles both list and NumPy array inputs, performs necessary checks, and returns the correctly normalized matrix or an error indicator when normalization is impossible."}
{"task_id": 363, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if the action and observation spaces of a given reinforcement learning environment are continuous. The solution involves checking the type of each space and ensuring that all sub-spaces within composite spaces (like Tuple or Dict) are continuous.\n\n### Approach\n1. **Problem Analysis**: The problem requires us to check if the action and observation spaces are continuous. A space is considered continuous if it is an instance of `Box`. For composite spaces (Tuple or Dict), the space is continuous only if all its sub-spaces are `Box`.\n\n2. **Intuition**: The function `is_continuous` takes an environment object and two flags indicating whether the action and observation spaces are composite (Tuple or Dict). Based on these flags, it checks each space to determine if it is continuous.\n\n3. **Algorithm**:\n   - For each space (action and observation), if the corresponding flag is `True`, check if all sub-spaces are `Box`.\n   - If the flag is `False`, simply check if the space itself is a `Box`.\n\n4. **Complexity**: The solution involves iterating through each sub-space in composite spaces, resulting in a time complexity of O(n) where n is the number of sub-spaces. This is efficient given the typical structure of RL environments.\n\n### Solution Code\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    # Check action space\n    cont_action = True\n    if tuple_action:\n        if isinstance(env.action_space, Tuple):\n            for space in env.action_space.spaces:\n                if not isinstance(space, Box):\n                    cont_action = False\n                    break\n        elif isinstance(env.action_space, Dict):\n            for space in env.action_space.spaces.values():\n                if not isinstance(space, Box):\n                    cont_action = False\n                    break\n        else:\n            # According to problem statement, this shouldn't happen\n            cont_action = False\n    else:\n        cont_action = isinstance(env.action_space, Box)\n    \n    # Check observation space\n    cont_obs = True\n    if tuple_obs:\n        if isinstance(env.observation_space, Tuple):\n            for space in env.observation_space.spaces:\n                if not isinstance(space, Box):\n                    cont_obs = False\n                    break\n        elif isinstance(env.observation_space, Dict):\n            for space in env.observation_space.spaces.values():\n                if not isinstance(space, Box):\n                    cont_obs = False\n                    break\n        else:\n            # According to problem statement, this shouldn't happen\n            cont_obs = False\n    else:\n        cont_obs = isinstance(env.observation_space, Box)\n    \n    return (cont_action, cont_obs)\n```\n\n### Explanation\n- **Helper Logic**: The function checks each space based on the provided flags. If a space is composite (Tuple or Dict), it iterates through all sub-spaces to ensure they are all `Box` instances.\n- **Efficiency**: The solution efficiently checks each sub-space, ensuring that the function runs in linear time relative to the number of sub-spaces, making it suitable for typical RL environments.\n- **Correctness**: The approach correctly handles both simple and composite spaces, ensuring accurate determination of continuity for both action and observation spaces."}
{"task_id": 369, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the outputs of multiple weak learners (decision trees) in a gradient boosting model. The aggregation process depends on whether the problem is a regression or a classification task.\n\n### Approach\n1. **Initialization**: Start with a running prediction initialized to zeros. The shape of this running prediction depends on the type of problem (regression or classification).\n2. **Update Aggregation**: For each tree's output (update), subtract the scaled update (by the learning rate) from the running prediction. This is done in a vectorized manner to ensure efficiency.\n3. **Post-Aggregation Processing**:\n   - For regression, the running prediction is directly used as the final output, rounded to 4 decimal places.\n   - For classification, the running prediction (logits) is converted to a probability distribution using the softmax function, and the class with the highest probability is selected using the argmax function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    if not updates:\n        if regression:\n            return np.array([], dtype=np.float64)\n        else:\n            return np.array([], dtype=np.int64)\n    \n    sum_updates = sum(updates)\n    running_pred = -learning_rate * sum_updates\n    \n    if regression:\n        return np.round(running_pred, decimals=4)\n    else:\n        return np.argmax(running_pred, axis=1)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if the list of updates is empty. If it is, it returns an empty array of appropriate type based on the problem type.\n2. **Summing Updates**: The updates from all trees are summed element-wise. This sum is then scaled by the learning rate and negated to form the running prediction.\n3. **Processing Running Prediction**:\n   - For regression, the running prediction is rounded to 4 decimal places and returned.\n   - For classification, the argmax function is applied row-wise to the running prediction to determine the predicted class labels.\n\nThis approach ensures that the aggregation is done efficiently using vectorized operations, making the solution both optimal and easy to understand."}
{"task_id": 373, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Gini impurity of a given sequence of integer class labels. The Gini impurity is a measure used in decision-tree learning to quantify how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n\n### Approach\nThe Gini impurity is defined as:\n$$\nGini = 1 - \\sum_{c=0}^{C-1} p_c^{\\,2},\n$$\nwhere \\( p_c \\) is the relative frequency of class \\( c \\), \\( n_c \\) is the number of samples having class \\( c \\), \\( C \\) is the number of distinct classes, and \\( N \\) is the total number of samples.\n\nThe steps to compute the Gini impurity are as follows:\n1. Convert the input list or NumPy array to a NumPy array for uniform processing.\n2. Check if the input array is empty. If it is, return 0.0.\n3. Use NumPy's `unique` function to get the unique class labels and their respective counts.\n4. Calculate the total number of samples \\( N \\).\n5. Compute the relative frequency \\( p_c \\) for each class and then the sum of the squares of these frequencies.\n6. Subtract this sum from 1 to get the Gini impurity.\n7. Round the result to four decimal places and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    y = np.asarray(y)\n    if len(y) == 0:\n        return 0.0\n    unique_classes, counts = np.unique(y, return_counts=True)\n    N = len(y)\n    sum_p_squared = np.sum((counts / N) ** 2)\n    gini_impurity = 1 - sum_p_squared\n    return round(gini_impurity, 4)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input is converted to a NumPy array to leverage efficient operations provided by NumPy.\n2. **Empty Check**: If the input array is empty, the function immediately returns 0.0.\n3. **Unique Classes and Counts**: Using `np.unique`, we obtain the unique class labels and their counts, which helps in determining the relative frequencies.\n4. **Relative Frequencies and Sum of Squares**: For each class, the relative frequency is calculated, squared, and summed up.\n5. **Gini Impurity Calculation**: The Gini impurity is computed by subtracting the sum of squared relative frequencies from 1.\n6. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n\nThis approach efficiently handles both lists and NumPy arrays, ensuring accurate and optimal computation of the Gini impurity."}
{"task_id": 374, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that trains a multiple linear regression model using batch gradient descent. The goal is to minimize the mean-squared error (MSE) by iteratively adjusting the model parameters.\n\n### Approach\n1. **Problem Analysis**: The task involves training a linear regression model using batch gradient descent. The model aims to minimize the MSE between the predicted values and the actual target values. The function should handle multiple features and return the learned parameters after a specified number of iterations.\n\n2. **Initialization**: We start by initializing the weight vector `\u03b8` with zeros and the bias `b` to zero. This sets the starting point for our gradient descent algorithm.\n\n3. **Gradient Descent Updates**: For each iteration, we compute the predicted values `\u0177` using the current weights and bias. We then calculate the gradients for the weights and bias using the MSE loss function. The gradients are used to update the weights and bias in the direction that minimizes the loss.\n\n4. **Convergence Check**: The process is repeated for a fixed number of iterations. After each iteration, the weights and bias are updated using the computed gradients scaled by the learning rate.\n\n5. **Return Parameters**: After completing the specified number of iterations, the learned parameters (weights and bias) are rounded to four decimal places and returned as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\"\"\"\n    if X.shape[0] != y.shape[0]:\n        return -1\n    \n    m = X.shape[0]\n    n = X.shape[1]\n    \n    theta = np.zeros(n)\n    b = 0.0\n    \n    for _ in range(n_iterations):\n        y_hat = X.dot(theta) + b\n        dw = (2 / m) * X.T.dot(y_hat - y)\n        db = (2 / m) * np.sum(y_hat - y)\n        \n        theta -= learning_rate * dw\n        b -= learning_rate * db\n    \n    theta_rounded = [round(x, 4) for x in theta]\n    b_rounded = round(b, 4)\n    \n    return (theta_rounded, b_rounded)\n```\n\n### Explanation\n- **Input Check**: The function first checks if the number of samples in the feature matrix `X` matches the number of samples in the target vector `y`. If not, it returns -1.\n- **Initialization**: The weight vector `\u03b8` is initialized to zeros, and the bias `b` is initialized to zero.\n- **Gradient Descent Loop**: For each iteration, the predicted values `\u0177` are computed. The gradients for the weights and bias are calculated using the MSE loss function. These gradients are then used to update the weights and bias.\n- **Rounding and Return**: After completing the specified number of iterations, the weights and bias are rounded to four decimal places and returned as a tuple.\n\nThis approach ensures that the model parameters are optimized to minimize the MSE, providing a robust solution for multiple linear regression using batch gradient descent."}
{"task_id": 376, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine if a given 2-D numeric array is symmetric with respect to its main diagonal. A matrix is symmetric if it is square and equal to its transpose. Additionally, due to potential floating-point rounding errors, we consider two numbers equal if their absolute difference does not exceed \\(1 \\times 10^{-8}\\).\n\n### Approach\n1. **Check if the Matrix is Square**: The first step is to verify if the given matrix is square. This is done by ensuring that the number of rows is equal to the number of columns in each row.\n2. **Convert to NumPy Array**: Convert the given list of lists into a NumPy array for efficient matrix operations.\n3. **Transpose the Matrix**: Compute the transpose of the matrix using NumPy's transpose method.\n4. **Check Symmetry with Tolerance**: Use NumPy's `allclose` function to check if the original matrix is equal to its transpose within an absolute tolerance of \\(1 \\times 10^{-8}\\). This ensures that we account for any floating-point precision issues.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Check if the matrix is square\n    n = len(X)\n    for row in X:\n        if len(row) != n:\n            return False\n    \n    # Convert to a NumPy array\n    arr = np.array(X)\n    \n    # Check if the array is equal to its transpose with the given tolerance\n    return np.allclose(arr, arr.T, rtol=0, atol=1e-8)\n```\n\n### Explanation\n1. **Square Check**: The function first checks if the matrix is square by comparing the length of each row with the number of rows. If any row does not match the required length, the function returns `False`.\n2. **NumPy Conversion**: The matrix is converted into a NumPy array to leverage efficient matrix operations.\n3. **Transpose and Comparison**: The transpose of the matrix is computed, and the original matrix is compared to its transpose using `np.allclose` with an absolute tolerance of \\(1 \\times 10^{-8}\\). This ensures that minor differences due to floating-point precision do not affect the result.\n\nThis approach efficiently checks for matrix symmetry while handling potential issues with floating-point arithmetic."}
{"task_id": 377, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the negative gradient for the logistic loss function used in Gradient Boosting for binary classification. The function should handle binary class labels and current prediction scores, returning the negative gradients rounded to four decimal places. If the input labels are invalid, the function should return -1.\n\n### Approach\n1. **Input Validation**: Check if the lengths of the input lists `y` and `f` are equal. If not, return -1. Also, ensure that all elements in `y` are either 0 or 1. If any element is invalid, return -1.\n2. **Conversion to Numpy Arrays**: Convert the input lists to numpy arrays for efficient vectorized operations.\n3. **Compute y'**: Convert each label `y_i` to `y'_i` using the formula `y'_i = 2 * y_i - 1`.\n4. **Compute Denominator**: For each element, compute the denominator as `1 + exp(y'_i * f_i)`.\n5. **Compute Negative Gradient**: Calculate the negative gradient using the formula `g_i = y'_i / denominator`.\n6. **Rounding**: Round each element of the gradient to four decimal places.\n7. **Return Result**: Convert the result back to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"Compute the negative gradient for logistic loss in gradient boosting.\n\n    Parameters:\n    y (list): Binary class labels (0 or 1)\n    f (list): Current prediction scores (floats)\n\n    Returns:\n    list: The list of negative gradients, rounded to four decimal places.\n    If y contains values other than 0 or 1, returns -1.\n    \"\"\"\n    # Check if the lengths of y and f are the same\n    if len(y) != len(f):\n        return -1\n    \n    # Check each label in y is either 0 or 1\n    for label in y:\n        if label not in (0, 1):\n            return -1\n    \n    # Convert lists to numpy arrays for efficient computation\n    y_np = np.array(y)\n    f_np = np.array(f)\n    \n    # Compute y' = 2y - 1\n    y_prime = 2 * y_np - 1\n    \n    # Compute the denominator: 1 + exp(y' * f)\n    denominator = 1 + np.exp(y_prime * f_np)\n    \n    # Compute the negative gradient g = y' / denominator\n    g = y_prime / denominator\n    \n    # Round each element to four decimal places\n    g_rounded = np.round(g, 4)\n    \n    # Convert numpy array back to a list and return\n    return g_rounded.tolist()\n```\n\n### Explanation\n- **Input Validation**: Ensures that the inputs are valid and compatible for further computations.\n- **Conversion to Numpy Arrays**: Utilizes numpy for efficient element-wise operations, which is crucial for handling potentially large datasets.\n- **Computation of y'**: Converts binary labels to the set {-1, 1} to align with the logistic loss function's requirements.\n- **Denominator Calculation**: Computes the exponential term to form the denominator, which is essential for the logistic loss gradient.\n- **Gradient Calculation**: Derives the negative gradient using the computed values, which is then rounded for precision.\n- **Return**: Converts the result back to a list to meet the function's return type requirements.\n\nThis approach ensures that the function efficiently computes the required gradients while handling edge cases and invalid inputs appropriately."}
{"task_id": 380, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Scaled Exponential Linear Unit (SELU) activation function along with its first and second derivatives. The function should handle scalar, 1-D, and multi-D inputs and return the results rounded to four decimal places.\n\n### Approach\n1. **SELU Function Definition**: The SELU function is defined as `SELU(x) = scale * ELU(x, alpha)`, where ELU is the Exponential Linear Unit function. The ELU function is piecewise, defined as `x` for `x > 0` and `alpha * (e^x - 1)` for `x <= 0`.\n\n2. **Derivatives**: \n   - The first derivative of ELU is `1` for `x > 0` and `alpha * e^x` for `x <= 0`.\n   - The second derivative of ELU is `0` for `x > 0` and `alpha * e^x` for `x <= 0`.\n\n3. **Handling Different Input Types**: The function should handle scalar, 1-D, and multi-D inputs uniformly by converting them into NumPy arrays for computation. After computation, the results are converted back to the appropriate type (scalar or list).\n\n4. **Rounding and Conversion**: The results are rounded to four decimal places and converted to a list using NumPy's `tolist()` method, except for scalar inputs which are returned as floats.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Determine if the original input was a scalar\n    original_is_scalar = False\n    if isinstance(x, (float, int)):\n        original_is_scalar = True\n    elif isinstance(x, np.ndarray) and x.ndim == 0:\n        original_is_scalar = True\n    else:\n        original_is_scalar = False\n\n    # Convert x to a numpy array\n    x = np.asarray(x)\n\n    # Constants\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    # Compute mask\n    mask = x > 0\n\n    if order == 0:\n        # Compute ELU\n        elu = np.where(mask, x, alpha * (np.exp(x) - 1))\n        result = scale * elu\n    elif order == 1:\n        # Compute first derivative of ELU\n        d_elu = np.where(mask, 1, alpha * np.exp(x))\n        result = scale * d_elu\n    elif order == 2:\n        # Compute second derivative of ELU\n        d2_elu = np.where(mask, 0, alpha * np.exp(x))\n        result = scale * d2_elu\n    else:\n        raise ValueError(\"order must be 0, 1, or 2\")\n\n    # Round to 4 decimals\n    result = np.round(result, 4)\n\n    # Convert to scalar or list\n    if original_is_scalar:\n        return result.item()\n    else:\n        return result.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function first checks if the input `x` is a scalar. It then converts `x` into a NumPy array to handle all input types uniformly.\n\n2. **Constants**: The constants `alpha` and `scale` are defined as per the original paper's recommendations.\n\n3. **Masking**: A boolean mask is created to identify elements of `x` that are greater than zero. This mask is used to apply the correct piecewise function for ELU and its derivatives.\n\n4. **Computation Based on Order**: Depending on the `order` parameter, the function computes the SELU value, its first derivative, or its second derivative using the appropriate piecewise functions.\n\n5. **Rounding and Conversion**: The results are rounded to four decimal places. If the original input was a scalar, the result is returned as a float; otherwise, it is returned as a list.\n\nThis approach ensures that the function handles various input types efficiently and returns the results in the required format."}
{"task_id": 387, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor that works with one-dimensional data. The model uses decision stumps, which are depth-1 trees, to iteratively improve predictions by minimizing the sum of squared residuals.\n\n### Approach\n1. **Initialization**: Start by initializing the predictions as the mean of the target values.\n2. **Iterative Boosting**: For each boosting round, compute the residuals (differences between true values and current predictions). These residuals are used to train a decision stump.\n3. **Sorting and Splitting**: Sort the data by the feature values and evaluate all possible split points between consecutive different feature values.\n4. **Optimal Split Selection**: For each candidate split, compute the mean residuals for the left and right groups and determine the split that minimizes the sum of squared errors.\n5. **Update Predictions**: Use the optimal split to update the predictions, scaled by the learning rate.\n6. **Edge Cases**: Handle cases where all feature values are identical by using the global mean of residuals.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n = len(x)\n    current_predictions = np.full(n, np.mean(y))\n    \n    for _ in range(n_estimators):\n        residuals = y - current_predictions\n        \n        # Sort the data by x\n        sorted_indices = np.argsort(x)\n        sorted_x = x[sorted_indices]\n        sorted_r = residuals[sorted_indices]\n        \n        # Generate possible split points\n        split_points = []\n        for i in range(len(sorted_x) - 1):\n            if sorted_x[i] != sorted_x[i + 1]:\n                t = (sorted_x[i] + sorted_x[i + 1]) / 2\n                split_points.append(t)\n        \n        # If no valid split, use global mean\n        if not split_points:\n            best_r_hat = np.mean(sorted_r)\n            current_predictions += learning_rate * best_r_hat\n            continue\n        \n        # Evaluate each split point\n        min_error = float('inf')\n        best_t = None\n        best_r_L = 0.0\n        best_r_R = 0.0\n        \n        for t in split_points:\n            # Split the sorted residuals\n            left_mask = sorted_x <= t\n            left = sorted_r[left_mask]\n            right = sorted_r[~left_mask]\n            \n            r_L = np.mean(left) if len(left) > 0 else 0.0\n            r_R = np.mean(right) if len(right) > 0 else 0.0\n            \n            # Calculate the sum of squared errors\n            error = np.sum((left - r_L) ** 2) + np.sum((right - r_R) ** 2)\n            \n            if error < min_error:\n                min_error = error\n                best_t = t\n                best_r_L = r_L\n                best_r_R = r_R\n        \n        # Create the stump predictions\n        r_hat = np.zeros(n)\n        for i in range(n):\n            if x[i] <= best_t:\n                r_hat[i] = best_r_L\n            else:\n                r_hat[i] = best_r_R\n        \n        # Update the current predictions\n        current_predictions += learning_rate * r_hat\n    \n    # Round to four decimal places and convert to list\n    return np.round(current_predictions, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial predictions are set to the mean of the target values, providing a baseline prediction.\n2. **Residual Calculation**: For each boosting round, residuals are computed to guide the next decision stump.\n3. **Sorting and Splitting**: The data is sorted by the feature values to evaluate potential split points, ensuring each split is between different consecutive values.\n4. **Optimal Split Selection**: Each candidate split is evaluated to find the one that minimizes the sum of squared residuals, ensuring the best possible improvement in predictions.\n5. **Prediction Update**: The optimal split's predictions are used to update the current predictions, scaled by the learning rate to control the model's update step size.\n6. **Edge Cases Handling**: If all feature values are identical, the model uses the global mean of residuals to avoid division by zero and ensure valid predictions.\n\nThis approach ensures that each decision stump contributes to minimizing the prediction error iteratively, leading to an improved model over multiple boosting rounds."}
{"task_id": 394, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the RMSprop optimization algorithm, which is used to update the parameters of a neural network during training. RMSprop adapts the learning rate for each parameter based on the average of the squared gradients, providing better convergence properties compared to standard gradient descent.\n\n### Approach\nThe RMSprop algorithm maintains a running average of the squared gradients, which is used to scale the learning rate for each parameter. The key steps in the algorithm are:\n\n1. **Initialize the Running Average**: If the running average of squared gradients (`Eg`) is not provided, initialize it to a zero array with the same shape as the gradient.\n2. **Update the Running Average**: Compute the new running average using the formula \\(E_g^{(t)} = \\rho E_g^{(t-1)} + (1 - \\rho) g^2\\), where \\(\\rho\\) is the decay rate.\n3. **Compute the Denominator**: Calculate the denominator for the parameter update as the square root of the new running average plus a small constant \\(\\varepsilon\\) (to avoid division by zero).\n4. **Update the Parameters**: Adjust the parameters using the formula \\(w^{(t)} = w^{(t-1)} - \\frac{\\alpha g}{\\sqrt{E_g^{(t)} + \\varepsilon}}\\), where \\(\\alpha\\) is the learning rate.\n5. **Round and Convert to Lists**: Round the updated parameters and the new running average to four decimal places and convert them to regular Python lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\"\"\"\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    new_Eg = rho * Eg + (1 - rho) * (grad ** 2)\n    denominator = np.sqrt(new_Eg + 1e-8)\n    w_next = w - (learning_rate * grad) / denominator\n    rounded_w = np.round(w_next, 4)\n    rounded_Eg = np.round(new_Eg, 4)\n    \n    # Convert to lists, handling 0D arrays\n    if rounded_w.ndim == 0:\n        w_list = [rounded_w.item()]\n    else:\n        w_list = rounded_w.tolist()\n    \n    if rounded_Eg.ndim == 0:\n        Eg_list = [rounded_Eg.item()]\n    else:\n        Eg_list = rounded_Eg.tolist()\n    \n    return (w_list, Eg_list)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if the running average `Eg` is `None`. If it is, `Eg` is initialized to a zero array with the same shape as the gradient.\n2. **Update Running Average**: The new running average `new_Eg` is computed using the given decay rate \\(\\rho\\) and the squared gradient values.\n3. **Compute Denominator**: The denominator for the parameter update is calculated as the square root of `new_Eg` plus a small constant to ensure numerical stability.\n4. **Parameter Update**: The parameters `w` are updated using the computed denominator and the given learning rate.\n5. **Rounding and Conversion**: The updated parameters and the new running average are rounded to four decimal places. They are then converted to Python lists, ensuring that even 0-dimensional arrays are converted to lists with a single element.\n\nThis approach ensures that the RMSprop update step is performed correctly, providing the necessary adaptivity in learning rates for efficient training of neural networks."}
{"task_id": 398, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Exponential Linear Unit (ELU) activation function and its gradient. The ELU function is widely used in deep learning due to its effectiveness in accelerating network convergence and mitigating the vanishing gradient problem.\n\n### Approach\nThe ELU activation function is defined as:\n- ELU(x, \u03b1) = x for x \u2265 0\n- ELU(x, \u03b1) = \u03b1(e\u02e3 - 1) for x < 0\n\nThe derivative of the ELU function is:\n- ELU'(x, \u03b1) = 1 for x \u2265 0\n- ELU'(x, \u03b1) = \u03b1e\u02e3 for x < 0\n\nThe approach involves the following steps:\n1. Convert the input `x` into a NumPy array for efficient element-wise operations.\n2. Create a mask to identify elements where `x` is non-negative.\n3. Compute the ELU activation or its gradient based on the mask and the `derivative` flag.\n4. Round the results to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    x = np.asarray(x)\n    mask = x >= 0\n    if not derivative:\n        result = np.where(mask, x, alpha * (np.exp(x) - 1))\n    else:\n        result = np.where(mask, 1.0, alpha * np.exp(x))\n    rounded = np.round(result, 4).tolist()\n    return rounded\n```\n\n### Explanation\n1. **Input Conversion**: The input `x` is converted to a NumPy array using `np.asarray(x)` to handle both lists and NumPy arrays efficiently.\n2. **Mask Creation**: A boolean mask is created to identify elements where `x` is non-negative. This helps in applying the correct transformation for each element based on its value.\n3. **ELU Activation Calculation**: If `derivative` is `False`, the function computes the ELU activation using `np.where` to apply the correct formula for each element based on the mask.\n4. **Gradient Calculation**: If `derivative` is `True`, the function computes the gradient using `np.where` to apply the correct formula for each element based on the mask.\n5. **Rounding and Conversion**: The results are rounded to four decimal places using `np.round` and converted to a list using `tolist()` before being returned.\n\nThis approach ensures efficient computation and correct handling of both the activation function and its gradient, providing the required results in a user-friendly format."}
{"task_id": 411, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute various statistics for collections of actions and observations in a reinforcement learning context. These statistics help in understanding the structure and nature of the data, such as whether the data is multi-dimensional, discrete, or continuous.\n\n### Approach\nThe approach involves processing both actions and observations to extract the required statistics. Here's a detailed breakdown of the steps:\n\n1. **Check for Iterable Elements**: Determine if any element in the actions or observations list is an iterable (list, tuple, or numpy array). This helps in identifying if the data is multi-dimensional.\n\n2. **Determine Dimensionality**: For each list, determine the dimensionality. If the list is empty, the dimensionality is 0. If the first element is an iterable, the dimensionality is the length of that iterable. Otherwise, it is 1.\n\n3. **Check for Continuous Values**: Flatten the list to check if any value is a non-integer float, indicating continuous data.\n\n4. **Collect Unique Values per Dimension**: For each dimension, collect all values, determine the unique values, and count them. This helps in understanding the range and distribution of values in each dimension.\n\n5. **Construct Result Dictionary**: Combine all the computed statistics into a dictionary with the specified keys.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\"\"\"\n    def process_list(lst):\n        # Determine if any element is an iterable (list, tuple, or numpy array with ndim > 0)\n        is_tuple = any(isinstance(e, (list, tuple)) or (isinstance(e, np.ndarray) and e.ndim > 0) for e in lst)\n        \n        # Determine dimensionality\n        if len(lst) == 0:\n            dim = 0\n        else:\n            first_element = lst[0]\n            if isinstance(first_element, (list, tuple)):\n                dim = len(first_element)\n            elif isinstance(first_element, np.ndarray):\n                if first_element.ndim == 0:\n                    dim = 1\n                else:\n                    dim = first_element.shape[0]\n            else:\n                dim = 1\n        \n        # Check if multidimensional\n        multidim = dim > 1\n        \n        # Check for continuous values\n        flattened = []\n        for e in lst:\n            if isinstance(e, (list, tuple)) or (isinstance(e, np.ndarray) and e.ndim > 0):\n                if isinstance(e, np.ndarray) and e.ndim == 0:\n                    flattened.append(e.item())\n                else:\n                    flattened.extend(e)\n            else:\n                flattened.append(e)\n        continuous = any(isinstance(v, float) and not v.is_integer() for v in flattened)\n        \n        # Collect unique values per dimension\n        if dim == 0:\n            n_per_dim = []\n            ids = []\n        else:\n            all_components = []\n            for e in lst:\n                if isinstance(e, (list, tuple)) or (isinstance(e, np.ndarray) and e.ndim > 0):\n                    if isinstance(e, np.ndarray) and e.ndim == 0:\n                        components = [e.item()]\n                    else:\n                        components = list(e)\n                else:\n                    components = [e]\n                all_components.append(components)\n            \n            n_per_dim = []\n            ids = []\n            for i in range(dim):\n                dim_values = [comp[i] for comp in all_components]\n                unique = sorted(list(set(dim_values)))\n                n_per_dim.append(len(unique))\n                ids.append(unique)\n        \n        return {\n            'tuple': is_tuple,\n            'multidim': multidim,\n            'continuous': continuous,\n            'dim': dim,\n            'n_per_dim': n_per_dim,\n            'ids': ids\n        }\n    \n    # Process actions and observations\n    action_stats = process_list(actions)\n    obs_stats = process_list(observations)\n    \n    # Construct the result dictionary\n    result = {\n        'tuple_actions': action_stats['tuple'],\n        'tuple_observations': obs_stats['tuple'],\n        'multidim_actions': action_stats['multidim'],\n        'multidim_observations': obs_stats['multidim'],\n        'continuous_actions': action_stats['continuous'],\n        'continuous_observations': obs_stats['continuous'],\n        'n_actions_per_dim': action_stats['n_per_dim'],\n        'action_dim': action_stats['dim'],\n        'n_obs_per_dim': obs_stats['n_per_dim'],\n        'obs_dim': obs_stats['dim'],\n        'action_ids': action_stats['ids'],\n        'obs_ids': obs_stats['ids']\n    }\n    \n    return result\n```\n\n### Explanation\nThe function `env_stats` processes both actions and observations to compute various statistics. It uses a helper function `process_list` to handle the common processing steps for each list. The helper function determines if the list contains iterables, computes the dimensionality, checks for continuous values, and collects unique values per dimension. The results are then combined into a dictionary with the required keys, providing a comprehensive summary of the data statistics."}
{"task_id": 416, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the probability density function (PDF) of a multivariate Gaussian distribution without using any third-party libraries such as SciPy. The goal is to compute the PDF values for each data point in the given dataset using the provided mean vector and covariance matrix.\n\n### Approach\nThe multivariate Gaussian PDF is defined by the formula:\n\n\\[ p(x) = \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\Sigma)}} \\exp\\left(-\\tfrac{1}{2} (x-\\mu)^\\top \\Sigma^{-1} (x-\\mu)\\right) \\]\n\nwhere:\n- \\( d \\) is the dimensionality of the data.\n- \\( \\mu \\) is the mean vector.\n- \\( \\Sigma \\) is the covariance matrix.\n- \\( \\Sigma^{-1} \\) is the inverse of the covariance matrix.\n- \\( \\det(\\Sigma) \\) is the determinant of the covariance matrix.\n\nThe steps to compute the PDF for each data point are as follows:\n1. **Reshape the Input Data**: If the input data \\( X \\) is a one-dimensional array, reshape it to a two-dimensional array to handle it uniformly.\n2. **Compute Inverse and Determinant**: Calculate the inverse and determinant of the covariance matrix. These are used in the normalization factor of the PDF.\n3. **Normalization Factor**: Compute the normalization factor using the determinant and the dimensionality \\( d \\).\n4. **Exponent Calculation**: For each data point, compute the exponent term involving the inverse covariance matrix and the difference between the data point and the mean vector.\n5. **PDF Calculation**: Combine the normalization factor and the exponential term to get the PDF value for each data point, rounding the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"Compute the multivariate Gaussian PDF for each sample in X.\"\"\"\n    # Reshape X if it's 1D\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    n_samples, d = X.shape\n    # Compute inverse and determinant of covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n    # Compute normalization factor\n    normalization = 1.0 / np.sqrt((2 * np.pi) ** d * det_cov)\n    # For each sample, compute the PDF\n    pdf_values = []\n    for x in X:\n        x_minus_mu = x - mean\n        exponent = -0.5 * (x_minus_mu.dot(inv_cov).dot(x_minus_mu))\n        pdf = normalization * np.exp(exponent)\n        pdf_values.append(round(pdf, 4))\n    return pdf_values\n```\n\n### Explanation\n1. **Reshape Input Data**: The input data \\( X \\) is checked to ensure it is a two-dimensional array. If it is one-dimensional, it is reshaped to a two-dimensional array with a single feature column.\n2. **Inverse and Determinant**: The inverse of the covariance matrix and its determinant are computed using NumPy's `np.linalg.inv` and `np.linalg.det` functions, respectively.\n3. **Normalization Factor**: The normalization factor is calculated using the formula \\( \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\Sigma)}} \\), where \\( d \\) is the number of features.\n4. **Exponent Calculation**: For each data point, the difference from the mean vector is computed. This difference is then used to compute the quadratic form involving the inverse covariance matrix, resulting in the exponent term.\n5. **PDF Calculation**: The PDF value for each data point is computed by multiplying the normalization factor with the exponential of the exponent term. The result is rounded to four decimal places and stored in a list, which is returned as the final output.\n\nThis approach ensures that the multivariate Gaussian PDF is computed efficiently and correctly for each data point in the given dataset."}
{"task_id": 419, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Bayesian linear regression with a conjugate Gaussian prior and known observation variance. The goal is to compute the Maximum A Posteriori (MAP) predictive mean for new data points.\n\n### Approach\n1. **Add Intercept Column**: If `fit_intercept` is"}
{"task_id": 423, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Root Mean Squared Logarithmic Error (RMSLE) for given ground-truth values and model predictions. RMSLE is a metric that evaluates the performance of regression models by penalizing the ratio between predicted and actual values rather than their absolute difference.\n\n### Approach\n1. **Input Validation**: \n   - Ensure both input lists are of the same length.\n   - Check that all elements in both lists are non-negative.\n   - If any of these conditions are not met, return -1.0.\n\n2. **Compute Logarithms**:\n   - Convert the actual and predicted values into logarithms of (1 + value) to handle non-negative values and avoid taking the logarithm of zero.\n\n3. **Calculate Differences**:\n   - Compute the differences between the logarithms of predicted and actual values.\n\n4. **Square and Sum**:\n   - Square each difference and sum these squared differences.\n\n5. **Compute Mean and Square Root**:\n   - Calculate the mean of the squared differences and then take the square root to get the RMSLE.\n\n6. **Return Result**:\n   - Return the RMSLE rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Stub for the RMSLE metric.  Complete the body of the function.\"\"\"\n    # Check if the lengths are equal\n    if len(actual) != len(predicted):\n        return -1.0\n    # Check for non-negative values\n    if any(x < 0 for x in actual) or any(x < 0 for x in predicted):\n        return -1.0\n    # Check if the lists are empty\n    n = len(actual)\n    if n == 0:\n        return -1.0\n    \n    # Compute the logarithms\n    log_actual = np.log(1 + np.array(actual))\n    log_predicted = np.log(1 + np.array(predicted))\n    \n    # Calculate the differences and their squares\n    diff = log_predicted - log_actual\n    squared_diff = diff ** 2\n    \n    # Compute the mean of squared differences\n    mean = np.mean(squared_diff)\n    \n    # Compute the square root to get RMSLE\n    rmsle = np.sqrt(mean)\n    \n    # Round to four decimal places\n    return round(rmsle, 4)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input lists are valid. If they are not of the same length, contain negative values, or are empty, it returns -1.0.\n- **Logarithmic Transformation**: Using numpy, the function computes the logarithm of (1 + value) for each element in both lists to handle non-negative values and avoid taking the logarithm of zero.\n- **Difference Calculation**: The differences between the logarithms of predicted and actual values are computed.\n- **Sum of Squares**: The squared differences are summed up.\n- **Mean and Square Root**: The mean of the squared differences is calculated, and the square root of this mean gives the RMSLE.\n- **Rounding**: The result is rounded to four decimal places before being returned.\n\nThis approach ensures that the function efficiently computes the RMSLE while handling edge cases and invalid inputs appropriately."}
{"task_id": 428, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a Hann window, which is a type of window function used in signal processing to reduce spectral leakage. The Hann window can be either symmetric or periodic, depending on the application. The function should handle special cases and return the window coefficients rounded to four decimal places.\n\n### Approach\n1. **Special Cases Handling**:\n   - If the window length (`window_len`) is less than or equal to 0, return an empty list.\n   - If the window length is 1, return a list containing a single element, 1.0, regardless of the symmetric flag.\n\n2. **Determine Window Length**:\n   - For a symmetric window, the length remains `window_len`.\n   - For a periodic window, compute the window for `window_len + 1` and then take the first `window_len` elements to ensure continuity between successive windows.\n\n3. **Compute the Hann Window**:\n   - Use the formula for the Hann window: `hann(n) = 0.5 - 0.5 * cos(2 * \u03c0 * n / (N - 1))`, where `N` is the determined window length.\n   - Generate the window coefficients using numpy for efficient computation.\n\n4. **Rounding and Conversion**:\n   - Round each coefficient to four decimal places.\n   - Convert the numpy array to a list of floats before returning.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    elif window_len == 1:\n        return [1.0]\n    else:\n        if symmetric:\n            N = window_len\n        else:\n            N = window_len + 1\n        n = np.arange(N)\n        window = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n        if not symmetric:\n            window = window[:window_len]\n        window = np.round(window, 4)\n        return window.tolist()\n```\n\n### Explanation\n- **Special Cases Handling**: The function immediately handles edge cases where the window length is non-positive or exactly 1, ensuring correct output without further computation.\n- **Window Length Adjustment**: Depending on whether the window is symmetric or periodic, the function adjusts the length of the window to be computed. This ensures that the periodic window is correctly formed by extending the symmetric window and truncating it.\n- **Hann Window Computation**: Using numpy, the function efficiently computes the window coefficients using vectorized operations, which is both fast and concise.\n- **Rounding and Conversion**: The coefficients are rounded to four decimal places to meet the precision requirement, and the result is converted to a list for the final output.\n\nThis approach ensures that the function is both efficient and correct, handling all specified cases and producing the required output format."}
{"task_id": 433, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the fan-in and fan-out values for a given weight tensor shape in a neural network. These values are crucial for initialization schemes like Xavier/Glorot. The approach involves checking the dimensionality of the tensor and applying specific rules based on whether it is a convolutional kernel or a general tensor.\n\n### Approach\n1. **Identify Tensor Type**: Check if the tensor has exactly 4 dimensions. If it does, it is treated as a 2D convolutional kernel. Otherwise, it is treated as a general tensor.\n2. **Convolutional Kernel Handling**: For a 4D tensor, the shape is assumed to be `(out_channels, in_channels, kernel_height, kernel_width)`. The fan-in is calculated as the product of `in_channels` and the receptive field size (product of kernel dimensions). Similarly, fan-out is the product of `out_channels` and the receptive field size.\n3. **General Tensor Handling**: For tensors with any other number of dimensions, the first two dimensions are directly taken as fan-in and fan-out.\n4. **Return Values**: The computed fan-in and fan-out values are returned as floats.\n\n### Solution Code\n```python\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\"\"\"\n    if len(shape) == 4:\n        out_ch, in_ch, k_h, k_w = shape\n        receptive_field = k_h * k_w\n        fan_in = in_ch * receptive_field\n        fan_out = out_ch * receptive_field\n    else:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    return (float(fan_in), float(fan_out))\n```\n\n### Explanation\n- **Convolutional Kernel Case**: When the tensor has 4 dimensions, the function extracts the number of input and output channels, as well as the kernel dimensions. The receptive field size is computed, and fan-in and fan-out are derived from this.\n- **General Tensor Case**: For tensors with fewer or more than 4 dimensions, the first two dimensions are used directly to determine fan-in and fan-out.\n- **Return Values**: The results are converted to floats to meet the problem's requirements.\n\nThis approach ensures that the function correctly handles both convolutional kernels and general tensors, providing accurate fan-in and fan-out values for various neural network initialization schemes."}
{"task_id": 435, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor using ordinary least squares (OLS) linear regression as the weak learner and squared error as the loss function. The goal is to iteratively improve predictions by fitting models to the residuals of the current predictions.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input features `X` and target `y` into NumPy arrays for efficient numerical operations.\n2. **Handle Special Cases**: If the number of estimators is zero or the learning rate is zero, return a vector filled with the mean of `y`.\n3. **Initialize Predictions**: Start with the mean of `y` as the initial prediction for all samples.\n4. **Iterative Residual Fitting**: For each estimator, compute the residuals (differences between actual and predicted values), fit an OLS model to these residuals, and update the predictions using the learned model.\n5. **Update Predictions**: Adjust the predictions using the learning rate and the predictions from the weak learner.\n6. **Return Results**: After all estimators are trained, round the final predictions to four decimal places and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to the nearest 4th decimal and packed into a plain\n    Python list.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    y_np = np.array(y, dtype=float)\n    m = len(y_np)\n    \n    if n_estimators <= 0 or learning_rate == 0:\n        mean_y = np.mean(y_np)\n        return [round(mean_y, 4)] * m\n    \n    y_hat = np.full(m, np.mean(y_np))\n    \n    for _ in range(n_estimators):\n        r = y_np - y_hat\n        X_with_intercept = np.hstack([np.ones((m, 1)), X_np])\n        XTX = X_with_intercept.T.dot(X_with_intercept)\n        XTy = X_with_intercept.T.dot(r)\n        beta = np.linalg.solve(XTX, XTy)\n        h_t = X_with_intercept.dot(beta)\n        y_hat += learning_rate * h_t\n    \n    y_hat_rounded = np.round(y_hat, 4)\n    return y_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to NumPy Arrays**: The input lists `X` and `y` are converted to NumPy arrays to leverage efficient matrix operations.\n2. **Special Cases Handling**: If no estimators are requested or the learning rate is zero, the function immediately returns the mean of `y` for all samples.\n3. **Initial Predictions**: The initial prediction vector is set to the mean of `y`, which serves as the starting point for the iterative process.\n4. **Residual Calculation**: For each estimator, the residuals are computed as the difference between the actual target values and the current predictions.\n5. **OLS Model Fitting**: An OLS model is fit to the residuals using the features `X` augmented with an intercept term. This model provides the weak learner's predictions.\n6. **Prediction Update**: The predictions are updated by adding the product of the learning rate and the weak learner's predictions to the current predictions.\n7. **Result Formatting**: The final predictions are rounded to four decimal places and returned as a list.\n\nThis approach ensures that each weak learner corrects the residuals of the previous model, iteratively improving the overall prediction accuracy."}
{"task_id": 437, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the logistic sigmoid function and its first and second derivatives for scalar, list, or NumPy array inputs. The function should return the result in the appropriate format, rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: The function should handle scalar values, lists, and NumPy arrays. Using `np.isscalar(x)` ensures that both Python and NumPy scalars are correctly identified.\n2. **Sigmoid Calculation**: The logistic sigmoid function is computed as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This is done for each element of the input.\n3. **Derivatives Calculation**: Based on the `order` parameter:\n   - `order = 0`: Return the sigmoid function value.\n   - `order = 1`: Return the first derivative, \\( \\sigma(x)(1 - \\sigma(x)) \\).\n   - `order = 2`: Return the second derivative, \\( \\sigma(x)(1 - \\sigma(x))(1 - 2\\sigma(x)) \\).\n4. **Rounding**: The results are rounded to four decimal places using NumPy's `round` function for consistency across both scalar and array inputs.\n5. **Return Format**: The result is returned as a float for scalar inputs and as a list for array inputs, preserving the original shape.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if order not in [0, 1, 2]:\n        return -1\n\n    if np.isscalar(x):\n        sigma = 1.0 / (1.0 + np.exp(-x))\n        is_scalar = True\n    else:\n        x_np = np.array(x)\n        sigma = 1.0 / (1.0 + np.exp(-x_np))\n        is_scalar = False\n\n    if order == 0:\n        result = sigma\n    elif order == 1:\n        result = sigma * (1 - sigma)\n    else:\n        result = sigma * (1 - sigma) * (1 - 2 * sigma)\n\n    if is_scalar:\n        rounded = np.round(result, decimals=4).item()\n    else:\n        rounded = np.round(result, decimals=4)\n\n    if is_scalar:\n        return rounded\n    else:\n        return rounded.tolist()\n```\n\n### Explanation\n- **Input Handling**: The function first checks if the input `x` is a scalar using `np.isscalar(x)`. If `x` is a scalar, it computes the sigmoid directly. If `x` is a list or NumPy array, it converts `x` to a NumPy array for efficient element-wise operations.\n- **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n- **Derivatives Calculation**: Depending on the `order` parameter, the function computes the first or second derivative using the precomputed sigmoid values.\n- **Rounding**: The results are rounded to four decimal places using NumPy's `round` function to ensure consistent rounding behavior.\n- **Return Format**: The function returns a float for scalar inputs and a list for array inputs, preserving the original structure of the input data.\n\nThis approach ensures that the function is efficient, handles various input types correctly, and returns results in the required format."}
{"task_id": 438, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a 1-D list or NumPy array of class labels into a one-hot encoded matrix. This is a common task in machine learning where categorical data needs to be converted into a format that can be used by various algorithms.\n\n### Approach\nThe function `to_categorical` takes two parameters: a 1-D list or NumPy array `y` containing non-negative integer class indices, and an optional integer `num_classes`. The function follows these steps:\n\n1. **Check for Empty Input**: If the input list or array `y` is empty, the function immediately returns an empty list.\n2. **Determine Number of Classes**: If `num_classes` is not provided (i.e., it is `None`), the function calculates it as the maximum value in `y` plus one. This ensures that all possible class indices are accounted for.\n3. **Validate Number of Classes**: If `num_classes` is provided, the function checks if it is smaller than the required number of classes (which is the maximum value in `y` plus one). If it is, the function returns `-1` to indicate an error.\n4. **Create One-Hot Encoded Matrix**: If the number of classes is valid, the function creates a 2-D NumPy array of zeros with dimensions `(len(y), num_classes)`. Each row in this array is then updated to have a `1` at the column corresponding to the class index in `y`.\n5. **Convert to List of Lists**: The resulting NumPy array is converted to a list of lists before being returned.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    if len(y) == 0:\n        return []\n    \n    max_y = max(y)\n    \n    if num_classes is None:\n        num_classes = max_y + 1\n    else:\n        if num_classes < max_y + 1:\n            return -1\n    \n    result = np.zeros((len(y), num_classes), dtype=int)\n    for i in range(len(y)):\n        result[i, y[i]] = 1\n    \n    return result.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input `y` is empty and returns an empty list if true.\n- **Determine Number of Classes**: If `num_classes` is not provided, it is calculated as the maximum value in `y` plus one to ensure all class indices are covered.\n- **Validation Check**: If `num_classes` is provided and is smaller than the required number of classes, the function returns `-1` to indicate an error.\n- **Matrix Creation**: A NumPy array of zeros is created, and each row is updated to have a `1` at the position corresponding to the class index in `y`.\n- **Conversion to List**: The resulting NumPy array is converted to a list of lists to meet the function's return type requirements.\n\nThis approach ensures that the function efficiently handles both list and NumPy array inputs, correctly computes the one-hot encoding, and returns the appropriate result or error indicator."}
{"task_id": 440, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to aggregate the probability predictions from multiple estimators in an ensemble learner, such as a Random Forest classifier. The goal is to compute the average probability distribution for each sample across all estimators, round these probabilities to four decimal places, and determine the predicted class label for each sample based on the highest probability.\n\n### Approach\n1. **Convert Input to NumPy Array**: The input is a three-level nested list. We convert this into a NumPy array for efficient computation.\n2. **Compute Averaged Probabilities**: For each sample, compute the mean probability across all estimators for each class.\n3. **Round Probabilities**: Round each averaged probability to four decimal places.\n4. **Determine Predicted Class Labels**: For each sample, find the class with the highest probability. In case of ties, the smallest index is chosen.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the input list to a NumPy array for efficient computation\n    arr = np.array(predictions)\n    \n    # Compute the mean across all estimators for each sample and class\n    averaged = np.mean(arr, axis=0)\n    \n    # Round the averaged probabilities to four decimal places\n    rounded = np.round(averaged, 4)\n    \n    # Determine the predicted class for each sample\n    class_indices = np.argmax(rounded, axis=1)\n    \n    # Convert the NumPy arrays back to Python lists\n    averaged_list = rounded.tolist()\n    class_indices_list = class_indices.tolist()\n    \n    return (averaged_list, class_indices_list)\n```\n\n### Explanation\n1. **Conversion to NumPy Array**: The input list is converted into a NumPy array to leverage efficient array operations.\n2. **Averaging Probabilities**: Using NumPy's `mean` function, we compute the average probability for each class across all estimators for each sample.\n3. **Rounding**: The averaged probabilities are rounded to four decimal places using NumPy's `round` function.\n4. **Class Prediction**: The `argmax` function is used to find the class with the highest probability for each sample. In case of ties, the smallest index is selected, which is the default behavior of `argmax`.\n\nThis approach ensures that we efficiently compute the required aggregated probabilities and class predictions while adhering to the constraints and requirements of the problem."}
{"task_id": 444, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Radial Basis Function (RBF) kernel matrix computation. The RBF kernel is a popular kernel function used in machine learning methods such as Gaussian Processes and Support Vector Machines. The task involves validating inputs, computing the kernel matrix, and handling edge cases appropriately.\n\n### Approach\n1. **Input Validation**: \n   - Check if the input matrices X and Y (if provided) have consistent feature dimensions.\n   - Validate the sigma parameter to ensure it is either a positive float, a list of positive floats matching the number of features, or None (which defaults to a computed value).\n\n2. **Sigma Handling**:\n   - If sigma is None, compute it as the square root of (number of features / 2).\n   - If sigma is a float, ensure it is positive and use it for all features.\n   - If sigma is a list, ensure it has the same length as the number of features and all elements are positive.\n\n3. **Kernel Matrix Computation**:\n   - For each pair of vectors from X and Y (or X if Y is None), compute the RBF kernel value using the formula: \n     \\[\n     k(x, y) = \\exp\\left(-0.5 \\times \\sum_j \\left(\\frac{x_j - y_j}{\\sigma_j}\\right)^2\\right)\n     \\]\n   - Round each kernel value to four decimal places.\n\n4. **Edge Cases**:\n   - Handle cases where X or Y is empty, ensuring the function returns appropriate results or error codes as specified.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Check if X is empty\n    if not X:\n        if Y is not None:\n            return -1\n        else:\n            return []\n    \n    # Get the number of features\n    C = len(X[0])\n    for vec in X:\n        if len(vec) != C:\n            return -1\n    \n    # Check Y if provided\n    if Y is not None:\n        if not Y:\n            return -1\n        c_y = len(Y[0])\n        if c_y != C:\n            return -1\n        for vec in Y:\n            if len(vec) != c_y:\n                return -1\n    \n    # Handle sigma\n    if sigma is None:\n        sigma_list = [np.sqrt(C / 2)] * C\n    elif isinstance(sigma, (int, float)):\n        if sigma <= 0:\n            return -1\n        sigma_list = [float(sigma)] * C\n    elif isinstance(sigma, list):\n        if len(sigma) != C:\n            return -1\n        for s in sigma:\n            if s <= 0:\n                return -1\n        sigma_list = sigma\n    else:\n        return -1\n    \n    # Determine Y_or_X\n    if Y is None:\n        Y_or_X = X\n    else:\n        Y_or_X = Y\n    \n    # Compute kernel matrix\n    kernel = []\n    for x in X:\n        row = []\n        for y in Y_or_X:\n            sum_sq = 0.0\n            for j in range(C):\n                diff = x[j] - y[j]\n                scaled_diff = diff / sigma_list[j]\n                sum_sq += scaled_diff ** 2\n            exponent = -0.5 * sum_sq\n            kernel_val = np.exp(exponent)\n            rounded_val = round(kernel_val, 4)\n            row.append(rounded_val)\n        kernel.append(row)\n    \n    return kernel\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the input matrices X and Y have consistent dimensions and valid sigma values. If any validation fails, it returns -1.\n- **Sigma Handling**: Depending on the type of sigma (None, float, list), it computes or validates the sigma values to be used for each feature.\n- **Kernel Matrix Computation**: For each pair of vectors from X and Y, it computes the RBF kernel value using the given formula, rounds it to four decimal places, and constructs the kernel matrix.\n- **Edge Cases**: The function handles cases where X or Y is empty, ensuring appropriate results or error codes are returned.\n\nThis approach ensures that the function is robust, handles various edge cases, and efficiently computes the RBF kernel matrix as required."}
{"task_id": 446, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function `decision_stump` that finds the best single-level split (decision stump) for a given feature matrix `X` and label vector `y` based on information gain. The goal is to maximize the information gain, which is calculated as the decrease in entropy after splitting the data.\n\n### Approach\n1. **Compute Initial Entropy**: Calculate the entropy of the parent node (the entire dataset) to determine the baseline entropy before any splits.\n2. **Check for Uniform Labels**: If all labels are the same, return the no-split case with the majority label.\n3. **Iterate Over Features**: For each feature, extract the column values and determine possible thresholds by considering midpoints between successive distinct values.\n4. **Evaluate Splits**: For each threshold, split the data into left and right subsets, compute their entropies, and determine the information gain.\n5. **Track Best Split**: Keep track of the split with the highest information gain, resolving ties by selecting the smallest feature index and threshold.\n6. **Return Result**: If no split improves the information gain, return the no-split case. Otherwise, return the best split details.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    def compute_entropy(labels):\n        if not labels:\n            return 0.0\n        counts = {}\n        for label in labels:\n            counts[label] = counts.get(label, 0) + 1\n        total = len(labels)\n        entropy = 0.0\n        for count in counts.values():\n            p = count / total\n            entropy -= p * np.log2(p)\n        return entropy\n\n    def get_majority(labels):\n        if not labels:\n            return None\n        counts = {}\n        for label in labels:\n            counts[label] = counts.get(label, 0) + 1\n        max_count = max(counts.values())\n        candidates = [k for k, v in counts.items() if v == max_count]\n        return min(candidates)\n\n    H_parent = compute_entropy(y)\n    if H_parent == 0:\n        majority_label = get_majority(y)\n        return (-1, None, majority_label, majority_label)\n\n    n_samples = len(y)\n    n_features = len(X[0]) if n_samples > 0 else 0\n\n    current_max_gain = 0.0\n    best_j = -1\n    best_t = None\n    left_majority = None\n    right_majority = None\n\n    for j in range(n_features):\n        column = [row[j] for row in X]\n        sorted_unique = sorted(list(set(column)))\n        if len(sorted_unique) < 2:\n            continue\n\n        for i in range(len(sorted_unique) - 1):\n            t = (sorted_unique[i] + sorted_unique[i + 1]) / 2\n            left_indices = [i for i in range(n_samples) if X[i][j] <= t]\n            right_indices = [i for i in range(n_samples) if X[i][j] > t]\n\n            left_labels = [y[i] for i in left_indices]\n            right_labels = [y[i] for i in right_indices]\n\n            H_left = compute_entropy(left_labels)\n            H_right = compute_entropy(right_labels)\n\n            p_left = len(left_labels) / n_samples\n            p_right = len(right_labels) / n_samples\n\n            gain = H_parent - (p_left * H_left + p_right * H_right)\n\n            if gain > current_max_gain:\n                current_max_gain = gain\n                best_j = j\n                best_t = t\n                left_majority = get_majority(left_labels)\n                right_majority = get_majority(right_labels)\n            elif gain == current_max_gain:\n                if best_j == -1:\n                    best_j = j\n                    best_t = t\n                    left_majority = get_majority(left_labels)\n                    right_majority = get_majority(right_labels)\n                else:\n                    if j < best_j:\n                        best_j = j\n                        best_t = t\n                        left_majority = get_majority(left_labels)\n                        right_majority = get_majority(right_labels)\n                    elif j == best_j:\n                        if t < best_t:\n                            best_t = t\n                            left_majority = get_majority(left_labels)\n                            right_majority = get_majority(right_labels)\n\n    if current_max_gain <= 0:\n        majority_label = get_majority(y)\n        return (-1, None, majority_label, majority_label)\n    else:\n        best_t_rounded = round(best_t, 4)\n        return (best_j, best_t_rounded, left_majority, right_majority)\n```\n\n### Explanation\n1. **Entropy Calculation**: The `compute_entropy` function calculates the Shannon entropy of a given list of labels.\n2. **Majority Label**: The `get_majority` function determines the majority label in a list, resolving ties by selecting the smallest label.\n3. **Initial Entropy Check**: If the initial entropy of the dataset is zero, it means all labels are the same, and no split is needed.\n4. **Feature and Threshold Iteration**: For each feature, possible thresholds are determined by midpoints between successive distinct values. Each threshold is evaluated to split the data and compute the resulting information gain.\n5. **Best Split Selection**: The split with the highest information gain is tracked, with ties resolved by selecting the smallest feature index and threshold.\n6. **Result Handling**: If no split improves the information gain, the function returns the no-split case. Otherwise, it returns the best split details, including the feature index, threshold, and majority labels for the left and right subsets."}
{"task_id": 452, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to split a given dataset into two subsets based on a specified feature and threshold. The dataset can be provided as either a list of lists or a NumPy 2-D array. The split is determined by whether the threshold is numeric or categorical.\n\n### Approach\n1. **Determine the Type of Threshold**: Check if the threshold is numeric (int or float) or categorical (any other type). This is done using the `numbers.Real` type check to include both Python and NumPy numeric types, while excluding boolean values.\n2. **Convert Dataset to List of Lists**: If the dataset is a NumPy array, convert it to a list of lists to handle both types uniformly.\n3. **Iterate Through Samples**: For each sample in the dataset, extract the value at the specified feature index.\n4. **Split Based on Threshold**:\n   - For numeric thresholds, split the dataset such that the first subset contains samples where the feature value is greater than or equal to the threshold.\n   - For categorical thresholds, split the dataset such that the first subset contains samples where the feature value is exactly equal to the threshold.\n5. **Maintain Order**: Ensure that both subsets maintain the original order of samples from the input dataset.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List\nimport numbers\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\"\"\"\n    # Convert X to a list of lists if it's a NumPy array\n    if isinstance(X, np.ndarray):\n        X_list = X.tolist()\n    else:\n        X_list = X\n\n    subset1 = []\n    subset2 = []\n\n    for sample in X_list:\n        value = sample[feature_i]\n\n        # Check if the threshold is numeric (int or float, excluding bool)\n        if isinstance(threshold, numbers.Real) and not isinstance(threshold, bool):\n            if value >= threshold:\n                subset1.append(sample)\n            else:\n                subset2.append(sample)\n        else:\n            # Categorical split\n            if value == threshold:\n                subset1.append(sample)\n            else:\n                subset2.append(sample)\n\n    return [subset1, subset2]\n```\n\n### Explanation\n- **Conversion to List of Lists**: The function first checks if the input dataset `X` is a NumPy array. If it is, it converts `X` to a list of lists to handle both list and array inputs uniformly.\n- **Threshold Check**: The function uses `numbers.Real` to determine if the threshold is numeric, ensuring it includes both Python and NumPy numeric types while excluding booleans.\n- **Splitting Logic**: For each sample, the function checks the value at the specified feature index. Depending on whether the threshold is numeric or categorical, it appends the sample to the appropriate subset.\n- **Order Preservation**: The function processes each sample in the order they appear in the input dataset, ensuring that the subsets maintain the original order.\n\nThis approach efficiently handles both numeric and categorical thresholds, ensuring the correct partitioning of the dataset while preserving the order of samples."}
{"task_id": 453, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the target Q-values for a batch of states in a Deep Q-Network (DQN). The target Q-values are used to update the neural network's weights during training. The target for each state depends on whether the next state is terminal or not, and it is calculated using the current Q-values and the next state's Q-values.\n\n### Approach\n1. **Input Validation**: Check if the input parameters are valid. Specifically, ensure that the gamma value is within the range [0, 1], and that all input lists (actions, rewards, dones) have the same length as the batch size. Also, ensure that the shapes of Q and Q_next are consistent.\n2. **Target Calculation**: For each sample in the batch, compute the target Q-value. If the next state is terminal (dones[i] is True), the target is just the reward. Otherwise, the target is the reward plus gamma multiplied by the maximum Q-value of the next state.\n3. **Update Q-values**: Create a copy of the original Q-values matrix. For each sample, update the Q-value corresponding to the action taken with the computed target value.\n4. **Rounding and Conversion**: Round each element of the updated Q-values matrix to four decimal places and convert it to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\"\"\"\n    # Check if gamma is within the valid range\n    if not (0 <= gamma <= 1):\n        return -1\n    \n    # Check if all input lists have the same length as the batch size\n    batch_size = Q.shape[0]\n    if len(actions) != batch_size or len(rewards) != batch_size or len(dones) != batch_size:\n        return -1\n    \n    # Check if Q and Q_next have the same shape\n    if Q.shape != Q_next.shape:\n        return -1\n    \n    # Create a copy of Q to modify\n    y = Q.copy()\n    \n    # Iterate over each sample in the batch\n    for i in range(batch_size):\n        a = actions[i]\n        r = rewards[i]\n        done = dones[i]\n        \n        if done:\n            target = r\n        else:\n            target = r + gamma * np.max(Q_next[i])\n        \n        y[i, a] = target\n    \n    # Round each element to four decimal places\n    y_rounded = np.round(y, 4)\n    \n    # Convert to a list of lists and return\n    return y_rounded.tolist()\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if gamma is within the valid range [0, 1]. It then verifies that the lengths of the input lists (actions, rewards, dones) match the batch size derived from the Q-values matrix. Additionally, it ensures that Q and Q_next have the same shape.\n2. **Target Calculation**: For each sample, the target Q-value is computed based on whether the next state is terminal. If the next state is terminal, the target is simply the reward. Otherwise, it is the reward plus gamma times the maximum Q-value of the next state.\n3. **Update Q-values**: A copy of the original Q-values matrix is made. For each sample, the Q-value corresponding to the action taken is updated with the computed target value.\n4. **Rounding and Conversion**: The updated Q-values matrix is rounded to four decimal places and converted to a list of lists to match the required output format.\n\nThis approach ensures that the target Q-values are correctly computed and formatted for use in training the DQN."}
{"task_id": 458, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional array of non-negative integer labels into a two-dimensional one-hot encoded matrix. This is a common requirement in machine learning where neural networks often expect class labels in a one-hot encoded format.\n\n### Approach\nThe approach to solve this problem involves several steps:\n\n1. **Check for Empty Labels**: If the input labels array is empty, return an empty list immediately.\n2. **Check for Negative Labels**: Ensure all labels are non-negative. If any label is negative, return -1.\n3. **Determine Number of Classes**: If the number of classes (`num_classes`) is not provided, infer it as the maximum label value plus one. If `num_classes` is provided, check if it is sufficient to cover all labels.\n4. **Validate Labels Against Classes**: Ensure that all labels are within the valid range (0 to `num_classes - 1`). If any label exceeds this range, return -1.\n5. **Generate One-Hot Encoded Matrix**: For each label, create a row of zeros with a single 1 at the index corresponding to the label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    if not labels:\n        return []\n    \n    # Check for negative labels\n    for label in labels:\n        if label < 0:\n            return -1\n    \n    max_label = max(labels)\n    required_num_classes = max_label + 1\n    \n    if num_classes is None:\n        num_classes = required_num_classes\n    else:\n        if num_classes < required_num_classes:\n            return -1\n    \n    # Check if any label is >= num_classes\n    for label in labels:\n        if label >= num_classes:\n            return -1\n    \n    # Create the one-hot encoded matrix\n    result = []\n    for label in labels:\n        row = [0] * num_classes\n        row[label] = 1\n        result.append(row)\n    \n    return result\n```\n\n### Explanation\n1. **Empty Labels Check**: The function immediately returns an empty list if the input labels array is empty.\n2. **Negative Labels Check**: Each label is checked to ensure it is non-negative. If any label is negative, the function returns -1.\n3. **Determine Number of Classes**: If `num_classes` is not provided, it is inferred as the maximum label value plus one. If `num_classes` is provided, it is checked against the inferred value to ensure it is sufficient.\n4. **Validate Labels**: Each label is checked to ensure it is within the valid range (0 to `num_classes - 1`). If any label is out of range, the function returns -1.\n5. **Generate One-Hot Matrix**: For each label, a row of zeros is created with a 1 at the index corresponding to the label. This row is added to the result list, which is returned as the final one-hot encoded matrix.\n\nThis approach ensures that the function handles various edge cases, such as empty input, negative labels, and insufficient number of classes, while efficiently generating the required one-hot encoded matrix."}
{"task_id": 461, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting regressor for one-dimensional data using decision stumps. The goal is to fit an ensemble of decision stumps to the training data and return the final predictions.\n\n### Approach\n1. **Initialization**: Start by initializing the predictions as the mean of the target values. This serves as the base prediction before any boosting iterations.\n2. **Iterative Boosting**: For each estimator (up to `n_estimators`), compute the residuals (differences between actual targets and current predictions). If all residuals are zero, stop early as no further improvement is possible.\n3. **Splitting**: For each possible split value `t` (from unique values of the feature `x` except the greatest one), split the data into left and right subsets. Compute the sum of squared errors (SSE) for each split and select the split with the smallest SSE.\n4. **Update Predictions**: Using the best split, compute the mean residuals for the left and right subsets. Update the predictions by adding the contribution of the current decision stump, scaled by the learning rate.\n5. **Edge Cases**: Handle cases where no valid split is possible by using the mean residual for the entire dataset.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\"\"\"\n    if n_estimators <= 0:\n        mean_y = np.mean(y)\n        return [round(mean_y, 4)] * len(y)\n    \n    x = np.array(x)\n    y = np.array(y)\n    n = len(x)\n    predictions = np.full(n, np.mean(y))\n    \n    for _ in range(n_estimators):\n        residuals = y - predictions\n        if np.allclose(residuals, 0):\n            break\n        \n        unique_x = np.unique(x)\n        if len(unique_x) == 1:\n            delta = np.full(n, np.mean(residuals))\n        else:\n            min_sse = float('inf')\n            best_t = None\n            best_vL = None\n            best_vR = None\n            for t in unique_x[:-1]:\n                left_mask = x <= t\n                right_mask = x > t\n                if not (left_mask.any() and right_mask.any()):\n                    continue\n                mean_L = np.mean(residuals[left_mask])\n                mean_R = np.mean(residuals[right_mask])\n                sse = np.sum((residuals[left_mask] - mean_L)**2) + np.sum((residuals[right_mask] - mean_R)**2)\n                if sse < min_sse:\n                    min_sse = sse\n                    best_t = t\n                    best_vL = mean_L\n                    best_vR = mean_R\n            if best_t is None:\n                delta = np.full(n, np.mean(residuals))\n            else:\n                delta = np.zeros(n)\n                delta[x <= best_t] = best_vL\n                delta[x > best_t] = best_vR\n        \n        predictions += learning_rate * delta\n    \n    return np.round(predictions, 4).tolist()\n```\n\n### Explanation\n1. **Initialization**: The initial predictions are set to the mean of the target values, providing a baseline prediction.\n2. **Residual Calculation**: For each iteration, residuals are computed as the difference between actual targets and current predictions.\n3. **Splitting and SSE Calculation**: For each possible split point, the data is split into left and right subsets. The SSE for each split is calculated, and the split with the smallest SSE is chosen.\n4. **Prediction Update**: The predictions are updated using the contribution of the best split, scaled by the learning rate. This helps in reducing overfitting by controlling the step size.\n5. **Edge Cases Handling**: If no valid split is possible (e.g., all feature values are the same), the mean residual is used for the entire dataset.\n\nThis approach ensures that the model iteratively improves its predictions by focusing on the residuals, effectively learning from its mistakes in a gradient descent manner."}
{"task_id": 471, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement one step of the on-policy TD(0) Expected\u2013SARSA algorithm for a tabular setting. The goal is to update the Q-table based on the given transition and hyper-parameters, and return the updated Q-table with all entries rounded to four decimal places.\n\n### Approach\n1. **Calculate the Target Value**: The target value is computed based on the immediate reward and the expected future reward from the next state. If the next state is terminal (i.e., `None`), the target is just the immediate reward. Otherwise, we compute the expected future reward using the \u03b5-soft policy derived from the current Q-table.\n2. **Update the Q-Table**: Using the target value, update the Q-value for the current state-action pair using the learning rate.\n3. **Round the Q-Table Entries**: After updating the Q-table, round every entry to four decimal places to ensure precision.\n\n### Solution Code\n```python\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\"\"\"\n    # Compute the target\n    if next_state is None:\n        target = reward\n    else:\n        q_next = q_table[next_state]\n        max_q = max(q_next)\n        a_g = q_next.index(max_q)\n        num_actions = len(q_next)\n        expectation = 0.0\n        for a in range(num_actions):\n            if a == a_g:\n                prob = (1 - epsilon) + (epsilon / num_actions)\n            else:\n                prob = epsilon / num_actions\n            expectation += q_next[a] * prob\n        target = reward + gamma * expectation\n    \n    # Update the Q-value\n    current_q = q_table[state][action]\n    new_q = current_q + lr * (target - current_q)\n    q_table[state][action] = new_q\n    \n    # Round all entries to four decimal places\n    rounded_q = []\n    for row in q_table:\n        rounded_row = [round(x, 4) for x in row]\n        rounded_q.append(rounded_row)\n    \n    return rounded_q\n```\n\n### Explanation\n1. **Target Calculation**: The target value is computed differently based on whether the next state is terminal or not. If the next state is terminal, the target is just the immediate reward. Otherwise, we compute the expected future reward by considering all possible actions from the next state, weighted by their probabilities under the \u03b5-soft policy.\n2. **Q-Table Update**: The Q-value for the current state-action pair is updated using the learning rate and the difference between the target value and the current Q-value.\n3. **Rounding**: After updating the Q-table, each entry is rounded to four decimal places to ensure precision and meet the problem's requirements.\n\nThis approach ensures that the Q-table is updated correctly according to the Expected\u2013SARSA algorithm and returns the updated Q-table with the required precision."}
{"task_id": 474, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a 2-D toy dataset suitable for a binary or multi-class classification problem. The dataset should be perfectly separable, with each class arranged in a rectangular cluster. The function should be reusable, accept various parameters, and handle both training and validation sets with proper labels.\n\n### Approach\n1. **Problem Analysis**: The task is to create a function that generates a 2-D dataset with specified parameters. Each class is arranged in a rectangular cluster, and the dataset should be perfectly separable. The function must handle both training and validation sets, ensuring that each set has the correct labels.\n\n2. **Point Generation Rule**: For each class index `i`, the x and y coordinates are generated from uniform distributions. The x-coordinate is determined by the floor division of `i` by 2, while the y-coordinate is determined by the modulo operation of `i` by 2, scaled by the given interval. The label for each class is calculated as `(i - 0.5) * 2`.\n\n3. **Seed Handling**: The function should optionally take a seed to ensure reproducibility of the generated data.\n\n4. **Rounding**: All coordinates are rounded to four decimal places to meet the specified precision.\n\n5. **Data Structure**: The function returns four NumPy arrays: `X_train`, `X_val`, `Y_train`, and `Y_val`, which represent the training and validation sets along with their respective labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    X_train = []\n    Y_train = []\n    X_val = []\n    Y_val = []\n    \n    for i in range(m):\n        x_floor = i // 2\n        x_low = (x_floor + 0.1) * 10\n        x_high = (x_floor + 0.9) * 10\n        \n        y_mod = i % 2\n        y_low = (y_mod * interval + 0.1) * 10\n        y_high = (y_mod * interval + 0.9) * 10\n        \n        # Generate training samples\n        x = np.random.uniform(x_low, x_high, n_train)\n        y = np.random.uniform(y_low, y_high, n_train)\n        samples = np.column_stack((x, y))\n        X_train.append(samples)\n        Y_train.extend([(i - 0.5) * 2] * n_train)\n        \n        # Generate validation samples\n        x_val = np.random.uniform(x_low, x_high, n_val)\n        y_val = np.random.uniform(y_low, y_high, n_val)\n        samples_val = np.column_stack((x_val, y_val))\n        X_val.append(samples_val)\n        Y_val.extend([(i - 0.5) * 2] * n_val)\n    \n    # Concatenate all samples\n    X_train = np.concatenate(X_train, axis=0)\n    X_val = np.concatenate(X_val, axis=0)\n    Y_train = np.array(Y_train)\n    Y_val = np.array(Y_val)\n    \n    # Round to 4 decimal places\n    X_train = np.round(X_train, 4)\n    X_val = np.round(X_val, 4)\n    \n    return X_train, X_val, Y_train, Y_val\n```\n\n### Explanation\n1. **Seed Handling**: If a seed is provided, it is used to set the random seed for NumPy to ensure reproducibility.\n\n2. **Loop Through Classes**: For each class index `i`, the x and y ranges are computed based on the given formulas. The x range is determined by the floor division of `i` by 2, and the y range is determined by the modulo operation of `i` by 2, scaled by the interval.\n\n3. **Generate Samples**: For each class, training and validation samples are generated using uniform distribution within the computed ranges. These samples are then appended to the respective lists.\n\n4. **Concatenate and Round**: After processing all classes, the samples are concatenated into NumPy arrays. The coordinates are rounded to four decimal places to meet the precision requirement.\n\n5. **Return Values**: The function returns the training and validation sets along with their labels, ensuring the correct shape and precision for each array."}
{"task_id": 475, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the core mathematics of the Adam optimizer, which is a popular optimization algorithm used in machine learning. The Adam optimizer uses two moving averages of the gradients to update the parameters, which helps in converging faster and more efficiently.\n\n### Approach\nThe Adam optimizer maintains two moving averages: the first moment (mean of gradients) and the second moment (uncentered variance of gradients). These moving averages are updated at each time step and then bias-corrected to provide more accurate estimates. The parameters are then updated using these bias-corrected estimates.\n\nThe steps involved in the Adam update are as follows:\n1. **Initialize Moments**: If the previous first moment (`m_prev`) or the previous second moment (`v_prev`) is `None`, initialize them to zero arrays of the same shape as the gradient.\n2. **Update Moments**: Compute the new first moment (`m_t`) and the new second moment (`v_t`) using the given decay rates `b1` and `b2`.\n3. **Bias Correction**: Apply bias correction to the updated moments to get the bias-corrected first moment (`m_hat`) and the bias-corrected second moment (`v_hat`).\n4. **Parameter Update**: Compute the parameter update using the bias-corrected moments and apply it to the current parameters to get the new parameters.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\"\"\"\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Update the first and second moments\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * (grad ** 2)\n    \n    # Apply bias correction\n    m_hat = m_t / (1 - (b1 ** t))\n    v_hat = v_t / (1 - (b2 ** t))\n    \n    # Compute the update step\n    update = learning_rate * (m_hat / (np.sqrt(v_hat) + eps))\n    w_new = w - update\n    \n    return (w_new, m_t, v_t)\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if `m_prev` or `v_prev` is `None`. If they are, they are initialized to zero arrays of the same shape as the gradient.\n2. **Moment Updates**: The first moment (`m_t`) and the second moment (`v_t`) are updated using the given decay rates `b1` and `b2`, respectively. These updates are done using the current gradient.\n3. **Bias Correction**: The bias-corrected moments are computed by dividing the updated moments by the terms `(1 - b1^t)` and `(1 - b2^t)`, respectively. This step corrects the bias introduced by the initial steps of the optimization process.\n4. **Parameter Update**: The parameter update is computed using the bias-corrected moments. The update step is scaled by the learning rate and adjusted by the second moment to ensure stability. The new parameters are then computed by subtracting this update from the current parameters.\n\nThis approach ensures that the Adam optimizer efficiently converges to optimal parameters by leveraging the moving averages of gradients and their variances, providing a robust and adaptive learning rate for each parameter."}
{"task_id": 479, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement an \u03b5-greedy algorithm for the stationary multi-armed bandit problem. The goal is to simulate one run of the \u03b5-greedy strategy and return the final estimates of the expected reward for every arm, rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start by initializing the estimated value of each arm to a given prior value and set the pull counters for each arm to zero.\n2. **Simulation Loop**: For each time step, decide whether to explore (randomly select an arm) or exploit (select the arm with the highest estimated value). If exploring, choose an arm uniformly at random. If exploiting, select the arm with the highest estimated value, breaking ties by choosing the smallest index.\n3. **Update Estimates**: After selecting an arm, update its estimated value using the incremental sample mean formula.\n4. **Return Results**: After processing all time steps, return the estimated values of each arm, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\"\"\"\n    if not rewards:\n        return []\n    N = len(rewards[0])\n    V = [ev_prior] * N\n    C = [0] * N\n    if seed is not None:\n        np.random.seed(seed)\n    for t in range(len(rewards)):\n        current_rewards = rewards[t]\n        if np.random.random() < epsilon:\n            chosen_arm = np.random.randint(0, N)\n        else:\n            max_v = max(V)\n            candidates = [i for i, v in enumerate(V) if v == max_v]\n            chosen_arm = min(candidates)\n        r = current_rewards[chosen_arm]\n        C[chosen_arm] += 1\n        V[chosen_arm] += (r - V[chosen_arm]) / C[chosen_arm]\n    return [round(v, 4) for v in V]\n```\n\n### Explanation\n1. **Initialization**: The function starts by initializing the estimated values (`V`) of each arm to a given prior value (`ev_prior`) and sets the pull counters (`C`) to zero.\n2. **Random Seed Handling**: If a seed is provided, it is used to seed the random number generator for reproducibility.\n3. **Time Step Loop**: For each time step, the algorithm decides whether to explore or exploit. If exploring, an arm is chosen uniformly at random. If exploiting, the arm with the highest estimated value is chosen, with ties broken by selecting the smallest index.\n4. **Reward Update**: After selecting an arm, the reward is used to update the estimated value of that arm using the incremental sample mean formula, which ensures that the estimate converges to the true expected reward as the number of pulls increases.\n5. **Result Compilation**: After processing all time steps, the estimated values of each arm are rounded to four decimal places and returned as a list.\n\nThis approach efficiently balances exploration and exploitation, ensuring that the algorithm can discover high-reward arms while also exploiting the best-known arms to maximize cumulative reward."}
{"task_id": 481, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm from scratch. DBSCAN is an unsupervised learning algorithm that groups together points that are closely packed and marks points in low-density regions as outliers. The algorithm is particularly useful for discovering clusters of varying shapes and sizes in spatial data.\n\n### Approach\nThe DBSCAN algorithm can be broken down into the following steps:\n\n1. **Identify Core Points**: A point is considered a core point if it has at least `min_samples` points within a distance `eps` (including itself). We compute the distance from each point to every other point and determine if it meets the criteria to be a core point.\n\n2. **Cluster Formation**: Using a breadth-first search (BFS), we expand from each core point to form clusters. Points reachable from a core point within `eps` distance are added to the same cluster. This process continues until all reachable points from the core point are included in the cluster.\n\n3. **Labeling**: Points that are part of a cluster are labeled with the cluster index. Points that are not reachable from any core point are labeled as noise (-1).\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\"\"\"\n    if data.size == 0:\n        return []\n    \n    n = data.shape[0]\n    if n == 0:\n        return []\n    \n    # Determine core points\n    core_points = np.zeros(n, dtype=bool)\n    for i in range(n):\n        distances = np.linalg.norm(data - data[i], axis=1)\n        count = np.sum(distances <= eps)\n        if count >= min_samples:\n            core_points[i] = True\n    \n    # Initialize visited array and cluster labels\n    visited = np.zeros(n, dtype=bool)\n    cluster_labels = [-1] * n\n    current_cluster = 0\n    \n    for i in range(n):\n        if not visited[i] and core_points[i]:\n            # Start a new cluster\n            queue = deque()\n            queue.append(i)\n            visited[i] = True\n            cluster_labels[i] = current_cluster\n            \n            while queue:\n                current_point = queue.popleft()\n                # Find all points within eps distance from current_point\n                distances = np.linalg.norm(data - data[current_point], axis=1)\n                neighbors = np.where(distances <= eps)[0]\n                \n                for neighbor in neighbors:\n                    if not visited[neighbor]:\n                        visited[neighbor] = True\n                        cluster_labels[neighbor] = current_cluster\n                        queue.append(neighbor)\n            \n            current_cluster += 1\n    \n    return cluster_labels\n```\n\n### Explanation\n1. **Core Points Identification**: For each point, we compute the Euclidean distance to all other points. If the number of points within `eps` distance (including itself) is at least `min_samples`, the point is marked as a core point.\n\n2. **BFS for Cluster Expansion**: For each unvisited core point, we start a new cluster. Using BFS, we explore all points within `eps` distance from the current point, marking them as part of the current cluster and continuing the process until no more points can be added to the cluster.\n\n3. **Labeling**: Points are labeled with their respective cluster index as they are discovered. Points that are not part of any cluster are labeled as noise (-1).\n\nThis approach ensures that all points are processed efficiently, and clusters are formed based on the density of points in the dataset. The algorithm handles various edge cases, such as empty datasets and datasets with noise points, correctly."}
{"task_id": 482, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a vector of integer class labels into a 2-D array where each row represents the one-hot encoded version of the corresponding label. This is a common technique in machine learning to handle categorical data.\n\n### Approach\nThe function `to_categorical` takes two parameters: a vector of integer labels `y` and an optional parameter `num_classes`. The function performs the following steps:\n\n1. **Convert Input to Numpy Array**: The input vector `y` is converted into a NumPy array to facilitate efficient operations.\n2. **Check Dimensionality**: Ensure that the input array `y` is 1-D. If not, a `ValueError` is raised.\n3. **Handle Empty Input**: If `y` is empty, return an empty array with the appropriate number of columns based on `num_classes`.\n4. **Determine Number of Classes**: If `num_classes` is not provided, it is inferred as the maximum value in `y` plus one. If `num_classes` is provided, it must be a positive integer.\n5. **Validate Labels**: Check that all labels are non-negative and less than `num_classes`. If any label is invalid, a `ValueError` is raised.\n6. **Create One-Hot Encoded Matrix**: Initialize a matrix of zeros with the appropriate shape. Use advanced indexing to set the corresponding positions to 1 based on the labels.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.asarray(y)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1-D array\")\n    if len(y) == 0:\n        if num_classes is None:\n            return np.zeros((0, 0), dtype=int)\n        else:\n            return np.zeros((0, num_classes), dtype=int)\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n        num_classes = int(num_classes)\n    else:\n        if not isinstance(num_classes, int) or num_classes < 1:\n            raise ValueError(\"num_classes must be a positive integer or None\")\n    if np.any(y < 0) or np.any(y >= num_classes):\n        raise ValueError(\"Labels must be non-negative and less than num_classes\")\n    n = len(y)\n    matrix = np.zeros((n, num_classes), dtype=int)\n    matrix[np.arange(n), y] = 1\n    return matrix\n```\n\n### Explanation\n- **Conversion to Numpy Array**: The input `y` is converted to a NumPy array to leverage efficient array operations.\n- **Dimensionality Check**: Ensures that the input is a 1-D array, which is a requirement for the function to work correctly.\n- **Empty Input Handling**: If the input vector is empty, the function returns an appropriately shaped empty array.\n- **Inference of Number of Classes**: If `num_classes` is not provided, it is determined based on the maximum value in `y`.\n- **Validation of Labels**: Ensures all labels are within the valid range, raising an error if any label is out of bounds.\n- **Matrix Initialization and Encoding**: Creates a matrix of zeros and uses advanced indexing to efficiently set the appropriate positions to 1, resulting in the one-hot encoded matrix.\n\nThis approach ensures that the function is both efficient and robust, handling various edge cases and ensuring correct output for valid inputs."}
{"task_id": 485, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a univariate regression tree using the CART algorithm. The goal is to build a binary tree by recursively splitting the data based on a single feature, selecting the split that minimizes the sum of squared errors (SSE). The tree stops growing when it reaches a specified maximum depth or when a node contains fewer than two samples. Each leaf node is labeled with the mean of the target values in that node. Finally, the tree is used to predict target values for new test points.\n\n### Approach\n1. **Tree Construction**: The tree is built recursively. At each node, if the current depth equals the maximum allowed depth or if the node contains fewer than two samples, the node becomes a leaf with the mean of the target values. Otherwise, the best split point is determined by minimizing the SSE of the left and right subsets.\n2. **Splitting**: For each possible split point between consecutive values of the sorted feature, compute the SSE for the left and right subsets. The split with the minimum SSE is chosen.\n3. **Prediction**: Once the tree is built, each test point is traversed through the tree to the appropriate leaf node, and the mean value of that leaf is returned as the prediction.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    if not X_train or not y_train:\n        return []\n    \n    def build_tree(X, y, depth):\n        if depth == max_depth or len(X) < 2 or len(set(X)) == 1:\n            return {'type': 'leaf', 'prediction': np.mean(y)}\n        \n        sorted_data = sorted(zip(X, y), key=lambda x: x[0])\n        X_sorted = [x for x, _ in sorted_data]\n        y_sorted = [y for _, y in sorted_data]\n        \n        best_split = None\n        min_sse = float('inf')\n        \n        for i in range(1, len(X_sorted)):\n            split_value = (X_sorted[i-1] + X_sorted[i]) / 2\n            left_X = X_sorted[:i]\n            left_y = y_sorted[:i]\n            right_X = X_sorted[i:]\n            right_y = y_sorted[i:]\n            \n            sse_left = np.sum((left_y - np.mean(left_y))**2)\n            sse_right = np.sum((right_y - np.mean(right_y))**2)\n            total_sse = sse_left + sse_right\n            \n            if total_sse < min_sse:\n                min_sse = total_sse\n                best_split = (split_value, left_X, left_y, right_X, right_y)\n        \n        if best_split is None:\n            return {'type': 'leaf', 'prediction': np.mean(y)}\n        \n        split_value, left_X, left_y, right_X, right_y = best_split\n        \n        left_child = build_tree(left_X, left_y, depth + 1)\n        right_child = build_tree(right_X, right_y, depth + 1)\n        \n        return {\n            'type': 'node',\n            'split_value': split_value,\n            'left': left_child,\n            'right': right_child\n        }\n    \n    def predict(x, tree):\n        if tree['type'] == 'leaf':\n            return tree['prediction']\n        else:\n            if x <= tree['split_value']:\n                return predict(x, tree['left'])\n            else:\n                return predict(x, tree['right'])\n    \n    tree = build_tree(X_train, y_train, 0)\n    return [predict(x, tree) for x in X_test]\n```\n\n### Explanation\n1. **Tree Construction**: The `build_tree` function recursively constructs the tree. It checks if the current node should be a leaf based on depth, sample count, or feature uniformity. If not, it finds the best split point by evaluating all possible splits and selecting the one with the minimum SSE.\n2. **Splitting**: For each potential split point, the data is divided into left and right subsets. The SSE for each subset is calculated, and the split with the lowest total SSE is chosen.\n3. **Prediction**: The `predict` function traverses the tree for each test point, following the appropriate path based on the feature value until it reaches a leaf node, where it returns the mean target value.\n\nThis approach ensures that the tree is built efficiently and accurately, providing predictions for new data points based on the learned structure."}
{"task_id": 490, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a one-dimensional vector into a square diagonal matrix. The input can be a list, tuple, or a one-dimensional NumPy array. The resulting matrix should have the elements of the input vector on its principal diagonal and zeros elsewhere. If the input vector is empty, the function should return an empty list.\n\n### Approach\n1. **Check for Empty Input**: If the input vector is empty, return an empty list immediately.\n2. **Determine Matrix Size**: The size of the matrix will be the length of the input vector. If the input vector has `n` elements, the resulting matrix will be an `n x n` matrix.\n3. **Create Diagonal Matrix**: Use NumPy's `diag` function to create a diagonal matrix from the input vector. This function efficiently handles lists, tuples, and NumPy arrays.\n4. **Convert to List of Lists**: Convert the resulting NumPy array to a list of lists using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if not x:\n        return []\n    matrix = np.diag(x)\n    return matrix.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input vector is empty. If it is, the function returns an empty list immediately.\n- **Matrix Creation**: Using NumPy's `diag` function, the function creates a square matrix where the diagonal elements are the elements of the input vector. This function is efficient and works with various input types, including lists, tuples, and NumPy arrays.\n- **Conversion to List of Lists**: The resulting NumPy array is converted to a list of lists using the `tolist()` method, ensuring the output format matches the requirement.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's capabilities for matrix operations while adhering to the problem constraints."}
{"task_id": 492, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a polynomial feature matrix from a given data matrix `X` and a specified degree `d`. The goal is to create a new matrix where each row contains all possible monomials of the original features with a total degree not exceeding `d`. The monomials should be ordered by ascending total degree and, within the same degree, in lexicographic order as produced by `itertools.combinations_with_replacement`.\n\n### Approach\n1. **Check for Negative Degree**: If the provided degree is negative, return `-1` immediately.\n2. **Determine Dimensions**: Extract the number of samples and features from the input matrix `X`.\n3. **Generate Exponent Tuples**: For each degree from 0 to `d`, generate all possible combinations of feature indices using `combinations_with_replacement`. For each combination, compute the exponents as the count of each feature index in the combination.\n4. **Compute Monomials**: For each sample in `X`, compute the product of each feature raised to its respective exponent for each precomputed exponent tuple. This product forms the monomial for that sample.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\"\"\"\n    if degree < 0:\n        return -1\n    \n    n_samples = len(X)\n    if n_samples == 0:\n        return []\n    \n    n_features = len(X[0])\n    \n    # Precompute all exponent tuples\n    exponents = []\n    for current_degree in range(0, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), current_degree):\n            counts = [0] * n_features\n            for idx in combo:\n                counts[idx] += 1\n            exponents.append(counts)\n    \n    # Generate the polynomial features for each sample\n    result = []\n    for sample in X:\n        monomials = []\n        for exp in exponents:\n            product = 1.0\n            for i in range(n_features):\n                product *= sample[i] ** exp[i]\n            monomials.append(product)\n        result.append(monomials)\n    \n    return result\n```\n\n### Explanation\n1. **Negative Degree Check**: The function first checks if the degree is negative and returns `-1` if true.\n2. **Dimensions Extraction**: The number of samples and features are determined from the input matrix `X`.\n3. **Exponent Tuples Generation**: For each degree from 0 to `d`, combinations of feature indices are generated. Each combination is converted into an exponent tuple where each element represents the count of the corresponding feature index in the combination.\n4. **Monomial Computation**: For each sample, the product of each feature raised to its respective exponent is computed for each precomputed exponent tuple, forming the monomials for that sample.\n\nThis approach ensures that the polynomial features are generated efficiently and in the correct order, leveraging the properties of combinations and exponents."}
{"task_id": 493, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the mean-squared error (MSE) of a given 1-D numeric sample. The MSE is a measure of the average squared difference between each element in the sample and the mean of the sample. If the input is an empty sequence, the function should return -1.\n\n### Approach\n1. **Check for Empty Input**: First, we check if the input is an empty list or an empty NumPy array. If it is, we return -1.\n2. **Convert to NumPy Array**: If the input is a list, we convert it to a NumPy array to handle both list and array inputs uniformly.\n3. **Compute the Mean**: Calculate the mean of the sample using NumPy's mean function.\n4. **Calculate Squared Differences**: For each element in the sample, compute the squared difference from the mean.\n5. **Sum and Average**: Sum all the squared differences and then divide by the number of elements to get the MSE.\n6. **Round the Result**: Return the MSE rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    if isinstance(y, list):\n        y = np.array(y)\n    n = y.size\n    if n == 0:\n        return -1\n    mean = np.mean(y)\n    squared_diffs = (y - mean) ** 2\n    sum_sq = np.sum(squared_diffs)\n    mse_val = sum_sq / n\n    return round(mse_val, 4)\n```\n\n### Explanation\n- **Empty Input Check**: The function first checks if the input is empty. If it is, it returns -1 immediately.\n- **Conversion to NumPy Array**: Converting the input to a NumPy array allows us to use efficient NumPy operations for mean calculation and element-wise operations.\n- **Mean Calculation**: Using `np.mean(y)` computes the average of the sample.\n- **Squared Differences**: The expression `(y - mean) ** 2` computes the squared differences between each element and the mean.\n- **Sum and Average**: The sum of squared differences is computed using `np.sum()`, and then divided by the number of elements to get the MSE.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to meet the problem's requirements.\n\nThis approach ensures that the function efficiently handles both lists and NumPy arrays, providing the correct MSE or handling empty inputs appropriately."}
{"task_id": 496, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the affine activation function and its first and second derivatives for each element in the input vector. The function should handle both scalar and vector inputs, and return the results rounded to four decimal places.\n\n### Approach\n1. **Convert Input to 1-D Array**: The input `x` can be a scalar, a list, or a NumPy array. We convert it into a 1-D NumPy array to handle both scalar and vector inputs uniformly.\n2. **Compute Affine Activation**: Using the formula \\(f(x) = \\text{slope} \\cdot x + \\text{intercept}\\), compute the affine activation for each element in the input array.\n3. **Compute First Derivative**: The first derivative of the affine function is a constant equal to the slope. Thus, each element in the gradient array is the slope value.\n4. **Compute Second Derivative**: The second derivative of the affine function is zero, as the first derivative is a constant.\n5. **Round Results**: Each element of the computed values (activation, first derivative, second derivative) is rounded to four decimal places.\n6. **Return Results**: Convert the NumPy arrays to lists and return them as a tuple.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert x to a 1-D NumPy array\n    x = np.array(x).reshape(-1)\n    \n    # Compute the affine activation\n    y = slope * x + intercept\n    \n    # Compute the first derivative (slope) and second derivative (0)\n    grad = np.full(x.shape, slope)\n    grad2 = np.zeros_like(x)\n    \n    # Round each element to 4 decimal places and convert to lists\n    y_rounded = np.round(y, 4).tolist()\n    grad_rounded = np.round(grad, 4).tolist()\n    grad2_rounded = np.round(grad2, 4).tolist()\n    \n    return (y_rounded, grad_rounded, grad2_rounded)\n```\n\n### Explanation\n- **Input Handling**: The function converts the input `x` into a 1-D NumPy array to handle both scalar and vector inputs uniformly.\n- **Affine Activation**: The affine function is applied element-wise to the input array using NumPy operations for efficiency.\n- **Derivatives**: The first derivative is a constant array filled with the slope value, and the second derivative is an array of zeros.\n- **Rounding**: Each element of the computed arrays is rounded to four decimal places to meet the problem's requirements.\n- **Return Type**: The results are converted to lists and returned as a tuple, ensuring the output format matches the specified requirements."}
{"task_id": 499, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Principal Component Analysis (PCA) dimensionality reduction routine from scratch. The goal is to project the given data onto its first `k` principal components, following a series of steps to ensure the result is deterministic and correctly scaled.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Standardize the Data**: Each feature is centered (zero mean) and scaled (unit population variance). If a feature has zero variance, it remains unchanged after centering.\n\n2. **Compute the Sample Covariance Matrix**: Using the standardized data, compute the covariance matrix with Bessel\u2019s correction (dividing by `n-1` where `n` is the number of samples).\n\n3. **Eigen-decomposition**: Decompose the covariance matrix into its eigenvalues and eigenvectors.\n\n4. **Sort Eigenvalues and Eigenvectors**: Sort the eigenvalues in descending order and reorder the corresponding eigenvectors accordingly.\n\n5. **Fix Eigenvector Signs**: Ensure each eigenvector has a positive entry with the largest absolute value to remove sign ambiguity.\n\n6. **Project Data**: Project the standardized data onto the first `k` eigenvectors.\n\n7. **Return Result**: Return the projected data rounded to four decimal places as a list of lists. If `k` is out of bounds, return `-1`.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    # Check if k is within the valid range\n    n_features = data.shape[1]\n    if k < 1 or k > n_features:\n        return -1\n\n    # Step 1: Standardize each feature\n    n_samples = data.shape[0]\n    standardized = np.zeros_like(data, dtype=np.float64)\n    for i in range(n_features):\n        col = data[:, i]\n        mean = np.mean(col)\n        centered = col - mean\n        variance = np.sum(centered ** 2) / n_samples\n        if variance == 0:\n            standardized[:, i] = centered\n        else:\n            std = np.sqrt(variance)\n            standardized[:, i] = centered / std\n\n    # Step 2: Compute the sample covariance matrix\n    cov_matrix = np.cov(standardized, rowvar=False, bias=False)\n\n    # Step 3: Perform eigen-decomposition\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Step 4: Sort eigenvalues and eigenvectors in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Step 5: Fix the sign of each eigenvector\n    for i in range(eigenvectors.shape[1]):\n        vec = eigenvectors[:, i]\n        max_abs_idx = np.argmax(np.abs(vec))\n        if vec[max_abs_idx] < 0:\n            eigenvectors[:, i] *= -1\n\n    # Step 6: Project the standardized data onto the first k eigenvectors\n    projection = standardized @ eigenvectors[:, :k]\n\n    # Round the result to 4 decimal places and convert to a list of lists\n    result = np.round(projection, 4).tolist()\n\n    return result\n```\n\n### Explanation\n1. **Standardization**: Each feature is centered and scaled to ensure zero mean and unit variance. This step is crucial for PCA as it ensures that all features contribute equally to the analysis.\n\n2. **Covariance Matrix**: The covariance matrix captures the relationships between features. Using Bessel\u2019s correction ensures an unbiased estimate of the population covariance.\n\n3. **Eigen-decomposition**: This step identifies the principal components (eigenvectors) and their corresponding variances (eigenvalues). The eigenvectors are sorted by their eigenvalues in descending order to prioritize the most significant components.\n\n4. **Sign Fixing**: Eigenvectors can have arbitrary signs, so we fix the sign such that the largest absolute value is positive to ensure a deterministic result.\n\n5. **Projection**: The data is projected onto the first `k` eigenvectors, effectively reducing the dimensionality while retaining the most variance.\n\nThis approach ensures that the PCA transformation is correctly implemented, providing a deterministic and scaled result as required."}
{"task_id": 500, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient boosting binary classifier using decision stumps as weak learners. The model will be built using an additive approach where each weak learner (decision stump) is trained to minimize the squared error loss on the residuals from the previous model.\n\n### Approach\n1. **Initialization**: Start with an initial prediction for all samples, which is the average of the training labels.\n2. **Iterative Training**: For each weak learner (decision stump):\n   - Compute the residuals (differences between actual labels and current predictions).\n   - Fit a decision stump to these residuals by finding the best split that minimizes the squared error.\n   - Update the model by adding the contribution of the new decision stump, scaled by a learning rate.\n3. **Prediction**: For each test sample, compute the final prediction by summing the contributions from all decision stumps and thresholding the result at 0.5 to get the binary class label.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    X_train_np = np.array(X_train)\n    y_train_np = np.array(y_train)\n    X_test_np = np.array(X_test)\n    \n    # Initialize F with the average of y_train\n    c = np.mean(y_train_np)\n    F_train = np.full_like(y_train_np, c, dtype=np.float64)\n    \n    stumps = []\n    \n    for _ in range(n_estimators):\n        residuals = y_train_np - F_train\n        \n        best_error = np.inf\n        best_feature = 0\n        best_threshold = 0.0\n        best_left = 0.0\n        best_right = 0.0\n        \n        # Iterate over each feature to find the best split\n        for feature in range(X_train_np.shape[1]):\n            feature_values = X_train_np[:, feature]\n            unique_vals = np.sort(np.unique(feature_values))\n            \n            # Consider splits between consecutive unique values\n            for i in range(len(unique_vals) - 1):\n                threshold = (unique_vals[i] + unique_vals[i+1]) / 2\n                left_mask = (feature_values <= threshold)\n                right_mask = ~left_mask\n                \n                left_r = residuals[left_mask]\n                right_r = residuals[right_mask]\n                \n                # Calculate means for left and right groups\n                left_mean = np.mean(left_r) if len(left_r) > 0 else 0\n                right_mean = np.mean(right_r) if len(right_r) > 0 else 0\n                \n                # Compute the total squared error for this split\n                error = np.sum((left_r - left_mean)**2) + np.sum((right_r - right_mean)**2)\n                \n                # Update best split if current error is smaller\n                if error < best_error:\n                    best_error = error\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_left = left_mean\n                    best_right = right_mean\n        \n        # Compute the stump's predictions on the training set\n        feature_values = X_train_np[:, best_feature]\n        stump_predictions = np.zeros_like(residuals)\n        stump_predictions[feature_values <= best_threshold] = best_left\n        stump_predictions[feature_values > best_threshold] = best_right\n        \n        # Update F_train with the new stump's predictions scaled by learning rate\n        F_train += learning_rate * stump_predictions\n        \n        # Store the stump for later prediction\n        stumps.append((best_feature, best_threshold, best_left, best_right))\n    \n    # Make predictions on the test set\n    predictions = []\n    for x in X_test_np:\n        F = c\n        for (feature, threshold, left_val, right_val) in stumps:\n            if x[feature] <= threshold:\n                F += learning_rate * left_val\n            else:\n                F += learning_rate * right_val\n        if F >= 0.5:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    return predictions\n```\n\n### Explanation\n1. **Initialization**: The initial prediction for all samples is set to the average of the training labels, which serves as the starting point for the model.\n2. **Residual Calculation**: For each iteration, the residuals (differences between actual labels and current predictions) are computed. These residuals guide the training of the next weak learner.\n3. **Decision Stump Fitting**: For each feature, potential splits are evaluated to find the one that minimizes the squared error. The best split is determined by considering all possible thresholds between consecutive unique values of the feature.\n4. **Model Update**: The contribution of each decision stump is added to the current model, scaled by the learning rate, to form the new model.\n5. **Prediction**: For each test sample, the final prediction is computed by summing the contributions from all decision stumps and thresholding the result at 0.5 to obtain the binary class label.\n\nThis approach ensures that each weak learner improves the model incrementally, leading to a robust ensemble model for binary classification."}
{"task_id": 505, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a single weight-update step for a binary Restricted Boltzmann Machine (RBM) using the Contrastive Divergence (CD-k) algorithm. The goal is to compute the weight matrix update based on the given mini-batch of visible vectors, current weights, biases, learning rate, and the number of Gibbs sampling steps.\n\n### Approach\n1. **Check for Empty Mini-batch**: If the mini-batch of visible vectors is empty, return an empty list immediately.\n2. **Positive Phase**: Compute the hidden probabilities given the visible units in the data. This is done using the sigmoid function applied to the dot product of the visible vectors and the weight matrix, plus the hidden biases.\n3. **Gibbs Sampling**: Perform `k` full Gibbs steps. Each step involves computing the visible probabilities from the hidden probabilities and then computing the hidden probabilities from the visible probabilities. This is done using the sigmoid function in each step.\n4. **Compute Gradients**: Calculate the positive gradient using the initial visible vectors and hidden probabilities. Calculate the negative gradient using the reconstructed visible and hidden probabilities after `k` Gibbs steps.\n5. **Weight Update**: Update the weight matrix using the computed gradients, scaled by the learning rate and the number of samples in the mini-batch.\n6. **Return Result**: Convert the updated weight matrix to a list of lists, rounded to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\"\"\"\n    if X.shape[0] == 0:\n        return []\n    \n    # Compute positive phase hidden probabilities\n    h0 = 1 / (1 + np.exp(-(X.dot(W) + hbias)))\n    \n    # Compute positive gradient\n    pos_grad = X.T.dot(h0)\n    \n    # Run k Gibbs steps\n    h = h0.copy()\n    for _ in range(k):\n        v = 1 / (1 + np.exp(-(h.dot(W.T) + vbias)))\n        h = 1 / (1 + np.exp(-(v.dot(W) + hbias)))\n    \n    # Compute negative gradient\n    neg_grad = v.T.dot(h)\n    \n    # Compute delta\n    m = X.shape[0]\n    delta = (pos_grad - neg_grad) / m\n    \n    # Update weights\n    W_new = W + learning_rate * delta\n    \n    # Round and convert to list of lists\n    W_new_rounded = np.round(W_new, 4)\n    return W_new_rounded.tolist()\n```\n\n### Explanation\n1. **Positive Phase**: The hidden probabilities `h0` are computed using the sigmoid function applied to the linear transformation of the visible vectors `X` with the weight matrix `W` and the hidden biases `hbias`.\n2. **Gibbs Sampling**: For each of the `k` steps, the visible probabilities `v` are computed from the current hidden probabilities `h`, and then the hidden probabilities `h` are recomputed from these visible probabilities. This process is repeated `k` times to get the reconstructed visible and hidden probabilities.\n3. **Gradients**: The positive gradient is the outer product of the transpose of the visible vectors and the initial hidden probabilities. The negative gradient is the outer product of the transpose of the reconstructed visible vectors and the reconstructed hidden probabilities.\n4. **Weight Update**: The weight matrix is updated by adding the learning rate scaled difference between the positive and negative gradients, divided by the number of samples in the mini-batch.\n5. **Result Conversion**: The updated weight matrix is rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach ensures that the RBM weights are updated efficiently using the CD-k algorithm, providing a robust method for training the model."}
{"task_id": 509, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Chebyshev distance between two real-valued vectors. The Chebyshev distance is defined as the maximum absolute difference between corresponding elements of the two vectors. The function should handle both Python lists and NumPy 1-D arrays as inputs and return the distance rounded to four decimal places.\n\n### Approach\n1. **Check Vector Lengths**: First, verify that the two input vectors have the same length. If they do not, return -1.\n2. **Handle Empty Vectors**: If both vectors are empty, return -1 as the Chebyshev distance is undefined for empty vectors.\n3. **Convert to NumPy Arrays**: Convert the input vectors into NumPy arrays to leverage efficient vectorized operations.\n4. **Compute Absolute Differences**: Calculate the absolute differences between corresponding elements of the two vectors.\n5. **Find Maximum Difference**: Determine the maximum value from the computed absolute differences.\n6. **Round the Result**: Round the maximum difference to four decimal places and return it as a standard Python float.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    if len(x) != len(y):\n        return -1\n    n = len(x)\n    if n == 0:\n        return -1\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    diffs = np.abs(x_arr - y_arr)\n    max_diff = np.max(diffs)\n    return round(max_diff, 4)\n```\n\n### Explanation\n- **Vector Length Check**: The function first checks if the lengths of the two vectors are the same. If not, it returns -1.\n- **Empty Vector Handling**: If both vectors are empty, the function returns -1 since the Chebyshev distance is undefined for empty vectors.\n- **Conversion to NumPy Arrays**: The input vectors are converted to NumPy arrays to facilitate efficient element-wise operations.\n- **Absolute Differences Calculation**: Using NumPy's vectorized operations, the function computes the absolute differences between corresponding elements of the two vectors.\n- **Maximum Difference Calculation**: The maximum value of these absolute differences is found using NumPy's `max` function.\n- **Rounding**: The maximum difference is rounded to four decimal places and returned as a Python float.\n\nThis approach ensures that the function efficiently computes the Chebyshev distance while handling various edge cases, such as different vector lengths and empty vectors."}
{"task_id": 510, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Variational Auto-Encoder (VAE) loss function for Bernoulli visible units. The VAE loss is composed of two main components: the reconstruction loss and the KL divergence. The goal is to compute the mini-batch average of the sum of these two losses and return it rounded to six decimal places.\n\n### Approach\n1. **Reconstruction Loss**: This is computed using the binary cross-entropy (BCE) between the true input and the reconstructed output. To ensure numerical stability, the reconstructed probabilities are clipped to a small value \u03b5 within the interval (0, 1).\n2. **KL Divergence**: This measures the divergence between the approximate posterior distribution and the prior Gaussian distribution. The KL divergence is computed analytically using the mean and log-variance of the latent variables.\n3. **Summing Losses**: For each sample in the mini-batch, the reconstruction loss and KL divergence are summed. The average of these sums across the entire mini-batch is then computed and rounded to six decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to exactly 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Compute reconstruction loss (binary cross-entropy)\n    bce = -y * np.log(y_pred_clipped) - (1 - y) * np.log(1 - y_pred_clipped)\n    reconstruction_loss = np.sum(bce, axis=1)\n    \n    # Compute KL divergence\n    kl_terms = 1 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    kl_per_sample = -0.5 * np.sum(kl_terms, axis=1)\n    \n    # Total loss per sample\n    total_loss = reconstruction_loss + kl_per_sample\n    \n    # Average over the batch\n    average_loss = np.mean(total_loss)\n    \n    # Round to six decimal places\n    return np.round(average_loss, 6)\n```\n\n### Explanation\n1. **Clipping Values**: The reconstructed probabilities `y_pred` are clipped to avoid taking the logarithm of zero or one, which can cause numerical instability.\n2. **Binary Cross-Entropy (BCE)**: For each element in the input, the BCE is computed and summed across all features for each sample in the mini-batch.\n3. **KL Divergence Calculation**: Using the mean and log-variance of the latent variables, the KL divergence is computed for each latent dimension and summed across all dimensions for each sample.\n4. **Total Loss**: The reconstruction loss and KL divergence are summed for each sample, and the average of these sums across the mini-batch is computed and returned as the final loss, rounded to six decimal places.\n\nThis approach ensures that the VAE loss is computed efficiently and accurately, taking into account numerical stability and proper averaging over the mini-batch."}
{"task_id": 513, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates predictions using a Factorization Machine (FM) model. The FM model combines linear regression with pairwise feature interactions to make predictions. The formula for the prediction is given, and we need to translate this into an efficient Python function using numpy for numerical computations.\n\n### Approach\nThe approach involves breaking down the prediction formula into three main parts: the bias term, the linear part, and the interaction part. Here's a detailed breakdown:\n\n1. **Bias Term (w0)**: This is a scalar value added directly to the prediction.\n2. **Linear Part**: This is computed as the dot product of the feature matrix `X` and the linear weights `w`.\n3. **Interaction Part**: This involves pairwise interactions between features, modeled using latent factors. The interaction part is computed using the formula:\n   \\[\n   \\frac{1}{2} \\sum_{f=1}^{k} \\left[ \\left( \\sum_{j} V_{j,f} x_j \\right)^2 - \\sum_{j} V_{j,f}^2 x_j^2 \\right]\n   \\]\n   where \\( V \\) is the matrix of latent factors.\n\nThe steps to compute the interaction part efficiently using numpy are:\n1. Compute the product of `X` and `V` to get pairwise sums.\n2. Square these sums.\n3. Compute the element-wise squares of `X` and `V`.\n4. Compute the product of these squared matrices.\n5. Subtract the squared product from the squared sums and sum over all latent factors.\n6. Multiply the result by 0.5.\n\nFinally, combine the bias, linear, and interaction parts to get the predictions, and round each prediction to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    X = np.array(X)\n    w = np.array(w)\n    V = np.array(V)\n    \n    # Compute the linear part\n    linear = X.dot(w)\n    \n    # Compute the interaction part\n    XV = X.dot(V)\n    XV_squared = XV ** 2\n    \n    X_squared = X ** 2\n    V_squared = V ** 2\n    X_squared_V_squared = X_squared.dot(V_squared)\n    \n    interaction = 0.5 * (XV_squared - X_squared_V_squared).sum(axis=1)\n    \n    # Combine all parts\n    y_hat = w0 + linear + interaction\n    \n    # Round to 4 decimal places\n    y_hat_rounded = np.round(y_hat, 4)\n    \n    return y_hat_rounded.tolist()\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The input lists are converted to numpy arrays for efficient matrix operations.\n2. **Linear Part Calculation**: The linear part is computed using the dot product of `X` and `w`.\n3. **Interaction Part Calculation**: This involves several steps:\n   - Compute the product of `X` and `V` to get pairwise sums.\n   - Square these sums.\n   - Compute the element-wise squares of `X` and `V`.\n   - Compute the product of these squared matrices.\n   - Subtract the squared product from the squared sums and sum over all latent factors.\n   - Multiply the result by 0.5.\n4. **Combining Parts**: The bias term, linear part, and interaction part are combined to form the final predictions.\n5. **Rounding**: Each prediction is rounded to 4 decimal places before returning the result.\n\nThis approach ensures that the function efficiently computes the predictions using vectorized operations, making it suitable for large datasets."}
{"task_id": 517, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Python function that computes the sigmoid activation function for various input types, ensuring numerical stability and preserving the input structure. The sigmoid function is defined as \\( g(z) = \\frac{1}{1 + e^{-z}} \\) and must handle very large positive or negative values of \\( z \\) without causing overflow.\n\n### Approach\n1. **Input Handling**: The function should accept inputs of various types, including scalars (int or float), lists, tuples, and NumPy arrays. The output should preserve the input structure, returning a float for scalar inputs and a list for other types.\n2. **Numerical Stability**: To avoid overflow issues with very large values of \\( z \\), the function uses a numerically stable approach. For \\( z \\geq 0 \\), it computes the sigmoid as \\( \\frac{1}{1 + e^{-z}} \\). For \\( z < 0 \\), it computes the sigmoid as \\( \\frac{e^z}{1 + e^z} \\).\n3. **Rounding**: The result is rounded to four decimal places to meet the problem's requirements.\n4. **Vectorization**: For array-like inputs, the function uses NumPy's vectorized operations to efficiently compute the sigmoid for all elements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        if z >= 0:\n            s = 1.0 / (1.0 + np.exp(-z))\n        else:\n            s = np.exp(z) / (1.0 + np.exp(z))\n        s_rounded = np.round(s, 4)\n        return float(s_rounded)\n    else:\n        z = np.array(z)\n        sigmoid_z = np.where(z >= 0, 1/(1 + np.exp(-z)), np.exp(z)/(1 + np.exp(z)))\n        sigmoid_z = np.round(sigmoid_z, 4)\n        return sigmoid_z.tolist()\n```\n\n### Explanation\n1. **Input Check**: The function first checks if the input \\( z \\) is a scalar (int or float). If so, it computes the sigmoid using the appropriate formula based on whether \\( z \\) is non-negative or negative.\n2. **Array Handling**: If \\( z \\) is not a scalar, it is converted to a NumPy array. The function then uses NumPy's vectorized operations to compute the sigmoid for each element, ensuring numerical stability by using different formulas for non-negative and negative values.\n3. **Rounding**: The computed sigmoid values are rounded to four decimal places using NumPy's `round` function.\n4. **Output Conversion**: The result is converted back to the appropriate structure. For scalar inputs, a float is returned. For array-like inputs, the result is converted to a Python list using NumPy's `tolist()` method.\n\nThis approach ensures that the function is efficient, numerically stable, and handles various input types correctly."}
{"task_id": 518, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a decorator called `unhot` that converts one-hot encoded NumPy arrays into their corresponding integer label representations before passing them to a metric function. This conversion simplifies the process of evaluating metrics that expect one-dimensional integer arrays.\n\n### Approach\n1. **Problem Analysis**: The task involves converting one-hot encoded arrays into integer labels. One-hot encoding is a common technique where each class is represented as a binary vector. For example, the class '2' in a three-class problem is represented as `[0, 0, 1]`. The goal is to convert such vectors into their respective integer labels (e.g., `2`).\n\n2. **Intuition**: The decorator will check if the input arrays are two-dimensional and if their second dimension has more than one element. If so, it converts them using the `argmax` function along the second axis, which effectively finds the index of the maximum value (the position of the '1' in the one-hot vector).\n\n3. **Algorithm**:\n   - The decorator function `unhot` takes a metric function as input and returns a wrapper function.\n   - The wrapper function processes the `actual` and `predicted` arrays.\n   - For each array, it checks if it is two-dimensional and if the second dimension has more than one element. If true, it converts the array using `argmax(axis=1)`.\n   - The processed arrays are then passed to the metric function, and the result is returned.\n\n### Solution Code\n```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual, predicted):\n        # Process the actual array\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        # Process the predicted array\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        # Call the original function with processed arrays\n        return function(actual, predicted)\n    return wrapper\n```\n\n### Explanation\n- **Decorator Function**: The `unhot` function is a decorator that takes a metric function as input and returns a wrapper function.\n- **Wrapper Function**: The wrapper function processes the input arrays `actual` and `predicted`.\n  - It checks if each array is two-dimensional and if the second dimension has more than one element.\n  - If the conditions are met, it converts the array using `argmax(axis=1)`, which converts the one-hot encoded vectors into integer labels.\n  - The processed arrays are then passed to the original metric function, and the result is returned.\n\nThis approach ensures that any metric function decorated with `unhot` can handle one-hot encoded arrays seamlessly, converting them into the required integer labels before evaluation."}
{"task_id": 520, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of the sigmoid function given its output. The function should handle various input types, including scalars, lists, and NumPy arrays, and return the result in the same structure as the input.\n\n### Approach\n1. **Identify Input Type**: The function first checks if the input is a scalar (either a float, int, or a 0D NumPy array). If it is, the function computes the derivative directly and returns it as a float.\n2. **Handle Non-Scalar Inputs**: For non-scalar inputs (lists or NumPy arrays), the function converts the input into a NumPy array to facilitate element-wise operations.\n3. **Compute Gradient**: Using the formula \\( g'(z) = g(z) \\times (1 - g(z)) \\), the function computes the gradient for each element in the array.\n4. **Round Results**: Each computed gradient value is rounded to 4 decimal places.\n5. **Convert Back to Original Structure**: The result is converted back to a list structure to match the input format, ensuring that the output has the same shape as the input.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    if np.isscalar(z):\n        result = z * (1 - z)\n        rounded = np.round(result, 4)\n        return rounded.item() if isinstance(rounded, np.ndarray) else rounded\n    else:\n        if isinstance(z, list) and len(z) == 0:\n            return []\n        arr = np.array(z)\n        grad = arr * (1 - arr)\n        grad_rounded = np.round(grad, 4)\n        result = grad_rounded.tolist()\n        return result\n```\n\n### Explanation\n- **Scalar Handling**: If the input is a scalar, the function computes the gradient directly and returns it as a float.\n- **List and Array Handling**: For lists or NumPy arrays, the function converts the input to a NumPy array to leverage vectorized operations for efficiency. The gradient is computed element-wise, rounded, and then converted back to a list to maintain the original structure.\n- **Empty List Handling**: Special handling ensures that an empty list input returns an empty list output.\n\nThis approach ensures that the function is versatile and can handle various input types while maintaining the structure of the output as required."}
{"task_id": 528, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a grid of points for a 2-D linear classifier and classify each point based on the given weights and bias. The goal is to create a dense grid that spans the range of the input data and then classify each grid point to determine the decision boundaries.\n\n### Approach\n1. **Extract Data Points**: Extract the x1 and x2 coordinates from the input data set `X`.\n2. **Determine Grid Range**: Compute the minimum and maximum values for both x1 and x2 to define the range of the grid.\n3. **Create Grid Points**: Use `numpy.linspace` to create equally spaced points along the x1 and x2 axes within the determined range.\n4. **Generate Meshgrid**: Use `numpy.meshgrid` to create a 2D grid of points from the x1 and x2 grid points.\n5. **Classify Grid Points**: For each point in the grid, compute the linear combination using the given weights and bias. Classify each point based on the sign of this linear combination.\n6. **Convert to List**: Convert the resulting numpy array of labels into a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Extract x1 and x2 from X\n    x1 = [point[0] for point in X]\n    x2 = [point[1] for point in X]\n    \n    # Compute min and max for each dimension\n    x1_min, x1_max = min(x1), max(x1)\n    x2_min, x2_max = min(x2), max(x2)\n    \n    # Create the grid points\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create meshgrid\n    X1, X2 = np.meshgrid(x1_plot, x2_plot)\n    \n    # Compute the linear function\n    Z = W[0] * X1 + W[1] * X2 + b\n    \n    # Compute labels\n    labels = np.where(Z >= 0, 1, -1)\n    \n    # Convert to list of lists\n    labels_list = labels.tolist()\n    \n    return labels_list\n```\n\n### Explanation\n1. **Extract Data Points**: The input data set `X` is processed to extract the x1 and x2 coordinates, which are stored in separate lists.\n2. **Determine Grid Range**: The minimum and maximum values for x1 and x2 are computed to define the range of the grid.\n3. **Create Grid Points**: Using `numpy.linspace`, we generate `grid_n` equally spaced points along the x1 and x2 axes within their respective ranges.\n4. **Generate Meshgrid**: The `numpy.meshgrid` function is used to create a 2D grid of points from the x1 and x2 grid points, resulting in two 2D arrays `X1` and `X2`.\n5. **Classify Grid Points**: For each point in the grid, the linear combination `W[0] * X1 + W[1] * X2 + b` is computed. The sign of this value determines the label (1 if non-negative, -1 otherwise).\n6. **Convert to List**: The resulting numpy array of labels is converted to a list of lists to meet the output requirements.\n\nThis approach ensures that we efficiently generate and classify a dense grid of points, which can then be used to visualize decision boundaries for the linear classifier."}
{"task_id": 537, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Gaussian Naive Bayes classifier from scratch. This classifier will be used to predict the class labels of test samples based on the training data. The approach involves learning the parameters of the Gaussian distribution for each feature in each class, computing class priors, and then using these parameters to predict the class of each test sample.\n\n### Approach\n1. **Split Training Data by Class**: Separate the training data into two groups based on their class labels (0 and 1).\n2. **Compute Parameters**: For each class, compute the mean and standard deviation of each feature. Add a small constant to the standard deviation to avoid division by zero.\n3. **Compute Class Priors**: Calculate the prior probability of each class as the relative frequency of the class in the training data.\n4. **Predict Class for Test Samples**: For each test sample, compute the log posterior probability for each class using the Gaussian likelihood model. The class with the higher log posterior probability is chosen as the predicted class.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\"\"\"\n    # Split the training data into two classes\n    class_0 = X_train[y_train == 0]\n    class_1 = X_train[y_train == 1]\n    \n    # Compute mean and standard deviation for each feature in each class\n    mu0 = class_0.mean(axis=0)\n    sigma0 = class_0.std(axis=0) + 1e-9  # Add epsilon to avoid division by zero\n    mu1 = class_1.mean(axis=0)\n    sigma1 = class_1.std(axis=0) + 1e-9\n    \n    # Compute class priors\n    count0 = len(class_0)\n    count1 = len(class_1)\n    total = count0 + count1\n    prior0 = count0 / total\n    prior1 = count1 / total\n    \n    # Precompute the constant term to avoid redundant calculations\n    const = -0.5 * np.log(2 * np.pi)\n    \n    predictions = []\n    for x in X_test:\n        # Calculate log probabilities for class 0\n        diff0 = x - mu0\n        term0 = const - np.log(sigma0) - (diff0 ** 2) / (2 * sigma0 ** 2)\n        log_p0 = np.log(prior0) + np.sum(term0)\n        \n        # Calculate log probabilities for class 1\n        diff1 = x - mu1\n        term1 = const - np.log(sigma1) - (diff1 ** 2) / (2 * sigma1 ** 2)\n        log_p1 = np.log(prior1) + np.sum(term1)\n        \n        # Compare and predict the class\n        if log_p0 > log_p1:\n            predictions.append(0)\n        else:\n            predictions.append(1)\n    \n    return predictions\n```\n\n### Explanation\n1. **Splitting Training Data**: The training data is divided into two groups based on the class labels. This allows us to compute class-specific statistics.\n2. **Parameter Computation**: For each class, the mean and standard deviation of each feature are computed. A small constant (epsilon) is added to the standard deviation to prevent division by zero during probability calculations.\n3. **Class Priors**: The prior probability of each class is calculated as the ratio of the number of samples in that class to the total number of samples. This reflects the relative frequency of each class in the training data.\n4. **Log Posterior Calculation**: For each test sample, the log posterior probability for each class is computed using the Gaussian likelihood model. This involves calculating the log probability for each feature and summing them up, then adding the log prior probability of the class. The class with the higher log posterior probability is selected as the predicted class.\n\nThis approach ensures that we efficiently and accurately classify test samples using the Gaussian Naive Bayes model."}
{"task_id": 539, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the binary cross-entropy cost for a binary classification neural network. This cost function measures the difference between the predicted probabilities and the true labels, and it is commonly used in binary classification tasks.\n\n### Approach\nThe binary cross-entropy cost function is defined as:\n\\[ J = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\cdot \\ln(a_i) + (1 - y_i) \\cdot \\ln(1 - a_i)] \\]\nwhere \\( m \\) is the number of training examples, \\( a_i \\) is the predicted probability for the \\( i \\)-th example, and \\( y_i \\) is the true label (0 or 1) for the \\( i \\)-th example.\n\nTo implement this function, we follow these steps:\n1. **Clip the predicted probabilities**: To avoid numerical overflow when taking the logarithm, we clip the predicted probabilities \\( a_i \\) to the range \\([1 \\times 10^{-15}, 1 - 1 \\times 10^{-15}]\\).\n2. **Compute the terms**: For each example, compute the terms \\( y_i \\cdot \\ln(a_i) \\) and \\( (1 - y_i) \\cdot \\ln(1 - a_i) \\).\n3. **Sum the terms**: Sum these terms for all examples.\n4. **Compute the average**: Average the sum by dividing by the number of examples \\( m \\) and take the negative to get the final cost.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\"\"\"\n    m = Y.size\n    AL_clipped = np.clip(AL, 1e-15, 1 - 1e-15)\n    term1 = Y * np.log(AL_clipped)\n    term2 = (1 - Y) * np.log(1 - AL_clipped)\n    sum_terms = term1 + term2\n    cost = (-1 / m) * sum_terms.sum()\n    return float(cost)\n```\n\n### Explanation\n1. **Clipping the Probabilities**: The predicted probabilities are clipped to avoid taking the logarithm of 0 or 1, which would cause numerical issues. This is done using `np.clip(AL, 1e-15, 1 - 1e-15)`.\n2. **Computing Terms**: For each example, we compute two terms. The first term \\( y_i \\cdot \\ln(a_i) \\) is non-zero only when the true label \\( y_i \\) is 1. The second term \\( (1 - y_i) \\cdot \\ln(1 - a_i) \\) is non-zero only when the true label \\( y_i \\) is 0.\n3. **Summing Terms**: The terms are summed element-wise for all examples.\n4. **Averaging**: The sum is divided by the number of examples \\( m \\) and negated to get the final cost.\n\nThis approach ensures that we handle numerical stability and correctly compute the binary cross-entropy cost for binary classification tasks."}
{"task_id": 552, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the posterior state distribution \u03b3\u209c for a given Hidden Markov Model (HMM) at a specific time step t. This involves using the forward-backward algorithm to compute the forward probabilities (\u03b1) and backward probabilities (\u03b2), and then combining them to get the posterior probabilities.\n\n### Approach\n1. **Check Validity of Time Step**: Ensure the given time step t is within the valid range [0, len(observations) - 1]. If not, return an empty list.\n2. **Compute Forward Probabilities (\u03b1)**: The forward probability \u03b1\u209c(i) is the probability of being in state i at time t, given the first t observations. This is computed using dynamic programming.\n3. **Compute Backward Probabilities (\u03b2)**: The backward probability \u03b2\u209c(i) is the probability of the observations from t+1 to the end, given state i at time t. This is also computed using dynamic programming.\n4. **Combine \u03b1 and \u03b2 to Compute \u03b3\u209c**: For each state i, compute \u03b3\u209c(i) as the product of \u03b1\u209c(i) and \u03b2\u209c(i), normalized by the sum of these products over all states.\n5. **Round the Results**: Round each component of \u03b3\u209c to four decimal places and return the result as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    if t < 0 or t >= len(observations):\n        return []\n    \n    N = len(hmm['pi'])\n    T = len(observations)\n    \n    # Compute forward probabilities alpha\n    alpha = np.zeros((T, N))\n    for i in range(N):\n        alpha[0][i] = hmm['pi'][i] * hmm['B'][i][observations[0]]\n    \n    for t_step in range(1, T):\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                sum_val += alpha[t_step-1][j] * hmm['A'][j][i]\n            alpha[t_step][i] = sum_val * hmm['B'][i][observations[t_step]]\n    \n    # Compute backward probabilities beta\n    beta = np.zeros((T, N))\n    for i in range(N):\n        beta[T-1][i] = 1.0\n    \n    for t_step in range(T-2, -1, -1):\n        for i in range(N):\n            sum_val = 0.0\n            for j in range(N):\n                sum_val += hmm['A'][i][j] * hmm['B'][j][observations[t_step+1]] * beta[t_step+1][j]\n            beta[t_step][i] = sum_val\n    \n    # Compute gamma\n    gamma = alpha[t] * beta[t]\n    gamma = gamma / gamma.sum()\n    \n    # Round to four decimal places\n    gamma = [round(x, 4) for x in gamma.tolist()]\n    \n    return gamma\n```\n\n### Explanation\n1. **Forward Probabilities (\u03b1)**: The forward algorithm initializes \u03b1[0][i] as the product of the initial state probability and the emission probability of the first observation. For each subsequent time step, \u03b1 is updated by summing over all possible previous states, multiplied by the transition and emission probabilities.\n2. **Backward Probabilities (\u03b2)**: The backward algorithm initializes \u03b2[T-1][i] to 1 for all states. For each previous time step, \u03b2 is updated by summing over all possible next states, multiplied by the transition, emission probabilities, and the next \u03b2 values.\n3. **Posterior Probabilities (\u03b3)**: The posterior probability \u03b3\u209c(i) is computed as the product of \u03b1\u209c(i) and \u03b2\u209c(i), normalized by the sum of these products over all states. This gives the probability distribution over states at time t, given the entire observation sequence.\n\nThis approach efficiently computes the required probabilities using dynamic programming, ensuring that the solution is both correct and optimal."}
{"task_id": 555, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from scratch. DBSCAN is a density-based clustering algorithm that groups together points that are closely packed together and marks points in low-density regions as noise. The algorithm identifies clusters by expanding around core points, which are points with a minimum number of neighbors within a specified radius.\n\n### Approach\n1. **Compute Pairwise Distances**: For each point, compute the Euclidean distance to every other point and build a neighborhood list where each point includes all other points within a specified radius (eps).\n2. **Identify Core Points**: A point is considered a core point if it has at least a minimum number of neighbors (min_samples) within the specified radius.\n3. **Cluster Expansion Using BFS**: Starting with the first unlabeled core point, expand the cluster using a breadth-first search (BFS). This involves labeling the core point, adding it to a queue, and then processing each point in the queue to include all its neighbors in the cluster. If a neighbor is a core point, it is added to the queue to continue expanding the cluster.\n4. **Label Noise Points**: After processing all core points, any remaining unlabeled points are marked as noise.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighborhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample. Noise points receive the label -1\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    # Compute the neighborhood for each point\n    neighborhood = [[] for _ in range(n_samples)]\n    for i in range(n_samples):\n        for j in range(n_samples):\n            dist = np.linalg.norm(data[i] - data[j])\n            if dist <= eps:\n                neighborhood[i].append(j)\n    # Identify core points\n    is_core = [len(neighbors) >= min_samples for neighbors in neighborhood]\n    # Initialize labels\n    labels = [-1] * n_samples\n    cluster_id = 0\n    for i in range(n_samples):\n        if labels[i] == -1 and is_core[i]:\n            cluster_id += 1\n            queue = deque()\n            queue.append(i)\n            labels[i] = cluster_id\n            while queue:\n                p = queue.popleft()\n                for q in neighborhood[p]:\n                    if labels[q] == -1:\n                        labels[q] = cluster_id\n                        if is_core[q]:\n                            queue.append(q)\n    return labels\n```\n\n### Explanation\n1. **Neighborhood Calculation**: For each point, we compute the Euclidean distance to every other point and store the indices of points within the specified radius (eps) in a neighborhood list.\n2. **Core Point Identification**: A point is marked as a core point if it has at least `min_samples` neighbors within the specified radius.\n3. **Cluster Expansion**: Using BFS, we start from the first unlabeled core point, label it, and expand the cluster by including all its neighbors. If a neighbor is a core point, it is added to the queue to continue expanding the cluster.\n4. **Noise Labeling**: Points that remain unlabeled after processing all core points are marked as noise with a label of -1.\n\nThis approach ensures that all points in dense regions are grouped into clusters, while points in sparse regions are identified as noise, providing an effective way to perform density-based clustering."}
{"task_id": 561, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Expectation-Maximization (EM) algorithm for a Gaussian Mixture Model (GMM). The goal is to cluster the given data into K components by iteratively refining the model parameters until convergence.\n\n### Approach\n1. **Initialization**: Randomly initialize the parameters for each Gaussian component, including mixture weights, means, and covariance matrices.\n2. **Expectation (E) Step**: Compute the responsibilities (posterior probabilities) of each sample belonging to each component.\n3. **Maximization (M) Step**: Update the parameters (mixture weights, means, and covariance matrices) using the responsibilities computed in the E-step.\n4. **Convergence Check**: Stop the iterations if the change in mixture weights is below a specified tolerance or if the maximum number of iterations is reached.\n5. **Label Assignment**: After convergence, assign each sample to the component with the highest responsibility.\n6. **Deterministic Output**: Sort the components by the first coordinate of their mean and relabel the clusters accordingly to ensure a deterministic output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Implements the Expectation-Maximization algorithm for Gaussian Mixture Models.\n    \n    Args:\n        data: A 2D NumPy array of shape (N, D) containing N samples with D features.\n        k: Number of Gaussian components.\n        max_iter: Maximum number of iterations for the EM algorithm.\n        epsilon: Tolerance for convergence based on the change in mixture weights.\n    \n    Returns:\n        A list of integers representing the cluster label for each sample.\n    \"\"\"\n    if k == 1:\n        return [0] * len(data)\n    \n    N, D = data.shape\n    \n    # Initialize parameters\n    indices = np.random.choice(N, size=k, replace=True)\n    mu = data[indices]\n    sigma = np.array([np.eye(D) for _ in range(k)])\n    pi = np.ones(k) / k\n    \n    for _ in range(max_iter):\n        # E-step: Compute responsibilities\n        responsibilities = np.zeros((N, k))\n        for k_idx in range(k):\n            mu_k = mu[k_idx]\n            sigma_k = sigma[k_idx]\n            inv_sigma_k = np.linalg.inv(sigma_k)\n            det_sigma_k = np.linalg.det(sigma_k)\n            denom = np.sqrt((2 * np.pi) ** D * det_sigma_k)\n            dx = data - mu_k\n            exponent = -0.5 * np.einsum('nd,dc,nc->n', dx, inv_sigma_k, dx)\n            pdf = (1 / denom) * np.exp(exponent)\n            responsibilities[:, k_idx] = pi[k_idx] * pdf\n        \n        # Normalize responsibilities\n        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n        \n        # M-step: Update parameters\n        sum_r = responsibilities.sum(axis=0)\n        new_pi = sum_r / N\n        \n        # Update means\n        new_mu = (data.T.dot(responsibilities)).T / sum_r[:, np.newaxis]\n        \n        # Update covariances\n        new_sigma = np.zeros_like(sigma)\n        for k_idx in range(k):\n            weighted = responsibilities[:, k_idx] * (data - new_mu[k_idx])\n            new_sigma[k_idx] = (weighted.T.dot(weighted)) / sum_r[k_idx]\n        \n        # Check for convergence\n        if np.allclose(pi, new_pi, atol=epsilon):\n            break\n        \n        # Update parameters for next iteration\n        pi = new_pi\n        mu = new_mu\n        sigma = new_sigma\n    \n    # Assign labels based on responsibilities\n    labels = np.argmax(responsibilities, axis=1)\n    \n    # Sort components by the first coordinate of their mean\n    sorted_order = np.argsort(mu[:, 0])\n    new_indices = np.zeros(k, dtype=int)\n    for new_idx, original_idx in enumerate(sorted_order):\n        new_indices[original_idx] = new_idx\n    \n    # Remap labels to ensure deterministic output\n    new_labels = new_indices[labels]\n    \n    return new_labels.tolist()\n```\n\n### Explanation\n1. **Initialization**: The parameters for each Gaussian component are initialized randomly. The means are selected from the data, the covariances are set to identity matrices, and the mixture weights are uniform.\n2. **E-step**: For each sample, the responsibility of each component is computed using the multivariate normal distribution formula. These responsibilities are then normalized.\n3. **M-step**: The parameters are updated using the responsibilities. The mixture weights are updated as the average responsibility, the means are updated as the weighted average of the samples, and the covariances are updated as the weighted average of the sample variances.\n4. **Convergence Check**: The algorithm stops if the change in mixture weights is below the tolerance or if the maximum number of iterations is reached.\n5. **Label Assignment**: Each sample is assigned to the component with the highest responsibility.\n6. **Deterministic Output**: The components are sorted by their mean's first coordinate, and the labels are remapped to ensure a deterministic output.\n\nThis approach ensures that the GMM is trained efficiently and the output is deterministic, meeting the problem constraints."}
{"task_id": 562, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Spectral Clustering algorithm from scratch without using any third-party machine learning libraries. The goal is to cluster a given set of data points into a specified number of clusters using the Gaussian kernel for similarity and the Laplacian matrix for spectral analysis.\n\n### Approach\nThe approach to solve this problem involves several key steps:\n\n1. **Build a Similarity Graph**: Compute the pairwise squared distances between all data points and construct a similarity matrix using the Gaussian kernel.\n2. **Construct the Laplacian Matrix**: Depending on the method specified ('unnormalized' or 'normalized'), compute the Laplacian matrix. For the normalized method, use the symmetric normalized Laplacian.\n3. **Compute Eigenvectors**: Calculate the eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix. If using the normalized method, row-normalize these eigenvectors.\n4. **K-Means Clustering**: Perform k-means clustering on the eigenvectors to assign cluster labels. Use the first `n_cluster` points as initial centroids.\n5. **Relabel Clusters**: Ensure the clusters are relabeled such that the cluster containing the smallest original index gets label 0, the next one label 1, and so on.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\"\"\"\n    if n_cluster == 1:\n        return [0] * data.shape[0]\n    \n    N = data.shape[0]\n    if N == 0:\n        return []\n    \n    # Step 1: Compute pairwise squared distances\n    sum_sq = np.sum(data ** 2, axis=1)\n    D = sum_sq[:, np.newaxis] + sum_sq - 2 * (data @ data.T)\n    \n    # Step 2: Compute similarity matrix using Gaussian kernel\n    W = np.exp(-gamma * D)\n    \n    # Step 3: Construct Laplacian matrix\n    D_deg = np.sum(W, axis=1)\n    L = np.diag(D_deg) - W\n    \n    if method == 'normalized':\n        # Compute D^(-1/2)\n        D_inv_sqrt = np.diag(1.0 / np.sqrt(D_deg))\n        L = D_inv_sqrt @ L @ D_inv_sqrt\n    \n    # Step 4: Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Select the first n_cluster eigenvectors\n    embedding = eigenvectors[:, :n_cluster]\n    \n    # If normalized, row-normalize the embedding\n    if method == 'normalized':\n        row_norms = np.linalg.norm(embedding, axis=1)\n        row_norms[row_norms == 0] = 1e-12  # Avoid division by zero\n        embedding = embedding / row_norms[:, np.newaxis]\n    \n    # Step 5: Run k-means on the embedding\n    def kmeans(data, n_cluster, initial_centroids):\n        centroids = initial_centroids.copy()\n        prev_assignment = None\n        while True:\n            # Compute distances to each centroid\n            distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n            # Find the closest centroid for each point\n            assignment = np.argmin(distances, axis=1)\n            if np.array_equal(assignment, prev_assignment):\n                break\n            prev_assignment = assignment.copy()\n            # Update centroids\n            for i in range(n_cluster):\n                mask = (assignment == i)\n                if np.any(mask):\n                    centroids[i] = np.mean(data[mask], axis=0)\n        return assignment\n    \n    initial_centroids = embedding[:n_cluster, :]\n    assignment = kmeans(embedding, n_cluster, initial_centroids)\n    \n    # Step 6: Relabel clusters based on original indices\n    min_indices = []\n    for label in range(n_cluster):\n        mask = (assignment == label)\n        indices = np.where(mask)[0]\n        if len(indices) == 0:\n            min_idx = -1\n        else:\n            min_idx = np.min(indices)\n        min_indices.append((min_idx, label))\n    \n    # Sort clusters based on the minimum index\n    sorted_min_indices = sorted(min_indices, key=lambda x: x[0])\n    \n    # Create a mapping from old labels to new labels\n    label_mapping = {old_label: new_label for new_label, (min_idx, old_label) in enumerate(sorted_min_indices)}\n    \n    # Relabel the assignment\n    new_labels = [label_mapping[old_label] for old_label in assignment]\n    \n    return new_labels\n```\n\n### Explanation\n1. **Similarity Graph Construction**: The Gaussian kernel is used to compute the similarity between each pair of points, resulting in a fully connected graph.\n2. **Laplacian Matrix**: The Laplacian matrix is constructed to capture the structure of the graph. For the normalized method, the matrix is adjusted to ensure each row sums to zero.\n3. **Eigenvalue Decomposition**: The eigenvectors corresponding to the smallest eigenvalues are computed, which help in reducing the dimensionality of the data while preserving the structure.\n4. **K-Means Clustering**: The reduced-dimensional data is clustered using k-means, starting with the first `n_cluster` points as initial centroids.\n5. **Relabeling**: The clusters are relabeled to ensure consistency based on the original indices, providing a clear and consistent labeling scheme.\n\nThis approach ensures that the data is clustered effectively using spectral methods, providing a robust solution for unsupervised learning tasks."}
{"task_id": 563, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the dual-form Perceptron learning algorithm. This algorithm updates the weight vector indirectly by maintaining a coefficient for each training example and performing updates in the feature-space inner-product. The goal is to classify binary data using this dual formulation.\n\n### Approach\n1. **Initialization**: Start by initializing the coefficient vector `alpha` to zeros and the bias term `b` to zero.\n2. **Gram Matrix Construction**: Compute the Gram matrix `G` where each element `G[i, j]` is the dot product of the feature vectors `X_data[i]` and `X_data[j]`.\n3. **Training Loop**: Iterate up to `max_iter` times. In each iteration, check each training sample to see if it is misclassified. If a sample is misclassified, update the corresponding coefficient and bias term.\n4. **Weight Vector Recovery**: After training, compute the weight vector `w` using the coefficients and feature vectors.\n5. **Rounding**: Round the components of the weight vector and bias term to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N = X_data.shape[0]\n    d = X_data.shape[1]\n    \n    alpha = np.zeros(N)\n    b = 0.0\n    \n    # Compute the Gram matrix\n    G = X_data.dot(X_data.T)\n    \n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            # Compute g for the i-th sample\n            g = np.dot(alpha * y_data, G[:, i])\n            # Check if the sample is misclassified\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        if not updated:\n            break\n    \n    # Compute the weight vector\n    w = (X_data.T * (alpha * y_data)).sum(axis=1)\n    \n    # Round the results\n    w = np.round(w, 4).tolist()\n    b = np.round(b, 4)\n    \n    return (w, b)\n```\n\n### Explanation\n1. **Initialization**: The coefficient vector `alpha` and bias `b` are initialized to zeros.\n2. **Gram Matrix**: The Gram matrix `G` is computed to store the inner products of all pairs of feature vectors, which helps in efficiently computing the dual updates.\n3. **Training Loop**: For each iteration, each sample is checked. If a sample is misclassified, the corresponding coefficient and bias are updated. The loop continues until no updates are needed or the maximum number of iterations is reached.\n4. **Weight Recovery**: The weight vector `w` is derived from the coefficients and feature vectors, ensuring the model can make predictions in the primal space.\n5. **Rounding**: The final weight vector and bias are rounded to four decimal places for precision in the output.\n\nThis approach efficiently trains a Perceptron using the dual formulation, ensuring convergence or termination after the specified maximum iterations."}
{"task_id": 564, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to traverse a decision tree to predict the outcome for a given sample. The decision tree can be either a classification tree or a regression tree, and the traversal rules differ between the two types.\n\n### Approach\n1. **Identify Leaf Node**: If the current node is a leaf node (indicated by the presence of the `y` attribute), return the value stored in `y`.\n2. **Classification Tree Traversal**:\n   - For a classification tree, inspect the feature value at the index specified by `node.label`.\n   - Traverse to the first child whose `x` attribute matches the inspected feature value.\n   - If no matching child is found, default to the second child in the list.\n3. **Regression Tree Traversal**:\n   - For a regression tree, inspect the feature value at the index specified by `node.label`.\n   - Compare this feature value with the split point `s`. If the feature value is less than or equal to `s`, traverse to the first child; otherwise, traverse to the second child.\n\n### Solution Code\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    if hasattr(root, 'y'):\n        return root.y\n    if task_type == 'classification':\n        feature_val = features[root.label]\n        for child in root.child:\n            if child.x == feature_val:\n                return predict_sample(child, features, task_type)\n        # Default to the second child if no match found\n        return predict_sample(root.child[1], features, task_type)\n    else:\n        feature_val = features[root.label]\n        if feature_val <= root.s:\n            return predict_sample(root.child[0], features, task_type)\n        else:\n            return predict_sample(root.child[1], features, task_type)\n```\n\n### Explanation\n- **Leaf Node Check**: The function first checks if the current node is a leaf by looking for the `y` attribute. If found, it returns the prediction value stored in `y`.\n- **Classification Traversal**: For classification, the function inspects the feature value at the index specified by `node.label`. It then iterates through the children to find a match for the feature value. If no match is found, it defaults to the second child.\n- **Regression Traversal**: For regression, the function compares the feature value with the split point `s`. Depending on whether the feature value is less than or equal to `s`, it traverses to the appropriate child node.\n\nThis approach ensures that the tree is traversed correctly according to the specified rules, leading to the correct prediction from the leaf node."}
{"task_id": 565, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a utility function used in the ID3 decision-tree learning algorithm. The goal is to select the feature that maximizes the information gain with respect to the class label. The function should return the index of the best feature and the maximum information gain rounded to six decimal places.\n\n### Approach\n1. **Compute Entropy (H(y))**: The entropy of the label distribution is calculated to measure the impurity of the dataset. This is done using the formula for entropy, which is the sum of -p * log2(p) for each class, where p is the probability of the class.\n\n2. **Compute Conditional Entropy (H(y|A))**: For each feature, we compute the conditional entropy, which measures the impurity of the dataset after splitting by the values of the feature. This involves:\n   - Grouping the data by each unique value of the feature.\n   - Calculating the entropy of each subset formed by these groups.\n   - Weighting each subset's entropy by the proportion of samples in that subset and summing these weighted entropies.\n\n3. **Calculate Information Gain (IG)**: For each feature, the information gain is computed as the difference between the entropy of the labels (H(y)) and the conditional entropy given the feature (H(y|A)).\n\n4. **Select the Best Feature**: Track the feature with the maximum information gain. In case of ties, the feature with the smallest index is chosen.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\"\"\"\n    def compute_entropy(y):\n        counts = np.bincount(y)\n        total = len(y)\n        entropy = 0.0\n        for count in counts:\n            if count == 0:\n                continue\n            p = count / total\n            entropy -= p * math.log2(p)\n        return entropy\n\n    def compute_conditional_entropy(X, y, feature_index):\n        feature = X[:, feature_index]\n        unique_values = np.unique(feature)\n        total = len(y)\n        conditional_entropy = 0.0\n        for value in unique_values:\n            mask = (feature == value)\n            subset_y = y[mask]\n            subset_size = len(subset_y)\n            if subset_size == 0:\n                continue\n            entropy = compute_entropy(subset_y)\n            conditional_entropy += (subset_size / total) * entropy\n        return conditional_entropy\n\n    h_y = compute_entropy(y)\n    n_features = X.shape[1]\n    max_ig = -1\n    best_feature = 0\n\n    for feature_index in range(n_features):\n        h_cond = compute_conditional_entropy(X, y, feature_index)\n        ig = h_y - h_cond\n        if ig > max_ig:\n            max_ig = ig\n            best_feature = feature_index\n        elif ig == max_ig:\n            if feature_index < best_feature:\n                best_feature = feature_index\n\n    max_ig_rounded = round(max_ig, 6)\n    return (best_feature, max_ig_rounded)\n```\n\n### Explanation\n1. **Entropy Calculation**: The `compute_entropy` function calculates the entropy of a given array of class labels. It uses the probabilities of each class to compute the entropy.\n\n2. **Conditional Entropy Calculation**: The `compute_conditional_entropy` function calculates the conditional entropy of the class labels given a specific feature. It groups the data by each unique value of the feature, computes the entropy for each group, and then weights these entropies by the size of each group.\n\n3. **Information Gain Calculation**: For each feature, the information gain is computed as the difference between the entropy of the labels and the conditional entropy given the feature. The feature with the highest information gain is selected, with ties broken by choosing the smallest feature index.\n\nThis approach ensures that we efficiently determine the best feature to split the dataset on, maximizing the information gain and thus optimizing the decision tree construction."}
{"task_id": 566, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a mini version of the Isolation Forest algorithm for outlier detection. The goal is to identify the top \u03b5 fraction of observations with the highest anomaly scores based on the Isolation Forest method.\n\n### Approach\n1. **Subsampling**: For each tree, draw a subsample of the data. If the subsample size is greater than or equal to the number of samples, use the entire dataset.\n2. **Tree Construction**: Build a purely random binary tree for each subsample. The tree is built recursively until the maximum depth (determined by the height limit) or until each node contains at most one sample.\n3. **Path Length Calculation**: For each data point, compute the path length through each tree. The path length is the number of edges traversed until a leaf node is reached, adjusted by a correction factor for the leaf size.\n4. **Anomaly Score Calculation**: Compute the average path length across all trees for each data point. Convert this average path length into an anomaly score using a specific formula.\n5. **Outlier Detection**: Identify the top \u03b5 fraction of data points with the highest anomaly scores as outliers.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    np.random.seed(42)\n    n_samples, n_features = data.shape\n    if n_samples == 0 or epsilon == 0:\n        return []\n    \n    def compute_c(n):\n        if n <= 1:\n            return 0\n        else:\n            return 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n    \n    def build_tree(node_indices, current_depth, data, height_limit):\n        if current_depth >= height_limit or len(node_indices) <= 1:\n            return {'is_leaf': True, 'size': len(node_indices)}\n        feature = np.random.randint(n_features)\n        min_val = data[node_indices, feature].min()\n        max_val = data[node_indices, feature].max()\n        if min_val == max_val:\n            return {'is_leaf': True, 'size': len(node_indices)}\n        threshold = np.random.uniform(min_val, max_val)\n        left_indices = []\n        right_indices = []\n        for idx in node_indices:\n            if data[idx, feature] <= threshold:\n                left_indices.append(idx)\n            else:\n                right_indices.append(idx)\n        left_child = build_tree(left_indices, current_depth + 1, data, height_limit)\n        right_child = build_tree(right_indices, current_depth + 1, data, height_limit)\n        return {\n            'is_leaf': False,\n            'feature': feature,\n            'threshold': threshold,\n            'left': left_child,\n            'right': right_child,\n            'size': len(node_indices)\n        }\n    \n    def compute_path_length(x, tree, data):\n        path_length = 0\n        current_node = tree\n        while True:\n            if current_node['is_leaf']:\n                n = current_node['size']\n                c = compute_c(n)\n                return path_length + c\n            feature = current_node['feature']\n            threshold = current_node['threshold']\n            if x[feature] <= threshold:\n                current_node = current_node['left']\n            else:\n                current_node = current_node['right']\n            path_length += 1\n    \n    average_path = np.zeros(n_samples)\n    \n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            subsample_indices = np.arange(n_samples)\n        else:\n            subsample_indices = np.random.choice(n_samples, sample_size, replace=False)\n        if len(subsample_indices) == 0:\n            continue\n        height_limit = math.ceil(math.log2(len(subsample_indices)))\n        tree = build_tree(subsample_indices, 0, data, height_limit)\n        for i in range(n_samples):\n            x = data[i]\n            pl = compute_path_length(x, tree, data)\n            average_path[i] += pl\n    \n    average_path /= n_trees\n    \n    n = n_samples\n    phi = 2 * math.log(n - 1) - 2 * (n - 1) / n\n    scores = 2 ** (-average_path / phi)\n    \n    k = int(epsilon * n_samples)\n    if k == 0:\n        return []\n    \n    sorted_scores = sorted(scores, reverse=True)\n    threshold = sorted_scores[k-1]\n    \n    outlier_indices = [i for i in range(n_samples) if scores[i] >= threshold]\n    outlier_indices.sort()\n    \n    return outlier_indices\n```\n\n### Explanation\n1. **Subsampling**: For each tree, a subsample of"}
{"task_id": 568, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm. The goal is to learn the weights for each (feature-value, label) pair from the training data and use these weights to predict the labels for the test data.\n\n### Approach\n1. **Build Feature Counts**: We first count the occurrences of each (feature, value, label) tuple in the training data. This helps in constructing the empirical distribution needed for the GIS algorithm.\n2. **Initialize Weights**: Initialize the weights for each (feature, value, label) tuple to zero.\n3. **Iterative Weight Update**: Using the GIS algorithm, iteratively update the weights until the largest absolute change in weights is smaller than a given threshold (epsilon) or the maximum number of iterations (n_iter) is reached. In each iteration:\n   - Compute the expected counts for each (feature, value, label) tuple under the current model.\n   - Update the weights based on the ratio of empirical counts to expected counts.\n4. **Predict Labels**: For each test sample, compute the conditional probabilities for each label using the learned weights and predict the label with the highest probability.\n\n### Solution Code\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Implements a Maximum Entropy classifier using Generalized Iterative Scaling.\"\"\"\n    # Build feature counts\n    feature_counts = defaultdict(int)\n    n_train = len(X_train)\n    for i in range(n_train):\n        x = X_train[i]\n        y = y_train[i]\n        for j in range(len(x)):\n            v = x[j]\n            key = (j, v, y)\n            feature_counts[key] += 1\n\n    # Get unique labels\n    labels = np.unique(y_train).tolist()\n    n_labels = len(labels)\n\n    # Initialize weights\n    weights = defaultdict(float)\n    for key in feature_counts:\n        weights[key] = 0.0\n\n    # Iterate until convergence or max iterations\n    for iter in range(n_iter):\n        # Compute expected counts\n        expected_counts = defaultdict(float)\n        for i in range(n_train):\n            x = X_train[i]\n            # Compute numerators for each label\n            numerators = {}\n            for l in labels:\n                sum_weights = 0.0\n                for j in range(len(x)):\n                    v = x[j]\n                    key = (j, v, l)\n                    sum_weights += weights.get(key, 0.0)\n                numerators[l] = sum_weights\n            # Compute Z with max subtraction to prevent overflow\n            max_num = max(numerators.values())\n            Z = 0.0\n            for l in labels:\n                Z += math.exp(numerators[l] - max_num)\n            # Compute probabilities and update expected counts\n            for l in labels:\n                p = math.exp(numerators[l] - max_num) / Z\n                for j in range(len(x)):\n                    v = x[j]\n                    key = (j, v, l)\n                    expected_counts[key] += p\n\n        # Compute deltas\n        deltas = {}\n        max_delta = 0.0\n        for key in feature_counts:\n            C = feature_counts[key]\n            E = expected_counts.get(key, 0.0)\n            if E == 0:\n                delta = 0.0\n            else:\n                r = C / E\n                delta = math.log(r)\n            deltas[key] = delta\n            if abs(delta) > max_delta:\n                max_delta = abs(delta)\n        # Check for convergence\n        if max_delta < epsilon:\n            break\n        # Update weights\n        for key in deltas:\n            weights[key] += deltas[key]\n\n    # Predict for test set\n    result = []\n    for x in X_test:\n        # Compute numerators for each label\n        numerators = {}\n        for l in labels:\n            sum_weights = 0.0\n            for j in range(len(x)):\n                v = x[j]\n                key = (j, v, l)\n                sum_weights += weights.get(key, 0.0)\n            numerators[l] = sum_weights\n        # Compute Z with max subtraction\n        max_num = max(numerators.values())\n        Z = 0.0\n        for l in labels:\n            Z += math.exp(numerators[l] - max_num)\n        # Find the label with maximum probability\n        max_p = -1\n        predicted = None\n        for l in labels:\n            p = math.exp(numerators[l] - max_num) / Z\n            if p > max_p:\n                max_p = p\n                predicted = l\n        result.append(predicted)\n\n    return result\n```\n\n### Explanation\n1. **Feature Counts**: We count how often each (feature, value, label) tuple appears in the training data. This helps in determining the empirical distribution.\n2. **Weight Initialization**: Weights are initialized to zero for all (feature, value, label) tuples.\n3. **GIS Algorithm**: In each iteration, we compute the expected counts for each (feature, value, label) tuple under the current model. We then update the weights based on the ratio of empirical counts to expected counts. This process continues until the weights converge or the maximum number of iterations is reached.\n4. **Prediction**: For each test sample, we compute the conditional probabilities for each label using the learned weights and predict the label with the highest probability.\n\nThis approach ensures that the model efficiently learns the weights and accurately predicts the labels for new data."}
{"task_id": 569, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to analyze a collection of text documents and produce two results: a list of distinct words with their document-frequency ratios, and a set of the top k most common words based on a given cut-off value.\n\n### Approach\n1. **Extract Words and Count Frequencies**: For each document, extract the unique words and count how many documents each word appears in.\n2. **Calculate Document-Frequency Ratios**: For each word, compute the ratio of the number of documents it appears in to the total number of documents. Round this ratio to four decimal places.\n3. **Sort Words by Frequency**: Sort the words based on their document-frequency ratios in descending order.\n4. **Determine Top Words**: Based on the cut-off value, determine the set of top k words. If the cut-off is `None`, include all words. If the cut-off is 0, return an empty set.\n\n### Solution Code\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    word_counts = Counter()\n    total_docs = len(data)\n    \n    if total_docs == 0:\n        return [], set()\n    \n    for doc in data:\n        words = doc[1]\n        unique_words = set(words)\n        for word in unique_words:\n            word_counts[word] += 1\n    \n    word_ratio = []\n    for word, count in word_counts.items():\n        ratio = count / total_docs\n        ratio_rounded = round(ratio, 4)\n        word_ratio.append((word, ratio_rounded))\n    \n    word_ratio.sort(key=lambda x: -x[1])\n    \n    if cut_off is None:\n        top_words = set(word for word, _ in word_ratio)\n    else:\n        if cut_off <= 0:\n            top_words = set()\n        else:\n            top_k = min(cut_off, len(word_ratio))\n            top_words = set(word for word, _ in word_ratio[:top_k])\n    \n    return word_ratio, top_words\n```\n\n### Explanation\n1. **Extract Words and Count Frequencies**: The function iterates through each document, converts the words into a set to ensure uniqueness within each document, and counts how many documents each word appears in using a `Counter`.\n2. **Calculate Document-Frequency Ratios**: For each word, the ratio of its document frequency to the total number of documents is calculated and rounded to four decimal places.\n3. **Sort Words by Frequency**: The words are sorted in descending order based on their document-frequency ratios.\n4. **Determine Top Words**: Depending on the cut-off value, the function either returns all words, the top k words, or an empty set. If the cut-off is larger than the number of distinct words, all words are included.\n\nThis approach efficiently processes the documents, calculates the required statistics, and handles edge cases such as empty data or zero cut-off values."}
{"task_id": 571, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the weight vector and bias term of a linear Support Vector Machine (SVM) using the given Lagrange multipliers obtained from the Sequential Minimal Optimization (SMO) algorithm. The solution involves using matrix operations to derive these parameters efficiently.\n\n### Approach\n1. **Weight Vector Calculation**: The weight vector \\( \\mathbf{w} \\) is computed using the formula \\( \\mathbf{w} = X^\\top (\\boldsymbol{\\alpha} \\odot \\mathbf{y}) \\), where \\( X \\) is the training matrix, \\( \\boldsymbol{\\alpha} \\) is the vector of Lagrange multipliers, and \\( \\mathbf{y} \\) is the vector of class labels. This involves transposing the training matrix and performing element-wise multiplication followed by a dot product.\n2. **Bias Term Calculation**: The bias term \\( b \\) is computed using the formula \\( b = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i) \\), where \\( n \\) is the number of training samples. This involves computing the dot product of the training matrix with the weight vector, subtracting this from the class labels, summing the results, and then averaging.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    n = X.shape[0]\n    # Compute the weight vector\n    w = X.T.dot(alpha * y)\n    # Compute the bias term\n    Xw = X.dot(w)\n    diff = y - Xw\n    sum_diff = np.sum(diff)\n    b = sum_diff / n\n    # Round the results to 4 decimal places\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    # Convert w to a list\n    w_list = w_rounded.tolist()\n    return (w_list, b_rounded)\n```\n\n### Explanation\n1. **Weight Vector Calculation**: The weight vector \\( \\mathbf{w} \\) is derived by transposing the training matrix \\( X \\) and performing a dot product with the element-wise product of the Lagrange multipliers \\( \\boldsymbol{\\alpha} \\) and the class labels \\( \\mathbf{y} \\). This efficiently computes the sum of the contributions of each training sample scaled by their respective Lagrange multipliers and class labels.\n2. **Bias Term Calculation**: The bias term \\( b \\) is calculated by first computing the dot product of the training matrix \\( X \\) with the weight vector \\( \\mathbf{w} \\). This gives the predicted values for each training sample. The difference between the actual class labels and these predicted values is then computed, summed, and averaged to get the bias term.\n\nThis approach ensures that the weight vector and bias term are computed efficiently using matrix operations, which is crucial for handling large datasets in SVM training. The results are rounded to four decimal places to meet the specified precision requirements."}
{"task_id": 572, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Local Outlier Factor (LOF) algorithm from scratch. The LOF algorithm is a density-based method used to detect outliers in a dataset. An outlier is a sample whose local density is significantly lower than the density of its neighbors. The algorithm involves several steps, including computing pairwise distances, finding nearest neighbors, and calculating reachability distances and local reachability densities.\n\n### Approach\n1. **Compute Pairwise Euclidean Distance Matrix**: For each data point, compute the Euclidean distance to every other data point, resulting in an n x n matrix where n is the number of samples.\n2. **Find k Nearest Neighbors and k-Distance**: For each sample, determine its k nearest neighbors and the distance to its k-th nearest neighbor (k-distance).\n3. **Compute Reachability Distance**: For each pair of points (p, q), the reachability distance is defined as the maximum of q's k-distance and the actual distance between p and q.\n4. **Calculate Local Reachability Density (LRD)**: For each sample, compute the LRD as the ratio of k to the sum of reachability distances to its k nearest neighbors.\n5. **Compute Local Outlier Factor (LOF)**: For each sample, the LOF is the ratio of the average LRD of its k nearest neighbors to its own LRD.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Compute the Local Outlier Factor (LOF) for each sample in the dataset.\"\"\"\n    n = data.shape[0]\n    \n    # Step 1: Compute the full pairwise Euclidean distance matrix\n    distance_matrix = np.sqrt(np.sum((data[:, np.newaxis] - data) ** 2, axis=2))\n    \n    # Step 2: For each sample, find its k nearest neighbors and k-distance\n    k_distances = np.zeros(n)\n    neighbors = [[] for _ in range(n)]\n    for p in range(n):\n        # Create a list of (distance, index) tuples\n        dist_indices = [(distance_matrix[p][j], j) for j in range(n)]\n        # Sort the list by distance\n        dist_indices.sort()\n        # Extract the indices of the k nearest neighbors (excluding self)\n        neighbor_indices = [j for (d, j) in dist_indices[1:k+1]]\n        # The k-distance is the distance to the k-th neighbor\n        k_dist = dist_indices[k][0]\n        k_distances[p] = k_dist\n        neighbors[p] = neighbor_indices\n    \n    # Step 3 and 4: Compute Local Reachability Density (LRD) for each sample\n    lrd = np.zeros(n)\n    for p in range(n):\n        sum_reach = 0.0\n        for q in neighbors[p]:\n            reach = max(k_distances[q], distance_matrix[p][q])\n            sum_reach += reach\n        lrd[p] = k / sum_reach\n    \n    # Step 5: Compute Local Outlier Factor (LOF) for each sample\n    lof = np.zeros(n)\n    for p in range(n):\n        sum_lrd_neighbors = 0.0\n        for q in neighbors[p]:\n            sum_lrd_neighbors += lrd[q]\n        average_lrd = sum_lrd_neighbors / k\n        lof[p] = average_lrd / lrd[p]\n    \n    # Round the LOF scores to four decimal places\n    lof_rounded = np.round(lof, 4)\n    \n    # Determine the outlier indices where LOF > epsilon\n    outlier_indices = [i for i in range(n) if lof_rounded[i] > epsilon]\n    outlier_indices.sort()\n    \n    return (lof_rounded.tolist(), outlier_indices)\n```\n\n### Explanation\n1. **Pairwise Distance Matrix**: The distance matrix is computed using vectorized operations for efficiency, resulting in an n x n matrix where each element represents the Euclidean distance between two points.\n2. **k Nearest Neighbors**: For each point, the distances to all other points are sorted, and the indices of the k nearest neighbors are extracted. The k-distance is the distance to the k-th nearest neighbor.\n3. **Reachability Distance**: For each point and its neighbors, the reachability distance is computed as the maximum of the neighbor's k-distance and the actual distance between the points.\n4. **Local Reachability Density (LRD)**: The LRD for each point is calculated by dividing k by the sum of reachability distances to its k nearest neighbors.\n5. **Local Outlier Factor (LOF)**: The LOF for each point is determined by comparing the average LRD of its neighbors to its own LRD. Points with LOF scores greater than a given threshold (epsilon) are identified as outliers.\n\nThis approach ensures that we accurately detect outliers based on their local density relative to their neighbors, providing a robust method for anomaly detection."}
{"task_id": 574, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate synthetic 2-D data for multiple classes, each with training and validation points. The points for each class are sampled from a uniform distribution within a specific square region. The goal is to ensure reproducibility by setting a seed if provided and to return the data in a specific format.\n\n### Approach\n1. **Seed Handling**: If a seed is provided, set the random seed to ensure reproducibility.\n2. **Data Generation**: For each class, generate training and validation points. Each class's points are sampled from a uniform distribution within a square region centered at a specific point.\n3. **Data Collection**: Collect all generated points into lists and then convert these lists into NumPy arrays.\n4. **Rounding**: Round each coordinate to 4 decimal places to ensure compact and deterministic output.\n5. **Return Format**: Return the data as a tuple of four NumPy arrays: training features, validation features, training labels, and validation labels.\n\n### Solution Code\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    X_train = []\n    X_val = []\n    y_train = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        # Generate training points\n        x_train = [random.uniform(base + 1, base + 9) for _ in range(n_train)]\n        y_train_c = [random.uniform(base + 1, base + 9) for _ in range(n_train)]\n        points_train = list(zip(x_train, y_train_c))\n        X_train.extend(points_train)\n        y_train.extend([c] * n_train)\n        # Generate validation points\n        x_val = [random.uniform(base + 1, base + 9) for _ in range(n_val)]\n        y_val_c = [random.uniform(base + 1, base + 9) for _ in range(n_val)]\n        points_val = list(zip(x_val, y_val_c))\n        X_val.extend(points_val)\n        y_val.extend([c] * n_val)\n    \n    # Convert to NumPy arrays and round to 4 decimal places\n    X_train = np.array(X_train).round(4)\n    X_val = np.array(X_val).round(4)\n    y_train = np.array(y_train)\n    y_val = np.array(y_val)\n    \n    return (X_train, X_val, y_train, y_val)\n```\n\n### Explanation\n1. **Seed Handling**: The function starts by setting the random seed if provided, ensuring that the same seed produces the same results.\n2. **Loop Through Classes**: For each class, the base coordinate is calculated. Points for training and validation are generated within a square region centered at this base.\n3. **Point Generation**: For each class, `n_train` training points and `n_val` validation points are generated using the uniform distribution. These points are collected into lists.\n4. **Conversion and Rounding**: The collected points are converted into NumPy arrays and rounded to 4 decimal places to ensure the output is compact and deterministic.\n5. **Return**: The function returns the training and validation data along with their respective labels as NumPy arrays.\n\nThis approach ensures that the data is generated efficiently and meets the specified requirements for reproducibility and formatting."}
{"task_id": 579, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the classical K-Means clustering algorithm from scratch. The goal is to cluster a given set of d-dimensional points into k groups using the Lloyd-style algorithm. The algorithm must be deterministic, meaning the initial centers are chosen as the first k points in the given order, ensuring reproducibility.\n\n### Approach\nThe K-Means algorithm involves the following steps:\n\n1. **Initialization**: The first k points from the input data are chosen as the initial cluster centers.\n2. **Assignment Step**: For each point, compute the Euclidean distance to each center and assign the point to the nearest center. In case of a tie, the center with the smaller index is chosen.\n3. **Update Step**: Recompute each center as the arithmetic mean of all points assigned to that center. If a center loses all its points, it remains unchanged.\n4. **Convergence Check**: Repeat the assignment and update steps until the assignments stop changing or the maximum number of iterations is reached.\n\nThe algorithm ensures that the results are deterministic and easily comparable by rounding the coordinates of the final cluster centers to 4 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n    centers = X[:k, :].copy()\n    previous_labels = None\n\n    for _ in range(max_iters):\n        # Compute distance matrix\n        distances = np.sqrt(np.sum((X[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2, axis=2))\n        # Compute labels\n        labels = np.argmin(distances, axis=1)\n        # Check for convergence\n        if np.array_equal(labels, previous_labels):\n            break\n        # Update centers\n        new_centers = []\n        for j in range(k):\n            mask = (labels == j)\n            if np.any(mask):\n                new_center = np.mean(X[mask, :], axis=0)\n            else:\n                new_center = centers[j]\n            new_centers.append(new_center)\n        centers = np.array(new_centers)\n        # Update previous labels\n        previous_labels = labels.copy()\n\n    # Round the centers to 4 decimal places\n    rounded_centers = [np.round(center, 4).tolist() for center in centers]\n    # Convert labels to a list of integers\n    labels_list = labels.tolist()\n    return labels_list, rounded_centers\n```\n\n### Explanation\n1. **Initialization**: The first k points from the input data `X` are selected as the initial cluster centers.\n2. **Distance Calculation**: For each point, the Euclidean distance to each center is computed using vectorized operations for efficiency.\n3. **Label Assignment**: Each point is assigned to the nearest center. In case of ties, the center with the smaller index is chosen.\n4. **Center Update**: New centers are computed as the mean of all points assigned to each cluster. If a cluster loses all its points, the center remains unchanged.\n5. **Convergence Check**: The algorithm stops when the labels do not change between iterations or when the maximum number of iterations is reached.\n6. **Result Preparation**: The final cluster centers are rounded to 4 decimal places, and the labels and centers are returned as specified.\n\nThis approach ensures that the K-Means algorithm is implemented efficiently and deterministically, providing clear and comparable results."}
{"task_id": 581, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Elastic-Net regularized linear regression using batch gradient descent. Elastic-Net regression combines both L1 and L2 regularization to prevent overfitting and improve model performance. The goal is to iteratively update the model parameters (weights and bias) to minimize the loss function.\n\n### Approach\n1. **Initialization**: Start with all weights and the bias term set to zero.\n2. **Gradient Descent**: For each iteration, compute the predictions, residuals, and gradients for both the weights and the bias.\n3. **Update Parameters**: Adjust the weights and bias using the computed gradients and the learning rate.\n4. **Regularization**: Incorporate both L1 and L2 penalties into the gradient computation to regularize the model.\n5. **Termination**: Stop after the specified number of iterations and return the rounded weights and bias.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\"\"\"\n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0.0\n\n    if learning_rate == 0 or iterations == 0:\n        return [0.0] * n, 0.0\n\n    for _ in range(iterations):\n        y_pred = X.dot(w) + b\n        r = y - y_pred\n\n        term1 = (-2) * (X.T.dot(r))\n        sign_w = np.sign(w)\n        sign_w[sign_w == 0] = -1\n        term2 = l1_penalty * sign_w\n        term3 = 2 * l2_penalty * w\n\n        gradient_w = (term1 + term2 + term3) / m\n        gradient_b = (-2 * np.sum(r)) / m\n\n        w = w - learning_rate * gradient_w\n        b = b - learning_rate * gradient_b\n\n    weights = [round(num, 4) for num in w]\n    bias = round(b, 4)\n    return (weights, bias)\n```\n\n### Explanation\n1. **Initialization**: The weights `w` are initialized to a zero vector of size `n` (number of features), and the bias `b` is initialized to zero.\n2. **Gradient Descent Loop**: For each iteration, compute the predicted values `y_pred` using the current weights and bias. The residuals `r` are calculated as the difference between the actual values `y` and the predicted values `y_pred`.\n3. **Gradient Calculation**: \n   - **Term1**: Computes the gradient contribution from the least squares loss, scaled by -2 and the transpose of the feature matrix.\n   - **Term2**: Computes the L1 regularization gradient, using the sign of the weights (with zero treated as -1).\n   - **Term3**: Computes the L2 regularization gradient, scaled by twice the L2 penalty.\n4. **Update Parameters**: The gradients are averaged over the number of samples `m` and used to update the weights and bias with the learning rate.\n5. **Termination and Rounding**: After completing the specified iterations, the weights and bias are rounded to four decimal places and returned.\n\nThis approach ensures that the model is trained efficiently using batch gradient descent while incorporating both L1 and L2 regularization to prevent overfitting."}
{"task_id": 591, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a forward pass through a fully-connected feed-forward neural network with given weights and an input vector. The network uses ReLU activation for all hidden layers and a linear activation for the output layer. The goal is to compute the network's prediction and return it rounded to four decimal places.\n\n### Approach\n1. **Input Handling**: Convert the input vector `x` into a numpy array for efficient computation.\n2. **Layer Processing**: For each layer in the network:\n   - **Augment Input**: Add a bias term (1) to the current input vector.\n   - **Dot Product**: Compute the dot product of the augmented input with the weight matrix of the current layer.\n   - **Activation Function**: Apply ReLU activation to the output if it is not the last layer.\n3. **Output Handling**: After processing all layers, convert the output to a list of floats, round each value to four decimal places, and return the result as a single float if there is only one output neuron, or as a list of floats otherwise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\"\"\"\n    current_input = np.array(x, dtype=np.float64)\n    for L in range(len(weights)):\n        # Augment the current input with a bias term\n        augmented_input = np.insert(current_input, 0, 1)\n        # Convert the layer's weights to a numpy array\n        layer_weights = np.array(weights[L])\n        # Compute the dot product\n        output = np.dot(layer_weights, augmented_input)\n        # Apply ReLU activation if not the last layer\n        if L != len(weights) - 1:\n            output = np.maximum(output, 0)\n        # Update current_input for the next layer\n        current_input = output\n    # Convert the output to a list of floats\n    output = current_input.tolist()\n    # Round each value to four decimal places\n    rounded_output = [round(val, 4) for val in output]\n    # Return as a single float if only one output, else as a list\n    return rounded_output[0] if len(rounded_output) == 1 else rounded_output\n```\n\n### Explanation\n- **Input Handling**: The input vector `x` is converted to a numpy array to facilitate vectorized operations.\n- **Layer Processing**: For each layer, the input is augmented with a bias term. The dot product with the weight matrix is computed, and ReLU activation is applied if it is not the last layer.\n- **Output Handling**: The final output is converted to a list of floats, each rounded to four decimal places. If there is only one output neuron, the result is returned as a single float; otherwise, it is returned as a list of floats.\n\nThis approach ensures that the network's prediction is computed efficiently and accurately, adhering to the specified constraints and requirements."}
{"task_id": 595, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the K-Means clustering algorithm from scratch. The goal is to compute the centroids of clusters after the algorithm converges and return them as a sorted list of lists, each rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start by initializing the centroids with the first `k` samples from the input data `X`. This ensures deterministic results.\n2. **Iterative Process**: For each iteration up to a maximum of `max_iters`:\n   - **Distance Calculation**: Compute the squared Euclidean distance between each sample and each centroid.\n   - **Assignment**: Assign each sample to the nearest centroid based on the computed distances.\n   - **Update Centroids**: Calculate the new centroids as the mean of the samples assigned to each cluster. If a cluster becomes empty, retain its previous centroid.\n   - **Early Stopping**: Check if all centroids have moved less than `1e-4` from their previous positions. If so, stop the iteration early.\n3. **Post-Processing**: After convergence, round each centroid's coordinates to four decimal places, sort them lexicographically, and return them as a list of lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Implement K-Means clustering algorithm to compute centroids.\"\"\"\n    centroids = X[:k].copy()\n    for _ in range(max_iters):\n        # Compute distance matrix\n        distances = np.sum((X[:, np.newaxis, :] - centroids) ** 2, axis=2)\n        # Assign each sample to the nearest centroid\n        assignments = np.argmin(distances, axis=1)\n        # Compute new centroids\n        new_centroids = []\n        for i in range(k):\n            mask = (assignments == i)\n            if np.any(mask):\n                new_centroid = np.mean(X[mask], axis=0)\n            else:\n                new_centroid = centroids[i]\n            new_centroids.append(new_centroid)\n        new_centroids = np.array(new_centroids)\n        # Check for early stopping\n        max_movement = np.max(np.linalg.norm(new_centroids - centroids, axis=1))\n        if max_movement < 1e-4:\n            break\n        # Update centroids\n        centroids = new_centroids\n    # Round, sort, and return\n    rounded_centroids = np.round(centroids, decimals=4)\n    rounded_centroids = rounded_centroids.tolist()\n    rounded_centroids.sort()\n    return rounded_centroids\n```\n\n### Explanation\n1. **Initialization**: The centroids are initialized using the first `k` samples from `X`.\n2. **Distance Calculation**: For each sample, the squared Euclidean distance to each centroid is computed using vectorized operations for efficiency.\n3. **Assignment**: Each sample is assigned to the nearest centroid by finding the index of the minimum distance.\n4. **Update Centroids**: New centroids are computed as the mean of the samples in each cluster. If a cluster has no samples, its centroid remains unchanged.\n5. **Early Stopping**: The algorithm stops early if the maximum movement of any centroid is less than `1e-4`.\n6. **Post-Processing**: The centroids are rounded to four decimal places, sorted lexicographically, and returned as a list of lists.\n\nThis approach ensures that the K-Means algorithm is implemented efficiently and correctly, adhering to the specified constraints and requirements."}
