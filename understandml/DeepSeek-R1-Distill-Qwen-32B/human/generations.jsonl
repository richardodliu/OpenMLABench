{"task_id": 1, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the dot product of a matrix and a vector. The function should return the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n\n### Approach\n1. **Check for Empty Matrix**: If the matrix `a` is empty, the resulting vector can only be valid if the vector `b` is also empty. Otherwise, return -1.\n2. **Determine Number of Columns**: For a non-empty matrix, the number of columns is determined by the length of the first row.\n3. **Validate Matrix Structure**: Ensure all rows in the matrix have the same number of columns as the first row. If not, return -1.\n4. **Check Vector Length Compatibility**: Verify that the length of the vector `b` matches the number of columns in the matrix. If not, return -1.\n5. **Compute Dot Product**: For each row in the matrix, compute the dot product with the vector and store the result in a list.\n\n### Solution Code\n```python\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a:\n        return [] if not b else -1\n    \n    num_cols = len(a[0])\n    \n    for row in a:\n        if len(row) != num_cols:\n            return -1\n    \n    if len(b) != num_cols:\n        return -1\n    \n    result = []\n    for row in a:\n        dot = 0\n        for i in range(num_cols):\n            dot += row[i] * b[i]\n        result.append(dot)\n    \n    return result\n```\n\n### Explanation\n1. **Empty Matrix Check**: The function first checks if the matrix `a` is empty. If it is, the function returns an empty list if `b` is also empty, otherwise, it returns -1.\n2. **Column Count Validation**: The number of columns in the matrix is determined by the length of the first row. Each subsequent row is checked to ensure it has the same number of columns.\n3. **Vector Length Check**: The function ensures that the length of the vector `b` matches the number of columns in the matrix. If not, it returns -1.\n4. **Dot Product Calculation**: For each row in the matrix, the function computes the dot product with the vector `b` by summing the products of corresponding elements. The results are collected in a list and returned.\n\nThis approach ensures that all edge cases, such as empty matrices and vectors, as well as matrices with varying row lengths, are handled correctly."}
{"task_id": 2, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transpose of a given matrix. The transpose of a matrix is obtained by converting its rows into columns and its columns into rows. This means that the element at position (i, j) in the original matrix will be at position (j, i) in the transposed matrix.\n\n### Approach\nThe approach to solve this problem efficiently involves using Python's built-in `zip` function. The `zip` function can be used to transpose a matrix by unpacking the rows of the matrix and then zipping them together. This method is both concise and efficient, handling all edge cases such as empty matrices and rectangular matrices.\n\n1. **Check for Empty Matrix**: If the input matrix is empty, the function should return an empty list immediately.\n2. **Transpose Using Zip**: For a non-empty matrix, use the `zip` function to transpose the matrix. The `zip` function takes the rows of the matrix as arguments and returns tuples of the columns. These tuples are then converted back to lists to form the transposed matrix.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]\n```\n\n### Explanation\n- **Using Zip for Transpose**: The `zip(*a)` expression unpacks the rows of the matrix `a` and zips them together, effectively transposing the matrix. Each resulting tuple from `zip` represents a column of the original matrix, which becomes a row in the transposed matrix.\n- **Handling Empty Matrix**: If the input matrix `a` is empty, `zip(*a)` will produce an empty iterator, and the list comprehension will result in an empty list, which is the correct transpose of an empty matrix.\n\nThis approach ensures that the solution is both efficient and easy to understand, leveraging Python's built-in functions to achieve the desired result with minimal code."}
{"task_id": 3, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to reshape a given matrix into a specified shape if possible. If the reshaping is not possible, we return an empty list.\n\n### Approach\n1. **Calculate Total Elements**: First, we compute the total number of elements in the given matrix. This is done by summing the lengths of each row in the matrix.\n2. **Check Reshaping Feasibility**: We then check if the product of the dimensions of the new shape matches the total number of elements. If they do not match, reshaping is not possible, and we return an empty list.\n3. **Reshape Using Numpy**: If reshaping is feasible, we convert the matrix into a numpy array, reshape it to the specified new shape, and then convert it back to a list of lists using numpy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    total_elements = sum(len(row) for row in a)\n    if new_shape[0] * new_shape[1] != total_elements:\n        return []\n    arr = np.array(a)\n    reshaped = arr.reshape(new_shape)\n    return reshaped.tolist()\n```\n\n### Explanation\n1. **Calculate Total Elements**: The total number of elements in the matrix is calculated by summing the lengths of each row. This ensures we account for all elements, even if the matrix is not rectangular.\n2. **Check Feasibility**: By comparing the product of the new shape dimensions with the total elements, we determine if reshaping is possible. If they do not match, we return an empty list.\n3. **Reshape and Convert**: Using numpy, we efficiently reshape the matrix and convert it back to a list of lists, ensuring the output is in the correct format.\n\nThis approach ensures that we handle all edge cases, including empty matrices and non-rectangular matrices, efficiently and correctly."}
{"task_id": 4, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates the mean of a matrix either by row or by column based on a specified mode. The function will take a matrix (a list of lists of floats) and a mode ('row' or 'column') as inputs and return a list of means according to the specified mode.\n\n### Approach\n1. **Problem Analysis**:\n   - The function needs to handle two modes: 'row' and 'column'.\n   - For the 'row' mode, the function calculates the mean of each row and returns a list of these means.\n   - For the 'column' mode, the function calculates the mean of each column and returns a list of these means.\n\n2. **Intuitions and Insights**:\n   - For the 'row' mode, iterate through each row, compute the mean by summing the elements of the row and dividing by the number of elements, and collect these means.\n   - For the 'column' mode, first check if the matrix is empty. If not, determine the number of columns, then for each column index, collect all elements from each row at that index, compute the mean, and collect these means.\n\n3. **Algorithm Selection**:\n   - Use a loop to iterate through each row for the 'row' mode.\n   - Use list comprehensions to efficiently collect elements for each column and compute their means for the 'column' mode.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        means = []\n        for row in matrix:\n            mean = sum(row) / len(row)\n            means.append(mean)\n        return means\n    else:\n        if not matrix:\n            return []\n        n_cols = len(matrix[0])\n        means = []\n        for i in range(n_cols):\n            column = [row[i] for row in matrix]\n            mean = sum(column) / len(column)\n            means.append(mean)\n        return means\n```\n\n### Explanation\n- **Row Mode**: The function iterates over each row in the matrix. For each row, it calculates the mean by summing the elements and dividing by the number of elements. These means are collected into a list and returned.\n- **Column Mode**: The function first checks if the matrix is empty. If not, it determines the number of columns. For each column index, it collects all elements from each row at that index, calculates the mean, and collects these means into a list which is then returned.\n\nThis approach ensures that the function efficiently handles both row and column mean calculations, providing the correct results for valid inputs."}
{"task_id": 5, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies each element of a given matrix by a scalar value and returns the resulting matrix. This operation is known as scalar multiplication of a matrix.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Initialize an empty result matrix**: This will store the resulting matrix after scalar multiplication.\n2. **Iterate through each row of the input matrix**: For each row, create a new row where each element is the product of the original element and the scalar.\n3. **Multiply each element by the scalar**: For each element in the row, compute the product with the scalar and store it in the new row.\n4. **Append the new row to the result matrix**: After processing all elements in a row, add the new row to the result matrix.\n5. **Return the result matrix**: After processing all rows, return the result matrix.\n\nThis approach ensures that each element of the matrix is processed exactly once, making the solution efficient with a time complexity of O(n*m), where n is the number of rows and m is the number of columns in the matrix.\n\n### Solution Code\n```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = []\n        for num in row:\n            new_row.append(num * scalar)\n        result.append(new_row)\n    return result\n```\n\n### Explanation\n- **Initialization**: The function starts by initializing an empty list `result` to store the resulting matrix.\n- **Row Iteration**: For each row in the input matrix, a new row `new_row` is initialized.\n- **Element Multiplication**: Each element in the current row is multiplied by the scalar value and appended to `new_row`.\n- **Row Addition**: After processing all elements in the current row, `new_row` is appended to `result`.\n- **Return Result**: Finally, the function returns the `result` matrix, which contains all elements multiplied by the scalar.\n\nThis solution efficiently handles all edge cases, including empty matrices and matrices with rows of varying lengths, ensuring robustness and correctness."}
{"task_id": 6, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the eigenvalues of a given 2x2 matrix and return them sorted from highest to lowest. Eigenvalues are crucial in various fields of mathematics and engineering, providing insights into the behavior of linear transformations.\n\n### Approach\n1. **Extract Matrix Elements**: The given matrix is a 2x2 matrix, so we extract its elements a, b, c, and d.\n2. **Compute Trace and Determinant**: The trace of the matrix is the sum of its diagonal elements (a + d), and the determinant is calculated as (ad - bc).\n3. **Characteristic Equation**: The eigenvalues are the roots of the characteristic equation, which for a 2x2 matrix is a quadratic equation. The roots are found using the quadratic formula.\n4. **Quadratic Formula**: Using the trace and determinant, we compute the discriminant. The eigenvalues are then derived using the quadratic formula.\n5. **Sort Eigenvalues**: The eigenvalues are sorted in descending order before returning.\n\n### Solution Code\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    \n    sqrt_discriminant = math.sqrt(discriminant)\n    eigen1 = (trace + sqrt_discriminant) / 2\n    eigen2 = (trace - sqrt_discriminant) / 2\n    \n    eigenvalues = [eigen1, eigen2]\n    eigenvalues.sort(reverse=True)\n    \n    return eigenvalues\n```\n\n### Explanation\n1. **Extracting Elements**: The matrix elements are extracted into variables a, b, c, and d.\n2. **Trace and Determinant**: The trace is the sum of the diagonal elements, and the determinant is computed using the formula for a 2x2 matrix.\n3. **Discriminant Calculation**: The discriminant of the characteristic equation is calculated to determine the nature of the eigenvalues.\n4. **Eigenvalue Calculation**: Using the quadratic formula, the eigenvalues are computed. The eigenvalues are then sorted in descending order to meet the problem's requirements.\n\nThis approach efficiently computes the eigenvalues using fundamental properties of matrices and quadratic equations, ensuring the solution is both correct and optimal for 2x2 matrices."}
{"task_id": 7, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to transform a given matrix \\( A \\) using the operation \\( T^{-1}AS \\), where \\( T \\) and \\( S \\) are invertible matrices. The function should first validate if \\( T \\) and \\( S \\) are invertible and if the dimensions of the matrices are compatible for the transformation. If any of these conditions are not met, the function should return -1.\n\n### Approach\n1. **Check if \\( T \\) and \\( S \\) are square matrices**: Since only square matrices can be invertible, we first check if \\( T \\) and \\( S \\) are square. If either is not square, return -1.\n2. **Check invertibility of \\( T \\) and \\( S \\)**: Compute the determinant of \\( T \\) and \\( S \\). If either determinant is close to zero (within a small threshold to account for floating-point precision), the matrix is not invertible, and we return -1.\n3. **Check matrix dimensions compatibility**: Ensure that the number of rows of \\( A \\) matches the number of rows of \\( T \\) and that the number of columns of \\( A \\) matches the number of rows of \\( S \\). If not, return -1.\n4. **Compute the transformation**: If all checks pass, compute \\( T^{-1} \\), then perform the matrix multiplications \\( T^{-1}A \\) and \\( (T^{-1}A)S \\).\n5. **Round the result**: Round each element of the resulting matrix to four decimal places.\n6. **Return the result**: Convert the resulting numpy array to a Python list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    \n    # Check if T and S are square matrices\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by computing their determinants\n    det_T = np.linalg.det(T_np)\n    if abs(det_T) < 1e-10:\n        return -1\n    det_S = np.linalg.det(S_np)\n    if abs(det_S) < 1e-10:\n        return -1\n    \n    # Check if the dimensions of A are compatible with T and S\n    if A_np.shape[0] != T_np.shape[0] or A_np.shape[1] != S_np.shape[0]:\n        return -1\n    \n    # Compute the inverse of T\n    T_inv = np.linalg.inv(T_np)\n    \n    # Perform the matrix multiplications\n    temp = T_inv @ A_np\n    result = temp @ S_np\n    \n    # Round the result to 4 decimal places\n    result = np.round(result, 4)\n    \n    # Convert the numpy array to a Python list\n    return result.tolist()\n```\n\n### Explanation\nThe function begins by converting the input matrices into numpy arrays for easier manipulation. It then checks if \\( T \\) and \\( S \\) are square matrices and if they are invertible by computing their determinants. If any matrix is not invertible or the dimensions are incompatible, the function returns -1. If all checks pass, it computes the inverse of \\( T \\), performs the matrix multiplications, rounds the result to four decimal places, and returns the transformed matrix as a Python list. This approach ensures that all edge cases are handled efficiently and correctly."}
{"task_id": 8, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the inverse of a 2x2 matrix. If the matrix is not invertible, we should return `None`.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Extract Matrix Elements**: Extract the elements of the 2x2 matrix into variables `a`, `b`, `c`, and `d`.\n2. **Calculate the Determinant**: Compute the determinant of the matrix using the formula `ad - bc`. If the determinant is zero, the matrix is not invertible, and we return `None`.\n3. **Compute the Inverse**: If the determinant is non-zero, compute the inverse using the formula for the inverse of a 2x2 matrix. The inverse is given by scaling the matrix `[[d, -b], [-c, a]]` by the reciprocal of the determinant.\n\n### Solution Code\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    return [\n        [d * inv_det, -b * inv_det],\n        [-c * inv_det, a * inv_det]\n    ]\n```\n\n### Explanation\n1. **Extract Matrix Elements**: The elements of the matrix are extracted into variables `a`, `b`, `c`, and `d` for easier manipulation.\n2. **Calculate the Determinant**: The determinant is calculated using the formula `a*d - b*c`. If the determinant is zero, the matrix does not have an inverse, and we return `None`.\n3. **Compute the Inverse**: If the determinant is non-zero, the inverse matrix is computed by scaling each element of the matrix `[[d, -b], [-c, a]]` by the reciprocal of the determinant. This ensures that the resulting matrix is the correct inverse of the input matrix.\n\nThis approach efficiently computes the inverse of a 2x2 matrix using basic arithmetic operations and conditional checks, ensuring correctness and handling non-invertible matrices appropriately."}
{"task_id": 9, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that multiplies two matrices. If the matrices cannot be multiplied, the function should return -1. \n\n### Approach\n1. **Check for Valid Matrices**: Ensure both matrices are non-empty and each row in both matrices has the same length.\n2. **Check Multiplication Compatibility**: Verify that the number of columns in the first matrix is equal to the number of rows in the second matrix. If not, return -1.\n3. **Matrix Multiplication**: If the matrices are compatible, compute the product matrix. Each element of the product matrix is the dot product of the corresponding row from the first matrix and column from the second matrix.\n\n### Solution Code\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if either matrix is empty\n    if not a or not b:\n        return -1\n    \n    # Check if all rows in a have the same length\n    rows_a = len(a)\n    cols_a = len(a[0])\n    for row in a:\n        if len(row) != cols_a:\n            return -1\n    \n    # Check if all rows in b have the same length\n    rows_b = len(b)\n    cols_b = len(b[0])\n    for row in b:\n        if len(row) != cols_b:\n            return -1\n    \n    # Check if multiplication is possible\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = []\n    for i in range(rows_a):\n        new_row = []\n        for j in range(cols_b):\n            sum_val = 0\n            for k in range(cols_a):\n                sum_val += a[i][k] * b[k][j]\n            new_row.append(sum_val)\n        result.append(new_row)\n    \n    return result\n```\n\n### Explanation\n1. **Check for Valid Matrices**: The function first checks if either matrix is empty. It then ensures that all rows in each matrix have the same length, which is a requirement for a valid matrix.\n2. **Check Multiplication Compatibility**: The function verifies if the number of columns in the first matrix matches the number of rows in the second matrix. If not, it returns -1.\n3. **Matrix Multiplication**: If the matrices are compatible, the function initializes an empty result matrix. It then iterates through each element of the result matrix, computing each element as the dot product of the corresponding row from the first matrix and column from the second matrix. The result is then returned.\n\nThis approach ensures that the function handles all edge cases, including empty matrices and matrices with varying row lengths, and correctly computes the product when possible."}
{"task_id": 10, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the covariance matrix for a given set of vectors. The covariance matrix is a square matrix where each element (i, j) represents the covariance between the i-th and j-th features. The covariance measures how much two random variables change together.\n\n### Approach\n1. **Problem Analysis**: The input is a list of lists, where each inner list represents a feature vector with its observations. The goal is to compute the covariance matrix, which is an n x n matrix where n is the number of features.\n2. **Covariance Calculation**: For each pair of features (i, j), the covariance is calculated using the formula:\n   \\[\n   \\text{Cov}(X, Y) = \\frac{1}{m-1} \\sum_{k=1}^{m} (X_k - \\bar{X})(Y_k - \\bar{Y})\n   \\]\n   where \\( \\bar{X} \\) and \\( \\bar{Y} \\) are the means of vectors X and Y, respectively, and m is the number of observations.\n3. **Matrix Initialization**: Initialize an n x n matrix to store the covariance values.\n4. **Iterate Over Pairs**: For each pair of features (i, j), compute the covariance and store it in the matrix.\n\n### Solution Code\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n = len(vectors)\n    if n == 0:\n        return []\n    m = len(vectors[0])\n    covariance_matrix = [[0.0 for _ in range(n)] for _ in range(n)]\n    \n    for i in range(n):\n        for j in range(n):\n            sum_product = 0.0\n            mean_i = sum(vectors[i]) / m\n            mean_j = sum(vectors[j]) / m\n            for k in range(m):\n                sum_product += (vectors[i][k] - mean_i) * (vectors[j][k] - mean_j)\n            covariance = sum_product / (m - 1)\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of features (n) and the number of observations (m) in each feature vector.\n2. **Matrix Setup**: An n x n matrix is initialized to store the covariance values.\n3. **Covariance Calculation**: For each pair of features (i, j), the mean of each feature vector is computed. The covariance is then calculated by summing the product of deviations from the mean for each observation, divided by (m-1) to get the sample covariance.\n4. **Result Storage**: Each computed covariance value is stored in the corresponding position in the covariance matrix.\n\nThis approach ensures that we correctly compute the covariance matrix by iterating over each pair of features and applying the covariance formula, resulting in an efficient and clear solution."}
{"task_id": 11, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Jacobi method to iteratively approximate the solution of a system of linear equations given by Ax = b. The Jacobi method is an iterative algorithm that updates each component of the solution vector based on the previous iteration's values.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the solution vector, typically a zero vector.\n2. **Iteration**: For a specified number of iterations, compute the new solution vector using the formula:\n   \\[\n   x_i^{(k+1)} = \\frac{b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)}}{A_{ii}}\n   \\]\n   where \\( x_i^{(k+1)} \\) is the new value of the i-th component, and \\( x_j^{(k)} \\) are the values from the previous iteration.\n3. **Rounding**: After each iteration, round the solution vector to four decimal places to ensure precision.\n4. **Return Result**: After completing the specified number of iterations, return the solution vector as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    m = len(b)\n    x = np.zeros(m)\n    for _ in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            s = np.dot(A[i], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The solution vector `x` is initialized to zeros with the same length as the vector `b`.\n2. **Iteration Loop**: For each iteration, a new solution vector `x_new` is computed. Each component of `x_new` is calculated using the current values of `x` from the previous iteration.\n3. **Sum Calculation**: For each component `i`, the sum of the products of the corresponding row elements of `A` and the current solution vector `x` (excluding the diagonal element) is computed.\n4. **Update Rule**: The new value for each component is derived by subtracting this sum from the corresponding element of `b` and dividing by the diagonal element of `A`.\n5. **Rounding**: After computing the new solution vector, it is rounded to four decimal places to maintain precision.\n6. **Result Conversion**: The final solution vector is converted to a list and returned.\n\nThis approach ensures that the solution is iteratively refined and meets the precision requirement specified by rounding to four decimal places after each iteration."}
{"task_id": 12, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the singular values of a 2x2 matrix using the Jacobian method without relying on numpy's built-in SVD function. The singular values are derived from the eigenvalues of the matrix \\( A^T A \\), which is a symmetric positive semi-definite matrix. \n\n### Approach\n1. **Compute \\( A^T A \\)**: This matrix is obtained by transposing matrix \\( A \\) and then multiplying it by \\( A \\).\n2. **Calculate Trace and Determinant**: The trace of \\( A^T A \\) is the sum of its diagonal elements, and the determinant is computed using the formula \\( ad - bc \\) for a 2x2 matrix.\n3. **Solve Quadratic Equation**: The eigenvalues of \\( A^T A \\) are found by solving the quadratic equation derived from its trace and determinant.\n4. **Compute Singular Values**: The singular values are the square roots of these eigenvalues, sorted in descending order.\n5. **Round Results**: The results are rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Compute A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Extract elements of ATA\n    a, b = ATA[0, 0], ATA[0, 1]\n    c, d = ATA[1, 0], ATA[1, 1]\n    \n    # Compute trace and determinant\n    trace = a + d\n    det = a * d - b * c\n    \n    # Compute discriminant\n    discriminant = trace ** 2 - 4 * det\n    sqrt_discriminant = np.sqrt(discriminant)\n    \n    # Compute eigenvalues\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    \n    # Compute singular values\n    singular_values = [np.sqrt(eigenvalue1), np.sqrt(eigenvalue2)]\n    \n    # Sort singular values in descending order\n    singular_values.sort(reverse=True)\n    \n    # Round to 4 decimal places\n    singular_values = [round(sv, 4) for sv in singular_values]\n    \n    return tuple(singular_values)\n```\n\n### Explanation\n1. **Matrix Transpose and Multiplication**: The matrix \\( A^T A \\) is computed to ensure we work with a symmetric matrix, which simplifies finding eigenvalues.\n2. **Trace and Determinant**: These are essential for solving the characteristic equation of the matrix \\( A^T A \\).\n3. **Quadratic Equation**: Solving this gives the eigenvalues, which are then used to find the singular values by taking their square roots.\n4. **Sorting and Rounding**: Ensures the singular values are presented in the correct order and with the required precision.\n\nThis approach efficiently computes the singular values using fundamental matrix operations and properties, providing a clear and precise solution."}
{"task_id": 13, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the determinant of a 4x4 matrix using Laplace's Expansion method. This method involves expanding along a row or column, computing the minors, and then recursively calculating their determinants. For a 4x4 matrix, the minors will be 3x3 matrices, so we will use a helper function to compute their determinants.\n\n### Approach\n1. **Laplace's Expansion**: We will expand along the first row of the 4x4 matrix. For each element in this row, we will compute its cofactor, which involves the determinant of the minor matrix formed by removing the corresponding row and column.\n2. **Minor Matrix Creation**: For each element in the first row, we create a minor matrix by removing the first row and the corresponding column.\n3. **3x3 Determinant Calculation**: We use a helper function to compute the determinant of each 3x3 minor matrix using the direct formula for a 3x3 matrix.\n4. **Summation of Cofactors**: Each cofactor is multiplied by the corresponding element and the appropriate sign, then summed to get the determinant of the 4x4 matrix.\n\n### Solution Code\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    def determinant_3x3(m):\n        a, b, c = m[0]\n        d, e, f = m[1]\n        g, h, i = m[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    \n    det = 0.0\n    for j in range(4):\n        minor = []\n        for row in matrix[1:]:\n            minor_row = row[:j] + row[j+1:]\n            minor.append(minor_row)\n        minor_det = determinant_3x3(minor)\n        sign = (-1) ** j\n        det += matrix[0][j] * sign * minor_det\n    return det\n```\n\n### Explanation\n1. **Helper Function for 3x3 Matrix**: The `determinant_3x3` function computes the determinant of a 3x3 matrix using the formula:\n   \\[\n   \\text{det} = a(ei - fh) - b(di - fg) + c(dh - eg)\n   \\]\n   where the matrix is:\n   \\[\n   \\begin{bmatrix}\n   a & b & c \\\\\n   d & e & f \\\\\n   g & h & i \\\\\n   \\end{bmatrix}\n   \\]\n\n2. **Main Function for 4x4 Matrix**: The `determinant_4x4` function iterates over each element in the first row of the 4x4 matrix. For each element, it constructs the corresponding 3x3 minor matrix by excluding the first row and the current column. It then computes the determinant of this minor matrix using the helper function and accumulates the result with the appropriate sign.\n\n3. **Sign Calculation**: The sign for each cofactor is determined by \\((-1)^{i+j}\\), where \\(i\\) is the row index (0 for the first row) and \\(j\\) is the column index. This results in signs of [1, -1, 1, -1] for the first row.\n\nBy following this approach, we efficiently compute the determinant of a 4x4 matrix using Laplace's Expansion, leveraging a helper function for the 3x3 determinant calculation."}
{"task_id": 14, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using the normal equation method. This method allows us to find the coefficients of a linear regression model directly without using iterative optimization techniques like gradient descent.\n\n### Approach\nThe normal equation method is based on solving the linear regression problem using matrix operations. The key steps are as follows:\n\n1. **Matrix Conversion**: Convert the input features matrix `X` and the target vector `y` into numpy arrays to facilitate matrix operations.\n2. **Compute X Transpose**: Calculate the transpose of the matrix `X`.\n3. **Compute XTX**: Multiply the transpose of `X` by `X` itself to get the matrix `XTX`.\n4. **Inverse of XTX**: Compute the inverse of the matrix `XTX`.\n5. **Compute Theta**: Multiply the inverse of `XTX` by the product of the transpose of `X` and the target vector `y` to get the coefficients `theta`.\n6. **Rounding**: Round each coefficient to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    XTX = X_np.T @ X_np\n    XTX_inv = np.linalg.inv(XTX)\n    theta = XTX_inv @ (X_np.T @ y_np)\n    theta_rounded = [round(num, 4) for num in theta]\n    return theta_rounded\n```\n\n### Explanation\n1. **Matrix Conversion**: The input lists `X` and `y` are converted into numpy arrays to leverage numpy's efficient matrix operations.\n2. **Transpose and Multiplication**: The transpose of `X` is computed and then multiplied by `X` to form the matrix `XTX`. This matrix is crucial as it represents the features' relationships.\n3. **Matrix Inversion**: The inverse of `XTX` is computed. This step is valid under the assumption that `XTX` is invertible, which is typically true if the features are not linearly dependent.\n4. **Theta Calculation**: The coefficients `theta` are obtained by multiplying the inverse of `XTX` with the product of the transpose of `X` and `y`. This step effectively solves the linear system to find the best fit line.\n5. **Rounding**: Each coefficient is rounded to four decimal places to ensure the result meets the specified precision.\n\nThis approach efficiently computes the linear regression coefficients using matrix operations, providing an exact solution without the need for iterative methods."}
{"task_id": 15, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs linear regression using gradient descent. The goal is to find the coefficients (theta) of the linear regression model that minimize the cost function. The function will take NumPy arrays for features and target, along with a learning rate and number of iterations, and return the coefficients as a list rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start with an initial guess for the coefficients (theta) as zeros. The number of coefficients is determined by the number of features in the input array X.\n2. **Gradient Descent Iterations**: For each iteration, compute the predictions using the current coefficients, calculate the error between predictions and actual values, and then update the coefficients using the gradient descent update rule.\n3. **Update Rule**: The update rule involves computing the gradient of the cost function with respect to each coefficient and adjusting the coefficients by a small step (learning rate) in the direction that reduces the cost.\n4. **Rounding and Conversion**: After completing the specified number of iterations, round the coefficients to four decimal places and convert them to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros(n)\n    \n    for _ in range(iterations):\n        h = X.dot(theta)\n        error = h - y\n        gradient = (1/m) * X.T.dot(error)\n        theta = theta - alpha * gradient\n    \n    theta = np.round(theta, 4)\n    return theta.tolist()\n```\n\n### Explanation\n1. **Initialization**: The coefficients (theta) are initialized to zeros. The number of coefficients is determined by the number of features in X.\n2. **Predictions and Error Calculation**: For each iteration, the predictions (h) are computed using the dot product of X and theta. The error is the difference between these predictions and the actual target values y.\n3. **Gradient Calculation**: The gradient of the cost function is computed as the average of the product of the feature matrix transpose and the error vector. This gives the direction in which the coefficients should be adjusted to minimize the cost.\n4. **Coefficient Update**: The coefficients are updated by subtracting the product of the learning rate (alpha) and the gradient from the current coefficients.\n5. **Rounding and Conversion**: After completing all iterations, the coefficients are rounded to four decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that the coefficients are optimized to best fit the linear regression model using gradient descent, providing an efficient and accurate solution."}
{"task_id": 16, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs feature scaling on a dataset using both standardization and min-max normalization. The function will take a 2D NumPy array as input and return two scaled versions of this data as lists of lists, with each value rounded to four decimal places.\n\n### Approach\n1. **Standardization**: This technique transforms the data such that each feature has a mean of 0 and a standard deviation of 1. The formula used is:\n   \\[\n   \\text{standardized}(x) = \\frac{x - \\text{mean}}{\\text{std}}\n   \\]\n   where \\( \\text{mean} \\) is the mean of the feature and \\( \\text{std} \\) is the standard deviation of the feature.\n\n2. **Min-Max Normalization**: This technique scales the data to a range between 0 and 1. The formula used is:\n   \\[\n   \\text{normalized}(x) = \\frac{x - \\text{min}}{\\text{max} - \\text{min}}\n   \\]\n   where \\( \\text{min} \\) is the minimum value of the feature and \\( \\text{max} \\) is the maximum value of the feature.\n\n3. **Rounding and Conversion**: After applying the transformations, each value is rounded to four decimal places. The resulting NumPy arrays are then converted to lists of lists using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = data.mean(axis=0)\n    std = data.std(axis=0)\n    standardized = (data - mean) / std\n    \n    # Min-Max Normalization\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    normalized = (data - min_vals) / (max_vals - min_vals)\n    \n    # Rounding to 4 decimal places\n    standardized_rounded = np.round(standardized, 4)\n    normalized_rounded = np.round(normalized, 4)\n    \n    # Convert to lists\n    standardized_list = standardized_rounded.tolist()\n    normalized_list = normalized_rounded.tolist()\n    \n    return standardized_list, normalized_list\n```\n\n### Explanation\n- **Standardization**: For each feature, the mean and standard deviation are computed. Each value in the feature is then adjusted by subtracting the mean and dividing by the standard deviation.\n- **Min-Max Normalization**: For each feature, the minimum and maximum values are computed. Each value in the feature is then adjusted by subtracting the minimum and dividing by the range (max - min).\n- **Rounding and Conversion**: The results are rounded to four decimal places to ensure precision and then converted to lists for the final output.\n\nThis approach ensures that the data is scaled appropriately for use in various machine learning algorithms, which often require features to be on a similar scale."}
{"task_id": 17, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the k-means clustering algorithm, which is a method used to partition n points into k clusters. The goal is to group similar points together and represent each group by its center, called the centroid. The algorithm iteratively assigns points to the nearest centroid and recalculates the centroids until convergence or a maximum number of iterations is reached.\n\n### Approach\n1. **Initialization**: Convert the input points and initial centroids into numpy arrays for efficient computation.\n2. **Iterative Process**: For each iteration up to the maximum allowed:\n   - **Distance Calculation**: Compute the Euclidean distance from each point to each centroid.\n   - **Cluster Assignment**: Assign each point to the cluster of the nearest centroid.\n   - **Centroid Update**: Calculate new centroids as the mean of all points in each cluster.\n   - **Convergence Check**: If the centroids do not change significantly, break the loop early.\n3. **Rounding**: After the loop, round each centroid to four decimal places.\n4. **Output**: Convert the final centroids into a list of tuples and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Calculate the distance from each point to each centroid\n        distances = np.sqrt(np.sum((points_np[:, np.newaxis] - centroids) ** 2, axis=2))\n        # Assign each point to the closest centroid\n        cluster_indices = np.argmin(distances, axis=1)\n        \n        # Compute new centroids\n        new_centroids = []\n        for i in range(k):\n            cluster_points = points_np[cluster_indices == i]\n            if len(cluster_points) == 0:\n                new_centroid = centroids[i]\n            else:\n                new_centroid = np.mean(cluster_points, axis=0)\n            new_centroids.append(new_centroid)\n        \n        # Check for convergence\n        if np.allclose(centroids, new_centroids):\n            break\n        \n        centroids = np.array(new_centroids)\n    \n    # Round the centroids to four decimal places\n    rounded_centroids = np.round(centroids, decimals=4)\n    \n    # Convert to list of tuples\n    final_centroids = [tuple(row) for row in rounded_centroids]\n    \n    return final_centroids\n```\n\n### Explanation\n- **Initialization**: The points and initial centroids are converted into numpy arrays to leverage efficient array operations.\n- **Distance Calculation**: Using numpy's broadcasting, we compute the Euclidean distance from each point to each centroid efficiently.\n- **Cluster Assignment**: Each point is assigned to the cluster with the nearest centroid by finding the minimum distance.\n- **Centroid Update**: New centroids are calculated as the mean of all points in each cluster. If a cluster has no points, the centroid remains unchanged.\n- **Convergence Check**: The algorithm stops early if the centroids do not change significantly between iterations.\n- **Rounding and Output**: The final centroids are rounded to four decimal places and converted into a list of tuples for the output.\n\nThis approach ensures that the algorithm efficiently converges to the optimal centroids, providing a clear and accurate clustering solution."}
{"task_id": 18, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function for K-Fold Cross-Validation, which is a technique used to assess the performance of a machine learning model. The idea is to divide the dataset into k subsets (folds), and for each fold, use it as the test set while the remaining subsets form the training set. This process is repeated k times, each time with a different fold as the test set.\n\n### Approach\n1. **Check Dataset Size**: Determine the number of samples in the dataset.\n2. **Shuffle Indices**: If shuffling is enabled, randomly permute the indices of the dataset to ensure randomness in the splits.\n3. **Split into Folds**: Divide the indices into k folds. The `np.array_split` function is used to handle both equal and unequal splits, ensuring that the first few folds may have one more element if the dataset size is not perfectly divisible by k.\n4. **Generate Splits**: For each fold, treat it as the test set and combine the remaining folds to form the training set. Collect these splits into a list of tuples, each containing the indices for the training and test sets.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n = X.shape[0]\n    indices = np.arange(n)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(indices)\n    folds = np.array_split(indices, k)\n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate(folds[:i] + folds[i+1:])\n        splits.append((train_indices, test_indices))\n    return splits\n```\n\n### Explanation\n1. **Dataset Size**: The number of samples `n` is determined from the shape of the input array `X`.\n2. **Shuffling**: If shuffling is enabled, the indices are randomly permuted using `np.random.permutation`. The random seed is set for reproducibility if provided.\n3. **Folding**: The indices are split into k folds using `np.array_split`, which handles both equal and unequal distribution of indices across folds.\n4. **Splits Generation**: For each fold, the indices of the current fold are used as the test set, and the remaining indices are concatenated to form the training set. Each split is stored as a tuple in a list, which is returned as the result.\n\nThis approach ensures that each fold is used exactly once as the test set, providing a robust estimate of the model's performance across different subsets of the data."}
{"task_id": 19, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Principal Component Analysis (PCA) from scratch. PCA is a statistical technique used to reduce the dimensionality of data while retaining as much variance as possible. The function will take a 2D NumPy array and an integer `k`, and return the top `k` principal components.\n\n### Approach\nThe PCA implementation involves several key steps:\n\n1. **Standardize the Data**: PCA is sensitive to the variances of the features, so we first standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n\n2. **Compute the Covariance Matrix**: The covariance matrix is computed from the standardized data. This matrix helps in understanding the relationships between the features.\n\n3. **Compute Eigenvalues and Eigenvectors**: The eigenvalues and eigenvectors of the covariance matrix are computed. The eigenvectors represent the principal components, and the eigenvalues represent the variance explained by each principal component.\n\n4. **Sort Eigenvectors by Eigenvalues**: The eigenvectors are sorted in descending order based on their corresponding eigenvalues to identify the principal components.\n\n5. **Select Top k Eigenvectors**: The top `k` eigenvectors are selected as the principal components.\n\n6. **Return Result**: The selected eigenvectors are converted to a list of lists and rounded to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the data\n    mean = np.mean(data, axis=0)\n    centered = data - mean\n    std = np.std(centered, axis=0, ddof=1)\n    standardized = centered / std\n\n    # Compute the covariance matrix\n    n = standardized.shape[0]\n    cov_matrix = (standardized.T @ standardized) / (n - 1)\n\n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n    # Create a list of (eigenvalue, eigenvector) pairs\n    eigen_pairs = [(eigenvalues[i], eigenvectors[:, i]) for i in range(len(eigenvalues))]\n\n    # Sort the eigen_pairs in descending order of eigenvalues\n    eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n\n    # Determine the number of components to return\n    num_components = min(k, len(eigen_pairs))\n    principal_components = [eigen_pairs[i][1] for i in range(num_components)]\n\n    # Convert to list of lists and round to 4 decimal places\n    result = []\n    for vec in principal_components:\n        rounded_vec = [round(x, 4) for x in vec]\n        result.append(rounded_vec)\n\n    return result\n```\n\n### Explanation\n1. **Standardization**: The data is centered by subtracting the mean and scaled by dividing by the standard deviation. This ensures that each feature has a mean of 0 and a standard deviation of 1.\n\n2. **Covariance Matrix**: The covariance matrix is computed to capture the linear relationships between features. This matrix is essential for determining the principal components.\n\n3. **Eigenvalues and Eigenvectors**: Using NumPy's `eigh` function, we compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the variance explained by each eigenvector.\n\n4. **Sorting**: The eigenvalues are sorted in descending order, and the corresponding eigenvectors are reordered accordingly to identify the principal components.\n\n5. **Selecting Principal Components**: The top `k` eigenvectors are selected to form the principal components, which are then rounded and returned as a list of lists.\n\nThis approach ensures that we efficiently compute the principal components while handling edge cases such as when `k` exceeds the number of features. The result is a concise and accurate implementation of PCA from scratch."}
{"task_id": 20, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a decision tree learning algorithm using recursive binary splitting based on entropy and information gain. The goal is to build a decision tree from a given set of examples, each represented as a dictionary of attribute-value pairs, and a list of attribute names. The decision tree will be represented as a nested dictionary.\n\n### Approach\n1. **Base Cases**:\n   - If all examples have the same target value, return that value as a leaf node.\n   - If there are no attributes left to split on, return the majority target value of the current examples.\n\n2. **Entropy Calculation**:\n   - Entropy is a measure of impurity. For a given set of examples, entropy is calculated based on the distribution of the target attribute values.\n\n3. **Information Gain Calculation**:\n   - For each attribute, calculate the information gain, which is the difference between the entropy of the current examples and the weighted average entropy of the subsets created by splitting on that attribute.\n\n4. **Recursive Splitting**:\n   - Select the attribute with the highest information gain as the splitting attribute.\n   - Split the examples into subsets based on each value of the selected attribute.\n   - Recursively build the decision tree for each subset, excluding the selected attribute from further splits.\n\n### Solution Code\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # Base case: all examples have the same target value\n    first_target = examples[0][target_attr]\n    if all(example[target_attr] == first_target for example in examples):\n        return first_target\n    \n    # Base case: no attributes left, return majority target value\n    if not attributes:\n        target_counts = Counter(example[target_attr] for example in examples)\n        return max(target_counts, key=target_counts.get)\n    \n    # Calculate the best attribute to split on\n    max_info_gain = -1\n    best_attr = None\n    for attr in attributes:\n        # Calculate current entropy\n        current_target_counts = Counter(example[target_attr] for example in examples)\n        current_entropy = 0.0\n        total = len(examples)\n        for count in current_target_counts.values():\n            p = count / total\n            current_entropy -= p * math.log2(p)\n        \n        # Calculate weighted entropy for this attribute\n        attribute_values = set(example[attr] for example in examples)\n        weighted_entropy = 0.0\n        for value in attribute_values:\n            subset = [example for example in examples if example[attr] == value]\n            subset_total = len(subset)\n            if subset_total == 0:\n                continue  # Avoid division by zero, though unlikely in practice\n            subset_target_counts = Counter(example[target_attr] for example in subset)\n            subset_entropy = 0.0\n            for count in subset_target_counts.values():\n                p = count / subset_total\n                subset_entropy -= p * math.log2(p)\n            weighted_entropy += (subset_total / total) * subset_entropy\n        \n        # Information gain\n        info_gain = current_entropy - weighted_entropy\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_attr = attr\n    \n    # Split the examples based on the best attribute\n    tree = {best_attr: {}}\n    values = set(example[best_attr] for example in examples)\n    for value in values:\n        subset = [example for example in examples if example[best_attr] == value]\n        # Recurse with the subset and remaining attributes\n        remaining_attrs = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        tree[best_attr][value] = subtree\n    \n    return tree\n```\n\n### Explanation\n- **Base Cases**: The function first checks if all examples have the same target value, returning that value immediately. If no attributes are left, it returns the majority target value.\n- **Entropy Calculation**: For each attribute, the entropy of the current examples is calculated. This is done by counting the occurrences of each target value and computing the entropy using the formula.\n- **Information Gain**: For each attribute, the function calculates the weighted average entropy of the subsets created by splitting on that attribute. The information gain is the difference between the current entropy and this weighted average.\n- **Recursive Splitting**: The attribute with the highest information gain is selected as the splitting attribute. The examples are split into subsets based on each value of this attribute, and the function recursively builds the decision tree for each subset, excluding the selected attribute from further splits.\n\nThis approach ensures that the decision tree is built optimally by choosing the attribute that provides the highest information gain at each step, leading to a tree that effectively classifies the examples."}
{"task_id": 21, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should handle both linear and RBF kernels, and it should return the model's alpha coefficients and bias after training.\n\n### Approach\n1. **Kernel Matrix Computation**: Depending on the chosen kernel (linear or RBF), compute the kernel matrix which represents the similarity between all pairs of data points.\n2. **Initialization**: Initialize the alpha coefficients to zeros and the bias term to zero.\n3. **Iterative Updates**: For each iteration, compute the current predictions using the kernel matrix and the current alpha coefficients and bias. Update the alpha coefficients and bias based on the samples that do not satisfy the margin condition.\n4. **Rounding and Returning Results**: After completing the iterations, round the alpha coefficients and bias to four decimal places and return them as lists.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n = data.shape[0]\n    \n    # Ensure labels are either +1 or -1\n    labels = np.where(labels == 0, -1, labels)\n    \n    # Compute the kernel matrix\n    if kernel == 'linear':\n        K = data.dot(data.T)\n    elif kernel == 'rbf':\n        pairwise_diff = data[:, np.newaxis, :] - data[np.newaxis, :, :]\n        squared_dist = np.sum(pairwise_diff ** 2, axis=2)\n        K = np.exp(-squared_dist / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Kernel must be 'linear' or 'rbf'\")\n    \n    alpha = np.zeros(n)\n    b = 0.0\n    \n    for t in range(1, iterations + 1):\n        s = K.dot(alpha * labels) + b\n        mask = labels * s < 1\n        \n        delta_alpha = np.zeros(n)\n        delta_b = 0.0\n        \n        for i in range(n):\n            if mask[i]:\n                yi = labels[i]\n                si = s[i]\n                term = (1 - yi * si) / yi\n                delta_alpha_i = (1 / (lambda_val * t)) * term\n                delta_alpha[i] = delta_alpha_i\n                delta_b += (1 / (lambda_val * t)) * (1 - yi * si)\n        \n        # Update alpha and b\n        alpha = (1 - 1 / (lambda_val * t)) * alpha + delta_alpha\n        b += delta_b\n    \n    # Round the results to 4 decimal places\n    alpha = np.round(alpha, 4)\n    b = np.round(b, 4)\n    \n    return alpha.tolist(), b.tolist()\n```\n\n### Explanation\n1. **Kernel Matrix Computation**: The kernel matrix is computed based on the chosen kernel type. For the linear kernel, it is the dot product of the data matrix with itself. For the RBF kernel, it involves computing the exponential of the negative squared distances scaled by sigma.\n2. **Initialization**: The alpha coefficients are initialized to zeros, and the bias term is initialized to zero.\n3. **Iterative Updates**: For each iteration, the current predictions are computed using the kernel matrix, alpha coefficients, and bias. For each sample that does not satisfy the margin condition (i.e., the product of the label and prediction is less than 1), the alpha coefficient and bias are updated. The updates are done in a deterministic manner, considering all samples in each iteration.\n4. **Rounding and Returning Results**: After completing the specified number of iterations, the alpha coefficients and bias are rounded to four decimal places and returned as lists.\n\nThis approach ensures that the model is trained using the deterministic Pegasos algorithm with the specified kernel, regularization parameter, and number of iterations, providing the necessary model parameters for classification."}
{"task_id": 22, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the output of the sigmoid activation function given an input value \\( z \\). The sigmoid function is a fundamental activation function in machine learning, particularly in neural networks, and it maps any real-valued number to a value between 0 and 1. This is useful for binary classification problems where we need to convert a real-valued output into a probability.\n\n### Approach\nThe sigmoid function is defined mathematically as:\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\nwhere \\( e \\) is the base of the natural logarithm. \n\nThe steps to compute the sigmoid function are as follows:\n1. Compute \\( e^{-z} \\) using the exponential function.\n2. Add 1 to the result of \\( e^{-z} \\).\n3. Take the reciprocal of the sum obtained in the previous step.\n4. Round the result to four decimal places to get the final output.\n\n### Solution Code\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\n```\n\n### Explanation\n1. **Exponential Calculation**: The term \\( e^{-z} \\) is computed using `math.exp(-z)`, which gives the exponential of \\(-z\\).\n2. **Sum Calculation**: We add 1 to the result of the exponential calculation to get the denominator of the sigmoid function.\n3. **Reciprocal Calculation**: The reciprocal of the sum is taken to get the value of the sigmoid function.\n4. **Rounding**: The result is rounded to four decimal places using the `round` function to ensure the output is precise to four decimal places.\n\nThis approach efficiently computes the sigmoid function and ensures the result is formatted correctly for practical use cases."}
{"task_id": 23, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the softmax activation for a given list of scores. The softmax function is commonly used in machine learning to convert a list of scores into probabilities that sum up to 1. Each element in the output list is the probability of the corresponding input score.\n\n### Approach\nThe softmax function is defined as follows for a list of scores \\( s_1, s_2, ..., s_n \\):\n\\[ \\text{softmax}(s_i) = \\frac{e^{s_i}}{\\sum_{j=1}^{n} e^{s_j}} \\]\nwhere \\( e \\) is the base of the natural logarithm.\n\nTo compute the softmax values efficiently and avoid numerical instability (such as overflow due to very large exponentials), we can adjust the scores by subtracting the maximum score from each element before exponentiating. This adjustment ensures that the largest exponent is zero, which helps in preventing overflow.\n\nThe steps to compute the softmax values are as follows:\n1. Compute the maximum score in the input list.\n2. Adjust each score by subtracting the maximum score.\n3. Compute the exponential of each adjusted score.\n4. Sum all the exponentials to get the denominator.\n5. Divide each exponential by the sum to get the softmax values.\n6. Round each softmax value to four decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if not scores:\n        return []\n    max_score = max(scores)\n    adjusted_scores = [s - max_score for s in scores]\n    exp_scores = [math.exp(s) for s in adjusted_scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [e / sum_exp for e in exp_scores]\n    rounded = [round(v, 4) for v in softmax_values]\n    return rounded\n```\n\n### Explanation\n1. **Handling Empty Input**: The function first checks if the input list is empty and returns an empty list if true.\n2. **Adjusting Scores**: The maximum score is found, and each score is adjusted by subtracting this maximum. This step ensures numerical stability by preventing very large exponentials.\n3. **Exponential Calculation**: Each adjusted score is exponentiated using `math.exp`.\n4. **Summing Exponentials**: The sum of all exponentials is computed to form the denominator for the softmax function.\n5. **Computing Softmax Values**: Each exponential value is divided by the sum to get the corresponding softmax value.\n6. **Rounding**: Each softmax value is rounded to four decimal places to meet the problem's requirements.\n\nThis approach ensures that the function is both efficient and numerically stable, providing accurate results for a wide range of input scores."}
{"task_id": 24, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with a sigmoid activation function for binary classification. The function will take in feature vectors, true binary labels, weights, and a bias term, and return the predicted probabilities and the mean squared error (MSE) between the predictions and the true labels.\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the input feature vectors, labels, and weights into numpy arrays for efficient computation.\n2. **Compute Linear Combination**: For each feature vector, compute the linear combination using the dot product of the feature vector and the weights, then add the bias term.\n3. **Apply Sigmoid Activation**: Apply the sigmoid function to the linear combination to get the predicted probabilities.\n4. **Compute Mean Squared Error (MSE)**: Calculate the MSE between the predicted probabilities and the true labels.\n5. **Round Results**: Round the predicted probabilities and the MSE to four decimal places for the final output.\n\n### Solution Code\n```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Convert inputs to numpy arrays for efficient computation\n    X = np.array(features)\n    y = np.array(labels)\n    w = np.array(weights)\n    \n    # Compute the linear combination (z = Xw + bias)\n    z = np.dot(X, w) + bias\n    \n    # Apply sigmoid activation function\n    predictions = 1 / (1 + np.exp(-z))\n    \n    # Calculate the mean squared error\n    mse = np.mean((y - predictions) ** 2)\n    \n    # Round the predictions and MSE to four decimal places\n    predictions_rounded = np.round(predictions, 4).tolist()\n    mse_rounded = np.round(mse, 4)\n    \n    return (predictions_rounded, mse_rounded)\n```\n\n### Explanation\n1. **Conversion to Numpy Arrays**: The function starts by converting the input lists into numpy arrays to leverage vectorized operations, which are more efficient for numerical computations.\n2. **Linear Combination Calculation**: Using numpy's dot product, the function computes the linear combination of features and weights, then adds the bias term to each result.\n3. **Sigmoid Activation**: The sigmoid function is applied to each element of the linear combination array to transform the outputs into probabilities between 0 and 1.\n4. **MSE Calculation**: The mean squared error is computed by taking the average of the squared differences between the true labels and the predicted probabilities.\n5. **Rounding**: The predicted probabilities and MSE are rounded to four decimal places to meet the output requirements.\n\nThis approach efficiently handles multidimensional input features and provides accurate predictions and error metrics for binary classification tasks."}
{"task_id": 25, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to simulate a single neuron with sigmoid activation and implement backpropagation to update the neuron's weights and bias using gradient descent based on the Mean Squared Error (MSE) loss. The function should return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n\n### Approach\n1. **Initialization**: Start with the given initial weights and bias. The number of samples is determined from the features array.\n2. **Forward Propagation**: For each epoch, compute the linear combination of features and weights, add the bias, and apply the sigmoid activation function to get the predictions.\n3. **Compute Error**: Calculate the error term using the difference between predictions and true labels, scaled by the derivative of the sigmoid function.\n4. **Gradient Calculation**: Compute the gradients for the weights and bias using the error term and features.\n5. **Update Weights and Bias**: Adjust the weights and bias using the computed gradients and the learning rate.\n6. **Compute MSE**: Calculate the Mean Squared Error for the current epoch and store it in a list.\n7. **Rounding**: After completing all epochs, round the final weights, bias, and each MSE value to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    N = features.shape[0]\n    mse_list = []\n    \n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        a = 1 / (1 + np.exp(-z))\n        delta = (a - labels) * a * (1 - a)\n        grad_weights = np.dot(features.T, delta) / N\n        grad_bias = np.sum(delta) / N\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n        mse = np.mean((a - labels) ** 2)\n        mse_rounded = round(mse, 4)\n        mse_list.append(mse_rounded)\n    \n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    return (weights, bias, mse_list)\n```\n\n### Explanation\n- **Initialization**: The function starts by copying the initial weights and bias to avoid modifying the original inputs. The number of samples is determined from the features array.\n- **Forward Propagation**: For each epoch, the linear combination of features and weights is computed, followed by adding the bias. The sigmoid function is applied to this result to get the predictions.\n- **Error Calculation**: The error term is computed using the difference between predictions and true labels, scaled by the derivative of the sigmoid function.\n- **Gradient Calculation**: The gradients for the weights and bias are computed using the error term and features. These gradients are averaged over all samples.\n- **Update Weights and Bias**: The weights and bias are updated using the computed gradients and the learning rate.\n- **MSE Calculation**: The Mean Squared Error for the current epoch is computed and stored in a list after rounding to four decimal places.\n- **Rounding**: After completing all epochs, the final weights and bias are rounded to four decimal places, and the list of MSE values is returned.\n\nThis approach ensures that the neuron's weights and bias are updated efficiently using backpropagation, and the results are presented in a clear and precise manner."}
{"task_id": 26, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Python class that can handle basic autograd operations: addition, multiplication, and ReLU activation. The class should correctly compute gradients for these operations through automatic differentiation.\n\n### Approach\nThe approach involves creating a `Value` class that can track the computation graph and compute gradients using backpropagation. Each instance of the `Value` class represents a node in the computation graph and contains the necessary information to compute gradients. The key steps are:\n\n1. **Initialization**: Each `Value` instance is initialized with a scalar value, a gradient, and references to its parent nodes and the operation that created it.\n2. **Addition Operation**: Overloading the `+` operator to create a new `Value` instance representing the sum of two values. The backward pass for addition distributes the gradient equally to both operands.\n3. **Multiplication Operation**: Overloading the `*` operator to create a new `Value` instance representing the product of two values. The backward pass for multiplication uses the product rule to compute gradients.\n4. **ReLU Activation**: Implementing a method to apply the ReLU activation function. The backward pass for ReLU propagates the gradient only if the input value is positive.\n\n### Solution Code\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), _children=(self,), _op='ReLU')\n        def _backward():\n            if self.data > 0:\n                self.grad += out.grad\n            else:\n                self.grad += 0\n        out._backward = _backward\n        return out\n```\n\n### Explanation\n- **Initialization**: The `__init__` method sets up the initial data, gradient, and references to parent nodes and operations.\n- **Addition (`__add__` method)**: This method creates a new `Value` instance representing the sum of two values. The backward pass adds the gradient of the result to both operands.\n- **Multiplication (`__mul__` method)**: This method creates a new `Value` instance representing the product of two values. The backward pass uses the product rule to distribute the gradient.\n- **ReLU Activation (`relu` method)**: This method applies the ReLU function and sets up the backward pass to propagate the gradient only if the input value is positive.\n\nThis implementation allows for the automatic computation of gradients through a computational graph, enabling efficient backpropagation for optimization in machine learning models."}
{"task_id": 27, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the transformation matrix from one basis B to another basis C in R^3. The transformation matrix allows us to convert coordinates from basis B to basis C.\n\n### Approach\n1. **Understanding the Problem**: The transformation matrix P from basis B to C is such that when a vector is expressed in basis B, multiplying by P converts it to basis C. The columns of P are the coordinates of the vectors in B expressed in terms of basis C.\n\n2. **Matrix Inversion**: Since C is a basis, it forms a square matrix which is invertible. The inverse of matrix C (denoted as C_inv) is used to convert vectors from the standard basis to basis C.\n\n3. **Matrix Multiplication**: To find the transformation matrix P, we multiply the inverse of matrix C (C_inv) by the matrix B. This operation effectively converts each vector in B into its representation in basis C.\n\n4. **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted to a list of lists for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the input lists to numpy arrays\n    B_np = np.array(B)\n    C_np = np.array(C)\n    \n    # Compute the inverse of matrix C\n    C_inv = np.linalg.inv(C_np)\n    \n    # Calculate the transformation matrix P\n    P = C_inv @ B_np\n    \n    # Round the elements to 4 decimal places\n    P_rounded = np.round(P, 4)\n    \n    # Convert the numpy array to a list of lists\n    return P_rounded.tolist()\n```\n\n### Explanation\n1. **Matrix Conversion**: The input bases B and C are converted into numpy arrays for efficient matrix operations.\n2. **Inverse Calculation**: The inverse of matrix C is computed using numpy's `linalg.inv` function.\n3. **Transformation Matrix Calculation**: The transformation matrix P is obtained by multiplying the inverse of C with B.\n4. **Rounding**: The resulting matrix is rounded to four decimal places to meet the output constraints.\n5. **Conversion to List**: The numpy array is converted to a list of lists using the `tolist()` method for the final output.\n\nThis approach efficiently computes the transformation matrix using linear algebra operations, ensuring accuracy and adherence to the problem constraints."}
{"task_id": 28, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Singular Value Decomposition (SVD) of a 2x2 matrix using eigenvalues and eigenvectors. The SVD of a matrix A is given by A = U * S * V^T, where U and V are orthogonal matrices, and S is a diagonal matrix of singular values. The approach involves computing the eigenvalues and eigenvectors of the matrix A^T A, constructing the singular values, and then deriving the matrices U, S, and V.\n\n### Approach\n1. **Compute A^T A**: This matrix is symmetric and positive semi-definite.\n2. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of A^T A. The eigenvalues are sorted in descending order to ensure the singular values are correctly ordered.\n3. **Construct Singular Values**: The singular values are the square roots of the eigenvalues of A^T A.\n4. **Construct Matrix V**: The eigenvectors of A^T A form the columns of matrix V.\n5. **Construct Matrix U**: For each non-zero singular value, compute the corresponding column of U by normalizing A multiplied by the corresponding eigenvector. If a singular value is zero, compute the corresponding column of U as a normalized vector orthogonal to the existing columns of U.\n6. **Handle Special Cases**: If all singular values are zero, U is set to the identity matrix.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Compute A^T A\n    B = A.T @ A\n    \n    # Compute eigenvalues and eigenvectors of B\n    eigenvalues, eigenvectors = np.linalg.eigh(B)\n    \n    # Sort eigenvalues in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[sorted_indices]\n    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n    \n    # Compute singular values\n    singular_values = np.sqrt(eigenvalues_sorted)\n    \n    # Construct S\n    S = np.diag(singular_values)\n    \n    # Construct V\n    V = eigenvectors_sorted\n    \n    # Compute U\n    if singular_values[0] == 0:\n        # Both singular values are zero\n        U = np.eye(2)\n    else:\n        v1 = eigenvectors_sorted[:, 0]\n        u1 = (A @ v1) / singular_values[0]\n        \n        if singular_values[1] != 0:\n            v2 = eigenvectors_sorted[:, 1]\n            u2 = (A @ v2) / singular_values[1]\n        else:\n            # Compute u2 as a vector orthogonal to u1\n            u1_vec = u1.flatten()\n            perp = np.array([u1_vec[1], -u1_vec[0]])\n            perp = perp / np.linalg.norm(perp)\n            u2 = perp.reshape(-1, 1)\n        \n        U = np.hstack([u1, u2])\n    \n    # Round to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to lists\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return (U_list, S_list, V_list)\n```\n\n### Explanation\n1. **Matrix A^T A**: This matrix is computed to leverage its properties for eigenvalue decomposition.\n2. **Eigenvalues and Eigenvectors**: The eigenvalues of A^T A are sorted in descending order to ensure the singular values are correctly ordered. The corresponding eigenvectors form the matrix V.\n3. **Singular Values**: These are derived from the square roots of the eigenvalues and form the diagonal matrix S.\n4. **Matrix U**: Each column of U is computed by normalizing the product of A and the corresponding eigenvector. If a singular value is zero, the corresponding column is computed as a normalized orthogonal vector to ensure U remains orthogonal.\n5. **Special Case Handling**: If all singular values are zero, U is set to the identity matrix to handle the trivial case where A is the zero matrix.\n\nThis approach ensures that the SVD is computed correctly for any 2x2 matrix, providing the matrices U, S, and V such that A = U * S * V^T."}
{"task_id": 29, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that shuffles two numpy arrays, X and y, in such a way that the corresponding order between them is maintained. This is essential for ensuring that each sample in X remains paired with its corresponding label in y after shuffling. The function should also allow for reproducibility by accepting an optional seed parameter.\n\n### Approach\n1. **Check Compatibility**: Ensure that the lengths of X and y are the same. If not, raise a ValueError.\n2. **Set Seed for Reproducibility**: If a seed is provided, set the random seed to ensure that the shuffling can be reproduced.\n3. **Generate Permutation Indices**: Create a random permutation of indices using numpy's `random.permutation` function.\n4. **Shuffle Arrays**: Apply the permutation indices to both X and y to shuffle them while maintaining their correspondence.\n5. **Convert to Lists**: Convert the shuffled numpy arrays to Python lists using the `tolist()` method and return them.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    if len(X) != len(y):\n        raise ValueError(\"X and y must have the same number of samples.\")\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    permutation = np.random.permutation(len(X))\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    \n    return X_shuffled.tolist(), y_shuffled.tolist()\n```\n\n### Explanation\n- **Check Compatibility**: The function first checks if the lengths of X and y are equal. This ensures that each sample in X has a corresponding label in y.\n- **Set Seed**: If a seed is provided, it sets the random seed to ensure that the shuffling process is reproducible.\n- **Generate Permutation Indices**: The permutation indices are generated using `np.random.permutation`, which creates a random sequence of indices that can be used to shuffle the arrays.\n- **Shuffle Arrays**: Both X and y are shuffled using the same permutation indices, ensuring that their correspondence is maintained.\n- **Convert to Lists**: The shuffled numpy arrays are converted to Python lists using the `tolist()` method, which is then returned as the result.\n\nThis approach ensures that the data is shuffled randomly while maintaining the integrity of the correspondence between samples and their labels, and it provides the flexibility to reproduce the shuffling process using a seed."}
{"task_id": 30, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a function that generates batches of data from a given dataset. The function should handle both cases where the dataset includes target values and where it does not. Each batch should be converted to a Python list for easier manipulation.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Determine the Number of Samples**: Calculate the total number of samples in the input data array `X`.\n2. **Iterate in Batches**: Loop through the data in increments of the specified batch size. For each increment, extract a slice of `X` and, if provided, a corresponding slice of `y`.\n3. **Convert to Lists**: Convert each extracted slice from numpy arrays to Python lists using the `tolist()` method.\n4. **Yield Batches**: If target values `y` are provided, yield a tuple of the batched `X` and `y` values. Otherwise, yield only the batched `X` values.\n\nThis approach ensures that the function efficiently handles both cases where target values are present and where they are not, while also correctly managing cases where the total number of samples is not a perfect multiple of the batch size.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i+batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i+batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\n```\n\n### Explanation\n- **Determine the Number of Samples**: The number of samples is obtained using `X.shape[0]`, which gives the number of rows in the numpy array `X`.\n- **Iterate in Batches**: Using a loop that steps through the indices in increments of `batch_size`, we extract slices of `X` and `y` (if provided) for each batch.\n- **Convert to Lists**: Each slice is converted to a Python list using the `tolist()` method, which is necessary for compatibility with many Python functions and libraries.\n- **Yield Batches**: The function yields each batch as a tuple of lists if `y` is provided, or as a single list if `y` is not provided. This allows the function to be used in a flexible manner depending on the dataset structure.\n\nThis solution efficiently handles the generation of batches, ensuring that all data is processed correctly and returned in the required format."}
{"task_id": 31, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that divides a dataset into two subsets based on a specified feature and a threshold. The function will return two subsets: one where the feature value is greater than or equal to the threshold, and another where it is not.\n\n### Approach\n1. **Problem Analysis**: The task is to split a dataset into two parts based on a given feature and threshold. Each sample in the dataset is checked against the threshold for the specified feature. If the feature value meets or exceeds the threshold, the sample goes into one subset; otherwise, it goes into the other subset.\n2. **Boolean Masking**: We use boolean indexing to efficiently split the dataset. A boolean mask is created where each element is `True` if the corresponding feature value meets the threshold condition, and `False` otherwise.\n3. **Splitting the Dataset**: Using the boolean mask, we split the dataset into two subsets. The first subset includes all samples where the mask is `True`, and the second subset includes all samples where the mask is `False`.\n4. **Conversion to Lists**: The resulting subsets, which are numpy arrays, are converted to Python lists using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    mask = X[:, feature_i] >= threshold\n    subset1 = X[mask]\n    subset2 = X[~mask]\n    return subset1.tolist(), subset2.tolist()\n```\n\n### Explanation\n1. **Boolean Mask Creation**: The mask is created by comparing each value of the specified feature in the dataset against the threshold. This results in a boolean array where each element indicates whether the corresponding sample meets the condition.\n2. **Subset Extraction**: Using the boolean mask, we extract two subsets from the dataset. The first subset (`subset1`) contains all samples where the feature value is greater than or equal to the threshold. The second subset (`subset2`) contains all other samples.\n3. **Conversion to Lists**: The numpy arrays `subset1` and `subset2` are converted to Python lists using the `tolist()` method to ensure the output is in the required format.\n\nThis approach efficiently splits the dataset using numpy's boolean indexing, ensuring that the solution is both concise and performant."}
{"task_id": 32, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate polynomial features for a given dataset. The function should take a 2D numpy array `X` and an integer `degree`, and return a new 2D numpy array with polynomial features up to the specified degree. The output should be converted to a list using numpy's `tolist()` method.\n\n### Approach\nThe approach involves generating all possible polynomial features up to the specified degree. This is done by considering all combinations of the input features with replacement, where the length of each combination corresponds to the current degree being processed. For each combination, we compute the product of the features in that combination for each sample in the input data. These products form the new polynomial features.\n\n1. **Initialization**: Determine the number of features in the input array `X`.\n2. **Generate Combinations**: For each degree from 1 to the specified degree, generate all possible combinations of the features with replacement. Each combination's length is equal to the current degree.\n3. **Compute Products**: For each combination, compute the product of the corresponding features for each sample in `X`.\n4. **Stack Features**: Collect all the computed features and stack them as columns to form the resulting polynomial features matrix.\n5. **Return Result**: Convert the resulting matrix to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    if degree == 0:\n        return np.array([]).tolist()\n    n_features = X.shape[1]\n    features = []\n    for d in range(1, degree + 1):\n        for comb in combinations_with_replacement(range(n_features), d):\n            feature = np.prod(X[:, comb], axis=1)\n            features.append(feature)\n    if not features:\n        return np.array([]).tolist()\n    result = np.column_stack(features)\n    return result.tolist()\n```\n\n### Explanation\n- **Initialization**: The number of features in `X` is determined using `X.shape[1]`.\n- **Generate Combinations**: Using `combinations_with_replacement`, we generate all possible combinations of feature indices for each degree from 1 to the specified degree.\n- **Compute Products**: For each combination, the product of the corresponding features is computed using `np.prod` along the rows of `X`.\n- **Stack Features**: The computed features are collected into a list and then stacked column-wise to form the resulting matrix.\n- **Return Result**: The resulting matrix is converted to a list using `tolist()` and returned.\n\nThis approach efficiently generates all polynomial features up to the specified degree, ensuring that all combinations of features are considered. The use of numpy operations ensures that the computations are performed efficiently, even for larger datasets."}
{"task_id": 33, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate random subsets of a given dataset. Each subset should be a tuple containing a random selection of features (X) and corresponding labels (y). The subsets can be generated with or without replacement based on the input parameter.\n\n### Approach\n1. **Problem Analysis**: The task is to create a function that generates multiple random subsets from a given dataset. Each subset should be a tuple of feature and label arrays. The function should handle both cases where subsets are generated with replacement (allowing duplicates) and without replacement (ensuring each element appears exactly once).\n\n2. **Intuition**: For each subset, we need to randomly select indices from the original dataset. If replacement is allowed, the same index can be selected multiple times, leading to possible duplicates in the subset. If replacement is not allowed, each index can be selected only once, resulting in a permutation of the original dataset.\n\n3. **Algorithm Selection**: We use numpy's `random.choice` function to generate indices. This function allows us to specify whether the selection should be with or without replacement. The indices are then used to extract the corresponding rows from the feature array (X) and elements from the label array (y).\n\n4. **Complexity Analysis**: The time complexity is O(n_subsets * m), where m is the number of samples in the dataset. This is because for each subset, we generate m indices and extract the corresponding data points.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    m = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(m, size=m, replace=replacements)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\n```\n\n### Explanation\n1. **Setting the Seed**: The seed is set to ensure reproducibility of the random results. This is important for testing and debugging purposes.\n\n2. **Generating Indices**: For each subset, we generate a set of indices using `np.random.choice`. The `replace` parameter is set based on the `replacements` input, determining whether indices can be reused.\n\n3. **Extracting Subsets**: Using the generated indices, we extract the corresponding rows from X and elements from y. These extracted arrays are then converted to lists using the `tolist()` method to meet the output requirements.\n\n4. **Storing Subsets**: Each generated subset (as a tuple of lists) is appended to the result list, which is returned after all subsets have been generated.\n\nThis approach efficiently handles both cases of subset generation with and without replacement, ensuring that the output meets the specified constraints."}
{"task_id": 34, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that performs one-hot encoding on a 1D numpy array of integer values. One-hot encoding is a technique used to convert categorical variables into a binary matrix, where each category is represented by a unique column. The function should handle both cases where the number of columns (n_col) is provided and where it needs to be determined from the input array.\n\n### Approach\n1. **Check for Empty Input**: If the input array is empty, return an empty list immediately.\n2. **Determine Number of Columns**: If `n_col` is not provided, compute it as the maximum value in the input array plus one. This ensures that all possible categories are covered.\n3. **Create Zero Matrix**: Initialize a 2D numpy array of zeros with dimensions (length of input array, n_col).\n4. **Set One-Hot Encoded Values**: For each element in the input array, set the corresponding position in the zero matrix to 1. This is done efficiently using numpy's indexing capabilities.\n5. **Convert to List**: Convert the resulting numpy array to a Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    if len(x) == 0:\n        return []\n    if n_col is None:\n        n_col = np.max(x) + 1\n    result = np.zeros((len(x), n_col), dtype=int)\n    result[np.arange(len(x)), x] = 1\n    return result.tolist()\n```\n\n### Explanation\n- **Empty Input Handling**: The function first checks if the input array is empty and returns an empty list if true.\n- **Determine n_col**: If `n_col` is not provided, it is calculated as the maximum value in the input array plus one, ensuring all categories are included.\n- **Zero Matrix Initialization**: A 2D array of zeros is created with the appropriate dimensions.\n- **Setting One-Hot Values**: Using numpy's advanced indexing, the function efficiently sets the appropriate positions to 1, creating the one-hot encoded matrix.\n- **Conversion to List**: The resulting numpy array is converted to a Python list for the final output.\n\nThis approach ensures that the function is both efficient and handles various edge cases, such as empty input and automatic determination of the number of columns."}
{"task_id": 35, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a 1D numpy array into a diagonal matrix and return it as a Python list. A diagonal matrix is a square matrix where all the elements outside the main diagonal are zero. \n\n### Approach\n1. **Problem Analysis**: The task is to take a 1D numpy array and convert it into a diagonal matrix. A diagonal matrix is a square matrix where the elements of the input array lie on the main diagonal, and all other elements are zero.\n2. **Intuition**: Using numpy's `diag` function, we can directly create a diagonal matrix from a 1D array. This function constructs a square matrix with the elements of the input array on the main diagonal.\n3. **Conversion to List**: After creating the diagonal matrix using `np.diag`, we convert the resulting numpy array into a Python list using the `tolist()` method to meet the output requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\n```\n\n### Explanation\n1. **Import Numpy**: We start by importing the numpy library, which is essential for handling arrays and matrix operations.\n2. **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n3. **Create Diagonal Matrix**: Using `np.diag(x)`, we create a diagonal matrix where the elements of `x` are placed on the main diagonal of the resulting square matrix.\n4. **Convert to List**: The resulting numpy array is converted to a Python list using the `tolist()` method to meet the output constraints.\n\nThis approach efficiently converts the input array into the desired diagonal matrix and returns it in the required format."}
{"task_id": 36, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is a measure of how often the model's predictions match the true labels. This is a common metric used in machine learning to evaluate the performance of classification models.\n\n### Approach\nThe approach to calculate the accuracy score involves the following steps:\n1. **Compare Predictions with True Labels**: We compare each element in the true labels array (`y_true`) with the corresponding element in the predicted labels array (`y_pred`). This comparison results in a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise.\n2. **Calculate the Mean of the Boolean Array**: The mean of this boolean array gives the proportion of correct predictions. Since `True` is treated as 1 and `False` as 0 in numerical contexts, the mean effectively calculates the ratio of correct predictions to the total number of predictions.\n3. **Round the Result**: The resulting accuracy score is then rounded to four decimal places to meet the output constraints.\n\nThis approach is efficient and concise, leveraging numpy's vectorized operations for optimal performance.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\n```\n\n### Explanation\n- **Comparison of Arrays**: The expression `y_true == y_pred` generates a boolean array where each element indicates whether the corresponding elements in `y_true` and `y_pred` are equal.\n- **Mean Calculation**: Using `np.mean` on the boolean array converts it into a float representing the proportion of correct predictions. This is equivalent to dividing the number of correct predictions by the total number of predictions.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis method efficiently computes the accuracy score with minimal computational overhead, making it suitable for large datasets."}
{"task_id": 37, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that calculates the correlation matrix for a given dataset. The function should handle both cases where only one dataset is provided and where two datasets are provided. The correlation matrix should be returned as a list of lists, with each value rounded to four decimal places.\n\n### Approach\n1. **Problem Analysis**: The correlation matrix is a square matrix where each element (i, j) represents the Pearson correlation coefficient between the i-th and j-th variables. If a second dataset Y is provided, the function should compute the correlation between each variable in X and each variable in Y.\n\n2. **Intuitions and Insights**:\n   - **Pearson Correlation Coefficient**: This measures the linear correlation between two variables. It is calculated by dividing the covariance of the variables by the product of their standard deviations.\n   - **Handling Different Datasets**: If Y is not provided, the function computes the correlation matrix of X with itself. If Y is provided, it computes the cross-correlation matrix between X and Y.\n\n3. **Algorithm Selection**:\n   - **Centering Data**: Subtract the mean of each variable to center the data.\n   - **Covariance Calculation**: Compute the covariance matrix between the centered variables.\n   - **Standard Deviation Calculation**: Compute the standard deviations of each variable.\n   - **Correlation Calculation**: Divide the covariance matrix by the outer product of the standard deviations to get the correlation matrix.\n\n4. **Complexity Analysis**: The algorithm involves matrix multiplications and operations that are efficient with numpy, making it suitable for large datasets.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is not None:\n        if X.shape[0] != Y.shape[0]:\n            raise ValueError(\"X and Y must have the same number of samples.\")\n        A = X\n        B = Y\n    else:\n        A = X\n        B = X\n\n    n = A.shape[0]\n    A_centered = A - np.mean(A, axis=0, keepdims=True)\n    B_centered = B - np.mean(B, axis=0, keepdims=True)\n\n    covariance = (A_centered.T @ B_centered) / (n - 1)\n\n    std_A = np.std(A, axis=0, ddof=1)\n    std_B = np.std(B, axis=0, ddof=1)\n    std_outer = np.outer(std_A, std_B)\n\n    # Avoid division by zero by setting correlation to 0 where std_outer is zero\n    correlation = np.zeros_like(covariance, dtype=np.float64)\n    mask = std_outer != 0\n    correlation[mask] = covariance[mask] / std_outer[mask]\n\n    # Round to 4 decimal places\n    correlation = np.round(correlation, 4)\n\n    # Convert to list\n    return correlation.tolist()\n```\n\n### Explanation\n1. **Input Handling**: The function checks if Y is provided and ensures that X and Y have the same number of samples if Y is provided.\n2. **Centering Data**: Each variable is centered by subtracting its mean.\n3. **Covariance Calculation**: The covariance matrix is computed using the centered data.\n4. **Standard Deviation Calculation**: The standard deviations of each variable are computed.\n5. **Correlation Calculation**: The covariance matrix is divided by the outer product of the standard deviations to get the correlation matrix. Division by zero is handled by setting the correlation to zero where the standard deviation is zero.\n6. **Rounding and Conversion**: The correlation matrix is rounded to four decimal places and converted to a list of lists for the final output.\n\nThis approach efficiently computes the correlation matrix using numpy operations, ensuring accuracy and performance."}
{"task_id": 38, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the AdaBoost fit method, which is a boosting algorithm that combines multiple weak classifiers to form a strong classifier. The weak classifiers used here are decision stumps, which are simple models that make decisions based on a single feature and a threshold.\n\n### Approach\n1. **Initialize Sample Weights**: Each sample starts with an equal weight, which is 1 divided by the number of samples.\n2. **Iterate for Each Classifier**: For each of the specified number of classifiers, perform the following steps:\n   - **Find the Best Threshold for Each Feature**: For each feature, consider all possible thresholds, including the unique values and midpoints between consecutive unique values. For each threshold, compute the error for both possible directions (left as -1 and right as 1, or vice versa).\n   - **Select the Best Weak Classifier**: Choose the feature, threshold, and direction that result in the lowest error. If the error is 0.5 or higher, skip this classifier as it is not useful.\n   - **Compute Alpha**: Calculate the weight (alpha) of the current classifier based on its error rate.\n   - **Update Sample Weights**: Adjust the weights of the samples based on whether they were correctly or incorrectly classified by the current weak classifier.\n3. **Return Classifiers**: Collect all the classifiers, each represented by their feature, threshold, direction, and alpha, and return them as a list.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    \n    for _ in range(n_clf):\n        best_error = 1.0\n        best_feature = None\n        best_threshold = None\n        best_direction = None\n        \n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            unique_vals = np.sort(np.unique(feature_values))\n            thresholds = []\n            for i in range(len(unique_vals)):\n                thresholds.append(unique_vals[i])\n                if i < len(unique_vals) - 1:\n                    mid = (unique_vals[i] + unique_vals[i+1]) / 2\n                    thresholds.append(mid)\n            \n            for threshold in thresholds:\n                left = (feature_values <= threshold)\n                # Direction 1: left is -1, right is 1\n                pred1 = np.where(left, -1, 1)\n                error1 = np.sum(sample_weights * (pred1 != y))\n                \n                # Direction 2: left is 1, right is -1\n                pred2 = np.where(left, 1, -1)\n                error2 = np.sum(sample_weights * (pred2 != y))\n                \n                if error1 < error2:\n                    current_error = error1\n                    current_direction = 1\n                else:\n                    current_error = error2\n                    current_direction = -1\n                \n                if current_error < best_error:\n                    best_error = current_error\n                    best_feature = feature\n                    best_threshold = threshold\n                    best_direction = current_direction\n        \n        if best_error >= 0.5:\n            continue\n        \n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        alpha = round(alpha, 4)\n        \n        feature_values = X[:, best_feature]\n        left = (feature_values <= best_threshold)\n        pred = np.where(left, -1, 1)\n        pred = pred * best_direction\n        \n        update_factor = np.exp(-alpha * y * pred)\n        sample_weights *= update_factor\n        sample_weights /= np.sum(sample_weights)\n        \n        rounded_threshold = round(best_threshold, 4)\n        classifiers.append((best_feature, rounded_threshold, best_direction, alpha))\n    \n    return classifiers\n```\n\n### Explanation\n- **Initialization**: The sample weights are initialized to be equal for all samples.\n- **Finding the Best Weak Classifier**: For each feature, all possible thresholds are considered, and the error for both possible directions is computed. The best threshold and direction are selected based on the lowest error.\n- **Alpha Calculation**: The weight of each classifier is determined by its error rate, with lower error leading to a higher weight.\n- **Updating Weights**: The sample weights are updated to give more weight to misclassified samples, ensuring that subsequent classifiers focus more on these samples.\n- **Result**: The function returns a list of classifiers, each with its parameters, which can be used for making predictions in the AdaBoost classification process."}
{"task_id": 39, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the log-softmax function, which is commonly used in machine learning for converting a vector of scores into probabilities in a numerically stable manner. The log-softmax function is the logarithm of the softmax function and helps prevent numerical overflow issues when dealing with large numbers.\n\n### Approach\nThe approach to compute the log-softmax function involves the following steps:\n\n1. **Convert the Input List to a Numpy Array**: This allows us to leverage numpy's efficient array operations.\n2. **Subtract the Maximum Value**: To prevent numerical overflow when computing exponentials, we subtract the maximum value from each element in the array.\n3. **Compute Exponentials**: Calculate the exponential of each element in the shifted array.\n4. **Sum the Exponentials**: Compute the sum of these exponentials to use as the denominator in the softmax function.\n5. **Compute the Logarithm of the Sum**: This step is crucial for numerical stability and forms part of the log-softmax calculation.\n6. **Calculate Log-Softmax Values**: Subtract the logarithm of the sum from each element in the shifted array.\n7. **Round the Results**: Each value is rounded to four decimal places for precision.\n8. **Convert to List**: Finally, convert the numpy array back to a Python list for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores_np = np.array(scores)\n    m = np.max(scores_np)\n    shifted = scores_np - m\n    exp_shifted = np.exp(shifted)\n    sum_exp = np.sum(exp_shifted)\n    log_sum = np.log(sum_exp)\n    log_softmax_values = shifted - log_sum\n    log_softmax_values = np.round(log_softmax_values, 4)\n    return log_softmax_values.tolist()\n```\n\n### Explanation\n1. **Convert Input to Numpy Array**: The input list is converted to a numpy array to facilitate vectorized operations.\n2. **Subtract Maximum Value**: By subtracting the maximum value from each element, we ensure that the largest element becomes zero, which helps in preventing overflow when computing exponentials.\n3. **Exponential Calculation**: Each element is exponentiated, which is a key step in the softmax function.\n4. **Sum of Exponentials**: The sum of these exponentials is computed to form the denominator in the softmax function.\n5. **Logarithm of Sum**: Taking the logarithm of this sum is part of the log-softmax calculation and helps in maintaining numerical stability.\n6. **Log-Softmax Calculation**: Each element in the shifted array is adjusted by subtracting the logarithm of the sum, resulting in the log-softmax values.\n7. **Rounding**: The results are rounded to four decimal places to meet the precision requirement.\n8. **Convert to List**: The final result is converted back to a Python list for the output.\n\nThis approach ensures that the log-softmax function is computed efficiently and accurately, even for large input values, by leveraging numerical stability techniques."}
{"task_id": 40, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a custom Dense layer in Python by extending a given base `Layer` class. The Dense layer is a fully connected neural network layer that performs a dot product between the input and the weight matrix, followed by adding a bias term. The implementation includes methods for weight initialization, forward pass, backward pass, parameter count, and output shape.\n\n### Approach\n1. **Weight Initialization**: The weights are initialized using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`, and the biases are initialized to zero. This helps in maintaining the variance of the activations throughout the network.\n2. **Forward Pass**: The output is computed as the dot product of the input and the weight matrix, followed by adding the bias term. The input is stored for use in the backward pass.\n3. **Backward Pass**: The gradients with respect to the weights and biases are computed using the chain rule. The gradients are then used to update the weights and biases if the layer is trainable.\n4. **Parameter Count**: The total number of trainable parameters is the sum of the weights and biases.\n5. **Output Shape**: The output shape of the layer is determined by the number of neurons in the layer.\n\n### Solution Code\n```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        super().__init__()\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n        self.layer_input = None\n\n    def initialize(self):\n        if self.input_shape is None:\n            raise ValueError(\"Input shape must be set before initializing the layer.\")\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(low=-limit, high=limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n\n    def parameters(self):\n        if self.input_shape is None:\n            return 0\n        return self.input_shape[0] * self.n_units + self.n_units\n\n    def forward_pass(self, X, training):\n        self.layer_input = X.copy()\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        dL_dW = np.dot(self.layer_input.T, accum_grad)\n        dL_dw0 = np.sum(accum_grad, axis=0)\n        dL_dX = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            if self.optimizer_W is not None and self.optimizer_w0 is not None:\n                self.W = self.optimizer_W.update(self.W, dL_dW)\n                self.w0 = self.optimizer_w0.update(self.w0, dL_dw0)\n        return dL_dX\n\n    def output_shape(self):\n        return (self.n_units,)\n```\n\n### Explanation\n- **Initialization**: The `__init__` method initializes the layer with the given number of units and input shape. The weights and biases are initialized to `None` and are set during the `initialize` method.\n- **Weight Initialization**: The `initialize` method sets up the weights using a uniform distribution and biases to zero. This ensures that the initial weights are small and help in maintaining the stability of the network during training.\n- **Forward Pass**: The `forward_pass` method computes the output by performing a dot product between the input and weights, then adding the bias. The input is stored for use in the backward pass.\n- **Backward Pass**: The `backward_pass` method computes the gradients of the loss with respect to the weights, biases, and input. If the layer is trainable, it updates the weights and biases using the respective optimizers.\n- **Parameter Count**: The `parameters` method returns the total number of trainable parameters in the layer, which is the sum of the weights and biases.\n- **Output Shape**: The `output_shape` method returns the shape of the output produced by the forward pass, which is determined by the number of neurons in the layer.\n\nThis implementation ensures that the Dense layer can be integrated into a neural network framework, providing the necessary functionality for training and inference."}
{"task_id": 41, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a 2D convolutional layer in Python. This function will process an input matrix using a specified convolutional kernel, padding, and stride. The goal is to compute the convolution of the input matrix with the kernel, considering the given padding and stride, and return the result as a list of lists with each element rounded to four decimal places.\n\n### Approach\n1. **Padding the Input Matrix**: The input matrix is padded with zeros on all sides to handle edge cases and control the output size. The padding is applied symmetrically around the input matrix.\n2. **Determine Output Dimensions**: The output dimensions are calculated based on the padded input matrix size, kernel size, and stride. The formula used is:\n   - Output height = (padded height - kernel height) // stride + 1\n   - Output width = (padded width - kernel width) // stride + 1\n3. **Initialize Output Matrix**: An output matrix of zeros is initialized with the computed dimensions.\n4. **Convolution Process**: For each possible position of the kernel over the padded input matrix, compute the dot product of the kernel and the corresponding patch of the input matrix. This value is then placed in the output matrix at the correct position.\n5. **Rounding and Conversion**: The output matrix is rounded to four decimal places and converted to a list of lists for the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Pad the input matrix with the specified padding\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), \n                         mode='constant', constant_values=0)\n    \n    # Get the dimensions of the kernel and padded input\n    kernel_height, kernel_width = kernel.shape\n    padded_height, padded_width = padded_input.shape\n    \n    # Calculate the output dimensions\n    output_height = (padded_height - kernel_height) // stride + 1\n    output_width = (padded_width - kernel_width) // stride + 1\n    \n    # Initialize the output matrix with zeros\n    output = np.zeros((output_height, output_width))\n    \n    # Perform the convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Calculate the starting indices for the current patch\n            start_row = i * stride\n            start_col = j * stride\n            end_row = start_row + kernel_height\n            end_col = start_col + kernel_width\n            \n            # Extract the current patch from the padded input\n            patch = padded_input[start_row:end_row, start_col:end_col]\n            \n            # Compute the convolution sum and store it in the output\n            output[i, j] = np.sum(patch * kernel)\n    \n    # Round the output to 4 decimal places and convert to a list\n    output = np.round(output, 4)\n    return output.tolist()\n```\n\n### Explanation\n- **Padding**: The input matrix is padded to ensure that the kernel can be applied to the edges of the input matrix, which helps in controlling the output size.\n- **Output Dimensions**: The output dimensions are calculated to ensure that the kernel can slide over the entire input matrix considering the stride.\n- **Convolution Process**: For each position of the kernel over the input matrix, the corresponding patch is extracted, multiplied element-wise with the kernel, and summed up to produce the output value for that position.\n- **Rounding and Conversion**: The result is rounded to four decimal places to meet the precision requirement and converted to a list for the final output.\n\nThis approach ensures that the convolution operation is performed efficiently and correctly, handling various edge cases and configurations of the input matrix, kernel, padding, and stride."}
{"task_id": 42, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function in Python. The ReLU function is a fundamental component in neural networks, known for its simplicity and effectiveness in introducing non-linearity to the model.\n\n### Approach\nThe ReLU function is defined such that for any input value `z`, it returns `z` if `z` is positive, and 0 otherwise. This can be efficiently implemented using Python's built-in `max` function, which returns the larger of two values. By using `max(z, 0)`, we ensure that the function returns `z` when `z` is positive and 0 when `z` is zero or negative.\n\n### Solution Code\n```python\ndef relu(z: float) -> float:\n    return max(z, 0)\n```\n\n### Explanation\nThe function `relu` takes a single float `z` as input. It uses the `max` function to compare `z` with 0. If `z` is greater than 0, `max(z, 0)` returns `z`. If `z` is 0 or negative, `max(z, 0)` returns 0. This approach is both concise and efficient, making it an optimal solution for implementing the ReLU activation function."}
{"task_id": 43, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the Ridge Regression loss. This loss function combines the Mean Squared Error (MSE) with a regularization term to prevent overfitting by penalizing large coefficients.\n\n### Approach\nThe Ridge Regression loss function is composed of two main parts:\n1. **Mean Squared Error (MSE)**: This measures the average squared difference between the predicted values and the true values. It is calculated as the mean of the squared residuals.\n2. **Regularization Term**: This term penalizes the magnitude of the coefficients to prevent overfitting. It is calculated as the sum of the squares of the coefficients multiplied by a regularization parameter, alpha.\n\nThe steps to compute the Ridge Regression loss are as follows:\n1. Compute the predicted values using the feature matrix `X` and the coefficients `w`.\n2. Calculate the residuals (differences between true values and predicted values).\n3. Compute the squared residuals and then the mean of these squared residuals to get the MSE.\n4. Compute the regularization term by summing the squares of the coefficients and multiplying by alpha.\n5. Combine the MSE and the regularization term to get the total loss.\n6. Round the result to four decimal places for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    residuals = y_true - y_pred\n    squared_errors = residuals ** 2\n    mse = np.mean(squared_errors)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)\n```\n\n### Explanation\n1. **Predicted Values Calculation**: The predicted values `y_pred` are obtained by performing a dot product of the feature matrix `X` and the coefficient vector `w`.\n2. **Residuals Calculation**: The residuals are the differences between the true values `y_true` and the predicted values `y_pred`.\n3. **Squared Errors and MSE**: The squared errors are computed by squaring the residuals. The mean of these squared errors gives the MSE.\n4. **Regularization Term**: The sum of the squares of the coefficients is computed and multiplied by the regularization parameter `alpha` to get the regularization term.\n5. **Total Loss**: The total loss is the sum of the MSE and the regularization term. This value is then rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the model not only fits the training data well (as measured by MSE) but also keeps the coefficients small to generalize better to unseen data (as enforced by the regularization term)."}
{"task_id": 44, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Leaky Rectified Linear Unit (Leaky ReLU) activation function. This function is a variant of the ReLU activation function, which is commonly used in neural networks. The Leaky ReLU function addresses the \"dying ReLU\" problem by allowing a small, non-zero gradient for negative input values.\n\n### Approach\nThe Leaky ReLU function is defined as follows:\n- For any input value \\( z \\) greater than zero, the function returns \\( z \\) itself.\n- For any input value \\( z \\) less than or equal to zero, the function returns \\( \\alpha \\times z \\), where \\( \\alpha \\) is a small positive constant (default value is 0.01).\n\nThe approach involves:\n1. Checking if the input value \\( z \\) is greater than zero.\n2. If \\( z \\) is positive, return \\( z \\).\n3. If \\( z \\) is zero or negative, return the product of \\( \\alpha \\) and \\( z \\).\n\nThis approach ensures that the function handles both positive and negative inputs appropriately, providing a small gradient for negative values to prevent the \"dying ReLU\" issue.\n\n### Solution Code\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    return z if z > 0 else alpha * z\n```\n\n### Explanation\n- The function `leaky_relu` takes two parameters: `z` (a float) and `alpha` (a float with a default value of 0.01).\n- The function uses a ternary operator to check if `z` is greater than zero. If true, it returns `z`.\n- If `z` is zero or negative, the function returns the product of `alpha` and `z`, which provides a small negative gradient for negative inputs.\n- This implementation efficiently handles both positive and negative inputs, ensuring that the function does not \"die\" for negative values, thus improving the training dynamics of neural networks."}
{"task_id": 45, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the linear kernel between two input vectors. The linear kernel is defined as the dot product (inner product) of the two vectors. This can be efficiently computed using numpy's dot function.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understanding the Linear Kernel**: The linear kernel is simply the dot product of two vectors. This operation is fundamental in linear algebra and is used extensively in machine learning for computing similarity between vectors.\n2. **Using Numpy for Efficient Computation**: Numpy provides a convenient and efficient way to compute the dot product using the `np.dot` function. This function handles the computation in a vectorized manner, which is both fast and memory efficient.\n3. **Handling Input Vectors**: The function assumes that the input vectors are in a format compatible with numpy's operations, typically 1D numpy arrays. If the inputs are not 1D arrays, numpy's dot function will handle them appropriately, but in the context of kernel functions, we expect 1D vectors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\n- **Importing Numpy**: The numpy library is imported to utilize its efficient array operations.\n- **Function Definition**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are the input vectors.\n- **Dot Product Calculation**: The function returns the result of `np.dot(x1, x2)`, which computes the dot product of the two input vectors. This result is the linear kernel value between the two vectors.\n\nThis approach ensures that the computation is both efficient and concise, leveraging numpy's optimized functions for numerical operations."}
{"task_id": 46, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the precision metric given two numpy arrays: `y_true` (true binary labels) and `y_pred` (predicted binary labels). Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n\n### Approach\n1. **True Positives (TP)**: These are cases where both the true label and the predicted label are 1. We can compute this by checking where both `y_true` and `y_pred` are 1 and summing these occurrences.\n2. **False Positives (FP)**: These are cases where the true label is 0 but the predicted label is 1. We compute this by checking where `y_true` is 0 and `y_pred` is 1 and summing these occurrences.\n3. **Precision Calculation**: Once we have TP and FP, precision is calculated as TP divided by the sum of TP and FP. If the sum of TP and FP is zero, we return 0 to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0\n    return tp / denominator\n```\n\n### Explanation\n- **True Positives (TP)**: We use boolean indexing to find where both `y_true` and `y_pred` are 1. Summing these boolean values gives the count of TP.\n- **False Positives (FP)**: Similarly, we find where `y_true` is 0 and `y_pred` is 1, and sum these values to get FP.\n- **Precision Calculation**: We compute the denominator as the sum of TP and FP. If this denominator is zero, it means there were no positive predictions, so we return 0. Otherwise, we return the ratio of TP to the denominator, which gives the precision.\n\nThis approach efficiently computes the precision metric using numpy operations, ensuring that we handle edge cases such as division by zero appropriately."}
{"task_id": 47, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs three variants of gradient descent: Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent. The function will use Mean Squared Error (MSE) as the loss function and will take an additional parameter to specify which variant to use.\n\n### Approach\n1. **Problem Analysis**: The task is to implement gradient descent variants using MSE loss. The function should handle three methods: batch, SGD, and mini-batch. Each method processes the data differently:\n   - **Batch Gradient Descent**: Uses the entire dataset in each iteration.\n   - **Stochastic Gradient Descent (SGD)**: Uses one random sample per iteration.\n   - **Mini-Batch Gradient Descent**: Uses a specified batch size of samples per iteration.\n\n2. **Intuitions and Insights**:\n   - For each iteration, the appropriate batch of data is selected based on the method.\n   - The predictions are computed using the current weights.\n   - The gradient of the loss with respect to the weights is calculated.\n   - The weights are updated using the learning rate and the computed gradient.\n\n3. **Algorithm Selection**:\n   - **Batch Method**: Uses all samples in each iteration.\n   - **SGD Method**: Randomly selects one sample in each iteration.\n   - **Mini-Batch Method**: Randomly selects a batch of specified size in each iteration.\n\n4. **Complexity Analysis**:\n   - The time complexity for each iteration depends on the method:\n     - Batch: O(n), where n is the number of samples.\n     - SGD: O(1).\n     - Mini-Batch: O(k), where k is the batch size.\n   - The overall complexity is O(n_iterations * complexity per iteration).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    weights = np.copy(weights)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_X = X\n            batch_y = y\n        elif method == 'sgd':\n            idx = np.random.randint(0, X.shape[0])\n            batch_X = X[idx:idx+1]\n            batch_y = y[idx:idx+1]\n        elif method == 'mini-batch':\n            batch_size = min(batch_size, X.shape[0])\n            idxs = np.random.choice(X.shape[0], batch_size, replace=False)\n            batch_X = X[idxs]\n            batch_y = y[idxs]\n        else:\n            raise ValueError(\"Invalid method\")\n        \n        y_pred = np.dot(batch_X, weights)\n        error = y_pred - batch_y\n        gradient = (1 / batch_size) * np.dot(batch_X.T, error)\n        weights -= learning_rate * gradient\n    \n    weights = np.round(weights, 4)\n    return weights.tolist()\n```\n\n### Explanation\n- **Initialization**: The function starts by copying the initial weights to avoid modifying the original array.\n- **Iteration Loop**: For each iteration, the appropriate batch of data is selected based on the method:\n  - **Batch Method**: Uses the entire dataset.\n  - **SGD Method**: Randomly selects one sample.\n  - **Mini-Batch Method**: Randomly selects a batch of specified size.\n- **Prediction and Error Calculation**: Computes the predictions using the current weights and calculates the error between predictions and actual values.\n- **Gradient Calculation**: Computes the gradient of the loss with respect to the weights.\n- **Weight Update**: Updates the weights using the learning rate and the computed gradient.\n- **Rounding and Return**: After all iterations, the weights are rounded to four decimal places and returned as a list.\n\nThis approach ensures that the function efficiently handles different gradient descent variants, providing flexibility and correctness for various optimization scenarios."}
{"task_id": 48, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that converts a given matrix into its Reduced Row Echelon Form (RREF). The RREF of a matrix is a special form where each leading entry in a row is 1, and all other elements in the column containing the leading 1 are zeros, except for the leading 1 itself. The function should handle various edge cases, including matrices with zero rows, all-zero rows, and matrices with more columns than rows.\n\n### Approach\nThe approach to solving this problem involves the following steps:\n\n1. **Initialization**: Convert the input matrix into a numpy array for easier manipulation and initialize the current row and column indices to 0.\n\n2. **Finding the Pivot Column**: For each row, find the first column (starting from the current column) that has a non-zero entry in any row from the current row downwards. This column is chosen as the pivot column.\n\n3. **Finding the Pivot Row**: Within the pivot column, find the first row (starting from the current row) that has a non-zero entry. This row is chosen as the pivot row.\n\n4. **Row Swapping**: Swap the current row with the pivot row to bring the pivot element to the top of the current column.\n\n5. **Normalization**: Normalize the pivot row so that the leading entry (pivot) becomes 1. This is done by dividing the entire row by the pivot value.\n\n6. **Elimination**: Use the pivot row to eliminate all other entries in the pivot column. This includes both the rows above and below the pivot row.\n\n7. **Increment Indices**: Move to the next row and the next column, and repeat the process until all rows and columns are processed.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=np.float64).copy()\n    rows = matrix.shape[0]\n    cols = matrix.shape[1]\n    \n    current_row = 0\n    current_col = 0\n    \n    while current_row < rows and current_col < cols:\n        # Find the pivot column\n        found = False\n        for j in range(current_col, cols):\n            for r in range(current_row, rows):\n                if abs(matrix[r, j]) > 1e-10:\n                    current_col = j\n                    found = True\n                    break\n            if found:\n                break\n        if not found:\n            break\n        \n        # Find the pivot row\n        pivot_row = None\n        for r in range(current_row, rows):\n            if abs(matrix[r, current_col]) > 1e-10:\n                pivot_row = r\n                break\n        if pivot_row is None:\n            current_col += 1\n            continue\n        \n        # Swap current_row and pivot_row\n        matrix[[current_row, pivot_row]] = matrix[[pivot_row, current_row]]\n        \n        # Normalize the pivot row\n        pivot_value = matrix[current_row, current_col]\n        matrix[current_row] = matrix[current_row] / pivot_value\n        \n        # Eliminate all other rows in current_col\n        for r in range(0, rows):\n            if r != current_row:\n                factor = matrix[r, current_col]\n                matrix[r] = matrix[r] - factor * matrix[current_row]\n        \n        current_row += 1\n        current_col += 1\n    \n    return matrix.tolist()\n```\n\n### Explanation\nThe function `rref` converts the input matrix into its Reduced Row Echelon Form by following a systematic approach. It iterates through each row and column, identifying pivot elements, normalizing rows, and eliminating other entries in the pivot column. This ensures that each leading entry is 1 and all other entries in the corresponding column are zero. The function handles various edge cases, including matrices with zero rows, all-zero rows, and matrices with more columns than rows, ensuring robustness and correctness. The use of numpy arrays allows for efficient matrix operations, and the inclusion of a small epsilon value (1e-10) helps manage floating-point precision issues."}
{"task_id": 49, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam optimization algorithm, which is an adaptive learning rate optimization method used in machine learning. Adam combines the benefits of two other optimization methods, AdaGrad and RMSProp, to provide efficient parameter updates.\n\n### Approach\nThe Adam optimization algorithm maintains two moving averages: the first moment (mean) and the second moment (uncentered variance) of the gradients. These moving averages are used to adaptively adjust the learning rate for each parameter. The steps involved in the Adam algorithm are as follows:\n\n1. **Initialization**: Start with initial parameter values and initialize the first and second moment estimates to zero vectors.\n2. **Gradient Computation**: For each iteration, compute the gradient of the objective function at the current parameter values.\n3. **Update Moments**: Update the first and second moment estimates using the computed gradient.\n4. **Bias Correction**: Adjust the first and second moment estimates to account for their bias towards zero, especially in the initial iterations.\n5. **Parameter Update**: Use the bias-corrected moments to update the parameters with an adaptive learning rate.\n\nThe algorithm iterates for a specified number of iterations, updating the parameters each time based on the computed gradients and the adaptive learning rate.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The parameters `x` are initialized from `x0`, and the first (`m`) and second (`v`) moment estimates are initialized to zero vectors of the same shape as `x`.\n2. **Gradient Computation**: For each iteration, the gradient of the objective function at the current parameter values is computed using the provided `grad` function.\n3. **Update Moments**: The first moment estimate `m` is updated using the gradient and the decay rate `beta1`. Similarly, the second moment estimate `v` is updated using the squared gradient and the decay rate `beta2`.\n4. **Bias Correction**: The bias-corrected first moment (`m_hat`) and second moment (`v_hat`) are computed by dividing `m` and `v` by `1 - beta1^t` and `1 - beta2^t` respectively, where `t` is the current iteration number.\n5. **Parameter Update**: The parameters are updated using the bias-corrected moments and the learning rate, ensuring numerical stability by adding a small constant `epsilon` to the denominator.\n\nAfter completing the specified number of iterations, the optimized parameters are rounded to four decimal places and returned as a Python list. This approach ensures efficient and adaptive parameter updates, making Adam a popular choice for training deep learning models."}
{"task_id": 50, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Lasso Regression algorithm using Gradient Descent. Lasso Regression adds an L1 penalty to the loss function, which helps in feature selection by shrinking some of the feature coefficients to zero.\n\n### Approach\n1. **Objective Function**: The objective function for Lasso Regression includes a penalty term that is the sum of the absolute values of the coefficients. This is given by:\n   \\[\n   J(w, b) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - (\\sum_{j=1}^p X_{ij} w_j + b))^2 + \\alpha \\sum_{j=1}^p |w_j|\n   \\]\n   where \\( \\alpha \\) is the regularization parameter.\n\n2. **Gradient Descent**: We use gradient descent to minimize the objective function. The gradients for the weights and bias are computed considering both the loss function and the L1 penalty term.\n\n3. **Initialization**: We start with initial weights and bias set to zero.\n\n4. **Iteration**: For each iteration, we compute the predictions, the error, and the gradients. The weights and bias are updated using the computed gradients and the learning rate.\n\n5. **Convergence Check**: The algorithm stops when the change in weights and bias falls below a specified tolerance level or when the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        error = y - y_pred\n        \n        gradient_w = (1 / n_samples) * (-X.T.dot(error)) + alpha * np.sign(w)\n        gradient_b = (1 / n_samples) * (-np.sum(error))\n        \n        new_w = w - learning_rate * gradient_w\n        new_b = b - learning_rate * gradient_b\n        \n        change_w = np.max(np.abs(new_w - w))\n        change_b = np.abs(new_b - b)\n        \n        if change_w < tol and change_b < tol:\n            break\n        \n        w = new_w\n        b = new_b\n    \n    w = np.round(w, 4)\n    b = np.round(b, 4)\n    \n    return (w.tolist(), b)\n```\n\n### Explanation\n1. **Initialization**: The weights `w` are initialized to zeros, and the bias `b` is set to zero.\n\n2. **Prediction and Error Calculation**: For each iteration, the predictions `y_pred` are computed using the current weights and bias. The error is the difference between the actual values `y` and the predicted values.\n\n3. **Gradient Calculation**: The gradients for the weights and bias are computed. The gradient for the weights includes the L1 penalty term, which is the sign of the weights multiplied by the regularization parameter `alpha`.\n\n4. **Update Weights and Bias**: The weights and bias are updated using the computed gradients and the learning rate.\n\n5. **Convergence Check**: The algorithm checks if the changes in weights and bias are below the tolerance level. If they are, the loop breaks early.\n\n6. **Rounding and Return**: After the loop, the weights and bias are rounded to four decimal places and returned as a tuple, with the weights as a list and the bias as a scalar.\n\nThis approach ensures that the Lasso Regression model is trained using gradient descent, effectively incorporating the L1 penalty to encourage sparsity in the feature coefficients."}
{"task_id": 51, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Optimal String Alignment (OSA) distance between two given strings. The OSA distance, also known as the Damerau\u2013Levenshtein distance, represents the minimum number of edits required to transform one string into another. The allowed edit operations are insert, delete, substitute, and transpose two adjacent characters, each costing 1 unit.\n\n### Approach\nThe approach to solve this problem involves using dynamic programming (DP) to efficiently compute the minimum number of edits. The key idea is to build a DP table where each cell (i, j) represents the minimum number of edits needed to convert the first i characters of the source string into the first j characters of the target string.\n\n1. **Initialization**: Create a DP table with dimensions (len(source)+1) x (len(target)+1). The first row and column are initialized to represent the base cases where one of the strings is empty.\n\n2. **Filling the DP Table**: For each character in the source and target strings, compute the cost of four possible operations:\n   - **Substitution**: Replace the current character of the source with the current character of the target.\n   - **Insertion**: Insert the current character of the target into the source.\n   - **Deletion**: Delete the current character of the source.\n   - **Transposition**: Swap two adjacent characters in the source if they match the corresponding characters in the target.\n\n3. **Compute Minimum Cost**: For each cell (i, j), the value is the minimum of the costs computed for the four operations.\n\n### Solution Code\n```python\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    # Create a DP table with (m+1) rows and (n+1) columns\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Initialize the first row and column\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    # Fill the DP table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            substitution = dp[i-1][j-1] + (0 if source[i-1] == target[j-1] else 1)\n            insertion = dp[i][j-1] + 1\n            deletion = dp[i-1][j] + 1\n            transposition = float('inf')\n            if i >= 2 and j >= 2:\n                if source[i-2] == target[j-1] and source[i-1] == target[j-2]:\n                    transposition = dp[i-2][j-2] + 1\n            dp[i][j] = min(substitution, insertion, deletion, transposition)\n    \n    return dp[m][n]\n```\n\n### Explanation\n- **Initialization**: The DP table is initialized such that converting an empty string to a string of length j requires j insertions, and converting a string of length i to an empty string requires i deletions.\n- **Filling the Table**: For each character in the source and target strings, the cost of each operation is computed. The substitution cost is based on whether the characters are the same. The transposition cost is considered only if the previous two characters in the source and target are swapped.\n- **Result**: The value at dp[m][n] gives the minimum number of edits required to transform the entire source string into the target string.\n\nThis approach efficiently computes the OSA distance using dynamic programming, ensuring that all possible edit operations are considered to find the optimal solution."}
{"task_id": 52, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to calculate the recall metric for a binary classification problem. Recall is a measure of how well a model can identify all the positive instances from the dataset. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).\n\n### Approach\n1. **Convert Inputs to Numpy Arrays**: Convert the input lists `y_true` and `y_pred` into numpy arrays for efficient element-wise operations.\n2. **Calculate True Positives (TP)**: Count the number of instances where both the true label and the predicted label are 1.\n3. **Calculate False Negatives (FN)**: Count the number of instances where the true label is 1 but the predicted label is 0.\n4. **Compute Denominator**: The denominator for recall is the sum of TP and FN. If this sum is zero, it means there are no positive instances in the true labels, so the recall is set to 0.0 to avoid division by zero.\n5. **Calculate Recall**: If the denominator is non-zero, compute recall as TP divided by the denominator. Round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        rec = tp / denominator\n        return round(rec, 3)\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: This allows us to use efficient element-wise operations to compute TP and FN.\n- **True Positives (TP)**: These are cases where the model correctly predicts the positive class.\n- **False Negatives (FN)**: These are cases where the model incorrectly predicts the negative class for actual positive instances.\n- **Denominator Check**: Ensures we do not divide by zero, which would happen if there are no positive instances in the true labels.\n- **Recall Calculation**: The ratio of TP to the sum of TP and FN gives the recall, which is then rounded to three decimal places for precision.\n\nThis approach efficiently computes the recall metric using numpy operations, ensuring accuracy and handling edge cases where division by zero might occur."}
{"task_id": 53, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the self-attention mechanism, a key component of transformer models. This mechanism allows the model to focus on different parts of the input sequence dynamically, which is crucial for tasks like natural language processing and computer vision.\n\n### Approach\nThe self-attention mechanism involves several steps:\n1. **Compute Queries, Keys, and Values**: Transform the input sequence using weight matrices to get queries (Q), keys (K), and values (V).\n2. **Calculate Attention Scores**: Compute the scores by taking the dot product of queries and keys, then scale these scores by the square root of the key dimension.\n3. **Apply Softmax**: Convert the scores into attention weights using the softmax function to ensure they sum to 1.\n4. **Compute Output**: Multiply the attention weights by the values to get the final output.\n5. **Format Output**: Round the output to four decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Compute Q, K, V matrices\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    \n    # Compute attention scores\n    scores = Q @ K.T\n    d_k = K.shape[1]\n    scores = scores / np.sqrt(d_k)\n    \n    # Compute softmax to get attention weights\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    \n    # Compute the output\n    output = attention_weights @ V\n    \n    # Round to 4 decimal places and convert to list\n    output = np.round(output, 4)\n    return output.tolist()\n```\n\n### Explanation\n1. **Compute Q, K, V**: The input matrix `X` is transformed using weight matrices `W_q`, `W_k`, and `W_v` to produce queries, keys, and values respectively.\n2. **Attention Scores**: The scores are calculated by taking the dot product of queries and the transpose of keys. These scores are then scaled by the square root of the key dimension to prevent values from becoming too large.\n3. **Softmax**: The scores are converted into probabilities (attention weights) using the softmax function, ensuring each row sums to 1.\n4. **Output Calculation**: The attention weights are multiplied by the values to produce the final output, which is then rounded to four decimal places and converted to a list for the result.\n\nThis approach efficiently computes the self-attention mechanism, allowing the model to focus on relevant parts of the input sequence dynamically."}
{"task_id": 54, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple Recurrent Neural Network (RNN) cell that processes a sequence of input vectors and produces the final hidden state. The RNN cell uses the tanh activation function for updating the hidden state. The function takes several inputs, including the sequence of input vectors, initial hidden state, weight matrices, and a bias vector, and returns the final hidden state after processing the entire sequence.\n\n### Approach\n1. **Initialization**: Convert the initial hidden state, weight matrices, and bias vector from lists to numpy arrays for efficient matrix operations.\n2. **Iterate Over Input Sequence**: For each input vector in the sequence, compute the new hidden state using the RNN update formula:\n   \\[\n   h_t = \\tanh(Wx \\cdot x_t + Wh \\cdot h_{t-1} + b)\n   \\]\n   where \\( h_t \\) is the new hidden state, \\( x_t \\) is the current input vector, \\( h_{t-1} \\) is the previous hidden state, \\( Wx \\) is the input-to-hidden weight matrix, \\( Wh \\) is the hidden-to-hidden weight matrix, and \\( b \\) is the bias vector.\n3. **Update Hidden State**: After processing each input vector, update the hidden state to the newly computed value.\n4. **Rounding and Conversion**: After processing all input vectors, round the final hidden state to four decimal places and convert it back to a list for the output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert initial hidden state to a numpy array\n    h = np.array(initial_hidden_state)\n    \n    # Convert weight matrices and bias to numpy arrays\n    Wx_np = np.array(Wx)\n    Wh_np = np.array(Wh)\n    b_np = np.array(b)\n    \n    # Iterate over each input in the sequence\n    for x in input_sequence:\n        x_t = np.array(x)\n        # Compute the input contribution\n        input_contrib = Wx_np.dot(x_t)\n        # Compute the hidden contribution\n        hidden_contrib = Wh_np.dot(h)\n        # Compute pre-activation\n        pre_activation = input_contrib + hidden_contrib + b_np\n        # Apply tanh activation\n        h = np.tanh(pre_activation)\n    \n    # Round the final hidden state to four decimal places\n    h_rounded = np.round(h, 4)\n    # Convert to list and return\n    return h_rounded.tolist()\n```\n\n### Explanation\n- **Conversion to Numpy Arrays**: The function starts by converting the initial hidden state, weight matrices, and bias vector into numpy arrays to facilitate efficient matrix operations.\n- **Processing Each Input**: For each input vector in the sequence, the function computes the contributions from both the input and the previous hidden state, adds the bias, and applies the tanh activation function to update the hidden state.\n- **Rounding and Conversion**: After processing all input vectors, the final hidden state is rounded to four decimal places and converted back to a list to meet the output requirements.\n\nThis approach ensures that the RNN cell efficiently processes the input sequence and updates the hidden state correctly, providing the final hidden state as required."}
{"task_id": 55, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that applies a 2D translation matrix to a set of points. The translation matrix will move each point in the set by specified distances in the x and y directions.\n\n### Approach\nThe approach involves using matrix multiplication to apply the translation. Here's a step-by-step breakdown:\n\n1. **Convert Points to Numpy Array**: Convert the input list of points into a numpy array for efficient matrix operations.\n2. **Homogeneous Coordinates**: Convert each point into homogeneous coordinates by adding a third coordinate of 1. This allows us to use matrix multiplication for translation.\n3. **Transformation Matrix**: Create a 3x3 translation matrix that includes the specified translation distances `tx` and `ty`.\n4. **Matrix Multiplication**: Multiply the transformation matrix with the matrix of homogeneous points. This results in a new matrix where each point has been translated.\n5. **Convert Back to 2D Coordinates**: Extract the translated x and y coordinates from the resulting matrix and convert them back to a list of points.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    if not points:\n        return []\n    \n    # Convert the list of points to a numpy array\n    points_np = np.array(points)\n    \n    # Create homogeneous coordinates by adding a third column of ones\n    homogeneous = np.hstack([points_np, np.ones((points_np.shape[0], 1))])\n    \n    # Create the translation matrix\n    trans_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Perform the matrix multiplication\n    translated = trans_matrix.dot(homogeneous.T)\n    \n    # Convert back to 2D points and transpose to get the correct shape\n    translated_points = translated[:2, :].T\n    \n    # Convert the numpy array back to a Python list\n    return translated_points.tolist()\n```\n\n### Explanation\n- **Conversion to Numpy Array**: The input list of points is converted into a numpy array to leverage numpy's efficient matrix operations.\n- **Homogeneous Coordinates**: Each point is converted to a 3-element vector [x, y, 1] to allow the use of matrix multiplication for translation.\n- **Transformation Matrix**: The translation matrix is a 3x3 matrix that, when multiplied by a point in homogeneous coordinates, translates the point by `tx` in the x-direction and `ty` in the y-direction.\n- **Matrix Multiplication**: The transformation matrix is multiplied with the matrix of homogeneous points. This operation efficiently applies the translation to all points simultaneously.\n- **Conversion Back to 2D**: The resulting matrix from the multiplication is converted back to 2D coordinates by taking the first two rows and transposing the result to match the original point format.\n\nThis approach ensures that the translation is applied efficiently and correctly to all points in the input list."}
{"task_id": 56, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Kullback-Leibler (KL) divergence between two normal distributions. The KL divergence is a measure of how one probability distribution diverges from another. For two normal distributions, there is a specific formula to compute this divergence efficiently.\n\n### Approach\nThe KL divergence between two normal distributions \\( P \\sim N(\\mu_P, \\sigma_P^2) \\) and \\( Q \\sim N(\\mu_Q, \\sigma_Q^2) \\) can be computed using the following formula:\n\n\\[ \\text{KL}(P || Q) = \\frac{(\\mu_P - \\mu_Q)^2 + \\sigma_P^2 - \\sigma_Q^2}{2 \\sigma_Q^2} + \\ln\\left(\\frac{\\sigma_Q}{\\sigma_P}\\right) \\]\n\nThis formula consists of two main parts:\n1. The first part involves the squared difference of the means and the difference of the variances, all divided by twice the variance of the second distribution.\n2. The second part is the natural logarithm of the ratio of the standard deviations of the two distributions.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    delta_mu = mu_p - mu_q\n    term1 = (delta_mu ** 2 + sigma_p**2 - sigma_q**2) / (2 * sigma_q**2)\n    term2 = np.log(sigma_q / sigma_p)\n    return term1 + term2\n```\n\n### Explanation\n1. **Compute the Difference in Means**: Calculate the difference between the means of the two distributions, \\( \\mu_P \\) and \\( \\mu_Q \\).\n2. **Compute the First Term**: This term involves the squared difference of the means and the difference of the variances, all divided by twice the variance of the second distribution.\n3. **Compute the Log Term**: This term is the natural logarithm of the ratio of the standard deviations of the two distributions.\n4. **Sum the Terms**: The KL divergence is the sum of the first term and the log term.\n\nThis approach efficiently computes the KL divergence using the specific formula for normal distributions, ensuring accuracy and performance."}
{"task_id": 57, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gauss-Seidel method, an iterative technique for solving a system of linear equations. The method updates each component of the solution vector using the most recent values available during the iteration process, which can lead to faster convergence compared to other methods like the Jacobi method.\n\n### Approach\n1. **Initialization**: If an initial guess for the solution vector is not provided, we initialize it as a vector of zeros. The size of this vector matches the size of the right-hand side vector `b`.\n2. **Iteration Process**: For each iteration, we update each component of the solution vector using the most recent values. Specifically, for each equation, we compute the new value of the solution component using the already updated values for previous components and the old values for subsequent components.\n3. **Matrix and Vector Handling**: The matrix `A` is a square matrix, and the vectors `x` and `b` are of the same length. We use numpy arrays for efficient computation.\n4. **Rounding and Conversion**: After completing the specified number of iterations, we round the solution vector to four decimal places and convert it to a list for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.float64)\n    else:\n        x = np.array(x_ini, dtype=np.float64)\n    \n    for _ in range(n):\n        for i in range(len(x)):\n            sum1 = 0.0\n            for j in range(i):\n                sum1 += A[i, j] * x[j]\n            sum2 = 0.0\n            for j in range(i + 1, len(x)):\n                sum2 += A[i, j] * x[j]\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    \n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by checking if an initial guess `x_ini` is provided. If not, it initializes `x` as a zero vector of the same length as `b`.\n2. **Iteration Process**: For each iteration, the function loops through each component of the solution vector `x`. For each component, it computes the sum of the products of the matrix elements and the corresponding solution vector elements, using the most recent updates for previous components and the old values for subsequent components.\n3. **Update Step**: Each component of `x` is updated using the computed sums and the corresponding element from the right-hand side vector `b`.\n4. **Rounding and Conversion**: After completing the specified number of iterations, the solution vector is rounded to four decimal places and converted to a list for the final output.\n\nThis approach ensures that the solution vector is iteratively improved using the most recent values, leading to efficient convergence for many types of linear systems."}
{"task_id": 58, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Gaussian Elimination method with partial pivoting to solve a system of linear equations. This method transforms the system into an upper triangular matrix, which can then be solved using back substitution.\n\n### Approach\n1. **Gaussian Elimination with Partial Pivoting**:\n   - **Partial Pivoting**: For each pivot column, find the row with the maximum absolute value in that column to avoid division by zero and improve numerical stability. Swap this row with the current pivot row.\n   - **Forward Elimination**: For each pivot row, eliminate the entries below the pivot by subtracting a multiple of the pivot row from each of the subsequent rows. This results in an upper triangular matrix.\n\n2. **Back Substitution**:\n   - Starting from the last equation, solve for each variable by substituting the known values of the variables from the subsequent equations.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    A = A.copy()\n    b = b.copy().flatten()\n    n = A.shape[0]\n    M = np.hstack((A, b.reshape(-1, 1)))\n    \n    for pivot in range(n):\n        max_row = pivot\n        for i in range(pivot + 1, n):\n            if abs(M[i, pivot]) > abs(M[max_row, pivot]):\n                max_row = i\n        M[[pivot, max_row]] = M[[max_row, pivot]]\n        \n        for row in range(pivot + 1, n):\n            factor = M[row, pivot] / M[pivot, pivot]\n            M[row, pivot:] -= factor * M[pivot, pivot:]\n    \n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        sum_val = 0.0\n        for j in range(i + 1, n):\n            sum_val += M[i, j] * x[j]\n        x[i] = (M[i, n] - sum_val) / M[i, i]\n    \n    x = np.round(x, 4)\n    return x.tolist()\n```\n\n### Explanation\n1. **Augmented Matrix**: The coefficient matrix `A` and the constants vector `b` are combined into an augmented matrix `M` for easier manipulation during the elimination process.\n2. **Partial Pivoting**: For each pivot column, the row with the maximum absolute value in that column is identified and swapped with the current pivot row to ensure numerical stability.\n3. **Forward Elimination**: Each element below the pivot in the current column is eliminated by subtracting a multiple of the pivot row from the current row. This transforms the matrix into an upper triangular form.\n4. **Back Substitution**: Starting from the last row, each variable is solved by substituting the known values of the subsequent variables. This is done iteratively from the last row to the first row.\n\nThis approach ensures that the system of linear equations is solved efficiently and accurately, even for larger matrices, by leveraging the properties of upper triangular matrices and back substitution."}
{"task_id": 59, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Long Short-Term Memory (LSTM) network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs. The LSTM network should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n\n### Approach\n1. **Initialization**: The LSTM class is initialized with input size and hidden size. It initializes weights and biases for the forget gate, input gate, candidate cell state, and output gate.\n2. **Forward Pass**: The forward method processes each input in the sequence, updating the hidden and cell states at each time step.\n3. **Gates Computation**: For each time step, the forget gate, input gate, candidate cell state, and output gate are computed using the current input and the previous hidden state.\n4. **State Updates**: The cell state and hidden state are updated using the computed gates.\n5. **Result Collection**: The hidden states at each time step, as well as the final hidden and cell states, are collected and returned after rounding to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        h_prev = initial_hidden_state.copy()\n        c_prev = initial_cell_state.copy()\n\n        for t in range(T):\n            x_t = x[t]\n            # Concatenate previous hidden state and current input\n            concat = np.concatenate((h_prev, x_t), axis=0)\n            \n            # Compute gates\n            f = np.expit(self.Wf.dot(concat) + self.bf)\n            i = np.expit(self.Wi.dot(concat) + self.bi)\n            c_tilde = np.tanh(self.Wc.dot(concat) + self.bc)\n            o = np.expit(self.Wo.dot(concat) + self.bo)\n            \n            # Update cell state\n            c_t = f * c_prev + i * c_tilde\n            # Update hidden state\n            h_t = o * np.tanh(c_t)\n            \n            # Append the rounded hidden state to the list\n            hidden_states.append(np.round(h_t, 4).reshape(-1).tolist())\n            \n            # Update previous states for next iteration\n            h_prev = h_t\n            c_prev = c_t\n        \n        # Prepare final states\n        final_h = np.round(h_prev, 4).reshape(-1).tolist()\n        final_c = np.round(c_prev, 4).reshape(-1).tolist()\n        \n        return (hidden_states, final_h, final_c)\n```\n\n### Explanation\n1. **Initialization**: The LSTM class initializes the weights and biases for the gates. These weights and biases are used to compute the gates at each time step.\n2. **Forward Method**: The forward method processes each input in the sequence. For each input, it computes the gates (forget, input, candidate cell state, and output) using the current input and the previous hidden state.\n3. **Gates Computation**: The gates are computed using matrix multiplication and activation functions (sigmoid for forget, input, and output gates; tanh for the candidate cell state).\n4. **State Updates**: The cell state is updated using the forget gate, input gate, and candidate cell state. The hidden state is updated using the output gate and the new cell state.\n5. **Result Collection**: The hidden states at each time step are collected and rounded to four decimal places. The final hidden and cell states are also rounded and converted to lists for the output.\n\nThis approach ensures that the LSTM network correctly processes the input sequence and updates the hidden and cell states at each time step, providing the required outputs."}
{"task_id": 60, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that computes the TF-IDF (Term Frequency-Inverse Document Frequency) scores for a given query against a corpus of documents. The function should handle various edge cases, such as empty documents, empty corpus, and query terms not present in the corpus.\n\n### Approach\n1. **Check for Empty Corpus**: If the corpus is empty, the function should return an empty list immediately.\n2. **Precompute Word Counts**: For each document in the corpus, precompute the word counts to efficiently calculate term frequencies later.\n3. **Compute Document Frequencies (DF)**: For each word in the query, compute how many documents contain that word.\n4. **Calculate TF-IDF Scores**: For each document and each query word, compute the Term Frequency (TF) and Inverse Document Frequency (IDF). Use these to compute the TF-IDF score, applying smoothing to avoid division by zero in IDF calculation.\n5. **Handle Edge Cases**: Ensure that empty documents and query terms not present in the corpus are handled gracefully.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    \n    N = len(corpus)\n    query_words = query\n    \n    # Precompute word counts for each document\n    word_counts_list = []\n    for doc in corpus:\n        word_counts = {}\n        for word in doc:\n            word_counts[word] = word_counts.get(word, 0) + 1\n        word_counts_list.append(word_counts)\n    \n    # Precompute document frequencies for each query word\n    df_dict = {}\n    for word in query_words:\n        df = 0\n        for word_counts in word_counts_list:\n            if word in word_counts:\n                df += 1\n        df_dict[word] = df\n    \n    # Compute TF-IDF scores for each document and each query word\n    result = []\n    for i in range(len(corpus)):\n        doc = corpus[i]\n        word_counts = word_counts_list[i]\n        doc_len = len(doc)\n        doc_scores = []\n        for word in query_words:\n            # Calculate Term Frequency (TF)\n            if doc_len == 0:\n                tf = 0.0\n            else:\n                count = word_counts.get(word, 0)\n                tf = count / doc_len\n            \n            # Calculate Inverse Document Frequency (IDF) with smoothing\n            df = df_dict[word]\n            idf = np.log10((N + 1) / (df + 1))\n            \n            # Compute TF-IDF score\n            tf_idf = tf * idf\n            # Round to 4 decimal places\n            rounded = round(tf_idf, 4)\n            doc_scores.append(rounded)\n        result.append(doc_scores)\n    \n    return result\n```\n\n### Explanation\n1. **Empty Corpus Handling**: The function first checks if the corpus is empty and returns an empty list if true.\n2. **Word Counts Precomputation**: For each document, a dictionary is created to count the occurrences of each word, which helps in efficiently calculating term frequencies later.\n3. **Document Frequencies Calculation**: For each word in the query, the number of documents containing that word is computed and stored in a dictionary.\n4. **TF-IDF Calculation**: For each document and each query word, the term frequency (TF) is calculated as the ratio of the word's count in the document to the document's length. The inverse document frequency (IDF) is calculated using a smoothed formula to avoid division by zero. The TF-IDF score is the product of TF and IDF, rounded to four decimal places.\n5. **Edge Cases Handling**: The function handles empty documents by setting TF to zero and ensures that query terms not present in the corpus are processed correctly with appropriate smoothing in IDF calculation.\n\nThis approach ensures that the function is robust and handles various edge cases efficiently, providing accurate TF-IDF scores for the given query against the corpus."}
{"task_id": 61, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the F-Score for a binary classification task. The F-Score is a measure that combines both Precision and Recall into a single metric, providing a balanced measure of a model's performance. The function should handle various edge cases, such as division by zero, and return the F-Score rounded to three decimal places.\n\n### Approach\n1. **Compute True Positives (TP), False Positives (FP), and False Negatives (FN)**:\n   - TP is the count of cases where both the true label and the predicted label are 1.\n   - FP is the count of cases where the true label is 0 but the predicted label is 1.\n   - FN is the count of cases where the true label is 1 but the predicted label is 0.\n\n2. **Calculate Precision and Recall**:\n   - Precision is the ratio of TP to the sum of TP and FP. If the sum of TP and FP is zero, precision is set to 0.\n   - Recall is the ratio of TP to the sum of TP and FN. If the sum of TP and FN is zero, recall is set to 0.\n\n3. **Compute the F-Score**:\n   - The F-Score is calculated using the formula: \n     \\[\n     F\\text{-Score} = \\frac{(1 + \\beta^2) \\times \\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}\n     \\]\n   - If the denominator in the F-Score formula is zero, the F-Score is set to 0.\n\n4. **Return the Result**:\n   - The result is rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Compute precision\n    denom_precision = tp + fp\n    precision = tp / denom_precision if denom_precision != 0 else 0.0\n    \n    # Compute recall\n    denom_recall = tp + fn\n    recall = tp / denom_recall if denom_recall != 0 else 0.0\n    \n    # Compute F-score\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = (beta ** 2 * precision) + recall\n    \n    if denominator == 0:\n        f = 0.0\n    else:\n        f = numerator / denominator\n    \n    return round(f, 3)\n```\n\n### Explanation\n- **True Positives (TP)**: These are cases where the model correctly predicts the positive class.\n- **False Positives (FP)**: These are cases where the model incorrectly predicts the positive class when the true label is negative.\n- **False Negatives (FN)**: These are cases where the model incorrectly predicts the negative class when the true label is positive.\n- **Precision**: This measures the accuracy of positive predictions. It is the ratio of TP to the sum of TP and FP.\n- **Recall**: This measures the ability of the model to find all positive cases. It is the ratio of TP to the sum of TP and FN.\n- **F-Score**: This is a weighted harmonic mean of Precision and Recall, providing a balanced measure of both metrics. The weight is determined by the parameter beta, which allows adjusting the importance of Precision and Recall.\n\nThe function handles edge cases such as division by zero by setting Precision, Recall, and F-Score to 0 when necessary, ensuring robustness and correctness across various input scenarios."}
{"task_id": 62, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple Recurrent Neural Network (RNN) with backpropagation through time (BPTT). The RNN will process sequential data, update hidden states, and adjust weights based on the error gradient using BPTT.\n\n### Approach\n1. **Forward Pass**: The RNN processes each input in the sequence step by step. For each time step, it computes the hidden state using the previous hidden state and the current input. The output is then generated from the hidden state. The forward pass returns the outputs, the last input, and the hidden states for each time step.\n\n2. **Backward Pass (BPTT)**: This involves computing the gradients of the loss with respect to the weights and biases by backpropagating the error through each time step. The gradients are accumulated and used to update the weights and biases using a specified learning rate.\n\n### Solution Code\n```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(x)\n        outputs = []\n        hidden_states = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for t in range(T):\n            x_t = x[t]\n            h_t = np.tanh(self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h)\n            y_t = self.W_hy @ h_t + self.b_y\n            outputs.append(y_t)\n            hidden_states.append(h_t)\n            h_prev = h_t\n        return outputs, x[-1], hidden_states\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through time (BPTT) to adjust weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        T = len(input_sequence)\n        delta_h_next = np.zeros_like(hidden_states[0])\n\n        for t in reversed(range(T)):\n            x_t = input_sequence[t]\n            h_t = hidden_states[t]\n            y_t = outputs[t]\n"}
{"task_id": 63, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. The CG method is particularly useful for systems where the matrix is large and sparse, making direct methods computationally expensive.\n\n### Approach\nThe Conjugate Gradient method works by iteratively improving the solution vector until the residual (the difference between the current solution and the exact solution) is below a specified tolerance. The steps involved in the CG method are as follows:\n\n1. **Initialization**: Start with an initial guess for the solution vector. If no initial guess is provided, use a zero vector.\n2. **Residual Calculation**: Compute the initial residual vector, which is the difference between the right-hand side vector and the product"}
{"task_id": 64, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the Gini Impurity for a given list of class labels. Gini Impurity is a measure used in decision tree algorithms to determine the impurity or disorder within a node. The lower the Gini Impurity, the more homogeneous the node is.\n\n### Approach\nThe Gini Impurity is calculated using the following steps:\n\n1. **Count Class Occurrences**: Determine the frequency of each class in the given list of class labels.\n2. **Calculate Probabilities**: For each class, compute the probability of that class occurring by dividing its count by the total number of elements.\n3. **Sum of Squared Probabilities**: Square each probability and sum these squared values.\n4. **Compute Gini Impurity**: Subtract the sum of squared probabilities from 1 to get the Gini Impurity.\n5. **Round the Result**: Return the result rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0  # Handle empty list case, though problem assumes non-empty\n    \n    unique, counts = np.unique(y, return_counts=True)\n    total = len(y)\n    sum_sq = 0.0\n    for count in counts:\n        p = count / total\n        sum_sq += p ** 2\n    gini = 1 - sum_sq\n    return round(gini, 3)\n```\n\n### Explanation\n1. **Count Class Occurrences**: Using `np.unique(y, return_counts=True)`, we get the unique class labels and their respective counts in the list.\n2. **Calculate Probabilities**: For each unique class, the probability is calculated by dividing the count of that class by the total number of elements in the list.\n3. **Sum of Squared Probabilities**: Each probability is squared and summed up. This sum represents the probability that a randomly selected element from the set would be correctly labeled.\n4. **Compute Gini Impurity**: The Gini Impurity is obtained by subtracting the sum of squared probabilities from 1. This value indicates the probability that a randomly selected element would be incorrectly labeled.\n5. **Round the Result**: The result is rounded to three decimal places to provide a precise and readable output.\n\nThis approach efficiently computes the Gini Impurity using numpy for handling the counts and probabilities, ensuring the solution is both concise and performant."}
{"task_id": 65, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a given dense matrix into its Compressed Row Sparse (CSR) format. The CSR format efficiently stores sparse matrices by only keeping track of non-zero elements, their column indices, and the cumulative count of non-zero elements per row.\n\n### Approach\nThe approach to convert a dense matrix to CSR format involves the following steps:\n\n1. **Initialization**: Start by initializing three lists: `values` to store non-zero elements, `col_indices` to store the column indices of these non-zero elements, and `row_pointers` to store the cumulative count of non-zero elements up to each row.\n\n2. **Iterate through each row**: For each row in the dense matrix, collect all non-zero elements and their corresponding column indices. This helps in building the `values` and `col_indices` lists.\n\n3. **Update row pointers**: For each row, calculate the number of non-zero elements and update the `row_pointers` list to reflect the cumulative count. This helps in determining the starting index of each row in the `values` list.\n\n4. **Return the result**: After processing all rows, return the three lists as a tuple representing the CSR format.\n\n### Solution Code\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]  # Start with 0 for the first row\n\n    for row in dense_matrix:\n        non_zero = []\n        cols = []\n        for idx, val in enumerate(row):\n            if val != 0:\n                non_zero.append(val)\n                cols.append(idx)\n        count = len(non_zero)\n        row_pointers.append(row_pointers[-1] + count)\n        values.extend(non_zero)\n        col_indices.extend(cols)\n\n    return (values, col_indices, row_pointers)\n```\n\n### Explanation\n- **Initialization**: The `values`, `col_indices`, and `row_pointers` lists are initialized. The `row_pointers` list starts with a zero to indicate the starting index of the first row.\n\n- **Row Processing**: For each row, we iterate through each element. Non-zero elements are collected into `non_zero` and their column indices into `cols`.\n\n- **Updating Pointers**: The number of non-zero elements in the current row is determined, and this count is added to the last value in `row_pointers` to get the starting index of the next row. This cumulative count is appended to `row_pointers`.\n\n- **Building Lists**: The non-zero elements and their column indices are extended into the `values` and `col_indices` lists respectively.\n\nThis approach efficiently constructs the CSR format by only storing necessary information, thus optimizing memory usage for sparse matrices."}
{"task_id": 66, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the orthogonal projection of a vector `v` onto another vector `L`. The orthogonal projection of `v` onto `L` is the vector on `L` that is closest to `v`. This can be achieved using vector projection formulas.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Compute the Dot Product**: Calculate the dot product of vectors `v` and `L`. The dot product is the sum of the products of corresponding components of the vectors.\n2. **Compute the Squared Magnitude**: Calculate the squared magnitude of vector `L`, which is the sum of the squares of its components.\n3. **Calculate the Scalar Multiplier**: The scalar multiplier is obtained by dividing the dot product by the squared magnitude of `L`.\n4. **Compute the Projection Vector**: Multiply each component of vector `L` by the scalar multiplier to get the projection vector.\n5. **Round the Result**: Round each component of the projection vector to three decimal places for the final result.\n\n### Solution Code\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum(x * y for x, y in zip(v, L))\n    squared_magnitude = sum(x ** 2 for x in L)\n    t = dot_product / squared_magnitude\n    projection = [t * x for x in L]\n    return [round(num, 3) for num in projection]\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of vectors `v` and `L` is computed using a generator expression that multiplies corresponding elements and sums them up.\n2. **Squared Magnitude Calculation**: The squared magnitude of `L` is computed by summing the squares of each component of `L`.\n3. **Scalar Multiplier**: The scalar `t` is calculated by dividing the dot product by the squared magnitude. This scalar determines how much of `v` lies in the direction of `L`.\n4. **Projection Vector**: Each component of `L` is multiplied by the scalar `t` to get the projection vector, which lies on the line defined by `L`.\n5. **Rounding**: Each component of the projection vector is rounded to three decimal places to ensure the result is formatted correctly.\n\nThis approach efficiently computes the orthogonal projection using basic vector operations, ensuring accuracy and clarity in the result."}
{"task_id": 67, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to convert a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format efficiently stores sparse matrices by only keeping track of non-zero elements, along with their row indices and column pointers.\n\n### Approach\nThe approach to convert a dense matrix into CSC format involves the following steps:\n\n1. **Initialization**: Start by initializing the necessary lists to store the non-zero values, their corresponding row indices, and the column pointers.\n2. **Iterate Over Columns**: For each column in the dense matrix, iterate through each row to collect non-zero elements.\n3. **Collect Non-Zero Elements**: For each non-zero element found, add it to the values list and record its row index in the row indices list.\n4. **Update Column Pointers**: After processing each column, update the column pointer list to indicate the starting index of the next column in the values list.\n\nThe column pointer list is constructed such that each entry points to the start of the corresponding column in the values list. This list starts with 0 and is extended by appending the current length of the values list after processing each column.\n\n### Solution Code\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    if rows == 0:\n        return ([], [], [0])\n    cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    column_pointer = [0]\n    \n    for j in range(cols):\n        for i in range(rows):\n            val = dense_matrix[i][j]\n            if val != 0:\n                values.append(val)\n                row_indices.append(i)\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n```\n\n### Explanation\n- **Initialization**: The function starts by checking if the input matrix is empty. If it is, it returns empty lists for values and row indices, along with a column pointer list initialized to [0].\n- **Column Iteration**: For each column, the function iterates through each row to check for non-zero elements.\n- **Non-Zero Collection**: Non-zero elements are collected into the `values` list, and their corresponding row indices are stored in the `row_indices` list.\n- **Column Pointers**: After processing each column, the current length of the `values` list is appended to the `column_pointer` list, indicating the start of the next column.\n\nThis approach ensures that the dense matrix is efficiently converted into a sparse format, which is particularly useful for matrices with a high number of zero elements."}
{"task_id": 68, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the column space (or image) of a given matrix \\( A \\). The column space consists of all linear combinations of the columns of \\( A \\). To determine this, we will use Gaussian elimination to transform the matrix into its row echelon form, identify the pivot columns, and extract the corresponding columns from the original matrix to form the basis of the column space.\n\n### Approach\n1. **Gaussian Elimination**: Convert the matrix \\( A \\) into its row echelon form. This involves iteratively finding the pivot elements, swapping rows to bring the pivot to the current row, and eliminating the entries below the pivot.\n2. **Identify Pivot Columns**: During the Gaussian elimination process, track the columns that contain pivot elements. These columns are the linearly independent columns in the original matrix.\n3. **Extract Basis Vectors**: Extract the columns from the original matrix corresponding to the pivot columns identified in the row echelon form. These columns form the basis of the column space.\n4. **Rounding and Conversion**: Round the resulting basis vectors to 8 decimal places and convert them to a list format for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    m, n = A.shape\n    mat = A.copy()\n    pivot_cols = []\n    current_row = 0\n    for col in range(n):\n        pivot_row = None\n        for r in range(current_row, m):\n            if abs(mat[r, col]) > 1e-10:\n                pivot_row = r\n                break\n        if pivot_row is None:\n            continue\n        mat[[current_row, pivot_row]] = mat[[pivot_row, current_row]]\n        pivot_cols.append(col)\n        for r in range(current_row + 1, m):\n            factor = mat[r, col] / mat[current_row, col]\n            mat[r, :] -= factor * mat[current_row, :]\n        current_row += 1\n        if current_row >= m:\n            break\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n    return basis.tolist()\n```\n\n### Explanation\n1. **Gaussian Elimination**: The matrix is transformed into row echelon form by iteratively selecting pivot elements, swapping rows to position the pivot, and eliminating entries below the pivot using row operations.\n2. **Pivot Columns**: These are the columns in the row echelon form that contain the leading non-zero elements (pivots). These columns correspond to the linearly independent columns in the original matrix.\n3. **Basis Extraction**: The columns from the original matrix corresponding to the pivot columns are extracted to form the basis of the column space.\n4. **Rounding and Conversion**: The resulting basis vectors are rounded to 8 decimal places to handle floating point precision issues and converted to a list format for the final output.\n\nThis approach efficiently identifies the linearly independent columns of the matrix and provides a clear and precise basis for the column space."}
{"task_id": 69, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the R-squared value, also known as the coefficient of determination, for given arrays of true values and predicted values. The R-squared value measures how well the independent variables explain the variability of the dependent variable in a regression model.\n\n### Approach\nThe R-squared value is calculated using the formula:\n\\[ R^2 = 1 - \\frac{RSS}{TSS} \\]\nwhere:\n- \\( RSS \\) (Residual Sum of Squares) is the sum of the squares of the differences between the true values and the predicted values.\n- \\( TSS \\) (Total Sum of Squares) is the sum of the squares of the differences between the true values and the mean of the true values.\n\nThe steps to compute the R-squared value are as follows:\n1. Compute the mean of the true values.\n2. Calculate the Total Sum of Squares (TSS) using the mean.\n3. Calculate the Residual Sum of Squares (RSS) using the true and predicted values.\n4. Handle the special case where TSS is zero to avoid division by zero. If TSS is zero, check if all predicted values are the same as the true values. If they are, return 1.0; otherwise, return 0.0.\n5. Compute the R-squared value using the formula and return it rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    mean_y = np.mean(y_true)\n    tss = np.sum((y_true - mean_y) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    if tss == 0:\n        if np.all(y_pred == y_true):\n            return 1.0\n        else:\n            return 0.0\n    else:\n        r_sq = 1 - (rss / tss)\n        return round(r_sq, 3)\n```\n\n### Explanation\n1. **Mean Calculation**: The mean of the true values is computed to determine the baseline for comparison.\n2. **TSS Calculation**: The total sum of squares measures the total variance in the true values from their mean.\n3. **RSS Calculation**: The residual sum of squares measures the variance in the true values that is not explained by the predicted values.\n4. **Special Case Handling**: If the total sum of squares (TSS) is zero, it means all true values are the same. In this case, if all predicted values match the true values, the R-squared value is 1.0. Otherwise, it is 0.0.\n5. **R-squared Calculation**: The R-squared value is computed as 1 minus the ratio of RSS to TSS, ensuring it is rounded to three decimal places for precision.\n\nThis approach ensures that the function handles edge cases gracefully and provides a robust calculation of the R-squared value."}
{"task_id": 70, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the average brightness of a grayscale image represented as a 2D matrix. The function should handle various edge cases and return the average brightness rounded to two decimal places or -1 for any edge case.\n\n### Approach\n1. **Check for Empty Image**: If the image matrix is empty, return -1.\n2. **Check for Empty Rows**: If any row in the image matrix is empty, return -1.\n3. **Check Row Length Consistency**: Ensure all rows have the same length. If not, return -1.\n4. **Validate Pixel Values**: Ensure all pixel values are within the valid range (0-255). If any pixel is out of range, return -1.\n5. **Calculate Average Brightness**: If all checks pass, compute the average brightness by summing all pixel values and dividing by the total number of pixels. Return the result rounded to two decimal places.\n\n### Solution Code\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img:\n        return -1\n    \n    # Check each row is non-empty\n    for row in img:\n        if not row:\n            return -1\n    \n    # Check all rows have the same length\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    \n    # Check each pixel is within the valid range (0-255)\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    \n    # Calculate the total brightness and count of pixels\n    total = 0\n    count = 0\n    for row in img:\n        total += sum(row)\n        count += len(row)\n    \n    # Compute the average and round to two decimal places\n    average = total / count\n    return round(average, 2)\n```\n\n### Explanation\n1. **Empty Image Check**: The function first checks if the image matrix is empty. If it is, the function returns -1.\n2. **Empty Rows Check**: Each row is checked to ensure it is not empty. If any row is empty, the function returns -1.\n3. **Row Length Consistency Check**: The function verifies that all rows have the same length. If any row has a different length, the function returns -1.\n4. **Pixel Value Validation**: Each pixel value is checked to ensure it falls within the valid range (0-255). If any pixel is out of range, the function returns -1.\n5. **Average Brightness Calculation**: If all checks pass, the function calculates the total brightness by summing all pixel values and counts the total number of pixels. The average brightness is then computed and rounded to two decimal places before being returned.\n\nThis approach ensures that all edge cases are handled efficiently and the average brightness is accurately calculated when the input is valid."}
{"task_id": 71, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function `rmse(y_true, y_pred)` that calculates the Root Mean Square Error (RMSE) between the actual values (`y_true`) and the predicted values (`y_pred`). The function should handle various edge cases such as mismatched array shapes, empty arrays, and invalid input types.\n\n### Approach\n1. **Input Validation**: Check if the input arrays are either lists or numpy arrays. If not, raise a `ValueError`.\n2. **Convert to Numpy Arrays**: Convert the input lists to numpy arrays for consistent handling of array operations.\n3. **Shape Check**: Ensure that the shapes of `y_true` and `y_pred` are the same. If not, raise a `ValueError`.\n4. **Empty Array Check**: Check if either array is empty. If so, raise a `ValueError`.\n5. **Numeric Check**: Ensure that the arrays contain numeric values. If not, raise a `ValueError`.\n6. **Compute RMSE**: Calculate the squared differences between corresponding elements, compute the mean of these squared differences, take the square root of this mean to get the RMSE, and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are valid types\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise ValueError(\"Invalid input type. y_true and y_pred must be lists or numpy arrays.\")\n    \n    # Convert to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check if both arrays are numeric\n    if not (np.issubdtype(y_true.dtype, np.number) and np.issubdtype(y_pred.dtype, np.number)):\n        raise ValueError(\"y_true and y_pred must contain numeric values.\")\n    \n    # Check if shapes are the same\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0:\n        raise ValueError(\"y_true and y_pred cannot be empty.\")\n    \n    # Calculate squared differences\n    squared_diff = (y_true - y_pred) ** 2\n    \n    # Compute mean of squared differences\n    mean_squared = np.mean(squared_diff)\n    \n    # Compute RMSE\n    rmse_value = np.sqrt(mean_squared)\n    \n    # Round to three decimal places\n    return round(rmse_value, 3)\n```\n\n### Explanation\n- **Input Validation**: The function first checks if the inputs are valid types (lists or numpy arrays). This ensures that only appropriate data types are processed.\n- **Conversion to Numpy Arrays**: Converting lists to numpy arrays allows for efficient element-wise operations.\n- **Shape and Empty Check**: Ensuring that the arrays have the same shape and are non-empty is crucial for correct computation and avoiding runtime errors.\n- **Numeric Check**: This step ensures that the arrays contain numeric values, which is necessary for mathematical operations.\n- **RMSE Calculation**: The function computes the squared differences, their mean, and the square root of this mean to get the RMSE, which is then rounded to three decimal places for the final result.\n\nThis approach ensures that the function handles various edge cases gracefully and computes the RMSE accurately for valid inputs."}
{"task_id": 72, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Jaccard Index for two binary arrays, `y_true` and `y_pred`. The Jaccard Index is a measure of similarity between two sets, and it is defined as the size of the intersection divided by the size of the union of the two sets. This measure is particularly useful in binary classification tasks to evaluate the overlap between predicted and true labels.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Ensure that the input arrays are NumPy arrays to leverage efficient element-wise operations.\n2. **Compute Intersection**: Calculate the number of elements where both `y_true` and `y_pred` are 1. This is done using element-wise multiplication and summing the result.\n3. **Compute Union**: Calculate the number of elements where either `y_true` or `y_pred` is 1. This is done using the bitwise OR operation and summing the result.\n4. **Handle Edge Cases**: If the union is zero (both arrays are all zeros), return 1.0 as the Jaccard Index, indicating perfect overlap.\n5. **Calculate Jaccard Index**: Divide the intersection by the union to get the Jaccard Index. If the union is zero, return 1.0 to handle the edge case where both arrays are empty.\n6. **Return Result**: Return the Jaccard Index rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true | y_pred)\n    if union == 0:\n        return 1.0\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)\n```\n\n### Explanation\n- **Conversion to NumPy Arrays**: The function starts by converting the input lists to NumPy arrays to utilize efficient array operations.\n- **Intersection Calculation**: The intersection is computed by element-wise multiplication of `y_true` and `y_pred`, followed by summing the result. This gives the count of elements where both arrays have 1s.\n- **Union Calculation**: The union is computed using the bitwise OR operation on `y_true` and `y_pred`, followed by summing the result. This gives the count of elements where either array has 1s.\n- **Edge Case Handling**: If the union is zero, it means both arrays are all zeros, and the Jaccard Index is defined to be 1.0 in this case.\n- **Jaccard Index Calculation**: The Jaccard Index is the ratio of the intersection to the union. The result is rounded to three decimal places for precision.\n\nThis approach efficiently handles all edge cases and provides an accurate measure of similarity between the two binary arrays."}
{"task_id": 73, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Dice Score, also known as the S\u00f8rensen-Dice coefficient or F1-score, for binary classification. The Dice Score measures the similarity between two sets and is particularly useful in tasks like image segmentation and binary classification.\n\n### Approach\nThe Dice Score is defined as:\n\\[ \\text{Dice Score} = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}} \\]\n\nThe steps to compute the Dice Score are as follows:\n1. Compute the intersection of the true and predicted labels, which is the count of elements where both are 1.\n2. Compute the sum of true labels (sum_true) and the sum of predicted labels (sum_pred).\n3. Calculate the denominator as the sum of sum_true and sum_pred.\n4. If the denominator is zero (both sum_true and sum_pred are zero), return 1.0 as both sets are empty, indicating perfect similarity.\n5. Otherwise, compute the Dice Score using the formula and round the result to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    denominator = sum_true + sum_pred\n    if denominator == 0:\n        return 1.0\n    else:\n        dice = (2 * intersection) / denominator\n    return round(dice, 3)\n```\n\n### Explanation\n- **Intersection Calculation**: The intersection is computed using element-wise multiplication of the true and predicted labels, followed by summing the results. This gives the count of true positives.\n- **Sum of Labels**: The sums of the true and predicted labels give the total number of true positives and predicted positives, respectively.\n- **Denominator Handling**: If both sums are zero, the denominator is zero, and we return 1.0 to indicate perfect similarity. Otherwise, we compute the Dice Score using the formula and round the result to three decimal places for precision.\n\nThis approach ensures that all edge cases are handled appropriately, providing a robust solution for calculating the Dice Score in binary classification tasks."}
{"task_id": 74, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). Each feature in the row is represented by binding hypervectors for the feature name and its value. The hypervectors for the values are created using seeds provided in a dictionary to ensure reproducibility. The composite hypervector is formed by summing the bound hypervectors of all features.\n\n### Approach\n1. **Generate Hypervectors for Feature Names and Values**: For each feature in the dataset row, generate two hypervectors: one for the feature name and one for its value. The feature name's hypervector is generated using a fixed seed (0), while the value's hypervector uses a seed from the provided dictionary.\n2. **Bind Hypervectors**: Bind the hypervectors for each feature name and value using element-wise multiplication.\n3. **Composite Hypervector**: Sum all the bound hypervectors to form the composite hypervector for the row.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    def generate_hv(seed, dim):\n        rng = np.random.RandomState(seed)\n        return rng.choice([-1, 1], size=dim)\n    \n    composite_hv = np.zeros(dim)\n    \n    for key, value in row.items():\n        # Generate hypervector for the feature name using a fixed seed (0)\n        name_hv = generate_hv(0, dim)\n        \n        # Get the seed for the value from random_seeds\n        value_seed = random_seeds[key]\n        value_hv = generate_hv(value_seed, dim)\n        \n        # Bind the name and value hypervectors\n        bound_hv = name_hv * value_hv\n        \n        # Add the bound hypervector to the composite\n        composite_hv += bound_hv\n    \n    return composite_hv.tolist()\n```\n\n### Explanation\n1. **Hypervector Generation**: The `generate_hv` function creates a hypervector of a given dimension using a specified seed. This ensures that the same hypervector is generated each time with the same seed.\n2. **Binding Process**: For each feature, the hypervector of the feature name (generated with a fixed seed) is element-wise multiplied with the hypervector of the feature value (generated with a seed from the provided dictionary). This binding operation combines the two hypervectors.\n3. **Composite Formation**: The bound hypervectors for all features are summed to form the composite hypervector, which is then converted to a list and returned.\n\nThis approach ensures that each feature's contribution is captured in the composite hypervector, leveraging the principles of Hyperdimensional Computing for efficient and reproducible vector representation."}
{"task_id": 75, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to generate a confusion matrix for a binary classification problem. A confusion matrix is a table that summarizes the performance of a classification model by showing the number of correct and incorrect predictions made by the model compared to the actual outcomes.\n\n### Approach\n1. **Extract Unique Labels**: First, we extract all unique labels from the given data. Since it's a binary classification problem, there will be exactly two unique labels.\n2. **Sort Labels**: To ensure a consistent order, we sort these unique labels.\n3. **Count Occurrences**: Using a counter, we count the occurrences of each (y_true, y_pred) pair in the data.\n4. **Build Confusion Matrix**: We then construct the confusion matrix by iterating over each possible combination of true and predicted labels, using the counts obtained in the previous step.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Extract all unique labels from the data\n    unique_labels = sorted({y for pair in data for y in pair})\n    \n    # Ensure the data is binary classification\n    if len(unique_labels) != 2:\n        raise ValueError(\"Data must be binary classification\")\n    \n    # Count the occurrences of each (y_true, y_pred) pair\n    counts = Counter()\n    for y_true, y_pred in data:\n        counts[(y_true, y_pred)] += 1\n    \n    # Build the confusion matrix\n    matrix = []\n    for true_label in unique_labels:\n        row = []\n        for pred_label in unique_labels:\n            row.append(counts.get((true_label, pred_label), 0))\n        matrix.append(row)\n    \n    return matrix\n```\n\n### Explanation\n1. **Extract Unique Labels**: The code uses a set comprehension to gather all unique labels from the data. These labels are then sorted to ensure a consistent order.\n2. **Count Occurrences**: Using the `Counter` from the `collections` module, the code counts how many times each (y_true, y_pred) pair appears in the data.\n3. **Build Confusion Matrix**: The confusion matrix is constructed by iterating over each unique label as the true label and each unique label as the predicted label. For each combination, the count of occurrences is retrieved from the counter and placed in the matrix.\n\nThis approach ensures that the confusion matrix is correctly structured and provides a clear summary of the model's performance in terms of true positives, true negatives, false positives, and false negatives."}
{"task_id": 76, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the cosine similarity between two vectors. Cosine similarity measures the cosine of the angle between two vectors, which indicates their directional similarity. This is a common technique used in various applications such as information retrieval, text mining, and recommendation systems.\n\n### Approach\nThe cosine similarity between two vectors can be computed using the following steps:\n1. **Compute the Dot Product**: The dot product of two vectors is calculated by summing the products of their corresponding elements.\n2. **Compute the Magnitudes**: The magnitude (or norm) of each vector is computed using the Euclidean norm, which is the square root of the sum of the squares of the vector's elements.\n3. **Calculate Cosine Similarity**: The cosine similarity is the dot product of the vectors divided by the product of their magnitudes.\n4. **Round the Result**: The result is rounded to three decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of vectors `v1` and `v2` is computed using `np.dot(v1, v2)`. This gives a scalar value representing the sum of the products of corresponding elements.\n2. **Magnitude Calculation**: The magnitudes of `v1` and `v2` are computed using `np.linalg.norm(v1)` and `np.linalg.norm(v2)`, respectively. This gives the Euclidean norm of each vector.\n3. **Cosine Similarity Formula**: The cosine similarity is obtained by dividing the dot product by the product of the magnitudes of the two vectors. This value ranges between -1 and 1, where 1 indicates identical direction, -1 indicates opposite directions, and 0 indicates orthogonality.\n4. **Rounding**: The result is rounded to three decimal places using the `round` function to ensure the output is precise and formatted correctly.\n\nThis approach efficiently computes the cosine similarity using numpy functions, ensuring both accuracy and performance."}
{"task_id": 77, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates various performance metrics for a binary classification model. The function will take two lists as input: one containing the actual class labels and the other containing the predicted class labels. The function will return a tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n\n### Approach\n1. **Confusion Matrix**: This is a 2x2 matrix that summarizes the performance of a classification model. It is constructed by counting the occurrences of each combination of actual and predicted labels.\n2. **Accuracy**: This is the ratio of correctly predicted labels to the total number of labels. It is calculated as (True Positives + True Negatives) divided by the total number of instances.\n3. **F1 Score**: This is the harmonic mean of precision and recall. Precision is the ratio of correctly predicted positive labels to the total predicted positive labels, and recall is the ratio of correctly predicted positive labels to the total actual positive labels.\n4. **Specificity**: This is the ratio of correctly predicted negative labels to the total number of actual negative labels.\n5. **Negative Predictive Value (NPV)**: This is the ratio of correctly predicted negative labels to the total number of predicted negative labels.\n\n### Solution Code\n```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    counts = Counter(zip(actual, predicted))\n    \n    TN = counts.get((0, 0), 0)\n    FP = counts.get((0, 1), 0)\n    FN = counts.get((1, 0), 0)\n    TP = counts.get((1, 1), 0)\n    \n    confusion_matrix = [\n        [TN, FP],\n        [FN, TP]\n    ]\n    \n    total = TN + FP + FN + TP\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    accuracy = round(accuracy, 3)\n    \n    precision = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n    \n    if (precision + recall) == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    f1_score = round(f1_score, 3)\n    \n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0.0\n    specificity = round(specificity, 3)\n    \n    npv = TN / (TN + FN) if (TN + FN) != 0 else 0.0\n    npv = round(npv, 3)\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, npv)\n```\n\n### Explanation\n1. **Confusion Matrix**: The matrix is constructed using counts of each combination of actual and predicted labels. This helps in visualizing the performance of the model.\n2. **Accuracy**: This metric gives an overall idea of how often the model is correct. It is calculated by dividing the sum of true positives and true negatives by the total number of instances.\n3. **F1 Score**: This metric balances both precision and recall, providing a single score that represents a balance between the two. It is particularly useful when the class distribution is uneven.\n4. **Specificity**: This metric measures the proportion of actual negatives that are correctly identified. It is crucial when the cost of false positives is high.\n5. **Negative Predictive Value (NPV)**: This metric measures the proportion of predicted negatives that are actually negative. It is important when the cost of false negatives is high.\n\nThe function handles edge cases where denominators might be zero by returning 0.0 for those metrics to avoid division by zero errors. Each metric is rounded to three decimal places for consistency and readability."}
{"task_id": 78, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a Python function that calculates various descriptive statistics for a given dataset. The function will take a list or NumPy array of numerical values and return a dictionary containing the calculated statistics.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Convert Data to NumPy Array**: If the input data is a list, convert it to a NumPy array to leverage NumPy's statistical functions.\n2. **Calculate Mean**: Use NumPy's `mean` function to compute the average value of the dataset.\n3. **Calculate Median**: Use NumPy's `median` function to find the middle value of the dataset.\n4. **Calculate Mode**: Use NumPy's `mode` function to determine the most frequently occurring value. If there are multiple modes, the smallest one is selected.\n5. **Calculate Variance**: Use NumPy's `var` function with `ddof=0` to compute the population variance.\n6. **Calculate Standard Deviation**: Use NumPy's `std` function with `ddof=0` to compute the population standard deviation.\n7. **Calculate Percentiles**: Use NumPy's `percentile` function to find the 25th, 50th, and 75th percentiles.\n8. **Calculate Interquartile Range (IQR)**: Compute the difference between the 75th and 25th percentiles.\n9. **Round Values**: Round all calculated values except the mode to four decimal places for consistency and readability.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    data = np.array(data)\n    \n    mean = np.mean(data)\n    median = np.median(data)\n    mode = np.mode(data).mode[0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = percentiles[2] - percentiles[0]\n    \n    # Rounding the values except for mode\n    mean = round(mean, 4)\n    median = round(median, 4)\n    variance = round(variance, 4)\n    std_dev = round(std_dev, 4)\n    p25 = round(percentiles[0], 4)\n    p50 = round(percentiles[1], 4)\n    p75 = round(percentiles[2], 4)\n    iqr = round(iqr, 4)\n    \n    result = {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': std_dev,\n        '25th_percentile': p25,\n        '50th_percentile': p50,\n        '75th_percentile': p75,\n        'interquartile_range': iqr\n    }\n    \n    return result\n```\n\n### Explanation\n- **Mean**: The average value of the dataset, calculated using `np.mean`.\n- **Median**: The middle value when the dataset is ordered, calculated using `np.median`.\n- **Mode**: The most frequently occurring value, calculated using `np.mode`. If there are multiple modes, the smallest one is selected.\n- **Variance**: A measure of how spread out the numbers are, calculated using `np.var` with `ddof=0` for population variance.\n- **Standard Deviation**: The square root of the variance, calculated using `np.std` with `ddof=0`.\n- **Percentiles**: The values below which a given percentage of observations in a group fall. The 25th, 50th, and 75th percentiles are calculated using `np.percentile`.\n- **Interquartile Range (IQR)**: The difference between the 75th and 25th percentiles, providing a measure of statistical dispersion.\n\nThis function efficiently computes all required statistics and returns them in a dictionary, ensuring that all values except the mode are rounded to four decimal places for clarity and precision."}
{"task_id": 79, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with a probability p of success. This can be efficiently done using the binomial distribution formula.\n\n### Approach\nThe binomial distribution formula is given by:\n\\[ P(k) = C(n, k) \\times p^k \\times (1-p)^{n-k} \\]\nwhere:\n- \\( C(n, k) \\) is the combination of n items taken k at a time, calculated as \\( \\frac{n!}{k!(n-k)!} \\)\n- \\( p^k \\) is the probability of k successes\n- \\( (1-p)^{n-k} \\) is the probability of (n-k) failures\n\nThe steps to solve the problem are:\n1. Compute the combination \\( C(n, k) \\) using the math.comb function.\n2. Calculate \\( p^k \\) and \\( (1-p)^{n-k} \\).\n3. Multiply these values together to get the probability.\n4. Round the result to five decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    return round(probability, 5)\n```\n\n### Explanation\n1. **Combination Calculation**: The function uses `math.comb(n, k)` to compute the binomial coefficient, which represents the number of ways to choose k successes out of n trials.\n2. **Probability Calculation**: The terms \\( p^k \\) and \\( (1-p)^{n-k} \\) are computed to account for the probabilities of exactly k successes and (n-k) failures, respectively.\n3. **Multiplication and Rounding**: The product of the combination, \\( p^k \\), and \\( (1-p)^{n-k} \\) gives the exact probability, which is then rounded to five decimal places for precision.\n\nThis approach ensures that we efficiently and accurately compute the desired probability using the binomial distribution formula."}
{"task_id": 80, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a Python function that calculates the probability density function (PDF) of the normal distribution for given values of x, mean, and standard deviation. The normal distribution's PDF is a well-known formula in statistics, and our task is to implement this formula accurately and efficiently.\n\n### Approach\nThe normal distribution's PDF is given by the formula:\n\\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\nwhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nThe approach involves the following steps:\n1. **Calculate the exponent**: This part of the formula involves squaring the difference between x and the mean, dividing by twice the square of the standard deviation, and then taking the negative of this value.\n2. **Compute the denominator**: This is the product of the standard deviation and the square root of \\( 2\\pi \\).\n3. **Compute the PDF value**: Multiply the result of the exponential function (applied to the exponent) by the reciprocal of the denominator.\n4. **Round the result**: The final PDF value is rounded to 5 decimal places for precision.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -((x - mean) ** 2) / (2 * (std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf = (1 / denominator) * math.exp(exponent)\n    return round(pdf, 5)\n```\n\n### Explanation\n1. **Exponent Calculation**: The exponent is calculated as \\(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\). This step ensures that the exponential term decays as we move away from the mean, which is a characteristic of the normal distribution.\n2. **Denominator Calculation**: The denominator is computed as \\( \\sigma \\sqrt{2\\pi} \\). This normalization factor ensures that the total area under the PDF curve is 1, a fundamental property of probability density functions.\n3. **PDF Calculation**: The PDF value is obtained by multiplying the exponential term by the reciprocal of the denominator. This gives the probability density at the point x.\n4. **Rounding**: The result is rounded to 5 decimal places to provide a precise and readable output.\n\nThis approach efficiently computes the PDF using the mathematical properties of the normal distribution, ensuring accuracy and correctness for a wide range of input values."}
{"task_id": 81, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the probability of observing exactly `k` events in a fixed interval using the Poisson distribution formula. The Poisson distribution is useful for modeling the number of times an event occurs in an interval of time or space.\n\n### Approach\nThe Poisson distribution formula is given by:\n\\[ P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\]\nwhere:\n- \\( \\lambda \\) is the average rate (mean) of occurrences in a fixed interval.\n- \\( k \\) is the number of events we are interested in.\n- \\( e \\) is the base of the natural logarithm, approximately equal to 2.71828.\n\nThe steps to compute the probability are as follows:\n1. Compute \\( \\lambda^k \\) using exponentiation.\n2. Compute \\( e^{-\\lambda} \\) using the exponential function.\n3. Compute the factorial of \\( k \\) using the factorial function.\n4. Combine these values into the Poisson probability formula.\n5. Round the result to 5 decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\n```\n\n### Explanation\n1. **Exponentiation**: The term \\( \\lambda^k \\) is computed using `lam ** k`.\n2. **Exponential Function**: The term \\( e^{-\\lambda} \\) is computed using `math.exp(-lam)`.\n3. **Factorial**: The factorial of \\( k \\) is computed using `math.factorial(k)`.\n4. **Combining Terms**: The product of \\( \\lambda^k \\) and \\( e^{-\\lambda} \\) is divided by \\( k! \\) to get the probability.\n5. **Rounding**: The result is rounded to 5 decimal places using `round(probability, 5)` to meet the problem's requirement.\n\nThis approach efficiently computes the Poisson probability using basic mathematical operations and functions, ensuring accuracy and correctness for a wide range of inputs."}
{"task_id": 82, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the contrast of a grayscale image. The contrast is defined as the difference between the maximum and minimum pixel values in the image. This can be efficiently computed using numpy functions to find the maximum and minimum values in the image array.\n\n### Approach\n1. **Problem Analysis**: The task is to determine the contrast of a grayscale image, which is given as a 2D numpy array. The contrast is simply the difference between the highest and lowest pixel values in the image.\n2. **Intuition**: By finding the maximum and minimum pixel values, we can directly compute the contrast by subtracting the minimum value from the maximum value. This approach is efficient and straightforward.\n3. **Algorithm Selection**: Using numpy's built-in functions `np.max()` and `np.min()` allows us to efficiently compute the maximum and minimum values of the image array. These functions are optimized for performance, making them suitable for this task.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val\n```\n\n### Explanation\n1. **Finding Maximum and Minimum Values**: The function uses `np.max(img)` to find the highest pixel value and `np.min(img)` to find the lowest pixel value in the image array.\n2. **Calculating Contrast**: The contrast is computed by subtracting the minimum pixel value from the maximum pixel value. This gives the range of pixel intensities present in the image.\n3. **Return the Result**: The computed contrast value is returned as the output of the function.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging numpy's optimized functions for performance."}
{"task_id": 83, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a Python function that calculates the dot product of two 1D NumPy arrays. The dot product is a fundamental operation in linear algebra, which involves multiplying corresponding elements of two vectors and then summing those products.\n\n### Approach\nThe approach to solve this problem involves using the built-in NumPy function `np.dot()`, which efficiently computes the dot product of two arrays. This function is both concise and optimized for performance, making it the ideal choice for this task. The steps are as follows:\n1. Import the NumPy library.\n2. Define a function that takes two 1D NumPy arrays as input.\n3. Use `np.dot()` to compute the dot product of the two input arrays.\n4. Return the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\nThe function `calculate_dot_product` takes two 1D NumPy arrays, `vec1` and `vec2`, as inputs. It uses the `np.dot()` function to compute their dot product. The `np.dot()` function multiplies corresponding elements of the two arrays and sums the results, which is exactly the definition of the dot product. This approach ensures that the solution is both efficient and easy to understand."}
{"task_id": 84, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to perform a Phi Transformation that maps input features into a higher-dimensional space by generating polynomial features. This transformation allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features.\n\n### Approach\nThe approach involves the following steps:\n1. **Check for Negative Degree**: If the given degree is less than 0, return an empty list immediately.\n2. **Generate Polynomial Features**: For each data point, generate a list of polynomial features from degree 0 up to the given degree. Each feature is computed as the data point raised to the respective power.\n3. **Round Values**: Each computed polynomial feature is rounded to 8 decimal places to ensure precision.\n4. **Return Result**: Construct and return a list of lists where each inner list contains the polynomial features of the corresponding data point.\n\n### Solution Code\n```python\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    return [[round(x ** d, 8) for d in range(degree + 1)] for x in data]\n```\n\n### Explanation\n- **Negative Degree Check**: The function first checks if the degree is negative. If it is, an empty list is returned as no transformation is possible.\n- **Polynomial Feature Generation**: For each data point, the function generates a list of features where each feature is the data point raised to the power from 0 up to the given degree. This is done using a list comprehension for efficiency and conciseness.\n- **Rounding**: Each computed feature is rounded to 8 decimal places to maintain precision and avoid floating-point inaccuracies.\n- **Result Construction**: The function constructs the result as a list of lists, where each inner list corresponds to the transformed features of a data point.\n\nThis approach ensures that the transformation is both efficient and easy to understand, leveraging Python's list comprehensions for concise and readable code."}
{"task_id": 85, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that calculates the positional encoding for a Transformer model using sine and cosine functions. The function should handle edge cases where the position is 0 or the model dimensionality is non-positive by returning -1. The positional encoding should be computed using higher precision and then converted to the required precision before returning as a list.\n\n### Approach\n1. **Edge Case Handling**: Check if the position is 0 or if the model dimensionality (d_model) is less than or equal to 0. If either condition is met, return -1.\n2. **Positional Encoding Calculation**: For each dimension in the model, compute the positional encoding using sine and cosine functions. The even indices use sine functions, while the odd indices use cosine functions.\n3. **Precision Management**: Compute the values using higher precision (float64) to maintain accuracy during calculations, then convert the result to the required precision (float16) before returning it as a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros(d_model, dtype=np.float64)\n    for j in range(d_model):\n        if j % 2 == 0:\n            i = j // 2\n            exponent = 2 * i / d_model\n            denominator = 10000 ** exponent\n            pe[j] = np.sin(position / denominator)\n        else:\n            i = (j - 1) // 2\n            exponent = 2 * i / d_model\n            denominator = 10000 ** exponent\n            pe[j] = np.cos(position / denominator)\n    pe = pe.astype(np.float16)\n    return pe.tolist()\n```\n\n### Explanation\n1. **Edge Case Handling**: The function first checks if the position is 0 or if d_model is non-positive. If either condition is true, it returns -1 immediately.\n2. **Array Initialization**: An array of size d_model is initialized with zeros using float64 precision to ensure accurate computations.\n3. **Loop Through Dimensions**: For each dimension index `j`:\n   - If `j` is even, compute the sine of the position divided by a denominator derived from the position and the dimension index.\n   - If `j` is odd, compute the cosine of the position divided by a similar denominator.\n4. **Precision Conversion**: After computing all values, the array is converted from float64 to float16 to meet the precision requirement.\n5. **Return Result**: The resulting array is converted to a list and returned.\n\nThis approach ensures that the positional encoding is computed accurately and efficiently, adhering to the constraints and requirements specified."}
{"task_id": 86, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function should return 1 for overfitting, -1 for underfitting, and 0 for a good fit.\n\n### Approach\nThe approach to solve this problem involves checking two main conditions in sequence:\n\n1. **Overfitting Check**: Determine if the model is overfitting by checking if the training accuracy is significantly higher than the test accuracy. Specifically, if the difference between training accuracy and test accuracy exceeds 0.2, the model is considered overfitting.\n\n2. **Underfitting Check**: If the model is not overfitting, check if both the training and test accuracies are below 0.7. If both are below this threshold, the model is considered underfitting.\n\nIf neither of the above conditions is met, the model is considered to be performing well, and the function returns 0.\n\n### Solution Code\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n- **Overfitting Check**: The function first checks if the training accuracy is more than 0.2 higher than the test accuracy. If true, the model is overfitting, and the function returns 1.\n- **Underfitting Check**: If the model is not overfitting, the function then checks if both the training and test accuracies are below 0.7. If true, the model is underfitting, and the function returns -1.\n- **Good Fit**: If neither overfitting nor underfitting conditions are met, the model is considered to be performing well, and the function returns 0.\n\nThis approach ensures that the model's performance is evaluated accurately based on the given criteria, providing a clear indication of whether the model is overfitting, underfitting, or performing well."}
{"task_id": 87, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Adam optimizer update step function. The Adam optimizer is a popular optimization algorithm used in machine learning, which adjusts the learning rate based on the moving averages of the gradient and squared gradient. The function should handle both scalar and array inputs and include bias correction for the moving averages.\n\n### Approach\nThe Adam optimizer maintains two moving averages: the first moment (m) and the second moment (v) of the gradients. These moving averages are updated at each step and then bias-corrected to provide accurate estimates of the gradients and their variances. The steps to implement the Adam optimizer are as follows:\n\n1. **Update Moving Averages**: Compute the new first and second moment estimates using the given gradients.\n2. **Bias Correction**: Adjust the moving averages to account for their initialization bias.\n3. **Parameter Update**: Use the bias-corrected estimates to compute the parameter update and adjust the learning rate accordingly.\n4. **Data Handling**: Ensure the function handles both scalar and array inputs, and returns the results rounded to 5 decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update the first and second moment estimates\n    new_m = beta1 * m + (1 - beta1) * grad\n    new_v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Apply bias correction\n    m_hat = new_m / (1 - beta1 ** t)\n    v_hat = new_v / (1 - beta2 ** t)\n    \n"}
{"task_id": 88, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified version of the GPT-2 text generation model. This model will generate text based on a given prompt by using token embeddings, positional embeddings, multi-head attention (simplified), a feed-forward network, and layer normalization. The goal is to understand the core concepts behind GPT-2's autoregressive text generation.\n\n### Approach\n1. **Token Embeddings**: Convert input tokens into dense vector representations using a predefined encoder.\n2. **Positional Embeddings**: Add positional information to the token embeddings to capture the position of each token in the sequence.\n3. **Layer Normalization**: Stabilize the training process by normalizing the embeddings.\n4. **Logit Calculation**: Project the normalized embeddings to the vocabulary size to get logits for the next token.\n5. **Token Sampling**: Use the logits to sample the next token, which is then added to the sequence for subsequent generations.\n\nThe model uses a helper function to load the encoder, hyperparameters, and parameters. The encoder converts text into token IDs and vice versa. The hyperparameters include the context window size and the number of attention heads. The parameters include token embeddings, positional embeddings, and layer normalization parameters.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt into token IDs\n    token_ids = encoder.encode(prompt)\n    \n    # Truncate if the prompt is longer than the context window\n    if len(token_ids) > hparams['n_ctx']:\n        token_ids = token_ids[-hparams['n_ctx']:]\n    \n    # Define the layer normalization function\n    def layer_norm(x, gamma, beta, epsilon=1e-5):\n        mean = x.mean(axis=-1, keepdims=True)\n        std = x.std(axis=-1, keepdims=True)\n        x_norm = (x - mean) / (std + epsilon)\n        return x_norm * gamma + beta\n    \n    for _ in range(n_tokens_to_generate):\n        # Get the current sequence, limited to the context window\n        current_sequence = token_ids[-hparams['n_ctx']:] if len(token_ids) > hparams['n_ctx'] else token_ids\n        seq_len = len(current_sequence)\n        positions = np.arange(seq_len)\n        \n        # Retrieve token embeddings\n        x = params['wte'][current_sequence]\n        \n        # Retrieve positional embeddings\n        pos_embeddings = params['wpe'][positions]\n        \n        # Combine embeddings\n        embeddings = x + pos_embeddings\n        \n        # Apply layer normalization\n        ln_output = layer_norm(embeddings, params['ln_f']['g'], params['ln_f']['b'])\n        \n        # Compute logits by projecting to vocabulary size\n        logits = ln_output @ params['wte'].T\n        \n        # Get logits for the last token in the current sequence\n        last_logits = logits[-1]\n        \n        # Sample the next token (using argmax for simplicity)\n        next_token_id = np.argmax(last_logits)\n        \n        # Append the generated token to the sequence\n        token_ids.append(next_token_id)\n    \n    # Decode the token IDs back to text\n    generated_text = encoder.decode(token_ids)\n    \n    return generated_text\n```\n\n### Explanation\n1. **Tokenization**: The input prompt is converted into token IDs using the encoder.\n2. **Embeddings**: Token and positional embeddings are retrieved and combined.\n3. **Layer Normalization**: The combined embeddings are normalized to stabilize training.\n4. **Logit Calculation**: The normalized embeddings are projected to the vocabulary size to get logits for the next token.\n5. **Token Sampling**: The next token is sampled using argmax (for simplicity) and added to the sequence.\n6. **Text Generation**: The process is repeated for the specified number of tokens, and the final sequence is decoded back into text.\n\nThis approach provides a simplified version of the GPT-2 model, demonstrating the core components and processes involved in text generation."}
{"task_id": 89, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simplified self-attention mechanism for a sequence of crystals. Each crystal has a unique numeric value, and the goal is to compute the enhanced patterns by analyzing the relationships between these crystals using self-attention.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Compute Attention Scores**: For each crystal, compute the attention scores with every other crystal. The attention score between two crystals is calculated as the product of their values divided by the square root of the given dimension.\n\n2. **Apply Softmax Function**: Convert these attention scores into probabilities using the softmax function. This ensures that the probabilities sum up to 1 for each crystal.\n\n3. **Compute Weighted Sum**: For each crystal, compute the final weighted pattern by taking the weighted sum of all crystal values, where the weights are the attention probabilities derived from the softmax function.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    \n    V = np.array(crystal_values)\n    d = dimension\n    scores = (V[:, None] * V[None, :]) / np.sqrt(d)\n    \n    # Compute attention weights\n    weights = np.zeros_like(scores)\n    for i in range(n):\n        weights[i] = softmax(scores[i])\n    \n    # Compute the output\n    output = np.dot(weights, V)\n    \n    # Round each element to 4 decimal places\n    return [round(x, 4) for x in output]\n```\n\n### Explanation\n1. **Softmax Function**: The `softmax` function is implemented to convert a list of scores into probabilities. This is done by exponentiating each score, subtracting the maximum value to prevent overflow, and then normalizing by the sum of these exponentiated values.\n\n2. **Scores Matrix**: The scores matrix is computed using the outer product of the crystal values matrix with itself, scaled by the square root of the dimension. This gives a matrix where each element represents the attention score between two crystals.\n\n3. **Weights Matrix**: Each row of the scores matrix is passed through the softmax function to obtain the attention weights. This results in a matrix where each row represents the attention probabilities for a given crystal.\n\n4. **Output Calculation**: The final output for each crystal is computed as the dot product of the corresponding row in the weights matrix with the crystal values vector. This gives the weighted sum of all crystal values, which is the enhanced pattern for that crystal.\n\nThe result is a list of floats, each rounded to four decimal places, representing the enhanced patterns for each crystal."}
{"task_id": 90, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the BM25 ranking function to calculate document scores for a given query in an information retrieval context. BM25 is an advanced variation of TF-IDF that incorporates term frequency saturation, document length normalization, and a configurable penalty for document length effects.\n\n### Approach\nThe BM25 ranking function is used to estimate the relevance of documents to a given search query. The approach involves several steps:\n\n1. **Preprocess the Corpus**: Convert each document into a list of lowercase tokens and count the frequency of each term using `Counter`.\n2. **Compute Document Frequencies**: Count how many documents contain each term.\n3. **Compute Average Document Length**: Calculate the average length of all documents in the corpus.\n4. **Preprocess the Query**: Convert the query into a list of lowercase tokens.\n5. **Calculate BM25 Scores**: For each document, compute the BM25 score by summing the contributions of each term in the query. The score for each term is calculated using the BM25 formula, which considers term frequency, document frequency, and document length normalization.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Preprocess each document in the corpus\n    processed_corpus = []\n    doc_freq = Counter()\n    doc_lengths = []\n    for doc in corpus:\n        words = doc.lower().split()\n        doc_lengths.append(len(words))\n        word_counts = Counter(words)\n        processed_corpus.append(word_counts)\n        for word in word_counts:\n            doc_freq[word] += 1\n    \n    # Calculate average document length\n    avgdl = np.mean(doc_lengths) if doc_lengths else 0\n    \n    # Preprocess the query\n    query_words = query.lower().split()\n    \n    # Calculate BM25 scores for each document\n    scores = []\n    for i, doc_counts in enumerate(processed_corpus):\n        score = 0.0\n        doc_length = doc_lengths[i]\n        denominator_part = k1 * (1 - b + b * (doc_length / avgdl)) if avgdl != 0 else 0\n        for word in query_words:\n            if word not in doc_counts:\n                continue\n            tf = doc_counts[word]\n            df = doc_freq[word]\n            n = len(corpus)\n            # Compute the two parts of the BM25 formula\n            numerator = (k1 + 1) * tf\n            denominator = denominator_part + tf\n            term1 = numerator / denominator if denominator != 0 else 0\n            term2 = np.log((n - df + 0.5) / (df + 0.5)) if (df + 0.5) != 0 else 0\n            score += term1 * term2\n        scores.append(round(score, 3))\n    \n    return scores\n```\n\n### Explanation\n1. **Preprocessing the Corpus**: Each document is converted to lowercase and split into tokens. The frequency of each term in each document is counted using `Counter`.\n2. **Document Frequencies**: For each term, we count how many documents contain that term.\n3. **Average Document Length**: The average length of all documents is computed to normalize the document length effect.\n4. **Query Preprocessing**: The query is converted to lowercase and split into tokens.\n5. **BM25 Score Calculation**: For each document, the BM25 score is calculated by summing the contributions of each term in the query. The contribution of each term is computed using the BM25 formula, which considers term frequency, document frequency, and document length normalization. The scores are then rounded to three decimal places.\n\nThis approach ensures that the BM25 ranking function effectively ranks documents based on their relevance to the query, considering both term frequency and document length effects."}
{"task_id": 91, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the F1 score given the true labels and predicted labels. The F1 score is a measure of a model's accuracy, combining both precision and recall. It is particularly useful for binary classification problems.\n\n### Approach\n1. **Understanding Precision and Recall**: \n   - **Precision** is the ratio of correctly predicted positive observations to the total predicted positives. It is calculated as TP / (TP + FP), where TP is True Positives and FP is False Positives.\n   - **Recall** is the ratio of correctly predicted positive observations to the all actual positives. It is calculated as TP / (TP + FN), where FN is False Negatives.\n\n2. **F1 Score Calculation**:\n   - The F1 score is the harmonic mean of precision and recall, given by the formula: F1 = 2 * (precision * recall) / (precision + recall).\n\n3. **Handling Edge Cases**:\n   - If there are no positive predictions (TP + FP = 0), precision is set to 0.\n   - If there are no actual positives (TP + FN = 0), recall is set to 0.\n   - If both precision and recall are zero, the F1 score is set to 0 to avoid division by zero.\n\n### Solution Code\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    TP = 0\n    FP = 0\n    FN = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            TP += 1\n        elif yt == 0 and yp == 1:\n            FP += 1\n        elif yt == 1 and yp == 0:\n            FN += 1\n    \n    # Calculate precision\n    if (TP + FP) == 0:\n        precision = 0.0\n    else:\n        precision = TP / (TP + FP)\n    \n    # Calculate recall\n    if (TP + FN) == 0:\n        recall = 0.0\n    else:\n        recall = TP / (TP + FN)\n    \n    # Calculate F1 score\n    if (precision + recall) == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    \n    return round(f1, 3)\n```\n\n### Explanation\n1. **Initialization**: We start by initializing counters for True Positives (TP), False Positives (FP), and False Negatives (FN) to zero.\n\n2. **Counting TP, FP, FN**: We iterate through each pair of true and predicted labels. Depending on whether each label is a true positive, false positive, or false negative, we increment the respective counter.\n\n3. **Precision Calculation**: We compute precision as the ratio of TP to the sum of TP and FP. If there are no positive predictions, precision is set to 0.\n\n4. **Recall Calculation**: We compute recall as the ratio of TP to the sum of TP and FN. If there are no actual positives, recall is set to 0.\n\n5. **F1 Score Calculation**: Using the harmonic mean formula, we compute the F1 score. If both precision and recall are zero, the F1 score is set to 0 to avoid division by zero.\n\n6. **Return Result**: The F1 score is rounded to three decimal places and returned.\n\nThis approach ensures that we handle all edge cases and compute the F1 score accurately for binary classification problems."}
{"task_id": 92, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to predict the power consumption for day 15 on Mars, considering both a linear trend and daily fluctuations. The goal is to remove the known daily fluctuation, fit a linear regression model to the detrended data, predict the base consumption for day 15, add back the fluctuation, and include a 5% safety margin.\n\n### Approach\n1. **Remove Daily Fluctuation**: Subtract the known daily fluctuation from each data point. The fluctuation is given by the formula \\( f(i) = 10 \\times \\sin(2\\pi i / 10) \\), where \\( i \\) is the day number (1 through 10).\n2. **Linear Regression**: Fit a linear regression model to the detrended data. This model will help predict the base consumption for day 15.\n3. **Predict Base Consumption**: Use the linear regression model to predict the base consumption for day 15.\n4. **Add Fluctuation**: Add the fluctuation for day 15 back to the predicted base consumption. For day 15, the fluctuation is zero.\n5. **Safety Margin**: Round the predicted value, add a 5% safety margin, and round up the result to ensure the colony's future needs are met.\n\n### Solution Code\n```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    detrended_data = []\n    for j in range(10):\n        i = j + 1\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended = consumption_data[j] - fluctuation\n        detrended_data.append(detrended)\n    \n    # Step 2: Perform linear regression on the detrended data\n    x = list(range(1, 11))  # Days 1 through 10\n    y = detrended_data\n    \n    x_mean = sum(x) / len(x)\n    y_mean = sum(y) / len(y)\n    \n    numerator = 0.0\n    denominator = 0.0\n    for xi, yi in zip(x, y):\n        numerator += (xi - x_mean) * (yi - y_mean)\n        denominator += (xi - x_mean) ** 2\n    \n    b = numerator / denominator\n    a = y_mean - b * x_mean\n    \n    # Step 3: Predict day 15's base consumption\n    predicted_base = a + b * 15\n    \n    # Step 4: Add the day 15 fluctuation (which is zero)\n    i_15 = 15\n    fluctuation_15 = 10 * math.sin(2 * PI * i_15 / 10)\n    total = predicted_base + fluctuation_15\n    \n    # Step 5: Round, add 5% safety margin, and round up\n    rounded_total = round(total)\n    safety = rounded_total * 1.05\n    final = math.ceil(safety)\n    \n    return final\n```\n\n### Explanation\n1. **Removing Fluctuation**: For each day from 1 to 10, the fluctuation is calculated using the given formula and subtracted from the consumption data to get the detrended data.\n2. **Linear Regression**: The detrended data is used to fit a linear regression model. This involves calculating the slope (b) and intercept (a) of the best fit line.\n3. **Prediction**: The linear model is used to predict the base consumption for day 15.\n4. **Fluctuation Addition**: The fluctuation for day 15 is calculated and added to the predicted base consumption. For day 15, this fluctuation is zero.\n5. **Safety Margin**: The predicted value is rounded, a 5% safety margin is added, and the result is rounded up to ensure the colony's energy needs are met.\n\nThis approach ensures that we accurately predict the power consumption for day 15, considering both the linear trend and daily fluctuations, while also accounting for a safety margin to handle unexpected increases in consumption."}
{"task_id": 93, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. MAE is a metric used to measure the average magnitude of errors in a set of predictions without considering their direction.\n\n### Approach\nThe approach to calculate MAE involves the following steps:\n1. **Compute Absolute Differences**: For each pair of corresponding values in the true and predicted arrays, compute the absolute difference. This is done using the `np.abs()` function.\n2. **Calculate Mean of Differences**: Compute the mean of these absolute differences using the `np.mean()` function.\n3. **Round the Result**: The result is then rounded to three decimal places for precision.\n\nThis approach ensures that we accurately measure the average magnitude of errors, providing a clear and concise metric for evaluating prediction accuracy.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    return round(np.mean(np.abs(y_true - y_pred)), 3)\n```\n\n### Explanation\n- **Absolute Differences**: The function `np.abs(y_true - y_pred)` computes the element-wise absolute differences between the true values (`y_true`) and the predicted values (`y_pred`).\n- **Mean Calculation**: The function `np.mean()` is used to compute the average of these absolute differences, giving the MAE.\n- **Rounding**: The result is rounded to three decimal places using the `round()` function to ensure the output is precise and formatted correctly.\n\nThis method efficiently computes the MAE using numpy operations, ensuring both accuracy and performance."}
{"task_id": 94, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the multi-head attention mechanism, a key component in the Transformer model. The goal is to compute the attention scores for multiple heads, apply softmax to these scores, and then use these scores to weight the value vectors. The results from all heads are then concatenated and returned as a list.\n\n### Approach\n1. **Compute Query, Key, and Value Vectors**: The input matrix `X` is transformed into query (Q), key (K), and value (V) vectors using the provided weight matrices `W_q`, `W_k`, and `W_v` respectively.\n2. **Split into Heads**: The Q, K, and V vectors are split into multiple heads along the last dimension. Each head processes its own set of Q, K, and V vectors.\n3. **Compute Attention Scores**: For each head, compute the attention scores by taking the dot product of the query and key vectors, then scaling by the square root of the dimension of the key vectors. Apply softmax to these scores to get attention weights.\n4. **Apply Attention Weights**: Multiply the attention weights with the value vectors to get the output for each head.\n5. **Concatenate Outputs**: Concatenate the outputs from all heads along the last dimension to form the final output.\n6. **Return Result**: Round the result to four decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    # Compute Q, K, V\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n\n    # Split into heads\n    d_k = Q.shape[1]\n    head_dim = d_k // n_heads\n\n    Q_heads = np.split(Q, n_heads, axis=1)\n    K_heads = np.split(K, n_heads, axis=1)\n    V_heads = np.split(V, n_heads, axis=1)\n\n    head_outputs = []\n    for q, k, v in zip(Q_heads, K_heads, V_heads):\n        # Compute dot product\n        dot_product = np.sum(q * k, axis=1, keepdims=True) / np.sqrt(head_dim)\n        # Apply softmax\n        scores = np.exp(dot_product - np.max(dot_product, axis=-1, keepdims=True))\n        scores = scores / np.sum(scores, axis=-1, keepdims=True)\n        # Multiply by V\n        output = scores * v\n        head_outputs.append(output)\n\n    # Concatenate all head outputs\n    output = np.concatenate(head_outputs, axis=1)\n\n    # Round to 4 decimals and convert to list\n    output = np.round(output, 4).tolist()\n    return output\n```\n\n### Explanation\n1. **Compute Q, K, V**: The input matrix `X` is multiplied by the weight matrices `W_q`, `W_k`, and `W_v` to get the query, key, and value vectors respectively.\n2. **Split into Heads**: Each of these vectors is split into `n_heads` parts along the last dimension. This allows each head to process a portion of the vectors independently.\n3. **Attention Scores**: For each head, the dot product of the query and key vectors is computed and scaled. This gives the attention scores, which are then normalized using softmax to ensure they sum to 1.\n4. **Apply Weights**: The attention weights are applied to the value vectors to produce the output for each head.\n5. **Concatenate Outputs**: The outputs from all heads are concatenated along the last dimension to form the final output matrix.\n6. **Return Result**: The final output is rounded to four decimal places and converted to a list for the result.\n\nThis approach efficiently computes the multi-head attention mechanism, allowing each head to focus on different aspects of the input data, thereby enhancing the model's ability to capture complex relationships."}
{"task_id": 95, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Phi coefficient, a measure of the correlation between two binary variables. The Phi coefficient is a statistical measure that assesses the strength of association between two binary variables, similar to the Pearson correlation coefficient but specifically designed for binary data.\n\n### Approach\nThe approach to calculate the Phi coefficient involves the following steps:\n\n1. **Contingency Table Construction**: Construct a 2x2 contingency table from the input lists. The table consists of four cells:\n   - `a`: Count of occurrences where both variables are 1.\n   - `b`: Count of occurrences where the first variable is 1 and the second is 0.\n   - `c`: Count of occurrences where the first variable is 0 and the second is 1.\n   - `d`: Count of occurrences where both variables are 0.\n\n2. **Phi Coefficient Formula**: Use the counts from the contingency table to compute the Phi coefficient using the formula:\n   \\[\n   \\phi = \\frac{ad - bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\n   \\]\n   Here, the numerator is the difference between the products of the diagonal elements of the contingency table, and the denominator is the square root of the product of the sums of the rows and columns.\n\n3. **Handling Edge Cases**: If the denominator is zero, which can happen if one of the variables does not vary (all values are the same), the function returns 0.0 to avoid division by zero.\n\n4. **Rounding**: The result is rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport math\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"x and y must be of the same length\")\n    \n    a = b = c = d = 0\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            a += 1\n        elif xi == 1 and yi == 0:\n            b += 1\n        elif xi == 0 and yi == 1:\n            c += 1\n        else:\n            d += 1\n    \n    numerator = a * d - b * c\n    denominator = math.sqrt((a + b) * (c + d) * (a + c) * (b + d))\n    \n    if denominator == 0:\n        return 0.0\n    \n    phi = numerator / denominator\n    return round(phi, 4)\n```\n\n### Explanation\n- **Contingency Table Construction**: The code iterates through each pair of elements in the input lists `x` and `y`, incrementing the respective counts `a`, `b`, `c`, and `d` based on the values of the elements.\n- **Phi Coefficient Calculation**: Using the counts from the contingency table, the numerator and denominator of the Phi coefficient formula are computed. If the denominator is zero, the function returns 0.0 to handle cases where the variables do not vary.\n- **Rounding**: The computed Phi coefficient is rounded to four decimal places to ensure precision in the result.\n\nThis approach efficiently computes the Phi coefficient, ensuring accuracy and handling edge cases appropriately."}
{"task_id": 96, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Hard Sigmoid activation function, which is a computationally efficient approximation of the standard sigmoid function. The Hard Sigmoid function is defined as a piecewise function that returns 0 for inputs less than -2.5, 1 for inputs greater than 2.5, and a linear transformation of the input for values in between.\n\n### Approach\nThe Hard Sigmoid function can be broken down into three cases:\n1. If the input value `x` is less than -2.5, the function returns 0.\n2. If the input value `x` is greater than 2.5, the function returns 1.\n3. For values of `x` between -2.5 and 2.5, the function returns the result of the linear transformation `(x + 2.5) / 5`.\n\nThis approach ensures that the function is computationally efficient and provides a smooth transition between 0 and 1 for inputs within the specified range.\n\n### Solution Code\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return (x + 2.5) / 5.0\n```\n\n### Explanation\nThe function `hard_sigmoid` takes a single input value `x` and returns the corresponding output based on the Hard Sigmoid definition. The function uses conditional checks to determine which of the three cases applies:\n- For `x < -2.5`, it returns 0.0.\n- For `x > 2.5`, it returns 1.0.\n- For values of `x` between -2.5 and 2.5, it computes the linear transformation `(x + 2.5) / 5.0` and returns the result.\n\nThis implementation efficiently handles the piecewise nature of the Hard Sigmoid function, ensuring optimal performance and correctness for all possible input values."}
{"task_id": 97, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the ELU (Exponential Linear Unit) activation function. ELU is designed to address some of the limitations of the ReLU activation function, particularly the issue of dead neurons in the network. ELU provides negative outputs for negative inputs, which helps in maintaining the gradient flow during training.\n\n### Approach\nThe ELU activation function is defined as a piecewise function:\n- For non-negative inputs (x \u2265 0), ELU returns the input value itself.\n- For negative inputs (x < 0), ELU returns a value computed as \u03b1 * (e^x - 1), where \u03b1 is a parameter that controls the value of the function for negative inputs.\n\nThe steps to implement the ELU function are as follows:\n1. Check if the input value `x` is non-negative. If it is, return `x` directly.\n2. If `x` is negative, compute the value using the formula \u03b1 * (e^x - 1), where `e` is the base of the natural logarithm.\n3. Round the result to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to four decimal places\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)\n```\n\n### Explanation\n- The function `elu` takes two parameters: `x` (the input value) and `alpha` (a parameter that scales the output for negative inputs, defaulting to 1.0).\n- For non-negative `x`, the function returns `x` directly.\n- For negative `x`, the function computes the value using the formula \u03b1 * (e^x - 1), where `math.exp(x)` computes e raised to the power of `x`.\n- The result is then rounded to four decimal places to ensure the output meets the specified precision.\n\nThis implementation efficiently handles both positive and negative inputs, providing the correct ELU activation value while maintaining the required precision."}
{"task_id": 98, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Parametric ReLU (PReLU) activation function. PReLU is a variant of the ReLU activation function that introduces a learnable parameter for negative inputs, allowing the function to adjust the slope for negative values.\n\n### Approach\nThe PReLU activation function is defined such that for any input value \\( x \\):\n- If \\( x \\) is positive, the output is \\( x \\).\n- If \\( x \\) is negative, the output is \\( \\alpha \\times x \\), where \\( \\alpha \\) is a learnable parameter that defaults to 0.25.\n\nThe approach involves:\n1. Checking if the input \\( x \\) is negative.\n2. If \\( x \\) is negative, returning the product of \\( x \\) and the parameter \\( \\alpha \\).\n3. If \\( x \\) is non-negative, returning \\( x \\) as is.\n\nThis approach ensures that the function handles both positive and negative inputs correctly, providing the appropriate output based on the value of \\( x \\).\n\n### Solution Code\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    else:\n        return x\n```\n\n### Explanation\nThe function `prelu` takes two parameters: `x` (the input value) and `alpha` (the slope parameter for negative values, defaulting to 0.25). The function checks if `x` is negative. If it is, the function returns the product of `x` and `alpha`. If `x` is non-negative, the function simply returns `x`. This implementation efficiently handles both positive and negative inputs, providing the correct output as per the PReLU activation function definition."}
{"task_id": 99, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softplus activation function, which is a smooth approximation of the ReLU function. The Softplus function is defined as log(1 + e^x). However, we need to handle edge cases to prevent numerical overflow or underflow when computing this function for very large or very small values of x.\n\n### Approach\nThe Softplus function can be computed using the formula log(1 + e^x). However, directly computing this for very large or very small values of x can lead to numerical issues. To address this, we use the following approach:\n\n1. **For very large positive values of x (x > 20)**: The term e^x becomes extremely large, making 1 negligible in comparison. Thus, log(1 + e^x) is approximately equal to x. Therefore, we return x directly to avoid overflow.\n2. **For very large negative values of x (x < -20)**: The term e^x becomes extremely small, making 1 + e^x approximately equal to 1. Thus, log(1 + e^x) is approximately 0. Therefore, we return 0 to avoid underflow.\n3. **For values of x between -20 and 20**: We compute the Softplus function using the formula log(1 + e^x) directly, as these values do not cause numerical issues.\n\n### Solution Code\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return 0.0\n    else:\n        result = math.log(1 + math.exp(x))\n        return round(result, 4)\n```\n\n### Explanation\n- **Handling Large Positive Values**: When x is greater than 20, the function returns x directly after rounding to four decimal places. This avoids overflow issues that would arise from computing e^x for very large x.\n- **Handling Large Negative Values**: When x is less than -20, the function returns 0.0. This avoids underflow issues that would arise from computing e^x for very large negative x.\n- **Intermediate Values**: For values of x between -20 and 20, the function computes the Softplus value using the formula log(1 + e^x) and returns the result rounded to four decimal places.\n\nThis approach ensures that the Softplus function is computed efficiently and accurately without encountering numerical overflow or underflow issues."}
{"task_id": 100, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Softsign activation function, which is a smooth activation function used in neural networks. The Softsign function is known for its ability to bound the output between -1 and 1, making it useful in various neural network applications.\n\n### Approach\nThe Softsign activation function is defined by the formula:\n\\[ \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\]\nThis function takes an input value \\( x \\) and returns a value that is always between -1 and 1. The function is symmetric around zero, meaning that for positive \\( x \\), the output approaches 1, and for negative \\( x \\), the output approaches -1.\n\nThe steps to implement the Softsign function are as follows:\n1. Compute the absolute value of the input \\( x \\).\n2. Add 1 to this absolute value.\n3. Divide the input \\( x \\) by the result from step 2.\n4. Round the result to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to four decimal places.\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n```\n\n### Explanation\n1. **Compute Absolute Value**: The absolute value of \\( x \\) is calculated to handle both positive and negative inputs uniformly.\n2. **Add 1**: Adding 1 to the absolute value ensures that the denominator is always positive and greater than zero, avoiding division by zero errors.\n3. **Division**: Dividing \\( x \\) by the sum of 1 and the absolute value of \\( x \\) ensures the output is bounded between -1 and 1.\n4. **Rounding**: The result is rounded to four decimal places to meet the specified precision requirement.\n\nThis approach efficiently computes the Softsign activation function and ensures the output is within the required bounds and precision."}
{"task_id": 101, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the GRPO (Group Relative Policy Optimization) objective function used in reinforcement learning to optimize policy parameters. The objective function combines a clipped likelihood ratio with advantage estimates and applies a KL divergence penalty to maintain training stability.\n\n### Approach\nThe GRPO objective function is designed to maximize the expected advantage while ensuring the policy remains close to a reference policy. The approach involves the following steps:\n\n1. **Clipping the Likelihood Ratios**: The likelihood ratios (rhos) are clipped to prevent large changes in the policy. This is done using a clipping parameter (epsilon), ensuring the ratios stay within the range [1 - epsilon, 1 + epsilon].\n\n2. **Compute the Advantage Contribution**: The clipped likelihood ratios are multiplied by the advantage estimates, and the mean of these products is computed. This forms the first part of the objective function.\n\n3. **Compute the KL Divergence Penalty**: The KL divergence between the new policy and the reference policy is computed. This term acts as a penalty to ensure the new policy does not deviate too much from the reference policy.\n\n4. **Combine the Components**: The final objective value is obtained by subtracting the KL divergence penalty (scaled by a coefficient beta) from the mean advantage contribution.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Clip the likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the first part of the objective: mean of clipped_rhos * A\n    part1 = np.mean(clipped_rhos * A)\n    \n    # Compute the new policy probabilities\n    pi_theta = rhos * pi_theta_old\n    \n    # Compute the KL divergence between the new policy and the reference policy\n    # Avoid division by zero by ensuring pi_theta_ref is non-zero where pi_theta is non-zero\n    # This assumes that pi_theta_ref is non-zero where pi_theta is non-zero\n    kl = np.sum(pi_theta * np.log(pi_theta / pi_theta_ref))\n    \n    # Compute the GRPO objective\n    objective = part1 - beta * kl\n    \n    # Round the result to 6 decimal places\n    return round(objective, 6)\n```\n\n### Explanation\n1. **Clipping the Likelihood Ratios**: The likelihood ratios are clipped to ensure they do not deviate too much from 1, which helps in maintaining stability during training.\n\n2. **Advantage Contribution**: The clipped ratios are multiplied by the advantage estimates, and the mean of these products is taken. This part of the objective function encourages the policy to take actions that are beneficial as indicated by the advantage estimates.\n\n3. **KL Divergence Penalty**: The KL divergence measures how much the new policy differs from the reference policy. By including this term with a negative sign, the objective function penalizes large deviations from the reference policy, ensuring stability.\n\n4. **Combining Components**: The final objective value is a balance between maximizing the advantage and maintaining proximity to the reference policy, achieved by combining the clipped advantage contribution and the KL divergence penalty.\n\nThis approach ensures that the policy optimization is both effective and stable, leveraging the benefits of both clipped policy gradients and KL divergence regularization."}
{"task_id": 102, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Swish activation function, which is a self-gated activation function that has shown superior performance in deep neural networks compared to ReLU. The Swish function is defined as the product of the input value and the sigmoid of the input value.\n\n### Approach\nThe Swish activation function is given by the formula:\n\\[ \\text{Swish}(x) = x \\cdot \\sigma(x) \\]\nwhere \\(\\sigma(x)\\) is the sigmoid function defined as:\n\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\n\nThe approach to implement the Swish function involves the following steps:\n1. Compute the sigmoid of the input value \\(x\\).\n2. Multiply the input value \\(x\\) by the computed sigmoid value.\n3. Round the result to the nearest 4th decimal place to ensure precision.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    result = x * sigmoid\n    return round(result, 4)\n```\n\n### Explanation\n1. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This function maps any real number to a value between 0 and 1, which helps in introducing non-linearity into the model.\n2. **Swish Function**: The Swish function is then calculated by multiplying the input value \\(x\\) with its corresponding sigmoid value. This self-gating mechanism allows the function to retain some properties of the input while introducing non-linearity.\n3. **Rounding**: The result is rounded to the nearest 4th decimal place to ensure the output is precise and formatted correctly.\n\nThis implementation efficiently computes the Swish activation function for a given input value, ensuring both accuracy and precision."}
{"task_id": 103, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the SELU (Scaled Exponential Linear Unit) activation function, which is a self-normalizing variant of the ELU activation function. The SELU function is designed to maintain the mean and variance of the inputs, which helps in training deep neural networks more effectively.\n\n### Approach\nThe SELU activation function is defined as follows:\n- For positive inputs (x > 0), the function is simply the input scaled by a factor.\n- For non-positive inputs (x \u2264 0), the function uses an exponential term scaled by two factors, alpha and the same scale factor used for positive inputs.\n\nThe specific parameters used are:\n- `alpha = 1.6732632423543772`\n- `scale = 1.0507009873554804`\n\nThe function computes the output based on whether the input is positive or non-positive. For numerical stability, especially when dealing with very large negative inputs, the exponential term is handled appropriately to avoid underflow issues. The result is then rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\nimport math\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to four decimal places.\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)\n```\n\n### Explanation\n1. **Positive Input Handling**: If the input `x` is positive, the function returns the input scaled by the factor `scale`.\n2. **Non-Positive Input Handling**: If the input `x` is zero or negative, the function computes the exponential term `math.exp(x)`, subtracts 1, and then scales the result by both `alpha` and `scale`.\n3. **Numerical Stability**: The use of `math.exp` ensures that even for very large negative values of `x`, the computation remains numerically stable.\n4. **Rounding**: The result is rounded to four decimal places to meet the specified output constraints.\n\nThis implementation efficiently handles both positive and non-positive inputs, ensuring accurate and stable computation of the SELU activation function."}
{"task_id": 104, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function for binary classification using logistic regression. The function will compute the probabilities of the input data belonging to a certain class using the sigmoid function and then make binary predictions based on a threshold of 0.5.\n\n### Approach\n1. **Linear Combination Calculation**: Compute the linear combination of the input features and the model weights, then add the bias term. This step transforms the input data into a form suitable for the sigmoid function.\n2. **Sigmoid Function Application**: Apply the sigmoid function to the linear combination to convert the output into probabilities. The sigmoid function maps any real-valued number to a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class.\n3. **Binary Prediction**: Convert the probabilities into binary predictions by checking if each probability is greater than or equal to 0.5. If it is, the prediction is 1; otherwise, it is 0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination\n    z = np.dot(X, weights) + bias\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    # Convert probabilities to binary predictions\n    predictions = (probabilities >= 0.5).astype(int)\n    # Return as a Python list\n    return predictions.tolist()\n```\n\n### Explanation\n1. **Linear Combination Calculation**: The input feature matrix `X` is multiplied by the weights vector, resulting in a vector of linear combinations. Adding the bias term to this vector adjusts the output to fit the data better.\n2. **Sigmoid Function Application**: The sigmoid function transforms the linear combinations into probabilities. This function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\), where \\( z \\) is the linear combination.\n3. **Binary Prediction**: Each probability is compared against the threshold of 0.5. If the probability is 0.5 or higher, the prediction is 1; otherwise, it is 0. This step converts the continuous probability outputs into binary class predictions.\n\nThis approach efficiently computes the predictions using vectorized operations, ensuring that the solution is both concise and performant."}
{"task_id": 105, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient descent-based training algorithm for Softmax regression. The goal is to compute the model parameters using Cross Entropy loss and return the optimized coefficients along with the collected loss values over iterations.\n\n### Approach\n1. **Problem Analysis**: Softmax regression is used for multi-class classification problems. The model uses the Softmax function to output probabilities for each class, and the Cross Entropy loss is used to measure the difference between the predicted probabilities and the true labels.\n\n2. **Data Preprocessing**: \n   - Add a column of ones to the feature matrix `X` to account for the bias term in the model.\n   - Convert the target labels `y` into one-hot encoding to facilitate the calculation of the Cross Entropy loss.\n\n3. **Initialization**: Initialize the weight matrix `B` with small random values. The shape of `B` is determined by the number of features (including the bias term) and the number of classes.\n\n4. **Gradient Descent**: For each iteration, perform the following steps:\n   - **Forward Pass**: Compute the logits by multiplying the feature matrix `X` with the weight matrix `B`. Apply the Softmax function to these logits to get the probabilities.\n   - **Loss Calculation**: Compute the Cross Entropy loss between the predicted probabilities and the one-hot encoded target labels.\n   - **Gradient Calculation**: Compute the gradients of the loss with respect to the weight matrix `B`.\n   - **Weight Update**: Update the weights using the computed gradients and the learning rate.\n\n5. **Result Preparation**: After completing all iterations, flatten the weight matrix `B` into a list of floats and round each value to four decimal places. Similarly, round each loss value to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Add bias term to X\n    m, n = X.shape\n    X = np.hstack((np.ones((m, 1)), X))\n    \n    # Convert y to one-hot encoding\n    y = y.flatten()\n    k = len(np.unique(y))\n    y_onehot = np.eye(k)[y]\n    \n    # Initialize B with small random values\n    B = np.random.randn(n + 1, k) * 0.01  # n+1 because of the added bias\n    \n    # Initialize losses list\n    losses = []\n    \n    for _ in range(iterations):\n        # Forward pass\n        Z = X @ B\n        # Compute Softmax\n        Z = Z - np.max(Z, axis=1, keepdims=True)\n        exp_Z = np.exp(Z)\n        A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n        \n        # Compute loss\n        epsilon = 1e-15\n        A_clipped = np.clip(A, epsilon, 1 - epsilon)\n        loss = - np.mean(np.sum(y_onehot * np.log(A_clipped), axis=1))\n        losses.append(round(loss, 4))\n        \n        # Compute gradients\n        grad = (X.T @ (A - y_onehot)) / m\n        \n        # Update B\n        B = B - learning_rate * grad\n    \n    # Flatten B and round to 4 decimals\n    B_flat = B.flatten().round(4).tolist()\n    \n    # Return as tuple\n    return (B_flat, losses)\n```\n\n### Explanation\n- **Data Preprocessing**: The feature matrix `X` is augmented with a column of ones to include a bias term. The target labels `y` are converted into one-hot encoding to facilitate the loss calculation.\n- **Initialization**: The weight matrix `B` is initialized with small random values to break symmetry and ensure proper learning.\n- **Forward Pass**: The logits are computed by matrix multiplication of `X` and `B`. The Softmax function is applied to these logits to get the probabilities.\n- **Loss Calculation**: The Cross Entropy loss is computed using the predicted probabilities and the one-hot encoded target labels.\n- **Gradient Calculation**: The gradients of the loss with respect to `B` are computed using the difference between the predicted probabilities and the target labels.\n- **Weight Update**: The weights are updated using the computed gradients and the learning rate.\n- **Result Preparation**: The final weights are flattened and rounded, and the loss values are collected and rounded for each iteration.\n\nThis approach ensures that the model parameters are optimized using gradient descent, and the loss values are tracked to monitor the training process."}
{"task_id": 106, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a gradient descent-based training algorithm for logistic regression. The goal is to compute the model parameters using Binary Cross Entropy (BCE) loss and return the optimized coefficients along with the collected loss values over iterations, both rounded to the nearest 4th decimal.\n\n### Approach\n1. **Problem Analysis**: Logistic regression is a classification algorithm that models the probability of a binary outcome. The BCE loss function is used to measure the difference between the predicted probabilities and the actual labels. Gradient descent is an optimization algorithm that minimizes this loss by adjusting the model parameters iteratively.\n\n2. **Intuition**: The logistic regression model uses the sigmoid function to map the linear combination of features to a probability between 0 and 1. The BCE loss function quantifies the error between the predicted probabilities and the actual labels. By computing the gradient of this loss with respect to the model parameters, we can update the parameters in the direction that minimizes the loss.\n\n3. **Algorithm Selection**: We use gradient descent to iteratively update the model parameters. The steps involve:\n   - Adding a bias term to the feature matrix to include an intercept in the model.\n   - Initializing the weights (parameters) to zeros.\n   - Iteratively computing the predictions, loss, gradients, and updating the weights.\n\n4. **Complexity Analysis**: The time complexity is O(iterations * n), where n is the number of features, due to the matrix multiplications involved in computing the predictions and gradients.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add a column of ones to X for the bias term\n    X_new = np.hstack((np.ones((X.shape[0], 1)), X))\n    m, n = X_new.shape\n    weights = np.zeros((n, 1))\n    losses = []\n    \n    for _ in range(iterations):\n        # Compute the linear combination\n        z = X_new @ weights\n        # Apply sigmoid activation\n        a = 1 / (1 + np.exp(-z))\n        # Reshape y to match dimensions\n        y_reshaped = y.reshape(-1, 1)\n        \n        # Compute the loss\n        loss = (-1/m) * np.sum(y_reshaped * np.log(a) + (1 - y_reshaped) * np.log(1 - a))\n        losses.append(round(loss, 4))\n        \n        # Compute the gradient\n        gradient = (X_new.T @ (a - y_reshaped)) / m\n        \n        # Update the weights\n        weights = weights - learning_rate * gradient\n    \n    # Round the weights to 4 decimal places\n    weights = np.round(weights, 4)\n    # Convert weights to a list\n    weights_list = weights.flatten().tolist()\n    \n    return (losses, weights_list)\n```\n\n### Explanation\n1. **Adding Bias Term**: A column of ones is added to the feature matrix `X` to account for the bias term in the model. This allows the model to learn an intercept.\n\n2. **Initialization**: The weights (parameters) are initialized to zeros. The number of weights is equal to the number of features plus one (for the bias).\n\n3. **Iterations**: For each iteration, the following steps are performed:\n   - **Linear Combination**: Compute the linear combination of features and weights.\n   - **Sigmoid Activation**: Apply the sigmoid function to convert the linear combination into probabilities.\n   - **Loss Calculation**: Compute the BCE loss between the predicted probabilities and the actual labels.\n   - **Gradient Calculation**: Compute the gradient of the loss with respect to the weights.\n   - **Weight Update**: Update the weights using the computed gradient and learning rate.\n\n4. **Result**: After completing the specified number of iterations, the optimized weights and the loss values collected during each iteration are returned. Both the weights and losses are rounded to four decimal places for precision."}
{"task_id": 107, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. The goal is to compute the attention scores using query (Q), key (K), and value (V) matrices, apply a mask to these scores, and then compute the final output using these masked attention scores.\n\n### Approach\n1. **Compute Attention Scores**: The attention scores are calculated using the dot product of the query (Q) and key (K) matrices. This is done using Einstein summation for efficient computation.\n2. **Scale the Scores**: The attention scores are scaled by the square root of the dimension of the key vectors to prevent them from becoming too large.\n3. **Apply Mask**: The mask is applied to the attention scores to zero out the contributions from certain positions. This is done by setting the scores at masked positions to a very large negative value, effectively making them zero after applying the softmax function.\n4. **Compute Softmax**: The softmax function is applied to the masked attention scores to obtain the attention weights, which sum up to 1 for each position.\n5. **Compute Output**: The final output is computed by taking the weighted sum of the value (V) vectors using the attention weights.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores using Einstein summation\n    scores = np.einsum('bik, bjk -> bij', Q, K)\n    # Scale the scores by the square root of the key dimension\n    d_k = K.shape[-1]\n    scores = scores / np.sqrt(d_k)\n    # Apply the mask to the scores\n    mask = mask.astype(scores.dtype)\n    scores = scores + (1 - mask) * (-1e9)\n    # Compute the attention weights using softmax\n    attention_weights = np.exp(scores)\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    # Compute the output using the attention weights and value vectors\n    output = np.einsum('bij, bjk -> bik', attention_weights, V)\n    # Convert the output to a list and return\n    return output.tolist()\n```\n\n### Explanation\n1. **Compute Q, K, V Matrices**: The `compute_qkv` function takes an input matrix `X` and weight matrices `W_q`, `W_k`, and `W_v` to compute the query, key, and value matrices respectively.\n2. **Attention Scores Calculation**: Using Einstein summation, the attention scores are computed efficiently. This step ensures that each element in the query matrix is dotted with each element in the key matrix to form the attention scores matrix.\n3. **Scaling**: The attention scores are scaled by the square root of the dimension of the key vectors to maintain numerical stability.\n4. **Mask Application**: The mask is applied to the attention scores to prevent certain positions from contributing to the attention mechanism. This is crucial for tasks like text generation where future positions should not be attended to in the decoder.\n5. **Softmax**: The softmax function is applied to the masked attention scores to convert them into probabilities, ensuring that the attention weights sum up to 1 for each position.\n6. **Output Computation**: The final output is computed by taking the weighted sum of the value vectors using the attention weights, resulting in the final output matrix which is then converted to a list and returned.\n\nThis approach efficiently computes the masked self-attention mechanism, ensuring that the model can handle sequence modeling tasks with appropriate masking to maintain the correct flow of information."}
{"task_id": 108, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to measure the disorder in a basket of apples based on their colors. The disorder should be 0 if all apples are the same color and should increase as the variety of colors increases. \n\n### Approach\nThe approach to measure disorder is based on the concept of entropy from information theory. Entropy is a measure of uncertainty or randomness in a system. In this context, higher entropy indicates a higher disorder in the distribution of apple colors.\n\n1. **Count Frequencies**: First, we count the frequency of each color in the list of apples using the `Counter` class from the `collections` module.\n2. **Calculate Probabilities**: For each color, we calculate its probability by dividing its count by the total number of apples.\n3. **Compute Entropy**: Using the probabilities, we compute the entropy using the formula:\n   \\[\n   H = -\\sum (p_i \\cdot \\log_2(p_i))\n   \\]\n   where \\( p_i \\) is the probability of each color.\n4. **Return Result**: The computed entropy is rounded to four decimal places and returned as the measure of disorder.\n\n### Solution Code\n```python\nfrom collections import Counter\nimport math\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy += p * math.log2(p)\n    entropy = -entropy\n    return round(entropy, 4)\n```\n\n### Explanation\n- **Counting Frequencies**: The `Counter` class efficiently counts the occurrences of each color in the list.\n- **Probability Calculation**: Each color's probability is determined by dividing its count by the total number of apples.\n- **Entropy Calculation**: The entropy formula sums the product of each probability and its logarithm (base 2), then negates the result to ensure a positive measure of disorder.\n- **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\nThis approach ensures that the disorder measure is 0 when all apples are the same color and increases as the variety of colors increases, satisfying all given conditions."}
{"task_id": 109, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function to perform Layer Normalization on a 3D input tensor. Layer Normalization is a technique used in neural networks to normalize the activations of a layer, which helps in stabilizing the training process and improving the model's performance.\n\n### Approach\nThe approach to implement Layer Normalization involves the following steps:\n\n1. **Compute the Mean**: Calculate the mean of the input tensor across the feature dimension for each sequence in the batch. This results in a tensor where each element represents the mean of the corresponding feature vector.\n2. **Compute the Variance**: Calculate the variance of the input tensor across the feature dimension. This is done by first centering the data (subtracting the mean), squaring the result, and then taking the mean of these squared values.\n3. **Normalize the Data**: Normalize each feature vector by subtracting the mean and dividing by the square root of the variance (plus a small epsilon to avoid division by zero).\n4. **Scale and Shift**: Apply scaling using the gamma parameter and shifting using the beta parameter to the normalized data. This allows the model to learn the optimal scale and shift for each feature.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Calculate the mean along the feature dimension (axis=2)\n    mean = X.mean(axis=2, keepdims=True)\n    \n    # Calculate the variance along the feature dimension\n    var = ((X - mean) ** 2).mean(axis=2, keepdims=True)\n    \n    # Compute the denominator with epsilon to prevent division by zero\n    denominator = np.sqrt(var + epsilon)\n    \n    # Normalize the input tensor\n    normalized = (X - mean) / denominator\n    \n    # Apply scaling and shifting\n    output = gamma * normalized + beta\n    \n    # Round to 5 decimal places and convert to list\n    return output.round(5).tolist()\n```\n\n### Explanation\n1. **Mean Calculation**: The mean is computed along the feature dimension (axis=2) using `X.mean(axis=2, keepdims=True)`. This ensures that the mean tensor has the same shape as the input tensor except for the feature dimension, which is reduced to 1.\n2. **Variance Calculation**: The variance is computed by first centering the data (subtracting the mean), squaring the result, and then taking the mean along the feature dimension. This is done using `((X - mean) ** 2).mean(axis=2, keepdims=True)`.\n3. **Normalization**: The input tensor is normalized by subtracting the mean and dividing by the square root of the variance (plus epsilon). This step ensures that each feature vector has a mean of 0 and a variance of 1.\n4. **Scaling and Shifting**: The normalized tensor is then scaled by the gamma parameter and shifted by the beta parameter. This allows the model to learn the optimal scale and shift for each feature.\n5. **Rounding and Conversion**: The final output is rounded to 5 decimal places and converted to a list using `output.round(5).tolist()`.\n\nThis approach ensures that the input tensor is properly normalized, which helps in stabilizing the training process and improving the model's performance."}
{"task_id": 110, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to evaluate the quality of machine translation using the METEOR score. The METEOR score combines precision, recall, and a penalty for word order fragmentation to provide a comprehensive measure of translation quality.\n\n### Approach\nThe approach to compute the METEOR score involves several steps:\n\n1. **Tokenization**: Convert the reference and candidate translations into lists of words.\n2. **Unigram Matches**: Count the number of words that appear in both the reference and candidate, considering their frequencies.\n3. **Precision and Recall**: Calculate precision as the ratio of matches to the total words in the candidate, and recall as the ratio of matches to the total words in the reference.\n4. **F-score**: Compute the F-score using a weighted harmonic mean of precision and recall, with a parameter beta to emphasize recall.\n5. **Longest Common Subsequence (LCS)**: Determine the length of the longest subsequence common to both the reference and candidate to assess word order fragmentation.\n6. **Penalty Calculation**: Apply a penalty based on the LCS length relative to the number of matches.\n7. **Final Score**: Combine the F-score and the penalty to get the final METEOR score.\n\n### Solution Code\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    can_tokens = candidate.split()\n    \n    ref_counts = Counter(ref_tokens)\n    can_counts = Counter(can_tokens)\n    \n    matches = 0\n    for word in ref_counts:\n        if word in can_counts:\n            matches += min(ref_counts[word], can_counts[word])\n    \n    len_ref = len(ref_tokens)\n    len_can = len(can_tokens)\n    \n    precision = matches / len_can if len_can != 0 else 0.0\n    recall = matches / len_ref if len_ref != 0 else 0.0\n    \n    beta_sq = beta ** 2\n    if (beta_sq * precision + recall) == 0:\n        f = 0.0\n    else:\n        f = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n    \n    def lcs(a, b):\n        m = len(a)\n        n = len(b)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m + 1):\n            for j in range(n + 1):\n                if i == 0 or j == 0:\n                    dp[i][j] = 0\n                elif a[i-1] == b[j-1]:\n                    dp[i][j] = dp[i-1][j-1] + 1\n                else:\n                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n        return dp[m][n]\n    \n    lcs_length = lcs(ref_tokens, can_tokens)\n    \n    if matches == 0:\n        penalty = 0.0\n    else:\n        penalty = gamma * (1 - (lcs_length / matches))\n    \n    if f == 0:\n        score = 0.0\n    else:\n        score = f * (1 - penalty)\n    \n    return round(score, 3)\n```\n\n### Explanation\n1. **Tokenization**: The input strings are split into lists of words to facilitate comparison.\n2. **Unigram Matches**: Using `Counter` from the `collections` module, we count occurrences of each word in both the reference and candidate. Matches are determined by the minimum count of each word in both lists.\n3. **Precision and Recall**: Precision is the ratio of matches to the length of the candidate, and recall is the ratio of matches to the length of the reference.\n4. **F-score**: The F-score is calculated using the formula for the weighted harmonic mean, emphasizing recall with the beta parameter.\n5. **LCS Calculation**: The longest common subsequence (LCS) is computed using dynamic programming to determine the longest sequence of words common to both the reference and candidate.\n6. **Penalty Calculation**: The penalty is applied based on the ratio of LCS length to the number of matches, scaled by the gamma parameter.\n7. **Final Score**: The final score is the product of the F-score and the penalty-adjusted factor, rounded to three decimal places.\n\nThis approach ensures a comprehensive evaluation of translation quality, considering both word matches and word order."}
{"task_id": 111, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\n### Approach\nThe PMI is calculated using the formula:\n\\[ \\text{PMI}(x, y) = \\log\\left(\\frac{p(x, y)}{p(x) \\cdot p(y)}\\right) \\]\nwhere:\n- \\( p(x, y) \\) is the joint probability of events x and y occurring together.\n- \\( p(x) \\) and \\( p(y) \\) are the marginal probabilities of events x and y occurring individually.\n\nGiven the counts, we can derive these probabilities as follows:\n- \\( p(x, y) = \\frac{\\text{joint\\_counts}}{\\text{total\\_samples}} \\)\n- \\( p(x) = \\frac{\\text{total\\_counts\\_x}}{\\text{total\\_samples}} \\)\n- \\( p(y) = \\frac{\\text{total\\_counts\\_y}}{\\text{total\\_samples}} \\)\n\nSubstituting these into the PMI formula, we get:\n\\[ \\text{PMI} = \\log\\left(\\frac{\\text{joint\\_counts} \\cdot \\text{total\\_samples}}{\\text{total\\_counts\\_x} \\cdot \\text{total\\_counts\\_y}}\\right) \\]\n\nWe use the natural logarithm for this calculation. The result is then rounded to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    ratio = (joint_counts * total_samples) / (total_counts_x * total_counts_y)\n    pmi = np.log(ratio)\n    return round(pmi, 3)\n```\n\n### Explanation\n1. **Calculate the Ratio**: The ratio is computed as the product of the joint counts and total samples divided by the product of the individual counts of x and y.\n2. **Compute the Logarithm**: The natural logarithm of the ratio is taken to get the PMI value.\n3. **Round the Result**: The PMI value is rounded to three decimal places to meet the output constraints.\n\nThis approach efficiently computes the PMI using basic arithmetic operations and logarithmic functions, ensuring the result is accurate and formatted as required."}
{"task_id": 112, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1]. This technique ensures that all features contribute equally to a model by scaling them to a common range.\n\n### Approach\nThe Min-Max Normalization technique involves the following steps:\n1. **Identify the Range**: Determine the minimum and maximum values in the input list. These values define the range of the data.\n2. **Handle Edge Cases**: If all values in the list are the same, the range is zero, and all normalized values should be 0.0.\n3. **Normalize Each Value**: For each value in the list, apply the Min-Max formula: \\( \\text{normalized value} = \\frac{\\text{value} - \\text{min}}{\\text{max} - \\text{min}} \\). This scales each value to the range [0, 1].\n4. **Round Values**: Each normalized value is rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    if max_val == min_val:\n        return [0.0 for _ in x]\n    normalized = [(num - min_val) / (max_val - min_val) for num in x]\n    return [round(num, 4) for num in normalized]\n```\n\n### Explanation\n1. **Finding Min and Max**: The function starts by finding the minimum and maximum values in the input list using Python's built-in `min` and `max` functions.\n2. **Edge Case Handling**: If the minimum and maximum values are the same, it means all values in the list are identical. In this case, the function returns a list of 0.0 values.\n3. **Normalization**: For each value in the list, the function computes the normalized value using the formula. This ensures that the smallest value becomes 0 and the largest becomes 1, with all other values scaled proportionally.\n4. **Rounding**: Each normalized value is rounded to four decimal places to ensure the output meets the specified precision.\n\nThis approach efficiently handles all edge cases and ensures that the output is correctly formatted and scaled."}
{"task_id": 113, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a simple residual block using NumPy. A residual block is a fundamental component in deep learning models, particularly in convolutional neural networks (CNNs), where it helps in training deeper networks by mitigating the vanishing gradient problem. The block processes the input through two weight layers with ReLU activations and includes a shortcut connection that adds the original input to the output of the two layers before the final ReLU activation.\n\n### Approach\n1. **Matrix Multiplication**: The input array is first processed through two weight layers using matrix multiplication. This involves multiplying the input array by the first weight matrix and then the result by the second weight matrix.\n2. **ReLU Activation**: After each matrix multiplication, a ReLU (Rectified Linear Unit) activation function is applied to introduce non-linearity into the model.\n3. **Shortcut Connection**: The output of the second weight layer is added to the original input array. This shortcut connection helps in preserving the gradient information and allows the model to learn identity mappings.\n4. **Final ReLU Activation**: After adding the shortcut connection, another ReLU activation is applied to the result.\n5. **Output Processing**: The final output is rounded to four decimal places and converted to a list for the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    # First layer: matrix multiplication and ReLU\n    y1 = x.dot(w1)\n    y1_relu = np.maximum(y1, 0)\n    \n    # Second layer: matrix multiplication and ReLU\n    y2 = y1_relu.dot(w2)\n    \n    # Add the shortcut connection\n    y_add = y2 + x\n    \n    # Final ReLU activation\n    y_final = np.maximum(y_add, 0)\n    \n    # Round to 4 decimal places and convert to list\n    return y_final.round(4).tolist()\n```\n\n### Explanation\n1. **Matrix Multiplication**: The input array `x` is multiplied by the first weight matrix `w1` to produce an intermediate result `y1`. This step transforms the input into a higher-dimensional space.\n2. **ReLU Activation**: The intermediate result `y1` is passed through the ReLU activation function, which sets all negative values to zero, introducing non-linearity.\n3. **Second Matrix Multiplication**: The result from the first ReLU activation is then multiplied by the second weight matrix `w2` to produce another intermediate result `y2`.\n4. **Shortcut Connection**: The intermediate result `y2` is added to the original input `x`, allowing the model to bypass the non-linear transformations and directly pass the input through.\n5. **Final ReLU Activation**: The result from the shortcut connection is passed through another ReLU activation to ensure non-negative outputs.\n6. **Output Processing**: The final output is rounded to four decimal places and converted to a list to meet the specified output constraints.\n\nThis approach ensures that the residual block effectively processes the input while preserving gradient information, making it suitable for training deeper neural networks."}
{"task_id": 114, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Global Average Pooling on a 3D NumPy array. The function will take an input array of shape (height, width, channels) and return a 1D array where each element is the average of all values in the corresponding feature map.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understanding the Input and Output**: The input is a 3D NumPy array with dimensions (height, width, channels). The goal is to compute the average value for each feature map (each channel) across all its elements, resulting in a 1D array of shape (channels,).\n2. **Using NumPy's Mean Function**: We can leverage NumPy's `mean` function to compute the average. By specifying the axes over which to compute the mean, we can efficiently reduce the 3D array to a 1D array.\n3. **Axis Specification**: The mean should be computed across the first two dimensions (height and width) for each channel. This is achieved by using `axis=(0, 1)` in the `mean` function, which collapses these dimensions and leaves only the channels dimension.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return x.mean(axis=(0, 1))\n```\n\n### Explanation\n- **Input Handling**: The function takes a 3D NumPy array `x` as input.\n- **Mean Calculation**: The `mean` function is called with `axis=(0, 1)`, which computes the mean across the height and width dimensions for each channel. This results in a 1D array where each element corresponds to the average value of each feature map.\n- **Efficiency**: This approach is efficient as it uses NumPy's optimized operations to perform the computation in a vectorized manner, avoiding explicit loops and thus improving performance.\n\nThis solution effectively reduces the 3D input array to a 1D array by computing the average across the spatial dimensions for each channel, achieving the desired global average pooling operation."}
{"task_id": 115, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format (batch, channels, height, width). The function will normalize the input across the batch and spatial dimensions for each channel, then apply scale (gamma) and shift (beta) parameters, ensuring numerical stability using a provided epsilon value.\n\n### Approach\n1. **Compute Mean and Variance**: For each channel, compute the mean and variance across the batch and spatial dimensions. This involves averaging over the batch, height, and width dimensions for each channel.\n2. **Normalize the Input**: Subtract the mean from each element in the channel and divide by the square root of the variance (plus epsilon to ensure numerical stability).\n3. **Apply Scale and Shift**: Multiply the normalized values by the gamma parameter and add the beta parameter for each channel.\n4. **Round and Convert**: Round the resulting values to 4 decimal places and convert the final output to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # Compute mean across batch and spatial dimensions for each channel\n    mean = X.mean(axis=(0, 2, 3))\n    # Compute variance across batch and spatial dimensions for each channel\n    var = X.var(axis=(0, 2, 3))\n    # Normalize the input\n    normalized = (X - mean) / np.sqrt(var + epsilon)\n    # Apply scale (gamma) and shift (beta)\n    output = gamma * normalized + beta\n    # Round to 4 decimal places and convert to list\n    return np.round(output, decimals=4).tolist()\n```\n\n### Explanation\n1. **Mean and Variance Calculation**: The mean and variance are computed for each channel by averaging over the batch, height, and width dimensions. This ensures that each channel's normalization is based on its own statistics.\n2. **Normalization**: Each element in the input array is adjusted by subtracting the channel's mean and dividing by the square root of the channel's variance plus a small epsilon value to prevent division by zero.\n3. **Scaling and Shifting**: After normalization, the values are scaled by the gamma parameter and shifted by the beta parameter, allowing the model to learn the optimal scale and shift for each channel.\n4. **Rounding and Conversion**: The final output is rounded to 4 decimal places for precision and converted to a list to meet the specified output format.\n\nThis approach ensures that the batch normalization is applied correctly across the specified dimensions, providing the necessary normalization and transformation steps efficiently."}
{"task_id": 116, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the derivative of a polynomial term of the form `c * x^n` at a given point `x`. The derivative is calculated using the power rule from calculus, which states that the derivative of `x^n` is `n * x^(n-1)`. This function will return the value of the derivative rounded to four decimal places.\n\n### Approach\n1. **Identify the Polynomial Term**: The polynomial term is given as `c * x^n`, where `c` is the coefficient and `n` is the exponent.\n2. **Power Rule Application**: Using the power rule, the derivative of `c * x^n` is `c * n * x^(n-1)`.\n3. **Special Case Handling**: If the exponent `n` is zero, the term simplifies to a constant `c`, and the derivative of a constant is zero.\n4. **Compute the Derivative**: For non-zero `n`, compute the derivative using the formula derived from the power rule.\n5. **Rounding**: The result is rounded to four decimal places to meet the output constraints.\n\n### Solution Code\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    if n == 0:\n        return round(0.0, 4)\n    else:\n        derivative = c * n * (x ** (n - 1))\n        return round(derivative, 4)\n```\n\n### Explanation\n- **Special Case Handling**: The function first checks if the exponent `n` is zero. If it is, the function returns 0.0 because the derivative of a constant term is zero.\n- **Derivative Calculation**: For non-zero `n`, the function computes the derivative using the formula `c * n * x^(n-1)`. This formula is derived directly from the power rule.\n- **Rounding**: The result is rounded to four decimal places to ensure the output meets the specified precision requirement.\n\nThis approach efficiently handles both the general case and the special case where the exponent is zero, ensuring accurate and precise results."}
{"task_id": 117, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. The goal is to return a list of orthonormal vectors that span the same subspace as the input vectors, with each vector rounded to 4 decimal places.\n\n### Approach\n1. **Initialization**: Start with an empty list to store the orthonormal basis vectors.\n2. **Iterate through each vector**: For each vector in the input list, process it to check if it can contribute to the basis.\n3. **Subtract projections**: For each vector, subtract the projections onto each of the already computed basis vectors. This step ensures that the vector is orthogonal to all previous basis vectors.\n4. **Check linear independence**: Compute the norm (length) of the resulting vector. If the norm is greater than a given tolerance, the vector is considered linearly independent and is added to the basis after normalization.\n5. **Normalization**: Convert the vector into a unit vector by dividing it by its norm.\n6. **Rounding and Conversion**: After processing all vectors, round each basis vector to 4 decimal places and convert it to a list format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    basis = []\n    for v in vectors:\n        current = np.array(v)\n        for e in basis:\n            proj = (current.dot(e)) * e\n            current -= proj\n        norm = np.linalg.norm(current)\n        if norm > tol:\n            e = current / norm\n            basis.append(e)\n    # Round each vector to 4 decimal places and convert to list\n    rounded_basis = [np.round(vec, 4).tolist() for vec in basis]\n    return rounded_basis\n```\n\n### Explanation\n- **Initialization**: The `basis` list is initialized to store the resulting orthonormal vectors.\n- **Vector Processing**: Each vector is converted to a numpy array for efficient computation. For each vector, we subtract the projections onto all previously computed basis vectors to ensure orthogonality.\n- **Norm Check**: The norm of the resulting vector is checked against a tolerance to determine if it is linearly independent. If it is, the vector is normalized and added to the basis.\n- **Rounding and Conversion**: Each vector in the basis is rounded to 4 decimal places and converted to a list to meet the output requirements.\n\nThis approach ensures that we efficiently compute an orthonormal basis using the Gram-Schmidt process, handling both linear independence and numerical precision appropriately."}
{"task_id": 118, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to compute the cross product of two 3-dimensional vectors. The cross product results in a third vector that is perpendicular to both input vectors and follows the right-hand rule. This concept is fundamental in various fields such as physics, engineering, and 3D graphics.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Compute the Cross Product**: Use the numpy library's `np.cross` function to compute the cross product of the two input vectors. This function efficiently handles the computation using the determinant of a 3x3 matrix, which simplifies the implementation.\n2. **Round the Result**: The resulting vector from the cross product computation is then rounded to 4 decimal places to meet the output constraints.\n3. **Convert to List**: Finally, the rounded numpy array is converted to a list using the `tolist()` method to provide the result in the required format.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    cross_rounded = np.round(cross, 4)\n    return cross_rounded.tolist()\n```\n\n### Explanation\n1. **Cross Product Computation**: The `np.cross` function computes the cross product of two vectors. For vectors `a = [a1, a2, a3]` and `b = [b1, b2, b3]`, the cross product is calculated as:\n   - x-component: `a2*b3 - a3*b2`\n   - y-component: `a3*b1 - a1*b3`\n   - z-component: `a1*b2 - a2*b1`\n   \n2. **Rounding**: The result of the cross product is rounded to 4 decimal places using `np.round` to ensure precision and meet the output constraints.\n\n3. **Conversion to List**: The numpy array result is converted to a Python list using `tolist()` to provide the final output in the required format.\n\nThis approach efficiently computes the cross product using numpy's optimized functions, ensuring both accuracy and performance."}
{"task_id": 119, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a function that uses Cramer's Rule to solve a system of linear equations represented by the matrix equation \\(Ax = b\\). The function should return the solution vector \\(x\\) if a unique solution exists, or return -1 if the system does not have a unique solution.\n\n### Approach\nCramer's Rule is a method for solving systems of linear equations using determinants. The steps to apply Cramer's Rule are as follows:\n\n1. **Calculate the determinant of matrix A**: If the determinant is zero, the system does not have a unique solution, and we return -1.\n2. **Form new matrices for each variable**: For each variable \\(x_i\\), create a new matrix \\(A_i\\) by replacing the \\(i\\)-th column of \\(A\\) with the vector \\(b\\).\n3. **Calculate determinants of new matrices**: Compute the determinant of each new matrix \\(A_i\\).\n4. **Compute each variable**: Each variable \\(x_i\\) is the ratio of the determinant of \\(A_i\\) to the determinant of \\(A\\).\n5. **Round the results**: Each solution value is rounded to four decimal places for precision.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    return x\n```\n\n### Explanation\n1. **Determinant Check**: The function first computes the determinant of matrix \\(A\\). If the absolute value of this determinant is below a small threshold (1e-10), it indicates that the matrix is singular, and the function returns -1.\n2. **Matrix Copy and Column Replacement**: For each variable \\(x_i\\), a copy of matrix \\(A\\) is made, and the \\(i\\)-th column is replaced with the vector \\(b\\) to form the matrix \\(A_i\\).\n3. **Determinant Calculation**: The determinant of each modified matrix \\(A_i\\) is computed.\n4. **Solution Calculation**: Each variable \\(x_i\\) is calculated as the ratio of the determinant of \\(A_i\\) to the determinant of \\(A\\). The result is rounded to four decimal places to ensure precision.\n5. **Return Result**: The solution vector \\(x\\) is returned as a list of rounded values.\n\nThis approach efficiently applies Cramer's Rule to solve the system of linear equations, ensuring numerical stability and precision in the results."}
{"task_id": 120, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the Bhattacharyya distance between two discrete probability distributions. The Bhattacharyya distance is a measure of similarity between two probability distributions and is derived from the Bhattacharyya coefficient. The coefficient is computed as the sum of the square roots of the products of corresponding elements in the two distributions. The distance is then obtained by taking the negative natural logarithm of this coefficient.\n\n### Approach\n1. **Input Validation**: Check if the input lists `p` and `q` have the same length and are non-empty. If not, return 0.0.\n2. **Element-wise Product**: Compute the element-wise product of the two distributions.\n3. **Square Root Calculation**: Take the square root of each element in the product.\n4. **Sum of Square Roots**: Sum all the square roots obtained in the previous step.\n5. **Natural Logarithm**: Compute the natural logarithm of the sum.\n6. **Bhattacharyya Distance**: The distance is the negative of the natural logarithm value.\n7. **Rounding**: Round the result to four decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    product = np.multiply(p, q)\n    sqrt_product = np.sqrt(product)\n    sum_sqrt = np.sum(sqrt_product)\n    ln_sum = np.log(sum_sqrt)\n    distance = -ln_sum\n    return round(distance, 4)\n```\n\n### Explanation\n1. **Input Validation**: The function first checks if the lengths of `p` and `q` are the same and non-zero. If not, it returns 0.0 immediately.\n2. **Element-wise Product**: Using `numpy.multiply`, the function computes the product of corresponding elements in `p` and `q`.\n3. **Square Root Calculation**: The square root of each product element is computed using `numpy.sqrt`.\n4. **Sum of Square Roots**: The sum of these square roots is calculated using `numpy.sum`.\n5. **Natural Logarithm**: The natural logarithm of the sum is computed using `numpy.log`.\n6. **Bhattacharyya Distance**: The negative of this logarithm value gives the Bhattacharyya distance.\n7. **Rounding**: The result is rounded to four decimal places using Python's `round` function.\n\nThis approach ensures that the function efficiently computes the Bhattacharyya distance while handling edge cases such as invalid input lengths."}
{"task_id": 121, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to write a function that computes the element-wise sum of two vectors. The function should return a new vector representing the resulting sum if the operation is valid, or -1 if the vectors have incompatible dimensions.\n\n### Approach\n1. **Check Vector Lengths**: The first step is to check if the two input vectors have the same length. If they do not, the function should return -1 as the vectors cannot be summed element-wise.\n2. **Element-wise Summation**: If the vectors are of the same length, we proceed to compute the element-wise sum. This involves iterating through each element of the vectors, adding corresponding elements from each vector, and storing the result in a new list.\n3. **Return Result**: The resulting list of sums is returned if the vectors are compatible. If not, -1 is returned.\n\n### Solution Code\n```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    return result\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the two input lists `a` and `b` are equal. If they are not, it returns -1 immediately.\n- **Sum Elements**: If the lengths are equal, the function initializes an empty list `result` to store the sums. It then iterates over each index, computes the sum of the corresponding elements from `a` and `b`, and appends this sum to `result`.\n- **Return Result**: Finally, the function returns the `result` list containing the element-wise sums of the input vectors.\n\nThis approach ensures that the function handles both valid and invalid cases efficiently, providing the correct output as specified."}
{"task_id": 122, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the policy gradient estimator using the REINFORCE algorithm. The goal is to compute the average gradient of the log-policy multiplied by the return at each time step for a given list of episodes.\n\n### Approach\n1. **Policy Representation**: The policy is parameterized by a 2D NumPy array `theta` of shape `(num_states, num_actions)`. For each state, the policy is computed using the softmax function over `theta[s, :]`.\n\n2. **Episodes Processing**: Each episode is a list of tuples `(state, action, reward)`. For each episode, we need to compute the cumulative rewards (returns) for each time step.\n\n3. **Return Calculation**: For each step in an episode, the return is the sum of rewards from that step to the end of the episode. This is computed efficiently by iterating from the end of the episode to the beginning.\n\n4. **Gradient Calculation**: For each step, compute the gradient of the log-policy using the softmax derivative. The gradient contribution for each step is the product of this gradient and the corresponding return. These contributions are accumulated across all episodes.\n\n5. **Averaging**: The total gradient is averaged over all time steps across all episodes to get the final result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    # Initialize the gradient with zeros\n    gradient = np.zeros_like(theta)\n    total_steps = 0\n\n    for episode in episodes:\n        # Extract rewards from the episode\n        rewards = [r for (s, a, r) in episode]\n        # Compute the returns for each step\n        returns = []\n        cumulative = 0\n        for r in reversed(rewards):\n            cumulative += r\n            returns.append(cumulative)\n        # Reverse to get the correct order of returns\n        returns = returns[::-1]\n\n        for t in range(len(episode)):\n            s, a, r = episode[t]\n            G = returns[t]\n            # Compute the policy probabilities for state s\n            theta_s = theta[s, :]\n            # Compute softmax\n            exp_theta = np.exp(theta_s - np.max(theta_s))\n            pi = exp_theta / np.sum(exp_theta)\n            # Create one-hot vector for action a\n            one_hot = np.zeros_like(pi)\n            one_hot[a] = 1\n            # Compute the gradient contribution\n            delta = one_hot - pi\n            gradient[s, :] += delta * G\n            total_steps += 1\n\n    # Compute the average gradient\n    if total"}
{"task_id": 123, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to calculate the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer. The goal is to determine the percentage savings in floating-point operations (FLOPs) when using an MoE layer instead of a dense layer, given the number of experts, sparsity (number of active experts), and input/output dimensions.\n\n### Approach\n1. **Understanding the Problem**: The MoE layer consists of multiple expert networks and a gating network. The gating network selects a subset of experts (sparsity) to process the input, which reduces the computational load compared to a dense layer that uses all experts.\n\n2. **FLOPs Calculation**:\n   - **Dense Layer**: The FLOPs for a dense layer are calculated as the product of the number of experts, input dimension, and output dimension.\n   - **MoE Layer**: The FLOPs for the MoE layer include three components:\n     - Gating network operations.\n     - Operations from the active experts.\n     - Combination of the experts' outputs.\n\n3. **Formulas**:\n   - **Dense Layer FLOPs**: \\( \\text{dense\\_FLOPs} = n\\_experts \\times d\\_in \\times d\\_out \\)\n   - **MoE Layer FLOPs**: \\( \\text{moe\\_FLOPs} = k\\_active \\times d\\_in \\times d\\_out + d\\_in \\times n\\_experts + d\\_out \\times k\\_active \\)\n\n4. **Savings Calculation**: The percentage savings is computed as the difference between the FLOPs of the dense layer and the MoE layer, divided by the FLOPs of the dense layer, multiplied by 100.\n\n### Solution Code\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense = n_experts * d_in * d_out\n    moe = k_active * d_in * d_out + d_in * n_experts + d_out * k_active\n    if dense == 0:\n        return 0.0  # Avoid division by zero, though dense should never be zero in practice\n    savings = ((dense - moe) / dense) * 100\n    return round(savings, 1)\n```\n\n### Explanation\n- **Dense Layer FLOPs**: This is calculated by considering the total number of operations required for a dense layer with the same input and output dimensions as the MoE layer but scaled by the number of experts.\n- **MoE Layer FLOPs**: This includes the operations from the gating network, the active experts, and the combination of their outputs. The gating network selects the active experts, reducing the number of operations compared to using all experts.\n- **Savings Calculation**: The savings percentage is derived by comparing the FLOPs of the MoE layer to the dense layer, providing a measure of computational efficiency.\n\nThis approach ensures that we accurately measure the efficiency gains of using an MoE layer over a traditional dense layer, considering both the reduction in operations from sparsity and the overhead of the gating mechanism."}
{"task_id": 124, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models. This mechanism helps in distributing the input data to a subset of experts based on their relevance, determined by a gating function. The goal is to compute the final gating probabilities matrix, which indicates the probability of each input being processed by each expert.\n\n### Approach\nThe Noisy Top-K gating mechanism involves several steps:\n\n1. **Compute Initial Logits**: The initial logits are computed by multiplying the input matrix `X` with the gating weight matrix `W_g`. This gives a matrix where each element represents the initial relevance of an expert for a given input.\n\n2. **Add Noise**: Pre-sampled noise is scaled by `W_noise` and added to the initial logits. This noise helps in preventing the model from relying too heavily on a single expert and promotes diversity in expert selection.\n\n3. **Select Top-K Logits**: For each input, the top-K logits are selected based on their values after adding the noise. The remaining logits are set to a very low value (negative infinity) to ensure they have negligible probability in the subsequent softmax step.\n\n4. **Compute Softmax Probabilities**: The modified logits (with top-K values and others set to negative infinity) are passed through a softmax function to compute the final probabilities. This ensures that the probabilities sum to 1 for each input.\n\n5. **Round and Convert**: The resulting probabilities are rounded to four decimal places and converted to a list format for the final output.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute initial logits\n    initial_logits = np.dot(X, W_g)\n    \n    # Compute noise term\n    noise_term = W_noise * N\n    \n    # Add noise to logits\n    noisy_logits = initial_logits + noise_term\n    \n    # For each row, set non-top k to -inf\n    for i in range(noisy_logits.shape[0]):\n        row = noisy_logits[i]\n        # Get top k indices\n        topk_indices = np.argpartition(row, -k)[-k:]\n        # Create mask\n        mask = np.zeros_like(row)\n        mask[topk_indices] = 1\n        # Apply mask\n        noisy_logits[i] = row * mask + (1 - mask) * (-1e9)\n    \n    # Compute softmax\n    def softmax(x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    probs = softmax(noisy_logits)\n    \n    # Round to 4 decimals\n    probs = np.round(probs, decimals=4)\n    \n    # Convert to list\n    return probs.tolist()\n```\n\n### Explanation\n1. **Initial Logits Calculation**: The initial logits are computed using matrix multiplication of the input matrix `X` and the gating weight matrix `W_g`. This step determines the initial relevance scores for each expert.\n\n2. **Noise Addition**: The noise term is computed by element-wise multiplication of `W_noise` and the pre-sampled noise matrix `N`. This noise is added to the initial logits to introduce stochasticity, which helps in preventing over-reliance on specific experts.\n\n3. **Top-K Selection**: For each input, the top-K logits are identified and the rest are set to a very low value (negative infinity). This ensures that only the top-K experts are considered for each input.\n\n4. **Softmax Application**: The modified logits are passed through a softmax function to compute the probabilities. This step ensures that the probabilities sum to 1, providing a valid probability distribution over the experts.\n\n5. **Rounding and Conversion**: The resulting probabilities are rounded to four decimal places for precision and converted to a list format for the final output.\n\nThis approach efficiently implements the Noisy Top-K gating mechanism, ensuring that each input is distributed to the most relevant subset of experts while maintaining diversity through the addition of noise."}
{"task_id": 125, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing. The goal is to compute the final MoE output by selecting the top-k experts per token, applying their transformations, and aggregating the results weighted by the normalized gating probabilities.\n\n### Approach\n1. **Compute Gating Logits**: The gating logits are computed by multiplying the input tensor with the gating weight matrix. This gives a tensor where each element represents the logit for an expert.\n2. **Compute Softmax Probabilities**: Convert the gating logits into probabilities using the softmax function. This ensures that the probabilities sum to 1 for each token.\n3. **Select Top-k Experts**: For each token, determine the indices of the top-k experts based on the computed probabilities.\n4. **Compute Expert Outputs**: Each expert applies its transformation to the input tensor, resulting in a tensor of expert outputs.\n5. **Aggregate Results**: For each token, aggregate the outputs of the top-k experts, weighted by their normalized probabilities. The normalization ensures that the weights sum to 1 for each token.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute the gating logits\n    g = x.dot(Wg)\n    \n    # Compute the softmax probabilities\n    exp_g = np.exp(g - np.max(g, axis=1, keepdims=True))\n    p = exp_g / np.sum(exp_g, axis=1, keepdims=True)\n    \n    # Get the indices of the top-k experts for each token\n    indices = np.argsort(-p, axis=1)[:, :top_k]\n    \n    # Compute the expert outputs\n    E = x.dot(We)  # Shape: (batch_size, n_experts, d_output)\n    \n    # Select the top-k expert outputs and their corresponding probabilities\n    batch_size = x.shape[0]\n    selected_E = E[np.arange(batch_size)[:, None], indices]\n    selected_p = p[np.arange(batch_size)[:, None], indices]\n    \n    # Normalize the selected probabilities\n    sum_p = selected_p.sum(axis=1, keepdims=True)\n    normalized_p = selected_p / sum_p\n    \n    # Reshape the normalized probabilities for broadcasting\n    normalized_p = normalized_p[:, :, None]\n    \n    # Compute the weighted sum of the selected expert outputs\n    output = (selected_E * normalized_p).sum(axis=1)\n    \n    # Round the output to 4 decimal places and convert to a list\n    output = np.round(output, 4).tolist()\n    \n    return output\n```\n\n### Explanation\n1. **Gating Logits and Probabilities**: The gating logits are computed using matrix multiplication of the input tensor with the gating weight matrix. These logits are then converted into probabilities using the softmax function, ensuring each token's probabilities sum to 1.\n2. **Top-k Selection**: For each token, the top-k experts are selected based on their probabilities. This is done by sorting the probabilities in descending order and taking the top-k indices.\n3. **Expert Outputs**: Each expert applies its transformation to the input tensor, resulting in a tensor where each element represents the output of an expert for a token.\n4. **Aggregation**: The outputs of the top-k experts are aggregated by weighting each expert's output with its normalized probability. The normalization ensures that the weights sum to 1 for each token, providing a proper weighted sum.\n\nThis approach efficiently computes the MoE output by leveraging matrix operations and advanced indexing, ensuring both correctness and performance."}
{"task_id": 126, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement Group Normalization on a 4D input tensor. Group Normalization is a technique that normalizes the input tensor by dividing the channels into smaller groups and normalizing each group separately. This helps in stabilizing the training of deep neural networks by normalizing the activations.\n\n### Approach\nThe approach to implement Group Normalization involves the following steps:\n\n1. **Reshape the Input Tensor**: The input tensor is reshaped to split the channels into the specified number of groups. This allows us to process each group independently.\n2. **Compute Mean and Variance**: For each group, compute the mean and variance across the spatial dimensions (height and width) and the group's channels.\n3. **Normalize Each Group**: Normalize each group by subtracting the computed mean and dividing by the square root of the computed variance, adding a small epsilon to avoid division by zero.\n4. **Reshape Back**: After normalization, reshape the tensor back to its original dimensions.\n5. **Apply Scale and Shift**: Multiply the normalized tensor by a learned scale (gamma) and add a learned shift (beta) to each channel.\n6. **Round and Convert**: Round the results to four decimal places and convert the output to a Python list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    G = C // num_groups\n    X_grouped = X.reshape(B, num_groups, G, H, W)\n    \n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    \n    X_grouped = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_grouped.reshape(B, C, H, W)\n    \n    out = gamma * X_normalized + beta\n    out = np.round(out, decimals=4)\n    return out.tolist()\n```\n\n### Explanation\n1. **Reshape the Input Tensor**: The input tensor `X` is reshaped from `(B, C, H, W)` to `(B, num_groups, C//num_groups, H, W)`. This allows us to process each group of channels separately.\n2. **Compute Mean and Variance**: For each group, the mean and variance are computed across the spatial dimensions (height and width) and the group's channels. This is done using `np.mean` and `np.var` with the appropriate axes.\n3. **Normalize Each Group**: Each element in the group is normalized by subtracting the mean and dividing by the square root of the variance, with a small epsilon added to the variance to prevent division by zero.\n4. **Reshape Back**: The normalized tensor is reshaped back to its original dimensions `(B, C, H, W)`.\n5. **Apply Scale and Shift**: The normalized tensor is scaled by the gamma array and shifted by the beta array. These operations are applied per channel.\n6. **Round and Convert**: The results are rounded to four decimal places and converted to a Python list using `tolist()`.\n\nThis approach ensures that each group of channels is normalized independently, which helps in stabilizing the training process and improving the performance of the model."}
{"task_id": 127, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to find the minimum value of the function \\( f(x) = x^4 - 3x^3 + 2 \\) using a gradient descent approach. The goal is to determine the value of \\( x \\) where the function reaches its minimum, starting from a given initial position.\n\n### Approach\nThe function \\( f(x) = x^4 - 3x^3 + 2 \\) is a quartic function, which has a single global minimum. The critical points of this function are found by setting its derivative to zero. The derivative \\( f'(x) = 4x^3 - 9x^2 \\) gives us the critical points at \\( x = 0 \\) and \\( x = 2.25 \\). By analyzing the second derivative, we determine that \\( x = 2.25 \\) is the point of global minimum.\n\nTo find this minimum using gradient descent, we start from an initial position and iteratively update the position using the gradient (derivative) of the function. The update rule is \\( x_{\\text{new}} = x - \\text{learning\\_rate} \\times \\text{gradient} \\). The algorithm stops when the change in \\( x \\) is below a specified tolerance or the maximum number of iterations is reached.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x**3 - 9 * x**2\n        if abs(gradient) < tolerance:\n            break\n        x_new = x - learning_rate * gradient\n        change = abs(x_new - x)\n        if change < tolerance:\n            x = x_new\n            break\n        x = x_new\n    return round(x, 4)\n```\n\n### Explanation\n1. **Initialization**: Start from the given initial position `start_x`.\n2. **Gradient Calculation**: Compute the gradient (derivative) of the function at the current position.\n3. **Stopping Condition**: Check if the absolute value of the gradient is below the tolerance. If it is, break the loop as we are near a minimum.\n4. **Update Position**: Calculate the new position using the learning rate and the gradient.\n5. **Change Check**: If the change in position is below the tolerance, break the loop to avoid unnecessary iterations.\n6. **Iteration**: Continue updating the position until the stopping conditions are met or the maximum number of iterations is reached.\n7. **Return Result**: Return the position rounded to four decimal places.\n\nThis approach ensures that we efficiently find the minimum of the function using gradient descent, considering both the gradient magnitude and the change in position to determine convergence."}
{"task_id": 128, "completion_id": 0, "completion": "\n\nTo solve this problem, we need to implement the Dynamic Tanh (DyT) function, which is a normalization-free activation function inspired by the Tanh function. This function is designed to replace layer normalization in Transformer architectures while maintaining the squashing behavior of the Tanh function and enabling stable training.\n\n### Approach\nThe Dynamic Tanh function is defined as follows:\n1. Scale the input array `x` by a factor `alpha`.\n2. Apply the hyperbolic tangent (tanh) function to the scaled input.\n3. Scale the result by a factor `gamma`.\n4. Shift the scaled result by a factor `beta`.\n\nThe formula for the DyT function is:\n\\[ \\text{DyT}(x) = \\gamma \\cdot \\tanh(\\alpha \\cdot x) + \\beta \\]\n\nThe steps to implement this function are:\n1. Multiply each element of the input array `x` by `alpha`.\n2. Apply the tanh function to each element of the resulting array.\n3. Multiply each element of the resulting array by `gamma`.\n4. Add `beta` to each element of the resulting array.\n5. Round each element of the resulting array to four decimal places.\n6. Convert the resulting numpy array to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    scaled_x = alpha * x\n    tanh_x = np.tanh(scaled_x)\n    result = gamma * tanh_x + beta\n    rounded_result = np.round(result, 4)\n    return rounded_result.tolist()\n```\n\n### Explanation\n1. **Scaling the Input**: The input array `x` is scaled by `alpha` to adjust the slope of the tanh function.\n2. **Applying Tanh**: The tanh function is applied to the scaled input to introduce non-linearity and squash the values between -1 and 1.\n3. **Scaling the Output**: The result of the tanh function is scaled by `gamma` to adjust the amplitude of the output.\n4. **Shifting the Output**: The scaled result is shifted by `beta` to adjust the bias of the output.\n5. **Rounding**: Each element of the resulting array is rounded to four decimal places to meet the output constraints.\n6. **Conversion to List**: The resulting numpy array is converted to a list using the `tolist()` method and returned.\n\nThis approach ensures that the Dynamic Tanh function maintains the properties of the tanh function while providing the flexibility to adjust its behavior through the parameters `alpha`, `gamma`, and `beta`."}
