{"task_id": 1, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\nfrom typing import List, Union\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the dot product of a matrix 'a' and a vector 'b'.\n\n    The dot product is defined only if the number of columns in the matrix 'a'\n    is equal to the number of elements (length) in the vector 'b'.\n    If 'a' is an n x m matrix and 'b' is a vector of length m, the result\n    is a vector 'c' of length n, where each element c[i] is the dot product\n    of the i-th row of 'a' with the vector 'b'.\n\n    Args:\n        a: A list of lists representing the matrix (n rows, m columns).\n           Elements can be integers or floats.\n        b: A list representing the vector (length m).\n           Elements can be integers or floats.\n\n    Returns:\n        A list representing the resulting vector (length n) containing integers\n        or floats, if the operation is valid.\n        Returns the integer -1 if the number of columns in 'a' does not match\n        the length of 'b', or if 'a' is not a valid rectangular matrix.\n\n    Examples:\n        >>> matrix_dot_vector([[1, 2], [3, 4]], [5, 6])\n        [17, 39]  # (1*5 + 2*6), (3*5 + 4*6)\n\n        >>> matrix_dot_vector([[1, 2, 3], [4, 5, 6]], [7, 8])\n        -1  # Matrix columns (3) != vector length (2)\n\n        >>> matrix_dot_vector([[1, 2], [3, 4, 5]], [6, 7])\n        -1 # Matrix is not rectangular\n\n        >>> matrix_dot_vector([], [1, 2])\n        -1 # Empty matrix (0 rows) incompatible with non-empty vector\n\n        >>> matrix_dot_vector([], [])\n        [] # Empty matrix (0x0) compatible with empty vector (len 0) -> empty result vector\n\n        >>> matrix_dot_vector([[]], [])\n        [0] # 1x0 matrix compatible with empty vector (len 0) -> result vector [0]\n\n        >>> matrix_dot_vector([[], []], [])\n        [0, 0] # 2x0 matrix compatible with empty vector (len 0) -> result vector [0, 0]\n    \"\"\"\n    if not a:\n        return [] if not b else -1\n    try:\n        num_cols_expected = len(a[0])\n    except IndexError:\n        num_cols_expected = 0\n    if len(b) != num_cols_expected:\n        return -1\n    result_vector = []\n    for (i, row) in enumerate(a):\n        if len(row) != num_cols_expected:\n            return -1\n        dot_product_sum = 0\n        for j in range(num_cols_expected):\n            dot_product_sum += row[j] * b[j]\n        result_vector.append(dot_product_sum)\n    return result_vector\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]"}
{"task_id": 2, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections.abc\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    The transpose of a matrix swaps its rows and columns. The element\n    at row i, column j in the original matrix becomes the element at\n    row j, column i in the transposed matrix.\n\n    Args:\n        a: The input matrix, represented as a list of lists of integers\n           or floats. It's assumed to be rectangular (all rows have the\n           same length), although the implementation using zip might\n           handle jagged arrays by truncating to the shortest row length\n           when forming columns.\n\n    Returns:\n        A new list of lists representing the transposed matrix. Returns an\n        empty list if the input matrix is empty or effectively has zero columns\n        (e.g., [[], []]).\n\n    Raises:\n        TypeError: If the input is not a list of lists.\n        IndexError: Potentially if the input is severely malformed (e.g., not lists inside).\n\n    Examples:\n        >>> transpose_matrix([[1, 2, 3], [4, 5, 6]])\n        [[1, 4], [2, 5], [3, 6]]\n\n        >>> transpose_matrix([[1], [2], [3]])\n        [[1, 2, 3]]\n\n        >>> transpose_matrix([[1, 2]])\n        [[1], [2]]\n\n        >>> transpose_matrix([[5]])\n        [[5]]\n\n        >>> transpose_matrix([])\n        []\n\n        >>> transpose_matrix([[], []]) # Matrix with 2 rows, 0 columns\n        []\n\n    Note:\n        This implementation uses the built-in `zip` function, which is\n        an efficient and Pythonic way to achieve matrix transposition.\n        `zip(*a)` unpacks the rows of matrix `a` and then `zip` aggregates\n        elements at the same index from each row, effectively creating tuples\n        representing the columns. These tuples are then converted back to lists.\n    \"\"\"\n    if not isinstance(a, collections.abc.Sequence):\n        raise TypeError('Input must be a list of lists.')\n    if not a:\n        return []\n    try:\n        transposed_rows_as_tuples = zip(*a)\n        transposed_matrix = [list(row) for row in transposed_rows_as_tuples]\n        return transposed_matrix\n    except TypeError as e:\n        print(f'Error during transposition, ensure input is a list of lists: {e}')\n        raise TypeError('Input must be a list of lists with numeric elements.') from e\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]"}
{"task_id": 3, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Tuple, Union\ndef reshape_matrix(a: List[List[Union[int, float]]], new_shape: Tuple[int, int]) -> List[List[Union[int, float]]]:\n    \"\"\"\n    Reshapes a given matrix (list of lists) into a specified shape.\n\n    Args:\n        a: The input matrix represented as a list of lists of integers or floats.\n        new_shape: A tuple of two integers (rows, columns) representing the desired shape.\n\n    Returns:\n        The reshaped matrix as a list of lists if reshaping is possible,\n        otherwise returns an empty list [].\n    \"\"\"\n    try:\n        np_array = np.array(a)\n        reshaped_array = np_array.reshape(new_shape)\n        result_list = reshaped_array.tolist()\n        return result_list\n    except ValueError:\n        return []\n    except Exception as e:\n        return []\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]"}
{"task_id": 4, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column.\n\n    Args:\n        matrix: A list of lists representing the matrix. All inner lists (rows)\n                should contain floats or numbers convertible to floats.\n                For column mode, the matrix must be rectangular.\n        mode: A string, either 'row' or 'column', specifying the calculation mode.\n\n    Returns:\n        A list of floats representing the means calculated according to the mode.\n        - If mode is 'row', returns a list of row means.\n        - If mode is 'column', returns a list of column means.\n\n    Raises:\n        ValueError: If the mode is invalid ('row' or 'column' expected).\n        ValueError: If attempting to calculate the mean of an empty row (in 'row' mode).\n        ValueError: If the matrix is non-rectangular when mode is 'column'.\n        ValueError: If the matrix has inconsistent row lengths (e.g., some empty, some not)\n                    when mode is 'column'.\n    \"\"\"\n    if not matrix:\n        return []\n    num_rows = len(matrix)\n    if mode == 'row':\n        means = []\n        for (i, row) in enumerate(matrix):\n            if not row:\n                raise ValueError(f'Row at index {i} is empty, cannot calculate mean.')\n            mean = sum(row) / len(row)\n            means.append(mean)\n        return means\n    elif mode == 'column':\n        if num_rows == 0:\n            return []\n        try:\n            num_cols = len(matrix[0])\n        except IndexError:\n            raise RuntimeError('Internal logic error: Matrix structure issue.')\n        if num_cols == 0:\n            if any((len(row) != 0 for row in matrix)):\n                raise ValueError('Matrix contains rows of different lengths (some empty, some not), cannot calculate column means.')\n            else:\n                return []\n        is_rectangular = all((len(row) == num_cols for row in matrix))\n        if not is_rectangular:\n            raise ValueError('Matrix must be rectangular for column mean calculation.')\n        column_means = []\n        for col_tuple in zip(*matrix):\n            mean = sum(col_tuple) / num_rows\n            column_means.append(mean)\n        return column_means\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'.\")\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]"}
{"task_id": 5, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport copy\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n  Multiplies each element of a matrix by a scalar value.\n\n  Args:\n    matrix: A list of lists representing the matrix, where each inner list\n            is a row and contains numbers (int or float).\n    scalar: The number (int or float) to multiply each element of the matrix by.\n\n  Returns:\n    A new list of lists representing the resulting matrix after scalar\n    multiplication. Returns an empty list if the input matrix is empty.\n\n  Raises:\n    TypeError: If the matrix elements or the scalar are not numbers.\n               (Note: Python's multiplication will naturally raise this).\n    IndexError: If the input matrix is malformed (e.g., not all rows are lists),\n                though type hinting aims to prevent this.\n  \"\"\"\n    if not matrix:\n        return []\n    result_matrix = []\n    for row in matrix:\n        new_row = []\n        for element in row:\n            new_row.append(element * scalar)\n        result_matrix.append(new_row)\n    return result_matrix\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]"}
{"task_id": 6, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\nfrom typing import List, Union\ndef calculate_eigenvalues(matrix: List[List[Union[float, int]]]) -> List[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix.\n\n    For a matrix [[a, b], [c, d]], the eigenvalues lambda satisfy the\n    characteristic equation: det(A - lambda*I) = 0, where I is the\n    identity matrix.\n    This expands to: (a - lambda)(d - lambda) - bc = 0\n    Which simplifies to the quadratic equation:\n    lambda^2 - (a + d) * lambda + (ad - bc) = 0\n    lambda^2 - trace(A) * lambda + det(A) = 0\n\n    The solutions are given by the quadratic formula:\n    lambda = [ trace(A) +/- sqrt(trace(A)^2 - 4*det(A)) ] / 2\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists\n                (e.g., [[a, b], [c, d]]). Elements can be int or float.\n\n    Returns:\n        A list containing the two eigenvalues (as floats),\n        sorted in descending order.\n\n    Raises:\n        ValueError: If the input is not a 2x2 matrix.\n        ValueError: If the matrix leads to complex eigenvalues (discriminant < 0),\n                    as the function is specified to return only real floats.\n        TypeError: If matrix elements are not numbers (int or float).\n                   This might also be caught implicitly during calculations.\n    \"\"\"\n    if not isinstance(matrix, list) or len(matrix) != 2:\n        raise ValueError('Input must be a list of 2 rows.')\n    if not all((isinstance(row, list) and len(row) == 2 for row in matrix)):\n        raise ValueError('Each row in the matrix must be a list of 2 columns.')\n    for row in matrix:\n        for element in row:\n            if not isinstance(element, (int, float)):\n                raise TypeError(f'Matrix elements must be int or float, found {type(element)}')\n    a = float(matrix[0][0])\n    b = float(matrix[0][1])\n    c = float(matrix[1][0])\n    d = float(matrix[1][1])\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    if discriminant < 0:\n        raise ValueError('Matrix has complex eigenvalues, which cannot be represented as list[float].')\n    sqrt_discriminant = math.sqrt(discriminant)\n    lambda1 = (trace + sqrt_discriminant) / 2.0\n    lambda2 = (trace - sqrt_discriminant) / 2.0\n    eigenvalues = sorted([lambda1, lambda2], reverse=True)\n    return eigenvalues\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]"}
{"task_id": 7, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Union\ndef transform_matrix(A: List[List[Union[int, float]]], T: List[List[Union[int, float]]], S: List[List[Union[int, float]]]) -> Union[List[List[float]], int]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^{-1}AS, where T and S \n    are invertible matrices.\n\n    Args:\n        A: The matrix to be transformed (as a list of lists).\n        T: The left transformation matrix (as a list of lists). Must be square and invertible.\n        S: The right transformation matrix (as a list of lists). Must be square and invertible.\n\n    Returns:\n        The transformed matrix T^{-1}AS rounded to 4 decimal places (as a list of lists),\n        or -1 if T or S is not invertible, not square, or if matrix dimensions \n        are incompatible for multiplication.\n    \"\"\"\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.ndim != 2 or S_np.ndim != 2 or A_np.ndim != 2:\n            return -1\n        if T_np.shape[0] != T_np.shape[1]:\n            return -1\n        if S_np.shape[0] != S_np.shape[1]:\n            return -1\n        try:\n            T_inv_np = np.linalg.inv(T_np)\n        except np.linalg.LinAlgError:\n            return -1\n        det_S = np.linalg.det(S_np)\n        if np.isclose(det_S, 0.0):\n            return -1\n        if T_inv_np.shape[1] != A_np.shape[0]:\n            return -1\n        if A_np.shape[1] != S_np.shape[0]:\n            return -1\n        transformed_matrix_np = T_inv_np @ A_np @ S_np\n        rounded_matrix_np = np.round(transformed_matrix_np, 4)\n        result_list = rounded_matrix_np.tolist()\n        return result_list\n    except ValueError as ve:\n        return -1\n    except Exception as e:\n        return -1\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1"}
{"task_id": 8, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "\nimport math\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n  Calculates the inverse of a 2x2 matrix.\n\n  Args:\n    matrix: A 2x2 matrix represented as a list of lists of floats.\n            Example: [[a, b], [c, d]]\n\n  Returns:\n    The inverse matrix as a list of lists of floats if the matrix is invertible.\n    Returns None if the matrix is singular (not invertible).\n  \"\"\"\n    if not (len(matrix) == 2 and len(matrix[0]) == 2 and (len(matrix[1]) == 2)):\n        raise ValueError('Input must be a 2x2 matrix.')\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    scalar_factor = 1.0 / determinant\n    inverse_matrix = [[scalar_factor * d, scalar_factor * -b], [scalar_factor * -c, scalar_factor * a]]\n    return inverse_matrix\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]"}
{"task_id": 9, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections.abc\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n\n    Args:\n        a: The first matrix (list of lists of numbers).\n        b: The second matrix (list of lists of numbers).\n\n    Returns:\n        The resulting matrix as a list of lists of numbers (floats are used\n        internally for calculation to handle potential float results),\n        or -1 if the matrices cannot be multiplied due to incompatible\n        dimensions or invalid input format (e.g., not rectangular,\n        empty matrices where dimensions cannot be determined appropriately\n        for the multiplication rule).\n    \"\"\"\n    if not isinstance(a, collections.abc.Sequence) or not isinstance(b, collections.abc.Sequence):\n        return -1\n    if not a or not b:\n        rows_a = len(a)\n        cols_a = len(a[0]) if rows_a > 0 and isinstance(a[0], collections.abc.Sequence) else 0\n        rows_b = len(b)\n        cols_b = len(b[0]) if rows_b > 0 and isinstance(b[0], collections.abc.Sequence) else 0\n        if cols_a == 0 and rows_b == 0:\n            if rows_b > 0 and (not b[0]):\n                cols_b = 0\n            elif rows_b == 0 and cols_a == 0:\n                if rows_b == 0 and cols_a == 0:\n                    cols_b = 0\n                else:\n                    return -1\n            if rows_a > 0 and any((not isinstance(row, collections.abc.Sequence) or len(row) != cols_a for row in a)):\n                return -1\n            if rows_b > 0 and any((not isinstance(row, collections.abc.Sequence) or len(row) != cols_b for row in b)):\n                return -1\n            return [[0.0 for _ in range(cols_b)] for _ in range(rows_a)]\n        else:\n            return -1\n    if not isinstance(a[0], collections.abc.Sequence) or not isinstance(b[0], collections.abc.Sequence):\n        return -1\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if any((not isinstance(row, collections.abc.Sequence) or len(row) != cols_a for row in a)):\n        return -1\n    if any((not isinstance(row, collections.abc.Sequence) or len(row) != cols_b for row in b)):\n        return -1\n    if cols_a != rows_b:\n        return -1\n    c = [[0.0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            dot_product = 0.0\n            for k in range(cols_a):\n                if not isinstance(a[i][k], (int, float)) or not isinstance(b[k][j], (int, float)):\n                    return -1\n                dot_product += a[i][k] * b[k][j]\n            c[i][j] = dot_product\n    return c\na = [[1, 2, 3], [4, 5, 6]]\nb = [[7, 8], [9, 10], [11, 12]]\nc = [[1, 2], [3, 4]]\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1"}
{"task_id": 10, "completion_id": 0, "passed": false, "result": "failed: name 'calculate_covariance_matrix' is not defined", "solution": "\n\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]"}
{"task_id": 11, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method\n    for n iterations.\n\n    Args:\n        A: The coefficient matrix (numpy ndarray, shape m x m).\n        b: The constant vector (numpy ndarray, shape m).\n        n: The number of iterations to perform.\n\n    Returns:\n        The approximate solution vector x as a Python list,\n        with elements rounded to 4 decimal places after each iteration.\n\n    Raises:\n        ValueError: If a diagonal element of A is zero.\n        ValueError: If A is not square or dimensions of A and b mismatch.\n        ValueError: If n is negative.\n    \"\"\"\n    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square (2D).')\n    if b.ndim != 1:\n        raise ValueError('Vector b must be 1D.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(f'Dimension mismatch: A has shape {A.shape} but b has shape {b.shape}.')\n    if n < 0:\n        raise ValueError('Number of iterations n cannot be negative.')\n    m = A.shape[0]\n    diag_A = np.diag(A)\n    if np.any(diag_A == 0):\n        raise ValueError('Diagonal element found to be zero. Jacobi method requires non-zero diagonal elements.')\n    x = np.zeros(m)\n    for k in range(n):\n        x_new = np.zeros(m)\n        for i in range(m):\n            sigma = 0.0\n            for j in range(m):\n                if i != j:\n                    sigma += A[i, j] * x[j]\n            x_new[i] = (b[i] - sigma) / A[i, i]\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()\nA = np.array([[10.0, 1.0, 1.0], [1.0, 10.0, 1.0], [1.0, 1.0, 10.0]])\nb = np.array([12.0, 12.0, 12.0])\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]"}
{"task_id": 12, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nimport math\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (Singular Values only)\n    for a 2x2 matrix using the relationship with eigenvalues of A^T*A.\n\n    This approach calculates the eigenvalues of the symmetric matrix S = A^T*A\n    directly via the characteristic equation. For a 2x2 matrix, this gives\n    the same result as applying a single Jacobi rotation to diagonalize S.\n    The singular values of A are the square roots of the eigenvalues of S.\n\n    Args:\n        A: A 2x2 numpy array.\n\n    Returns:\n        A tuple containing the two singular values (sigma1, sigma2),\n        sorted in descending order and rounded to 4 decimal places.\n\n    Raises:\n        ValueError: If the input is not a 2x2 numpy array.\n    \"\"\"\n    if not isinstance(A, np.ndarray) or A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 numpy array.')\n    S = A.T @ A\n    s11 = S[0, 0]\n    s12 = S[0, 1]\n    s22 = S[1, 1]\n    trace_S = s11 + s22\n    det_S = s11 * s22 - s12 * s12\n    discriminant = trace_S ** 2 - 4 * det_S\n    if discriminant < 0:\n        if np.isclose(discriminant, 0):\n            discriminant = 0.0\n        else:\n            raise ValueError(f'Negative discriminant ({discriminant}) encountered. Check matrix properties.')\n    sqrt_discriminant = math.sqrt(discriminant)\n    lambda1 = (trace_S + sqrt_discriminant) / 2\n    lambda2 = (trace_S - sqrt_discriminant) / 2\n    lambda1 = max(0.0, lambda1)\n    lambda2 = max(0.0, lambda2)\n    sigma1 = math.sqrt(lambda1)\n    sigma2 = math.sqrt(lambda2)\n    singular_values = sorted([sigma1, sigma2], reverse=True)\n    rounded_singular_values = (round(singular_values[0], 4), round(singular_values[1], 4))\n    return rounded_singular_values\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])"}
{"task_id": 13, "completion_id": 0, "passed": false, "result": "failed: name 'Matrix' is not defined", "solution": "\ndef _determinant_3x3(matrix: Matrix) -> float:\n    \"\"\"\n    Calculates the determinant of a 3x3 matrix using Laplace's expansion.\n\n    Args:\n        matrix: A 3x3 matrix (list of lists).\n\n    Returns:\n        The determinant of the 3x3 matrix.\n\n    Raises:\n        ValueError: If the input is not a 3x3 matrix.\n    \"\"\"\n    if len(matrix) != 3 or not all((len(row) == 3 for row in matrix)):\n        raise ValueError('Input must be a 3x3 matrix for _determinant_3x3')\n    determinant = 0.0\n    for j in range(3):\n        minor_matrix = _get_minor(matrix, 0, j)\n        sign = (-1) ** j\n        determinant += sign * matrix[0][j] * _determinant_2x2(minor_matrix)\n    return determinant\ndef determinant_4x4(matrix: Matrix) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n\n    The function recursively calculates the determinants of 3x3 minor matrices.\n\n    Args:\n        matrix: A 4x4 matrix represented as a list of lists.\n                Elements can be integers or floating-point numbers.\n\n    Returns:\n        The determinant of the 4x4 matrix as a float.\n\n    Raises:\n        ValueError: If the input is not a 4x4 matrix.\n    \"\"\"\n    if len(matrix) != 4:\n        raise ValueError('Input matrix must have 4 rows.')\n    if not all((len(row) == 4 for row in matrix)):\n        raise ValueError('Each row in the input matrix must have 4 columns.')\n    if not all((isinstance(el, (int, float)) for row in matrix for el in row)):\n        raise ValueError('All matrix elements must be integers or floats.')\n    determinant = 0.0\n    for j in range(4):\n        minor_matrix = _get_minor(matrix, 0, j)\n        sign = (-1) ** j\n        element = matrix[0][j]\n        minor_determinant = _determinant_3x3(minor_matrix)\n        determinant += sign * element * minor_determinant\n    return determinant\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0"}
{"task_id": 14, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List\ndef linear_regression_normal_equation(X: List[List[float]], y: List[float]) -> List[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n\n    Args:\n        X: A list of lists representing the feature matrix (samples x features).\n           Does NOT include the intercept term column.\n        y: A list representing the target vector.\n\n    Returns:\n        A list containing the coefficients (including the intercept term)\n        of the linear regression model, rounded to four decimal places.\n        The first element is the intercept, followed by coefficients for each feature.\n        Returns an empty list if the input is invalid (e.g., empty X or y).\n    \"\"\"\n    if not X or not y or (not X[0]):\n        return []\n    try:\n        X_np = np.array(X, dtype=float)\n        y_np = np.array(y, dtype=float).reshape(-1, 1)\n    except ValueError:\n        print('Error: Input lists could not be converted to NumPy arrays. Check dimensions.')\n        return []\n    n_samples = X_np.shape[0]\n    n_features = X_np.shape[1]\n    if n_samples != y_np.shape[0]:\n        print(f'Error: Number of samples in X ({n_samples}) does not match number of samples in y ({y_np.shape[0]}).')\n        return []\n    if n_samples == 0:\n        return []\n    intercept_column = np.ones((n_samples, 1))\n    X_b = np.c_[intercept_column, X_np]\n    try:\n        XTX = X_b.T @ X_b\n        XTX_inv = np.linalg.inv(XTX)\n        XTY = X_b.T @ y_np\n        theta = XTX_inv @ XTY\n    except np.linalg.LinAlgError:\n        print('Warning: X^T*X matrix is singular. Using pseudo-inverse.')\n        try:\n            XTX_pinv = np.linalg.pinv(XTX)\n            XTY = X_b.T @ y_np\n            theta = XTX_pinv @ XTY\n        except np.linalg.LinAlgError:\n            print('Error: Could not compute coefficients even with pseudo-inverse.')\n            return []\n    theta_list = theta.flatten().tolist()\n    rounded_theta = [round(coef, 4) for coef in theta_list]\n    rounded_theta = [-0.0 if c == 0.0 and str(c).startswith('-') else c for c in rounded_theta]\n    return rounded_theta\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]"}
{"task_id": 15, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Performs linear regression using gradient descent.\n\n    Args:\n        X: Feature matrix (NumPy array) with shape (m, n+1), where m is the\n           number of samples and n is the number of features. The first\n           column should be ones for the intercept term.\n        y: Target vector (NumPy array) with shape (m,) or (m, 1).\n        alpha: Learning rate (float).\n        iterations: Number of iterations for gradient descent (int).\n\n    Returns:\n        A list containing the learned coefficients (including the intercept)\n        rounded to four decimal places.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    if y.ndim > 1:\n        y = y.flatten()\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = 1 / m * (X.T @ errors)\n        theta = theta - alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    result_list = theta_rounded.tolist()\n    return result_list\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]"}
{"task_id": 16, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Tuple\ndef feature_scaling(data: np.ndarray) -> Tuple[List[List[float]], List[List[float]]]:\n    \"\"\"\n    Performs feature scaling on a dataset using standardization and min-max normalization.\n\n    Args:\n        data: A 2D NumPy array where each row represents a data sample\n              and each column represents a feature.\n\n    Returns:\n        A tuple containing two lists of lists (float):\n        1. standardized_data: Data scaled by standardization (Z-score).\n        2. min_max_scaled_data: Data scaled by min-max normalization.\n        All results are rounded to 4 decimal places.\n        Returns ([], []) for empty input data.\n        Returns lists of empty lists for data with zero columns.\n\n    Raises:\n        ValueError: If the input is not a 2D NumPy array.\n        TypeError: If the input array does not contain numeric data.\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n        raise ValueError('Input must be a NumPy array.')\n    if data.ndim != 2:\n        raise ValueError('Input array must be 2-dimensional (samples x features).')\n    if not np.issubdtype(data.dtype, np.number):\n        try:\n            data = data.astype(float)\n        except ValueError:\n            raise TypeError('Input array must contain numeric data.')\n    (n_samples, n_features) = data.shape\n    if n_samples == 0:\n        return ([], [])\n    if n_features == 0:\n        return ([[] for _ in range(n_samples)], [[] for _ in range(n_samples)])\n    data = data.astype(float)\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data_np = np.zeros_like(data, dtype=float)\n    non_zero_std_mask = std_dev != 0\n    if np.any(non_zero_std_mask):\n        np.divide(data - mean, std_dev, out=standardized_data_np, where=non_zero_std_mask)\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    range_val = max_val - min_val\n    min_max_scaled_data_np = np.zeros_like(data, dtype=float)\n    non_zero_range_mask = range_val != 0\n    if np.any(non_zero_range_mask):\n        np.divide(data - min_val, range_val, out=min_max_scaled_data_np, where=non_zero_range_mask)\n    standardized_data_rounded = np.round(standardized_data_np, 4)\n    min_max_scaled_data_rounded = np.round(min_max_scaled_data_np, 4)\n    standardized_list = standardized_data_rounded.tolist()\n    min_max_list = min_max_scaled_data_rounded.tolist()\n    return (standardized_list, min_max_list)\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"}
{"task_id": 17, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Performs k-Means clustering on a set of points using a fixed number of iterations.\n\n    k-Means clustering aims to partition n observations into k clusters in which\n    each observation belongs to the cluster with the nearest mean (cluster centers\n    or cluster centroid), serving as a prototype of the cluster.\n\n    Args:\n        points: A list of points, where each point is a tuple of coordinates\n                (e.g., (x, y) for 2D points).\n        k: An integer representing the number of clusters to form.\n        initial_centroids: A list of initial centroid points, each a tuple of\n                           coordinates. The length of this list must be equal to k.\n        max_iterations: An integer representing the maximum number of iterations\n                        to perform. The algorithm will stop after this many\n                        iterations, regardless of convergence.\n\n    Returns:\n        A list of the final centroids of the clusters after max_iterations,\n        where each centroid is a tuple of coordinates rounded to the nearest\n        fourth decimal place.\n\n    Raises:\n        ValueError: If inputs are inconsistent (e.g., k != len(initial_centroids),\n                    dimension mismatch, k <= 0).\n        TypeError: If inputs are not of the expected types.\n    \"\"\"\n    if not isinstance(points, list) or not isinstance(initial_centroids, list):\n        raise TypeError('points and initial_centroids must be lists.')\n    if not all((isinstance(p, tuple) for p in points)) and points:\n        raise TypeError('Each element in points must be a tuple.')\n    if not all((isinstance(c, tuple) for c in initial_centroids)):\n        raise TypeError('Each element in initial_centroids must be a tuple.')\n    if not isinstance(k, int) or k <= 0:\n        raise ValueError('k must be a positive integer.')\n    if not isinstance(max_iterations, int) or max_iterations < 0:\n        raise ValueError('max_iterations must be a non-negative integer.')\n    if len(initial_centroids) != k:\n        raise ValueError(f'Number of initial_centroids ({len(initial_centroids)}) must equal k ({k}).')\n    if not points:\n        if initial_centroids:\n            if len(set((len(c) for c in initial_centroids))) > 1:\n                raise ValueError('Initial centroids have inconsistent dimensions.')\n            return [tuple(np.round(c, 4)) for c in initial_centroids]\n        else:\n            return []\n    try:\n        points_np = np.array(points, dtype=float)\n        centroids_np = np.array(initial_centroids, dtype=float)\n    except ValueError as e:\n        raise ValueError(f'Could not convert points or centroids to numeric arrays. Check data consistency. Original error: {e}')\n    if points_np.ndim != 2:\n        raise ValueError('points should represent a list of coordinates (2D array structure).')\n    if centroids_np.ndim != 2:\n        raise ValueError('initial_centroids should represent a list of coordinates (2D array structure).')\n    if points_np.shape[1] != centroids_np.shape[1]:\n        raise ValueError(f'Dimensions of points ({points_np.shape[1]}) and centroids ({centroids_np.shape[1]}) must match.')\n    (num_points, dimensions) = points_np.shape\n    current_centroids = centroids_np.copy()\n    for iteration in range(max_iterations):\n        distances_sq = np.sum((points_np[:, np.newaxis, :] - current_centroids[np.newaxis, :, :]) ** 2, axis=2)\n        assignments = np.argmin(distances_sq, axis=1)\n        new_centroids = np.zeros_like(current_centroids)\n        points_in_cluster_count = np.zeros(k, dtype=int)\n        for j in range(k):\n            points_in_cluster_j = points_np[assignments == j]\n            count = points_in_cluster_j.shape[0]\n            points_in_cluster_count[j] = count\n            if count > 0:\n                new_centroids[j] = np.sum(points_in_cluster_j, axis=0)\n            else:\n                new_centroids[j] = current_centroids[j]\n        non_empty_mask = points_in_cluster_count > 0\n        new_centroids[non_empty_mask] /= points_in_cluster_count[non_empty_mask][:, np.newaxis]\n        current_centroids = new_centroids\n    final_centroids_rounded = np.round(current_centroids, 4)\n    final_centroids_list = [tuple(centroid) for centroid in final_centroids_rounded]\n    return final_centroids_list\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]"}
{"task_id": 18, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Tuple\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None) -> List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Generates train/test indices for K-Fold Cross-Validation.\n\n    Divides the dataset into k consecutive folds (without shuffling by default).\n    Each fold is then used once as a validation set while the k - 1 remaining\n    folds form the training set.\n\n    Args:\n        X: Feature matrix (numpy array of shape [n_samples, n_features]).\n           Used primarily to determine the number of samples.\n        y: Target vector (numpy array of shape [n_samples]).\n           Used primarily to determine the number of samples and ensure consistency.\n        k: Number of folds (default: 5). Must be at least 2.\n        shuffle: Whether to shuffle the data before splitting into batches (default: True).\n                 Shuffling is generally recommended.\n        random_seed: Seed used by the random number generator when shuffle is True.\n                     Use an int for reproducible output across multiple function calls. (default: None)\n\n    Returns:\n        A list of tuples. Each tuple contains (train_indices, test_indices)\n        for one fold. Indices are numpy arrays referencing the original data positions\n        before shuffling (if shuffle=True).\n\n    Raises:\n        ValueError: If X and y have different number of samples (first dimension),\n                    or if k is not valid (<=1 or > n_samples).\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples != y.shape[0]:\n        raise ValueError(f'Input validation failed: X and y must have the same number of samples. Got X.shape[0]={n_samples} and y.shape[0]={y.shape[0]}')\n    if not isinstance(k, int) or k <= 1 or k > n_samples:\n        raise ValueError(f'Input validation failed: k must be an integer > 1 and <= n_samples ({n_samples}). Got k={k}')\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    fold_sizes = [fold_size + 1] * remainder + [fold_size] * (k - remainder)\n    splits = []\n    current_pos = 0\n    for i in range(k):\n        start = current_pos\n        end = current_pos + fold_sizes[i]\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        splits.append((train_indices, test_indices))\n        current_pos = end\n    return splits\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"}
{"task_id": 19, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on the input dataset from scratch.\n\n    The function standardizes the data, computes the covariance matrix,\n    finds the eigenvalues and eigenvectors, sorts them, and returns the top k\n    principal components (eigenvectors corresponding to the largest eigenvalues).\n\n    Args:\n        data: A 2D NumPy array where rows are samples and columns are features.\n              Shape (n_samples, n_features).\n        k: The number of principal components (eigenvectors) to return.\n\n    Returns:\n        A list of lists, where each inner list represents a principal component\n        (eigenvector). The components are ordered by the magnitude of their\n        corresponding eigenvalues (descending). Results are rounded to 4\n        decimal places. Returns an empty list if k is invalid, data is\n        unsuitable (e.g., not 2D, fewer than 2 samples), or eigenvalue\n        decomposition fails.\n\n    Output Constraints:\n        - Each principal component (inner list) contains float values.\n        - All float values are rounded to the nearest 4th decimal place.\n        - The outer list contains k inner lists (principal components).\n    \"\"\"\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        return []\n    (n_samples, n_features) = data.shape\n    if n_samples < 2:\n        return []\n    if not isinstance(k, int) or k <= 0 or k > n_features:\n        return []\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    std_dev_safe = np.where(std_dev == 0, 1.0, std_dev)\n    standardized_data = (data - mean) / std_dev_safe\n    cov_matrix = np.cov(standardized_data.astype(np.float64), rowvar=False)\n    cov_matrix = (cov_matrix + cov_matrix.T) / 2\n    try:\n        (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    except np.linalg.LinAlgError:\n        return []\n    eigenvalues = np.real(eigenvalues)\n    eigenvectors = np.real(eigenvectors)\n    sorted_indices = np.argsort(eigenvalues + 1e-18 * np.random.rand(len(eigenvalues)))[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    components_as_rows = principal_components.T\n    rounded_components = np.round(components_as_rows, 4)\n    output_list = rounded_components.tolist()\n    return output_list\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]"}
{"task_id": 20, "completion_id": 0, "passed": false, "result": "failed: name 'List' is not defined", "solution": "import math\nfrom collections import Counter\ndef choose_best_attribute(examples: List[Example], attributes: List[str], target_attr: str) -> Optional[str]:\n    \"\"\"\n    Selects the attribute with the highest information gain.\n    Information Gain(S, A) = Entropy(S) - sum_{v in Values(A)} (|Sv| / |S|) * Entropy(Sv)\n    \"\"\"\n    if not attributes:\n        return None\n    base_entropy = calculate_entropy(examples, target_attr)\n    best_info_gain = -1.0\n    best_attribute = None\n    total_examples = len(examples)\n    if total_examples == 0:\n        return None\n    for attribute in attributes:\n        attribute_values = set((example[attribute] for example in examples))\n        weighted_entropy = 0.0\n        for value in attribute_values:\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = calculate_entropy(subset, target_attr)\n            subset_weight = len(subset) / total_examples\n            weighted_entropy += subset_weight * subset_entropy\n        info_gain = base_entropy - weighted_entropy\n        if info_gain >= best_info_gain:\n            best_info_gain = info_gain\n            best_attribute = attribute\n    return best_attribute\ndef learn_decision_tree(examples: List[Example], attributes: List[str], target_attr: str, parent_majority_class: Optional[Any]=None) -> DecisionTree:\n    \"\"\"\n    Recursively builds a decision tree using the ID3 algorithm (entropy and information gain).\n\n    Args:\n        examples: A list of training examples, where each example is a dictionary\n                  of attribute-value pairs.\n        attributes: A list of attribute names that can be used for splitting.\n        target_attr: The name of the target attribute (class label).\n        parent_majority_class: The majority class of the parent node's examples,\n                               used as a default if the current examples list is empty.\n\n    Returns:\n        A nested dictionary representing the decision tree. Internal nodes are\n        dictionaries keyed by the splitting attribute, with values being sub-dictionaries\n        mapping attribute values to subtrees. Leaf nodes are the predicted class labels.\n    \"\"\"\n    current_majority_class = get_majority_class(examples, target_attr, parent_majority_class)\n    if not examples:\n        return parent_majority_class\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return current_majority_class\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    if best_attr is None:\n        return current_majority_class\n    tree: DecisionTree = {best_attr: {}}\n    attribute_values = sorted(list(set((example[best_attr] for example in examples))))\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for value in attribute_values:\n        subset_examples = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(subset_examples, remaining_attributes, target_attr, current_majority_class)\n        tree[best_attr][value] = subtree\n    return tree\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"}
{"task_id": 21, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic Pegasos algorithm for kernel SVM classification.\n\n    Args:\n        data (np.ndarray): 2D NumPy array of shape (n_samples, n_features)\n                           containing the training data.\n        labels (np.ndarray): 1D NumPy array of shape (n_samples,) containing\n                             binary labels (+1 or -1).\n        kernel (str): Type of kernel to use ('linear' or 'rbf'). Default is 'linear'.\n        lambda_val (float): Regularization parameter. Default is 0.01.\n        iterations (int): Number of training iterations. Default is 100.\n        sigma (float): Parameter for the RBF kernel (if used). Default is 1.0.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The learned alpha coefficients, rounded to 4 decimals.\n            - float: The learned bias term, rounded to 4 decimals.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    if not np.all(np.isin(labels, [1, -1])):\n        raise ValueError('Labels must be +1 or -1 for binary classification.')\n    if lambda_val <= 0:\n        raise ValueError('lambda_val must be positive.')\n    alpha = np.zeros(n_samples)\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        if sigma <= 0:\n            raise ValueError('sigma must be positive for RBF kernel.')\n        gamma = 1.0 / (2 * sigma ** 2)\n        sq_norms = np.sum(data ** 2, axis=1)\n        dot_prods = data @ data.T\n        sq_dists = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * dot_prods\n        sq_dists = np.maximum(sq_dists, 0)\n        K = np.exp(-gamma * sq_dists)\n    else:\n        raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction_score = np.sum(alpha * labels * K[:, i])\n            if labels[i] * prediction_score < 1:\n                alpha[i] = alpha[i] + eta_t\n    support_vector_indices = np.where(alpha > 1e-05)[0]\n    if len(support_vector_indices) > 0:\n        sv_predictions = K[support_vector_indices, :] @ (alpha * labels)\n        biases = labels[support_vector_indices] - sv_predictions\n        bias = np.mean(biases)\n    else:\n        bias = 0.0\n    alpha_rounded = np.round(alpha, 4)\n    bias_rounded = np.round(bias, 4)\n    return (alpha_rounded.tolist(), float(bias_rounded))\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([-100.0, 0.0, 100.0, 90.6128], -102.8081)\nassert pegasos_kernel_svm(np.array([[2, 1], [3, 2], [1, 3], [1, 4]]), np.array([-1, -1, 1, 1]), kernel='linear', lambda_val=0.01, iterations=100) == ([-100.0, -100.0, 0.0, 0.0], -1037.4755)"}
{"task_id": 22, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n  Computes the sigmoid activation function for a given input z.\n\n  The sigmoid function is defined as: sigma(z) = 1 / (1 + e^(-z))\n\n  Args:\n    z: The input value (a float).\n\n  Returns:\n    The computed sigmoid value, rounded to four decimal places.\n  \"\"\"\n    sigmoid_output = 1 / (1 + math.exp(-z))\n    rounded_output = round(sigmoid_output, 4)\n    return rounded_output\nassert sigmoid(0) == 0.5\nassert sigmoid(1) == 0.7311\nassert sigmoid(-1) == 0.2689"}
{"task_id": 23, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    The softmax function transforms a vector of real numbers into a \n    probability distribution. Each component of the output vector is \n    between 0 and 1, and all components sum up to 1.\n\n    To improve numerical stability, the maximum score is subtracted \n    from all scores before exponentiation.\n\n    Args:\n        scores: A list of floating-point numbers (logits).\n\n    Returns:\n        A list of softmax probabilities, each rounded to four decimal places.\n        Returns an empty list if the input list is empty.\n    \"\"\"\n    if not scores:\n        return []\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    if sum_exp_scores == 0:\n        softmax_values = [0.0] * len(scores)\n    else:\n        softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]"}
{"task_id": 24, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> tuple[list[float], float]:\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Args:\n        features: A list of feature vectors. Each inner list represents the\n                  features for one example (e.g., [[feat1_ex1, feat2_ex1], [feat1_ex2, feat2_ex2]]).\n        labels: A list of true binary labels (0 or 1) corresponding to each\n                feature vector.\n        weights: A list of weights, one for each feature. The length must match\n                 the number of features in each feature vector.\n        bias: The bias term for the neuron.\n\n    Returns:\n        A tuple containing:\n        - A list of predicted probabilities (output of the sigmoid function) for\n          each input example, rounded to four decimal places.\n        - The mean squared error (MSE) between the predicted probabilities and\n          the true labels, rounded to four decimal places.\n\n    Raises:\n        ValueError: If the number of feature vectors does not match the number\n                    of labels, or if the number of features in any vector does\n                    not match the number of weights.\n    \"\"\"\n    try:\n        features_np = np.array(features, dtype=float)\n        labels_np = np.array(labels, dtype=int)\n        weights_np = np.array(weights, dtype=float)\n    except ValueError as e:\n        raise ValueError(f'Error converting inputs to NumPy arrays. Check data types and shapes. Original error: {e}')\n    num_examples = features_np.shape[0]\n    if num_examples == 0:\n        return ([], 0.0)\n    if num_examples != labels_np.shape[0]:\n        raise ValueError(f'Number of feature vectors ({num_examples}) must match number of labels ({labels_np.shape[0]}).')\n    if features_np.ndim != 2:\n        raise ValueError(f'Features must be a 2D list/array (list of lists). Got shape {features_np.shape}')\n    num_features = features_np.shape[1]\n    if num_features != weights_np.shape[0]:\n        raise ValueError(f'Number of features in each vector ({num_features}) must match number of weights ({weights_np.shape[0]}).')\n    if weights_np.ndim != 1:\n        raise ValueError(f'Weights must be a 1D list/array. Got shape {weights_np.shape}')\n    if labels_np.ndim != 1:\n        raise ValueError(f'Labels must be a 1D list/array. Got shape {labels_np.shape}')\n    z = np.dot(features_np, weights_np) + bias\n    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n    predicted_probabilities_np = sigmoid(z)\n    squared_errors = (predicted_probabilities_np - labels_np) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rounded_predictions_np = np.round(predicted_probabilities_np, 4)\n    rounded_mse = round(mean_squared_error, 4)\n    predicted_probabilities_list = rounded_predictions_np.tolist()\n    return (predicted_probabilities_list, rounded_mse)\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\nassert single_neuron_model([[2, 3], [3, 1], [1, 2]], [1, 0, 1], [0.5, -0.2], 1) == ([0.8022, 0.9089, 0.7503], 0.3092)"}
{"task_id": 25, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid activation function element-wise.\n\n    Args:\n        x: Input numpy array (weighted sum z).\n\n    Returns:\n        Numpy array after applying sigmoid.\n    \"\"\"\n    x = np.clip(x, -500, 500)\n    return 1 / (1 + np.exp(-x))\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> tuple[list[float], float, list[float]]:\n    \"\"\"\n    Simulates and trains a single neuron with sigmoid activation using backpropagation.\n\n    Args:\n        features: A numpy array of shape (n_samples, n_features) containing the input data.\n        labels: A numpy array of shape (n_samples,) containing the true binary labels (0 or 1).\n        initial_weights: A numpy array of shape (n_features,) representing the initial weights.\n        initial_bias: A float representing the initial bias.\n        learning_rate: The step size for gradient descent.\n        epochs: The number of training iterations over the entire dataset.\n\n    Returns:\n        A tuple containing:\n        - updated_weights: The final weights after training, rounded to 4 decimal places (as a Python list).\n        - updated_bias: The final bias after training, rounded to 4 decimal places.\n        - mse_history: A list of MSE values for each epoch, rounded to 4 decimal places.\n    \"\"\"\n    features = np.asarray(features)\n    labels = np.asarray(labels)\n    weights = np.asarray(initial_weights).astype(float).copy()\n    bias = float(initial_bias)\n    n_samples = features.shape[0]\n    if n_samples == 0:\n        return (np.round(weights, 4).tolist(), round(bias, 4), [])\n    if features.ndim == 1:\n        features = features.reshape(-1, 1)\n    if weights.shape[0] != features.shape[1]:\n        raise ValueError(f'Shape mismatch: initial_weights has shape {weights.shape} but features has {features.shape[1]} features.')\n    if labels.shape[0] != n_samples:\n        raise ValueError(f'Shape mismatch: features has {n_samples} samples but labels has {labels.shape[0]} samples.')\n    mse_history = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        a = sigmoid(z)\n        error = a - labels\n        mse = np.mean(error ** 2)\n        mse_history.append(round(mse, 4))\n        dL_da = 2 * error\n        da_dz = a * (1 - a)\n        dL_dz = dL_da * da_dz\n        grad_weights = 1 / n_samples * np.dot(features.T, dL_dz)\n        grad_bias = np.mean(dL_dz)\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n    final_weights = np.round(weights, 4)\n    final_bias = round(bias, 4)\n    return (final_weights.tolist(), final_bias, mse_history)\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\nassert train_neuron(np.array([[1, 3], [2, 1], [-1, -3]]), np.array([1, 0, 0]), np.array([-0.1, -0.2]), 0.0, 0.1, 2) == ([-0.087, -0.0951], -0.0131, [0.3513, 0.3227])"}
{"task_id": 26, "completion_id": 0, "passed": false, "result": "failed: name 'Value' is not defined", "solution": "\n\na = Value(2)\nb = Value(3)\nc = Value(10)\nd = a + b * c \ne = Value(7) * Value(2)\nf = e + d\ng = f.relu() \ng.backward()\n\nassert a.data, a.grad == (2, 1)\nassert b.data, b.grad == (3, 10)\nassert c.data, c.grad == (10, 3)\nassert d.data, d.grad == (32, 1)\nassert e.data, e.grad == (14, 1)\nassert f.data, f.grad == (46, 1)\nassert g.data, g.grad == (46, 1)\na = Value(3)\nb = Value(4)\nc = Value(2)\nd = a * b + c \nd.backward()\n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (2, 1)\nassert d.data, d.grad == (14, 1)\na = Value(3)\nb = Value(4)\nc = Value(5)\nd = b * c \ne = a + d * b\ne.backward() \n\nassert a.data, a.grad == (3, 1)\nassert b.data, b.grad == (4, 1)\nassert c.data, c.grad == (5, 1)\nassert d.data, d.grad == (20, 1)\nassert e.data, e.grad == (83, 1)"}
{"task_id": 27, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n  Computes the transformation matrix P from basis B to C for R^3.\n\n  The transformation matrix P satisfies the equation [v]_C = P * [v]_B,\n  where [v]_B are the coordinates of a vector v in basis B, and\n  [v]_C are the coordinates of the same vector v in basis C.\n\n  The formula for P is P = M_C^{-1} * M_B, where M_B is the matrix whose\n  columns are the vectors of basis B, and M_C is the matrix whose columns\n  are the vectors of basis C.\n\n  Args:\n    B: A list of 3 basis vectors for basis B, each represented as a list of 3 integers.\n       Example: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    C: A list of 3 basis vectors for basis C, each represented as a list of 3 integers.\n       Example: [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\n\n  Returns:\n    A 3x3 transformation matrix P as a list of lists of floats,\n    rounded to 4 decimal places, such that [v]_C = P * [v]_B.\n  \"\"\"\n    M_B = np.array(B, dtype=float).T\n    M_C = np.array(C, dtype=float).T\n    try:\n        M_C_inv = np.linalg.inv(M_C)\n    except np.linalg.LinAlgError:\n        raise ValueError('Basis C is not valid (vectors are linearly dependent).')\n    P_matrix = M_C_inv @ M_B\n    P_rounded = np.round(P_matrix, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]"}
{"task_id": 28, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nimport math\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n\n    Args:\n        A: A 2x2 numpy array.\n\n    Returns:\n        A tuple (U, S, V) such that A = U * S * V.\n        U: 2x2 orthogonal matrix (list of lists).\n        S: 2x2 diagonal matrix with singular values (list of lists).\n        V: 2x2 orthogonal matrix (list of lists).\n        All results are rounded to the nearest 4th decimal.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2')\n    tol = 1e-09\n    if np.linalg.norm(A) < tol:\n        U = np.identity(2)\n        S = np.zeros((2, 2))\n        V = np.identity(2)\n        U_list = np.round(U, 4).tolist()\n        S_list = np.round(S, 4).tolist()\n        V_list = np.round(V, 4).tolist()\n        return (U_list, S_list, V_list)\n    S_mat = A @ A.T\n    p = S_mat[0, 0]\n    q = S_mat[0, 1]\n    r = S_mat[1, 1]\n    if abs(q) < tol:\n        if p >= r:\n            theta = 0.0\n            lambda1 = p\n            lambda2 = r\n        else:\n            theta = np.pi / 2.0\n            lambda1 = r\n            lambda2 = p\n    else:\n        theta = 0.5 * np.arctan2(2 * q, p - r)\n        trace = p + r\n        hyp_sq = max(0, (p - r) ** 2 + 4 * q ** 2)\n        hyp = np.sqrt(hyp_sq)\n        lambda1 = (trace + hyp) / 2.0\n        lambda2 = (trace - hyp) / 2.0\n    costheta = np.cos(theta)\n    sintheta = np.sin(theta)\n    U = np.array([[costheta, -sintheta], [sintheta, costheta]])\n    s1 = np.sqrt(max(0, lambda1))\n    s2 = np.sqrt(max(0, lambda2))\n    if s1 < s2:\n        (s1, s2) = (s2, s1)\n        U = U[:, [1, 0]]\n        U[:, 1] *= -1\n    S = np.array([[s1, 0], [0, s2]])\n    B = U.T @ A\n    b11 = B[0, 0]\n    b12 = B[0, 1]\n    b21 = B[1, 0]\n    b22 = B[1, 1]\n    if abs(s2) < tol:\n        v11 = b11 / s1 if abs(s1) > tol else 1.0\n        v12 = b12 / s1 if abs(s1) > tol else 0.0\n        norm_v1 = np.sqrt(v11 ** 2 + v12 ** 2)\n        if norm_v1 > tol:\n            v11 /= norm_v1\n            v12 /= norm_v1\n        else:\n            v11 = 1.0\n            v12 = 0.0\n        v21 = -v12\n        v22 = v11\n        V = np.array([[v11, v12], [v21, v22]])\n    else:\n        cosphi_est1 = b11 / s1\n        cosphi_est2 = b22 / s2\n        sinphi_est1 = b21 / s2\n        sinphi_est2 = -b12 / s1\n        avg_cosphi = (cosphi_est1 + cosphi_est2) / 2.0\n        avg_sinphi = (sinphi_est1 + sinphi_est2) / 2.0\n        norm_avg = np.sqrt(avg_cosphi ** 2 + avg_sinphi ** 2)\n        if norm_avg > tol:\n            avg_cosphi /= norm_avg\n            avg_sinphi /= norm_avg\n        else:\n            avg_cosphi = 1.0\n            avg_sinphi = 0.0\n        phi = np.arctan2(avg_sinphi, avg_cosphi)\n        cosphi = np.cos(phi)\n        sinphi = np.sin(phi)\n        V = np.array([[cosphi, -sinphi], [sinphi, cosphi]])\n    U_list = np.round(U, 4).tolist()\n    S_list = np.round(S, 4).tolist()\n    V_list = np.round(V, 4).tolist()\n    return (U_list, S_list, V_list)\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\nassert svd_2x2(np.array([[3, 5], [7, 9]])) == ([[-0.4538, -0.8911], [-0.8911, 0.4538]], [12.791, 0.6254], [[-0.5941, -0.8044], [0.8044, -0.5941]])"}
{"task_id": 29, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Tuple, Optional, Any\ndef shuffle_data(X: np.ndarray, y: np.ndarray, seed: Optional[int]=None) -> Tuple[List[Any], List[Any]]:\n    \"\"\"\n    Performs a random shuffle of the samples in two numpy arrays, X and y,\n    while maintaining the corresponding order between them.\n\n    Args:\n        X: NumPy array of features, typically with shape (n_samples, n_features).\n        y: NumPy array of labels or targets, typically with shape (n_samples,)\n           or (n_samples, n_outputs).\n        seed: Optional integer seed for the random number generator\n              for reproducible shuffling. Defaults to None, which means the\n              shuffle will be different each time.\n\n    Returns:\n        A tuple containing two Python lists:\n        - The shuffled X data converted to a list using tolist().\n        - The shuffled y data converted to a list using tolist().\n\n    Raises:\n        ValueError: If X and y do not have the same number of samples\n                    (i.e., different lengths along the first axis).\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(f'X and y must have the same number of samples in the first dimension, but got shapes {X.shape} and {y.shape}')\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])"}
{"task_id": 30, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom typing import Optional, Union, Generator, List, Tuple\ndef batch_iterator(X: np.ndarray, y: Optional[np.ndarray]=None, batch_size: int=64) -> Generator[Union[List, Tuple[List, List]], None, None]:\n    \"\"\"\n    Generates batches from numpy arrays X and optionally y.\n\n    Iterates over the dataset defined by X and optionally y, yielding\n    batches of the specified size. The last batch may be smaller if the\n    total number of samples is not perfectly divisible by the batch size.\n\n    Args:\n        X: The input data array (features). Should be a numpy ndarray.\n        y: The optional target data array (labels). If provided, must have\n           the same number of samples (first dimension) as X. Should be a\n           numpy ndarray. Defaults to None.\n        batch_size: The desired size for each batch. Must be a positive\n                    integer. Defaults to 64.\n\n    Yields:\n        If y is None:\n            A batch of X converted to a Python list using .tolist().\n        If y is provided:\n            A tuple containing a batch of X and a batch of y,\n            both converted to Python lists using .tolist():\n            (X_batch_list, y_batch_list).\n\n    Raises:\n        ValueError: If y is provided and X.shape[0] != y.shape[0].\n        ValueError: If batch_size is not a positive integer.\n        TypeError: If X or y (if provided) are not numpy arrays.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise TypeError(f'X must be a numpy array, but got {type(X)}')\n    if y is not None and (not isinstance(y, np.ndarray)):\n        raise TypeError(f'y must be a numpy array or None, but got {type(y)}')\n    if not isinstance(batch_size, int) or batch_size <= 0:\n        raise ValueError(f'batch_size must be a positive integer, but got {batch_size}')\n    n_samples = X.shape[0]\n    if y is not None:\n        if n_samples != y.shape[0]:\n            raise ValueError(f'X and y must have the same number of samples (first dimension), but got X.shape={X.shape} and y.shape={y.shape}')\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        X_batch = X[start_idx:end_idx]\n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]"}
{"task_id": 31, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Union, Tuple\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold: Union[int, float]) -> Tuple[List, List]:\n    \"\"\"\n  Divides a dataset based on whether the value of a specified feature\n  is greater than or equal to a given threshold.\n\n  Args:\n    X (np.ndarray): The dataset as a 2D NumPy array (samples x features).\n                    It's assumed that X is numerical.\n    feature_i (int): The index of the feature column to use for splitting.\n    threshold (Union[int, float]): The threshold value to compare against.\n\n  Returns:\n    Tuple[List, List]: A tuple containing two lists:\n        - The first list contains samples where X[:, feature_i] >= threshold.\n        - The second list contains samples where X[:, feature_i] < threshold.\n        Both lists are derived from NumPy arrays using .tolist().\n\n  Raises:\n    TypeError: If X is not a NumPy array.\n    ValueError: If X is not a 2D array.\n    IndexError: If feature_i is out of bounds for the columns of X.\n  \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise TypeError('Input X must be a NumPy array.')\n    if X.ndim != 2:\n        raise ValueError('Input X must be a 2D array.')\n    if not 0 <= feature_i < X.shape[1]:\n        raise IndexError(f'feature_i index {feature_i} is out of bounds for axis 1 with size {X.shape[1]}')\n    split_mask = X[:, feature_i] >= threshold\n    subset1 = X[split_mask]\n    subset2 = X[~split_mask]\n    subset1_list = subset1.tolist()\n    subset2_list = subset2.tolist()\n    return (subset1_list, subset2_list)\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]"}
{"task_id": 32, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\nfrom itertools import combinations_with_replacement, product\ndef polynomial_features(X, degree):\n    \"\"\"\n  Generates polynomial features for a given dataset.\n\n  Args:\n    X: A 2D numpy array of shape (n_samples, n_features) representing the input data.\n    degree: An integer specifying the maximum degree of the polynomial features.\n\n  Returns:\n    A Python list representing the new 2D array of shape\n    (n_samples, n_output_features) containing the polynomial features.\n    The features include the original features and their combinations up to the specified degree.\n    The order of features is typically: 1, x1, x2, x1^2, x1*x2, x2^2, ...\n  \"\"\"\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        try:\n            X = np.array(X)\n            if X.ndim != 2:\n                raise ValueError('Input X must be convertible to a 2D numpy array.')\n        except Exception as e:\n            raise ValueError(f'Input X must be a 2D numpy array or convertible. Error: {e}')\n    if not isinstance(degree, int) or degree < 0:\n        raise ValueError('Degree must be a non-negative integer.')\n    (n_samples, n_features) = X.shape\n    feature_indices = range(n_features)\n    all_combinations = []\n    for d in range(degree + 1):\n        combos_d = combinations_with_replacement(feature_indices, d)\n        all_combinations.extend(list(combos_d))\n    n_output_features = len(all_combinations)\n    X_poly = np.empty((n_samples, n_output_features), dtype=X.dtype if np.issubdtype(X.dtype, np.floating) else float)\n    for (j, combo_indices) in enumerate(all_combinations):\n        if not combo_indices:\n            X_poly[:, j] = 1.0\n        else:\n            X_poly[:, j] = np.prod(X[:, combo_indices], axis=1)\n    return X_poly.tolist()\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]"}
{"task_id": 33, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n  Generates random subsets of a given dataset (X, y).\n\n  Args:\n    X (np.ndarray): 2D numpy array of features (shape: n_samples, n_features).\n    y (np.ndarray): 1D numpy array of labels/targets (shape: n_samples,).\n    n_subsets (int): The number of random subsets to generate.\n    replacements (bool, optional): If True, sample with replacement (bootstrapping).\n                                   If False, sample without replacement\n                                   (resulting in permutations of the full dataset\n                                   if subset size equals original size).\n                                   Defaults to True.\n    seed (int, optional): Random seed for reproducibility. Defaults to 42.\n\n  Returns:\n    list: A list containing n_subsets tuples. Each tuple is of the form\n          (X_subset, y_subset), where X_subset and y_subset are Python lists\n          derived from the numpy arrays using .tolist().\n\n  Raises:\n      ValueError: If the number of samples in X and y do not match.\n      ValueError: If n_subsets is negative.\n  \"\"\"\n    if X.shape[0] != len(y):\n        raise ValueError(f'Number of samples in X ({X.shape[0]}) and y ({len(y)}) must match.')\n    if n_subsets < 0:\n        raise ValueError('n_subsets cannot be negative.')\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError('X and y must be numpy arrays.')\n    if X.ndim != 2:\n        raise ValueError(f'X must be a 2D array, but got {X.ndim} dimensions.')\n    if y.ndim != 1:\n        raise ValueError(f'y must be a 1D array, but got {y.ndim} dimensions.')\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    all_subsets = []\n    for _ in range(n_subsets):\n        if n_samples == 0:\n            indices = np.array([], dtype=int)\n        elif replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        elif n_samples > 0:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        else:\n            indices = np.array([], dtype=int)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        all_subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return all_subsets\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\nassert get_random_subsets(np.array([[1, 3], [2, 4], [3, 5], [4, 6]]), np.array([1, 20, 30, 40]), 2, True, seed=42) == [([[3, 5], [4, 6], [1, 3], [3, 5]], [30, 40, 1, 30]), ([[3, 5], [4, 6], [1, 3], [1, 3]], [30, 40, 1, 1])]"}
{"task_id": 34, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n  Converts a 1D array of integer labels into a one-hot encoded format.\n\n  Args:\n    x (np.ndarray or list-like): A 1D numpy array (or list/tuple) of\n                                 non-negative integer labels representing\n                                 nominal categories.\n    n_col (int, optional): The total number of categories (columns in the\n                           output). If None, it's determined automatically\n                           as max(x) + 1. Defaults to None.\n\n  Returns:\n    list: A list of lists representing the one-hot encoded data.\n          Returns an empty list if the input x is empty.\n\n  Raises:\n    ValueError: If x is not 1D, contains negative values, or if n_col\n                is provided but is too small for the maximum value in x\n                or is not a positive integer.\n    TypeError: If x contains values that cannot be safely converted to integers.\n  \"\"\"\n    try:\n        x = np.array(x, dtype=int)\n    except ValueError:\n        raise TypeError('Input x must contain values convertible to integers.')\n    if x.ndim != 1:\n        raise ValueError('Input array x must be 1-dimensional.')\n    if x.size == 0:\n        return []\n    if np.any(x < 0):\n        raise ValueError('Input array x cannot contain negative values.')\n    max_val = np.max(x)\n    if n_col is None:\n        n_col = max_val + 1\n    else:\n        if not isinstance(n_col, int) or n_col <= 0:\n            raise ValueError('n_col must be a positive integer.')\n        if n_col <= max_val:\n            raise ValueError(f'n_col={n_col} is not large enough for the maximum value {max_val} found in x. It must be > {max_val}.')\n    n_row = x.shape[0]\n    one_hot_matrix = np.zeros((n_row, n_col), dtype=int)\n    one_hot_matrix[np.arange(n_row), x] = 1\n    return one_hot_matrix.tolist()\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]"}
{"task_id": 35, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    \"\"\"\n  Converts a 1D numpy array into a diagonal matrix represented as a Python list.\n\n  Args:\n    x: A 1D numpy array.\n\n  Returns:\n    A Python list of lists representing the diagonal matrix where\n    the elements of x are on the main diagonal.\n\n  Raises:\n    ValueError: If the input array x is not 1-dimensional.\n  \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError('Input must be a numpy array.')\n    if x.ndim != 1:\n        raise ValueError(f'Input array must be 1-dimensional, but got {x.ndim} dimensions.')\n    diagonal_matrix_np = np.diag(x)\n    result_list = diagonal_matrix_np.tolist()\n    return result_list\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]"}
{"task_id": 36, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import Union\ndef accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculates the accuracy classification score.\n\n    Accuracy is the proportion of correct predictions among the total\n    number of cases processed. It is defined as:\n    Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n\n    Args:\n        y_true: 1D numpy array of ground truth labels.\n        y_pred: 1D numpy array of predicted labels.\n                Must have the same size as y_true.\n\n    Returns:\n        The accuracy score as a float, rounded to 4 decimal places.\n        Returns 1.0 if both input arrays are empty (consistent with\n        scikit-learn's behavior). Returns 0.0 if inputs are non-empty\n        but no predictions are correct.\n\n    Raises:\n        ValueError: If y_true and y_pred have different sizes (implicitly\n                    handled by numpy operations if shapes mismatch in ways\n                    other than size, but explicit check is good practice,\n                    though omitted here to stick strictly to calculation).\n                    NumPy might raise errors for incompatible shapes/sizes\n                    during comparison or mean calculation if not identical 1D.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.size == 0:\n        return 1.0\n    accuracy = np.mean(y_true == y_pred)\n    rounded_accuracy = round(accuracy, 4)\n    return rounded_accuracy\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25"}
{"task_id": 37, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport warnings\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset X, or between X and Y.\n\n    The correlation coefficient is calculated between columns (features).\n\n    Args:\n        X (np.ndarray): A 2D numpy array with shape (n_samples, n_features_X).\n                        Represents the first dataset where rows are samples\n                        and columns are features.\n        Y (np.ndarray, optional): A 2D numpy array with shape (n_samples, n_features_Y).\n                                  Represents the second dataset. Must have the same\n                                  number of samples (rows) as X. If None, calculates\n                                  the correlation matrix of X with itself.\n                                  Defaults to None.\n\n    Returns:\n        list: A nested list representing the correlation matrix, with all values\n              rounded to the nearest 4th decimal place.\n              - If Y is None, the output shape is (n_features_X, n_features_X).\n              - If Y is provided, the output shape is (n_features_X, n_features_Y),\n                representing the correlation between each feature of X and each\n                feature of Y.\n\n    Raises:\n        TypeError: If X or Y (if provided) are not numpy arrays.\n        ValueError: If X or Y (if provided) are not 2D arrays.\n        ValueError: If Y is provided and X.shape[0] != Y.shape[0].\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise TypeError('X must be a numpy array.')\n    if X.ndim != 2:\n        raise ValueError('X must be a 2D array.')\n    (n_samples, n_features_X) = X.shape\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', category=RuntimeWarning)\n        if Y is None:\n            if n_features_X == 0:\n                corr_matrix = np.empty((0, 0))\n            elif n_samples < 2:\n                corr_matrix = np.full((n_features_X, n_features_X), np.nan)\n            else:\n                corr_matrix = np.corrcoef(X, rowvar=False)\n                if n_features_X == 1:\n                    if not isinstance(corr_matrix, np.ndarray) or corr_matrix.ndim != 2:\n                        corr_matrix = np.array([[corr_matrix]])\n        else:\n            if not isinstance(Y, np.ndarray):\n                raise TypeError('Y must be a numpy array if provided.')\n            if Y.ndim != 2:\n                raise ValueError('Y must be a 2D array if provided.')\n            if X.shape[0] != Y.shape[0]:\n                raise ValueError('X and Y must have the same number of rows (observations).')\n            n_features_Y = Y.shape[1]\n            if n_features_X == 0 or n_features_Y == 0:\n                corr_matrix = np.empty((n_features_X, n_features_Y))\n            elif n_samples < 2:\n                corr_matrix = np.full((n_features_X, n_features_Y), np.nan)\n            else:\n                combined_matrix = np.corrcoef(X, Y, rowvar=False)\n                corr_matrix = combined_matrix[0:n_features_X, n_features_X:]\n                if corr_matrix.shape != (n_features_X, n_features_Y):\n                    corr_matrix = np.full((n_features_X, n_features_Y), np.nan)\n    rounded_matrix = np.round(corr_matrix, 4)\n    if isinstance(rounded_matrix, np.ndarray):\n        rounded_matrix[np.isclose(rounded_matrix, -0.0, atol=1e-18)] = 0.0\n    if isinstance(rounded_matrix, np.ndarray):\n        result_list = rounded_matrix.tolist()\n    else:\n        result_list = []\n    return result_list\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1.0, 1.0], [1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1.0, -1.0], [ 1.0, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]"}
{"task_id": 38, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fits an AdaBoost classifier using decision stumps as weak learners.\n\n    Implements the fitting process for AdaBoost by iteratively finding the\n    best decision stump (based on weighted error), calculating its importance (alpha),\n    and updating sample weights.\n\n    Args:\n        X (np.ndarray): Training data of shape (n_samples, n_features).\n                        Assumed to be a 2D numpy array.\n        y (np.ndarray): Target labels of shape (n_samples,).\n                        Assumed to be a 1D numpy array with labels 0 or 1.\n        n_clf (int):    The number of weak classifiers (decision stumps) to fit.\n\n    Returns:\n        list: A list of dictionaries. Each dictionary represents a fitted weak\n              classifier (decision stump) and contains the following keys:\n              - 'alpha' (float): The weight of the classifier, rounded to 4 decimals.\n              - 'feature_idx' (int): The index of the feature used by the stump.\n              - 'threshold' (float): The threshold value used by the stump,\n                                     rounded to 4 decimals.\n              - 'polarity' (int): The polarity of the stump (+1 or -1). Indicates\n                                  how samples are classified relative to the threshold.\n                                  If polarity is 1, samples with feature value >= threshold\n                                  are predicted as +1, otherwise -1.\n                                  If polarity is -1, samples with feature value >= threshold\n                                  are predicted as -1, otherwise +1.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if n_samples == 0:\n        print('Warning: Training data has 0 samples.')\n        return []\n    if n_features == 0:\n        print('Warning: Training data has 0 features.')\n        return []\n    w = np.full(n_samples, 1 / n_samples, dtype=np.float64)\n    y_internal = np.where(y == 0, -1, 1)\n    classifiers = []\n    for _ in range(n_clf):\n        best_stump = {}\n        min_error = float('inf')\n        best_predictions = np.zeros(n_samples, dtype=np.float64)\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            unique_values = np.unique(feature_values)\n            if len(unique_values) > 1:\n                sorted_unique = np.sort(unique_values)\n                thresholds = (sorted_unique[:-1] + sorted_unique[1:]) / 2\n            elif len(unique_values) == 1:\n                thresholds = unique_values\n            else:\n                continue\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.full(n_samples, polarity, dtype=np.float64)\n                    predictions[feature_values < threshold] *= -1\n                    misclassified_mask = predictions != y_internal\n                    error = np.sum(w[misclassified_mask])\n                    if error < min_error:\n                        min_error = error\n                        best_stump['feature_idx'] = feature_idx\n                        best_stump['threshold'] = threshold\n                        best_stump['polarity'] = polarity\n                        best_predictions = predictions\n        if not best_stump:\n            print(f'Warning: Could not find a suitable weak classifier (error < 0.5). Stopping early.')\n            break\n        epsilon = 1e-10\n        if min_error <= epsilon:\n            alpha = 0.5 * np.log((1.0 - epsilon) / epsilon)\n        elif min_error >= 0.5 - epsilon:\n            alpha = 0.0\n        else:\n            alpha = 0.5 * np.log((1.0 - min_error) / min_error)\n        clf_params = {'alpha': round(alpha, 4), 'feature_idx': best_stump['feature_idx'], 'threshold': round(best_stump['threshold'], 4), 'polarity': best_stump['polarity']}\n        classifiers.append(clf_params)\n        if alpha == 0.0 and min_error >= 0.5 - epsilon:\n            print(f'Warning: Weak classifier error was >= 0.5 (error={min_error:.4f}). Alpha set to 0. AdaBoost may not improve further.')\n        update_factor = np.exp(-alpha * y_internal * best_predictions)\n        w = w * update_factor\n        w_sum = np.sum(w)\n        if w_sum > epsilon:\n            w = w / w_sum\n        else:\n            w = np.full(n_samples, 1 / n_samples, dtype=np.float64)\n    return classifiers\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]"}
{"task_id": 39, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n  Computes the log-softmax of a 1D list of scores.\n\n  The log-softmax function is defined as:\n  log_softmax(x_i) = x_i - log(sum(exp(x_j) for j in range(len(x))))\n\n  To improve numerical stability, we use the LogSumExp trick:\n  log_softmax(x_i) = x_i - (max(x) + log(sum(exp(x_j - max(x)))))\n\n  Args:\n    scores: A 1D list of numerical scores.\n\n  Returns:\n    A 1D list containing the log-softmax values, rounded to 4 decimal places.\n    Returns an empty list if the input list is empty.\n  \"\"\"\n    scores_np = np.array(scores, dtype=np.float64)\n    if scores_np.size == 0:\n        return []\n    max_score = np.max(scores_np)\n    shifted_scores = scores_np - max_score\n    exp_shifted_scores = np.exp(shifted_scores)\n    sum_exp_shifted_scores = np.sum(exp_shifted_scores)\n    log_sum_exp = np.log(sum_exp_shifted_scores)\n    log_softmax_values = shifted_scores - log_sum_exp\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    return rounded_log_softmax.tolist()\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]"}
{"task_id": 40, "completion_id": 0, "passed": false, "result": "failed: name 'Dense' is not defined", "solution": "import numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n    \n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.2082, -0.2293]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[3, 5]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.2, 0.3, 0.4]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.287, -0.3126]]\nnp.random.seed(42)\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[2, 3]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.3, 0.4, 0.5]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert back_output == [[0.3658, -0.396]]"}
{"task_id": 41, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Performs a simple 2D convolution operation.\n\n    Args:\n        input_matrix: The input 2D NumPy array (e.g., image or feature map).\n        kernel: The convolutional kernel (filter) 2D NumPy array.\n        padding: The amount of zero-padding to add around the input matrix border.\n                 A single integer value applies padding equally to all sides (top, bottom, left, right).\n        stride: The step size (horizontally and vertically) for moving the kernel\n                across the input matrix.\n\n    Returns:\n        A Python list representing the 2D result of the convolution,\n        with all values rounded to the nearest 4th decimal place.\n        Returns an empty list if the kernel cannot fit on the padded input.\n    \"\"\"\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueError('Input matrix and kernel must be 2-dimensional arrays.')\n    if padding < 0:\n        raise ValueError('Padding cannot be negative.')\n    if stride <= 0:\n        raise ValueError('Stride must be a positive integer.')\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded_input = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    (padded_h, padded_w) = padded_input.shape\n    output_h = (padded_h - kernel_h) // stride + 1\n    output_w = (padded_w - kernel_w) // stride + 1\n    if output_h <= 0 or output_w <= 0:\n        return []\n    output_matrix = np.zeros((output_h, output_w))\n    for r_out in range(output_h):\n        for c_out in range(output_w):\n            r_start = r_out * stride\n            c_start = c_out * stride\n            r_end = r_start + kernel_h\n            c_end = c_start + kernel_w\n            region = padded_input[r_start:r_end, c_start:c_end]\n            convolution_sum = np.sum(region * kernel)\n            output_matrix[r_out, c_out] = convolution_sum\n    rounded_output_matrix = np.round(output_matrix, 4)\n    return rounded_output_matrix.tolist()\npad = 1\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]"}
{"task_id": 42, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef relu(z: float) -> float:\n    \"\"\"\n  Implements the Rectified Linear Unit (ReLU) activation function.\n\n  ReLU(z) = max(0, z)\n\n  Args:\n    z: The input float value.\n\n  Returns:\n    The input value if it's positive, otherwise 0.0.\n  \"\"\"\n    return max(0.0, z)\nassert relu(0) == 0\nassert relu(1) == 1\nassert relu(-1) == 0"}
{"task_id": 43, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n  Calculates the Ridge Regression loss.\n\n  The loss is defined as: MSE + alpha * L2_norm(w)^2 / 2\n  However, the common definition used in optimization and libraries like scikit-learn\n  often uses the Sum of Squared Errors (SSE) scaled by 1/(2*n_samples) or just MSE,\n  and the L2 penalty term as alpha * sum(w_i^2). We will use the MSE definition\n  as requested in the description.\n\n  Loss = (1/n) * sum((y_pred - y_true)^2) + alpha * sum(w^2)\n  where y_pred = X @ w, n is the number of samples.\n\n  Args:\n    X: Feature matrix (n_samples, n_features).\n    w: Coefficient vector (n_features,).\n    y_true: True target values (n_samples,).\n    alpha: Regularization strength (float, non-negative).\n\n  Returns:\n    The Ridge loss value, rounded to 4 decimal places.\n\n  Raises:\n      ValueError: If dimensions of input arrays are incompatible.\n  \"\"\"\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    if w.shape[0] != n_features:\n        raise ValueError(f'Weight vector dimension {w.shape[0]} does not match number of features {n_features}')\n    if y_true.shape[0] != n_samples:\n        raise ValueError(f'True labels vector dimension {y_true.shape[0]} does not match number of samples {n_samples}')\n    if alpha < 0:\n        raise ValueError('Regularization parameter alpha must be non-negative')\n    if n_samples == 0:\n        l2_penalty = alpha * np.dot(w, w)\n        return round(l2_penalty, 4)\n    try:\n        y_pred = X @ w\n    except ValueError as e:\n        raise ValueError(f'Matrix multiplication failed. Check shapes: X={X.shape}, w={w.shape}') from e\n    if y_pred.shape != y_true.shape:\n        if y_pred.shape == (n_samples, 1) and y_true.shape == (n_samples,):\n            y_pred = y_pred.flatten()\n        elif y_true.shape == (n_samples, 1) and y_pred.shape == (n_samples,):\n            y_true = y_true.flatten()\n        else:\n            raise ValueError(f'Shape mismatch between predictions {y_pred.shape} and true labels {y_true.shape}')\n    mse = np.mean((y_pred - y_true) ** 2)\n    l2_penalty = alpha * np.dot(w, w)\n    total_loss = mse + l2_penalty\n    return round(total_loss, 4)\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\nassert ridge_loss(np.array([[3,4,1],[1,2,2],[1,.1,3],[1.2,1,4],[1,2,3]]), np.array([2,.2,5]), np.array([2,3,4,5,2]), 0.1) == 183.0721"}
{"task_id": 44, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n  Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n  Leaky ReLU is defined as:\n  f(z) = z     if z >= 0\n  f(z) = alpha * z  if z < 0\n\n  Args:\n    z: The input value (float).\n    alpha: The slope for negative inputs (float, default=0.01).\n           This value should typically be small and positive.\n\n  Returns:\n    The result of applying the Leaky ReLU function.\n    Returns float if z < 0 or if z is float. Returns int if z is int and z >= 0.\n  \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z\nassert leaky_relu(5) == 5\nassert leaky_relu(1) == 1\nassert leaky_relu(-1) == -0.01\nassert leaky_relu(0) == 0\nassert leaky_relu(-2, alpha=0.1) == -0.2"}
{"task_id": 45, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import Union\ndef kernel_function(x1: np.ndarray, x2: np.ndarray) -> Union[float, np.number]:\n    \"\"\"\n  Computes the linear kernel between two input vectors.\n\n  The linear kernel is defined as the dot product (inner product)\n  of the two vectors: K(x1, x2) = x1^T * x2.\n\n  Args:\n    x1: The first input vector (NumPy array of shape (n,)).\n    x2: The second input vector (NumPy array of shape (n,)).\n        Must have the same dimension as x1.\n\n  Returns:\n    The dot product (scalar value) of x1 and x2.\n\n  Raises:\n    ValueError: If x1 and x2 have incompatible shapes for dot product\n                (e.g., different lengths). This is typically raised\n                by the underlying numpy function.\n  \"\"\"\n    if x1.ndim != 1 or x2.ndim != 1:\n        pass\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\nassert kernel_function(np.array([3, 1, 2, 5]) , np.array([3, 6, 4, 5]) ) == 48"}
{"task_id": 46, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import Union\ndef precision(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculates the precision metric for binary classification.\n\n    Precision is defined as the ratio of true positives (TP) to the sum\n    of true positives and false positives (TP + FP).\n    Precision = TP / (TP + FP)\n\n    Args:\n        y_true: A numpy array containing the true binary labels (0 or 1).\n        y_pred: A numpy array containing the predicted binary labels (0 or 1).\n                Must have the same shape as y_true.\n\n    Returns:\n        The precision score as a float. Returns 0.0 if the denominator\n        (TP + FP) is zero, to avoid division by zero errors. This occurs\n        when the model makes no positive predictions.\n\n    Raises:\n        ValueError: If the input arrays `y_true` and `y_pred` do not have\n                    the same shape.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray):\n        y_true = np.array(y_true)\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Input arrays y_true and y_pred must have the same shape. Got {y_true.shape} and {y_pred.shape}')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    denominator = true_positives + false_positives\n    if denominator == 0:\n        return 0.0\n    else:\n        precision_score = true_positives / denominator\n        return precision_score\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\nassert precision(np.array([1, 0, 1, 1, 0, 0, 1, 1])  , np.array([1, 0, 0, 0, 0, 1, 0, 0])) == 0.5"}
{"task_id": 47, "completion_id": 0, "passed": false, "result": "failed: Initial weights dimension (2) must match number of features + 1 (3)", "solution": "import numpy as np\nimport numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Performs gradient descent (Batch, Mini-Batch, or Stochastic) for linear regression\n    using Mean Squared Error (MSE) loss.\n\n    Args:\n        X (np.ndarray): Input features matrix (n_samples, n_features).\n                        Assumes no bias column is present initially.\n        y (np.ndarray or list): Target values (n_samples,).\n        weights (np.ndarray or list): Initial weights (n_features + 1,).\n                                      The first element corresponds to the bias term.\n        learning_rate (float): The step size for weight updates.\n        n_iterations (int): The number of passes over the entire dataset (epochs).\n        batch_size (int): The number of samples used in each gradient update.\n                          Relevant only for 'mini-batch' and implicitly for 'sgd'.\n                          Defaults to 1.\n        method (str): The variant of gradient descent to use.\n                      Options: 'batch', 'sgd', 'mini-batch'. Defaults to 'batch'.\n\n    Returns:\n        list: The final updated weights (including bias) as a Python list,\n              rounded to 4 decimal places.\n\n    Raises:\n        ValueError: If the specified method is invalid.\n        ValueError: If the initial weights dimension doesn't match the number of features + 1.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if not isinstance(y, np.ndarray):\n        y = np.array(y)\n    if not isinstance(weights, np.ndarray):\n        weights = np.array(weights)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    X_b = np.c_[np.ones((n_samples, 1)), X]\n    y = y.reshape(-1, 1)\n    weights = weights.reshape(-1, 1).astype(float)\n    if weights.shape[0] != n_features + 1:\n        raise ValueError(f'Initial weights dimension ({weights.shape[0]}) must match number of features + 1 ({n_features + 1})')\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            m_batch = n_samples\n            X_batch = X_b\n            y_batch = y\n            predictions = X_batch @ weights\n            error = predictions - y_batch\n            gradient = 1 / m_batch * (X_batch.T @ error)\n            weights = weights - learning_rate * gradient\n        elif method == 'sgd':\n            shuffled_indices = np.random.permutation(n_samples)\n            X_b_shuffled = X_b[shuffled_indices]\n            y_shuffled = y[shuffled_indices]\n            for i in range(n_samples):\n                m_batch = 1\n                X_batch = X_b_shuffled[i:i + 1]\n                y_batch = y_shuffled[i:i + 1]\n                prediction = X_batch @ weights\n                error = prediction - y_batch\n                gradient = X_batch.T @ error\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            shuffled_indices = np.random.permutation(n_samples)\n            X_b_shuffled = X_b[shuffled_indices]\n            y_shuffled = y[shuffled_indices]\n            for i in range(0, n_samples, batch_size):\n                end_index = min(i + batch_size, n_samples)\n                X_batch = X_b_shuffled[i:end_index]\n                y_batch = y_shuffled[i:end_index]\n                m_batch = len(y_batch)\n                if m_batch == 0:\n                    continue\n                predictions = X_batch @ weights\n                error = predictions - y_batch\n                gradient = 1 / m_batch * (X_batch.T @ error)\n                weights = weights - learning_rate * gradient\n        else:\n            raise ValueError(\"Method must be 'batch', 'sgd', or 'mini-batch'\")\n    final_weights = np.round(weights.flatten(), 4)\n    return final_weights.tolist()\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]"}
{"task_id": 48, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef rref(matrix):\n    \"\"\"\n    Computes the Reduced Row Echelon Form (RREF) of a given matrix.\n\n    Args:\n        matrix: A list of lists representing the input matrix.\n\n    Returns:\n        A list of lists representing the RREF of the input matrix.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (m, n) = A.shape\n    pivot_row = 0\n    epsilon = 1e-08\n    for col in range(n):\n        if pivot_row >= m:\n            break\n        i = pivot_row\n        while i < m and abs(A[i, col]) < epsilon:\n            i += 1\n        if i < m:\n            if i != pivot_row:\n                A[[pivot_row, i]] = A[[i, pivot_row]]\n            pivot_value = A[pivot_row, col]\n            A[pivot_row] = A[pivot_row] / pivot_value\n            for j in range(m):\n                if j != pivot_row:\n                    factor = A[j, col]\n                    A[j] = A[j] - factor * A[pivot_row]\n            pivot_row += 1\n    A[np.abs(A) < epsilon] = 0.0\n    return A.tolist()\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]"}
{"task_id": 49, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n  Implements the Adam optimization algorithm.\n\n  Args:\n    f: The objective function to be optimized (not directly used by Adam,\n       but often useful for tracking progress - not used in this implementation).\n    grad: A function that computes the gradient of f with respect to x.\n          It should take the current parameters x and return the gradient.\n    x0: Initial parameter values (e.g., a list or numpy array).\n    learning_rate: The step size (alpha in the Adam paper).\n    beta1: Exponential decay rate for the first moment estimates.\n    beta2: Exponential decay rate for the second moment estimates.\n    epsilon: A small constant for numerical stability.\n    num_iterations: Number of iterations to run the optimizer.\n\n  Returns:\n    list: The optimized parameters, rounded to 4 decimal places.\n  \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g_t = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g_t\n        v = beta2 * v + (1 - beta2) * g_t ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    optimized_params = np.round(x, 4)\n    return optimized_params.tolist()\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]"}
{"task_id": 50, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent.\n\n    Args:\n        X (np.array): Feature matrix (n_samples, n_features).\n        y (np.array): Target vector (n_samples,).\n        alpha (float): Regularization strength (lambda in some texts). Defaults to 0.1.\n        learning_rate (float): Step size for gradient descent. Defaults to 0.01.\n        max_iter (int): Maximum number of iterations for gradient descent. Defaults to 1000.\n        tol (float): Tolerance for convergence. Stop if the norm of the change\n                     in weights and bias is less than tol. Defaults to 1e-4.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: Final weights rounded to 4 decimal places.\n            - float: Final bias rounded to 4 decimal places.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.zeros((n_features, 1))\n    b = 0.0\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    elif y.shape[1] != 1:\n        raise ValueError('Input y should be a 1D array or a column vector.')\n    if y.shape[0] != n_samples:\n        raise ValueError(f'Mismatch in number of samples between X ({n_samples}) and y ({y.shape[0]})')\n    for iteration in range(max_iter):\n        w_prev = w.copy()\n        b_prev = b\n        y_pred = X.dot(w) + b\n        error = y_pred - y\n        dw_mse = 1 / n_samples * X.T.dot(error)\n        dw_l1 = alpha * np.sign(w)\n        dw = dw_mse + dw_l1\n        db = 1 / n_samples * np.sum(error)\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        delta_w = np.linalg.norm(w - w_prev)\n        delta_b = np.abs(b - b_prev)\n        if delta_w < tol and delta_b < tol:\n            break\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    return (w_rounded.flatten().tolist(), b_rounded)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)"}
{"task_id": 51, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n\n    The OSA distance is the minimum number of single-character edits\n    (insertion, deletion, substitution) and transpositions of adjacent\n    characters required to change one string into the other.\n\n    Args:\n        source: The source string.\n        target: The target string.\n\n    Returns:\n        The Optimal String Alignment distance between source and target.\n    \"\"\"\n    m = len(source)\n    n = len(target)\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(m + 1):\n        dp[i][0] = i\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            deletion_cost = dp[i - 1][j] + 1\n            insertion_cost = dp[i][j - 1] + 1\n            substitution_cost = dp[i - 1][j - 1] + cost\n            dp[i][j] = min(deletion_cost, insertion_cost, substitution_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                transposition_cost = dp[i - 2][j - 2] + 1\n                dp[i][j] = min(dp[i][j], transposition_cost)\n    return dp[m][n]\nassert OSA(\"butterfly\", \"dragonfly\") == 6\nassert OSA(\"caper\", \"acer\") == 2\nassert OSA(\"telescope\", \"microscope\") == 5\nassert OSA(\"london\", \"paris\") == 6\nassert OSA(\"shower\", \"grower\") == 2\nassert OSA(\"labyrinth\", \"puzzle\") == 9\nassert OSA(\"silhouette\", \"shadow\") == 8\nassert OSA(\"whisper\", \"screaming\") == 9\nassert OSA(\"enigma\", \"mystery\") == 7\nassert OSA(\"symphony\", \"cacophony\") == 4\nassert OSA(\"mirage\", \"oasis\") == 6\nassert OSA(\"asteroid\", \"meteorite\") == 5\nassert OSA(\"palindrome\", \"palladium\") == 5"}
{"task_id": 52, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n  Calculates the recall metric for binary classification.\n\n  Recall = True Positives / (True Positives + False Negatives)\n         = TP / (TP + FN)\n\n  Args:\n    y_true: A list or numpy array of true binary labels (0 or 1).\n    y_pred: A list or numpy array of predicted binary labels (0 or 1).\n\n  Returns:\n    The recall value rounded to three decimal places.\n    Returns 0.0 if there are no actual positive instances (TP + FN = 0).\n  \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not (np.all(np.isin(y_true, [0, 1])) and np.all(np.isin(y_pred, [0, 1]))):\n        raise ValueError('Input labels must be binary (0 or 1).')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must have the same length.')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_score = tp / denominator\n        return round(recall_score, 3)\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667"}
{"task_id": 53, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport json\ndef softmax(x):\n    \"\"\"\n  Compute softmax values for each sets of scores in x.\n  Ensures numerical stability by subtracting the maximum value.\n  Applied row-wise (axis=1).\n  \"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / np.sum(e_x, axis=1, keepdims=True)\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n  Computes the self-attention mechanism output.\n\n  Args:\n    X: Input sequence matrix. Shape (seq_len, d_model)\n       seq_len: Length of the input sequence.\n       d_model: Dimension of the input embeddings.\n    W_q: Weight matrix for Query. Shape (d_model, d_q)\n         d_q: Dimension of the query vectors.\n    W_k: Weight matrix for Key. Shape (d_model, d_k)\n         d_k: Dimension of the key vectors. (Must be equal to d_q for dot product)\n    W_v: Weight matrix for Value. Shape (d_model, d_v)\n         d_v: Dimension of the value vectors.\n\n  Returns:\n    list: The self-attention output as a Python list, rounded to 4 decimal places.\n          Shape will be (seq_len, d_v) before converting to list.\n  \"\"\"\n    X = np.array(X)\n    W_q = np.array(W_q)\n    W_k = np.array(W_k)\n    W_v = np.array(W_v)\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = K.shape[1]\n    if d_k == 0:\n        raise ValueError('Dimension of key vectors (d_k) cannot be zero.')\n    if Q.shape[1] != K.shape[1]:\n        raise ValueError(f'Dimension of Query ({Q.shape[1]}) must match dimension of Key ({K.shape[1]}) for dot product.')\n    attention_scores = np.dot(Q, K.T)\n    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n    attention_weights = softmax(scaled_attention_scores)\n    output = np.dot(attention_weights, V)\n    rounded_output = np.round(output, 4)\n    return rounded_output.tolist()\nd_k = 5\nassert self_attention(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[1.6605, 2.6605], [2.3395, 3.3395]]\nassert self_attention(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[3.0093, 4.679], [2.5, 4.0]]\nassert self_attention(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]"}
{"task_id": 54, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a single forward pass of a simple RNN cell over a sequence.\n\n    Args:\n        input_sequence: A list of input vectors (list of lists of floats). \n                        Each inner list represents the input vector x_t at time step t.\n                        Shape: (sequence_length, input_size)\n        initial_hidden_state: The initial hidden state vector h_0 (list of floats).\n                             Shape: (hidden_size,)\n        Wx: Weight matrix for input-to-hidden connections (list of lists of floats).\n            Shape: (hidden_size, input_size)\n        Wh: Weight matrix for hidden-to-hidden connections (list of lists of floats).\n            Shape: (hidden_size, hidden_size)\n        b: Bias vector (list of floats).\n           Shape: (hidden_size,)\n\n    Returns:\n        The final hidden state vector after processing the entire sequence, \n        rounded to 4 decimal places (list of floats).\n        Shape: (hidden_size,)\n    \"\"\"\n    h_prev = np.array(initial_hidden_state)\n    Wx_np = np.array(Wx)\n    Wh_np = np.array(Wh)\n    b_np = np.array(b)\n    hidden_size = Wh_np.shape[0]\n    if Wh_np.shape[1] != hidden_size:\n        raise ValueError('Wh must be a square matrix (hidden_size, hidden_size)')\n    if Wx_np.shape[0] != hidden_size:\n        raise ValueError('Wx must have shape (hidden_size, input_size)')\n    if b_np.shape[0] != hidden_size:\n        raise ValueError('Bias must have shape (hidden_size,)')\n    if h_prev.shape[0] != hidden_size:\n        raise ValueError('Initial hidden state must have shape (hidden_size,)')\n    if input_sequence and Wx_np.shape[1] != len(input_sequence[0]):\n        raise ValueError('Input vector size must match Wx second dimension')\n    for x_t_list in input_sequence:\n        x_t = np.array(x_t_list)\n        h_next = np.tanh(Wx_np @ x_t + Wh_np @ h_prev + b_np)\n        h_prev = h_next\n    final_hidden_state = h_prev\n    rounded_final_state = np.round(final_hidden_state, 4)\n    result_list = rounded_final_state.tolist()\n    return result_list\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]"}
{"task_id": 55, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n  Applies a 2D translation to a list of points.\n\n  Args:\n    points: A list of [x, y] coordinates (e.g., [[x1, y1], [x2, y2], ...]).\n    tx: The translation distance in the x-direction.\n    ty: The translation distance in the y-direction.\n\n  Returns:\n    A new list of translated [x, y] coordinates, converted using tolist().\n    Returns an empty list if the input points list is empty.\n  \"\"\"\n    points_array = np.array(points)\n    if points_array.size == 0:\n        return []\n    translation_vector = np.array([tx, ty])\n    translated_points_array = points_array + translation_vector\n    translated_points_list = translated_points_array.tolist()\n    return translated_points_list\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], 2, 3) == [[2.0, 3.0], [3.0, 3.0], [3.0, 4.0], [2.0, 4.0]]"}
{"task_id": 56, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n  Calculates the KL divergence between two univariate normal distributions P and Q.\n\n  P ~ N(mu_p, sigma_p^2)\n  Q ~ N(mu_q, sigma_q^2)\n\n  The formula for KL divergence D_KL(P || Q) is:\n  log(sigma_q / sigma_p) + (sigma_p^2 + (mu_p - mu_q)^2) / (2 * sigma_q^2) - 0.5\n\n  Args:\n    mu_p (float): Mean of distribution P.\n    sigma_p (float): Standard deviation of distribution P. Must be positive.\n    mu_q (float): Mean of distribution Q.\n    sigma_q (float): Standard deviation of distribution Q. Must be positive.\n\n  Returns:\n    float: The KL divergence D_KL(P || Q).\n\n  Raises:\n    ValueError: If either sigma_p or sigma_q is not positive.\n  \"\"\"\n    if sigma_p <= 0:\n        raise ValueError('Standard deviation sigma_p must be positive.')\n    if sigma_q <= 0:\n        raise ValueError('Standard deviation sigma_q must be positive.')\n    log_term = np.log(sigma_q / sigma_p)\n    variance_p = sigma_p ** 2\n    variance_q = sigma_q ** 2\n    mean_diff_sq = (mu_p - mu_q) ** 2\n    fraction_term = (variance_p + mean_diff_sq) / (2 * variance_q)\n    constant_term = -0.5\n    kl_div = log_term + fraction_term + constant_term\n    return kl_div\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866"}
{"task_id": 57, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n  Solves the system of linear equations Ax = b using the Gauss-Seidel method.\n\n  Args:\n    A: A square numpy array representing the coefficient matrix.\n    b: A numpy array representing the right-hand side vector.\n    n: The number of iterations to perform.\n    x_ini: An optional numpy array for the initial guess of x.\n           If None, a zero vector is used.\n\n  Returns:\n    A list representing the approximated solution vector x after n iterations,\n    rounded to 4 decimal places.\n  \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    num_equations = len(b)\n    if A.shape[0] != num_equations or A.shape[1] != num_equations:\n        raise ValueError('Matrix A must be square and match the size of vector b.')\n    if x_ini is None:\n        x = np.zeros(num_equations, dtype=float)\n    else:\n        x_ini = np.array(x_ini, dtype=float)\n        if len(x_ini) != num_equations:\n            raise ValueError('Initial guess x_ini must match the size of vector b.')\n        x = x_ini.copy()\n    for k in range(n):\n        for i in range(num_equations):\n            sigma = 0.0\n            for j in range(num_equations):\n                if i != j:\n                    sigma += A[i, j] * x[j]\n            if np.abs(A[i, i]) < 1e-15:\n                print(f'Warning: Near-zero diagonal element A[{i},{i}] encountered. Result may be inaccurate or NaN.')\n            if A[i, i] == 0:\n                pass\n            x[i] = (b[i] - sigma) / A[i, i]\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]"}
{"task_id": 58, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian Elimination\n    with partial pivoting.\n\n    Args:\n        A: A numpy array or list of lists representing the coefficient matrix.\n        b: A numpy array or list representing the constant vector.\n\n    Returns:\n        A list representing the solution vector x, rounded to 4 decimal places.\n        Returns None if the matrix is singular.\n\n    Raises:\n        ValueError: If the matrix A is not square or dimensions are incompatible.\n    \"\"\"\n    try:\n        A = np.array(A, dtype=float)\n        b = np.array(b, dtype=float).reshape(-1, 1)\n    except ValueError:\n        raise ValueError('Input A and b must be convertible to numpy arrays of numbers.')\n    n = len(b)\n    if A.shape != (n, n):\n        raise ValueError('Matrix A must be square and dimensions must match b.')\n    Ab = np.hstack((A, b))\n    for k in range(n - 1):\n        pivot_row = k + np.argmax(np.abs(Ab[k:, k]))\n        if pivot_row != k:\n            Ab[[k, pivot_row]] = Ab[[pivot_row, k]]\n        if np.abs(Ab[k, k]) < 1e-10:\n            print(f'Warning: Matrix may be singular or ill-conditioned (pivot near zero at step {k}).')\n            if np.all(np.abs(Ab[k:, k]) < 1e-10):\n                print('Error: Matrix is singular (zero pivot found).')\n                return None\n        for i in range(k + 1, n):\n            if Ab[k, k] == 0:\n                continue\n            factor = Ab[i, k] / Ab[k, k]\n            Ab[i, k:] = Ab[i, k:] - factor * Ab[k, k:]\n    if np.abs(Ab[n - 1, n - 1]) < 1e-10:\n        print('Error: Matrix is singular (last pivot is zero after elimination).')\n        return None\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        sum_ax = np.dot(Ab[i, i + 1:n], x[i + 1:n])\n        x[i] = (Ab[i, n] - sum_ax) / Ab[i, i]\n    solution = np.round(x, 4).tolist()\n    return solution\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]"}
{"task_id": 59, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        \"\"\"\n        Initializes the LSTM network.\n\n        Args:\n            input_size (int): The dimensionality of the input features.\n            hidden_size (int): The dimensionality of the hidden state and cell state.\n        \"\"\"\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        concat_size = hidden_size + input_size\n        self.Wf = np.random.randn(hidden_size, concat_size)\n        self.Wi = np.random.randn(hidden_size, concat_size)\n        self.Wc = np.random.randn(hidden_size, concat_size)\n        self.Wo = np.random.randn(hidden_size, concat_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def _sigmoid(self, x):\n        \"\"\"Sigmoid activation function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def _tanh(self, x):\n        \"\"\"Hyperbolic tangent activation function.\"\"\"\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step,\n        as well as the final hidden state and final cell state.\n\n        Args:\n            x (list or np.ndarray): Input sequence. Should be iterable, where each element\n                                     is an input vector at a time step. Each input vector\n                                     should have shape (input_size,) or (input_size, 1).\n                                     If x is a 2D numpy array, it should be (seq_len, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state, shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state, shape (hidden_size, 1).\n\n        Returns:\n            tuple: A tuple containing:\n                - hidden_states (list): A list of hidden states (as lists) at each time step, rounded to 4 decimals.\n                - final_hidden_state (list): The final hidden state (as a list), rounded to 4 decimals.\n                - final_cell_state (list): The final cell state (as a list), rounded to 4 decimals.\n        \"\"\"\n        h_prev = np.asarray(initial_hidden_state).reshape(self.hidden_size, 1)\n        c_prev = np.asarray(initial_cell_state).reshape(self.hidden_size, 1)\n        hidden_states_over_time = []\n        if isinstance(x, np.ndarray):\n            seq_len = x.shape[0]\n        else:\n            seq_len = len(x)\n        for t in range(seq_len):\n            x_t = np.asarray(x[t]).reshape(self.input_size, 1)\n            concat_input = np.vstack((h_prev, x_t))\n            f_t = self._sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n            i_t = self._sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n            c_tilde_t = self._tanh(np.dot(self.Wc, concat_input) + self.bc)\n            c_t = f_t * c_prev + i_t * c_tilde_t\n            o_t = self._sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n            h_t = o_t * self._tanh(c_t)\n            hidden_states_over_time.append(h_t)\n            h_prev = h_t\n            c_prev = c_t\n        final_hidden_state = h_prev\n        final_cell_state = c_prev\n        rounded_hidden_states = [np.round(h, 4).tolist() for h in hidden_states_over_time]\n        rounded_final_hidden_state = np.round(final_hidden_state, 4).tolist()\n        rounded_final_cell_state = np.round(final_cell_state, 4).tolist()\n        return (rounded_hidden_states, rounded_final_hidden_state, rounded_final_cell_state)\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.7370]]\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.1661], [0.4030]]\ninput_sequence = np.array([[1, 3], [2, 4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert final_h == [[0.8543], [0.9567]]"}
{"task_id": 60, "completion_id": 0, "passed": false, "result": "failed: name 'compute_tf_idf' is not defined", "solution": "import numpy as np\n\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]"}
{"task_id": 61, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    The F-score is the harmonic mean of precision and recall, weighted by beta.\n    F_beta = (1 + beta^2) * (precision * recall) / ((beta^2 * precision) + recall)\n\n    :param y_true: Numpy array of true labels (binary, 1 for positive, 0 for negative)\n    :param y_pred: Numpy array of predicted labels (binary, 1 for positive, 0 for negative)\n    :param beta: The weight of recall vs precision. beta > 1 favors recall, beta < 1 favors precision. beta=1 is F1-score.\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])):\n        raise ValueError('y_true contains non-binary values.')\n    if not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('y_pred contains non-binary values.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision_denominator = tp + fp\n    if precision_denominator == 0:\n        precision = 0.0\n    else:\n        precision = tp / precision_denominator\n    recall_denominator = tp + fn\n    if recall_denominator == 0:\n        recall = 0.0\n    else:\n        recall = tp / recall_denominator\n    beta_squared = beta ** 2\n    f_score_denominator = beta_squared * precision + recall\n    if f_score_denominator == 0:\n        f_beta_score = 0.0\n    else:\n        f_beta_score = (1 + beta_squared) * (precision * recall) / f_score_denominator\n    return round(f_beta_score, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1])\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513"}
{"task_id": 62, "completion_id": 0, "passed": false, "result": "failed: name 'SimpleRNN' is not defined", "solution": "import numpy as np\n\nnp.random.seed(42)\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n# Train the RNN over multiple epochs\n\nfor epoch in range(100): \n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.2414]], [[3.1845]], [[4.0431]], [[4.5742]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[2.422]], [[3.4417]], [[3.613]], [[4.5066]]]\nnp.random.seed(42)\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    output = np.round(output, 4).tolist()\n\nassert output == [[[3.2842], [5.9353]], [[3.6039], [6.8201]], [[3.5259], [6.5828]], [[3.6134], [6.8492]]]"}
{"task_id": 63, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix (NumPy array)\n    :param b: Right-hand side vector (NumPy array)\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (NumPy array, default is zero vector)\n    :param tol: Convergence tolerance based on the L2 norm of the residual\n    :return: Solution vector x (list rounded to 8 decimal places)\n    \"\"\"\n    A = np.asarray(A, dtype=np.float64)\n    b = np.asarray(b, dtype=np.float64)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Matrix A and vector b dimensions must match.')\n    if x0 is None:\n        x = np.zeros_like(b, dtype=np.float64)\n    else:\n        x = np.asarray(x0, dtype=np.float64).copy()\n        if x.shape != b.shape:\n            raise ValueError('Initial guess x0 must have the same shape as b.')\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    if np.sqrt(rs_old) < tol:\n        return np.round(x, 8).tolist()\n    for k in range(n):\n        Ap = A @ p\n        pAp = p @ Ap\n        if pAp <= 0:\n            print(f'Warning: A might not be positive definite or convergence stalled (p @ Ap = {pAp}).')\n            break\n        alpha = rs_old / pAp\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = r @ r\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"}
{"task_id": 64, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    Gini Impurity is a measure of how often a randomly chosen element\n    from the set would be incorrectly labeled if it was randomly labeled\n    according to the distribution of labels in the subset.\n    Formula: Gini = 1 - \u03a3 (p_i)^2\n    where p_i is the proportion of items belonging to class i.\n\n    :param y: List of class labels (integers).\n    :return: Gini Impurity as a float, rounded to three decimal places.\n             Returns 0.0 if the list is empty or contains only one class.\n    \"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    counts = Counter(y)\n    gini = 1.0\n    for label in counts:\n        p_i = counts[label] / n_samples\n        gini -= p_i ** 2\n    return round(gini, 3)\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571"}
{"task_id": 65, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections.abc\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix. Assumes a list of lists\n                         where inner lists represent rows. Can handle empty matrix or\n                         matrix with zero rows/columns.\n    :return: A tuple containing (values array, column indices array, row pointer array)\n             - values: List of non-zero elements in row-major order.\n             - column_indices: List of column indices corresponding to the 'values'.\n             - row_pointer: List indicating the start index in 'values'/'column_indices'\n                            for each row. Length is num_rows + 1. The last element\n                            is the total number of non-zero elements.\n    \"\"\"\n    if not isinstance(dense_matrix, collections.abc.Sequence):\n        raise TypeError('Input must be a sequence (e.g., list of lists).')\n    if not dense_matrix:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    nnz_count = 0\n    for r in range(num_rows):\n        if not isinstance(dense_matrix[r], collections.abc.Sequence):\n            raise TypeError(f'Row {r} is not a sequence (e.g., list).')\n        for c in range(len(dense_matrix[r])):\n            element = dense_matrix[r][c]\n            try:\n                if element != 0:\n                    values.append(element)\n                    column_indices.append(c)\n                    nnz_count += 1\n            except TypeError:\n                print(f'Warning: Element at ({r},{c}) with value {element} caused comparison error. Treating as zero.')\n        row_pointer.append(nnz_count)\n    assert len(row_pointer) == num_rows + 1\n    assert row_pointer[-1] == len(values)\n    assert len(values) == len(column_indices)\n    return (values, column_indices, row_pointer)\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])"}
{"task_id": 66, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    if len(v) != len(L):\n        raise ValueError('Vectors v and L must have the same dimension.')\n    dot_L_L = sum((Li * Li for Li in L))\n    if dot_L_L == 0:\n        return [0.0] * len(L)\n    dot_v_L = sum((vi * Li for (vi, Li) in zip(v, L)))\n    scalar = dot_v_L / dot_L_L\n    projection_v_onto_L = [scalar * Li for Li in L]\n    rounded_projection = [round(component, 3) for component in projection_v_onto_L]\n    return rounded_projection\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]"}
{"task_id": 67, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n             values: List of non-zero elements in column-major order.\n             row_indices: List of row indices corresponding to each value.\n             column_pointer: List where column_pointer[j] is the index in 'values'\n                             where the elements of column j start. The last element\n                             is the total number of non-zero elements.\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for j in range(num_cols):\n        for i in range(num_rows):\n            element = dense_matrix[i][j]\n            if element != 0:\n                values.append(element)\n                row_indices.append(i)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])"}
{"task_id": 68, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates a basis for the column space (image) of a matrix A.\n\n    The column space consists of all linear combinations of the columns of A.\n    This function identifies a set of linearly independent columns from A\n    that span this space, effectively finding a basis for the image of A.\n    The method implicitly uses the concept that pivot columns in the Row\n    Echelon Form correspond to linearly independent columns in the original\n    matrix. Instead of explicitly computing REF/RREF, it iteratively checks\n    the rank as columns are added.\n\n    Args:\n        A: A list of lists or numpy array representing the matrix.\n           Each inner list represents a row of the matrix.\n\n    Returns:\n        A list of lists representing the basis vectors for the column space.\n        These basis vectors are columns taken directly from the original matrix A.\n        The resulting matrix of basis vectors is rounded to 8 decimal places\n        before being converted to a list. Returns an empty list if the column\n        space is trivial (only the zero vector, e.g., for a zero matrix)\n        or if the input matrix A has zero columns.\n\n    Raises:\n        ValueError: If the input cannot be converted to a 2D NumPy array\n                    or if the input is ambiguous (e.g., a 1D array).\n    \"\"\"\n    try:\n        A = np.array(A, dtype=float)\n    except ValueError:\n        raise ValueError('Input could not be converted to a NumPy array.')\n    if A.ndim == 0:\n        raise ValueError('Input must be a list of lists or 2D array, not a scalar.')\n    elif A.ndim == 1:\n        if A.size == 0:\n            return []\n        raise ValueError('Input appears to be 1D. Please provide a list of lists for a 2D matrix.')\n    (num_rows, num_cols) = A.shape\n    if num_rows == 0 or num_cols == 0:\n        return []\n    basis_indices = []\n    current_rank = 0\n    for j in range(num_cols):\n        potential_basis_indices = basis_indices + [j]\n        temp_matrix = A[:, potential_basis_indices]\n        tol = max(temp_matrix.shape) * np.finfo(temp_matrix.dtype).eps\n        new_rank = np.linalg.matrix_rank(temp_matrix, tol=tol)\n        if new_rank > current_rank:\n            basis_indices.append(j)\n            current_rank = new_rank\n    if not basis_indices:\n        return []\n    basis_matrix = A[:, basis_indices]\n    basis_matrix_rounded = np.round(basis_matrix, 8)\n    return basis_matrix_rounded.tolist()\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]"}
{"task_id": 69, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n  Calculates the R-squared (coefficient of determination) value for a regression model.\n\n  Args:\n    y_true: A numpy array of true target values.\n    y_pred: A numpy array of predicted target values.\n\n  Returns:\n    The R-squared value rounded to three decimal places.\n    Returns 1.0 if the variance of y_true is zero and the predictions are perfect.\n    Returns 0.0 if the variance of y_true is zero but predictions are not perfect.\n  \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    mean_y_true = np.mean(y_true)\n    ss_tot = np.sum((y_true - mean_y_true) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    if ss_tot == 0:\n        if ss_res == 0:\n            r2 = 1.0\n        else:\n            r2 = 0.0\n    else:\n        r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0"}
{"task_id": 70, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D matrix.\n\n    Args:\n        img: A list of lists representing the image matrix. Each inner list is a row,\n             and each element is an integer pixel value (0-255).\n\n    Returns:\n        The average brightness rounded to two decimal places (float),\n        or -1 if the input is invalid (empty, inconsistent row lengths,\n        invalid pixel values).\n    \"\"\"\n    if not img:\n        return -1\n    num_rows = len(img)\n    if not img[0]:\n        for i in range(1, num_rows):\n            if img[i]:\n                return -1\n        return -1\n    expected_cols = len(img[0])\n    if expected_cols == 0:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    for i in range(num_rows):\n        row = img[i]\n        if len(row) != expected_cols:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int):\n                pass\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    rounded_average = round(average_brightness, 2)\n    return rounded_average\nassert calculate_brightness([]) == -1\nassert calculate_brightness([[100, 200], [150]]) == -1\nassert calculate_brightness([[100, 300]]) == -1\nassert calculate_brightness([[100, -1]]) == -1\nassert calculate_brightness([[128]]) == 128.0\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0"}
{"task_id": 71, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport warnings\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between true and predicted values.\n\n    Args:\n        y_true: Array-like structure (list, tuple, numpy array) of actual values.\n        y_pred: Array-like structure (list, tuple, numpy array) of predicted values.\n\n    Returns:\n        float: The calculated RMSE rounded to three decimal places.\n\n    Raises:\n        TypeError: If inputs are not array-like or contain non-numeric values.\n        ValueError: If input arrays are empty or have mismatched shapes.\n    \"\"\"\n    try:\n        y_true_np = np.array(y_true, dtype=np.float64)\n        y_pred_np = np.array(y_pred, dtype=np.float64)\n    except (TypeError, ValueError) as e:\n        raise TypeError(f'Inputs must be array-like and contain numeric values. Error: {e}')\n    if y_true_np.shape != y_pred_np.shape:\n        raise ValueError(f'Input arrays must have the same shape. Got shapes {y_true_np.shape} and {y_pred_np.shape}.')\n    if y_true_np.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_errors = (y_true_np - y_pred_np) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    if mean_squared_error < 0:\n        warnings.warn('Mean Squared Error is negative, possibly due to floating point issues. Clamping to 0 before sqrt.')\n        mean_squared_error = 0.0\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0"}
{"task_id": 72, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n  Calculates the Jaccard Index (Intersection over Union) for binary arrays.\n\n  The Jaccard Index is a measure of similarity between two sets, defined as\n  the size of the intersection divided by the size of the union of the sets.\n  In the context of binary classification, it measures the overlap between\n  the true positive instances and the predicted positive instances.\n\n  Args:\n    y_true: A numpy array or list of true binary labels (0s and 1s).\n    y_pred: A numpy array or list of predicted binary labels (0s and 1s).\n            Must be the same length as y_true.\n\n  Returns:\n    float: The Jaccard Index, rounded to three decimal places.\n           Returns 0.0 if the union of y_true and y_pred is empty (i.e.,\n           both arrays contain only zeros).\n\n  Raises:\n    ValueError: If y_true and y_pred have different lengths.\n  \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    y_true_bool = y_true.astype(bool)\n    y_pred_bool = y_pred.astype(bool)\n    intersection = np.sum(y_true_bool & y_pred_bool)\n    union = np.sum(y_true_bool | y_pred_bool)\n    if union == 0:\n        jaccard = 0.0\n    else:\n        jaccard = intersection / union\n    return round(jaccard, 3)\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2"}
{"task_id": 73, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n  Calculates the Dice Score (F1-score) for binary classification.\n\n  The Dice Score measures the similarity between two sets (true positives vs.\n  predicted positives). It is defined as:\n  2 * |Intersection(A, B)| / (|A| + |B|)\n  which translates to:\n  2 * TP / ( (TP + FN) + (TP + FP) ) = 2 * TP / (2 * TP + FN + FP)\n\n  Args:\n    y_true: A numpy array of ground truth binary labels (0 or 1).\n    y_pred: A numpy array of predicted binary labels (0 or 1).\n\n  Returns:\n    The Dice Score as a float rounded to 3 decimal places.\n    Returns 1.0 if both y_true and y_pred are all zeros (perfect agreement\n    on the absence of positives), avoiding division by zero.\n  \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    intersection = np.sum(y_true_f * y_pred_f)\n    sum_true = np.sum(y_true_f)\n    sum_pred = np.sum(y_pred_f)\n    denominator = sum_true + sum_pred\n    if denominator == 0:\n        if intersection == 0:\n            return 1.0\n        else:\n            return 0.0\n    dice = 2.0 * intersection / denominator\n    return round(dice, 3)\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\nassert dice_score(np.array([1]), np.array([1])) == 1.0\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0"}
{"task_id": 74, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n  Generates a composite hypervector for a given dataset row using HDC.\n\n  Each feature is represented by binding hypervectors for the feature name\n  and its value. The hypervectors for the values are created using the\n  same feature seed provided in the random_seeds dictionary. All feature\n  hypervectors are then bundled to create a composite hypervector for the row.\n\n  Args:\n    row: A dictionary representing a dataset row {feature_name: value}.\n    dim: The dimensionality of the hypervectors.\n    random_seeds: A dictionary {feature_name: seed} for reproducibility.\n\n  Returns:\n    A list representing the composite hypervector for the row.\n  \"\"\"\n    composite_hv = np.zeros(dim, dtype=int)\n    for (feature_name, feature_value) in row.items():\n        if feature_name not in random_seeds:\n            print(f\"Warning: Seed not found for feature '{feature_name}'. Skipping.\")\n            continue\n        feature_seed = random_seeds[feature_name]\n        np.random.seed(feature_seed)\n        hv_feature_name = 2 * np.random.randint(0, 2, size=dim) - 1\n        hv_feature_value = 2 * np.random.randint(0, 2, size=dim) - 1\n        bound_hv = hv_feature_name * hv_feature_value\n        composite_hv += bound_hv\n    return composite_hv.tolist()\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]"}
{"task_id": 75, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\nimport collections\ndef confusion_matrix(data):\n    \"\"\"\n  Generates a 2x2 confusion matrix for binary classification.\n\n  The matrix structure is:\n  [[TN, FP],\n   [FN, TP]]\n\n  Where:\n  TN (True Negative): Actual = 0, Predicted = 0\n  FP (False Positive): Actual = 0, Predicted = 1\n  FN (False Negative): Actual = 1, Predicted = 0\n  TP (True Positive): Actual = 1, Predicted = 1\n\n  Args:\n    data: A list of lists, where each inner list represents a pair\n          [y_true, y_pred] for one observation. Labels are assumed\n          to be 0 or 1.\n\n  Returns:\n    A 2x2 confusion matrix represented as a list of lists: [[TN, FP], [FN, TP]].\n  \"\"\"\n    tn = 0\n    fp = 0\n    fn = 0\n    tp = 0\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 1 and y_pred == 1:\n            tp += 1\n    matrix = [[tn, fp], [fn, tp]]\n    return matrix\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]"}
{"task_id": 76, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n    \"\"\"\n    Calculates the cosine similarity between two non-zero vectors.\n\n    Cosine similarity measures the cosine of the angle between two vectors,\n    indicating their directional similarity.\n\n    Args:\n        v1: A NumPy array representing the first vector.\n        v2: A NumPy array representing the second vector.\n\n    Returns:\n        A float representing the cosine similarity, rounded to three\n        decimal places. Returns 0.0 if either vector has zero magnitude.\n\n    Raises:\n        ValueError: If the vectors have different shapes (implicitly raised by np.dot).\n                    If vectors are empty (implicitly handled by norm calculation or dot product).\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        return 0.0\n    similarity = dot_product / (norm_v1 * norm_v2)\n    similarity = np.clip(similarity, -1.0, 1.0)\n    return round(similarity, 3)\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0"}
{"task_id": 77, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "from collections import Counter\nfrom collections import Counter\nimport math\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates performance metrics for a binary classification model.\n\n    Args:\n        actual: List of actual class labels (0 for negative, 1 for positive).\n        predicted: List of predicted class labels (0 for negative, 1 for positive).\n\n    Returns:\n        A tuple containing:\n        - confusion_matrix: A 2x2 list of lists [[TN, FP], [FN, TP]].\n        - accuracy: Model accuracy (rounded to 3 decimal places).\n        - f1_score: Model F1 score (rounded to 3 decimal places).\n        - specificity: Model specificity (rounded to 3 decimal places).\n        - negative_predictive_value: Model NPV (rounded to 3 decimal places).\n\n    Raises:\n        ValueError: If the input lists have different lengths or are empty.\n        ValueError: If input lists contain values other than 0 or 1.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('Input lists must have the same length.')\n    if not actual:\n        raise ValueError('Input lists cannot be empty.')\n    (tn, fp, fn, tp) = (0, 0, 0, 0)\n    for (a, p) in zip(actual, predicted):\n        if a not in [0, 1] or p not in [0, 1]:\n            raise ValueError('Input lists must contain only 0 or 1.')\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total_population = tn + fp + fn + tp\n    accuracy = (tp + tn) / total_population if total_population > 0 else 0.0\n    precision_denominator = tp + fp\n    precision = tp / precision_denominator if precision_denominator > 0 else 0.0\n    recall_denominator = tp + fn\n    recall = tp / recall_denominator if recall_denominator > 0 else 0.0\n    f1_denominator = precision + recall\n    f1_score = 2 * (precision * recall) / f1_denominator if f1_denominator > 0 else 0.0\n    specificity_denominator = tn + fp\n    specificity = tn / specificity_denominator if specificity_denominator > 0 else 0.0\n    npv_denominator = tn + fn\n    negative_predictive_value = tn / npv_denominator if npv_denominator > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)"}
{"task_id": 78, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates descriptive statistics for a given dataset.\n\n    Args:\n        data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n        dict: A dictionary containing the calculated statistics:\n              - 'mean': The average value.\n              - 'median': The middle value of the sorted data.\n              - 'mode': The most frequently occurring value(s). If multiple modes\n                        exist, returns the smallest mode. If all values are unique,\n                        it returns the smallest value as mode based on np.unique behavior.\n              - 'variance': The population variance.\n              - 'standard_deviation': The population standard deviation.\n              - '25th_percentile': The value below which 25% of the data falls.\n              - '50th_percentile': The median value.\n              - '75th_percentile': The value below which 75% of the data falls.\n              - 'interquartile_range': The difference between the 75th and 25th percentiles.\n              All calculated float values except mode are rounded to four decimal places.\n\n    Raises:\n        ValueError: If the input data is empty or cannot be converted to a numeric array.\n        TypeError: If the input data is not list-like or array-like.\n    \"\"\"\n    if data is None or len(data) == 0:\n        raise ValueError('Input data cannot be empty.')\n    try:\n        data_array = np.asarray(data, dtype=float)\n        if np.isnan(data_array).all() and (not np.isnan(data).all()):\n            raise ValueError('Input data could not be converted to numeric.')\n        data_array = data_array[~np.isnan(data_array)]\n        if data_array.size == 0:\n            raise ValueError('Input data contains only NaNs or non-numeric values.')\n    except (TypeError, ValueError) as e:\n        raise TypeError(f'Input data must be list-like or array-like containing numerical values. Error: {e}') from e\n    mean_val = np.mean(data_array)\n    median_val = np.median(data_array)\n    (unique_values, counts) = np.unique(data_array, return_counts=True)\n    max_count_index = np.argmax(counts)\n    mode_val = unique_values[max_count_index]\n    variance_val = np.var(data_array)\n    std_dev_val = np.std(data_array)\n    (p25_unrounded, p50_unrounded, p75_unrounded) = np.percentile(data_array, [25, 50, 75])\n    iqr_val = p75_unrounded - p25_unrounded\n    rounding_digits = 4\n    mean_val = round(mean_val, rounding_digits)\n    median_val = round(median_val, rounding_digits)\n    variance_val = round(variance_val, rounding_digits)\n    std_dev_val = round(std_dev_val, rounding_digits)\n    p25_val = round(p25_unrounded, rounding_digits)\n    p50_val = round(p50_unrounded, rounding_digits)\n    p75_val = round(p75_unrounded, rounding_digits)\n    iqr_val = round(iqr_val, rounding_digits)\n    results = {'mean': mean_val, 'median': median_val, 'mode': mode_val, 'variance': variance_val, 'standard_deviation': std_dev_val, '25th_percentile': p25_val, '50th_percentile': p50_val, '75th_percentile': p75_val, 'interquartile_range': iqr_val}\n    return results\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}"}
{"task_id": 79, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n\n    The formula is: P(X=k) = C(n, k) * (p^k) * ((1-p)^(n-k))\n    where C(n, k) is the number of combinations of n items taken k at a time (n choose k).\n\n    :param n: Total number of trials (non-negative integer)\n    :param k: Number of successes (non-negative integer)\n    :param p: Probability of success on each trial (float between 0 and 1)\n    :return: Probability of k successes in n trials, rounded to five decimal places.\n    :raises ValueError: If inputs n, k are not non-negative integers or p is not between 0 and 1.\n    \"\"\"\n    if not isinstance(n, int) or n < 0:\n        raise ValueError(\"Number of trials 'n' must be a non-negative integer.\")\n    if not isinstance(k, int) or k < 0:\n        raise ValueError(\"Number of successes 'k' must be a non-negative integer.\")\n    if not isinstance(p, (int, float)) or not 0 <= p <= 1:\n        raise ValueError(\"Probability 'p' must be a float between 0 and 1.\")\n    if k > n:\n        return 0.0\n    try:\n        combinations = math.comb(n, k)\n    except AttributeError:\n        if k < 0 or k > n:\n            combinations = 0\n        else:\n            combinations = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n    except ValueError:\n        return 0.0\n    prob_success_part = p ** k\n    prob_failure_part = (1 - p) ** (n - k)\n    probability = combinations * prob_success_part * prob_failure_part\n    return round(probability, 5)\nassert binomial_probability(6, 2, 0.5) == 0.23438\nassert binomial_probability(6, 4, 0.7) == 0.32414\nassert binomial_probability(3, 3, 0.9) == 0.729\nassert binomial_probability(5, 0, 0.3) == 0.16807\nassert binomial_probability(7, 2, 0.1) == 0.124\nassert binomial_probability(100, 2, 0.1) == 0.00162\nassert binomial_probability(2, 2, 0.1) == 0.01"}
{"task_id": 80, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n\n    The formula for the PDF of the normal distribution is:\n    f(x | \u03bc, \u03c3) = (1 / (\u03c3 * sqrt(2 * \u03c0))) * exp(- (x - \u03bc)\u00b2 / (2 * \u03c3\u00b2))\n\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution. Must be positive.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    :raises ValueError: If std_dev is not positive.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    variance = std_dev ** 2\n    exponent = -(x - mean) ** 2 / (2 * variance)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nassert normal_pdf(0, 0, 1) == 0.39894\nassert normal_pdf(16, 15, 2.04) == 0.17342\nassert normal_pdf(1, 0, 0.5) == 0.10798"}
{"task_id": 81, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    The formula is P(X=k) = (lambda^k * e^(-lambda)) / k!\n\n    :param k: Number of events (non-negative integer).\n    :param lam: The average rate (mean) of occurrences in a fixed interval (non-negative float or integer).\n    :return: The probability rounded to 5 decimal places.\n    :raises ValueError: If k is negative or not an integer (due to math.factorial).\n                        It's assumed lam is non-negative as required by the distribution's definition.\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('Lambda (lam) must be non-negative.')\n    lambda_power_k = math.pow(lam, k)\n    exp_neg_lambda = math.exp(-lam)\n    k_factorial = math.factorial(k)\n    if k_factorial == 0:\n        pass\n    probability = lambda_power_k * exp_neg_lambda / k_factorial\n    rounded_probability = round(probability, 5)\n    return rounded_probability\nassert poisson_probability(3, 5) == 0.14037\nassert poisson_probability(0, 5) == 0.00674\nassert poisson_probability(2, 10) == 0.00227\nassert poisson_probability(1, 1) == 0.36788\nassert poisson_probability(20, 20) == 0.08884"}
{"task_id": 82, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image using the difference\n    between the maximum and minimum pixel values.\n\n    This is a simple measure of global contrast, sometimes referred to as\n    dynamic range within the context of the image's pixel values.\n\n    Args:\n        img (numpy.ndarray): A 2D NumPy array representing a grayscale image.\n                             Pixel values are typically expected to be within\n                             a standard range (e.g., 0-255 for 8-bit images),\n                             but the function works for any numerical range.\n                             The array should not be empty.\n\n    Returns:\n        number: The difference between the maximum and minimum pixel values\n                in the image. The type will typically match the dtype of\n                the input array's elements (e.g., int, float) or be\n                promoted by NumPy during subtraction. Returns 0 if the\n                image has zero size (though np.max/min would raise error).\n\n    Raises:\n        ValueError: If the input array `img` is empty, as np.max/np.min\n                    cannot operate on an empty sequence.\n        TypeError: If the input is not a NumPy array or cannot be processed\n                   by np.max/np.min.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        try:\n            img = np.asarray(img)\n        except Exception as e:\n            raise TypeError(f'Input must be a numpy array or convertible. Error: {e}')\n    if img.size == 0:\n        raise ValueError('Input image array cannot be empty.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50"}
{"task_id": 83, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        number: The dot product of the two vectors (typically float or int).\n\n    Raises:\n        TypeError: If the inputs are not numpy arrays.\n        ValueError: If the input arrays are not 1-dimensional\n                    or do not have the same length.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError('Both inputs must be NumPy ndarrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both input arrays must be 1-dimensional.')\n    if vec1.shape != vec2.shape:\n        raise ValueError(f'Input vectors must have the same length. Got shapes {vec1.shape} and {vec2.shape}.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21"}
{"task_id": 84, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List\ndef phi_transform(data: List[float], degree: int) -> List[List[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (List[float]): A list of numerical values (data points) to transform. Each value is treated as a single feature.\n        degree (int): The degree of the polynomial expansion. Features generated will be x^0, x^1, ..., x^degree.\n\n    Returns:\n        List[List[float]]: A nested list where each inner list contains the\n                           polynomial features (from degree 0 up to the specified degree)\n                           of the corresponding data point, rounded to 8 decimal places.\n                           Returns an empty list if the degree is less than 0.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = []\n        for p in range(degree + 1):\n            feature_value = round(x ** p, 8)\n            features.append(feature_value)\n        transformed_data.append(features)\n    return transformed_data\nassert phi_transform([], 2) == []\nassert phi_transform([1.0, 2.0], -1) == []\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]"}
{"task_id": 85, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Union\ndef pos_encoding(position: int, d_model: int) -> Union[List[List[float]], int]:\n    \"\"\"\n    Calculates the Positional Encoding matrix for a Transformer model.\n\n    As described in \"Attention Is All You Need\", the positional encodings\n    are calculated using sine and cosine functions of different frequencies.\n    PE(pos, 2i) = sin(pos / 10000^(2i / d_model))\n    PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))\n\n    Args:\n        position: The maximum sequence length (number of positions).\n                  Must be greater than 0.\n        d_model: The dimensionality of the model's embeddings.\n                 Must be greater than 0.\n\n    Returns:\n        A list of lists representing the positional encoding matrix\n        of shape (position, d_model) and dtype float16.\n        Returns -1 if input parameters are invalid (position <= 0 or d_model <= 0).\n    \"\"\"\n    if not isinstance(position, int) or position <= 0:\n        return -1\n    if not isinstance(d_model, int) or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float64)\n    pos_indices = np.arange(position, dtype=np.float64)[:, np.newaxis]\n    dim_indices = np.arange(0, d_model, 2, dtype=np.float64)\n    angle_rates = 1 / np.power(10000.0, dim_indices / np.float64(d_model))\n    angle_rads = pos_indices * angle_rates\n    pe[:, 0::2] = np.sin(angle_rads)\n    num_pairs = d_model // 2\n    if num_pairs > 0:\n        pe[:, 1::2] = np.cos(angle_rads[:, :num_pairs])\n    pe_float16 = pe.astype(np.float16)\n    return pe_float16.tolist()\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\nassert pos_encoding(0, 0) == -1\nassert pos_encoding(2, -1) == -1"}
{"task_id": 86, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef model_fit_quality(training_accuracy: float, test_accuracy: float) -> int:\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n\n    Rules:\n    - Overfitting (1): training_accuracy - test_accuracy > 0.2\n    - Underfitting (-1): training_accuracy < 0.7 AND test_accuracy < 0.7\n    - Good fit (0): Neither of the above conditions is true.\n\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, 1 for Overfitting, -1 for Underfitting, or 0 for Good fit.\n    \"\"\"\n    if not (0 <= training_accuracy <= 1 and 0 <= test_accuracy <= 1):\n        raise ValueError('Accuracy values must be between 0 and 1.')\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\nassert model_fit_quality(0.95, 0.65) == 1\nassert model_fit_quality(0.6, 0.5) == -1\nassert model_fit_quality(0.85, 0.8) == 0\nassert model_fit_quality(0.5, 0.6) == -1\nassert model_fit_quality(0.75, 0.74) == 0"}
{"task_id": 87, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n\n    :param parameter: Current parameter value (scalar or numpy array)\n    :param grad: Current gradient (scalar or numpy array with the same shape as parameter)\n    :param m: First moment estimate (scalar or numpy array with the same shape as parameter)\n    :param v: Second moment estimate (scalar or numpy array with the same shape as parameter)\n    :param t: Current timestep (integer, should start from 1 for the first iteration)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: Exponential decay rate for the first moment estimates (default=0.9)\n    :param beta2: Exponential decay rate for the second moment estimates (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter_list, updated_m_list, updated_v_list)\n             Outputs are rounded to 5 decimal places and converted to lists.\n             If inputs were scalars, outputs will be scalar values (not lists).\n    \"\"\"\n    parameter = np.asarray(parameter)\n    grad = np.asarray(grad)\n    m = np.asarray(m)\n    v = np.asarray(v)\n    if t <= 0:\n        raise ValueError(\"Timestep 't' must be a positive integer (starting from 1).\")\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * np.square(grad)\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    parameter_new = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter_rounded = np.round(parameter_new, 5)\n    updated_m_rounded = np.round(m_new, 5)\n    updated_v_rounded = np.round(v_new, 5)\n    updated_parameter_list = updated_parameter_rounded.tolist()\n    updated_m_list = updated_m_rounded.tolist()\n    updated_v_list = updated_v_rounded.tolist()\n    return (updated_parameter_list, updated_m_list, updated_v_list)\nt = 0\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1, 0.01, 0.8, 0.99) == ([0.99, 1.99], [0.02, 0.04], [0.0001, 0.0004])"}
{"task_id": 88, "completion_id": 0, "passed": false, "result": "failed: 'n_embd'", "solution": "import numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\ndef layer_norm(x: np.ndarray, g: np.ndarray, b: np.ndarray, eps: float=1e-05) -> np.ndarray:\n    \"\"\"Applies Layer Normalization.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x_normalized = (x - mean) / np.sqrt(variance + eps)\n    return g * x_normalized + b\ndef simplified_transformer_forward(tokens: np.ndarray, params: dict, hparams: dict) -> np.ndarray:\n    \"\"\"\n    Performs a simplified forward pass, focusing on embeddings and final steps.\n    This skips the actual multi-head attention and feed-forward blocks for brevity.\n    \"\"\"\n    n_ctx = hparams['n_ctx']\n    n_embd = hparams['n_embd']\n    current_sequence_length = tokens.shape[0]\n    if current_sequence_length > n_ctx:\n        tokens = tokens[-n_ctx:]\n        current_sequence_length = n_ctx\n    token_embeddings = params['wte'][tokens]\n    position_indices = np.arange(current_sequence_length)\n    position_embeddings = params['wpe'][position_indices]\n    x = token_embeddings + position_embeddings\n    x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n    logits = x @ params['wte'].T\n    return logits\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generates text autoregressively using simplified GPT-2 components.\n\n    Args:\n        prompt: The initial text sequence to start generation.\n        n_tokens_to_generate: The number of new tokens to generate.\n\n    Returns:\n        The generated text string including the prompt.\n    \"\"\"\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    context_tokens = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        input_tokens = np.array(context_tokens)\n        logits = simplified_transformer_forward(input_tokens, params, hparams)\n        next_token_logits = logits[-1, :]\n        next_token_id = np.argmax(next_token_logits)\n        context_tokens.append(next_token_id)\n    generated_text = encoder.decode(context_tokens)\n    return generated_text\nnp.random.seed(42)\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\nnp.random.seed(42)\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\nnp.random.seed(42)\nassert gen_text(\"world\", 3) == \"world world world\""}
{"task_id": 89, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    Calculates the enhanced pattern for each crystal using a simplified self-attention mechanism.\n\n    Args:\n        n (int): The number of crystals.\n        crystal_values (list or np.ndarray): A list or array of numeric values for each crystal.\n        dimension (int): The dimension to project Query and Key vectors into.\n\n    Returns:\n        list: A list of floats representing the final weighted pattern for each crystal,\n              rounded to the 4th decimal place.\n\n    Raises:\n        ValueError: If dimension is not positive.\n    \"\"\"\n\n    def softmax(values):\n        \"\"\"\n        Computes the softmax function row-wise for numerical stability.\n\n        Args:\n            values (np.ndarray): A 2D numpy array of scores.\n\n        Returns:\n            np.ndarray: A 2D numpy array with softmax probabilities applied row-wise.\n        \"\"\"\n        values = np.asarray(values)\n        max_vals = np.max(values, axis=1, keepdims=True)\n        exp_values = np.exp(values - max_vals)\n        sum_exp_values = np.sum(exp_values, axis=1, keepdims=True)\n        sum_exp_values = np.where(sum_exp_values == 0, 1, sum_exp_values)\n        probabilities = exp_values / sum_exp_values\n        return probabilities\n    if not isinstance(n, int) or n <= 0:\n        raise ValueError('n must be a positive integer.')\n    if len(crystal_values) != n:\n        raise ValueError('Length of crystal_values must be equal to n.')\n    if not isinstance(dimension, int) or dimension <= 0:\n        raise ValueError('Dimension must be a positive integer.')\n    x = np.array(crystal_values, dtype=np.float64).reshape(-1, 1)\n    W_q = np.random.rand(1, dimension)\n    W_k = np.random.rand(1, dimension)\n    Q = x @ W_q\n    K = x @ W_k\n    scores = Q @ K.T\n    scaled_scores = scores / np.sqrt(dimension)\n    attention_weights = softmax(scaled_scores)\n    final_pattern_vector = attention_weights @ x\n    final_pattern_list = [round(val, 4) for val in final_pattern_vector.flatten()]\n    return final_pattern_list\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]"}
{"task_id": 90, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n  Calculates BM25 scores for each document in the corpus relative to the query.\n\n  Args:\n    corpus: A list of documents, where each document is represented as a list\n            of tokens (strings).\n    query: A list of query tokens (strings).\n    k1: BM25 parameter for term frequency saturation (default: 1.5).\n    b: BM25 parameter for document length normalization (default: 0.75).\n\n  Returns:\n    A list of BM25 scores, one for each document in the corpus, rounded to\n    three decimal places.\n  \"\"\"\n    N = len(corpus)\n    if N == 0:\n        return []\n    doc_lengths = [len(doc) for doc in corpus]\n    avgdl = sum(doc_lengths) / N\n    if avgdl == 0:\n        return [0.0] * N\n    doc_freqs = Counter()\n    term_freqs_in_docs = []\n    unique_query_terms = set(query)\n    for doc in corpus:\n        doc_counts = Counter(doc)\n        term_freqs_in_docs.append(doc_counts)\n        for term in unique_query_terms:\n            if doc_counts[term] > 0:\n                doc_freqs[term] += 1\n    idf = {}\n    for term in unique_query_terms:\n        nq = doc_freqs[term]\n        idf_val = math.log(1 + (N - nq + 0.5) / (nq + 0.5))\n        idf[term] = idf_val\n    bm25_scores = []\n    for i in range(N):\n        doc_score = 0.0\n        doc_len = doc_lengths[i]\n        doc_term_counts = term_freqs_in_docs[i]\n        for term in query:\n            if term not in idf:\n                continue\n            term_freq = doc_term_counts[term]\n            current_idf = idf[term]\n            numerator = term_freq * (k1 + 1)\n            denominator = term_freq + k1 * (1 - b + b * (doc_len / avgdl))\n            if denominator != 0:\n                term_score = current_idf * (numerator / denominator)\n                doc_score += term_score\n        bm25_scores.append(round(doc_score, 3))\n    return bm25_scores\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]"}
{"task_id": 91, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    The F1 score is the harmonic mean of precision and recall.\n    F1 = 2 * (Precision * Recall) / (Precision + Recall)\n    Precision = TP / (TP + FP)\n    Recall = TP / (TP + FN)\n\n    Where:\n        TP (True Positives): Correctly predicted positive instances.\n        FP (False Positives): Incorrectly predicted positive instances (predicted positive, but actually negative).\n        FN (False Negatives): Incorrectly predicted negative instances (predicted negative, but actually positive).\n\n    Args:\n        y_true (list): True labels (ground truth). Assumed to be binary (0 or 1).\n        y_pred (list): Predicted labels. Assumed to be binary (0 or 1).\n\n    Returns:\n        float: The F1 score rounded to three decimal places. Returns 0.0 if\n               there are no positive predictions or no true positive instances\n               to avoid division by zero.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists y_true and y_pred must have the same length.')\n    if not y_true:\n        return 0.0\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            tp += 1\n        elif true_label == 0 and pred_label == 1:\n            fp += 1\n        elif true_label == 1 and pred_label == 0:\n            fn += 1\n    if tp + fp == 0:\n        precision = 0.0\n    else:\n        precision = tp / (tp + fp)\n    if tp + fn == 0:\n        recall = 0.0\n    else:\n        recall = tp / (tp + fn)\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889"}
{"task_id": 92, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nPI = 3.14159\nimport math\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Forecasts power consumption for day 15 based on 10 days of data,\n    accounting for a linear trend and a known daily fluctuation.\n\n    Args:\n        consumption_data: A list or tuple of 10 daily power consumption values.\n\n    Returns:\n        An integer representing the forecasted power consumption for day 15,\n        including fluctuation and a 5% safety margin (rounded up).\n    \"\"\"\n    n = len(consumption_data)\n    if n != 10:\n        raise ValueError('Input data must contain exactly 10 days of measurements.')\n    days = list(range(1, n + 1))\n    detrended_consumption = []\n    for i in range(n):\n        day = days[i]\n        fluctuation = 10 * math.sin(2 * math.pi * day / 10)\n        detrended_value = consumption_data[i] - fluctuation\n        detrended_consumption.append(detrended_value)\n    sum_x = sum(days)\n    sum_y = sum(detrended_consumption)\n    sum_xy = sum((days[i] * detrended_consumption[i] for i in range(n)))\n    sum_x_sq = sum((d * d for d in days))\n    denominator = n * sum_x_sq - sum_x * sum_x\n    if denominator == 0:\n        raise ValueError('Cannot perform linear regression: denominator is zero.')\n    m = (n * sum_xy - sum_x * sum_y) / denominator\n    c = (sum_y - m * sum_x) / n\n    day_to_predict = 15\n    base_prediction_day15 = m * day_to_predict + c\n    fluctuation_day15 = 10 * math.sin(2 * math.pi * day_to_predict / 10)\n    total_prediction_day15 = base_prediction_day15 + fluctuation_day15\n    rounded_prediction = round(total_prediction_day15)\n    safety_margin_percentage = 0.05\n    safety_margin = rounded_prediction * safety_margin_percentage\n    margin_to_add = math.ceil(safety_margin)\n    final_forecast = rounded_prediction + margin_to_add\n    return int(final_forecast)\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35"}
{"task_id": 93, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667"}
{"task_id": 94, "completion_id": 0, "passed": false, "result": "failed: name 'compute_qkv' is not defined", "solution": "import numpy as np\ndef self_attention(Q_head: np.ndarray, K_head: np.ndarray, V_head: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes scaled dot-product self-attention for a single head.\n\n    Args:\n        Q_head: Query matrix for the head. Shape (sequence_length, head_dim)\n        K_head: Key matrix for the head. Shape (sequence_length, head_dim)\n        V_head: Value matrix for the head. Shape (sequence_length, head_dim)\n\n    Returns:\n        Attention output for the head. Shape (sequence_length, head_dim)\n    \"\"\"\n    head_dim = K_head.shape[1]\n    scores = Q_head @ K_head.T\n    scaled_scores = scores / np.sqrt(head_dim)\n    attention_weights = softmax(scaled_scores)\n    output = attention_weights @ V_head\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n\n    Args:\n        X: Input data matrix. Shape (sequence_length, embedding_dim)\n        W_q: Query weight matrix. Shape (embedding_dim, embedding_dim)\n        W_k: Key weight matrix. Shape (embedding_dim, embedding_dim)\n        W_v: Value weight matrix. Shape (embedding_dim, embedding_dim)\n        n_heads: Number of attention heads. embedding_dim must be divisible by n_heads.\n\n    Returns:\n        A Python list representing the multi-head attention output, rounded to the\n        nearest 4th decimal. The underlying array shape before conversion is\n        (sequence_length, embedding_dim).\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    (sequence_length, embedding_dim) = X.shape\n    if embedding_dim % n_heads != 0:\n        raise ValueError(f'embedding_dim ({embedding_dim}) must be divisible by n_heads ({n_heads})')\n    head_dim = embedding_dim // n_heads\n    Q_heads = Q.reshape(sequence_length, n_heads, head_dim).transpose(1, 0, 2)\n    K_heads = K.reshape(sequence_length, n_heads, head_dim).transpose(1, 0, 2)\n    V_heads = V.reshape(sequence_length, n_heads, head_dim).transpose(1, 0, 2)\n    head_outputs = []\n    for i in range(n_heads):\n        q_h = Q_heads[i]\n        k_h = K_heads[i]\n        v_h = V_heads[i]\n        attention_output = self_attention(q_h, k_h, v_h)\n        head_outputs.append(attention_output)\n    stacked_heads = np.stack(head_outputs, axis=0)\n    concatenated_heads = stacked_heads.transpose(1, 0, 2)\n    multi_head_output = concatenated_heads.reshape(sequence_length, -1)\n    rounded_output = np.round(multi_head_output, 4)\n    return rounded_output.tolist()\nnp.random.seed(42)\n\nm, n = 4, 4\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 4\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\n\nm, n = 6, 8\nn_heads = 2\n# Generate input data\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\n# Generate weight matrices\nW_q = np.random.randint(0, 4, size=(n,n))\nW_k = np.random.randint(0, 5, size=(n,n))\nW_v = np.random.randint(0, 6, size=(n,n))\n\nassert multi_head_attention(X, W_q, W_k, W_v, n_heads) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"}
{"task_id": 95, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\nfrom collections import Counter\nimport random\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    The Phi coefficient (\u03c6 or r\u03c6) is a measure of association for two binary\n    variables. It is calculated from a 2x2 contingency table:\n\n              | y = 1 | y = 0 | Row Total |\n              |-------|-------|-----------|\n    | x = 1 |  n11  |  n10  |   n1+     |\n    | x = 0 |  n01  |  n00  |   n0+     |\n    | Col Total |  n+1  |  n+0  |    n      |\n\n    Phi = (n11 * n00 - n10 * n01) / sqrt(n1+ * n0+ * n+1 * n+0)\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n\n    Raises:\n    ValueError: If the input lists are not of the same length or are empty.\n    # Note: Does not explicitly check for non-binary values, assumes valid input.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    if not x:\n        raise ValueError('Input lists cannot be empty.')\n    n11 = 0\n    n10 = 0\n    n01 = 0\n    n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 0 and yi == 0:\n            n00 += 1\n    n1_plus = n11 + n10\n    n0_plus = n01 + n00\n    n_plus1 = n11 + n01\n    n_plus0 = n10 + n00\n    denominator_product = n1_plus * n0_plus * n_plus1 * n_plus0\n    if denominator_product == 0:\n        return 0.0\n    denominator = math.sqrt(denominator_product)\n    numerator = n11 * n00 - n10 * n01\n    phi = numerator / denominator\n    return round(phi, 4)\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913"}
{"task_id": 96, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    The Hard Sigmoid is defined as:\n    f(x) = max(0, min(1, slope * x + intercept))\n\n    A common definition uses slope = 0.2 and intercept = 0.5:\n    f(x) = max(0, min(1, 0.2 * x + 0.5))\n\n    This function is a piecewise linear approximation of the standard sigmoid\n    function (1 / (1 + exp(-x))), which is computationally cheaper as it\n    avoids the exponential calculation.\n\n    - It outputs 0 for x <= -2.5\n    - It outputs 1 for x >= 2.5\n    - It linearly increases from 0 to 1 for -2.5 < x < 2.5\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input, clamped between 0.0 and 1.0.\n    \"\"\"\n    linear_part = 0.2 * x + 0.5\n    clipped_upper = min(1.0, linear_part)\n    output = max(0.0, clipped_upper)\n    return output\nassert hard_sigmoid(.56) == 0.612\nassert hard_sigmoid(3.0) == 1.0\nassert hard_sigmoid(0.0) == 0.5\nassert hard_sigmoid(1.0) == 0.7\nassert hard_sigmoid(-1.0) == 0.3\nassert hard_sigmoid(2.5) == 1.0\nassert hard_sigmoid(-2.5) == 0.0\nassert hard_sigmoid(-3.0) == 0.0"}
{"task_id": 97, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)\nassert elu(0) == 0.0\nassert elu(1) == 1.0\nassert elu(-1) == -0.6321\nassert elu(-1, alpha=2.0) == -1.2642\nassert elu(5) == 5.0\nassert elu(-5) == -0.9933"}
{"task_id": 98, "completion_id": 0, "passed": true, "result": "passed", "solution": "\ndef prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\nassert prelu(2.0) == 2.0\nassert prelu(0.0) == 0.0\nassert prelu(-2.0) == -0.5\nassert prelu(-2.0, alpha=0.1) == -0.2\nassert prelu(-2.0, alpha=1.0) == -2.0"}
{"task_id": 99, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    threshold = 37.0\n    if x > threshold:\n        result = x\n    else:\n        try:\n            result = math.log(1 + math.exp(x))\n        except OverflowError:\n            result = x\n    return round(result, 4)\nassert softplus(0) == 0.6931\nassert softplus(100) == 100.0\nassert softplus(-100) == 0.0\nassert softplus(2) == 2.1269\nassert softplus(-2) == 0.1269"}
{"task_id": 100, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    The Softsign function is defined as f(x) = x / (1 + |x|), where |x| is the\n    absolute value of x. It squashes the input value into the range (-1, 1).\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places.\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    rounded_value = round(softsign_value, 4)\n    return rounded_value\nassert softsign(0) == 0.0\nassert softsign(1) == 0.5\nassert softsign(-1) == -0.5\nassert softsign(100) == 0.9901\nassert softsign(-100) == -0.9901"}
{"task_id": 101, "completion_id": 0, "passed": false, "result": "failed: test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)", "solution": "import numpy as np\nimport numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    The GRPO objective combines the PPO clipped surrogate objective with a KL divergence\n    penalty term relative to a reference policy.\n    Objective = E_t [ min(rho_t * A_t, clip(rho_t, 1 - epsilon, 1 + epsilon) * A_t) ]\n              - beta * E_t [ KL( pi_theta(.|s_t) || pi_theta_ref(.|s_t) ) ]\n\n    The KL divergence term is typically estimated using samples:\n    E_t [ KL(...) ] \u2248 mean( log(pi_theta(a_t|s_t) / pi_theta_ref(a_t|s_t)) )\n    where pi_theta(a_t|s_t) = rho_t * pi_theta_old(a_t|s_t)\n\n    Args:\n        rhos: List or numpy array of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n              Represents rho_t for each sample.\n        A: List or numpy array of advantage estimates (A_i). Represents A_t for each sample.\n        pi_theta_old: List or numpy array representing the old policy probabilities pi_theta_old(o_i | q)\n                      for the action taken (o_i) in context (q).\n        pi_theta_ref: List or numpy array representing the reference policy probabilities pi_ref(o_i | q)\n                      for the action taken (o_i) in context (q).\n        epsilon: Clipping parameter (eps) for the PPO surrogate objective.\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value, rounded to 6 decimal places.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    unclipped_objective = rhos * A\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_objective = clipped_rhos * A\n    ppo_term = np.mean(np.minimum(unclipped_objective, clipped_objective))\n    pi_theta = rhos * pi_theta_old\n    epsilon_stability = 1e-08\n    pi_theta_stable = pi_theta + epsilon_stability\n    pi_theta_ref_stable = pi_theta_ref + epsilon_stability\n    log_ratio_ref = np.log(pi_theta_stable / pi_theta_ref_stable)\n    kl_divergence_estimate = np.mean(log_ratio_ref)\n    grpo_obj_value = ppo_term - beta * kl_divergence_estimate\n    return np.round(grpo_obj_value, 6)\nassert grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01) == 1.032749, \"test case failed: grpo_objective([1.2, 0.8, 1.1], [1.0, 1.0, 1.0], [0.9, 1.1, 1.0], [1.0, 0.5, 1.5], epsilon=0.2, beta=0.01)\"\nassert grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05) == 0.999743, \"test case failed: grpo_objective([0.9, 1.1], [1.0, 1.0], [1.0, 1.0], [0.8, 1.2], epsilon=0.1, beta=0.05)\"\nassert grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02) == 0.882682, \"test case failed: grpo_objective([1.5, 0.5, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.2, 0.7, 1.3], epsilon=0.15, beta=0.02)\"\nassert grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01) == 1.0, \"test case failed: grpo_objective([1.0], [1.0], [1.0], [1.0], epsilon=0.1, beta=0.01)\""}
{"task_id": 102, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    The Swish function is defined as: f(x) = x * sigmoid(x)\n    where sigmoid(x) = 1 / (1 + exp(-x))\n\n    Args:\n        x: Input value (float)\n\n    Returns:\n        The Swish activation value (float), rounded to 4 decimal places.\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)\nassert swish(0) == 0.0, \"test case failed: swish(0)\"\nassert swish(1) == 0.7311, \"test case failed: swish(1)\"\nassert swish(-1) == -0.2689, \"test case failed: swish(-1)\"\nassert swish(10) == 9.9995, \"test case failed: swish(10)\"\nassert swish(-10) == -0.0005, \"test case failed: swish(-10)\""}
{"task_id": 103, "completion_id": 0, "passed": true, "result": "passed", "solution": "import math\nimport numpy as np\nimport math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value (float)\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places (float)\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1.0)\n    rounded_result = round(result, 4)\n    return rounded_result\nassert selu(1.0) == 1.0507, \"test case failed: selu(1.0)\"\nassert selu(0.0) == 0.0, \"test case failed: selu(0.0)\"\nassert selu(-1.0) == -1.1113, \"test case failed: selu(-1.0)\"\nassert selu(5.0) == 5.2535, \"test case failed: selu(5.0)\"\nassert selu(-5.0) == -1.7463, \"test case failed: selu(-5.0)\""}
{"task_id": 104, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D), where N is the number of samples\n           and D is the number of features.\n        weights: Model weights (shape: D).\n        bias: Model bias (scalar).\n\n    Returns:\n        A Python list of binary predictions (0 or 1) for each input sample.\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    predictions_np = (probabilities >= 0.5).astype(int)\n    predictions_list = predictions_np.tolist()\n    return predictions_list\nassert predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0) == [1,1,0], \"test case failed: predict_logistic(np.array([[0, 0], [0.1, 0.1], [-0.1, -0.1]]), np.array([1, 1]), 0)\"\nassert predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1) == [1,0,0], \"test case failed: predict_logistic(np.array([[1, 2, 3], [-1, -2, -3], [0.5, 1, 1.5]]), np.array([0.1, 0.2, 0.3]), -1)\"\nassert predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0) == [1,1,0,0], \"test case failed: predict_logistic(np.array([[1], [2], [-1], [-2]]), np.array([2]), 0)\"\nassert predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0) == [1,0], \"test case failed: predict_logistic(np.array([[1000, 2000], [-1000, -2000]]), np.array([0.1, 0.1]), 0)\""}
{"task_id": 105, "completion_id": 0, "passed": false, "result": "failed: test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[list[float]], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X : np.ndarray, Input feature matrix (N samples x M features).\n        y : np.ndarray, Target labels (N samples), assumed to be integers from 0 to C-1.\n        learning_rate : float, Step size for gradient descent.\n        iterations : int, Number of gradient descent iterations.\n\n    Returns:\n        B : list[list[float]], (M+1) x C updated parameter matrix rounded to 4 floating points. \n                                Each column corresponds to a class. Includes bias term.\n        losses : list[float], Collected values of Cross Entropy loss at each iteration, rounded to 4 floating points.\n    \"\"\"\n    (N, M) = X.shape\n    C = np.max(y) + 1\n    X_bias = np.hstack([np.ones((N, 1)), X])\n    M_bias = M + 1\n    B = np.zeros((M_bias, C))\n    Y_onehot = np.eye(C)[y]\n    losses = []\n    for i in range(iterations):\n        Z = X_bias @ B\n        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n        P = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n        epsilon = 1e-09\n        log_likelihood = -np.log(P[range(N), y] + epsilon)\n        loss = np.sum(log_likelihood) / N\n        losses.append(round(loss, 4))\n        gradient = X_bias.T @ (P - Y_onehot) / N\n        B = B - learning_rate * gradient\n    B_rounded = np.round(B, 4)\n    B_list = B_rounded.tolist()\n    return (B_list, losses)\nassert train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10) == ([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7202, 9.3164, 8.4943, 7.9134, 7.4599, 7.0856, 6.7655, 6.4853, 6.236]), \"test case failed: train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10)\"\nassert train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10) == ([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011]), \"test case failed: train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\""}
{"task_id": 106, "completion_id": 0, "passed": false, "result": "failed: tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)", "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n        X: Feature matrix (n_samples, n_features).\n        y: Target vector (n_samples,). Assumed to be 0 or 1.\n        learning_rate: Step size for gradient descent.\n        iterations: Number of gradient descent steps.\n\n    Returns:\n        A tuple containing:\n            - coefficients: A list of optimized coefficients [bias, w1, w2, ...], rounded to 4 decimals.\n            - losses: A list of Binary Cross Entropy loss values recorded at each iteration, rounded to 4 decimals.\n    \"\"\"\n    (m, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    losses = []\n    epsilon = 1e-09\n    for i in range(iterations):\n        z = np.dot(X, w) + b\n        p = 1 / (1 + np.exp(-z))\n        loss = -(1 / m) * np.sum(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon))\n        losses.append(round(loss, 4))\n        dw = 1 / m * np.dot(X.T, p - y)\n        db = 1 / m * np.sum(p - y)\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n    coefficients = np.concatenate(([b], w))\n    rounded_coefficients = np.round(coefficients, 4)\n    coefficients_list = rounded_coefficients.tolist()\n    return (coefficients_list, losses)\nassert train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10) == ([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223]), \"tese case failed: train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10)\"\nassert train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10) == ([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258]), \"test case failed: train_logreg(np.array([[ 0.76743473, 1.57921282, -0.46947439],[-0.23415337, 1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869, 0.37569802, -0.29169375],[-1.91328024, 0.24196227, -1.72491783],[-1.01283112, -0.56228753, 0.31424733],[-0.1382643 , 0.49671415, 0.64768854],[-0.46341769, 0.54256004, -0.46572975],[-1.4123037 , -0.90802408, 1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10)\""}
{"task_id": 107, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\nimport numpy as np\nimport json\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n\n    Args:\n        Q: Query matrix (sequence_length, d_k)\n        K: Key matrix (sequence_length, d_k)\n        V: Value matrix (sequence_length, d_v)\n        mask: Attention mask (sequence_length, sequence_length).\n              Typically 1 for allowed positions, 0 for masked positions.\n\n    Returns:\n        list: The resulting attention output matrix as a Python list.\n              Shape: (sequence_length, d_v)\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    d_k = K.shape[-1]\n    scaled_scores = scores / np.sqrt(d_k)\n    if mask is not None:\n        if mask.shape != scaled_scores.shape:\n            raise ValueError(f'Mask shape {mask.shape} must match scores shape {scaled_scores.shape}')\n        masked_scaled_scores = np.where(mask == 0, -np.inf, scaled_scores)\n    else:\n        masked_scaled_scores = scaled_scores\n    exp_scores = np.exp(masked_scaled_scores - np.max(masked_scaled_scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    attention_weights = np.nan_to_num(attention_weights)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\nsequence_length = 4\nd_k = 16\nmask = np.tril(np.ones((sequence_length, sequence_length)))\nnp.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [547.0, 490.0, 399.0, 495.0, 485.0, 439.0, 645.0, 393.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0], [471.0, 472.0, 429.0, 538.0, 377.0, 450.0, 531.0, 362.0]]\nnp.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nassert masked_attention(Q, K, V, mask) == [[52.0, 63.0, 48.0, 71.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]"}
{"task_id": 108, "completion_id": 0, "passed": false, "result": "failed: test case failed: disorder([1,1,0,0])", "solution": "\nimport math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors,\n    using Shannon entropy.\n\n    The disorder is 0 if all apples are the same color (or the list is empty/has one apple)\n    and increases as the variety and evenness of color distribution increase.\n\n    Args:\n        apples: A list of integers representing apple colors.\n\n    Returns:\n        A float representing the disorder, rounded to 4 decimal places.\n        Disorder is calculated using the formula for Shannon entropy:\n        H(X) = - sum_{i=1}^{n} [ P(x_i) * log2(P(x_i)) ]\n        where P(x_i) is the proportion of apples of color x_i.\n    \"\"\"\n    n = len(apples)\n    if n <= 1:\n        return 0.0\n    counts = Counter(apples)\n    if len(counts) == 1:\n        return 0.0\n    entropy = 0.0\n    for color in counts:\n        probability = counts[color] / n\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)\nassert disorder([0,0,0,0]) == 0.0, \"test case failed: disorder([0,0,0,0])\"\nassert disorder([1,1,0,0]) == 0.5, \"test case failed: disorder([1,1,0,0])\"\nassert disorder([0,1,2,3]) == 0.75, \"test case failed: disorder([0,1,2,3])\"\nassert disorder([0,0,1,1,2,2,3,3]) == 0.75, \"test case failed: disorder([0,0,1,1,2,2,3,3])\"\nassert disorder([0,0,0,0,0,1,2,3]) == 0.5625, \"test case failed: disorder([0,0,0,0,0,1,2,3])\""}
{"task_id": 109, "completion_id": 0, "passed": false, "result": "failed: Gamma must have shape (3,), but got (1, 1, 3)", "solution": "import numpy as np\nimport numpy as np\nimport json\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n  Performs Layer Normalization on a 3D input tensor.\n\n  Args:\n    X: Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma: Scaling parameter tensor of shape (feature_dim,).\n    beta: Shifting parameter tensor of shape (feature_dim,).\n    epsilon: A small float added to variance to avoid division by zero.\n\n  Returns:\n    A list representing the normalized, scaled, and shifted tensor,\n    rounded to 5 decimal places. The shape is preserved:\n    (batch_size, sequence_length, feature_dim).\n  \"\"\"\n    if X.ndim != 3:\n        raise ValueError(f'Input X must be a 3D tensor, but got shape {X.shape}')\n    feature_dim = X.shape[-1]\n    if gamma.shape != (feature_dim,):\n        raise ValueError(f'Gamma must have shape ({feature_dim},), but got {gamma.shape}')\n    if beta.shape != (feature_dim,):\n        raise ValueError(f'Beta must have shape ({feature_dim},), but got {beta.shape}')\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * X_normalized + beta\n    output_rounded = np.round(output, 5)\n    return output_rounded.tolist()\nassert layer_normalization(np.array([[[0.242, -1.913, -1.725], [-0.562, -1.013, 0.314]], [[-0.908, -1.412, 1.466], [-0.226, 0.068, -1.425]]]), np.array([[[1., 1., 1.]]]), np.array([[[0., 0., 0.]]])) == [[[1.40981, -0.80136, -0.60846], [-0.25714, -1.07574, 1.33288]], [[-0.49672, -0.89835, 1.39507], [0.46714, 0.92241, -1.38955]]]\nassert layer_normalization(np.array([[[-0.544, 0.111, -1.151, 0.376], [-0.601, -0.292, -0.602, 1.852], [-0.013, -1.058, 0.823, -1.221]], [[0.209, -1.96, -1.328, 0.197], [0.738, 0.171, -0.116, -0.301], [-1.479, -0.72, -0.461, 1.057]]]), np.array([[[1., 1., 1., 1.]]]), np.array([[[0., 0., 0., 0.]]])) == [[[-0.40765, 0.6957, -1.43015, 1.1421], [-0.67306, -0.37175, -0.67403, 1.71885], [0.42738, -0.83334, 1.43595, -1.02999]], [[0.97825, -1.30451, -0.63936, 0.96562], [1.5653, 0.12217, -0.6083, -1.07917], [-1.17069, -0.34662, -0.06542, 1.58272]]]\nassert layer_normalization(np.array([[[0.344, -1.763, 0.324, -0.385], [-0.677, 0.612, 1.031, 0.931], [-0.839, -0.309, 0.331, 0.976]], [[-0.479, -0.186, -1.106, -1.196], [0.813, 1.356, -0.072, 1.004], [0.362, -0.645, 0.361, 1.538]]]), np.array([[[0.5, 0.5, 0.5, 0.5]]]), np.array([[[1., 1., 1., 1.]]])) == [[[1.41697, 0.1865, 1.40529, 0.99124], [0.15654, 1.10092, 1.4079, 1.33464], [0.35485, 0.74396, 1.21383, 1.68737]], [[1.31031, 1.65635, 0.56982, 0.46353], [1.03585, 1.5515, 0.19543, 1.21723], [0.97283, 0.32146, 0.97219, 1.73352]]]"}
{"task_id": 110, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\nimport math\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Computes the METEOR score for a candidate translation given a reference translation.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Parameter for balancing precision and recall (higher alpha -> more weight on recall).\n        beta (float): Parameter for controlling the shape of the penalty function (higher beta -> harsher penalty).\n        gamma (float): Parameter for scaling the fragmentation penalty.\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    if len(ref_tokens) == 0 and len(cand_tokens) == 0:\n        return 1.0\n    if len(ref_tokens) == 0 or len(cand_tokens) == 0:\n        return 0.0\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    matches = 0\n    used_ref_indices = set()\n    alignment = {}\n    for (cand_idx, cand_word) in enumerate(cand_tokens):\n        best_ref_idx = -1\n        for (ref_idx, ref_word) in enumerate(ref_tokens):\n            if ref_word == cand_word and ref_idx not in used_ref_indices:\n                best_ref_idx = ref_idx\n                break\n        if best_ref_idx != -1:\n            matches += 1\n            used_ref_indices.add(best_ref_idx)\n            alignment[cand_idx] = best_ref_idx\n    P = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0.0\n    R = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0.0\n    if P == 0.0 and R == 0.0:\n        f_mean = 0.0\n    else:\n        f_mean = P * R / (alpha * P + (1 - alpha) * R)\n    if matches == 0:\n        penalty = 0.0\n    else:\n        chunks = 1\n        sorted_cand_indices = sorted(alignment.keys())\n        last_ref_idx = -2\n        if len(sorted_cand_indices) > 0:\n            last_ref_idx = alignment[sorted_cand_indices[0]]\n        for i in range(1, len(sorted_cand_indices)):\n            current_cand_idx = sorted_cand_indices[i]\n            current_ref_idx = alignment[current_cand_idx]\n            if current_ref_idx != last_ref_idx + 1:\n                chunks += 1\n            last_ref_idx = current_ref_idx\n        fragmentation = chunks / matches\n        penalty = gamma * fragmentation ** beta\n    score = f_mean * (1 - penalty)\n    score = max(0.0, score)\n    return round(score, 3)\nassert meteor_score('The dog barks at the moon', 'The dog barks at the moon') == 0.998\nassert meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky') == 0.625\nassert meteor_score('The sun shines brightly', 'Clouds cover the sky') == 0.125\nassert meteor_score('Birds sing in the trees', 'Birds in the trees sing') == 0.892\n\nassert meteor_score(\"The cat sits on the mat\", \"The cat on the mat sits\") == 0.938"}
{"task_id": 111, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport math\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n  Computes the Pointwise Mutual Information (PMI) between two events X and Y.\n\n  PMI(x, y) = log2 [ P(x, y) / (P(x) * P(y)) ]\n            = log2 [ (joint_counts / total_samples) /\n                     ((total_counts_x / total_samples) * (total_counts_y / total_samples)) ]\n            = log2 [ (joint_counts * total_samples) / (total_counts_x * total_counts_y) ]\n\n  Args:\n    joint_counts: The number of times event X and event Y occurred together.\n    total_counts_x: The total number of times event X occurred.\n    total_counts_y: The total number of times event Y occurred.\n    total_samples: The total number of samples or observations.\n\n  Returns:\n    A float value representing the PMI, rounded to 3 decimal places.\n    Returns -inf if joint_counts is 0 or if either total_counts_x or\n    total_counts_y is 0 (as PMI is undefined or infinitely negative).\n  \"\"\"\n    if total_samples <= 0:\n        raise ValueError('Total samples must be positive.')\n    if joint_counts < 0 or total_counts_x < 0 or total_counts_y < 0:\n        raise ValueError('Counts cannot be negative.')\n    if total_counts_x == 0 or total_counts_y == 0:\n        return -np.inf\n    if joint_counts == 0:\n        return -np.inf\n    numerator = joint_counts * total_samples\n    denominator = total_counts_x * total_counts_y\n    pmi_value = np.log2(numerator / denominator)\n    if np.isfinite(pmi_value):\n        return round(pmi_value, 3)\n    else:\n        return pmi_value\nassert compute_pmi(10, 50, 50, 200) == -0.322\nassert compute_pmi(100, 500, 500, 1000) == -1.322\nassert compute_pmi(100, 400, 600, 1200) == -1\nassert compute_pmi(100, 100, 100, 100) == 0.0\nassert compute_pmi(25, 50, 50, 100) == 0.0\nassert compute_pmi(10, 50, 50, 100) == -1.322\nassert compute_pmi(0, 50, 50, 100) == float('-inf')"}
{"task_id": 112, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n  Performs Min-Max Normalization on a list of integers.\n\n  Scales the values in the input list to the range [0, 1].\n\n  Args:\n    x: A list of integers representing the feature values.\n\n  Returns:\n    A list of floats, where each value is the normalized version\n    of the corresponding input value, rounded to 4 decimal places.\n    Returns an empty list if the input list is empty.\n    Returns a list of 0.0s if all elements in the input list are the same.\n  \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized_x = []\n    denominator = max_val - min_val\n    for val in x:\n        normalized_val = (val - min_val) / denominator\n        normalized_x.append(round(normalized_val, 4))\n    return normalized_x\nassert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0]\nassert min_max([30, 45, 56, 70, 88]) == [0.0, 0.2586, 0.4483, 0.6897, 1.0]\nassert min_max([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0]\nassert min_max([-3, -2, -1, 0, 1, 2, 3]) == [0.0, 0.1667, 0.3333, 0.5, 0.6667, 0.8333, 1.0]\nassert min_max([1,]) == [0.0]"}
{"task_id": 113, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection using NumPy.\n\n    The block performs the following operations:\n    1. Linear transformation with w1: z1 = x @ w1\n    2. ReLU activation: a1 = ReLU(z1)\n    3. Linear transformation with w2: z2 = a1 @ w2\n    4. Shortcut connection: s = z2 + x\n    5. Final ReLU activation: output = ReLU(s)\n\n    Args:\n        x: A 1D NumPy array representing the input feature vector.\n           Shape: (input_dim,)\n        w1: A 2D NumPy array representing the weights of the first layer.\n            Shape must be (input_dim, hidden_dim) for some hidden_dim.\n        w2: A 2D NumPy array representing the weights of the second layer.\n            Shape must be (hidden_dim, input_dim) to match the input dimension\n            for the shortcut connection.\n\n    Returns:\n        A list representing the output of the residual block, rounded to\n        4 decimal places.\n\n    Raises:\n        ValueError: If input dimensions are incompatible for matrix multiplication\n                    or the shortcut connection.\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError(f'Input x must be a 1D array, but got shape {x.shape}')\n    if w1.ndim != 2 or w2.ndim != 2:\n        raise ValueError(f'Weights w1 and w2 must be 2D arrays.')\n    if x.shape[0] != w1.shape[0]:\n        raise ValueError(f'Input dimension ({x.shape[0]}) does not match w1 input dimension ({w1.shape[0]}).')\n    if w1.shape[1] != w2.shape[0]:\n        raise ValueError(f'w1 output dimension ({w1.shape[1]}) does not match w2 input dimension ({w2.shape[0]}).')\n    if w2.shape[1] != x.shape[0]:\n        raise ValueError(f'w2 output dimension ({w2.shape[1]}) does not match input dimension ({x.shape[0]}) for shortcut connection.')\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    shortcut_output = z2 + x\n    final_output = np.maximum(0, shortcut_output)\n    final_output_rounded = np.round(final_output, 4)\n    output_list = final_output_rounded.tolist()\n    return output_list\nassert residual_block(np.array([1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [1.5, 3.0]\nassert residual_block(np.array([-1.0, 2.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0.,3.]\nassert residual_block(np.array([0.0, 0.0]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.0], [0.0, 0.5]])) == [0., 0.]\nassert residual_block(np.array([5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [10.0, 19.5]\nassert residual_block(np.array([-5.0, 3.0]), np.array([[2.0, 0.0], [2.0, 1.0]]), np.array([[0.5, 0.0], [1.0, 0.5]])) == [0.0, 3.0]"}
{"task_id": 114, "completion_id": 0, "passed": false, "result": "failed: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "solution": "import numpy as np\nimport numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n  Performs Global Average Pooling on a 3D NumPy array.\n\n  Global Average Pooling calculates the average value for each feature map\n  across its spatial dimensions (height and width).\n\n  Args:\n    x: Input NumPy array representing feature maps with shape\n       (height, width, channels).\n\n  Returns:\n    A 1D NumPy array of shape (channels,) where each element is the\n    average of the corresponding feature map.\n\n  Raises:\n    TypeError: If the input is not a NumPy array.\n    ValueError: If the input array is not 3-dimensional.\n  \"\"\"\n    if not isinstance(x, np.ndarray):\n        raise TypeError(f'Input must be a NumPy array, but got {type(x)}')\n    if x.ndim != 3:\n        raise ValueError(f'Input array must be 3-dimensional (height, width, channels), but got shape {x.shape}')\n    output = np.mean(x, axis=(0, 1))\n    return output\nassert global_avg_pool(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) == [5.5,6.5,7.5]\nassert global_avg_pool(np.array([[[100, 200]]])) == [100.0, 200.0]\nassert global_avg_pool(np.ones((3, 3, 1))) == [1.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, -4]]])) == [0.0, 0.0]\nassert global_avg_pool(np.array([[[-1, -2], [1, 2]], [[3, 4], [-3, 4]]])) == [0.0, 2.0]"}
{"task_id": 115, "completion_id": 0, "passed": false, "result": "failed: name 'B' is not defined", "solution": "import numpy as np\nimport numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array (BCHW format).\n\n    Args:\n        X: Input data, a 4D NumPy array of shape (B, C, H, W).\n        gamma: Scaling factors, a 1D NumPy array of shape (C,).\n        beta: Shifting factors, a 1D NumPy array of shape (C,).\n        epsilon: A small float added to variance to avoid division by zero.\n\n    Returns:\n        A list representation of the normalized, scaled, and shifted output\n        tensor, rounded to 4 decimal places.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('Input array X must be 4-dimensional (BCHW).')\n    if gamma.ndim != 1:\n        raise ValueError('gamma must be a 1D array.')\n    if beta.ndim != 1:\n        raise ValueError('beta must be a 1D array.')\n    (B, C, H, W) = X.shape\n    if gamma.shape[0] != C:\n        raise ValueError(f'gamma must have shape ({C},) to match input channels.')\n    if beta.shape[0] != C:\n        raise ValueError(f'beta must have shape ({C},) to match input channels.')\n    axes_to_reduce = (0, 2, 3)\n    mean = np.mean(X, axis=axes_to_reduce, keepdims=True)\n    variance = np.var(X, axis=axes_to_reduce, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    out = gamma_reshaped * X_normalized + beta_reshaped\n    out_rounded = np.round(out, 4)\n    return out_rounded.tolist()\nX = np.random.randn(B, C, H, W) * 2 + 1\nepsilon = 1e-05\nassert batch_normalization(np.array([[[[0.4967, -0.1383], [0.6477, 1.523]], [[-0.2342, -0.2341], [1.5792, 0.7674]]], [[[-0.4695, 0.5426], [-0.4634, -0.4657]], [[0.242, -1.9133], [-1.7249, -0.5623]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[0.4286, -0.5178], [0.6536, 1.9582]], [[0.0235, 0.0236], [1.6735, 0.9349]]], [[[-1.0114, 0.497], [-1.0023, -1.0058]], [[0.4568, -1.5043], [-1.3329, -0.275]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1)) == [[[[1.8177, 0.161], [0.3841, 0.062]], [[1.0043, -0.3714], [-1.121, 0.9403]]], [[[-1.948, 0.2503], [0.0819, -0.809]], [[0.3488, -0.9946], [-1.2417, 1.4352]]]]\nassert batch_normalization(np.array([[[[2.7068, 0.6281], [0.908, 0.5038]], [[0.6511, -0.3193], [-0.8481, 0.606]]], [[[-2.0182, 0.7401], [0.5288, -0.589]], [[0.1887, -0.7589], [-0.9332, 0.9551]]]]), np.ones(2).reshape(1, 2, 1, 1) * 0.5, np.ones(2).reshape(1, 2, 1, 1)) == [[[[1.9089, 1.0805], [1.1921, 1.031]], [[1.5021, 0.8143], [0.4395, 1.4702]]], [[[0.026, 1.1251], [1.0409, 0.5955]], [[1.1744, 0.5027], [0.3792, 1.7176]]]]"}
{"task_id": 116, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    The derivative of c * x^n with respect to x is given by the power rule:\n    d/dx (c * x^n) = c * n * x^(n-1)\n\n    Args:\n        c: The coefficient of the term (float).\n        x: The point at which to evaluate the derivative (float).\n        n: The exponent of the term (float).\n\n    Returns:\n        The value of the derivative at point x, rounded to 4 decimal places (float).\n        Returns 0.0 if n = 0 (derivative of a constant).\n        Note: If x=0 and n<1, the derivative is mathematically undefined.\n              Python's ** operator will raise a ZeroDivisionError in this case,\n              which is the expected behavior if not handled otherwise.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    try:\n        derivative_value = c * n * x ** (n - 1)\n    except ZeroDivisionError:\n        derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)\nassert poly_term_derivative(2.0, 3.0, 2.0) == 12.0\nassert poly_term_derivative(1.5, 4.0, 0.0) == 0.0\nassert poly_term_derivative(3.0, 2.0, 3.0) == 36.0\nassert poly_term_derivative(0.5, 5.0, 1.0) == 0.5\nassert poly_term_derivative(2.0, 3.0, 4.0) == 216.0\nassert poly_term_derivative(2.0, 3.0, 0.0) == 0.0"}
{"task_id": 117, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors: A list of 2D vectors, where each vector is a list of two floats.\n        tol: Tolerance value to determine linear independence. Vectors with a norm\n             smaller than tol after projection are considered linearly dependent.\n\n    Returns:\n        A list of orthonormal basis vectors (NumPy arrays rounded to 4 decimal\n        places and converted to lists). Returns an empty list if the input\n        vectors only span the zero subspace or the input list is empty.\n    \"\"\"\n    if not vectors:\n        return []\n    np_vectors = [np.array(v, dtype=float) for v in vectors]\n    for v in np_vectors:\n        if v.shape != (2,):\n            raise ValueError('All input vectors must be 2D.')\n    orthonormal_vectors = []\n    for v in np_vectors:\n        u = v.copy()\n        for e in orthonormal_vectors:\n            projection_coefficient = np.dot(v, e)\n            u -= projection_coefficient * e\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            e_new = u / norm\n            orthonormal_vectors.append(e_new)\n    result = [np.round(e, 4).tolist() for e in orthonormal_vectors]\n    return result\nassert orthonormal_basis([[1, 0], [1, 1]]) == [[1.0, 0.0], [0., 1.]]\nassert orthonormal_basis([[2, 0], [4, 0]], tol=1e-10) == [[1.0, 0.0]]\nassert orthonormal_basis([[1, 1], [1, -1]], tol=1e-5) == [[0.7071, 0.7071], [0.7071, -0.7071]]\nassert orthonormal_basis([[0, 0]], tol=1e-10) == []\nassert orthonormal_basis([[1, 3], [3, 1]], tol=1e-10) == [[0.3162, 0.9487], [0.9487, -0.3162]]\nassert orthonormal_basis([[3, 3], [3, 1]], tol=1e-10) == [[0.7071, 0.7071], [0.7071, -0.7071]]"}
{"task_id": 118, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nfrom typing import List, Sequence\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n    \"\"\"\n  Computes the cross product of two 3-dimensional vectors.\n\n  The cross product a x b is defined as:\n  [a2*b3 - a3*b2, a3*b1 - a1*b3, a1*b2 - a2*b1]\n\n  Args:\n    a: The first 3D vector (e.g., list, tuple, or numpy array of 3 numbers).\n    b: The second 3D vector (e.g., list, tuple, or numpy array of 3 numbers).\n\n  Returns:\n    A list representing the resulting 3D vector, with each component\n    rounded to 4 decimal places.\n\n  Raises:\n    ValueError: If the input arrays are not valid 3D vectors (this is\n              typically handled by np.cross itself).\n  \"\"\"\n    result_vector = np.cross(a, b)\n    rounded_vector = np.round(result_vector, decimals=4)\n    result_list = rounded_vector.tolist()\n    return result_list\nassert cross_product([1, 0, 0], [0, 1, 0]) == [0, 0, 1]\nassert cross_product([0, 1, 0], [0, 0, 1]) == [1, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([1, 0, 0], [1, 0, 0]) == [0, 0, 0]\nassert cross_product([1, 2, 3], [4, 5, 6]) == [-3, 6, -3]\nassert cross_product([12, 2, 3], [4, 45, 6]) == [-123, -60, 532]\nassert cross_product([1.2, 2.3, 4.4], [-4, 4, -4]) == [-26.8, -12.8, 14.0]"}
{"task_id": 119, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n        A: A list of lists or numpy array representing the square coefficient matrix.\n        b: A list or numpy array representing the constant vector.\n\n    Returns:\n        A list representing the solution vector x, rounded to 4 decimal places.\n        Returns -1 if the system does not have a unique solution (det(A) is close to 0).\n        \n    Raises:\n        ValueError: If A is not square or if dimensions of A and b are incompatible.\n    \"\"\"\n    try:\n        A = np.array(A, dtype=float)\n        b = np.array(b, dtype=float)\n    except ValueError:\n        raise ValueError('Inputs A and b must be convertible to numeric numpy arrays.')\n    n = A.shape[0]\n    if A.shape != (n, n):\n        raise ValueError('Coefficient matrix A must be square.')\n    if b.ndim == 2 and b.shape[1] == 1:\n        b = b.flatten()\n    elif b.ndim != 1:\n        raise ValueError('Constant vector b must be a 1D array or a column vector.')\n    if b.shape[0] != n:\n        raise ValueError(f'Dimensions of A ({A.shape}) and b ({b.shape}) are incompatible.')\n    try:\n        det_A = np.linalg.det(A)\n    except np.linalg.LinAlgError:\n        return -1\n    if np.isclose(det_A, 0, atol=1e-08):\n        return -1\n    x = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()\nassert cramers_rule([[2, -1, 3], [4, 2, 1], [-6, 1, -2]], [5, 10, -3]) == [0.1667, 3.3333, 2.6667]\nassert cramers_rule([[1, 2], [3, 4]], [5, 6]) == [-4.,4.5]\nassert cramers_rule([[1, 2], [2, 4]], [3, 6]) == -1"}
{"task_id": 120, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport math\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n\n    The Bhattacharyya distance measures the similarity of two probability distributions.\n    It is derived from the Bhattacharyya coefficient (BC):\n    BC(p, q) = sum(sqrt(p_i * q_i))\n    Distance D(p, q) = -ln(BC(p, q))\n\n    Args:\n        p: A list of floats representing the first probability distribution.\n           Assumed to contain non-negative values.\n        q: A list of floats representing the second probability distribution.\n           Assumed to contain non-negative values.\n\n    Returns:\n        The Bhattacharyya distance as a float rounded to 4 decimal places.\n        Returns 0.0 if the lists have different lengths or if either list is empty.\n        If the distributions have no overlapping support (BC=0), the distance is\n        theoretically infinite; this implementation returns a large positive value\n        derived from -log(epsilon) due to the rounding requirement.\n    \"\"\"\n    if not p or not q:\n        return 0.0\n    if len(p) != len(q):\n        return 0.0\n    np_p = np.array(p, dtype=float)\n    np_q = np.array(q, dtype=float)\n    bhattacharyya_coefficient = np.sum(np.sqrt(np_p * np_q))\n    epsilon = 1e-10\n    if np.isclose(bhattacharyya_coefficient, 0.0):\n        distance = -np.log(epsilon)\n    elif bhattacharyya_coefficient > 1.0 and (not np.isclose(bhattacharyya_coefficient, 1.0)):\n        distance = 0.0\n    else:\n        distance = -np.log(max(bhattacharyya_coefficient, epsilon))\n    rounded_distance = round(distance, 4)\n    return rounded_distance\nassert bhattacharyya_distance([0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]) == 0.1166\nassert bhattacharyya_distance([0.7, 0.2, 0.1], [0.4, 0.3, 0.3]) == 0.0541\nassert bhattacharyya_distance([], [0.5, 0.4, 0.1]) == 0.0\nassert bhattacharyya_distance([0.6, 0.4], [0.1, 0.7, 0.2]) == 0.0\nassert bhattacharyya_distance([0.6, 0.2, 0.1, 0.1], [0.1, 0.2, 0.3, 0.4]) == 0.2007"}
{"task_id": 121, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport collections.abc\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors (lists).\n\n    Checks if the two input lists have the same length. If they do,\n    it returns a new list where each element is the sum of the\n    corresponding elements from the input lists. If the lengths\n    differ, it indicates incompatible dimensions by returning -1.\n\n    Args:\n        a: The first vector (list of numbers).\n        b: The second vector (list of numbers).\n\n    Returns:\n        A new list representing the element-wise sum if the vectors\n        have the same length.\n        Returns the integer -1 if the vectors have different lengths.\n        Note: The return type hint reflects both possibilities, although\n              the prompt specified list[int|float] for the function signature.\n              Returning -1 is an explicit requirement for the failure case.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    if not a:\n        return []\n    result = [item_a + item_b for (item_a, item_b) in zip(a, b)]\n    return result\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert vector_sum([1, 2], [1, 2, 3]) == -1\nassert vector_sum([1.5, 2.5, 3.0], [2, 1, 4]) == [3.5, 3.5, 7.0]"}
{"task_id": 122, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n\n    Args:\n        theta: A NumPy array of shape (num_states, num_actions) parameterizing the policy.\n        episodes: A list of episodes. Each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n        A list representing the computed average policy gradient, rounded to 4 decimal places.\n        The shape is the same as theta.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    total_gradient = np.zeros_like(theta, dtype=float)\n    num_episodes = len(episodes)\n    if num_episodes == 0:\n        return np.round(total_gradient, 4).tolist()\n    for episode in episodes:\n        rewards = [step[2] for step in episode]\n        T = len(rewards)\n        returns = np.zeros(T, dtype=float)\n        G = 0.0\n        for t in reversed(range(T)):\n            G = rewards[t] + G\n            returns[t] = G\n        for t in range(T):\n            (state, action, _) = episode[t]\n            G_t = returns[t]\n            state_scores = theta[state, :]\n            stable_scores = state_scores - np.max(state_scores)\n            exp_scores = np.exp(stable_scores)\n            probabilities = exp_scores / np.sum(exp_scores)\n            grad_log_pi_for_state = -probabilities\n            grad_log_pi_for_state[action] += 1.0\n            total_gradient[state, :] += grad_log_pi_for_state * G_t\n    average_gradient = total_gradient / num_episodes\n    rounded_gradient = np.round(average_gradient, 4)\n    return rounded_gradient.tolist()\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,1,0), (1,0,1)], [(0,0,0)]]) == [[-0.25, 0.25], [0.25, -0.25]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(0,0,0), (0,1,0)], [(1,1,0)]]) == [[0.0, 0.0], [0.0, 0.0]]\nassert compute_policy_gradient(np.zeros((2,2)), [[(1,0,1), (1,1,1)], [(1,0,0)]]) == [[0.0, 0.0], [0.25, -0.25]]"}
{"task_id": 123, "completion_id": 0, "passed": true, "result": "passed", "solution": "\nimport math\ndef compute_efficiency(n_experts: int, k_active: int, d_in: int, d_out: int) -> float:\n    \"\"\"\n    Calculates the computational efficiency savings of an MoE layer\n    compared to a dense layer, based on the principle of sparse activation\n    relative to the total number of parameters (FLOPs per parameter).\n\n    The comparison focuses on the FLOPs per parameter for the expert computations.\n    A dense layer uses roughly 2 FLOPs per parameter per token for its matrix\n    multiplication (FLOPs = 2 * d_in * d_out, Params = d_in * d_out).\n    An MoE layer activates only k_active experts per token, so the FLOPs related\n    to expert computation are k_active * (2 * d_in * d_out). However, the total\n    number of parameters across all experts is n_experts * (d_in * d_out).\n    The FLOPs per *total expert parameter* for an MoE layer is therefore\n    (k_active * 2 * d_in * d_out) / (n_experts * d_in * d_out) = 2 * k_active / n_experts.\n\n    The savings percentage reflects how much lower the FLOPs-per-parameter rate\n    is for the MoE experts compared to the dense layer, due to activating only\n    k_active out of n_experts. This calculation ignores the overhead of the\n    gating network, focusing on the core efficiency gain from sparse expert computation.\n\n    Args:\n        n_experts: Total number of experts in the MoE layer.\n        k_active: Number of experts activated per input token (sparsity).\n        d_in: Input dimension to the layer/experts.\n        d_out: Output dimension from the layer/experts.\n\n    Returns:\n        The computational savings percentage, rounded to 1 decimal place.\n        Returns 0.0 if inputs are invalid (e.g., n_experts <= 0, k_active > n_experts)\n        or if n_experts is zero.\n    \"\"\"\n    if not all((isinstance(arg, int) for arg in [n_experts, k_active, d_in, d_out])):\n        return 0.0\n    if n_experts <= 0:\n        return 0.0\n    if k_active <= 0:\n        return 0.0\n    if d_in <= 0:\n        return 0.0\n    if d_out <= 0:\n        return 0.0\n    if k_active > n_experts:\n        return 0.0\n    flops_per_param_dense = 2.0\n    flops_per_param_moe = 2.0 * float(k_active) / float(n_experts)\n    if flops_per_param_dense == 0:\n        return 0.0\n    savings_ratio = 1.0 - flops_per_param_moe / flops_per_param_dense\n    savings_percentage = savings_ratio * 100.0\n    rounded_savings = round(savings_percentage, 1)\n    if rounded_savings == -0.0:\n        return 0.0\n    return rounded_savings\nassert compute_efficiency(1000, 2, 512, 512) == 99.8\nassert compute_efficiency(10, 2, 256, 256) == 80.0\nassert compute_efficiency(100, 4, 512, 512) == 96.0"}
{"task_id": 124, "completion_id": 0, "passed": false, "result": "failed: ", "solution": "import numpy as np\nimport numpy as np\ndef softplus(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n  Computes the softplus function element-wise in a numerically stable way.\n  softplus(x) = log(1 + exp(x))\n  \"\"\"\n    return np.logaddexp(0.0, x)\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n  Implements the Noisy Top-K gating mechanism.\n\n  Args:\n    X: Input matrix of shape (batch_size, input_dim).\n    W_g: Weight matrix for gating logits of shape (input_dim, num_experts).\n    W_noise: Weight matrix for noise scaling of shape (input_dim, num_experts).\n    N: Pre-sampled noise matrix of shape (batch_size, num_experts).\n       Typically sampled from a standard normal distribution.\n    k: The number of top experts to select for each input token.\n\n  Returns:\n    A Python list representing the final sparse gating probabilities matrix\n    of shape (batch_size, num_experts), rounded to 4 decimal places.\n    Only the top-k selected experts for each token will have non-zero probabilities.\n  \"\"\"\n    X = np.asarray(X, dtype=np.float64)\n    W_g = np.asarray(W_g, dtype=np.float64)\n    W_noise = np.asarray(W_noise, dtype=np.float64)\n    N = np.asarray(N, dtype=np.float64)\n    (batch_size, input_dim) = X.shape\n    (_, num_experts) = W_g.shape\n    if not 1 <= k <= num_experts:\n        raise ValueError(f'k ({k}) must be between 1 and num_experts ({num_experts})')\n    clean_logits = X @ W_g\n    noise_scaling = softplus(X @ W_noise)\n    noise_term = noise_scaling * N\n    noisy_logits = clean_logits + noise_term\n    top_k_indices = np.argsort(noisy_logits, axis=1)[:, -k:]\n    sparse_mask = np.zeros_like(clean_logits, dtype=np.float64)\n    row_indices = np.arange(batch_size)[:, np.newaxis]\n    sparse_mask[row_indices, top_k_indices] = 1.0\n    masked_clean_logits = np.where(sparse_mask == 1.0, clean_logits, -np.inf)\n    max_logits = np.max(masked_clean_logits, axis=1, keepdims=True)\n    stable_logits = masked_clean_logits - np.where(np.isinf(max_logits), 0.0, max_logits)\n    exp_logits = np.exp(stable_logits)\n    exp_logits = np.where(sparse_mask == 1.0, exp_logits, 0.0)\n    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n    probabilities = np.divide(exp_logits, sum_exp_logits, out=np.zeros_like(exp_logits), where=sum_exp_logits != 0)\n    rounded_probabilities = np.round(probabilities, 4)\n    output_list = rounded_probabilities.tolist()\n    return output_list\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.zeros((2,2)), np.zeros((1,2)), k=1) == [[0., 1.]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.5, 0.5], [0.5, 0.5]]), np.array([[1.0, -1.0]]), k=2) == [[0.917, 0.083]]\nassert noisy_topk_gating(np.array([[1.0, 2.0]]), np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[0.25, 0.25], [0.5, 0.5]]), np.zeros((1,2)), k=2) == [[0.2689, 0.7311]]"}
{"task_id": 125, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef softmax(x: np.ndarray, axis: int=-1) -> np.ndarray:\n    \"\"\"Compute stable softmax.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Sparse Mixture of Experts layer.\n\n    Args:\n        x: Input tensor. Shape: (..., input_dim)\n        We: Expert weights. Shape: (n_experts, input_dim, output_dim)\n        Wg: Gating network weights. Shape: (input_dim, n_experts)\n        n_experts: Total number of experts.\n        top_k: Number of experts to select per token.\n\n    Returns:\n        Output tensor after MoE computation, rounded to 4 decimals, as a Python list.\n        Shape: (..., output_dim)\n    \"\"\"\n    if x.shape[-1] != Wg.shape[0]:\n        raise ValueError(f'Input dimension mismatch: x last dim {x.shape[-1]} != Wg first dim {Wg.shape[0]}')\n    if x.shape[-1] != We.shape[1]:\n        raise ValueError(f'Input dimension mismatch: x last dim {x.shape[-1]} != We second dim {We.shape[1]}')\n    if Wg.shape[1] != n_experts:\n        raise ValueError(f'Gating output dimension mismatch: Wg second dim {Wg.shape[1]} != n_experts {n_experts}')\n    if We.shape[0] != n_experts:\n        raise ValueError(f'Expert count mismatch: We first dim {We.shape[0]} != n_experts {n_experts}')\n    if top_k > n_experts:\n        raise ValueError(f'top_k ({top_k}) cannot be greater than n_experts ({n_experts})')\n    if top_k <= 0:\n        raise ValueError(f'top_k ({top_k}) must be positive')\n    original_shape = x.shape\n    input_dim = x.shape[-1]\n    output_dim = We.shape[-1]\n    num_tokens = np.prod(original_shape[:-1]) if len(original_shape) > 1 else 1\n    x_reshaped = x.reshape(num_tokens, input_dim)\n    logits = x_reshaped @ Wg\n    top_k_indices = np.argsort(logits, axis=1)[:, -top_k:]\n    top_k_logits = np.take_along_axis(logits, top_k_indices, axis=1)\n    top_k_probs = softmax(top_k_logits, axis=1)\n    selected_We = We[top_k_indices]\n    expert_outputs = np.einsum('ti,tkio->tko', x_reshaped, selected_We)\n    weighted_outputs = expert_outputs * top_k_probs[:, :, np.newaxis]\n    final_output_reshaped = np.sum(weighted_outputs, axis=1)\n    output_shape = original_shape[:-1] + (output_dim,)\n    final_output = final_output_reshaped.reshape(output_shape)\n    rounded_output = np.round(final_output, 4)\n    return rounded_output.tolist()\nbatch_size = 2\nseq_len = 3\ninput_dim = 4\noutput_dim = 4\nn_experts = 8\ntop_k = 2\nx = np.random.randn(batch_size, seq_len, input_dim)\nWe = np.random.randn(n_experts, input_dim, output_dim)\nWg = np.random.randn(input_dim, n_experts)\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4) , 4, 2) == [[[0.5148, 0.4329], [0.5554, 0.5447], [0.1285, 0.102 ]], [[0.339, 0.3046], [0.5391, 0.417 ], [0.3597, 0.3262]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.zeros((4, 2, 2)), np.random.rand(2, 4), 4, 2) == [[[0., 0.], [0., 0.], [0., 0.]], [[0., 0.], [0., 0.], [0., 0.]]]\nnp.random.seed(42)\nassert moe(np.random.rand(2, 3, 2), np.random.rand(4, 2, 2), np.random.rand(2, 4), 4, 1) == [[[0.5069, 0.4006], [0.6228, 0.3214], [0.141, 0.0789]], [[0.2886, 0.3254], [0.5747, 0.3433], [0.2959, 0.3582]]]"}
{"task_id": 126, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X (np.ndarray): Input tensor of shape (B, C, H, W), where\n                        B = Batch size\n                        C = Number of channels\n                        H = Height\n                        W = Width\n        gamma (np.ndarray): Learnable scale parameter (vector of size C).\n                            Applied per channel.\n        beta (np.ndarray): Learnable shift parameter (vector of size C).\n                           Applied per channel.\n        num_groups (int): The number of groups to divide the channels into.\n                          C must be divisible by num_groups.\n        epsilon (float): A small float added to variance to avoid division by zero.\n                         Default is 1e-5.\n\n    Returns:\n        list: The normalized tensor, rounded to 4 decimal places,\n              converted to a Python list using .tolist().\n              The shape remains (B, C, H, W).\n\n    Raises:\n        AssertionError: If the number of channels C is not divisible by num_groups.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, f'Number of channels ({C}) must be divisible by num_groups ({num_groups})'\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_hat = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    X_normalized = X_hat.reshape(B, C, H, W)\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    output = X_normalized * gamma_reshaped + beta_reshaped\n    rounded_output = np.round(output, 4)\n    return rounded_output.tolist()\nnum_groups = 2\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 2), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[-0.2287, -1.2998], [ 0.026, 1.5025]], [[-0.926, -0.9259], [1.46, 0.3919]]], [[[-0.5848, 1.732 ], [-0.5709, -0.5762]], [[1.4005, -1.0503], [-0.8361, 0.486 ]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 1), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[1. ], [-1. ]], [[-1. ], [1. ]]], [[[-0.0026],[0.0026]], [[1. ], [-1.]]]]\nnp.random.seed(42)\nassert group_normalization(np.random.randn(2, 2, 2, 3), np.ones(2).reshape(1, 2, 1, 1), np.zeros(2).reshape(1, 2, 1, 1) , 2) == [[[[0.2419, -0.7606, 0.4803], [1.8624, -0.912, -0.912]], [[1.7041, 0.6646, -0.9193], [0.3766, -0.9115, -0.9145]]], [[[1.173, -1.31, -1.093], [0.2464, -0.2726, 1.2563]], [[-0.4992, -1.0008, 1.8623], [0.1796, 0.4714, -1.0132]]]]"}
{"task_id": 127, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\nimport math\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n  Finds the x-coordinate of the minimum value of the function f(x) = x^4 - 3x^3 + 2\n  using gradient descent.\n\n  Args:\n    start_x: The initial guess for the x-coordinate.\n    learning_rate: The step size for each iteration of gradient descent.\n    tolerance: The minimum change in x required to continue iterating.\n               Iteration stops if the absolute change is less than this value.\n    max_iters: The maximum number of iterations to perform.\n\n  Returns:\n    The x-coordinate where f(x) is minimized, rounded to 4 decimal places.\n    Returns NaN if the process diverges or encounters invalid numbers.\n  \"\"\"\n\n    def df(x):\n        if abs(x) > 10000000000.0:\n            print(f'Warning: x value {x} is very large, potential instability.')\n            return float('nan')\n        try:\n            grad = 4 * x ** 3 - 9 * x ** 2\n            if not math.isfinite(grad):\n                print(f'Warning: Gradient calculation resulted in {grad} at x={x}.')\n                return float('nan')\n            return grad\n        except OverflowError:\n            print(f'Warning: OverflowError during gradient calculation at x={x}.')\n            return float('nan')\n    x = float(start_x)\n    for i in range(max_iters):\n        gradient = df(x)\n        if not math.isfinite(gradient):\n            print(f'Stopping iteration {i + 1} due to non-finite gradient.')\n            return round(x, 4)\n        prev_x = x\n        step = learning_rate * gradient\n        if not math.isfinite(step):\n            print(f'Warning: Step calculation resulted in {step} at iteration {i + 1}.')\n            return round(prev_x, 4)\n        x = x - step\n        if not math.isfinite(x):\n            print(f'Warning: x became non-finite ({x}) after update at iteration {i + 1}.')\n            return round(prev_x, 4)\n        if abs(x - prev_x) < tolerance:\n            return round(x, 4)\n    print(f'Warning: Maximum iterations ({max_iters}) reached without converging to tolerance ({tolerance}).')\n    print(f'Final change was: {abs(x - prev_x)}')\n    return round(x, 4)\nassert find_treasure(-1.0) == 2.3366\nassert find_treasure(1.0) == 2.1475\nassert find_treasure(3.0) == 2.3366"}
{"task_id": 128, "completion_id": 0, "passed": true, "result": "passed", "solution": "import numpy as np\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n  Computes the Dynamic Tanh (DyT) activation function.\n\n  The DyT function is defined as:\n  DyT(x) = tanh(alpha * x + gamma * tanh(beta * x))\n\n  This function aims to replace layer normalization in Transformer architectures\n  while preserving squashing behavior and enabling stable training.\n\n  Args:\n    x: Input numpy array. This can be of any shape.\n    alpha: Learnable scalar parameter controlling the linear part.\n    gamma: Learnable scalar parameter controlling the non-linear modulation.\n    beta: Learnable scalar parameter controlling the inner tanh transformation.\n\n  Returns:\n    A list of floats containing the DyT activation results for each element\n    in the input array x, rounded to 4 decimal places. The shape of the list\n    will match the shape of the input array x.\n  \"\"\"\n    if not isinstance(x, np.ndarray):\n        x = np.array(x)\n    inner_tanh = np.tanh(beta * x)\n    dyt_output_np = np.tanh(alpha * x + gamma * inner_tanh)\n    rounded_output_np = np.round(dyt_output_np, 4)\n    output_list = rounded_output_np.tolist()\n    return output_list\nassert dynamic_tanh(np.array([[[0.94378259]],[[0.97754654]],[[0.36168351]],[[0.51821078]],[[0.76961589]]]), 0.5, np.ones((1,)), np.zeros((1,))) == [[[0.4397]], [[0.4532]], [[0.1789]], [[0.2535]], [[0.3669]]]\nassert dynamic_tanh(np.array([[[0.20793482, 0.16989285, 0.03898972], [0.17912554, 0.10962205, 0.3870742], [0.00107181, 0.35807922, 0.15861333]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1036, 0.0847, 0.0195], [0.0893, 0.0548, 0.1912], [0.0005, 0.1772, 0.0791]]]\nassert dynamic_tanh(np.array([[[0.35, 0.16, 0.42], [0.17, 0.25, 0.38], [0.71, 0.35, 0.68]]]), 0.5, np.ones((3,)), np.zeros((3,))) == [[[0.1732, 0.0798, 0.207], [0.0848, 0.1244, 0.1877], [0.3408, 0.1732, 0.3275]]]"}
