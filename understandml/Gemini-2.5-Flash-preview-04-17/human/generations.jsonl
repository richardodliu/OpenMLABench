{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given matrix (list of lists) into a specified shape using numpy.\n\n    Args:\n        a: The input matrix as a list of lists.\n        new_shape: A tuple (rows, columns) representing the desired new shape.\n\n    Returns:\n        The reshaped matrix as a list of lists if reshaping is possible,\n        otherwise an empty list [].\n    \"\"\"\n    # Convert the input list of lists to a numpy array\n    try:\n        np_array = np.array(a)\n    except Exception:\n        # Handle cases where the input might not be a valid matrix structure\n        return []\n\n    # Get the total number of elements in the original matrix\n    original_size = np_array.size\n\n    # Calculate the total number of elements required for the new shape\n    new_size = new_shape[0] * new_shape[1]\n\n    # Check if reshaping is possible (total number of elements must be the same)\n    if original_size != new_size:\n        return []\n\n    # Reshape the numpy array\n    try:\n        reshaped_array = np_array.reshape(new_shape)\n    except Exception:\n        # This catch is mostly for unexpected numpy errors, though the size check\n        # should prevent most common reshape errors.\n        return []\n\n    # Convert the reshaped numpy array back to a list of lists\n    return reshaped_array.tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n    matrix1 = [[1, 2], [3, 4]]\n    new_shape1 = (1, 4)\n    result1 = reshape_matrix(matrix1, new_shape1)\n    print(f\"Original matrix: {matrix1}\")\n    print(f\"New shape: {new_shape1}\")\n    print(f\"Reshaped matrix: {result1}\")\n    # Expected output: [[1, 2, 3, 4]]\n\n    matrix2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\n    new_shape2 = (4, 2)\n    result2 = reshape_matrix(matrix2, new_shape2)\n    print(f\"\\nOriginal matrix: {matrix2}\")\n    print(f\"New shape: {new_shape2}\")\n    print(f\"Reshaped matrix: {result2}\")\n    # Expected output: [[1, 2], [3, 4], [5, 6], [7, 8]]\n\n    matrix3 = [[1, 2], [3, 4]]\n    new_shape3 = (2, 4) # Cannot reshape 2x2 into 2x4\n    result3 = reshape_matrix(matrix3, new_shape3)\n    print(f\"\\nOriginal matrix: {matrix3}\")\n    print(f\"New shape: {new_shape3}\")\n    print(f\"Reshaped matrix: {result3}\")\n    # Expected output: []\n\n    matrix4 = [[1, 2, 3], [4, 5, 6]]\n    new_shape4 = (3, 2)\n    result4 = reshape_matrix(matrix4, new_shape4)\n    print(f\"\\nOriginal matrix: {matrix4}\")\n    print(f\"New shape: {new_shape4}\")\n    print(f\"Reshaped matrix: {result4}\")\n    # Expected output: [[1, 2], [3, 4], [5, 6]]\n\n    matrix5 = [[1.1, 2.2], [3.3, 4.4], [5.5, 6.6]]\n    new_shape5 = (2, 3)\n    result5 = reshape_matrix(matrix5, new_shape5)\n    print(f\"\\nOriginal matrix: {matrix5}\")\n    print(f\"New shape: {new_shape5}\")\n    print(f\"Reshaped matrix: {result5}\")\n    # Expected output: [[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]]\n\n    matrix6 = [[1, 2], [3]] # Invalid matrix structure\n    new_shape6 = (2, 2)\n    result6 = reshape_matrix(matrix6, new_shape6)\n    print(f\"\\nOriginal matrix: {matrix6}\")\n    print(f\"New shape: {new_shape6}\")\n    print(f\"Reshaped matrix: {result6}\")\n    # Expected output: []\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n\n    Args:\n        matrix: A list of lists representing the matrix.\n                Each inner list is a row of the matrix.\n                Elements can be integers or floats.\n        scalar: The scalar value to multiply the matrix by.\n                Can be an integer or a float.\n\n    Returns:\n        A new matrix (list of lists) where each element of the\n        original matrix is multiplied by the scalar.\n    \"\"\"\n    # Use a nested list comprehension for a concise solution\n    result_matrix = [[element * scalar for element in row] for row in matrix]\n\n    return result_matrix\n\n# Example Usage:\nif __name__ == \"__main__\":\n    # Example 1: Integer matrix and integer scalar\n    matrix1 = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar1 = 2\n    result1 = scalar_multiply(matrix1, scalar1)\n    print(f\"Original Matrix 1:\\n{matrix1}\")\n    print(f\"Scalar 1: {scalar1}\")\n    print(f\"Result Matrix 1 (Matrix 1 * Scalar 1):\\n{result1}\")\n    print(\"-\" * 20)\n\n    # Example 2: Float matrix and float scalar\n    matrix2 = [\n        [1.0, 0.5],\n        [2.5, 3.0]\n    ]\n    scalar2 = 1.5\n    result2 = scalar_multiply(matrix2, scalar2)\n    print(f\"Original Matrix 2:\\n{matrix2}\")\n    print(f\"Scalar 2: {scalar2}\")\n    print(f\"Result Matrix 2 (Matrix 2 * Scalar 2):\\n{result2}\")\n    print(\"-\" * 20)\n\n    # Example 3: Mixed matrix and integer scalar\n    matrix3 = [\n        [10, 0.1, 5],\n        [0, -2, 3.5]\n    ]\n    scalar3 = -3\n    result3 = scalar_multiply(matrix3, scalar3)\n    print(f\"Original Matrix 3:\\n{matrix3}\")\n    print(f\"Scalar 3: {scalar3}\")\n    print(f\"Result Matrix 3 (Matrix 3 * Scalar 3):\\n{result3}\")\n    print(\"-\" * 20)\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport math\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n\n    Args:\n        matrix: A 2x2 matrix represented as a list of lists of floats.\n                Expected format: [[a, b], [c, d]]\n\n    Returns:\n        The inverse matrix as a list of lists of floats, or None if the\n        matrix is not invertible.\n    \"\"\"\n    # Check if the input is a valid 2x2 matrix\n    if not isinstance(matrix, list) or len(matrix) != 2:\n        return None\n    if not all(isinstance(row, list) and len(row) == 2 for row in matrix):\n        return None\n    if not all(isinstance(elem, (int, float)) for row in matrix for elem in row):\n         return None\n\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n\n    # Calculate the determinant\n    determinant = a * d - b * c\n\n    # Check if the determinant is close to zero (matrix is not invertible)\n    # Use a small tolerance for floating-point comparison\n    if abs(determinant) < 1e-9:\n        return None\n\n    # Calculate the inverse matrix\n    inv_determinant = 1 / determinant\n\n    # The inverse of [[a, b], [c, d]] is (1/det) * [[d, -b], [-c, a]]\n    inverse_matrix = [\n        [d * inv_determinant, -b * inv_determinant],\n        [-c * inv_determinant, a * inv_determinant]\n    ]\n\n    return inverse_matrix\n\nif __name__ == '__main__':\n    # Example 1: Invertible matrix\n    matrix1 = [[4.0, 7.0], [2.0, 6.0]]\n    inverse1 = inverse_2x2(matrix1)\n    print(f\"Matrix 1: {matrix1}\")\n    print(f\"Inverse 1: {inverse1}\")\n    # Expected inverse: 1/(24-14) * [[6, -7], [-2, 4]] = 1/10 * [[6, -7], [-2, 4]] = [[0.6, -0.7], [-0.2, 0.4]]\n\n    print(\"-\" * 20)\n\n    # Example 2: Non-invertible matrix (determinant is 0)\n    matrix2 = [[4.0, 6.0], [2.0, 3.0]]\n    inverse2 = inverse_2x2(matrix2)\n    print(f\"Matrix 2: {matrix2}\")\n    print(f\"Inverse 2: {inverse2}\")\n    # Expected inverse: None (determinant = 4*3 - 6*2 = 12 - 12 = 0)\n\n    print(\"-\" * 20)\n\n    # Example 3: Identity matrix (inverse is itself)\n    matrix3 = [[1.0, 0.0], [0.0, 1.0]]\n    inverse3 = inverse_2x2(matrix3)\n    print(f\"Matrix 3: {matrix3}\")\n    print(f\"Inverse 3: {inverse3}\")\n    # Expected inverse: [[1.0, 0.0], [0.0, 1.0]]\n\n    print(\"-\" * 20)\n\n    # Example 4: Matrix with integer values\n    matrix4 = [[3, 5], [1, 2]]\n    inverse4 = inverse_2x2(matrix4)\n    print(f\"Matrix 4: {matrix4}\")\n    print(f\"Inverse 4: {inverse4}\")\n    # Expected inverse: 1/(6-5) * [[2, -5], [-1, 3]] = 1/1 * [[2, -5], [-1, 3]] = [[2.0, -5.0], [-1.0, 3.0]]\n\n    print(\"-\" * 20)\n\n    # Example 5: Invalid input (not 2x2)\n    matrix5 = [[1, 2, 3], [4, 5, 6]]\n    inverse5 = inverse_2x2(matrix5)\n    print(f\"Matrix 5: {matrix5}\")\n    print(f\"Inverse 5: {inverse5}\")\n    # Expected: None\n\n    print(\"-\" * 20)\n\n    # Example 6: Invalid input (not list of lists)\n    matrix6 = [1, 2, 3, 4]\n    inverse6 = inverse_2x2(matrix6)\n    print(f\"Matrix 6: {matrix6}\")\n    print(f\"Inverse 6: {inverse6}\")\n    # Expected: None\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n\n    Args:\n        X: A list of lists representing the feature matrix.\n        y: A list of floats representing the target vector.\n\n    Returns:\n        A list of floats representing the coefficients (including the intercept),\n        rounded to four decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n\n    # Get the number of samples\n    m = X_np.shape[0]\n\n    # Add a column of ones to X for the intercept term (bias)\n    # The design matrix X_b will have shape (m, n + 1)\n    X_b = np.c_[np.ones((m, 1)), X_np]\n\n    # Calculate the coefficients using the normal equation: theta = (X_b^T * X_b)^(-1) * X_b^T * y\n    # Calculate (X_b^T * X_b)\n    X_b_T_X_b = X_b.T @ X_b\n\n    # Calculate the inverse of (X_b^T * X_b)\n    # Use np.linalg.inv for matrix inversion\n    try:\n        X_b_T_X_b_inv = np.linalg.inv(X_b_T_X_b)\n    except np.linalg.LinAlgError:\n        # Handle cases where the matrix is singular (e.g., perfectly correlated features)\n"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n  \"\"\"\n  Computes the sigmoid activation function for a given input z.\n\n  Args:\n    z: The input value (float).\n\n  Returns:\n    The output of the sigmoid function, rounded to four decimal places (float).\n  \"\"\"\n  # Compute the sigmoid value: 1 / (1 + exp(-z))\n  sigmoid_value = 1 / (1 + math.exp(-z))\n\n  # Round the result to four decimal places\n  rounded_sigmoid_value = round(sigmoid_value, 4)\n\n  return rounded_sigmoid_value\n\nif __name__ == '__main__':\n  # Example usage:\n  z1 = 0.0\n  output1 = sigmoid(z1)\n  print(f\"sigmoid({z1}) = {output1}\") # Expected output: 0.5000\n\n  z2 = 1.0\n  output2 = sigmoid(z2)\n  print(f\"sigmoid({z2}) = {output2}\") # Expected output: approx 0.7311\n\n  z3 = -1.0\n  output3 = sigmoid(z3)\n  print(f\"sigmoid({z3}) = {output3}\") # Expected output: approx 0.2689\n\n  z4 = 10.0\n  output4 = sigmoid(z4)\n  print(f\"sigmoid({z4}) = {output4}\") # Expected output: approx 0.9999\n\n  z5 = -10.0\n  output5 = sigmoid(z5)\n  print(f\"sigmoid({z5}) = {output5}\") # Expected output: approx 0.0000\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a list of scores.\n\n    Args:\n        scores: A list of floating-point numbers representing the scores.\n\n    Returns:\n        A list of floating-point numbers representing the softmax probabilities,\n        each rounded to four decimal places. Returns an empty list if the\n        input list is empty.\n    \"\"\"\n    if not scores:\n        return []\n\n    # Subtract the maximum score for numerical stability\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n\n    # Calculate the sum of exponentiated scores\n    sum_exp_scores = sum(exp_scores)\n\n    # Handle the case where the sum is zero (e.g., due to extreme underflow)\n    # Although subtracting max_score helps, it's good practice.\n    # If sum is zero, all exp_scores must have been zero, which means\n    # original scores were very small negative numbers. In this case,\n    # the softmax is effectively uniform if all original scores were equal,\n    # but numerical limitations might yield zeros. Returning zeros might be\n    # the most numerically stable outcome in such extreme cases.\n    if sum_exp_scores == 0:\n         return [0.0] * len(scores)\n\n    # Compute softmax probabilities and round to 4 decimal places\n    softmax_probs = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n\n    return softmax_probs\n\nif __name__ == '__main__':\n    # Example Usage:\n    scores1 = [1.0, 2.0, 3.0]\n    softmax_output1 = softmax(scores1)\n    print(f\"Scores: {scores1}\")\n    print(f\"Softmax: {softmax_output1}\")\n    # Expected output is approximately [0.0900, 0.2447, 0.6652] (sum is 0.9999 due to rounding)\n\n    scores2 = [-1.0, -0.5, 0.0, 0.5, 1.0]\n    softmax_output2 = softmax(scores2)\n    print(f\"\\nScores: {scores2}\")\n    print(f\"Softmax: {softmax_output2}\")\n\n    scores3 = [100.0, 101.0, 102.0] # Test with large values (stability check)\n    softmax_output3 = softmax(scores3)\n    print(f\"\\nScores: {scores3}\")\n    print(f\"Softmax: {softmax_output3}\")\n    # Expected output should be similar to scores1 because only relative differences matter\n\n    scores4 = [5.0] # Test with single element\n    softmax_output4 = softmax(scores4)\n    print(f\"\\nScores: {scores4}\")\n    print(f\"Softmax: {softmax_output4}\")\n    # Expected output: [1.0000]\n\n    scores5 = [] # Test with empty list\n    softmax_output5 = softmax(scores5)\n    print(f\"\\nScores: {scores5}\")\n    print(f\"Softmax: {softmax_output5}\")\n    # Expected output: []\n\n    scores6 = [-100"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Performs a random shuffle of the samples in two numpy arrays, X and y,\n    while maintaining the corresponding order between them.\n\n    Args:\n        X (np.ndarray): The feature data array.\n        y (np.ndarray): The target data array. Must have the same number\n                        of samples as X.\n        seed (int, optional): Seed for the random number generator for\n                              reproducible shuffling. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing two Python lists:\n               - The shuffled X data.\n               - The shuffled y data.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n\n    n_samples = X.shape[0]\n    \n    if seed is not None:\n        rng = np.random.RandomState(seed)\n        permutation = rng.permutation(n_samples)\n    else:\n        permutation = np.random.permutation(n_samples)\n\n    shuffled_X = X[permutation]\n    shuffled_y = y[permutation]\n\n    return shuffled_X.tolist(), shuffled_y.tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n\n    # Create some dummy data\n    X_data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y_data = np.array([0, 1, 0, 1, 0])\n\n    print(\"Original X:\")\n    print(X_data)\n    print(\"\\nOriginal y:\")\n    print(y_data)\n\n    # Shuffle without seed (will be different each time)\n    shuffled_X_no_seed, shuffled_y_no_seed = shuffle_data(X_data, y_data)\n    print(\"\\nShuffled (no seed) X (list):\")\n    print(shuffled_X_no_seed)\n    print(\"\\nShuffled (no seed) y (list):\")\n    print(shuffled_y_no_seed)\n\n    # Shuffle with a specific seed (will be the same each time for this seed)\n    seed_value = 42\n    shuffled_X_with_seed1, shuffled_y_with_seed1 = shuffle_data(X_data, y_data, seed=seed_value)\n    print(f\"\\nShuffled (seed={seed_value}) X (list):\")\n    print(shuffled_X_with_seed1)\n    print(f\"\\nShuffled (seed={seed_value}) y (list):\")\n    print(shuffled_y_with_seed1)\n\n    # Shuffle again with the same seed to show reproducibility\n    shuffled_X_with_seed2, shuffled_y_with_seed2 = shuffle_data(X_data, y_data, seed=seed_value)\n    print(f\"\\"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from numpy arrays X and optionally y.\n\n    Args:\n        X (np.ndarray): The input features array.\n        y (np.ndarray, optional): The target labels array. Defaults to None.\n        batch_size (int, optional): The size of each batch. Defaults to 64.\n\n    Yields:\n        list or tuple: If y is None, yields a list representing a batch of X.\n                       If y is provided, yields a tuple (X_batch_list, y_batch_list)\n                       where each element is a list representing a batch.\n    Raises:\n        ValueError: If X and y have different numbers of samples.\n    \"\"\"\n    n_samples = X.shape[0]\n\n    if y is not None and y.shape[0] != n_samples:\n        raise ValueError(\"X and y must have the same number of samples.\")\n\n    if batch_size <= 0:\n        raise ValueError(\"batch_size must be a positive integer.\")\n\n    for i in range(0, n_samples, batch_size):\n        # Get the batch indices\n        batch_end = min(i + batch_size, n_samples)\n        batch_indices = np.arange(i, batch_end)\n\n        # Extract batches\n        X_batch = X[batch"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset into two subsets based on a feature threshold.\n\n    Parameters\n    ----------\n    X : array-like\n        The dataset (features). Expected to be a 2D structure.\n    feature_i : int\n        The index of the feature (column) to use for division.\n    threshold : float or int\n        The threshold value. Samples with feature_i >= threshold go to\n        the first subset, others to the second.\n\n    Returns\n    -------\n    tuple of list\n        A tuple containing two lists:\n        - The first list contains samples where X[:, feature_i] >= threshold.\n        - The second list contains samples where X[:, feature_i] < threshold.\n        Each list contains rows from the original dataset X.\n    \"\"\"\n    # Ensure X is a numpy array for efficient indexing\n    X = np.asarray(X)\n\n    # Create a boolean mask based on the condition\n    mask_greater_equal = X[:, feature_i] >= threshold\n\n    # Use the mask to select rows for the two subsets\n    subset_greater_equal = X[mask_greater_equal]\n    subset_less = X["}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generates random subsets of a dataset (X, y).\n\n    Args:\n        X (np.ndarray): A 2D numpy array representing the features.\n        y (np.ndarray): A 1D numpy array representing the labels.\n        n_subsets (int): The number of random subsets to generate.\n        replacements (bool): If True, sample with replacement. If False, sample\n                             without replacement. Defaults to True.\n        seed (int): Seed for the random number generator for reproducibility.\n                    Defaults to 42.\n\n    Returns:\n        list: A list of n_subsets, where each subset is a tuple\n              (X_subset_list, y_subset_list). X_subset_list and y_subset_list\n              are Python lists converted from the corresponding numpy arrays.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        raise ValueError(\"X must be a 2D numpy array.\")\n    if not isinstance(y, np.ndarray) or y.ndim != 1:\n         raise ValueError(\"y must be a 1D numpy array.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    if not isinstance(n_subsets, int) or n_subsets <= 0:\n        raise ValueError(\"n_subsets must be a positive integer.\")\n    if not isinstance(replacements, bool):\n         raise ValueError(\"replacements must be a boolean.\")\n\n"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n  \"\"\"\n  Converts a 1D numpy array into a diagonal matrix.\n\n  Args:\n    x: A 1D numpy array.\n\n  Returns:\n    A Python list representing the diagonal matrix (2D numpy array\n    converted to list).\n  \"\"\"\n  # Use numpy.diag to create a 2D diagonal matrix from the 1D array\n  diagonal_matrix = np.diag(x)\n\n  # Convert the numpy array to a Python list and return\n  return diagonal_matrix.tolist()\n\nif __name__ == '__main__':\n  # Example usage:\n  vector = np.array([1, 2, 3, 4])\n  diagonal_matrix_list = make_diagonal(vector)\n  print(f\"Original vector: {vector}\")\n  print(f\"Diagonal matrix (as list):\")\n  print(diagonal_matrix_list)\n\n  vector_empty = np.array([])\n  diagonal_matrix_empty = make_diagonal(vector_empty)\n  print(f\"\\nOriginal vector: {vector_empty}\")\n  print(f\"Diagonal matrix (as list):\")\n  print(diagonal_matrix_empty)\n\n  vector_single = np.array([5])\n  diagonal_matrix_single = make_diagonal(vector_single)\n  print(f\"\\nOriginal vector: {vector_single}\")\n  print(f\"Diagonal matrix (as list):\")\n  print(diagonal_matrix_single)\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Calculates the accuracy score between true labels and predicted labels.\n\n    Args:\n        y_true: A 1D numpy array containing the true labels.\n        y_pred: A 1D numpy array containing the predicted labels.\n\n    Returns:\n        The accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays y_true and y_pred must have the same shape.\")\n    if y_true.ndim != 1:\n         raise ValueError(\"Input arrays y_true and y_pred must be 1-dimensional.\")\n\n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n\n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n\n    # Avoid division by zero if the input arrays are empty\n    if total_predictions == 0:\n        return 0.0\n\n    # Calculate accuracy\n    accuracy = correct_predictions / total_predictions\n\n    # Round the result to 4 decimal places\n    rounded_accuracy = round(accuracy, 4)\n\n    return rounded_accuracy\n\nif __name__ == '__main__':\n    # Example Usage:\n    y_true_1 = np.array([0, 1, 0, 1, 1, 0])\n    y_pred_1 = np.array([0, 1, 1, 1, 0, 0])\n    acc_1 = accuracy_score(y_true_1, y_pred_1)\n    print(f\"y_true: {y_true_1}\")\n    print(f\"y_pred: {y_pred_1}\")\n    print(f\"Accuracy: {acc_1}\") # Expected: (0+1+0+1+0+1)/6 = 3/6 = 0.5 -> 0.5000\n\n    print(\"-\" * 20)\n\n    y_true_2 = np.array([1, 1, 1, 1, 1])\n    y_pred_2 = np.array([1, 1, 1, 1, 1])\n    acc_2 = accuracy_score(y_true_2, y_pred_2)\n    print(f\"y_true: {y_true_2}\")\n    print(f\"y_pred: {y_pred_2}\")\n    print(f\"Accuracy: {acc_2}\") # Expected: 5/5 = 1.0 -> 1.0000\n\n    print(\"-\" * 20)\n\n    y_true_3 = np.array([0, 0, 0])\n    y_pred_3 = np.array([1, 1, 1])\n    acc_3 = accuracy_score(y_true_3, y_pred_3)\n    print(f\"y_true: {y_true_3}\")\n    print(f\"y_pred: {y_pred_3}\")\n    print(f\"Accuracy: {acc_3}\") # Expected: 0/3 = 0.0 -> 0.0000\n\n    print(\"-\" * 20)\n\n    y_true_4 = np.array([])\n    y_pred_4 = np.array([])\n    acc_4 = accuracy_score(y_true_4, y_pred_4)\n    print(f\"y_true: {y_true_4}\")\n    print(f\"y_pred: {y_pred_4}\")\n    print(f\"Accuracy: {acc_4}\") # Expected: 0.0000 (handled empty case)\n\n    print(\"-\" * 20)\n\n    y_true_5 = np.array([1, 2, 3, 4])\n    y_pred_5 = np.array([1, 2, 4, 4])\n    acc_5 = accuracy_score(y_true_5, y_pred_5)\n    print(f\"y_true: {y_true_5}\")\n    print(f\"y_pred: {y_pred_5}\")\n    print(f\"Accuracy: {acc_5}\") # Expected: (1+1+0+1)/4 = 3/4 = 0.75 -> 0.7500\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset(s).\n\n    Args:\n        X (np.ndarray): A 2D numpy array representing the first dataset\n                        (rows are observations, columns are variables).\n        Y (np.ndarray, optional): A 2D numpy array representing the second dataset\n                                  (rows are observations, columns are variables).\n                                  If None, calculates the correlation matrix of X with itself.\n\n    Returns:\n        list: The correlation matrix as a 2D Python list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        # Calculate correlation matrix of X with itself\n        # rowvar=False means columns are variables\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        # Calculate correlation matrix between X and Y (of the combined dataset)\n        # np.corrcoef(X, Y, rowvar=False) calculates the correlation matrix\n        # of the concatenated data [X, Y].\n        # The resulting matrix will be (n+m) x (n+m) where n is the number\n        # of columns in X and m is the number of columns in Y.\n        # The top-left n x n block is corr(X, X), the bottom-right m x m block\n        # is corr(Y, Y), and the off-diagonal blocks are corr(X, Y) and corr(Y, X).\n        corr_matrix = np.corrcoef(X, Y, rowvar=False)\n\n    # Round the result to 4 decimal places\n    rounded_corr_matrix = np.round(corr_matrix, 4)\n\n    # Convert to a Python list\n    return rounded_corr_matrix.tolist()\n\nif __name__ == '__main__':\n    # Example Usage 1: Correlation matrix of X with itself\n    X1 = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9],\n        [10, 11, 12]\n    ])\n    print(\"Correlation matrix of X1 with itself:\")\n    corr_matrix_X1 = calculate_correlation_matrix(X1)\n    for row in corr_matrix_X1:\n        print(row)\n    print(\"-\" * 20)\n\n    # Example Usage 2: Correlation matrix of X2 with itself (less perfect correlation)\n    X2 = np.array([\n        [1, 5, 10],\n        [2, 6, 12],\n        [3, 7, 15],\n        [4, 8, 8]\n    ])\n    print(\"Correlation matrix of X2 with itself:\")\n    corr_matrix_X2 = calculate_correlation_matrix(X2)\n    for row in corr_matrix_X2:\n        "}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n\n    The ReLU function returns the input if it's greater than 0,\n    otherwise, it returns 0.\n\n    Args:\n        z: The input value (float).\n\n    Returns:\n        The output after applying the ReLU function (float).\n        Returns z if z > 0, otherwise returns 0.\n    \"\"\"\n    return max(0.0, z)\n\n# Example Usage (optional, for testing)\n# print(f\"relu(5.0) = {relu(5.0)}\")\n# print(f\"relu(-2.5) = {relu(-2.5)}\")\n# print(f\"relu(0.0) = {relu(0.0)}\")\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n  \"\"\"\n  Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n  Args:\n    z: The input value (float).\n    alpha: The slope for negative inputs (float, default is 0.01).\n\n  Returns:\n    The output after applying the Leaky ReLU function (float or int).\n  \"\"\"\n  if z > 0:\n    return z\n  else:\n    return alpha * z\n\n# Example Usage:\nprint(f\"Leaky ReLU(5.0): {leaky_relu(5.0)}\")\nprint(f\"Leaky ReLU(0.0): {leaky_relu(0.0)}\")\nprint(f\"Leaky ReLU(-3.0): {leaky_relu(-3.0)}\")\nprint(f\"Leaky ReLU(-3.0, alpha=0.1): {leaky_relu(-3.0, alpha=0.1)}\")\nprint(f\"Leaky ReLU(10.0, alpha=0.5): {leaky_relu(10.0, alpha=0.5)}\")\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n  \"\"\"\n  Computes the linear kernel between two input vectors x1 and x2.\n\n  The linear kernel is defined as the dot product (inner product) of the two vectors.\n\n  Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n\n  Returns:\n    A scalar value representing the linear kernel (dot product) of x1 and x2.\n  \"\"\"\n  # Ensure inputs are numpy arrays for consistent behavior\n  x1 = np.asarray(x1)\n  x2 = np.asarray(x2)\n\n  # Check if the vectors have the same dimension\n  if x1.shape != x2.shape:\n      raise ValueError(\"Input vectors must have the same dimension.\")\n  if x1.ndim > 1 or x2.ndim > 1:\n      raise ValueError(\"Input must be 1-dimensional vectors.\")\n\n\n  # Compute the dot product\n  # Using np.dot is explicit for dot products of 1D arrays\n  # Alternatively, x1 @ x2 could be used for numpy arrays\n  linear_kernel = np.dot(x1, x2)\n\n  return linear_kernel\n\n# Example Usage:\nif __name__ == '__main__':\n    # Define two example vectors\n    vec1 = np.array([1, 2, 3])\n    vec2 = np.array([4, 5, 6])\n\n    # Compute the linear kernel\n    kernel_val = kernel_function(vec1, vec2)\n\n    # Print the result\n    print(f\"Vector 1: {vec1}\")\n    print(f\"Vector 2: {vec2}\")\n    print(f\"Linear Kernel (Dot Product): {kernel_val}\") # Expected: 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n\n    # Example with different vectors\n    vec3 = np.array([-1, 0, 1])\n    vec4 = np.array([5, -2, 3])\n    kernel_val_2 = kernel_function(vec3, vec4)\n    print(f\"\\nVector 3: {vec3}\")\n    print(f\"Vector 4: {vec4}\")\n    print(f\"Linear Kernel (Dot Product): {kernel_val_2}\") # Expected: -1*5 + 0*(-2) + 1*3 = -5 + 0 + 3 = -2\n\n    # Example with different dimensions (will raise error)\n    try:\n        vec5 = np.array([1, 2])\n        vec6 = np.array([1, 2, 3])\n        kernel_function(vec5, vec6)\n    except ValueError as e:\n        print(f\"\\nCaught expected error: {e}\")\n\n    # Example with non-1D arrays (will raise error)\n    try:\n        mat1 = np.array([[1, 2], [3, 4]])\n        mat2 = np.array([[5, 6], [7, 8]])\n        kernel_function(mat1, mat2)\n    except ValueError as e:\n        print(f\"\\nCaught expected error: {e}\")\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision metric.\n\n    Precision is defined as the ratio of true positives to the sum of\n    true positives and false positives.\n\n    Args:\n        y_true (np.ndarray): A numpy array containing the true binary labels (0 or 1).\n        y_pred (np.ndarray): A numpy array containing the predicted binary labels (0 or 1).\n\n    Returns:\n        float: The precision score. Returns 0 if the denominator (TP + FP) is zero.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Calculate True Positives (TP)\n    # Cases where y_true is 1 and y_pred is 1\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n\n    # Calculate False Positives (FP)\n    # Cases where y_true is 0 and y_pred is 1\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n\n    # Calculate the denominator (TP + FP)\n    denominator = true_positives + false_positives\n\n    # Handle the case where the denominator is zero to avoid division by zero\n    if denominator == 0:\n        return 0.0\n    else:\n        return true_positives / denominator\n\nif __name__ == '__main__':\n    # Example Usage:\n\n    # Example 1: Perfect prediction\n    y_true1 = np.array([1, 1, 0, 0, 1])\n    y_pred1 = np.array([1, 1, 0, 0, 1])\n    # TP = 3, FP = 0, Denominator = 3, Precision = 3/3 = 1.0\n    print(f\"y_true: {y_true1}\")\n    print(f\"y_pred: {y_pred1}\")\n    print(f\"Precision: {precision(y_true1, y_pred1):.4f}\") # Expected: 1.0\n\n    print(\"-\" * 20)\n\n    # Example 2: Some false positives\n    y_true2 = np.array([1, 1, 0, 0, 1])\n    y_pred2 = np.array([1, 0, 1, 0, 1])\n    # TP = 2 (first and fifth elements), FP = 1 (third element)\n    # Denominator = 2 + 1 = 3, Precision = 2/3 = 0.6667\n    print(f\"y_true: {y_true2}\")\n    print(f\"y_pred: {y_pred2}\")\n    print(f\"Precision: {precision(y_true2, y_pred2):.4f}\") # Expected: 0.6667\n\n    print(\"-\" * 20)\n\n    # Example 3: No positive predictions\n    y_true3 = np.array([1, 1, 0, 0, 1])\n    y_pred3 = np.array([0, 0, 0, 0, 0])\n    # TP = 0, FP = 0, Denominator = 0, Precision = 0.0 (handled case)\n    print(f\"y_true: {y_true3}\")\n    print(f\"y_pred: {y_pred3}\")\n    print(f\"Precision: {precision(y_true3, y_pred3):.4f}\") # Expected: 0.0\n\n    print(\"-\" * 20)\n\n    # Example 4: All predictions are positive, some are false positives\n    y_true4 = np.array([1, 0, 0, 0, 1])\n    y_pred4 = np.array([1, 1, 1, 1, 1])\n    # TP = 2 (first and fifth), FP = 3 (second, third, fourth)\n    # Denominator = 2 + 3 = 5, Precision = 2/5 = 0.4\n    print(f\"y_true: {y_true4}\")\n    print(f\"y_pred: {y_pred4}\")\n    print(f\"Precision: {precision(y_true4, y_pred4):.4f}\") # Expected: 0.4\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm.\n\n    Args:\n        f: The objective function to be optimized.\n        grad: A function that computes the gradient of f.\n        x0: Initial parameter values (can be a list or numpy array).\n        learning_rate: The step size.\n        beta1: Exponential decay rate for the first moment estimates.\n        beta2: Exponential decay rate for the second moment estimates.\n        epsilon: A small constant for numerical stability.\n        num_iterations: Number of iterations to run the optimizer.\n\n    Returns:\n        A list containing the optimized parameters, rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0, dtype=float) # Ensure x is a numpy array\n    m = np.zeros_like(x)         # Initialize first moment vector\n    v = np.zeros_like(x)         # Initialize second moment vector\n    t = 0                        # Initialize time step\n\n    for i in range(num_iterations):\n        t += 1\n        g = grad(x) # Compute gradient\n\n        # Update biased first and second moment estimates\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n\n        # Compute bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n\n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round the final parameters to 4 decimal places and convert to list\n    optimized_params = np.round(x, 4).tolist()\n\n    return optimized_params\n\n# Example Usage (Optional - for testing)\nif __name__ == '__main__':\n    # Example 1: Minimize a simple quadratic function f(x) = x^2\n    def quadratic_f(x):\n        return np.sum(x**2)\n\n    def quadratic_grad(x):\n        return 2 * x\n\n    initial_params_1 = [5.0]\n    optimized_params_1 = adam_optimizer(quadratic_f, quadratic_grad, initial_params_1, num_iterations=1000)\n    print(f\"Example 1 (Quadratic): Initial: {initial_params_1}, Optimized: {optimized_params_1}\")\n    # Expected output should be close to [0.0000]\n\n    # Example 2: Minimize a 2D function f(x, y) = (x-1)^2 + (y-2)^2\n    def multi_dim_f(params):\n        x, y = params\n        return (x - 1)**2 + (y - 2)**2\n\n    def multi_dim_grad(params):\n        x, y = params\n        grad_x = 2 * (x - 1)\n        grad_y = 2 * (y - 2)\n        return np.array([grad_x, grad_y])\n\n    initial_params_2 = [0.0, 0.0]\n    optimized_params_2 = adam_optimizer(multi_dim_f, multi_dim_grad, initial_params_2, num_iterations=1000)\n    print(f\"Example 2 (2D): Initial: {initial_params_2}, Optimized: {optimized_params_2}\")\n    # Expected output should be close to [1.0000, 2.0000]\n\n    # Example 3: Using default num_iterations=10 (less optimization)\n    initial_params_3 = [5.0]\n    optimized_params_3 = adam_optimizer(quadratic_f, quadratic_grad, initial_params_3, num_iterations=10)\n    print(f\"Example 3 (Quadratic, 10 iter): Initial: {initial_params_3}, Optimized: {optimized_params_3}\")\n    # Expected output will be closer to initial_params_3 than Example 1\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculates the recall metric for binary classification.\n\n    Recall is the ratio of true positives to the total number of actual positives.\n\n    Args:\n        y_true: A list or numpy array of true binary labels (0 or 1).\n        y_pred: A list or numpy array of predicted binary labels (0 or 1).\n\n    Returns:\n        The recall value rounded to three decimal places. Returns 0.0 if there\n        are no actual positive instances.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier processing\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Ensure inputs are binary (optional but good practice)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n         raise ValueError(\"Inputs y_true and y_pred must contain only 0s and 1s\")\n\n    # Calculate True Positives (TP): instances where true is 1 and pred is 1\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n\n    # Calculate False Negatives (FN): instances where true is 1 and pred is 0\n    # false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n\n    # The denominator for recall is TP + FN, which is the total number of actual positives\n    actual_positives = np.sum(y_true == 1) # This is equivalent to TP + FN\n\n    # Handle the case where there are no actual positive instances\n    if actual_positives == 0:\n        return 0.0\n    else:\n        # Calculate recall\n        recall_value = true_positives / actual_positives\n        # Return the recall value rounded to three decimal places\n        return round(recall_value, 3)\n\n# Example Usage:\n# y_true = [0, 1, 0, 1, 1, 0, 1, 0]\n# y_pred = [0, 1, 1, 1, 0, 0, 1, 0]\n#\n# # TP = 3 (instances 2, 3, 6)\n# # FN = 1 (instance 4)\n# # Actual Positives = 4 (instances 1, 3, 4, 6)\n# # Recall = TP / (TP + FN) = 3 / 4 = 0.75\n#\n# print(f\"True labels: {y_true}\")\n# print(f\"Predicted labels: {y_pred}\")\n# print(f\"Calculated Recall: {recall(y_true, y_pred)}\") # Expected output: 0.75\n\n# Example with no actual positives\n# y_true_no_pos = [0, 0, 0, 0]\n# y_pred_no_pos = [0, 1, 0, 1]\n# print(f\"True labels: {y_true_no_pos}\")\n# print(f\"Predicted labels: {y_pred_no_pos}\")\n# print(f\"Calculated Recall (no positives): {recall(y_true_no_pos, y_pred_no_pos)}\") # Expected output: 0.0\n\n# Example with perfect recall\n# y_true_perfect = [0, 1, 0, 1]\n# y_pred_perfect = [0, 1, 0, 1]\n# print(f\"True labels: {y_true_perfect}\")\n# print(f\"Predicted labels: {y_pred_perfect}\")\n# print(f\"Calculated Recall (perfect): {recall(y_true_perfect, y_pred_perfect)}\") # Expected output: 1.0\n\n# Example with zero recall\n# y_true_zero = [0, 1, 0, 1]\n# y_pred_zero = [0, 0, 0, 0]\n# print(f\"True labels: {y_true_zero}\")\n# print(f\"Predicted labels: {y_pred_zero}\")\n# print(f\"Calculated Recall (zero): {recall(y_true_zero, y_pred_zero)}\") # Expected output: 0.0\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n\n    Args:\n        X (np.ndarray): Input matrix (e.g., word embeddings) of shape (seq_len, embed_dim).\n        W_q (np.ndarray): Weight matrix for queries of shape (embed_dim, d_k).\n        W_k (np.ndarray): Weight matrix for keys of shape (embed_dim, d_k).\n        W_v (np.ndarray): Weight matrix for values of shape (embed_dim, d_v).\n\n    Returns:\n        list: The self-attention output as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    # 1. Linear Transformations to get Q, K, V\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n\n    # Get dimension of keys (d_k)\n    d_k = K.shape[-1]\n\n    # 2. Calculate Scaled Dot-Product Attention\n    # Calculate QK^T\n    scores = Q @ K.T\n\n    # Scale scores\n    scaled_scores = scores / np.sqrt(d_k)\n\n    # Apply softmax to get attention weights\n    # Softmax function: exp(x) / sum(exp(x))\n    # To prevent numerical instability, subtract the maximum value before exponentiation\n    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n\n    # Multiply attention weights by V\n    output = attention_weights @ V\n\n    # Round the output to 4 decimal places\n    rounded_output = np.round(output, 4)\n\n    # Return as a Python list\n    return rounded_output.tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Define dimensions\n    seq_len = 3\n    embed_dim = 4\n    d_k = 2 # Dimension of keys and queries\n    d_v = 3 # Dimension of values\n\n    # Create dummy input data and weights\n    np.random.seed(42) # for reproducibility\n    X = np.random.rand(seq_len, embed_dim)\n    W_q = np.random.rand(embed_dim, d_k)\n    W_k = np.random.rand(embed_dim, d_k)\n    W_v = np.random.rand(embed_dim, d_v)\n\n    print(\"Input X:\\n\", np.round(X, 4))\n    print(\"\\nWeight W_q:\\n\", np.round(W_q, 4))\n    print(\"\\nWeight W_k:\\n\", np.round(W_k, 4))\n    print(\"\\nWeight W_v:\\n\", np.round(W_v, 4))\n\n    # Calculate self-attention output\n    attention_output = self_attention(X, W_q, W_k, W_v)\n\n    print(\"\\nSelf-Attention Output (list, rounded to 4 decimals):\")\n    # Print the list content nicely\n    for row in attention_output:\n        print(row)\n\n    # Verify shapes during intermediate steps (optional)\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    print(\"\\nIntermediate Shapes:\")\n    print(\"Q shape:\", Q.shape) # Should be (seq_len, d_k)\n    print(\"K shape:\", K.shape) # Should be (seq_len, d_k)\n    print(\"V shape:\", V.shape) # Should be (seq_len, d_v)\n    scores = Q @ K.T\n    print(\"Scores (QK^T) shape:\", scores.shape) # Should be (seq_len, seq_len)\n    d_k_val = K.shape[-1]\n    scaled_scores = scores / np.sqrt(d_k_val)\n    exp_scores = np.exp(scaled_scores - np.max"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass of a simple RNN cell for a sequence.\n\n    Args:\n        input_sequence: A list of input vectors (list of lists of floats).\n                        Shape: (sequence_length, input_size)\n        initial_hidden_state: The initial hidden state vector (list of floats).\n                              Shape: (hidden_size,)\n        Wx: The input-to-hidden weight matrix (list of lists of floats).\n            Shape: (hidden_size, input_size)\n        Wh: The hidden-to-hidden weight matrix (list of lists of floats).\n            Shape: (hidden_size, hidden_size)\n        b: The bias vector (list of floats).\n           Shape: (hidden_size,)\n\n    Returns:\n        The final hidden state vector after processing the sequence,\n        "}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the KL divergence between two univariate normal distributions.\n\n    The KL divergence D(P || Q) between two normal distributions\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2) is given by:\n    D(P || Q) = 0.5 * (log(sigma_q^2 / sigma_p^2) + (sigma_p^2 + (mu_p - mu_q)^2) / sigma_q^2 - 1)\n\n    Args:\n        mu_p (float): Mean of the first normal distribution P.\n        sigma_p (float): Standard deviation of the first normal distribution P.\n        mu_q (float): Mean of the second normal distribution Q.\n        sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n        float: The KL divergence D(P || Q).\n               Returns np.nan if sigma_p or sigma_q are non-positive,\n               as standard deviation must be positive for a normal distribution.\n    \"\"\"\n    # Ensure standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        # Standard deviation must be positive for a valid normal distribution.\n        # The formula involves log(sigma_p) and division by sigma_q^2.\n        # Returning NaN indicates invalid input for the formula.\n        return np.nan\n\n    sigma_p_sq = sigma_p**2\n    sigma_q_sq = sigma_q**2\n    mu_diff_sq = (mu_p - mu_q)**2\n\n    # KL divergence formula for univariate normal distributions\n    kl_div = 0.5 * (np.log(sigma_q_sq / sigma_p_sq) + (sigma_p_sq + mu_diff_sq) / sigma_q_sq - 1)\n\n    return kl_div\n\nif __name__ == '__main__':\n    # Example Usage:\n    # P ~"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Gauss-Seidel method.\n\n    Args:\n        A (np.ndarray): The square coefficient matrix.\n        b (np.ndarray): The right-hand side vector.\n        n (int): The number of iterations to perform.\n        x_ini (np.ndarray, optional): The initial guess for the solution vector x.\n                                      If None, a vector of zeros is used.\n\n    Returns:\n        list: The approximated solution vector x after n iterations,\n              with elements rounded to 4 decimal places.\n    \"\"\"\n    # Ensure A and b are numpy arrays\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    # Get the size of the system\n    N = A.shape[0]\n\n    # Initialize the solution vector x\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.asarray(x_ini, dtype=float).copy() # Use a copy to avoid modifying input\n\n    # Perform n iterations\n    for _ in range(n):\n        # Iterate through each equation (row)\n        for i in range(N):\n            # Calculate the sum of terms A[i, j] * x[j] for j != i\n            # In Gauss-Seidel, we use the most recently updated values of x\n            sum_terms = 0\n            for j in range(N):\n                if i != j:\n                    sum_terms += A[i, j] * x[j]\n\n            # Update x[i] using the Gauss-Seidel formula\n            # x_i = (b_i - sum(A_ij * x_j for j != i)) / A_ii\n            # Check for zero diagonal element to avoid division by zero\n            if A[i, i] == 0:\n                # This case indicates the method might fail or require pivoting.\n                # For this task, we assume valid input where A[i,i] is non-zero\n                # or the user handles potential issues outside this function.\n                # A more robust implementation might raise an error or return NaN.\n                # We'll proceed assuming A[i,i] is non-zero for the relevant problems.\n                 pass # Or handle error: print(f\"Warning: Diagonal element A[{i},{i}] is zero.\")\n            else:\n"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Gaussian Elimination\n    with partial pivoting and backward substitution.\n\n    Args:\n        A (np.ndarray): The square coefficient matrix (n x n).\n        b (np.ndarray): The right-hand side vector (n).\n\n    Returns:\n        list: The solution vector x as a Python list, rounded to 4 decimal places.\n\n    Raises:\n        ValueError: If the input dimensions are"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(x)\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        # Weights for forget, input, candidate, output gates\n        # Each weight matrix combines input and previous hidden state\n        # Shape: (hidden_size, input_size + hidden_size)\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01 # Scale down for better initialization\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n\n        # Biases for forget, input, candidate, output gates\n        # Shape: (hidden_size, 1)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step,\n        as well as the final hidden state and final cell state.\n\n        Args:\n            x (np.ndarray): Input sequence of shape (sequence_length, input_size).\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1).\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1).\n\n        Returns:\n            tuple: A tuple containing:\n                - hidden_states_list (list): List of hidden states at each time step,\n                                             each element is a list of shape (hidden_size, 1)\n                                             converted to list and rounded.\n                - final_hidden_state (list): The final hidden state, a list of shape (hidden_size, 1)\n                                             converted to list and rounded.\n                - final_cell_state (list): The final cell state, a list of shape (hidden_size, 1)\n                                           converted to list and rounded.\n        \"\"\"\n        sequence_length = x.shape[0]\n        ht = initial_hidden_state\n        ct = initial_cell_state\n\n        hidden_states_at_each_step = []\n\n        for t in range(sequence_length):\n            # Get current input xt (shape: (input_size,)) and reshape to (input_size, 1)\n            xt = x[t].reshape(-1, 1)\n\n            # Concatenate previous hidden state and current input\n            # combined_input shape: (input_size + hidden_size, 1)\n            combined_input = np.vstack((ht, xt))\n\n            # Forget Gate: decides what to throw away from the cell state\n            # ft shape: (hidden_size, 1)\n            ft = sigmoid(np.dot(self.Wf, combined_input) + self.bf)\n\n            # Input Gate: decides what new information to store in the cell state\n            # it shape: (hidden_size, 1)\n            it = sigmoid(np.dot(self.Wi, combined_input) + self.bi)\n\n            # Candidate Cell State: new candidate values that could be added to the cell state\n            # ct_candidate shape: (hidden_size, 1)\n            ct_candidate = tanh(np.dot(self.Wc, combined_input) + self.bc)\n\n            # Update Cell State\n            # ct shape: (hidden_size, 1)\n            ct"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels (binary, e.g., 0 or 1)\n    :param y_pred: Numpy array of predicted labels (binary, e.g., 0 or 1)\n    :param beta: The weight of precision in the harmonic mean. beta > 0.\n                 beta=1 gives the F1-Score.\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Ensure inputs are binary (optional but good practice)\n    # assert np.all(np.isin(y_true, [0, 1])), \"y_true must be binary (0 or 1)\"\n    # assert np.all(np.isin(y_pred, [0, 1])), \"y_pred must be binary (0 or 1)\"\n    # assert beta > 0, \"beta must be greater than 0\"\n\n\n    # Calculate True Positives, False Positives, False Negatives\n    # Assuming positive class is 1 and negative class is 0\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Calculate Precision\n    # Handle case where tp + fp is 0 (no positive predictions)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n\n    # Calculate Recall\n    # Handle case where tp + fn is 0 (no actual positive instances)\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n\n    # Calculate F-Score\n    beta_squared = beta**2\n    denominator = (beta_squared * precision) + recall\n\n    # Handle case where both precision and recall are 0 (denominator is 0)\n    if denominator == 0:\n        f_score_val = 0.0\n    else:\n        f_score_val = (1 + beta_squared) * (precision * recall) / denominator\n\n    return round(f_score_val, 3)\n\nif __name__ == '__main__':\n    # Example Usage:\n\n    # Example 1: Perfect prediction\n    y_true1 = np.array([1, 1, 0, 0, 1])\n    y_pred1 = np.array([1, 1, 0, 0, 1])\n    f1_score1 = f_score(y_true1, y_pred1, beta=1)\n    f0_5_score1 = f_score(y_true1, y_pred1, beta=0.5)\n    f2_score1 = f_score(y_true1, y_pred1, beta=2)\n    print(f\"Example 1 (Perfect): F1={f1_score1}, F0.5={f0_5_score1}, F2={f2_score1}\") # Expected: 1.0, 1.0, 1.0\n\n    # Example 2: Some errors\n    y_true2 = np.array([1, 1, 0, 0, 1, 0, 1, 0])\n    y_pred2 = np.array([1, 0, 0, 1, 1, 0, 1, 1])\n    # TP = 3 (indices 0, 4, 6)\n    # FP = 2 (indices 3, 7)\n    # FN = 1 (index 1)\n    # TN = 2 (indices 2, 5)\n    # Precision = 3 / (3 + 2) = 3/5 = 0.6\n    # Recall = 3 / (3 + 1) = 3/4 = 0.75\n    # F1 = 2 * (0.6 * 0.75) / (0.6 + 0.75) = 2 * 0.45 / 1.35 = 0.9 / 1.35 = 0.666...\n    f1_score2 = f_score(y_true2, y_pred2, beta=1)\n    f0_5_score2 = f_score(y_true2, y_pred2, beta=0.5)\n    f2_score2 = f_score(y_true2, y_pred2, beta=2)\n    print(f\"Example 2 (Errors): F"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Handle empty list case\n    if not y:\n        return 0.0\n\n    # Convert list to numpy array for easier processing\n    y_np = np.array(y)\n\n    # Get unique classes and their counts\n    classes, counts = np.unique(y_np, return_counts=True)\n\n    # Calculate total number of instances\n    total_instances = len(y_np)\n\n    # Calculate the proportion of each class\n    proportions = counts / total_instances\n\n    # Calculate the sum of squared proportions\n    sum_sq_proportions = np.sum(proportions**2)\n\n    # Calculate Gini impurity\n    gini = 1 - sum_sq_proportions\n\n    # Return rounded result\n    return round(gini, 3)\n\nif __name__ == '__main__':\n    # Example Usage:\n    labels1 = [0, 1, 0, 2, 1, 0]\n    gini1 = gini_impurity(labels1)\n    print(f\"Labels: {labels1}\")\n    print(f\"Gini Impurity: {gini1}\") # Expected: 1 - ((3/6)^2 + (2/6)^2 + (1/6)^2) = 1 - (9/36 + 4/36 + 1/36) = 1 - 14/36 = 1 - 7/18 \u2248 0.611\n\n    labels2 = [0, 0, 0, 0] # Pure node\n    gini2 = gini_impurity(labels2)\n    print(f\"\\nLabels: {labels2}\")\n    print(f\"Gini Impurity: {gini2}\") # Expected: 1 - (4/4)^2 = 1 - 1 = 0.0\n\n    labels3 = [0, 1, 0, 1] # Maximum impurity for 2 classes\n    gini3 = gini_impurity(labels3)\n    print(f\"\\nLabels: {labels3}\")\n    print(f\"Gini Impurity: {gini3}\") # Expected: 1 - ((2/4)^2 + (2/4)^2) = 1 - (1/4 + 1/4) = 1 - 1/2 = 0.5\n\n    labels4 = [] # Empty list\n    gini4 = gini_impurity(labels4)\n    print(f\"\\nLabels: {labels4}\")\n    print(f\"Gini Impurity: {gini4}\") # Expected: 0.0\n\n    labels5 = [1, 2, 3, 4, 5] # All unique classes\n    gini5 = gini_impurity(labels5)\n    print(f\"\\nLabels: {labels5}\")\n    print(f\"Gini Impurity: {gini5}\") # Expected: 1 - 5 * (1/5)^2 = 1 - 5/25 = 1 - 1/5 = 0.8\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = []\n    nnz_count = 0  # Non-zero count\n\n    row_pointers.append(0) # The first element of row_pointers is always 0\n\n    for row in dense_matrix:\n        for j, element in enumerate(row):\n            # Assuming non-zero means any value other than 0.\n            # If specific type (e.g., float) requires tolerance, adjust condition.\n            if element != 0:\n                values.append(element)\n                col_indices.append(j)\n                nnz_count += 1\n        # After processing a row, the current nnz_count is the start index\n        # for the *next* row.\n        row_pointers.append(nnz_count)\n\n    # The last element of row_pointers is the total number of non-zeros,\n    # which is already added in the loop after the last row is processed.\n    # So the loop structure naturally builds the correct row_pointers.\n\n    return (values, col_indices, row_pointers)\n\nif __name__ == '__main__':\n    # Example Usage:\n    dense_matrix_1 = [\n        [1, 0, 0, 2],\n        [0, 0, 3, 0],\n        [0, 4, 0, 5]\n    ]\n\n    values_1, col_indices_1, row_pointers_1 = compressed_row_sparse_matrix(dense_matrix_1)\n\n    print(\"Dense Matrix 1:\")\n    for row in dense_matrix_1:\n        print(row)\n\n    print(\"\\nCSR Representation 1:\")\n    print(\"Values:\", values_1)\n    print(\"Column Indices:\", col_indices_1)\n    print(\"Row Pointers:\", row_pointers_1)\n    # Expected:\n    # Values: [1, 2, 3, 4, 5]\n    # Column Indices: [0, 3, 2, 1, 3]\n    # Row Pointers: [0, 2, 3, 5]\n\n    print(\"-\" * 20)\n\n    dense_matrix_2 = [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]\n    ]\n\n    values_2, col_indices_2, row_pointers_2 = compressed_row_sparse_matrix(dense_matrix_2)\n\n    print(\"Dense Matrix 2 (All Zeros):\")\n    for row in dense_matrix_2:\n        print(row)\n\n    print(\"\\nCSR Representation 2:\")\n    print(\"Values:\", values_2)\n    print(\"Column Indices:\", col_indices_2)\n    print(\"Row Pointers:\", row_pointers_2)\n    # Expected:\n    # Values: []\n    # Column Indices: []\n    # Row Pointers: [0, 0, 0, 0]\n\n    print(\"-\" * 20)\n\n    dense_matrix_3 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n\n    values_3, col_indices_3, row_pointers_3 = compressed_row_sparse_matrix(dense_matrix_3)\n\n    print(\"Dense Matrix 3 (No Zeros):\")\n    for row in dense_matrix_3:\n        print(row)\n\n    print(\"\\nCSR Representation 3:\")\n    print(\"Values:\", values_3)\n    print(\"Column Indices:\", col_indices_3)\n    print(\"Row Pointers:\", row_pointers_3)\n    # Expected:\n    # Values: [1, 2, 3, 4,"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix:\n        return [], [], [0]\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    values = []\n    row_indices = []\n    column_pointer = [0]\n\n    for j in range(num_cols):\n        for i in range(num_rows):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\nif __name__ == '__main__':\n    # Example Usage\n    dense_matrix_1 = [\n        [1, 0, 0, 2],\n        [0, 0, 3, 0],\n        [4, 5, 6, 0]\n    ]\n\n    values_1, row_indices_1, column_pointer_1 = compressed_col_sparse_matrix(dense_matrix_1)\n    print(\"Dense Matrix 1:\")\n    for row in dense_matrix_1:\n        print(row)\n    print(\"CSC Representation 1:\")\n    print(\"Values:\", values_1)\n    print(\"Row Indices:\", row_indices_1)\n    print(\"Column Pointer:\", column_pointer_1)\n    print(\"-\" * 20)\n\n    dense_matrix_2 = [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]\n    ]\n\n    values_2, row_indices_2, column_pointer_2 = compressed_col_sparse_matrix(dense_matrix_2)\n    print(\"Dense Matrix 2:\")\n    for row in dense_matrix_2:\n        print(row)\n    print(\"CSC Representation 2:\")\n    print(\"Values:\", values_2)\n    print(\"Row Indices:\", row_indices_2)\n    print(\"Column Pointer:\", column_pointer_2)\n    print(\"-\" * 20)\n\n    dense_matrix_3 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n\n    values_3, row_indices_3, column_pointer_3 = compressed_col_sparse_matrix(dense_matrix_3)\n    print(\"Dense Matrix 3:\")\n    for row in dense_matrix_3:\n        print(row)\n    print(\"CSC Representation 3:\")\n    print(\"Values:\", values_3)\n    print(\"Row Indices:\", row_indices_3)\n    print(\"Column Pointer:\", column_pointer_3)\n    print(\"-\" * 20)\n\n    dense_matrix_4 = []\n    values_4, row_indices_4, column_pointer_4 = compressed_col_sparse_matrix(dense_matrix_4)\n    print(\"Dense Matrix 4 (Empty):\")\n    print(dense_matrix_4)\n    print(\"CSC Representation 4:\")\n    print(\"Values:\", values_4)\n    print(\"Row Indices:\", row_indices_4)\n    print(\"Column Pointer:\", column_pointer_4)\n    print(\"-\" * 20)\n\n    dense_matrix_5 = [[]]\n    values_5, row_indices_5, column_pointer_5 = compressed_col_sparse_matrix(dense_matrix_5)\n    print(\"Dense Matrix 5 (Empty rows):\")\n    print(dense_matrix_5)\n    print(\"CSC Representation 5:\")\n    print(\"Values:\", values_5)\n    print(\"Row Indices:\", row_indices_5)\n    print(\"Column Pointer:\", column_pointer_5)\n    print(\"-\" * 20)\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Calculates the basis for the column space (image) of a matrix A.\n\n    Args:\n        A: A NumPy array representing the matrix.\n\n    Returns:\n        A list of lists representing the basis vectors for the column space\n        of A, rounded to 8 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for consistent handling\n    A = np.array(A, dtype=float)\n\n    # Get the shape of the matrix\n    m, n = A.shape\n\n    # Create a copy to avoid modifying the original matrix\n    A_copy = A.copy()\n\n    # Perform Gaussian elimination to find the row echelon form\n    # We track the original column indices to identify pivot columns\n    pivot_columns = []\n    r = 0  # Current row\n    for j in range(n):  # Iterate through columns\n        # Find the pivot element in the current column below the current row\n        pivot_row = r\n        for i in range(r + 1, m):\n            if abs(A_copy[i, j]) > abs(A_copy[pivot_row, j]):\n                pivot_row = i\n\n        # If a non-zero pivot is found\n        if abs(A_copy[pivot_row, j]) > 1e-9:  # Use a tolerance for floating point comparisons\n            # Swap the current row with the pivot row\n            A_copy[[r, pivot_row]] = A_copy[[pivot_row, r]]\n\n            # Normalize the pivot row\n            pivot_element = A_copy[r, j]\n            A_copy[r, :] /= pivot_element\n\n            # Eliminate other entries in the current column\n            for i in range(m):\n                if i != r:\n                    factor = A_copy[i, j]\n                    A_copy[i, :] -= factor * A_copy[r, :]\n\n            # Record the original column index as a pivot column\n            pivot_columns.append(j)\n\n            # Move to the next row\n            r += 1\n\n            # If we have processed all rows, we can stop\n            if r == m:\n                break\n\n    # The basis for the column space are the original columns of A\n    # corresponding to the pivot columns in the row echelon form.\n    basis_vectors = A[:, pivot_columns]\n\n    # Round the basis vectors to 8 decimal places and convert to a list\n    return np.round(basis_vectors, 8).tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n    matrix1 = np.array([[1, 2, 3],\n                        [4, 5, 6],\n                        [7, 8, 9]])\n    image1 = matrix_image(matrix1)\n    print(\"Matrix 1:\")\n    print(matrix1)\n    print(\"Image (Column Space) Basis:\")\n    print(image1)\n    print(\"-\" * 20)\n\n    matrix2 = np.array([[1, 0, 1],\n                        [0, 1, 1],\n                        [0, 0, 0]])\n    image2 = matrix_image(matrix2)\n    print(\"Matrix 2:\")\n    print(matrix2)\n    print(\"Image (Column Space) Basis:\")\n    print(image2)\n    print(\"-\" * 20)\n\n    matrix3 = np.array([[1, 2, 3, 4],\n                        [5, 6, 7, 8],\n                        [9, 10, 11, 12],\n                        [13, 14, 15, 16]])\n    image3 = matrix_image(matrix3)\n    print(\"Matrix 3:\")\n    print(matrix3)\n    print(\"Image (Column Space) Basis:\")\n    print(image3)\n    print(\"-\" * 20)\n\n    matrix4 = np.array([[1, 1],\n                        [1, 1]])\n    image4 = matrix_image(matrix4)\n    print(\"Matrix 4:\")\n    print(matrix4)\n    print(\"Image (Column Space) Basis:\")\n    print(image4)\n    print(\"-\" * 20)\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value for regression analysis.\n\n    Args:\n        y_true (np.ndarray): Array of true values.\n        y_pred (np.ndarray): Array of predicted values.\n\n    Returns:\n        float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    # Calculate the total sum of squares (SST)\n    sst = np.sum((y_true - np.mean(y_true))**2)\n\n    # Calculate the residual sum of squares (SSR)\n    ssr = np.sum((y_true - y_pred)**2)\n\n    # Calculate R-squared\n    r2 = 1 - (ssr / sst)\n\n    return round(r2, 3)\n\nif __name__ == '__main__':\n    # Example usage:\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n\n    r2_value = r_squared(y_true, y_pred)\n    print(f\"R-squared: {r2_value}\")\n\n    y_true_perfect = np.array([1, 2, 3, 4])\n    y_pred_perfect = np.array([1, 2, 3, 4])\n    r2_perfect = r_squared(y_true_perfect, y_pred_perfect)\n    print(f\"R-squared (perfect prediction): {r2_perfect}\")\n\n    y_true_poor = np.array([1, 2, 3, 4])\n    y_pred_poor = np.array([5, 5, 5, 5])\n    r2_poor = r_squared(y_true_poor, y_pred_poor)\n    print(f\"R-squared (poor prediction): {r2_poor}\")\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n\n    Args:\n        img: A 2D list representing the grayscale image.\n\n    Returns:\n        The average brightness rounded to two decimal places, or -1 if\n        there are edge cases (empty image, inconsistent row lengths,\n        invalid pixel values).\n    \"\"\"\n    if not img:\n        return -1\n\n    num_rows = len(img)\n    num_cols = len(img[0]) if num_rows > 0 else 0\n\n    if num_cols == 0 and num_rows > 0:\n        return -1\n\n    total_brightness = 0\n    num_pixels = 0\n\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n\n    if num_pixels == 0:\n        return -1\n\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)\n\nif __name__ == '__main__':\n    # Example Usage:\n    img1 = [\n        [10, 20, 30],\n        [40, 50, 60],\n        [70, 80, 90]\n    ]\n    print(f\"Brightness of img1: {calculate_brightness(img1)}\")  # Expected: 50.0\n\n    img2 = []\n    print(f\"Brightness of img2 (empty): {calculate_brightness(img2)}\")  # Expected: -1\n\n    img3 = [\n        [10, 20],\n        [30, 40, 50]  # Inconsistent row length\n    ]\n    print(f\"Brightness of img3 (inconsistent rows): {calculate_brightness(img3)}\")  # Expected: -1\n\n    img4 = [\n        [10, 20, 300],  # Invalid pixel value\n        [40, 50, 60]\n    ]\n    print(f\"Brightness of img4 (invalid pixel): {calculate_brightness(img4)}\")  # Expected: -1\n\n    img5 = [\n        [255, 255],\n        [0, 0]\n    ]\n    print(f\"Brightness of img5: {calculate_brightness(img5)}\") # Expected: 127.5\n\n    img6 = [\n        [100]\n    ]\n    print(f\"Brightness of img6: {calculate_brightness(img6)}\") # Expected: 100.0\n\n    img7 = [\n        []\n    ]\n    print(f\"Brightness of img7 (empty row): {calculate_brightness(img7)}\") # Expected: -1\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Args:\n        y_true (array-like): Array of actual values.\n        y_pred (array-like): Array of predicted values.\n\n    Returns:\n        float: The calculated RMSE rounded to three decimal places.\n\n    Raises:\n        ValueError: If input arrays have different shapes or are empty.\n        TypeError: If input arrays contain non-numeric elements that cannot be\n                   converted to a numeric type by numpy.\n    \"\"\"\n    try:\n        y_true_np = np.asarray(y_true, dtype=float)\n        y_pred_np = np.asarray(y_pred, dtype=float)\n    except ValueError:\n        # This handles cases where asarray might fail due to non-numeric strings etc.\n        raise TypeError(\"Input arrays must contain numeric values.\")\n\n    if y_true_np.shape != y_pred_np.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    if y_true_np.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n\n    # Calculate squared differences\n    squared_errors = (y_true_np - y_pred_np) ** 2\n\n    # Calculate mean of squared errors\n    mean_squared_error = np.mean(squared_errors)\n\n    # Calculate RMSE\n    rmse_value = np.sqrt(mean_squared_error)\n\n    # Return rounded value\n    return round(rmse_value, 3)\n\nif __name__ == '__main__':\n    # Example Usage\n    y_true1 = [1.0, 2.0, 3.0, 4.0, 5.0]\n    y_pred1 = [1.1, 2.1, 3.1, 4.1, 5.1]\n    print(f\"y_true: {y_true1}\")\n    print(f\"y_pred: {y_pred1}\")\n    print(f\"RMSE: {rmse(y_true1, y_pred1)}\") # Expected: sqrt(0.01) = 0.1\n\n    y_true2 = np.array([10, 20, 30])\n    y_pred2 = np.array([12, 18, 35])\n    print(f\"\\ny_true: {y_true2}\")\n    print(f\"y_pred: {y_pred2}\")\n    # Errors: (10-12)^2=4, (20-18)^2=4, (30-35)^2=25\n    # Mean squared error = (4+4+25)/3 = 33/3 = 11\n    # RMSE = sqrt(11) approx 3.3166\n    print(f\"RMSE: {rmse(y_true2, y_pred2)}\") # Expected: ~3.317\n\n    # Edge Cases\n    print(\"\\nTesting edge cases:\")\n\n    # Mismatched shapes\n    y_true_shape_mismatch = [1, 2]\n    y_pred_shape_mismatch = [1, 2, 3]\n    try:\n        rmse(y_true_shape_mismatch, y_pred_shape_mismatch)\n    except ValueError as e:\n        print(f\"Caught expected error for shape mismatch: {e}\")\n\n    # Empty arrays\n    y_true_empty = []\n    y_pred_empty = []\n    try:\n        rmse(y_true_empty, y_pred_empty)\n    except ValueError as e:\n        print(f\"Caught expected error for empty arrays: {e}\")\n\n    # Invalid input types (non-numeric string)\n    y_true_invalid_type = [1, 2, 'a']\n    y_pred_invalid_type = [1, 2, 3]\n    try:\n        rmse(y_true_invalid_type, y_pred_invalid_type)\n    except TypeError as e:\n        print(f\"Caught expected error for invalid type: {e}\")\n\n    # Invalid input types (None)\n    y_true_invalid_type2 = [1, 2, None]\n    y_pred_invalid_type2 = [1, 2, 3]\n    try:\n        rmse(y_true_invalid_type2, y_pred_invalid_type2)\n    except TypeError as e:\n         print(f\"Caught expected error for invalid type (None): {e}\")\n\n    # Valid input with different types (list and numpy array)\n    y_true_mixed = [1, 2, 3]\n    y_pred_mixed = np.array([1.1, 2.1, 3.1])\n    print(f\"\\ny_true: {y_true_mixed}\")\n    print(f\"y_pred: {y_pred_mixed}\")\n    print(f\"RMSE: {rmse(y_true_mixed, y_pred_mixed)}\") # Expected: 0.1\n\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Args:\n        y_true (np.ndarray): True binary labels.\n        y_pred (np.ndarray): Predicted binary labels.\n\n    Returns:\n        float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n\n    if union == 0:\n        # Handle the case where both arrays are all zeros\n        return 1.0\n    else:\n        return round(intersection / union, 3)\n\nif __name__ == '__main__':\n    # Example usage:\n    y_true1 = np.array([0, 1, 1, 0, 1])\n    y_pred1 = np.array([1, 1, 0, 0, 1])\n    jaccard1 = jaccard_index(y_true1, y_pred1)\n    print(f\"Jaccard Index 1: {jaccard1}\")  # Expected output: 0.5\n\n    y_true2 = np.array([1, 1, 1, 1])\n    y_pred2 = np.array([1, 1, 1, 1])\n    jaccard2 = jaccard_index(y_true2, y_pred2)\n    print(f\"Jaccard Index 2: {jaccard2}\")  # Expected output: 1.0\n\n    y_true3 = np.array([0, 0, 0, 0])\n    y_pred3 = np.array([1, 1, 1, 1])\n    jaccard3 = jaccard_index(y_true3, y_pred3)\n    print(f\"Jaccard Index 3: {jaccard3}\")  # Expected output: 0.0\n\n    y_true4 = np.array([0, 0, 0, 0])\n    y_pred4 = np.array([0, 0, 0, 0])\n    jaccard4 = jaccard_index(y_true4, y_pred4)\n    print(f\"Jaccard Index 4: {jaccard4}\")  # Expected output: 1.0\n\n    y_true5 = np.array([1, 0, 1, 0])\n    y_pred5 = np.array([0, 1, 0, 1])\n    jaccard5 = jaccard_index(y_true5, y_pred5)\n    print(f\"Jaccard Index 5: {jaccard5}\")  # Expected output: 0.0\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score (S\u00f8rensen-Dice coefficient or F1-score) for binary classification.\n\n    Args:\n        y_true (np.ndarray): A binary numpy array representing the true labels.\n        y_pred (np.ndarray): A binary numpy array representing the predicted labels.\n\n    Returns:\n        float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(np.bool_)\n    y_pred = np.asarray(y_pred).astype(np.bool_)\n\n    intersection = np.sum(y_true & y_pred)\n    sum_of_sets = np.sum(y_true) + np.sum(y_pred)\n\n    # Handle the case where both y_true and y_pred are all zeros\n    if sum_of_sets == 0:\n        return 1.0  # Perfect overlap when both are empty sets\n\n    dice = (2. * intersection) / sum_of_sets\n\n    return round(dice, 3)\n\nif __name__ == '__main__':\n    # Example usage:\n    y_true1 = np.array([0, 1, 1, 0, 1])\n    y_pred1 = np.array([0, 1, 0, 0, 1])\n    score1 = dice_score(y_true1, y_pred1)\n    print(f\"Dice Score 1: {score1}\")  # Expected output: ~0.667\n\n    y_true2 = np.array([1, 1, 1, 1])\n    y_pred2 = np.array([1, 1, 1, 1])\n    score2 = dice_score(y_true2, y_pred2)\n    print(f\"Dice Score 2: {score2}\")  # Expected output: 1.0\n\n    y_true3 = np.array([0, 0, 0, 0])\n    y_pred3 = np.array([0, 0, 0, 0])\n    score3 = dice_score(y_true3, y_pred3)\n    print(f\"Dice Score 3: {score3}\")  # Expected output: 1.0 (edge case)\n\n    y_true4 = np.array([1, 1, 0, 0])\n    y_pred4 = np.array([0, 0, 1, 1])\n    score4 = dice_score(y_true4, y_pred4)\n    print(f\"Dice Score 4: {score4}\")  # Expected output: 0.0\n\n    y_true5 = np.array([1, 0, 1, 0])\n    y_pred5 = np.array([1, 1, 0, 0])\n    score5 = dice_score(y_true5, y_pred5)\n    print(f\"Dice Score 5: {score5}\")  # Expected output: ~0.571\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds`\n    dictionary to ensure reproducibility.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n\n    Args:\n        row: A dictionary representing a dataset row, where keys are feature names and values are their\n             corresponding values.\n        dim: The dimensionality of the hypervectors.\n        random_seeds: A dictionary where keys are feature names and values are seeds to ensure\n                      reproducibility of hypervectors.\n\n    Returns:\n        A composite hypervector representing the entire row, converted to a list.\n    \"\"\"\n    row_hv = np.zeros(dim, dtype=int)\n\n    for feature_name, feature_value in row.items():\n        # Create hypervector for the feature name\n        np.random.seed(hash(feature_name) % (2**32 - 1))  # Use hash for feature name seed\n        feature_name_hv = np.random.randint(0, 2, dim) * 2 - 1\n\n        # Create hypervector for the feature value using the provided seed\n        if feature_name in random_seeds:\n            np.random.seed(random_seeds[feature_name])\n            feature_value_hv = np.random.randint(0, 2, dim) * 2 - 1\n        else:\n            # Fallback if seed is not provided (though the task implies it will be)\n            np.random.seed(hash(str(feature_value)) % (2**32 - 1))\n            feature_value_hv = np.random.randint(0, 2, dim) * 2 - 1\n\n        # Bind feature name and value hypervectors\n        feature_hv = feature_name_hv * feature_value_hv\n\n        # Bundle feature hypervector into the row hypervector\n        row_hv += feature_hv\n\n    # Binarize the bundled hypervector\n    row_hv = np.where(row_hv > 0, 1, -1)\n\n    return row_hv.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    dataset_row = {\n        \"feature1\": 10,\n        \"feature2\": \"A\",\n        \"feature3\": 3.14\n    }\n    hypervector_dimension = 1000\n    feature_seeds = {\n        \"feature1\": 123,\n        \"feature2\": 456,\n        \"feature3\": 789\n    }\n\n    composite_hv = create_row_hv(dataset_row, hypervector_dimension, feature_seeds)\n    print(f\"Composite Hypervector (first 20 elements): {composite_hv[:20]}\")\n    print(f\"Composite Hypervector length: {len(composite_hv)}\")\n\n    # Demonstrate reproducibility\n    composite_hv_2 = create_row_hv(dataset_row, hypervector_dimension, feature_seeds)\n    print(f\"Composite Hypervector 2 (first 20 elements): {composite_hv_2[:20]}\")\n    print(f\"Are the two hypervectors identical? {composite_hv == composite_hv_2}\")\n\n    # Example with a different value for a feature\n    dataset_row_2 = {\n        \"feature1\": 20,\n        \"feature2\": \"A\",\n        \"feature3\": 3.14\n    }\n    composite_hv_3 = create_row_hv(dataset_row_2, hypervector_dimension, feature_seeds)\n    print(f\"Composite Hypervector 3 (first 20 elements): {composite_hv_3[:20]}\")\n    print(f\"Are the first and third hypervectors identical? {composite_hv == composite_hv_3}\")\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n\n    Args:\n        data: A list of lists, where each inner list represents a pair\n              [y_true, y_pred] for one observation.\n\n    Returns:\n        A 2x2 confusion matrix represented as a list of lists.\n        The matrix is structured as:\n        [[True Negative, False Positive],\n         [False Negative, True Positive]]\n    \"\"\"\n    # Initialize counts\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n\n    # Construct the confusion matrix\n    matrix = [\n        [true_negative, false_positive],\n        [false_negative, true_positive]\n    ]\n\n    return matrix\n\nif __name__ == '__main__':\n    # Example usage:\n    # Data format: [[y_true, y_pred], ...]\n    sample_data = [\n        [1, 1],  # True Positive\n        [0, 0],  # True Negative\n        [1, 0],  # False Negative\n        [0, 1],  # False Positive\n        [1, 1],  # True Positive\n        [0, 0],  # True Negative\n        [1, 0],  # False Negative\n        [0, 1],  # False Positive\n        [1, 1],  # True Positive\n        [0, 0]   # True Negative\n    ]\n\n    conf_matrix = confusion_matrix(sample_data)\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n\n    # Expected output for sample_data:\n    # [[3, 2],\n    #  [2, 3]]\n\n    sample_data_2 = [\n        [1, 1],\n        [1, 1],\n        [0, 0],\n        [0, 0],\n        [1, 0],\n        [0, 1]\n    ]\n\n    conf_matrix_2 = confusion_matrix(sample_data_2)\n    print(\"\\nConfusion Matrix 2:\")\n    print(conf_matrix_2)\n\n    # Expected output for sample_data_2:\n    # [[2, 1],\n    #  [1, 2]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors.\n\n    Args:\n        v1: A numpy array representing the first vector.\n        v2: A numpy array representing the second vector.\n\n    Returns:\n        A float representing the cosine similarity, rounded to three decimal places.\n\n    Raises:\n        ValueError: If the input vectors have different shapes, are empty,\n                    or have zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\n\nif __name__ == '__main__':\n    # Example Usage\n    v_a = np.array([1, 2, 3])\n    v_b = np.array([4, 5, 6])\n    similarity_ab = cosine_similarity(v_a, v_b)\n    print(f\"Cosine similarity between {v_a} and {v_b}: {similarity_ab}\")\n\n    v_c = np.array([1, 0, 0])\n    v_d = np.array([0, 1, 0])\n    similarity_cd = cosine_similarity(v_c, v_d)\n    print(f\"Cosine similarity between {v_c} and {v_d}: {similarity_cd}\")\n\n    v_e = np.array([1, 1])\n    v_f = np.array([2, 2])\n    similarity_ef = cosine_similarity(v_e, v_f)\n    print(f\"Cosine similarity between {v_e} and {v_f}: {similarity_ef}\")\n\n    # Example of error handling\n    try:\n        v_g = np.array([1, 2])\n        v_h = np.array([1, 2, 3])\n        cosine_similarity(v_g, v_h)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    try:\n        v_i = np.array([])\n        v_j = np.array([])\n        cosine_similarity(v_i, v_j)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    try:\n        v_k = np.array([0, 0])\n        v_l = np.array([1, 2])\n        cosine_similarity(v_k, v_l)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates various performance metrics for a binary classification problem.\n\n    Args:\n        actual: The actual class labels (1 for positive, 0 for negative).\n        predicted: The predicted class labels from the model.\n\n    Returns:\n        A tuple containing:\n            - confusion_matrix: A 2x2 matrix.\n            - accuracy: A float representing the accuracy of the model.\n            - f1_score: A float representing the F1 score of the model.\n            - specificity: A float representing the specificity of the model.\n            - negative_predictive_value: A float representing the negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"Actual and predicted lists must have the same length.\")\n\n    for label in actual + predicted:\n        if label not in [0, 1]:\n            raise ValueError(\"Labels must be either 0 or 1.\")\n\n    tp = 0  # True Positives\n    tn = 0  # True Negatives\n    fp = 0  # False Positives\n    fn = 0  # False Negatives\n\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n\n    confusion_matrix = [[tn, fp], [fn, tp]]\n\n    total_samples = len(actual)\n    accuracy = (tp + tn) / total_samples if total_samples > 0 else 0.0\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n\n    return (\n        confusion_matrix,\n        round(accuracy, 3),\n        round(f1_score, 3),\n        round(specificity, 3),\n        round(negative_predictive_value, 3)\n    )\n\nif __name__ == '__main__':\n    actual_labels = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n    predicted_labels = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n\n    confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value = performance_metrics(actual_labels, predicted_labels)\n\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix)\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"F1 Score: {f1_score}\")\n    print(f\"Specificity: {specificity}\")\n    print(f\"Negative Predictive Value: {negative_predictive_value}\")\n\n    # Example with perfect prediction\n    actual_perfect = [1, 0, 1, 0]\n    predicted_perfect = [1, 0, 1, 0]\n    cm_perfect, acc_perfect, f1_perfect, spec_perfect, npv_perfect = performance_metrics(actual_perfect, predicted_perfect)\n    print(\"\\nPerfect Prediction Example:\")\n    print(\"Confusion Matrix:\")\n    print(cm_perfect)\n    print(f\"Accuracy: {acc_perfect}\")\n    print(f\"F1 Score: {f1_perfect}\")\n    print(f\"Specificity: {spec_perfect}\")\n    print(f\"Negative Predictive Value: {npv_perfect}\")\n\n    # Example with all predicted as positive\n    actual_all_pos = [1, 0, 1, 0]\n    predicted_all_pos = [1, 1, 1, 1]\n    cm_all_pos, acc_all_pos, f1_all_pos, spec_all_pos, npv_all_pos = performance_metrics(actual_all_pos, predicted_all_pos)\n    print(\"\\nAll Predicted as Positive Example:\")\n    print(\"Confusion Matrix:\")\n    print(cm_all_pos)\n    print(f\"Accuracy: {acc_all_pos}\")\n    print(f\"F1 Score: {f1_all_pos}\")\n    print(f\"Specificity: {spec_all_pos}\")\n    print(f\"Negative Predictive Value: {npv_all_pos}\")\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n\n    Args:\n        data: A list or NumPy array of numerical values.\n\n    Returns:\n        A dictionary containing mean, median, mode, variance, standard deviation,\n        percentiles (25th, 50th, 75th), and interquartile range (IQR).\n    \"\"\"\n    data = np.asarray(data)\n\n    if data.size == 0:\n        return {\n            'mean': None,\n            'median': None,\n            'mode': None,\n            'variance': None,\n            'standard_deviation': None,\n            '25th_percentile': None,\n            '50th_percentile': None,\n            '75th_percentile': None,\n            'interquartile_range': None\n        }\n\n    mean = np.mean(data)\n    median = np.median(data)\n\n    # Calculate mode\n    counts = Counter(data)\n    if counts:\n        max_count = max(counts.values())\n        mode = [item for item, count in counts.items() if count == max_count]\n        if len(mode) == len(data): # Handle cases where all values are unique\n            mode = None\n        elif len(mode) == 1:\n            mode = mode[0]\n    else:\n        mode = None\n\n\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n\n    q1 = np.percentile(data, 25)\n    q2 = np.percentile(data, 50)\n    q3 = np.percentile(data, 75)\n    iqr = q3 - q1\n\n    stats = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,  # Mode is not rounded as it's a value from the data\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(q1, 4),\n        '50th_percentile': round(q2, 4),\n        '75th_percentile': round(q3, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n\n    return stats\n\nif __name__ == '__main__':\n    # Example usage\n    data1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    stats1 = descriptive_statistics(data1)\n    print(\"Statistics for data1:\", stats1)\n\n    data2 = [10, 20, 30, 30, 40, 50]\n    stats2 = descriptive_statistics(data2)\n    print(\"Statistics for data2:\", stats2)\n\n    data3 = [5, 5, 5, 5, 5]\n    stats3 = descriptive_statistics(data3)\n    print(\"Statistics for data3:\", stats3)\n\n    data4 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10]\n    stats4 = descriptive_statistics(data4)\n    print(\"Statistics for data4:\", stats4)\n\n    data5 = []\n    stats5 = descriptive_statistics(data5)\n    print(\"Statistics for data5:\", stats5)\n\n    data6 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n    stats6 = descriptive_statistics(data6)\n    print(\"Statistics for data6:\", stats6)\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if not (0 <= k <= n):\n        return 0.0  # Probability is 0 if k is outside the valid range\n\n    # Binomial coefficient (n choose k)\n    n_choose_k = math.comb(n, k)\n\n    # Probability of k successes and n-k failures\n    prob_successes = p**k\n    prob_failures = (1 - p)**(n - k)\n\n    # Binomial probability formula: P(X=k) = (n choose k) * p^k * (1-p)^(n-k)\n    probability = n_choose_k * prob_successes * prob_failures\n\n    return round(probability, 5)\n\nif __name__ == '__main__':\n    # Example usage:\n    n_trials = 10\n    k_successes = 3\n    p_success = 0.5\n\n    prob = binomial_probability(n_trials, k_successes, p_success)\n    print(f\"The probability of exactly {k_successes} successes in {n_trials} trials with probability {p_success} is: {prob}\")\n\n    n_trials = 5\n    k_successes = 2\n    p_success = 0.8\n\n    prob = binomial_probability(n_trials, k_successes, p_success)\n    print(f\"The probability of exactly {k_successes} successes in {n_trials} trials with probability {p_success} is: {prob}\")\n\n    n_trials = 7\n    k_successes = 8\n    p_success = 0.3\n\n    prob = binomial_probability(n_trials, k_successes, p_success)\n    print(f\"The probability of exactly {k_successes} successes in {n_trials} trials with probability {p_success} is: {prob}\")\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be positive.\")\n\n    # Calculate the exponent part of the formula\n    exponent = -((x - mean)**2) / (2 * std_dev**2)\n\n    # Calculate the denominator part of the formula\n    denominator = std_dev * math.sqrt(2 * math.pi)\n\n    # Calculate the PDF\n    pdf_value = (1 / denominator) * math.exp(exponent)\n\n    # Return the PDF value rounded to 5 decimal places\n    return round(pdf_value, 5)\n\nif __name__ == '__main__':\n    # Example usage:\n    x_value = 0\n    mean_value = 0\n    std_dev_value = 1\n\n    pdf_at_x = normal_pdf(x_value, mean_value, std_dev_value)\n    print(f\"The PDF of the normal distribution at x={x_value} with mean={mean_value} and std_dev={std_dev_value} is: {pdf_at_x}\")\n\n    x_value = 1.5\n    mean_value = 2\n    std_dev_value = 0.5\n\n    pdf_at_x = normal_pdf(x_value, mean_value, std_dev_value)\n    print(f\"The PDF of the normal distribution at x={x_value} with mean={mean_value} and std_dev={std_dev_value} is: {pdf_at_x}\")\n\n    try:\n        normal_pdf(1, 0, 0)\n    except ValueError as e:\n        print(f\"Caught expected error: {e}\")\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    # The Poisson probability formula is P(X=k) = (e^(-lam) * lam^k) / k!\n    if k < 0 or not isinstance(k, int):\n        raise ValueError(\"k must be a non-negative integer.\")\n    if lam < 0:\n        raise ValueError(\"lam must be non-negative.\")\n\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\nif __name__ == '__main__':\n    # Example usage:\n    # Calculate the probability of exactly 2 events occurring when the average rate is 3\n    k_example = 2\n    lam_example = 3\n    prob = poisson_probability(k_example, lam_example)\n    print(f\"The probability of observing exactly {k_example} events with a mean rate of {lam_example} is: {prob}\")\n\n    # Another example: Probability of 0 events with a mean rate of 1.5\n    k_example_2 = 0\n    lam_example_2 = 1.5\n    prob_2 = poisson_probability(k_example_2, lam_example_2)\n    print(f\"The probability of observing exactly {k_example_2} events with a mean rate of {lam_example_2} is: {prob_2}\")\n\n    # Example with a larger k and lam\n    k_example_3 = 5\n    lam_example_3 = 4.2\n    prob_3 = poisson_probability(k_example_3, lam_example_3)\n    print(f\"The probability of observing exactly {k_example_3} events with a mean rate of {lam_example_3} is: {prob_3}\")\n\n    # Example with invalid input (negative k)\n    try:\n        poisson_probability(-1, 2)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    # Example with invalid input (negative lam)\n    try:\n        poisson_probability(2, -2)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    if img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D grayscale image.\")\n    if img.dtype != np.uint8:\n        # Optionally convert to uint8 if the input is a different type but within the 0-255 range\n        # For simplicity, we'll assume the input is already uint8 or compatible.\n        # A more robust implementation might handle different dtypes and ranges.\n        pass # Or raise a warning/error if strict uint8 is required\n\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n\n    contrast = float(max_pixel - min_pixel)\n\n    return contrast\n\nif __name__ == '__main__':\n    # Example Usage:\n\n    # Create a sample grayscale image (numpy array)\n    # Image with high contrast\n    high_contrast_img = np.array([\n        [0, 0, 0, 255, 255],\n        [0, 0, 0, 255, 255],\n        [0, 0, 0, 255, 255],\n        [0, 0, 0, 255, 255],\n        [0, 0, 0, 255, 255]\n    ], dtype=np.uint8)\n\n    # Image with low contrast\n    low_contrast_img = np.array([\n        [100, 110, 105, 115, 108],\n        [102, 108, 112, 106, 110],\n        [105, 111, 107, 109, 113],\n        [108, 104, 110, 112, 106],\n        [110, 106, 108, 104, 111]\n    ], dtype=np.uint8)\n\n    # Image with uniform intensity (zero contrast)\n    uniform_img = np.array([\n        [150, 150, 150],\n        [150, 150, 150],\n        [150, 150, 150]\n    ], dtype=np.uint8)\n\n    # Calculate and print the contrast for each image\n    contrast_high = calculate_contrast(high_contrast_img)\n    contrast_low = calculate_contrast(low_contrast_img)\n    contrast_uniform = calculate_contrast(uniform_img)\n\n    print(f\"Contrast of high contrast image: {contrast_high}\")\n    print(f\"Contrast of low contrast image: {contrast_low}\")\n    print(f\"Contrast of uniform image: {contrast_uniform}\")\n\n    # Example of handling invalid input (optional)\n    try:\n        invalid_input = [[1, 2], [3, 4]] # Not a numpy array\n        calculate_contrast(invalid_input)\n    except TypeError as e:\n        print(f\"\\nCaught expected error for invalid input type: {e}\")\n\n    try:\n        color_image = np.array([[[10, 20, 30], [40, 50, 60]]], dtype=np.uint8) # 3D array\n        calculate_contrast(color_image)\n    except ValueError as e:\n        print(f\"Caught expected error for non-2D image: {e}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    Raises:\n        ValueError: If the input arrays are not 1D or do not have the same length.\n    \"\"\"\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise TypeError(\"Inputs must be NumPy arrays.\")\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Input arrays must be 1D.\")\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Input arrays must have the same length.\")\n\n    return np.dot(vec1, vec2)\n\nif __name__ == '__main__':\n    # Example usage:\n    vector_a = np.array([1, 2, 3])\n    vector_b = np.array([4, 5, 6])\n\n    dot_product_result = calculate_dot_product(vector_a, vector_b)\n    print(f\"Vector A: {vector_a}\")\n    print(f\"Vector B: {vector_b}\")\n    print(f\"Dot product: {dot_product_result}\")\n\n    # Example with different vectors\n    vector_c = np.array([-1, 0, 2])\n    vector_d = np.array([3, -2, 1])\n    dot_product_result_2 = calculate_dot_product(vector_c, vector_d)\n    print(f\"\\nVector C: {vector_c}\")\n    print(f\"Vector D: {vector_d}\")\n    print(f\"Dot product: {dot_product_result_2}\")\n\n    # Example of error handling (uncomment to test)\n    # try:\n    #     invalid_vector_1 = np.array([[1, 2], [3, 4]])\n    #     invalid_vector_2 = np.array([5, 6])\n    #     calculate_dot_product(invalid_vector_1, invalid_vector_2)\n    # except ValueError as e:\n    #     print(f\"\\nError: {e}\")\n\n    # try:\n    #     invalid_vector_3 = np.array([1, 2, 3])\n    #     invalid_vector_4 = np.array([4, 5])\n    #     calculate_dot_product(invalid_vector_3, invalid_vector_4)\n    # except ValueError as e:\n    #     print(f\"\\nError: {e}\")\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    transformed_data = []\n    for x in data:\n        transformed_point = [round(x**i, 8) for i in range(degree + 1)]\n        transformed_data.append(transformed_point)\n\n    return transformed_data\n\nif __name__ == '__main__':\n    # Example Usage\n    data = [1.0, 2.0, 3.0]\n    degree = 2\n    transformed_features = phi_transform(data, degree)\n    print(f\"Original data: {data}\")\n    print(f\"Degree: {degree}\")\n    print(f\"Transformed features:\\n{transformed_features}\")\n\n    data2 = [0.5, -1.0]\n    degree2 = 3\n    transformed_features2 = phi_transform(data2, degree2)\n    print(f\"\\nOriginal data: {data2}\")\n    print(f\"Degree: {degree2}\")\n    print(f\"Transformed features:\\n{transformed_features2}\")\n\n    data3 = [10.0]\n    degree3 = 0\n    transformed_features3 = phi_transform(data3, degree3)\n    print(f\"\\nOriginal data: {data3}\")\n    print(f\"Degree: {degree3}\")\n    print(f\"Transformed features:\\n{transformed_features3}\")\n\n    data4 = [1.0, 2.0]\n    degree4 = -1\n    transformed_features4 = phi_transform(data4, degree4)\n    print(f\"\\nOriginal data: {data4}\")\n    print(f\"Degree: {degree4}\")\n    print(f\"Transformed features:\\n{transformed_features4}\")\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates positional encodings for a given position and model dimensionality.\n\n    Args:\n        position: The position in the sequence (integer).\n        d_model: The dimensionality of the model (integer).\n\n    Returns:\n        A list of positional encodings (np.float16) for the given position,\n        or -1 if position is 0 or d_model is less than or equal to 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n\n    pe = np.zeros((d_model,), dtype=np.float32)\n    for i in range(0, d_model, 2):\n        denominator = np.power(10000, (2 * i) / d_model)\n        pe[i] = np.sin(position / denominator)\n        if i + 1 < d_model:\n            pe[i + 1] = np.cos(position / denominator)\n\n    return pe.astype(np.float16).tolist()\n\nif __name__ == '__main__':\n    # Example usage:\n    position = 10\n    d_model = 512\n    encoding = pos_encoding(position, d_model)\n    print(f\"Positional encoding for position {position} and d_model {d_model}:\")\n    print(encoding)\n\n    position = 0\n    d_model = 512\n    encoding = pos_encoding(position, d_model)\n    print(f\"\\nPositional encoding for position {position} and d_model {d_model}:\")\n    print(encoding)\n\n    position = 10\n    d_model = 0\n    encoding = pos_encoding(position, d_model)\n    print(f\"\\nPositional encoding for position {position} and d_model {d_model}:\")\n    print(encoding)\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for Overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    # Check for Underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1 # Underfitting\n    # Otherwise, it's a Good fit\n    else:\n        return 0\n\nif __name__ == '__main__':\n    # Example Usage:\n    print(f\"Training: 0.95, Test: 0.70 -> {model_fit_quality(0.95, 0.70)}\") # Overfitting\n    print(f\"Training: 0.60, Test: 0.55 -> {model_fit_quality(0.60, 0.55)}\") # Underfitting\n    print(f\"Training: 0.85, Test: 0.80 -> {model_fit_quality(0.85, 0.80)}\") # Good fit\n    print(f\"Training: 0.75, Test: 0.60 -> {model_fit_quality(0.75, 0.60)}\") # Good fit (difference is 0.15)\n    print(f\"Training: 0.90, Test: 0.65 -> {model_fit_quality(0.90, 0.65)}\") # Overfitting (difference is 0.25)\n    print(f\"Training: 0.65, Test: 0.75 -> {model_fit_quality(0.65, 0.75)}\") # Good fit\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Ensure inputs are numpy arrays for consistent handling\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n\n    # Update biased second moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1**t)\n\n    # Compute bias-corrected second moment estimate\n    v_hat = v / (1 - beta2**t)\n\n    # Update parameters\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round to 5 decimal places and convert to list\n    updated_parameter = np.round(updated_parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n\n    return updated_parameter.tolist(), updated_m.tolist(), updated_v.tolist()\n\nif __name__ == '__main__':\n    # Example usage with scalar values\n    param_scalar = 10.0\n    grad_scalar = 0.5\n    m_scalar = 0.0\n    v_scalar = 0.0\n    t_scalar = 1\n\n    updated_param_scalar, updated_m_scalar, updated_v_scalar = adam_optimizer(\n        param_scalar, grad_scalar, m_scalar, v_scalar, t_scalar\n    )\n    print(f\"Scalar Example:\")\n    print(f\"Updated Parameter: {updated_param_scalar}\")\n    print(f\"Updated m: {updated_m_scalar}\")\n    print(f\"Updated v: {updated_v_scalar}\")\n    print(\"-\" * 20)\n\n    # Example usage with array values\n    param_array = np.array([10.0, 5.0])\n    grad_array = np.array([0.5, -0.2])\n    m_array = np.array([0.0, 0.0])\n    v_array = np.array([0.0, 0.0])\n    t_array = 1\n\n    updated_param_array, updated_m_array, updated_v_array = adam_optimizer(\n        param_array, grad_array, m_array, v_array, t_array\n    )\n    print(f\"Array Example:\")\n    print(f\"Updated Parameter: {updated_param_array}\")\n    print(f\"Updated m: {updated_m_array}\")\n    print(f\"Updated v: {updated_v_array}\")\n    print(\"-\" * 20)\n\n    # Example with multiple timesteps\n    param_multi = np.array([10.0, 5.0])\n    grad_multi_1 = np.array([0.5, -0.2])\n    grad_multi_2 = np.array([-0.1, 0.3])\n    m_multi = np.array([0.0, 0.0])\n    v_multi = np.array([0.0, 0.0])\n\n    # Timestep 1\n    t_multi_1 = 1\n    updated_param_multi_1, m_multi, v_multi = adam_optimizer(\n        param_multi, grad_multi_1, m_multi, v_multi, t_multi_1\n    )\n    print(f\"Multi-Timestep Example (Timestep {t_multi_1}):\")\n    print(f\"Updated Parameter: {updated_param_multi_1}\")\n    print(f\"Updated m: {m_multi}\")\n    print(f\"Updated v: {v_multi}\")\n    print(\"-\" * 20)\n\n    # Timestep 2\n    t_multi_2 = 2\n    updated_param_multi_2, m_multi, v_multi = adam_optimizer(\n        updated_param_multi_1, grad_multi_2, m_multi, v_multi, t_multi_2\n    )\n    print(f\"Multi-Timestep Example (Timestep {t_multi_2}):\")\n    print(f\"Updated Parameter: {updated_param_multi_2}\")\n    print(f\"Updated m: {m_multi}\")\n    print(f\"Updated v: {v_multi}\")\n    print(\"-\" * 20)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12,\n        \"n_embd\": 10,  # Added embedding dimension for clarity\n        \"n_layer\": 2 # Added number of layers for dummy blocks\n    }\n\n    # Dummy parameters for a simplified model\n    params = {\n        \"wte\": np.random.rand(len(DummyBPE().encoder_dict), hparams[\"n_embd\"]),\n        \"wpe\": np.random.rand(hparams[\"n_ctx\"], hparams[\"n_embd\"]),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(hparams[\"n_embd\"]),\n            \"b\": np.zeros(hparams[\"n_embd\"]),\n        },\n        \"ln_f_g\": np.ones(hparams[\"n_embd\"]), # Added for layer norm\n        \"ln_f_b\": np.zeros(hparams[\"n_embd\"]), # Added for layer norm\n        \"wte_ln\": { # Added for dummy layer norm before blocks\n            \"g\": np.ones(hparams[\"n_embd\"]),\n            \"b\": np.zeros(hparams[\"n_embd\"]),\n        }\n    }\n\n    # Create dummy blocks\n    for _ in range(hparams[\"n_layer\"]):\n        block_params = {\n            \"ln_1\": {\"g\": np.ones(hparams[\"n_embd\"]), \"b\": np.zeros(hparams[\"n_embd\"])},\n            \"attn\": {\n                \"c_attn\": {\"w\": np.random.rand(hparams[\"n_embd\"], 3 * hparams[\"n_embd\"]), \"b\": np.random.rand(3 * hparams[\"n_embd\"])},\n                \"c_proj\": {\"w\": np.random.rand(hparams[\"n_embd\"], hparams[\"n_embd\"]), \"b\": np.random.rand(hparams[\"n_embd\"])}\n            },\n            \"ln_2\": {\"g\": np.ones(hparams[\"n_embd\"]), \"b\": np.zeros(hparams[\"n_embd\"])},\n            \"mlp\": {\n                \"c_fc\": {\"w\": np.random.rand(hparams[\"n_embd\"], 4 * hparams[\"n_embd\"]), \"b\": np.random.rand(4 * hparams[\"n_embd\"])},\n                \"c_proj\": {\"w\": np.random.rand(4 * hparams[\"n_embd\"], hparams[\"n_embd\"]), \"b\": np.random.rand(hparams[\"n_embd\"])}\n            }\n        }\n        params[\"blocks\"].append(block_params)\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax(x, axis=-1):\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\ndef layer_norm(x, g, b, epsilon=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n    return g * x_normalized + b\n\ndef attention(q, k, v, mask):\n    scores = np.matmul(q, k.transpose(0, 2, 1))\n    scores = scores / np.sqrt(q.shape[-1])\n    scores = scores + mask\n    weights = softmax(scores, axis=-1)\n    output = np.matmul(weights, v)\n    return output\n\ndef multi_head_attention(x, c_attn_w, c_attn_b, c_proj_w, c_proj_b, n_head, n_embd, mask):\n    qkv = np.matmul(x, c_attn_w) + c_attn_b\n    qkv = qkv.reshape(x.shape[0], x.shape[1], n_head, 3 * (n_embd // n_head))\n    qkv = qkv.transpose(0, 2, 1, 3)\n    q, k, v = np.split(qkv, 3, axis=-1)\n\n    output = attention(q, k, v, mask)\n    output = output.transpose(0, 2, 1, 3).reshape(x.shape[0], x.shape[1], n_embd)\n    output = np.matmul(output, c_proj_w) + c_proj_b\n    return output\n\ndef mlp(x, c_fc_w, c_fc_b, c_proj_w, c_proj_b):\n    x = np.matmul(x, c_fc_w) + c_fc_b\n    x = gelu(x)\n    x = np.matmul(x, c_proj_w) + c_proj_b\n    return x\n\ndef transformer_block(x, params, hparams, mask):\n    ln_1_g, ln_1_b = params[\"ln_1\"][\"g\"], params[\"ln_1\"][\"b\"]\n    attn_c_attn_w, attn_c_attn_b = params[\"attn\"][\"c_attn\"][\"w\"], params[\"attn\"][\"c_attn\"][\"b\"]\n    attn_c_proj_w, attn_c_proj_b = params[\"attn\"][\"c_proj\"][\"w\"], params[\"attn\"][\"c_proj\"][\"b\"]\n    ln_2_g, ln_2_b = params[\"ln_2\"][\"g\"], params[\"ln_2\"][\"b\"]\n    mlp_c_fc_w, mlp_c_fc_b = params[\"mlp\"][\"c_fc\"][\"w\"], params[\"mlp\"][\"c_fc\"][\"b\"]\n    mlp_c_proj_w, mlp_c_proj_b = params[\"mlp\"][\"c_proj\"][\"w\"], params[\"mlp\"][\"c_proj\"][\"b\"]\n\n    residual = x\n    x = layer_norm(x, ln_1_g, ln_1_b)\n    x = multi_head_attention(x, attn_c_attn_w, attn_c_attn_b, attn_c_proj_w, attn_c_proj_b, hparams[\"n_head\"], hparams[\"n_embd\"], mask)\n    x = x + residual\n\n    residual = x\n    x = layer_norm(x, ln_2_g, ln_2_b)\n    x = mlp(x, mlp_c_fc_w, mlp_c_fc_b, mlp_c_proj_w, mlp_c_proj_b)\n    x = x + residual\n    return x\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt\n    input_ids = encoder.encode(prompt)\n    generated_ids = input_ids[:]\n\n    # Create attention mask\n    seq_len = hparams[\"n_ctx\"]\n    mask = np.full((1, 1, seq_len, seq_len), -np.inf)\n    mask = np.triu(mask, k=1)\n\n    for _ in range(n_tokens_to_generate):\n        current_sequence = generated_ids[-hparams[\"n_ctx\"]:]\n        current_seq_len = len(current_"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function first\n        exp_values = np.exp(values - np.max(values))  # Subtract max for numerical stability\n        return exp_values / np.sum(exp_values)\n\n    # For simplicity, we'll use the crystal values directly as queries, keys, and values.\n    # In a real self-attention, these would be derived from the input using learned weights.\n    queries = np.array(crystal_values).reshape(-1, 1)\n    keys = np.array(crystal_values).reshape(-1, 1)\n    values = np.array(crystal_values).reshape(-1, 1)\n\n    # Calculate attention scores (dot product of queries and keys)\n    # We need to broadcast the dot product across all pairs of crystals\n    attention_scores = np.dot(queries, keys.T)\n\n    # Scale the attention scores (optional but common)\n    # Here, we scale by the square root of the dimension (which is 1 in this simplified case)\n    scaled_attention_scores = attention_scores / np.sqrt(dimension)\n\n    # Apply softmax to get attention weights\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=scaled_attention_scores)\n\n    # Compute the weighted sum of values using attention weights\n    weighted_patterns = np.dot(attention_weights, values)\n\n    # Flatten the result and round to 4 decimal places\n    flattened_patterns = weighted_patterns.flatten()\n    rounded_patterns = [round(float(pattern), 4) for pattern in flattened_patterns]\n\n    return rounded_patterns\n\nif __name__ == '__main__':\n    # Example Usage:\n    n_crystals = 3\n    crystal_values = [1.0, 2.0, 3.0]\n    dimension = 1  # Dimension of the crystal values\n\n    enhanced_patterns = pattern_weaver(n_crystals, crystal_values, dimension)\n    print(f\"Crystal Values: {crystal_values}\")\n    print(f\"Enhanced Patterns: {enhanced_patterns}\")\n\n    n_crystals_2 = 4\n    crystal_values_2 = [0.5, 1.5, 2.5, 3.5]\n    dimension_2 = 1\n\n    enhanced_patterns_2 = pattern_weaver(n_crystals_2, crystal_values_2, dimension_2)\n    print(f\"Crystal Values: {crystal_values_2}\")\n    print(f\"Enhanced Patterns: {enhanced_patterns_2}\")\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for each document in a corpus given a query.\n\n    Args:\n        corpus: A list of strings, where each string is a document.\n        query: A string representing the query.\n        k1: A parameter controlling term frequency saturation (default is 1.5).\n        b: A parameter controlling document length normalization (default is 0.75).\n\n    Returns:\n        A list of floats, where each float is the BM25 score of the corresponding\n        document in the corpus, rounded to three decimal places.\n    \"\"\"\n    N = len(corpus)\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    tokenized_corpus = [doc.split() for doc in corpus]\n    tokenized_query = query.split()\n\n    # Calculate inverse document frequency (IDF) for each term in the query\n    idf = {}\n    for term in tokenized_query:\n        doc_count = sum(1 for doc in tokenized_corpus if term in doc)\n        idf[term] = np.log((N - doc_count + 0.5) / (doc_count + 0.5) + 1)\n\n    scores = []\n    for i, doc in enumerate(tokenized_corpus):\n        doc_score = 0\n        doc_len = len(doc)\n        term_counts = Counter(doc)\n\n        for term in tokenized_query:\n            if term in term_counts:\n                tf = term_counts[term]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))\n                doc_score += idf.get(term, 0) * (numerator / denominator)\n\n        scores.append(round(doc_score, 3))\n\n    return scores\n\nif __name__ == '__main__':\n    corpus = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"a quick brown dog jumps over the lazy fox\",\n        \"the lazy dog is brown\",\n        \"the quick fox is lazy\"\n    ]\n    query = \"quick brown fox\"\n\n    bm25_scores = calculate_bm25_scores(corpus, query)\n    print(f\"BM25 scores for the query '{query}':\")\n    for i, score in enumerate(bm25_scores):\n        print(f\"Document {i+1}: {score}\")\n\n    corpus2 = [\n        \"information retrieval is a field of study\",\n        \"bm25 is a ranking function used in information retrieval\",\n        \"ranking functions are important for search engines\",\n        \"search engines use information retrieval techniques\"\n    ]\n    query2 = \"information retrieval ranking\"\n\n    bm25_scores2 = calculate_bm25_scores(corpus2, query2)\n    print(f\"\\nBM25 scores for the query '{query2}':\")\n    for i, score in enumerate(bm25_scores2):\n        print(f\"Document {i+1}: {score}\")\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for true_label, pred_label in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            true_positives += 1\n        elif true_label == 0 and pred_label == 1:\n            false_positives += 1\n        elif true_label == 1 and pred_label == 0:\n            false_negatives += 1\n\n    # Calculate Precision\n    # Avoid division by zero if no positive predictions were made\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n\n    # Calculate Recall\n    # Avoid division by zero if no true positives exist\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n\n    # Calculate F1 Score\n    # Avoid division by zero if both precision and recall are zero\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n\n    return round(f1_score, 3)\n\nif __name__ == '__main__':\n    # Example Usage\n    y_true_1 = [1, 0, 1, 1, 0, 1]\n    y_pred_1 = [1, 1, 1, 0, 0, 1]\n    f1_1 = calculate_f1_score(y_true_1, y_pred_1)\n    print(f\"Example 1 F1 Score: {f1_1}\") # Expected output: ~0.714\n\n    y_true_2 = [0, 0, 0, 0, 0]\n    y_pred_2 = [0, 0, 0, 0, 0]\n    f1_2 = calculate_f1_score(y_true_2, y_pred_2)\n    print(f\"Example 2 F1 Score: {f1_2}\") # Expected output: 0.0\n\n    y_true_3 = [1, 1, 1, 1, 1]\n    y_pred_3 = [1, 1, 1, 1, 1]\n    f1_3 = calculate_f1_score(y_true_3, y_pred_3)\n    print(f\"Example 3 F1 Score: {f1_3}\") # Expected output: 1.0\n\n    y_true_4 = [1, 0, 1, 0]\n    y_pred_4 = [0, 1, 0, 1]\n    f1_4 = calculate_f1_score(y_true_4, y_pred_4)\n    print(f\"Example 4 F1 Score: {f1_4}\") # Expected output: 0.0\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\nimport numpy as np\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = []\n    for i, consumption in enumerate(consumption_data):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended_data.append(consumption - fluctuation)\n\n    # 2) Perform linear regression on the detrended data.\n    days = np.arange(1, len(consumption_data) + 1).reshape(-1, 1)\n    detrended_data_np = np.array(detrended_data)\n\n    # Using numpy's polyfit for linear regression (degree 1)\n    slope, intercept = np.polyfit(days.flatten(), detrended_data_np, 1)\n\n    # 3) Predict day 15's base consumption.\n    day_15 = 15\n    predicted_base_consumption_day_15 = slope * day_15 + intercept\n\n    # 4) Add the day 15 fluctuation back.\n    fluctuation_day_15 = 10 * math.sin(2 * PI * day_15 / 10)\n    predicted_total_consumption_day_15 = predicted_base_consumption_day_15 + fluctuation_day_15\n\n    # 5) Round, then add a 5% safety margin (rounded up).\n    rounded_prediction = round(predicted_total_consumption_day_15)\n    safety_margin = math.ceil(rounded_prediction * 0.05)\n    final_forecast = rounded_prediction + safety_margin\n\n    # 6) Return the final integer.\n    return int(final_forecast)\n\nif __name__ == '__main__':\n    # Example consumption data for 10 days\n    # This data is just an example and should reflect a linear trend + fluctuation\n    example_consumption_data = [\n        105, 112, 118, 125, 130, 135, 140, 145, 150, 155\n    ]\n\n    # Let's create a more realistic example data based on the description\n    # Base linear trend: starts at 100 and increases by 5 each day\n    # Fluctuation: 10 * sin(2\u03c0 * i / 10)\n    realistic_consumption_data = []\n    base_start = 100\n    linear_increase = 5\n    for i in range(1, 11):\n        base_consumption = base_start + linear_increase * (i - 1)\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        realistic_consumption_data.append(base_consumption + fluctuation)\n\n    print(f\"Example Consumption Data: {example_consumption_data}\")\n    forecast_example = power_grid_forecast(example_consumption_data)\n    print(f\"Forecast for Day 15 (Example Data): {forecast_example}\")\n\n    print(f\"\\nRealistic Consumption Data: {realistic_consumption_data}\")\n    forecast_realistic = power_grid_forecast(realistic_consumption_data)\n    print(f\"Forecast for Day 15 (Realistic Data): {forecast_realistic}\")\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError(\"Inputs must be numpy arrays.\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n\n    return round(mean_absolute_error, 3)\n\nif __name__ == '__main__':\n    # Example Usage\n    y_true_example = np.array([3, -0.5, 2, 7])\n    y_pred_example = np.array([2.5, 0.0, 2, 8])\n\n    calculated_mae = mae(y_true_example, y_pred_example)\n    print(f\"True values: {y_true_example}\")\n    print(f\"Predicted values: {y_pred_example}\")\n    print(f\"Mean Absolute Error: {calculated_mae}\")\n\n    # Another example\n    y_true_example_2 = np.array([10, 20, 30, 40, 50])\n    y_pred_example_2 = np.array([12, 18, 35, 38, 55])\n\n    calculated_mae_2 = mae(y_true_example_2, y_pred_example_2)\n    print(f\"\\nTrue values: {y_true_example_2}\")\n    print(f\"Predicted values: {y_pred_example_2}\")\n    print(f\"Mean Absolute Error: {calculated_mae_2}\")\n\n    # Example with different shapes (will raise ValueError)\n    try:\n        y_true_invalid = np.array([1, 2])\n        y_pred_invalid = np.array([1, 2, 3])\n        mae(y_true_invalid, y_pred_invalid)\n    except ValueError as e:\n        print(f\"\\nCaught expected error: {e}\")\n\n    # Example with non-numpy input (will raise TypeError)\n    try:\n        y_true_invalid_type = [1, 2]\n        y_pred_invalid_type = [1, 2]\n        mae(y_true_invalid_type, y_pred_invalid_type)\n    except TypeError as e:\n        print(f\"Caught expected error: {e}\")\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices.\n\n    Args:\n        X: Input matrix (batch_size, seq_len, d_model).\n        W_q: Weight matrix for Query (d_model, d_k).\n        W_k: Weight matrix for Key (d_model, d_k).\n        W_v: Weight matrix for Value (d_model, d_v).\n\n    Returns:\n        A tuple containing Q, K, and V matrices (batch_size, seq_len, d_k/d_v).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention output.\n\n    Args:\n        Q: Query matrix (batch_size, seq_len, d_k).\n        K: Key matrix (batch_size, seq_len, d_k).\n        V: Value matrix (batch_size, seq_len, d_v).\n\n    Returns:\n        The self-attention output matrix (batch_size, seq_len, d_v).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n\n    Args:\n        X: Input matrix (batch_size, seq_len, d_model).\n        W_q: Weight matrix for Query (d_model, d_model).\n        W_k: Weight matrix for Key (d_model, d_model).\n        W_v: Weight matrix for Value (d_model, d_model).\n        n_heads: Number of attention heads.\n\n    Returns:\n        A list representing the concatenated output of all attention heads\n        (batch_size, seq_len, d_model).\n    \"\"\"\n    batch_size, seq_len, d_model = X.shape\n    d_k = d_model // n_heads\n    d_v = d_model // n_heads\n\n    # Reshape weight matrices for multi-head\n    W_q_heads = W_q.reshape(d_model, n_heads, d_k)\n    W_k_heads = W_k.reshape(d_model, n_heads, d_k)\n    W_v_heads = W_v.reshape(d_model, n_heads, d_v)\n\n    outputs = []\n    for i in range(n_heads):\n        # Compute Q, K, V for each head\n        Q_head = np.dot(X, W_q_heads[:, i, :])\n        K_head = np.dot(X, W_k_heads[:, i, :])\n        V_head = np.dot(X, W_v_heads[:, i, :])\n\n        # Compute self-attention for each head\n        output_head = self_attention(Q_head, K_head, V_head)\n        outputs.append(output_head)\n\n    # Concatenate the outputs of all heads\n    concatenated_output = np.concatenate(outputs, axis=-1)\n\n    # Round the results to the nearest 4th decimal\n    rounded_output = np.round(concatenated_output, 4)\n\n    return rounded_output.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    batch_size = 2\n    seq_len = 3\n    d_model = 6\n    n_heads = 2\n\n    # Example input and weight matrices\n    X = np.array([\n        [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n         [7.0, 8.0, 9.0, 10.0, 11.0, 12.0],\n         [13.0, 14.0, 15.0, 16.0, 17.0, 18.0]],\n\n        [[19.0, 20.0, 21.0, 22.0, 23.0, 24.0],\n         [25.0, 26.0, 27.0, 28.0, 29.0, 30.0],\n         [31.0, 32.0, 33.0, 34.0, 35.0, 36.0]]\n    ])\n\n    W_q = np.random.rand(d_model, d_model)\n    W_k = np.random.rand(d_model, d_model)\n    W_v = np.random.rand(d_model, d_model)\n\n    multi_head_output = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(\"Multi-Head Attention Output:\")\n    print(multi_head_output)\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport math\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must have the same length.\")\n\n    n11 = sum(1 for i in range(len(x)) if x[i] == 1 and y[i] == 1)\n    n10 = sum(1 for i in range(len(x)) if x[i] == 1 and y[i] == 0)\n    n01 = sum(1 for i in range(len(x)) if x[i] == 0 and y[i] == 1)\n    n00 = sum(1 for i in range(len(x)) if x[i] == 0 and y[i] == 0)\n\n    numerator = n11 * n00 - n10 * n01\n    denominator_squared = (n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00)\n\n    if denominator_squared == 0:\n        return 0.0  # Avoid division by zero, indicates no variation in one or both variables\n\n    denominator = math.sqrt(denominator_squared)\n\n    phi = numerator / denominator\n\n    return round(phi, 4)\n\nif __name__ == '__main__':\n    # Example usage:\n    x1 = [1, 1, 0, 0, 1]\n    y1 = [1, 0, 1, 0, 1]\n    phi1 = phi_corr(x1, y1)\n    print(f\"Phi coefficient for x1 and y1: {phi1}\")\n\n    x2 = [1, 1, 1, 1, 1]\n    y2 = [0, 0, 0, 0, 0]\n    phi2 = phi_corr(x2, y2)\n    print(f\"Phi coefficient for x2 and y2: {phi2}\")\n\n    x3 = [1, 0, 1, 0]\n    y3 = [1, 0, 1, 0]\n    phi3 = phi_corr(x3, y3)\n    print(f\"Phi coefficient for x3 and y3: {phi3}\")\n\n    x4 = [1, 0, 1, 0]\n    y4 = [0, 1, 0, 1]\n    phi4 = phi_corr(x4, y4)\n    print(f\"Phi coefficient for x4 and y4: {phi4}\")\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\nif __name__ == '__main__':\n    # Example usage\n    test_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\n    print(\"Testing Hard Sigmoid function:\")\n    for val in test_values:\n        result = hard_sigmoid(val)\n        print(f\"hard_sigmoid({val}) = {result}\")\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n\nif __name__ == '__main__':\n    # Example usage\n    input_value_positive = 2.5\n    input_value_negative = -1.0\n    alpha_value = 0.5\n\n    elu_positive = elu(input_value_positive)\n    elu_negative_default_alpha = elu(input_value_negative)\n    elu_negative_custom_alpha = elu(input_value_negative, alpha=alpha_value)\n\n    print(f\"ELU({input_value_positive}) = {elu_positive}\")\n    print(f\"ELU({input_value_negative}) with default alpha = {elu_negative_default_alpha}\")\n    print(f\"ELU({input_value_negative}) with alpha={alpha_value} = {elu_negative_custom_alpha}\")\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\nif __name__ == '__main__':\n    # Example usage\n    input_value_positive = 5.0\n    input_value_negative = -3.0\n    input_value_zero = 0.0\n    custom_alpha = 0.1\n\n    prelu_positive = prelu(input_value_positive)\n    prelu_negative = prelu(input_value_negative)\n    prelu_zero = prelu(input_value_zero)\n    prelu_negative_custom = prelu(input_value_negative, alpha=custom_alpha)\n\n    print(f\"PReLU({input_value_positive}) = {prelu_positive}\")\n    print(f\"PReLU({input_value_negative}) = {prelu_negative}\")\n    print(f\"PReLU({input_value_zero}) = {prelu_zero}\")\n    print(f\"PReLU({input_value_negative}, alpha={custom_alpha}) = {prelu_negative_custom}\")\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handle large positive x to prevent overflow of exp(x)\n    # log(1 + e^x) approaches x for large x\n    if x > 20:  # A threshold to avoid large exp(x)\n        return round(x, 4)\n    # Handle large negative x to prevent underflow of exp(x)\n    # log(1 + e^x) approaches log(1) = 0 for large negative x\n    elif x < -20: # A threshold to avoid small exp(x)\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\nif __name__ == '__main__':\n    # Example usage\n    print(f\"softplus(0) = {softplus(0)}\")\n    print(f\"softplus(1) = {softplus(1)}\")\n    print(f\"softplus(-1) = {softplus(-1)}\")\n    print(f\"softplus(10) = {softplus(10)}\")\n    print(f\"softplus(-10) = {softplus(-10)}\")\n    print(f\"softplus(30) = {softplus(30)}\")\n    print(f\"softplus(-30) = {softplus(-30)}\")\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\nif __name__ == '__main__':\n    # Example usage\n    input_value1 = 5.0\n    output_value1 = softsign(input_value1)\n    print(f\"Softsign({input_value1}) = {output_value1}\")\n\n    input_value2 = -3.0\n    output_value2 = softsign(input_value2)\n    print(f\"Softsign({input_value2}) = {output_value2}\")\n\n    input_value3 = 0.0\n    output_value3 = softsign(input_value3)\n    print(f\"Softsign({input_value3}) = {output_value3}\")\n\n    input_value4 = 100.0\n    output_value4 = softsign(input_value4)\n    print(f\"Softsign({input_value4}) = {output_value4}\")\n\n    input_value5 = -100.0\n    output_value5 = softsign(input_value5)\n    print(f\"Softsign({input_value5}) = {output_value5}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Ensure no division by zero in KL divergence calculation\n    pi_theta_old = np.maximum(pi_theta_old, 1e-8)\n    pi_theta_ref = np.maximum(pi_theta_ref, 1e-8)\n\n    # Compute the clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    # Compute the policy objective term\n    policy_objective_term = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n\n    # Compute the KL divergence penalty term\n    # KL(pi_theta_old || pi_theta_ref) = sum(pi_theta_old * log(pi_theta_old / pi_theta_ref))\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    kl_penalty_term = -beta * kl_divergence\n\n    # Compute the total GRPO objective\n    grpo_obj = policy_objective_term + kl_penalty_term\n\n    return round(grpo_obj, 6)\n\nif __name__ == '__main__':\n    # Example Usage\n    rhos_example = [1.1, 0.9, 1.5, 0.8, 1.2]\n    A_example = [0.5, -0.3, 1.0, -0.1, 0.7]\n    pi_theta_old_example = [0.2, 0.3, 0.1, 0.25, 0.15]\n    pi_theta_ref_example = [0.15, 0.35, 0.1, 0.2, 0.2]\n    epsilon_example = 0.2\n    beta_example = 0.01\n\n    objective_value = grpo_objective(rhos_example, A_example, pi_theta_old_example, pi_theta_ref_example, epsilon_example, beta_example)\n    print(f\"GRPO Objective Value: {objective_value}\")\n\n    # Another example\n    rhos_example_2 = [0.5, 1.8, 1.0, 0.7]\n    A_example_2 = [1.2, -0.8, 0.1, 0.5]\n    pi_theta_old_example_2 = [0.3, 0.2, 0.4, 0.1]\n    pi_theta_ref_example_2 = [0.25, 0.25, 0.3, 0.2]\n    epsilon_example_2 = 0.1\n    beta_example_2 = 0.05\n\n    objective_value_2 = grpo_objective(rhos_example_2, A_example_2, pi_theta_old_example_2, pi_theta_ref_example_2, epsilon_example_2, beta_example_2)\n    print(f\"GRPO Objective Value (Example 2): {objective_value_2}\")\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)\n\nif __name__ == '__main__':\n    # Example usage\n    input_value = 2.0\n    swish_output = swish(input_value)\n    print(f\"Swish({input_value}) = {swish_output}\")\n\n    input_value = -1.0\n    swish_output = swish(input_value)\n    print(f\"Swish({input_value}) = {swish_output}\")\n\n    input_value = 0.0\n    swish_output = swish(input_value)\n    print(f\"Swish({input_value}) = {swish_output}\")\n\n    input_value = 5.5\n    swish_output = swish(input_value)\n    print(f\"Swish({input_value}) = {swish_output}\")\n\n    input_value = -3.2\n    swish_output = swish(input_value)\n    print(f\"Swish({input_value}) = {swish_output}\")\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * (math.exp(x) - 1)), 4)\n\nif __name__ == '__main__':\n    # Example usage\n    input_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n\n    print(\"Input values:\", input_values)\n    print(\"SELU activation values:\")\n    for val in input_values:\n        selu_val = selu(val)\n        print(f\"selu({val}) = {selu_val}\")\n\n    # Example with a numpy array\n    numpy_input = np.array([-3.0, -0.5, 0.5, 3.0])\n    print(\"\\nNumpy input array:\", numpy_input)\n    print(\"SELU activation values for numpy array:\")\n    # Applying the function element-wise for numpy array\n    selu_numpy_output = np.vectorize(selu)(numpy_input)\n    print(selu_numpy_output)\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\u0097 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    linear_model = np.dot(X, weights) + bias\n\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_model))\n\n    # Convert probabilities to binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n\n    return predictions.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    # Create some sample data\n    X_sample = np.array([[1.0, 2.0],\n                         [3.0, 4.0],\n                         [5.0, 6.0],\n                         [-1.0, -2.0]])\n\n    # Sample weights and bias\n    weights_sample = np.array([0.5, -0.2])\n    bias_sample = 0.1\n\n    # Get predictions\n    predictions_sample = predict_logistic(X_sample, weights_sample, bias_sample)\n\n    print(\"Input Feature Matrix (X):\")\n    print(X_sample)\n    print(\"\\nModel Weights:\")\n    print(weights_sample)\n    print(\"\\nModel Bias:\")\n    print(bias_sample)\n    print(\"\\nBinary Predictions:\")\n    print(predictions_sample)\n\n    # Another example\n    X_sample_2 = np.array([[0.1, 0.9],\n                           [0.6, 0.4],\n                           [-0.5, 0.8]])\n    weights_sample_2 = np.array([1.0, -1.0])\n    bias_sample_2 = 0.0\n\n    predictions_sample_2 = predict_logistic(X_sample_2, weights_sample_2, bias_sample_2)\n\n    print(\"\\nInput Feature Matrix (X):\")\n    print(X_sample_2)\n    print(\"\\nModel Weights:\")\n    print(weights_sample_2)\n    print(\"\\nModel Bias:\")\n    print(bias_sample_2)\n    print(\"\\nBinary Predictions:\")\n    print(predictions_sample_2)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = len(np.unique(y))\n\n    # Initialize weights (coefficients)\n    # Add a bias term by adding a column of ones to X\n    X = np.hstack((np.ones((n_samples, 1)), X))\n    n_features += 1  # Update n_features to include the bias term\n\n    # Initialize weights with zeros\n    B = np.zeros((n_features, n_classes))\n\n    losses = []\n\n    for i in range(iterations):\n        # Compute scores\n        scores = np.dot(X, B)\n\n        # Compute softmax probabilities\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))  # Subtract max for numerical stability\n        softmax_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Compute Cross Entropy loss\n        # Convert y to one-hot encoding\n        one_hot_y = np.zeros((n_samples, n_classes))\n        one_hot_y[np.arange(n_samples), y] = 1\n\n        loss = -np.mean(np.sum(one_hot_y * np.log(softmax_probs + 1e-9), axis=1))  # Add small epsilon for stability\n        losses.append(round(loss, 4))\n\n        # Compute gradient\n        gradient = -np.dot(X.T, (one_hot_y - softmax_probs)) / n_samples\n\n        # Update weights\n        B -= learning_rate * gradient\n\n    # Round the final weights\n    B = np.round(B, 4)\n\n    return B.tolist(), losses\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Generate some synthetic data\n    np.random.seed(42)\n    X_train = np.random.rand(100, 2) * 10\n    y_train = (X_train[:, 0] + X_train[:, 1] > 10).astype(int) + (X_train[:, 0] * X_train[:, 1] > 30).astype(int)\n\n    # Train the Softmax Regression model\n    learning_rate = 0.01\n    iterations = 1000\n    coefficients, training_losses = train_softmaxreg(X_train, y_train, learning_rate, iterations)\n\n    print(\"Optimized Coefficients (B):\")\n    print(coefficients)\n    print(\"\\nCollected Losses over Iterations:\")\n    print(training_losses[:10]) # Print first 10 losses as an example\n    print(\"...\")\n    print(training_losses[-10:]) # Print last 10 losses as an example\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n        X: Input features (numpy array of shape (n_samples, n_features)).\n        y: Target labels (numpy array of shape (n_samples,)).\n        learning_rate: The learning rate for gradient descent.\n        iterations: The number of iterations for gradient descent.\n\n    Returns:\n        A tuple containing:\n            - A list of the optimized coefficients (including the intercept).\n            - A list of the loss values collected over iterations.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Add a column of ones for the intercept term\n    X = np.insert(X, 0, 1, axis=1)\n\n    # Initialize coefficients (including intercept)\n    weights = np.zeros(n_features + 1)\n\n    loss_history = []\n\n    for i in range(iterations):\n        # Calculate predicted probabilities using the sigmoid function\n        linear_model = X @ weights\n        y_predicted = 1 / (1 + np.exp(-linear_model))\n\n        # Calculate the Binary Cross Entropy loss\n        # Add a small epsilon to avoid log(0)\n        epsilon = 1e-15\n        loss = -np.mean(y * np.log(y_predicted + epsilon) + (1 - y) * np.log(1 - y_predicted + epsilon))\n        loss_history.append(round(loss, 4))\n\n        # Calculate the gradient\n        gradient = (1 / n_samples) * X.T @ (y_predicted - y)\n\n        # Update weights\n        weights -= learning_rate * gradient\n\n    # Round the final weights to 4 decimal places\n    rounded_weights = [round(w, 4) for w in weights]\n\n    return rounded_weights, loss_history\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Generate some synthetic data\n    np.random.seed(42)\n    X_train = 2 * np.random.rand(100, 2)\n    y_train = (X_train[:, 0] + X_train[:, 1] > 1.5).astype(int)\n\n    # Set hyperparameters\n    learning_rate = 0.1\n    iterations = 1000\n\n    # Train the logistic regression model\n    optimized_weights, loss_values = train_logreg(X_train, y_train, learning_rate, iterations)\n\n    print(\"Optimized Weights (including intercept):\", optimized_weights)\n    print(\"Loss Values over Iterations (last 10):\", loss_values[-10:])\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores\n    scores = np.dot(Q, K.T)\n\n    # Scale scores\n    d_k = Q.shape[-1]\n    scaled_scores = scores / np.sqrt(d_k)\n\n    # Apply mask\n    # The mask should be 1 where attention is allowed, and 0 where it's not.\n    # We set the masked positions to a very small negative number so that\n    # their softmax probability becomes close to zero.\n    masked_scores = scaled_scores + (1 - mask) * -1e9\n\n    # Compute attention weights using softmax\n    attention_weights = np.exp(masked_scores - np.max(masked_scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n\n    # Compute the output\n    output = np.dot(attention_weights, V)\n\n    return output.tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Assume input sequence length is 4, embedding dimension is 8, and head dimension is 8\n    seq_len = 4\n    embed_dim = 8\n    head_dim = 8\n\n    # Example input sequence (batch size 1 for simplicity)\n    X = np.random.rand(seq_len, embed_dim)\n\n    # Example weight matrices for Q, K, V\n    W_q = np.random.rand(embed_dim, head_dim)\n    W_k = np.random.rand(embed_dim, head_dim)\n    W_v = np.random.rand(embed_dim, head_dim)\n\n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Example causal mask (for text generation, preventing attending to future tokens)\n    # The mask is a lower triangular matrix of ones.\n    mask = np.tril(np.ones((seq_len, seq_len)))\n\n    # Compute masked self-attention\n    attention_output = masked_attention(Q, K, V, mask)\n\n    print(\"Input sequence (X):\\n\", X)\n    print(\"\\nQuery matrix (Q):\\n\", Q)\n    print(\"\\nKey matrix (K):\\n\", K)\n    print(\"\\nValue matrix (V):\\n\", V)\n    print(\"\\nAttention mask:\\n\", mask)\n    print(\"\\nMasked self-attention output (as list):\\n\", attention_output)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon entropy as the measure of disorder.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0\n\n    color_counts = Counter(apples)\n    disorder_value = 0.0\n\n    for count in color_counts.values():\n        probability = count / n\n        if probability > 0:  # Avoid log(0)\n            disorder_value -= probability * math.log2(probability)\n\n    return round(disorder_value, 4)\n\nif __name__ == '__main__':\n    print(f\"[0,0,0,0]: {disorder([0,0,0,0])}\")\n    print(f\"[1,1,0,0]: {disorder([1,1,0,0])}\")\n    print(f\"[0,1,2,3]: {disorder([0,1,2,3])}\")\n    print(f\"[0,0,1,1,2,2,3,3]: {disorder([0,0,1,1,2,2,3,3])}\")\n    print(f\"[]: {disorder([])}\")\n    print(f\"[0]: {disorder([0])}\")\n    print(f\"[0,0,0,0,0,1,2,3]: {disorder([0,0,0,0,0,1,2,3])}\")\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Layer Normalization on an input tensor.\n\n    Args:\n        X: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n        gamma: Scaling parameter of shape (feature_dimension,).\n        beta: Shifting parameter of shape (feature_dimension,).\n        epsilon: A small constant to avoid division by zero.\n\n    Returns:\n        A list representing the normalized tensor, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * normalized_X + beta\n    return np.round(output, 5).tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    batch_size = 2\n    sequence_length = 3\n    feature_dimension = 4\n\n    # Create a sample input tensor\n    X = np.array([\n        [[1.0, 2.0, 3.0, 4.0],\n         [5.0, 6.0, 7.0, 8.0],\n         [9.0, 10.0, 11.0, 12.0]],\n\n        [[13.0, 14.0, 15.0, 16.0],\n         [17.0, 18.0, 19.0, 20.0],\n         [21.0, 22.0, 23.0, 24.0]]\n    ])\n\n    # Create sample gamma and beta parameters\n    gamma = np.array([1.0, 1.0, 1.0, 1.0])\n    beta = np.array([0.0, 0.0, 0.0, 0.0])\n\n    # Perform layer normalization\n    normalized_output = layer_normalization(X, gamma, beta)\n\n    print(\"Input Tensor X:\")\n    print(X)\n    print(\"\\nGamma:\")\n    print(gamma)\n    print(\"\\nBeta:\")\n    print(beta)\n    print(\"\\nNormalized Output (List):\")\n    print(normalized_output)\n\n    # Another example with different gamma and beta\n    gamma_2 = np.array([2.0, 2.0, 2.0, 2.0])\n    beta_2 = np.array([1.0, 1.0, 1.0, 1.0])\n\n    normalized_output_2 = layer_normalization(X, gamma_2, beta_2)\n    print(\"\\nNormalized Output with different gamma and beta (List):\")\n    print(normalized_output_2)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Computes the METEOR score for evaluating machine translation quality.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Parameter for the F-mean calculation (default: 0.9).\n        beta (float): Parameter for the penalty calculation (default: 3).\n        gamma (float): Parameter for the penalty calculation (default: 0.5).\n\n    Returns:\n        float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n\n    # 1. Unigram Matching\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n\n    matches = 0\n    matched_cand_indices = set()\n    matched_ref_indices = set()\n\n    for i, token in enumerate(cand_tokens):\n        if token in ref_counts and ref_counts[token] > 0:\n            # Find the first available index in the reference\n            try:\n                ref_index = ref_tokens.index(token, 0 if not matched_ref_indices else max(matched_ref_indices) + 1)\n                if ref_index not in matched_ref_indices:\n                    matches += 1\n                    ref_counts[token] -= 1\n                    matched_cand_indices.add(i)\n                    matched_ref_indices.add(ref_index)\n            except ValueError:\n                pass # Token not found in remaining reference tokens\n\n    # 2. Precision and Recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n\n    # 3. F-mean\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if (alpha * precision + (1 - alpha) * recall) > 0 else 0\n\n    # 4. Penalty for Fragmentation\n    # Find contiguous chunks of matched words\n    chunks = 0\n    if matches > 0:\n        sorted_matched_cand_indices = sorted(list(matched_cand_indices))\n        if sorted_matched_cand_indices:\n            chunks = 1\n            for i in range(1, len(sorted_matched_cand_indices)):\n                if sorted_matched_cand_indices[i] != sorted_matched_cand_indices[i-1] + 1:\n                    chunks += 1\n\n    penalty = gamma * (chunks / matches) ** beta if matches > 0 else 0\n\n    # 5. Final METEOR Score\n    meteor = f_mean * (1 - penalty)\n\n    return round(meteor, 3)\n\nif __name__ == '__main__':\n    reference1 = \"the quick brown fox jumps over the lazy dog\"\n    candidate1 = \"the quick brown fox jumps over the lazy dog\"\n    print(f\"Reference: {reference1}\")\n    print(f\"Candidate: {candidate1}\")\n    print(f\"METEOR Score: {meteor_score(reference1, candidate1)}\") # Expected: close to 1.0\n\n    reference2 = \"this is a test sentence\"\n    candidate2 = \"this is a sentence test\"\n    print(f\"\\nReference: {reference2}\")\n    print(f\"Candidate: {candidate2}\")\n    print(f\"METEOR Score: {meteor_score(reference2, candidate2)}\") # Expected: lower than 1.0 due to word order\n\n    reference3 = \"the cat sat on the mat\"\n    candidate3 = \"a dog sat on a mat\"\n    print(f\"\\nReference: {reference3}\")\n    print(f\"Candidate: {candidate3}\")\n    print(f\"METEOR Score: {meteor_score(reference3, candidate3)}\") # Expected: lower due to different words\n\n    reference4 = \"hello world\"\n    candidate4 = \"goodbye world\"\n    print(f\"\\nReference: {reference4}\")\n    print(f\"Candidate: {candidate4}\")\n    print(f\"METEOR Score: {meteor_score(reference4, candidate4)}\")\n\n    reference5 = \"the quick brown fox\"\n    candidate5 = \"the fox quick brown\"\n    print(f\"\\nReference: {reference5}\")\n    print(f\"Candidate: {candidate5}\")\n    print(f\"METEOR Score: {meteor_score(reference5, candidate5)}\")\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events.\n\n    Args:\n        joint_counts: The number of times both events occurred together.\n        total_counts_x: The number of times the first event occurred.\n        total_counts_y: The number of times the second event occurred.\n        total_samples: The total number of samples or observations.\n\n    Returns:\n        The Pointwise Mutual Information (PMI) as a float, rounded to 3 decimal places.\n        Returns 0.0 if any of the counts are zero to avoid division by zero or log(0).\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0 or total_samples == 0:\n        return 0.0\n\n    # Calculate probabilities\n    prob_xy = joint_counts / total_samples\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n\n    # Calculate expected joint probability under independence\n    expected_prob_xy = prob_x * prob_y\n\n    # Avoid log(0) if expected_prob_xy is zero\n    if expected_prob_xy == 0:\n        return 0.0\n\n    # Compute PMI\n    pmi = np.log2(prob_xy / expected_prob_xy)\n\n    return round(pmi, 3)\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Let's say we have 100 sentences.\n    # The word \"apple\" appears 20 times.\n    # The word \"pie\" appears 15 times.\n    # The phrase \"apple pie\" appears 10 times.\n    joint_counts_example = 10\n    total_counts_x_example = 20\n    total_counts_y_example = 15\n    total_samples_example = 100\n\n    pmi_value = compute_pmi(joint_counts_example, total_counts_x_example, total_counts_y_example, total_samples_example)\n    print(f\"PMI for the example: {pmi_value}\")\n\n    # Example with zero counts\n    joint_counts_zero = 0\n    total_counts_x_zero = 10\n    total_counts_y_zero = 10\n    total_samples_zero = 100\n    pmi_zero = compute_pmi(joint_counts_zero, total_counts_x_zero, total_counts_y_zero, total_samples_zero)\n    print(f\"PMI with zero joint counts: {pmi_zero}\")\n\n    # Example with one of the individual counts being zero\n    joint_counts_zero_individual = 5\n    total_counts_x_zero_individual = 0\n    total_counts_y_zero_individual = 10\n    total_samples_zero_individual = 100\n    pmi_zero_individual = compute_pmi(joint_counts_zero_individual, total_counts_x_zero_individual, total_counts_y_zero_individual, total_samples_zero_individual)\n    print(f\"PMI with zero individual count: {pmi_zero_individual}\")\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n\n    Args:\n        x: A list of integers.\n\n    Returns:\n        A list of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n\n    min_val = min(x)\n    max_val = max(x)\n\n    if min_val == max_val:\n        # Handle the case where all values are the same\n        return [0.0] * len(x)\n\n    normalized_x = []\n    for val in x:\n        normalized_val = (val - min_val) / (max_val - min_val)\n        normalized_x.append(round(normalized_val, 4))\n\n    return normalized_x\n\nif __name__ == '__main__':\n    # Example usage:\n    data1 = [1, 2, 3, 4, 5]\n    normalized_data1 = min_max(data1)\n    print(f\"Original data: {data1}\")\n    print(f\"Normalized data: {normalized_data1}\")\n\n    data2 = [10, 20, 30, 40, 50]\n    normalized_data2 = min_max(data2)\n    print(f\"Original data: {data2}\")\n    print(f\"Normalized data: {normalized_data2}\")\n\n    data3 = [5, 5, 5, 5, 5]\n    normalized_data3 = min_max(data3)\n    print(f\"Original data: {data3}\")\n    print(f\"Normalized data: {normalized_data3}\")\n\n    data4 = []\n    normalized_data4 = min_max(data4)\n    print(f\"Original data: {data4}\")\n    print(f\"Normalized data: {normalized_data4}\")\n\n    data5 = [100]\n    normalized_data5 = min_max(data5)\n    print(f\"Original data: {data5}\")\n    print(f\"Normalized data: {normalized_data5}\")\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection using NumPy.\n\n    Args:\n        x: A 1D NumPy array representing the input.\n        w1: A 2D NumPy array representing the weights for the first layer.\n        w2: A 2D NumPy array representing the weights for the second layer.\n\n    Returns:\n        A list representing the output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure input is 1D\n    if x.ndim != 1:\n        raise ValueError(\"Input 'x' must be a 1D array.\")\n\n    # Ensure weight matrices have compatible dimensions\n    if w1.shape[1] != x.shape[0]:\n        raise ValueError(f\"Weight matrix w1 has incompatible dimensions. Expected {x.shape[0]} columns, got {w1.shape[1]}.\")\n    if w2.shape[1] != w1.shape[0]:\n        raise ValueError(f\"Weight matrix w2 has incompatible dimensions. Expected {w1.shape[0]} columns, got {w2.shape[1]}.\")\n    if w2.shape[0] != x.shape[0]:\n         raise ValueError(f\"Output dimension of w2 ({w2.shape[0]}) must match input dimension of x ({x.shape[0]}) for shortcut connection.\")\n\n\n    # First layer\n    h1 = np.dot(w1, x)\n    h1_relu = np.maximum(0, h1)\n\n    # Second layer\n    h2 = np.dot(w2, h1_relu)\n\n    # Shortcut connection\n    output_before_relu = h2 + x\n\n    # Final ReLU activation\n    output = np.maximum(0, output_before_relu)\n\n    # Round to 4 decimal places and convert to list\n    return np.round(output, 4).tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    input_data = np.array([1.0, 2.0, 3.0])\n\n    # Define weight matrices (dimensions must be compatible)\n    # w1: (output_dim_1, input_dim)\n    # w2: (output_dim_2, output_dim_1)\n    # For shortcut, output_dim_2 must equal input_dim\n    weights1 = np.array([[0.1, 0.2, 0.3],\n                         [0.4, 0.5, 0.6]])\n    weights2 = np.array([[0.7, 0.8],\n                         [0.9, 1.0],\n                         [1.1, 1.2]]) # This will cause an error due to dimension mismatch for shortcut\n\n    # Corrected weights2 for shortcut connection\n    weights2_correct = np.array([[0.7, 0.8],\n                                 [0.9, 1.0]]) # Output dimension is 2, input dimension is 3. Still mismatch.\n\n    # Let's make the dimensions compatible for a valid example\n    input_data_valid = np.array([1.0, 2.0])\n    weights1_valid = np.array([[0.1, 0.2],\n                               [0.3, 0.4],\n                               [0.5, 0.6]])\n    weights2_valid = np.array([[0.7, 0.8, 0.9],\n                               [1.0, 1.1, 1.2]]) # Output dimension is 2, input dimension is 2. Compatible.\n\n    try:\n        output = residual_block(input_data_valid, weights1_valid, weights2_valid)\n        print(\"Output of the residual block:\", output)\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\n    # Example with different dimensions (will raise an error)\n    try:\n        output_invalid = residual_block(input_data, weights1, weights2)\n        print(\"Output of the invalid residual block:\", output_invalid)\n    except ValueError as e:\n        print(f\"Error with invalid dimensions: {e}\")\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Args:\n        x: A NumPy array of shape (height, width, channels) representing feature maps.\n\n    Returns:\n        A 1D NumPy array of shape (channels,) where each element is the average\n        of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(\"Input array must be 3-dimensional (height, width, channels)\")\n\n    # Calculate the average along the height and width dimensions\n    # The result will have shape (channels,)\n    return np.mean(x, axis=(0, 1))\n\nif __name__ == '__main__':\n    # Example usage:\n    # Create a sample 3D array representing feature maps\n    # Shape: (height, width, channels)\n    feature_maps = np.array([\n        [[1, 2, 3],\n         [4, 5, 6]],\n\n        [[7, 8, 9],\n         [10, 11, 12]]\n    ])\n\n    print(\"Input feature maps:\")\n    print(feature_maps)\n    print(\"Input shape:\", feature_maps.shape)\n\n    # Perform global average pooling\n    pooled_output = global_avg_pool(feature_maps)\n\n    print(\"\\nPooled output:\")\n    print(pooled_output)\n    print(\"Output shape:\", pooled_output.shape)\n\n    # Expected output calculation:\n    # Channel 0: (1 + 4 + 7 + 10) / 4 = 22 / 4 = 5.5\n    # Channel 1: (2 + 5 + 8 + 11) / 4 = 26 / 4 = 6.5\n    # Channel 2: (3 + 6 + 9 + 12) / 4 = 30 / 4 = 7.5\n    # Expected pooled_output: [5.5, 6.5, 7.5]\n\n    # Another example with different dimensions\n    feature_maps_2 = np.arange(2 * 3 * 4).reshape((2, 3, 4))\n    print(\"\\nInput feature maps 2:\")\n    print(feature_maps_2)\n    print(\"Input shape:\", feature_maps_2.shape)\n\n    pooled_output_2 = global_avg_pool(feature_maps_2)\n    print(\"\\nPooled output 2:\")\n    print(pooled_output_2)\n    print(\"Output shape:\", pooled_output_2.shape)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array in BCHW format.\n\n    Args:\n        X: Input NumPy array of shape (batch, channels, height, width).\n        gamma: Scale parameter (NumPy array of shape (channels,)).\n        beta: Shift parameter (NumPy array of shape (channels,)).\n        epsilon: Small value for numerical stability.\n\n    Returns:\n        A list representing the normalized output array, rounded to 4 decimal places.\n    \"\"\"\n    # Calculate mean and variance across batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n\n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n\n    # Apply scale and shift\n    # Reshape gamma and beta to match the dimensions of X_normalized for broadcasting\n    gamma_reshaped = gamma.reshape(1, -1, 1, 1)\n    beta_reshaped = beta.reshape(1, -1, 1, 1)\n    output = gamma_reshaped * X_normalized + beta_reshaped\n\n    # Round to 4 decimal places and convert to list\n    return np.round(output, 4).tolist()\n\nif __name__ == '__main__':\n    # Example Usage:\n    # Create a sample input array (batch=2, channels=3, height=4, width=4)\n    X = np.arange(2 * 3 * 4 * 4).reshape(2, 3, 4, 4).astype(np.float32)\n\n    # Create sample gamma and beta parameters (channels=3)\n    gamma = np.array([1.0, 1.5, 0.8], dtype=np.float32)\n    beta = np.array([0.1, -0.2, 0.5], dtype=np.float32)\n\n    # Perform batch normalization\n    normalized_output = batch_normalization(X, gamma, beta)\n\n    # Print the output\n    print(\"Input array X:\")\n    print(X)\n    print(\"\\nGamma:\", gamma)\n    print(\"Beta:\", beta)\n    print(\"\\nNormalized output (rounded to 4 decimal places and converted to list):\")\n    print(normalized_output)\n\n    # Another example with different values\n    X_2 = np.random.rand(1, 2, 3, 3) * 10\n    gamma_2 = np.array([2.0, 0.5])\n    beta_2 = np.array([-0.5, 0.3])\n\n    normalized_output_2 = batch_normalization(X_2, gamma_2, beta_2)\n    print(\"\\nInput array X_2:\")\n    print(X_2)\n    print(\"\\nGamma_2:\", gamma_2)\n    print(\"Beta_2:\", beta_2)\n    print(\"\\nNormalized output 2 (rounded to 4 decimal places and converted to list):\")\n    print(normalized_output_2)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Args:\n        c: The coefficient of the polynomial term.\n        x: The point at which to evaluate the derivative.\n        n: The exponent of the polynomial term.\n\n    Returns:\n        The value of the derivative of the polynomial term at x, rounded to 4 decimal places.\n    \"\"\"\n    # The derivative of c * x^n is c * n * x^(n-1)\n    if n == 0:\n        # The derivative of a constant term (c * x^0) is 0\n        derivative_value = 0.0\n    else:\n        derivative_value = c * n * (x**(n - 1))\n\n    return round(derivative_value, 4)\n\nif __name__ == '__main__':\n    # Example usage:\n    # Derivative of 3 * x^2 at x = 2\n    c1 = 3.0\n    x1 = 2.0\n    n1 = 2.0\n    derivative1 = poly_term_derivative(c1, x1, n1)\n    print(f\"The derivative of {c1} * x^{n1} at x = {x1} is: {derivative1}\") # Expected: 12.0\n\n    # Derivative of 5 * x^3 at x = 1\n    c2 = 5.0\n    x2 = 1.0\n    n2 = 3.0\n    derivative2 = poly_term_derivative(c2, x2, n2)\n    print(f\"The derivative of {c2} * x^{n2} at x = {x2} is: {derivative2}\") # Expected: 15.0\n\n    # Derivative of 7 * x^0 (constant) at x = 5\n    c3 = 7.0\n    x3 = 5.0\n    n3 = 0.0\n    derivative3 = poly_term_derivative(c3, x3, n3)\n    print(f\"The derivative of {c3} * x^{n3} at x = {x3} is: {derivative3}\") # Expected: 0.0\n\n    # Derivative of -2 * x^1 at x = 3\n    c4 = -2.0\n    x4 = 3.0\n    n4 = 1.0\n    derivative4 = poly_term_derivative(c4, x4, n4)\n    print(f\"The derivative of {c4} * x^{n4} at x = {x4} is: {derivative4}\") # Expected: -2.0\n\n    # Derivative of 4 * x^0.5 at x = 4\n    c5 = 4.0\n    x5 = 4.0\n    n5 = 0.5\n    derivative5 = poly_term_derivative(c5, x5, n5)\n    print(f\"The derivative of {c5} * x^{n5} at x = {x5} is: {derivative5}\") # Expected: 1.0\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Args:\n        vectors: A list of 2D vectors (list of lists of floats).\n        tol: A tolerance value to determine linear independence.\n\n    Returns:\n        A list of orthonormal vectors (list of lists of floats) that span the\n        same subspace, rounded to 4 decimal places.\n    \"\"\"\n    vectors_np = [np.array(v) for v in vectors]\n    orthonormal_basis_list = []\n\n    for v in vectors_np:\n        u = v.copy()\n        for b in orthonormal_basis_list:\n            projection = np.dot(v, b) * b\n            u -= projection\n\n        # Check for linear independence\n        if np.linalg.norm(u) > tol:\n            orthonormal_basis_list.append(u / np.linalg.norm(u))\n\n    # Round and convert to list\n    rounded_basis = [np.round(b, 4).tolist() for b in orthonormal_basis_list]\n\n    return rounded_basis\n\nif __name__ == '__main__':\n    # Example Usage\n    vectors1 = [[1, 1], [1, 0]]\n    basis1 = orthonormal_basis(vectors1)\n    print(f\"Orthonormal basis for {vectors1}: {basis1}\")\n\n    vectors2 = [[1, 0], [0, 1]]\n    basis2 = orthonormal_basis(vectors2)\n    print(f\"Orthonormal basis for {vectors2}: {basis2}\")\n\n    vectors3 = [[1, 1], [2, 2]] # Linearly dependent\n    basis3 = orthonormal_basis(vectors3)\n    print(f\"Orthonormal basis for {vectors3}: {basis3}\")\n\n    vectors4 = [[3, 4], [5, 12]]\n    basis4 = orthonormal_basis(vectors4)\n    print(f\"Orthonormal basis for {vectors4}: {basis4}\")\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n\n    Args:\n        a (list or np.ndarray): The first 3D vector.\n        b (list or np.ndarray): The second 3D vector.\n\n    Returns:\n        list: The cross product vector, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError(\"Input vectors must be 3-dimensional.\")\n\n    cross_prod = np.cross(a, b)\n    rounded_cross_prod = np.round(cross_prod, 4)\n    return rounded_cross_prod.tolist()\n\nif __name__ == '__main__':\n    # Example usage:\n    vector_a = [1, 0, 0]\n    vector_b = [0, 1, 0]\n    result = cross_product(vector_a, vector_b)\n    print(f\"The cross product of {vector_a} and {vector_b} is: {result}\")\n\n    vector_c = [1, 2, 3]\n    vector_d = [4, 5, 6]\n    result2 = cross_product(vector_c, vector_d)\n    print(f\"The cross product of {vector_c} and {vector_d} is: {result2}\")\n\n    vector_e = [1.123456, 2.789012, 3.456789]\n    vector_f = [4.987654, 5.321098, 6.654321]\n    result3 = cross_product(vector_e, vector_f)\n    print(f\"The cross product of {vector_e} and {vector_f} is: {result3}\")\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Args:\n        A: A square numpy array representing the coefficient matrix.\n        b: A numpy array representing the constant vector.\n\n    Returns:\n        A list representing the solution vector x, rounded to 4 decimal places,\n        or -1 if the determinant of A is zero.\n    \"\"\"\n    det_A = np.linalg.det(A)\n\n    if np.isclose(det_A, 0):\n        return -1\n\n    n = A.shape[0]\n    x = np.zeros(n)\n\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n\n    return np.round(x, 4).tolist()\n\nif __name__ == '__main__':\n    # Example 1: Unique solution\n    A1 = np.array([[2, 1], [1, -1]])\n    b1 = np.array([4, 1])\n    solution1 = cramers_rule(A1, b1)\n    print(f\"System 1: A = {A1.tolist()}, b = {b1.tolist()}\")\n    print(f\"Solution 1: {solution1}\") # Expected output: [1.6667, 0.6667] (approximately)\n\n    # Example 2: No unique solution (determinant is zero)\n    A2 = np.array([[2, -4], [-1, 2]])\n    b2 = np.array([1, 3])\n    solution2 = cramers_rule(A2, b2)\n    print(f\"\\nSystem 2: A = {A2.tolist()}, b = {b2.tolist()}\")\n    print(f\"Solution 2: {solution2}\") # Expected output: -1\n\n    # Example 3: 3x3 system\n    A3 = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]])\n    b3 = np.array([6, 7, 8])\n    solution3 = cramers_rule(A3, b3)\n    print(f\"\\nSystem 3: A = {A3.tolist()}, b = {b3.tolist()}\")\n    print(f\"Solution 3: {solution3}\")\n\n    # Example 4: Another system with unique solution\n    A4 = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\n    b4 = np.array([1, -2, 0])\n    solution4 = cramers_rule(A4, b4)\n    print(f\"\\nSystem 4: A = {A4.tolist()}, b = {b4.tolist()}\")\n    print(f\"Solution 4: {solution4}\")\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n\n    Args:\n        p: A list of floats representing the first probability distribution.\n        q: A list of floats representing the second probability distribution.\n\n    Returns:\n        The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if the\n        inputs have different lengths or are empty.\n    \"\"\"\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n\n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(np.array(p) * np.array(q)))\n\n    # Calculate the Bhattacharyya distance\n    # Ensure the argument to log is positive to avoid NaNs\n    if bc <= 0:\n        # If coefficient is 0 or negative (due to floating point issues),\n        # the distance is infinite. For practical purposes in this context,\n        # returning a large value or handling as an edge case might be needed.\n        # Given the problem constraints and typical use cases, assuming valid\n        # probability distributions where bc > 0 is reasonable.\n        # However, to be safe and avoid log(0), we can return a large value\n        # or 0.0 based on interpretation. Returning 0.0 for edge cases\n        # aligns with the problem's handling of invalid inputs.\n        return 0.0\n    else:\n        distance = -np.log(bc)\n\n    return round(distance, 4)\n\nif __name__ == '__main__':\n    # Example usage:\n    p1 = [0.1, 0.2, 0.3, 0.4]\n    q1 = [0.2, 0.1, 0.4, 0.3]\n    dist1 = bhattacharyya_distance(p1, q1)\n    print(f\"Bhattacharyya distance between p1 and q1: {dist1}\")\n\n    p2 = [0.5, 0.5]\n    q2 = [0.5, 0.5]\n    dist2 = bhattacharyya_distance(p2, q2)\n    print(f\"Bhattacharyya distance between p2 and q2: {dist2}\")\n\n    p3 = [0.1, 0.9]\n    q3 = [0.9, 0.1]\n    dist3 = bhattacharyya_distance(p3, q3)\n    print(f\"Bhattacharyya distance between p3 and q3: {dist3}\")\n\n    # Different lengths\n    p4 = [0.1, 0.2]\n    q4 = [0.1, 0.2, 0.3]\n    dist4 = bhattacharyya_distance(p4, q4)\n    print(f\"Bhattacharyya distance between p4 and q4 (different lengths): {dist4}\")\n\n    # Empty lists\n    p5 = []\n    q5 = []\n    dist5 = bhattacharyya_distance(p5, q5)\n    print(f\"Bhattacharyya distance between p5 and q5 (empty lists): {dist5}\")\n\n    # One empty list\n    p6 = [0.1, 0.9]\n    q6 = []\n    dist6 = bhattacharyya_distance(p6, q6)\n    print(f\"Bhattacharyya distance between p6 and q6 (one empty list): {dist6}\")\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n\n    Args:\n        a: The first vector (list of numbers).\n        b: The second vector (list of numbers).\n\n    Returns:\n        A new vector representing the sum if the dimensions are compatible,\n        otherwise -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    \n    result = []\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    \n    return result\n\nif __name__ == '__main__':\n    # Example usage\n    vector1 = [1, 2, 3]\n    vector2 = [4, 5, 6]\n    vector3 = [7, 8]\n\n    sum_result1 = vector_sum(vector1, vector2)\n    print(f\"Sum of {vector1} and {vector2}: {sum_result1}\")\n\n    sum_result2 = vector_sum(vector1, vector3)\n    print(f\"Sum of {vector1} and {vector3}: {sum_result2}\")\n\n    vector4 = [1.1, 2.2, 3.3]\n    vector5 = [4.4, 5.5, 6.6]\n    sum_result3 = vector_sum(vector4, vector5)\n    print(f\"Sum of {vector4} and {vector5}: {sum_result3}\")\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n\n    Args:\n        theta: A 2D NumPy array of shape (num_states, num_actions) representing\n               the policy parameters.\n        episodes: A list of episodes, where each episode is a list of\n                  (state, action, reward) tuples.\n\n    Returns:\n        A NumPy array of the same shape as theta, representing the average\n        policy gradient.\n    \"\"\"\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    num_episodes = len(episodes)\n\n    for episode in episodes:\n        episode_gradient = np.zeros_like(theta)\n        returns = []\n        # Compute returns for the episode\n        G = 0\n        for t in reversed(range(len(episode))):\n            state, action, reward = episode[t]\n            G = reward + G  # Assuming discount factor gamma = 1\n            returns.insert(0, G)\n\n        # Compute gradient for the episode\n        for t in range(len(episode)):\n            state, action, reward = episode[t]\n            G_t = returns[t]\n\n            # Compute softmax probabilities\n            exp_theta = np.exp(theta[state, :] - np.max(theta[state, :]))\n            policy_probs = exp_theta / np.sum(exp_theta)\n\n            # Compute gradient of log policy\n            log_policy_gradient = np.zeros(num_actions)\n            log_policy_gradient[action] = 1 - policy_probs[action]\n            log_policy_gradient -= policy_probs\n\n            episode_gradient[state, :] += log_policy_gradient * G_t\n\n        gradient += episode_gradient\n\n    # Average the gradients over all episodes\n    average_gradient = gradient / num_episodes\n\n    # Round to the nearest 4th decimal\n    rounded_gradient = np.round(average_gradient, 4)\n\n    return rounded_gradient.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    num_states = 2\n    num_actions = 3\n    theta = np.array([[0.1, 0.2, 0.3],\n                      [0.4, 0.5, 0.6]])\n\n    # Example episodes\n    episodes = [\n        [(0, 0, 1.0), (1, 1, 2.0)],\n        [(0, 1, 0.5), (1, 2, 1.5), (0, 0, 1.0)],\n    ]\n\n    policy_gradient = compute_policy_gradient(theta, episodes)\n    print(\"Computed Policy Gradient:\")\n    print(policy_gradient)\n\n    # Another example\n    num_states_2 = 3\n    num_actions_2 = 2\n    theta_2 = np.array([[0.0, 0.0],\n                        [0.0, 0.0],\n                        [0.0, 0.0]])\n\n    episodes_2 = [\n        [(0, 0, 1.0), (1, 1, 0.0), (2, 0, 1.0)],\n        [(0, 1, 0.0), (1, 0, 1.0), (2, 1, 0.0)],\n    ]\n\n    policy_gradient_2 = compute_policy_gradient(theta_2, episodes_2)\n    print(\"\\nComputed Policy Gradient (Example 2):\")\n    print(policy_gradient_2)\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculates the computational cost savings of an MoE layer compared to a dense layer.\n\n    Args:\n        n_experts (int): The total number of experts in the MoE layer.\n        k_active (int): The number of active experts per input.\n        d_in (int): The input dimension.\n        d_out (int): The output dimension.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: FLOPs for the dense layer.\n            - float: FLOPs for the MoE layer.\n            - float: Percentage of FLOPs saved by the MoE layer.\n    \"\"\"\n    # FLOPs for a dense layer: input_dim * output_dim * 2 (for multiply and add)\n    dense_flops = d_in * d_out * 2\n\n    # FLOPs for an MoE layer:\n    # Each active expert performs input_dim * output_dim * 2 FLOPs.\n    # The gating network also has a cost, but for simplicity and focusing on the\n    # expert layer comparison as in the paper's core argument, we primarily\n    # compare the expert computations. The paper's main point about efficiency\n    # comes from the sparsity in the expert layer.\n    # So, the FLOPs for the MoE layer are k_active * (input_dim * output_dim * 2)\n    moe_flops = k_active * d_in * d_out * 2\n\n    # Calculate savings\n    flops_saved = dense_flops - moe_flops\n    savings_percentage = (flops_saved / dense_flops) * 100 if dense_flops > 0 else 0\n\n    return round(dense_flops, 1), round(moe_flops, 1), round(savings_percentage, 1)\n\nif __name__ == '__main__':\n    # Example usage based on typical MoE configurations\n    n_experts_example = 128\n    k_active_example = 2\n    d_in_example = 1024\n    d_out_example = 4096\n\n    dense_flops, moe_flops, savings_percentage = compute_efficiency(\n        n_experts_example, k_active_example, d_in_example, d_out_example\n    )\n\n    print(f\"Dense Layer FLOPs: {dense_flops}\")\n    print(f\"MoE Layer FLOPs (k={k_active_example}): {moe_flops}\")\n    print(f\"Computational Savings: {savings_percentage}%\")\n\n    # Another example\n    n_experts_example_2 = 1000\n    k_active_example_2 = 4\n    d_in_example_2 = 512\n    d_out_example_2 = 2048\n\n    dense_flops_2, moe_flops_2, savings_percentage_2 = compute_efficiency(\n        n_experts_example_2, k_active_example_2, d_in_example_2, d_out_example_2\n    )\n\n    print(\"\\n--- Another Example ---\")\n    print(f\"Dense Layer FLOPs: {dense_flops_2}\")\n    print(f\"MoE Layer FLOPs (k={k_active_example_2}): {moe_flops_2}\")\n    print(f\"Computational Savings: {savings_percentage_2}%\")\n```"}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n\n    Args:\n        X: Input matrix (batch_size, input_dim).\n        W_g: Weight matrix for the gating function (input_dim, num_experts).\n        W_noise: Weight matrix for the noise (input_dim, num_experts).\n        N: Pre-sampled noise matrix (batch_size, num_experts).\n        k: Sparsity constraint (number of experts to select).\n\n    Returns:\n        A list representing the gating probabilities matrix (batch_size, num_experts),\n        rounded to the nearest 4th decimal.\n    \"\"\"\n    # Compute the clean gating scores\n    clean_logits = np.dot(X, W_g)\n\n    # Compute the noisy gating scores\n    noisy_logits = clean_logits + np.dot(X, W_noise) * N\n\n    # Find the top-k noisy logits for each sample in the batch\n    topk_indices = np.argsort(noisy_logits, axis=-1)[:, -k:]\n\n    # Create a mask for the top-k experts\n    mask = np.zeros_like(noisy_logits, dtype=bool)\n    for i in range(mask.shape[0]):\n        mask[i, topk_indices[i]] = True\n\n    # Apply the mask to the clean logits\n    masked_logits = np.where(mask, clean_logits, -np.inf)\n\n    # Compute the softmax over the masked logits\n    # Subtract the maximum for numerical stability\n    max_logits = np.max(masked_logits, axis=-1, keepdims=True)\n    exp_logits = np.exp(masked_logits - max_logits)\n    sum_exp_logits = np.sum(exp_logits, axis=-1, keepdims=True)\n    gating_probabilities = exp_logits / sum_exp_logits\n\n    # Handle cases where sum_exp_logits is zero (e.g., all masked logits are -inf)\n    gating_probabilities = np.nan_to_num(gating_probabilities, nan=0.0)\n\n    # Round the results to the nearest 4th decimal\n    gating_probabilities = np.round(gating_probabilities, 4)\n\n    return gating_probabilities.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    batch_size = 4\n    input_dim = 8\n    num_experts = 6\n    k = 2\n\n    # Generate random input data, weights, and noise\n    X = np.random.rand(batch_size, input_dim)\n    W_g = np.random.rand(input_dim, num_experts)\n    W_noise = np.random.rand(input_dim, num_experts)\n    N = np.random.randn(batch_size, num_experts) # Standard normal noise\n\n    # Compute the gating probabilities\n    gating_probs = noisy_topk_gating(X, W_g, W_noise, N, k)\n\n    print(\"Input Matrix X:\")\n    print(X)\n    print(\"\\nWeight Matrix W_g:\")\n    print(W_g)\n    print(\"\\nWeight Matrix W_noise:\")\n    print(W_noise)\n    print(\"\\nNoise Matrix N:\")\n    print(N)\n    print(f\"\\nSparsity Constraint k: {k}\")\n    print(\"\\nGating Probabilities (rounded to 4 decimals):\")\n    for row in gating_probs:\n        print(row)\n\n    # Verify that only k experts have non-zero probabilities for each sample\n    gating_probs_np = np.array(gating_probs)\n    for i in range(batch_size):\n        non_zero_count = np.count_nonzero(gating_probs_np[i])\n        print(f\"\\nSample {i}: Number of non-zero probabilities = {non_zero_count}\")\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Sparse Mixture of Experts (MoE) layer with softmax gating and top-k routing.\n\n    Args:\n        x: Input tensor of shape (batch_size, sequence_length, input_dim).\n        We: Expert weight matrices of shape (n_experts, input_dim, output_dim).\n        Wg: Gating weight matrix of shape (input_dim, n_experts).\n        n_experts: The total number of experts.\n        top_k: The number of top experts to select for each token.\n\n    Returns:\n        A list representing the output tensor of shape (batch_size, sequence_length, output_dim),\n        with values rounded to the nearest 4th decimal.\n    \"\"\"\n    batch_size, sequence_length, input_dim = x.shape\n    output_dim = We.shape[2]\n\n    # Reshape input for easier processing\n    x_flat = x.reshape(-1, input_dim)\n    num_tokens = x_flat.shape[0]\n\n    # Compute gating scores\n    gating_scores = np.dot(x_flat, Wg)\n\n    # Apply softmax to gating scores\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=-1, keepdims=True))\n    gating_probs = gating_probs / np.sum(gating_probs, axis=-1, keepdims=True)\n\n    # Select top-k experts\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n\n    # Normalize top-k probabilities\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n\n    # Initialize output tensor\n    output = np.zeros((num_tokens, output_dim))\n\n    # Apply expert transformations and aggregate\n    for i in range(num_tokens):\n        for j in range(top_k):\n            expert_index = top_k_indices[i, j]\n            expert_weight = We[expert_index]\n            expert_output = np.dot(x_flat[i], expert_weight)\n            output[i] += top_k_probs_normalized[i, j] * expert_output\n\n    # Reshape output back to original dimensions\n    output = output.reshape(batch_size, sequence_length, output_dim)\n\n    # Round to the nearest 4th decimal\n    output = np.round(output, 4)\n\n    return output.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    batch_size = 2\n    sequence_length = 3\n    input_dim = 4\n    output_dim = 5\n    n_experts = 6\n    top_k = 2\n\n    # Generate dummy data\n    x = np.random.rand(batch_size, sequence_length, input_dim)\n    We = np.random.rand(n_experts, input_dim, output_dim)\n    Wg = np.random.rand(input_dim, n_experts)\n\n    # Compute MoE output\n    moe_output = moe(x, We, Wg, n_experts, top_k)\n\n    print(\"Input tensor (x):\")\n    print(x)\n    print(\"\\nExpert weights (We):\")\n    print(We)\n    print(\"\\nGating weights (Wg):\")\n    print(Wg)\n    print(f\"\\nNumber of experts: {n_experts}\")\n    print(f\"Top-k: {top_k}\")\n    print(\"\\nMoE Output:\")\n    print(moe_output)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Performs Group Normalization on a 4D input tensor.\n\n    Args:\n        X: Input tensor of shape (B, C, H, W).\n        gamma: Learned scale parameter of shape (C,).\n        beta: Learned shift parameter of shape (C,).\n        num_groups: Number of groups to divide channels into.\n        epsilon: Small value to avoid division by zero.\n\n    Returns:\n        Normalized tensor as a Python list after reshaping and rounding.\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of channels must be divisible by the number of groups.\"\n\n    channels_per_group = C // num_groups\n\n    # Reshape X to (B, num_groups, channels_per_group, H, W)\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n\n    # Calculate mean and variance over the group dimensions (channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n\n    # Reshape back to (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n\n    # Apply learned scale and shift\n    gamma_reshaped = gamma.reshape(1, C, 1, 1)\n    beta_reshaped = beta.reshape(1, C, 1, 1)\n    output = gamma_reshaped * X_normalized + beta_reshaped\n\n    # Round to 4 decimal places\n    output_rounded = np.round(output, 4)\n\n    # Convert to list\n    return output_rounded.tolist()\n\nif __name__ == '__main__':\n    # Example Usage\n    B, C, H, W = 2, 6, 4, 4\n    num_groups = 3\n\n    # Create dummy input data\n    X = np.random.randn(B, C, H, W)\n    gamma = np.random.randn(C)\n    beta = np.random.randn(C)\n\n    # Perform Group Normalization\n    normalized_output = group_normalization(X, gamma, beta, num_groups)\n\n    print(\"Input shape:\", X.shape)\n    print(\"Gamma shape:\", gamma.shape)\n    print(\"Beta shape:\", beta.shape)\n    print(\"Number of groups:\", num_groups)\n    print(\"Output (first element of the list):\", normalized_output[0])\n    print(\"Output shape (from the list structure):\", np.array(normalized_output).shape)\n\n    # Another example with different shapes\n    B, C, H, W = 1, 8, 3, 3\n    num_groups = 4\n    X_2 = np.random.randn(B, C, H, W)\n    gamma_2 = np.random.randn(C)\n    beta_2 = np.random.randn(C)\n    normalized_output_2 = group_normalization(X_2, gamma_2, beta_2, num_groups)\n    print(\"\\nAnother example:\")\n    print(\"Input shape:\", X_2.shape)\n    print(\"Gamma shape:\", gamma_2.shape)\n    print(\"Beta shape:\", beta_2.shape)\n    print(\"Number of groups:\", num_groups)\n    print(\"Output (first element of the list):\", normalized_output_2[0])\n    print(\"Output shape (from the list structure):\", np.array(normalized_output_2).shape)\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n\n    Args:\n        start_x: The initial guess for the value of x.\n        learning_rate: The step size for each iteration of gradient descent.\n        tolerance: The convergence criterion. The algorithm stops when the absolute\n                   change in x is less than this value.\n        max_iters: The maximum number of iterations to perform.\n\n    Returns:\n        The value of x where the function f(x) is minimized, rounded to 4 decimal places.\n    \"\"\"\n\n    # Define the function and its derivative\n    def f(x):\n        return x**4 - 3*x**3 + 2\n\n    def df(x):\n        return 4*x**3 - 9*x**2\n\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n\n        if abs(new_x - x) < tolerance:\n            break\n\n        x = new_x\n\n    return round(x, 4)\n\nif __name__ == '__main__':\n    # Example usage:\n    start_position = 1.0\n    treasure_location = find_treasure(start_position)\n    print(f\"Starting from x = {start_position}, the treasure is found at x = {treasure_location}\")\n\n    start_position = -2.0\n    treasure_location = find_treasure(start_position)\n    print(f\"Starting from x = {start_position}, the treasure is found at x = {treasure_location}\")\n\n    start_position = 3.0\n    treasure_location = find_treasure(start_position)\n    print(f\"Starting from x = {start_position}, the treasure is found at x = {treasure_location}\")\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function.\n\n    Args:\n        x: Input numpy array.\n        alpha: Scaling parameter.\n        gamma: Shifting parameter.\n        beta: Slope parameter.\n\n    Returns:\n        A list of floats representing the output of the DyT function,\n        rounded to the nearest 4th decimal.\n    \"\"\"\n    # Calculate the DyT function\n    dy_tanh = alpha * np.tanh(beta * x + gamma)\n\n    # Round the results to the nearest 4th decimal\n    rounded_dy_tanh = np.round(dy_tanh, 4)\n\n    # Return as a Python list\n    return rounded_dy_tanh.tolist()\n\nif __name__ == '__main__':\n    # Example usage:\n    input_array = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    alpha_val = 1.0\n    gamma_val = 0.0\n    beta_val = 1.0\n\n    output_list = dynamic_tanh(input_array, alpha_val, gamma_val, beta_val)\n    print(f\"Input array: {input_array}\")\n    print(f\"Alpha: {alpha_val}, Gamma: {gamma_val}, Beta: {beta_val}\")\n    print(f\"Dynamic Tanh output: {output_list}\")\n\n    # Another example with different parameters\n    input_array_2 = np.array([0.5, 1.5, -0.5])\n    alpha_val_2 = 2.0\n    gamma_val_2 = 0.5\n    beta_val_2 = 0.8\n\n    output_list_2 = dynamic_tanh(input_array_2, alpha_val_2, gamma_val_2, beta_val_2)\n    print(f\"\\nInput array: {input_array_2}\")\n    print(f\"Alpha: {alpha_val_2}, Gamma: {gamma_val_2}, Beta: {beta_val_2}\")\n    print(f\"Dynamic Tanh output: {output_list_2}\")\n```"}
