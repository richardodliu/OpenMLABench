{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Closed-form Ridge (L2-regularised) linear regression.\n    \n    Parameters\n    ----------\n    data  : np.ndarray\n        2-D array where the last column is the target vector y and all\n        preceding columns are the feature matrix X.\n    alpha : float, optional (default=0.0)\n        Non-negative regularisation strength.\n    \n    Returns\n    -------\n    list[float]\n        Weight vector (including the bias term as the first element),\n        rounded to 4 decimal places.\n        If the regularised normal-equation matrix is singular, returns -1.\n    \"\"\"\n    if data.ndim != 2 or data.shape[0] == 0:\n        raise ValueError(\"`data` must be a non-empty 2-D array.\")\n    if alpha < 0:\n        raise ValueError(\"`alpha` must be non-negative.\")\n\n    # Split features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n\n    # Add bias (intercept) column of ones\n    ones = np.ones((X.shape[0], 1))\n    X_bias = np.hstack((ones, X))\n\n    # Closed-form matrices\n    XtX = X_bias.T @ X_bias\n    I = np.eye(X_bias.shape[1])\n    A = XtX + alpha * I\n\n    # Check invertibility\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0.0):\n        return -1\n\n    # Solve for weights\n    w = np.linalg.inv(A) @ X_bias.T @ y\n\n    # Round and convert to Python list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of parameter updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        (w, b) where w is a list and every value of w as well as b is\n        rounded to 4 decimals.\n    \"\"\"\n    # Ensure the label vector is one-dimensional float array\n    y = y_train.astype(float).ravel()\n    X = X_train.astype(float)\n\n    n_samples, n_features = X.shape\n\n    # 1. initialise \u03b1 and bias b\n    alpha = np.zeros(n_samples, dtype=float)\n    b = 0.0\n\n    # Pre-compute Gram matrix  K(i,j)=x_i\u00b7x_j\n    gram = X @ X.T       # shape : (n_samples, n_samples)\n\n    updates = 0          # how many updates have been performed\n    i = 0                # current sample index during the scan\n\n    # 2-5. main learning loop\n    while i < n_samples and updates < n_iter:\n        # 3. activation for current sample i\n        activation = np.sum(alpha * y * gram[:, i])     # \u03a3_j \u03b1_j y_j K(x_j,x_i)\n        margin = y[i] * (activation + b)\n\n        if margin <= 0:         # 4. mis-classified -> update\n            alpha[i] += eta\n            b += eta * y[i]\n            updates += 1\n            i = 0               # restart scan from the beginning\n        else:\n            i += 1              # correct -> continue with next sample\n\n    # 6. compute primal weight vector  w = \u03a3_i \u03b1_i y_i x_i\n    w = (alpha * y) @ X        # shape: (n_features,)\n\n    # Rounding as requested\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n\n    return w_rounded, b_rounded\n```\n"}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef glm_links():\n   Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        Dictionary with entries for the three most common GLM links\n        ('identity', 'log', 'logit').  Each entry itself is a dictionary with\n        three callables:\n            link      \u2013  g(\u03bc)\n            inv_link  \u2013  g\u207b\u00b9(\u03b7)\n            deriv     \u2013  g\u2032(\u03bc)      (derivative w.r.t. \u03bc)\n\n        Every callable works element-wise on numpy scalars as well as on\n        1-D/2-D numpy arrays.\n       # ---------- identity -------------------------------------------------\n    def id_link(mu):\n        return mu\n\n    def id_inv_link(eta):\n        return eta\n\n    def id_deriv(mu):\n        # Same shape as mu, filled with ones\n        return np.ones_like(mu, dtype=float)\n\n    # ---------- log ------------------------------------------------------\n    def log_link(mu):\n        return np.log(mu)\n\n    def log_inv_link(eta):\n        return np.exp(eta)\n\n    def log_deriv(mu):\n        return 1.0 / mu\n\n    # ---------- logit ----------------------------------------------------\n    def logit_link(mu):\n        return np.log(mu / (1.0 - mu))\n\n    def logit_inv_link(eta):\n        # numerically stable logistic / expit\n        eta = np.asarray(eta, dtype=float)\n        out = np.empty_like(eta)\n        # positive part\n        pos = eta >= 0\n        out[pos] = 1.0 / (1.0 + np.exp(-eta[pos]))\n        # negative part\n        neg = ~pos\n        exp_eta = np.exp(eta[neg])\n        out[neg] = exp_eta / (1.0 + exp_eta)\n        return out\n\n    def logit_deriv(mu):\n        return 1.0 / (mu * (1.0 - mu))\n\n    # ---------- collect & return ----------------------------------------\n    return {\n       identity {\n           link id_link,\n           inv_link id_inv_link,\n           deriv id_deriv,\n        },\n       log {\n           link log_link,\n           inv_link log_inv_link,\n           deriv log_deriv,\n        },\n       logit {\n           link logit_link,\n           inv_link logit_inv_link,\n           deriv logit_deriv,\n        },\n    }\n\n\n# ----------------- usage example (rounded to 4 decimals) -----------------\nif __name__ ==main    links = glm_links()\n\n    mu = np.array([0.2, 0.8])\n    eta = linkslogitlinkmu)\n    mu2 = linkslogitinv_linketa)\n\n    printeta np.round(eta, 4))   # [-1.3863  1.3863]\n    printmu2 np.round(mu2, 4))   # [0.2     0.8   ]\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter   # (not really needed, kept because it is in the stub)\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Return, for every sample in *data*, the depth of the leaf it reaches in a\n    single random isolation tree grown according to the specification.\n    \"\"\"\n    # ------------------------------------------------------------------ helpers\n    def grow(node_indices: np.ndarray, depth: int) -> None:\n        \"\"\"Recursively grow the tree and store depths for the samples that\n        become part of a leaf.\n        \"\"\"\n        n = len(node_indices)\n\n        # 1. stopping rule \u2500 fewer than three samples\n        if n < 3:\n            depths[node_indices] = depth\n            return\n\n        # 2. pick a random feature\n        f = rng.randint(0, n_features)\n\n        # 3. determine splitting interval on that feature\n        col_values = data[node_indices, f]\n        down, up = col_values.min(), col_values.max()\n\n        #    if there is no variability on the chosen feature,\n        #    this node cannot be split => make it a leaf\n        if down == up:\n            depths[node_indices] = depth\n            return\n\n        # 4. draw a random split value\n        v = rng.uniform(down, up)\n\n        # 5. send samples left / right\n        left_mask = col_values <= v\n        left_indices  = node_indices[left_mask]\n        right_indices = node_indices[~left_mask]\n\n        #    If one side is empty the split is useless -> make this node a leaf\n        if len(left_indices) == 0 or len(right_indices) == 0:\n            depths[node_indices] = depth\n            return\n\n        # 6. recurse\n        grow(left_indices,  depth + 1)\n        grow(right_indices, depth + 1)\n\n    # ------------------------------------------------------------------ main\n    # make sure the input is truly 2-D\n    if data.ndim != 2:\n        raise ValueError(\"data must be a 2-D NumPy array\")\n\n    # reproducibility\n    np.random.seed(0)\n    rng = np.random\n\n    n_samples, n_features = data.shape\n    depths = np.empty(n_samples, dtype=int)\n\n    all_indices = np.arange(n_samples)\n    grow(all_indices, depth=0)\n\n    return depths.tolist()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Return the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of lists where the element [i][j] is the squared\n        Euclidean distance between samples i and j, rounded to\n        4 decimal places.  If `X` is empty, an empty list is returned.\n    \"\"\"\n    # Handle the empty-input corner case\n    if X.size == 0 or X.shape[0] == 0:\n        return []\n\n    # Make sure we work with a floating type to avoid integer overflow\n    X = X.astype(np.float64, copy=False)\n\n    # Squared L2 norms of each row: shape -> (n_samples, 1)\n    sq_norms = np.sum(X ** 2, axis=1, keepdims=True)\n\n    # Pair-wise squared distances using \u2016x\u2212y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u1d40y\n    D = sq_norms + sq_norms.T - 2 * X @ X.T\n\n    # Numerical stability: clamp tiny negative values to zero\n    np.maximum(D, 0, out=D)\n\n    # Ensure exact zeros on the diagonal\n    np.fill_diagonal(D, 0.0)\n\n    # Round to 4 decimal places and convert to Python lists\n    return np.round(D, 4).tolist()\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef xgboost_predict(predictions: list, learning_rate: float) -> list[int]:\n   Aggregate XGBoost tree outputs and produce final class predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        A list where each element is a 2-D array-like object of shape\n        (n_samples, n_classes) containing the raw outputs produced by one tree.\n    learning_rate : float\n        The learning rate used by the booster.  This value scales every tree\n        output before it is added to the ensemble score.\n\n    Returns\n    -------\n    list[int]\n        A list of length *n_samples* with the predicted class label for every\n        sample.\n       # 1. Sum the individual tree outputs.\n    #    Convert every element to a NumPy array to guarantee correct arithmetic.\n    stacked = np.stack([np.asarray(tree) for tree in predictions], axis=0)\n    total_raw = stacked.sum(axis=0)             # shape: (n_samples, n_classes)\n\n    # 2. Build the ensemble\u2019s raw score according to XGBoost\u2019s convention.\n    raw_score = -learning_rate * total_raw      # shape: (n_samples, n_classes)\n\n    # 3. Row-wise soft-max (numerically stable).\n    max_per_row = raw_score.max(axis=1, keepdims=True)\n    exp_shifted = np.exp(raw_score - max_per_row)\n    softmax_probs = exp_shifted / exp_shifted.sum(axis=1, keepdims=True)\n\n    # 4. Predicted class = argmax of probabilities.\n    predicted_labels = softmax_probs.argmax(axis=1)\n\n    # Return as a plain Python list.\n    return predicted_labels.tolist()\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable, Set\n\n\ndef _canon(itemset: Set[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"\n    Return the itemset as a tuple with its elements *deterministically*\n    ordered.  We resort to `repr` as key so that heterogeneous types\n    (str, int, \u2026) can still be sorted without raising TypeError.\n    \"\"\"\n    return tuple(sorted(itemset, key=repr))\n\n\ndef find_frequent_itemsets(\n    transactions: List[List[Hashable]], minsup: float\n) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent item-sets together with their absolute support.\n\n    Parameters\n    ----------\n    transactions : list of transactions (each itself a list of hashables)\n    minsup       : minimum support given as a fraction in (0, 1]\n\n    Returns\n    -------\n    dict[tuple, int]   Mapping  (sorted itemset)  ->  support\n                       Empty dict if no itemset is frequent.\n    \"\"\"\n    # --- basic checks -------------------------------------------------------\n    if not 0 < minsup <= 1:\n        raise ValueError(\"minsup must be in the interval (0, 1].\")\n\n    n_transactions = len(transactions)\n    if n_transactions == 0:                       # trivial corner case\n        return {}\n\n    min_support = math.ceil(minsup * n_transactions)\n\n    # ------------------------------------------------------------------------\n    # 1. Pre-process: convert every transaction to a *set* to get rid of\n    #    duplicates and make \u2286 tests O(1).\n    # ------------------------------------------------------------------------\n    t_sets = [set(t) for t in transactions]\n\n    # ------------------------------------------------------------------------\n    # 2. Frequent 1-itemsets\n    # ------------------------------------------------------------------------\n    item_counter: Counter = Counter()\n    for t in t_sets:\n        item_counter.update(t)\n\n    result: Dict[Tuple[Hashable, ...], int] = {}\n    prev_level: List[Tuple[Hashable, ...]] = []\n\n    for item, sup in item_counter.items():\n        if sup >= min_support:\n            tpl = (item,)\n            result[tpl] = sup\n            prev_level.append(tpl)\n\n    # ------------------------------------------------------------------------\n    # 3. Apriori iteration  (k  = size of the itemsets currently generated)\n    # ------------------------------------------------------------------------\n    k = 2\n    prev_level.sort()  # deterministic order\n\n    while prev_level:\n        prev_level_set = set(prev_level)     # for O(1) membership look-ups\n\n        # ---- 3a. Candidate generation (join step + prune) ------------------\n        candidates: Set[Tuple[Hashable, ...]] = set()\n        for i in range(len(prev_level)):\n            for j in range(i + 1, len(prev_level)):\n                a, b = prev_level[i], prev_level[j]\n\n                # only join if their first k-2 items are identical\n                if a[: k - 2] == b[: k - 2]:\n                    cand_items = set(a) | set(b)\n                    if len(cand_items) != k:\n                        continue\n\n                    cand_tpl = _canon(cand_items)\n\n                    # prune: every (k\u22121) subset must already be frequent\n                    if all(\n                        _canon(sub) in prev_level_set\n                        for sub in combinations(cand_tpl, k - 1)\n                    ):\n                        candidates.add(cand_tpl)\n\n        if not candidates:\n            break\n\n        # ---- 3b. Support counting -----------------------------------------\n        support_counter = {cand: 0 for cand in candidates}\n        cand_sets       = {cand: set(cand) for cand in candidates}\n\n        for t in t_sets:\n            for cand_tpl, cand_set in cand_sets.items():\n                if cand_set.issubset(t):\n                    support_counter[cand_tpl] += 1\n\n        # ---- 3c. Keep the frequent ones, advance to next level ------------\n        prev_level = []\n        for cand_tpl, sup in support_counter.items():\n            if sup >= min_support:\n                result[cand_tpl] = sup\n                prev_level.append(cand_tpl)\n\n        prev_level.sort()\n        k += 1\n\n    return result\n```\n\nExplanation of the algorithm\n1. Transactions are turned into sets so that duplicates vanish and subset checks (`\u2286`) are fast.  \n2. All 1-itemsets are counted. Those reaching the minimum support form the first level of frequent itemsets.  \n3. From level `k\u22121` we build level `k` with the classic Apriori join\u2013prune strategy:  \n   \u2022 Two (k\u22121)-itemsets are joined if they share their first `k\u22122` items.  \n   \u2022 A candidate is kept only if **all** its (k\u22121) subsets are already known to be frequent.  \n4. Supports of the candidates are counted by a single scan of the data.  \n5. Steps 3\u20134 repeat until no new frequent itemsets are found.  \n6. The function finally returns every frequent itemset as an ascendingly sorted tuple mapped to its absolute support count; if none exists the empty dictionary is returned."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        n_iterations: int = 4000) -> list[int]:\n    \"\"\"Train a binary Logistic Regression model via batch GD and predict labels.\n\n    Args:\n        X_train: Training feature matrix of shape (n_samples, n_features).\n        y_train: Binary training labels of shape (n_samples,).\n        X_test:  Feature matrix to classify, shape (m_samples, n_features).\n        learning_rate: Step size used by gradient descent.\n        n_iterations:  Number of gradient-descent iterations.\n\n    Returns:\n        List with the predicted class (0 or 1) for every row of X_test.\n    \"\"\"\n\n    # Ensure inputs are numpy arrays and labels are 1-dimensional\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=float).ravel()\n    X_test  = np.asarray(X_test , dtype=float)\n\n    n_samples, n_features = X_train.shape\n\n    # Initialise weights and bias to zero\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n\n    # Sigmoid helper\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        # Numerically stable sigmoid\n        # For large negative z, exp(-z) can overflow,\n        # so we clip values to avoid warnings.\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # -------- Training (batch gradient descent) --------\n    for _ in range(n_iterations):\n        z = X_train @ w + b            # shape: (n_samples,)\n        p = _sigmoid(z)                # predicted probabilities\n        error = p - y_train            # shape: (n_samples,)\n\n        # Gradients\n        dw = (X_train.T @ error) / n_samples      # shape: (n_features,)\n        db = error.mean()                         # scalar\n\n        # Parameter update\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n    # -------- Prediction --------\n    probs = _sigmoid(X_test @ w + b)\n    preds = np.rint(probs).astype(int)    # round to 0 or 1\n\n    # Return a plain Python list of ints\n    return preds.tolist()\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef smooth(prev, cur, weight):\n   Apply first-order exponential smoothing.\n\n    Parameters\n    ----------\n    prev, cur, weight : float | list | tuple | numpy.ndarray\n        Scalars or one-dimensional array-likes.  `weight` (\u03b5) must satisfy\n        0 \u2264 weight \u2264 1.\n\n    Returns\n    -------\n    float or list[float]\n        Smoothed value(s) rounded to four decimal places.  A scalar is returned\n        if *all* three inputs are scalars; otherwise a Python list is returned.\n       # Convert to NumPy arrays (scalars become 0-D arrays)\n    prev_arr   = np.asarray(prev,   dtype=float)\n    cur_arr    = np.asarray(cur,    dtype=float)\n    weight_arr = np.asarray(weight, dtype=float)\n\n    # Ensure inputs are at most 1-D\n    for name, arr in (('prev', prev_arr), ('cur', cur_arr), ('weight', weight_arr)):\n        if arr.ndim > 1:\n            raise ValueError(fname} must be a scalar or 1-D array    # Validate weight range\n    if np.any((weight_arr < 0) | (weight_arr > 1)):\n        raise ValueErrorweight must lie in the closed interval [0, 1].    # Broadcast to common shape\n    try:\n        prev_b, cur_b, weight_b = np.broadcast_arrays(prev_arr, cur_arr, weight_arr)\n    except ValueError as exc:\n        raise ValueErrorInputs could not be broadcast to a common shape from exc\n\n    # Exponential smoothing\n    result = weight_b * prev_b + (1.0 - weight_b) * cur_b\n\n    # Round to four decimal places\n    result = np.round(result, 4)\n\n    # Decide on return type\n    if np.isscalar(prev) and np.isscalar(cur) and np.isscalar(weight):\n        return float(result)        # single scalar\n    else:\n        return result.tolist()      # list for any vector input\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray, k: int,\n           epsilon: float = 1e-3,\n           max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n   Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of wanted clusters (1 \u2264 k \u2264 n_samples).\n    epsilon : float, optional\n        Convergence threshold on the maximum centre displacement.\n    max_iter : int, optional\n        Hard iteration cap.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        Rounded centres and the label of every sample.\n       # ---- basic checks -------------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError`data` must be a 2-D array (n_samples \u00d7 n_features).    n_samples, n_features = data.shape\n    if not (1 <= k <= n_samples):\n        raise ValueError`k` must satisfy 1 \u2264 k \u2264 number of samples    # ---- deterministic initialisation: first k samples ---------------------\n    centres = data[:k].astype(float).copy()          # (k \u00d7 n_features)\n    labels = np.zeros(n_samples, dtype=int)          # placeholder\n\n    # ---- main loop ----------------------------------------------------------\n    for _ in range(max_iter):\n\n        # 1) assign every sample to its nearest centre\n        #    squared Euclidean distance is enough \u2013 we avoid the costly sqrt\n        diff = data[:, None, :] - centres[None, :, :]        # (n \u00d7 k \u00d7 d)\n        sq_dists = np.einsum('nkd,nkd->nk', diff, diff)      # (n \u00d7 k)\n        labels = np.argmin(sq_dists, axis=1)                 # (n,)\n\n        # 2) recompute new centres\n        new_centres = np.empty_like(centres)\n        for ci in range(k):\n            members = data[labels == ci]\n            if members.size == 0:                            # empty cluster\n                rand_idx = np.random.randint(0, n_samples)\n                new_centres[ci] = data[rand_idx]\n            else:\n                new_centres[ci] = members.mean(axis=0)\n\n        # 3) check for convergence\n        shifts = np.linalg.norm(new_centres - centres, axis=1)  # (k,)\n        if np.max(shifts) < epsilon:\n            centres = new_centres\n            break\n\n        centres = new_centres\n\n    # ---- final formatting ---------------------------------------------------\n    centres_rounded = np.round(centres, 4).tolist()\n    return centres_rounded, labels.tolist()\n```"}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef power_spectrum(frames: np.ndarray, scale: bool = False) -> list[list[float]]:\n   Compute the power spectrum for every frame of a real-valued signal.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray, shape (M, N)\n        M frames with N real-valued samples each.\n    scale : bool, default False\n        If True, normalise the power spectrum by the number of returned\n        DFT bins (N//2 + 1).\n\n    Returns\n    -------\n    list[list[float]]\n        Power spectrum (positive-frequency part) for every frame,\n        rounded to 4 decimal places and returned as a regular Python\n        list of lists.\n       # Number of samples per frame and number of FFT bins returned by rfft\n    N = frames.shape[1]\n    n_bins = N // 2 + 1\n\n    # Real FFT along the frame axis (axis=1) and power spectrum\n    power = np.abs(np.fft.rfft(frames, axis=1)) ** 2\n\n    # Optional scaling\n    if scale:\n        power /= n_bins\n\n    # Round to 4 decimals and convert to plain Python list\n    return np.round(power, 4).tolist()\n```"}
{"task_id": 13, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import defaultdict     # still here because it was in the stub\n\n\ndef _sample_std(x: np.ndarray) -> float:\n   Sample standard deviation; returns 0 for |x| < 2    if x.size < 2:\n        return 0.0\n    return float(np.std(x, ddof=1))\n\n\ndef _cosine_centered(v1: np.ndarray, v2: np.ndarray) -> float:\n   Cosine similarity after the *optional* mean\u2013centring step    std1, std2 = _sample_std(v1), _sample_std(v2)\n\n    if std1 > 1e-3:            # mean\u2013centre only if the sample \u03c3 is large enough\n        v1 = v1 - v1.mean()\n    if std2 > 1e-3:\n        v2 = v2 - v2.mean()\n\n    denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n    if denom == 0.0:\n        return 0.0\n    return float(np.dot(v1, v2) / denom)\n\n\ndef _pearson(v1: np.ndarray, v2: np.ndarray) -> float:\n   Sample Pearson correlation (returns 0 when undefined).    if v1.size < 2:                 # correlation not defined for < 2 points\n        return 0.0\n    std1, std2 = _sample_std(v1), _sample_std(v2)\n    if std1 < 1e-3 or std2 < 1e-3:  # at least one vector is (almost) constant\n        return 0.0\n    return float(np.corrcoef(v1, v2, ddof=1)[0, 1])\n\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n   Item\u2013based k-NN collaborative filtering recommender    # ------------------------------------------------------------------\n    # 0. Simple parameter checks (kept minimal \u2013 the focus is the algo)\n    # ------------------------------------------------------------------\n    if criterion not in ('cosine', 'pearson'):\n        raise ValueErrorcriterion must be 'cosine' or 'pearson'    n_user, n_item = data.shape\n\n    # ------------------------------------------------------------------\n    # 1. Build item\u2013item similarity matrix S\n    # ------------------------------------------------------------------\n    S = np.zeros((n_item, n_item), dtype=float)\n\n    # for every item remember the users that rated it (> 0)\n    users_per_item = [np.where(data[:, j] > 0)[0] for j in range(n_item)]\n\n    # pair\u2013wise similarities (only for i < j, then copy to (j, i))\n    for i in range(n_item - 1):\n        ui = users_per_item[i]\n        if ui.size == 0:                         # no user rated item i\n            continue\n        for j in range(i + 1, n_item):\n            uj = users_per_item[j]\n            common = np.intersect1d(ui, uj, assume_unique=True)\n\n            if common.size == 0:                 # empty intersection\n                sim = 0.0\n            else:\n                v1 = data[common, i].astype(float)\n                v2 = data[common, j].astype(float)\n\n                if criterion == 'cosine':\n                    sim = _cosine_centered(v1, v2)\n                else:                            # 'pearson'\n                    sim = _pearson(v1, v2)\n\n            S[i, j] = S[j, i] = sim\n\n    # ------------------------------------------------------------------\n    # 2. Collect indices and ratings of the items the user has rated\n    # ------------------------------------------------------------------\n    user_ratings = data[user_ind]\n    rated_idx = np.where(user_ratings > 0)[0]\n\n    # if the user has already rated everything, nothing to recommend\n    if rated_idx.size == n_item:\n        return []\n\n    rated_scores = user_ratings[rated_idx]       # vector r\u1d62 in the formula\n\n    # ------------------------------------------------------------------\n    # 3. Predict attractiveness for every *unrated* item\n    # ------------------------------------------------------------------\n    scores = {}\n    for t in np.where(user_ratings == 0)[0]:     # loop over still unseen items\n        sims = S[t, rated_idx]                   # similarities S[t, i]\n\n        denom = np.sum(np.abs(sims))\n        if denom == 0.0:                         # undefined \u2192 score 0\n            scores[t] = 0.0\n        else:\n            num = float(np.dot(rated_scores, sims))\n            scores[t] = num / denom\n\n    # ------------------------------------------------------------------\n    # 4. Return the indices of the best `k` items (ties \u2192 lower index wins)\n    # ------------------------------------------------------------------\n    sorted_items = sorted(scores, key=lambda idx: (-scores[idx], idx))\n    return sorted_items[:k]\n"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_metrics(y: np.ndarray, y_pred: np.ndarray) -> tuple[float, list[float], list[float]]:\n   Compute binary logistic loss together with its gradient and Hessian.\n\n    Args:\n        y (np.ndarray): Binary ground-truth labels (0 or 1) of shape (n,).\n        y_pred (np.ndarray): Predicted logits of shape (n,).\n\n    Returns:\n        tuple: (mean_loss, gradient, hessian) where\n            mean_loss (float): Mean cross-entropy loss (rounded to 4 decimals).\n            gradient (list[float]): First derivative per sample (rounded to 4 decimals).\n            hessian (list[float]): Diagonal of the Hessian per sample (rounded to 4 decimals).\n       # =======  Write your code below  =======\n    y      = y.ravel().astype(float)\n    y_pred = y_pred.ravel().astype(float)\n\n    eps = 1e-15\n    # Sigmoid probabilities\n    p = 1.0 / (1.0 + np.exp(-y_pred))\n    \n    # Loss (with clipping for numerical stability)\n    p_clip = np.clip(p, eps, 1.0 - eps)\n    losses = -(y * np.log(p_clip) + (1.0 - y) * np.log(1.0 - p_clip))\n    mean_loss = float(np.round(np.mean(losses), 4))\n    \n    # Gradient and Hessian diagonal\n    grad = p - y               # \u2202\u2113/\u2202\u0177\n    hess = p * (1.0 - p)       # \u2202\u00b2\u2113/\u2202\u0177\u00b2\n    \n    # Rounding\n    grad_out = [float(np.round(g, 4)) for g in grad]\n    hess_out = [float(np.round(h, 4)) for h in hess]\n    \n    return mean_loss, grad_out, hess_out\n    # =======  End of your code  =======\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nfrom typing import Any, Sequence, List, Tuple\n\ndef ngrams(sequence: Sequence[Any], N: int) -> List[Tuple[Any, ...]]:\n    \"\"\"Return all contiguous N-grams of *sequence*.\n\n    Args:\n        sequence: An ordered, sliceable container (e.g., list, tuple, string).\n        N: Length of each n-gram (must be an integer \u2265 1).\n\n    Returns:\n        A list of tuples, each containing exactly *N* consecutive elements\n        from *sequence*.  If *N* is larger than *len(sequence)* the function\n        returns an empty list.\n    \"\"\"\n    seq_len = len(sequence)\n\n    # If N is longer than the sequence, no n-grams can be formed.\n    if N > seq_len:\n        return []\n\n    # Build the list of n-grams with a sliding window.\n    return [tuple(sequence[i : i + N]) for i in range(seq_len - N + 1)]\n```\n\nExample usage:\n```python\n>>> ngrams([1, 2, 3, 4], 2)\n[(1, 2), (2, 3), (3, 4)]\n\n>>> ngrams(\"hello\", 3)\n[('h', 'e', 'l'), ('e', 'l', 'l'), ('l', 'l', 'o')]\n\n>>> ngrams((10, 20), 5)\n[]\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\ndef build_tree(items: list[str], counts: list[int], parents: list[int]) -> dict:\n    \"\"\"Construct a rooted tree from parallel *items*, *counts* and *parents* lists.\n\n    Parameters\n    ----------\n    items : list[str]\n        The label (item) stored in each node.\n    counts : list[int]\n        The frequency (count) stored in each node.\n    parents : list[int]\n        The parent index of each node; \u20131 marks the single root.\n\n    Returns\n    -------\n    dict\n        The root node represented as a nested dictionary with keys\n        'item', 'count' and 'children'.  If the inputs are empty, returns\n        an empty dictionary.\n    \"\"\"\n    # Empty input -> empty tree\n    if not items:            # covers the three lists (they are parallel)\n        return {}\n\n    n = len(items)           # number of nodes (lists are assumed to be the same length)\n\n    # Step 1: build a flat list of node dictionaries (no edges yet)\n    nodes = [\n        {'item': items[i], 'count': counts[i], 'children': []}\n        for i in range(n)\n    ]\n\n    root = None  # will hold the root dictionary\n\n    # Step 2: connect children to their parent nodes\n    for i in range(n):\n        p = parents[i]\n        if p == -1:\n            # This is the root node\n            root = nodes[i]\n        else:\n            # Append the current node to its parent's children list.\n            # Iterating in index order preserves the desired sibling order.\n            nodes[p]['children'].append(nodes[i])\n\n    # At this point 'root' references the fully-linked tree\n    return root if root is not None else {}\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef zero(shape, **kwargs):\n    \"\"\"Return an ndarray of zeros with a specified shape.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired array shape. If an int is provided, a 1-D array of that\n        length is created. If a tuple/list is provided, it specifies the\n        dimensions of the resulting array.\n    **kwargs\n        dtype : data-type, optional\n            Desired NumPy dtype of the result (default is float).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array filled with zeros matching the requested shape and dtype.\n\n    Raises\n    ------\n    TypeError\n        If unsupported keyword arguments are supplied.\n    \"\"\"\n    # Accept only the 'dtype' keyword.\n    dtype = kwargs.pop('dtype', float) if 'dtype' in kwargs else float\n    if kwargs:                        # any remaining kwargs are unexpected\n        unexpected = \", \".join(kwargs)\n        raise TypeError(f\"zero() got unexpected keyword argument(s): {unexpected}\")\n\n    return np.zeros(shape, dtype=dtype)\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None`` (or if fewer than two samples could be put\n    on either side of any split).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* with feature values.\n    g : np.ndarray\n        1-D array with first-order gradients of length *n_samples*.\n    h : np.ndarray\n        1-D array with second-order gradients (Hessians) of length *n_samples*.\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to leaf denominators.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        (feature_index, threshold) that yields the largest positive gain, or\n        ``None`` when no valid split improves the objective.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # A legal split must leave \u22652 samples on each side\n    if n_samples < 4:\n        return None\n\n    # Statistics for the whole node (parent)\n    G_parent = g.sum(dtype=float)\n    H_parent = h.sum(dtype=float)\n    parent_score = G_parent ** 2 / (H_parent + lam)   # used in gain formula\n\n    best_gain   = -np.inf\n    best_feat   = None\n    best_thresh = None\n\n    # ---------------------------------------------------------------------\n    # iterate over every feature and evaluate all unique values as threshold\n    # ---------------------------------------------------------------------\n    for feat_idx in range(n_features):\n        values   = X[:, feat_idx]\n        order    = np.argsort(values)               # sort instances by feature\n        v_sorted = values[order]\n        g_sorted = g[order]\n        h_sorted = h[order]\n\n        G_left = 0.0\n        H_left = 0.0\n        n_left = 0\n\n        # traverse the sorted values once, considering a split *after*\n        # every block of identical values.\n        for i in range(n_samples - 1):  # last position can never be a split\n            G_left += g_sorted[i]\n            H_left += h_sorted[i]\n            n_left += 1\n\n            # Only evaluate after the last occurrence of the current value.\n            if v_sorted[i] == v_sorted[i + 1]:\n                continue\n\n            n_right = n_samples - n_left\n            if n_left < 2 or n_right < 2:           # respect the rule\n                continue\n\n            G_right = G_parent - G_left\n            H_right = H_parent - H_left\n\n            # gain = 0.5*(score_left + score_right - score_parent) - gamma\n            gain = 0.5 * (\n                (G_left  ** 2) / (H_left  + lam) +\n                (G_right ** 2) / (H_right + lam) -\n                parent_score\n            ) - gamma\n\n            if gain > best_gain and gain > 0.0:     # keep strictly positive\n                best_gain   = gain\n                best_feat   = feat_idx\n                best_thresh = v_sorted[i]            # threshold = unique value\n\n    return (best_feat, best_thresh) if best_gain > 0 else None\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        (sigmoid_x, gradient_x) rounded to 4 decimals\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # -------------------- helper: numpy view of x ----------------------------\n    arr = np.asarray(x)        # works for scalar, list, or ndarray\n\n    # -------------------- sigmoid and gradient -------------------------------\n    sigmoid_vals = 1.0 / (1.0 + np.exp(-arr))\n    grad_vals    = sigmoid_vals * (1.0 - sigmoid_vals)\n\n    # -------------------- rounding -------------------------------------------\n    sigmoid_vals = np.round(sigmoid_vals, 4)\n    grad_vals    = np.round(grad_vals,    4)\n\n    # -------------------- packaging the output -------------------------------\n    # Scalar case  (np.asarray(scalar).ndim == 0)\n    if np.isscalar(x) or (isinstance(x, np.ndarray) and x.ndim == 0):\n        return float(sigmoid_vals), float(grad_vals)\n\n    # Array / list case  -> convert to Python lists\n    return sigmoid_vals.tolist(), grad_vals.tolist()\n```\n\nExample usages\n```python\n>>> sigmoid_activation(0)\n(0.5, 0.25)\n\n>>> sigmoid_activation([0, 2])\n([0.5, 0.8808], [0.25, 0.1049])\n\n>>> a = np.array([[-1, 0, 1],\n...               [ 2, 3, 4]])\n>>> sigmoid_activation(a)\n([ [0.2689, 0.5, 0.7311],\n  [0.8808, 0.9526, 0.982] ],\n [ [0.1966, 0.25, 0.1966],\n  [0.1049, 0.0452, 0.0177] ])\n```\nAll outputs are rounded to 4 decimal places and returned as floats for scalars or Python lists for array-like inputs, exactly as required."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom collections import defaultdict          # (kept only because required by the stub)\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"AdaBoost using one\u2013dimensional decision stumps (from scratch).\"\"\"\n    # ---------- preparation --------------------------------------------------\n    x_tr = np.asarray(x_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=int)\n    x_te = np.asarray(x_test,  dtype=float)\n\n    n = len(x_tr)\n    if n == 0:\n        return [1] * len(x_te)                       # nothing to learn\n\n    # initial uniform weights\n    w = np.full(n, 1.0 / n)\n\n    # pre\u2013compute all possible thresholds\n    uniq = np.unique(x_tr)\n    thresholds = [uniq[0] - 1.0]                     # sentinel left\n    thresholds += [(uniq[i] + uniq[i + 1]) / 2.0     # mid-points\n                   for i in range(len(uniq) - 1)]\n    thresholds.append(uniq[-1] + 1.0)                # sentinel right\n\n    # store the selected weak learners\n    stumps: list[tuple[int, float]] = []             # (direction, \u03b8)\n    alphas: list[float] = []\n    agg_tr = np.zeros(n)                             # running \u03a3 \u03b1 h(x)\n\n    # ------------------------------------------------------------------------\n    while True:\n        best_err = float('inf')\n        best_pred = None\n        best_stump = (0, thresholds[0])\n\n        # enumerate every stump\n        for theta in thresholds:\n            # direction 0  : +1 if x \u2264 \u03b8 else \u22121\n            pred0 = np.where(x_tr <= theta, 1, -1)\n            err0  = w[pred0 != y_tr].sum()\n            if err0 < best_err:\n                best_err, best_pred, best_stump = err0, pred0, (0, theta)\n\n            # direction 1  : +1 if x > \u03b8 else \u22121\n            pred1 = np.where(x_tr > theta, 1, -1)\n            err1  = w[pred1 != y_tr].sum()\n            if err1 < best_err:\n                best_err, best_pred, best_stump = err1, pred1, (1, theta)\n\n        # if even the best stump is no better than random, stop\n        if best_err >= 0.5 - 1e-12:\n            break\n\n        # \u03b1 for the chosen stump (protect against err = 0)\n        if best_err == 0.0:\n            alpha = 0.5 * math.log((1 - 1e-10) / 1e-10)\n        else:\n            alpha = 0.5 * math.log((1 - best_err) / best_err)\n\n        # keep the stump\n        stumps.append(best_stump)\n        alphas.append(alpha)\n\n        # update sample weights:  w_i \u2190 w_i \u00b7 exp(\u2212\u03b1 y_i h_i(x_i))\n        w *= np.exp(-alpha * y_tr * best_pred)\n        w /= w.sum()                                 # normalise\n\n        # update aggregated prediction on the training data\n        agg_tr += alpha * best_pred\n        train_pred = np.where(agg_tr >= 0.0, 1, -1)\n        train_err  = (train_pred != y_tr).mean()\n\n        if train_err <= epsilon or best_err == 0.0:\n            break                                    # desired accuracy reached\n\n        # (optional) safety cap on the number of rounds\n        if len(stumps) >= n * 2:                     # far more than enough\n            break\n\n    # ---------------------- prediction on the test set ----------------------\n    if not stumps:                                   # no weak learner selected\n        majority = 1 if (y_tr == 1).sum() >= (y_tr == -1).sum() else -1\n        return [majority] * len(x_te)\n\n    agg_te = np.zeros(len(x_te))\n    for (direction, theta), alpha in zip(stumps, alphas):\n        if direction == 0:\n            h = np.where(x_te <= theta, 1, -1)\n        else:\n            h = np.where(x_te > theta, 1, -1)\n        agg_te += alpha * h\n\n    y_pred = np.where(agg_te >= 0.0, 1, -1)\n    return y_pred.tolist()\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_tensor(shape: tuple[int, ...], standardize: bool = False) -> np.ndarray:\n    \"\"\"Generate a random real-valued tensor and optionally standardize it.\n\n    Each element is obtained as  offset + u  where\n        \u2022 offset \u223c Uniform{\u2212300, \u2026, 299}                (discrete, integers)\n        \u2022 u      \u223c Uniform[0, 1)                        (continuous)\n\n    If `standardize` is True, every *column* (i.e. the second dimension of a\n    2-D array) is independently centred to mean 0 and scaled to standard\n    deviation 1.  To avoid division by zero an \u03b5 = np.finfo(float).eps term is\n    added to the divisor.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired output shape.  For standardisation the function expects\n        `shape` to be at least two-dimensional; all leading dimensions are\n        treated as samples and the last dimension as features.\n    standardize : bool, default False\n        Whether to perform column-wise standardisation.\n\n    Returns\n    -------\n    np.ndarray\n        The generated (and possibly standardised) tensor.\n    \"\"\"\n    # draw integer offsets in [\u2212300, 299]\n    offsets = np.random.randint(-300, 300, size=shape, dtype=np.int64)\n    # draw uniform[0,1) noise\n    noise = np.random.random(size=shape)\n    x = offsets + noise             # resulting float array\n\n    if standardize:\n        eps = np.finfo(float).eps\n\n        if x.ndim == 1:     # 1-D: treat whole vector as a single feature\n            mean = x.mean()\n            std = x.std()\n            x = (x - mean) / (std + eps)\n        else:\n            # regard the last axis as \u201cfeature / column\u201d axis\n            mean = x.mean(axis=0, keepdims=True)\n            std = x.std(axis=0, keepdims=True)\n            x = (x - mean) / (std + eps)\n\n    return x\n```\n\nThe function fulfils all requirements:\n\n\u2022 Each element is an integer offset in [\u2212300, 299] plus a uniform[0, 1) value.  \n\u2022 `standardize=True` rescales every column so its mean is 0 and its standard\n  deviation is 1 up to numerical precision (\u03b5 added to avoid division by zero).  \n\u2022 The result is returned as a NumPy `ndarray`."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nesterov_update(w, velocity, grad_func, learning_rate=0.001, momentum=0.9):\n    \"\"\"Perform one Nesterov Accelerated Gradient (NAG) update.\"\"\"\n    \n    # Convert inputs to numpy arrays for vectorised maths\n    w = np.asarray(w, dtype=float)\n    \n    # If velocity is empty -> initialise as zeros shaped like w\n    if len(velocity) == 0:\n        v = np.zeros_like(w, dtype=float)\n    else:\n        v = np.asarray(velocity, dtype=float)\n    \n    # 1. Look-ahead parameters\n    w_tilde = w - momentum * v\n    \n    # 2. Gradient at look-ahead point with element-wise clipping to [-1, 1]\n    grad = np.clip(grad_func(w_tilde), -1.0, 1.0)\n    \n    # 3. Velocity update\n    v_new = momentum * v + learning_rate * grad\n    \n    # 4. Parameter (weight) update \u2013 gradient-descent direction\n    w_new = w - v_new\n    \n    # Round to 4 decimal places and convert back to ordinary Python lists\n    w_new_rounded = np.round(w_new, 4).tolist()\n    v_new_rounded = np.round(v_new, 4).tolist()\n    \n    return (w_new_rounded, v_new_rounded)\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        (n, d) matrix with the training samples.\n    y_train : numpy.ndarray\n        Length-n vector with training labels (each is 1 or \u22121).\n    alpha : numpy.ndarray\n        Length-n vector with the Lagrange multipliers.\n    b : float\n        Bias (intercept) term.\n    gamma : float\n        Positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        (m, d) matrix with the samples to classify.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u22121) for all m test samples.\n    \"\"\"\n    # Pre-compute the product \u03b1_i y_i  (shape: n)\n    coef = alpha * y_train                        # (n,)\n\n    # ---------- Gaussian kernel between all test and train samples ----------\n    # Squared \u21132 norms of the rows\n    test_sq = np.sum(X_test ** 2, axis=1)[:, None]    # (m, 1)\n    train_sq = np.sum(X_train ** 2, axis=1)[None, :]  # (1, n)\n\n    # Pair-wise squared distances \u2016x_i \u2212 z\u2016\u00b2  (broadcasting does the job)\n    sq_dists = test_sq + train_sq - 2.0 * X_test @ X_train.T  # (m, n)\n\n    # Apply the Gaussian kernel\n    K = np.exp(-gamma * sq_dists)                   # (m, n)\n\n    # ---------------- decision values ----------------\n    g = K @ coef + b                                # (m,)\n\n    # ---------------- predictions -------------------\n    preds = np.where(g >= 0.0, 1, -1)               # (m,)\n    return preds.astype(int).tolist()\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(x: list[list[int | float]] | list[int | float]) -> tuple[list, list]:\n   Compute the element-wise ReLU activation and its gradient mask.\n\n    Parameters\n    ----------\n    x : list | nested list\n        Arbitrary-shaped (possibly nested) Python list of numbers.\n\n    Returns\n    -------\n    (activation, gradient) : tuple[list, list]\n        `activation` \u2013 ReLU(x) applied element-wise,  \n        `gradient`   \u2013 element-wise derivative mask (1 where x \u2265 0, else 0),\n        both returned as Python lists with exactly the same shape as `x`.\n       # Convert the (nested) list to a NumPy array for vectorised operations\n    arr = np.asarray(x)\n\n    # ReLU activation: max(0, x)\n    act = np.maximum(0, arr)\n\n    # Gradient mask: 1 where x >= 0, else 0\n    grad = np.where(arr >= 0, 1, 0)\n\n    # Convert back to (nested) Python lists\n    return act.tolist(), grad.tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float] | int:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean-squared reconstruction error, both rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Convert to ndarray (float) and basic validation\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2:\n        return -1\n    m, n = X_arr.shape\n\n    # Validate k\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # Full SVD and truncation to rank-k\n    U, S, Vt = np.linalg.svd(X_arr, full_matrices=False)\n    U_k = U[:, :k]                  # (m \u00d7 k)\n    S_k = S[:k]                     # (k,)\n    Vt_k = Vt[:k, :]                # (k \u00d7 n)\n\n    # Optimal rank-k reconstruction\n    X_hat = U_k @ (S_k[:, None] * Vt_k)   # Faster than U_k @ np.diag(S_k) @ Vt_k\n\n    # Mean-squared reconstruction error\n    mse = np.mean((X_arr - X_hat) ** 2)\n\n    # Round outputs to four decimals\n    X_hat_rounded = np.round(X_hat, 4).tolist()\n    mse_rounded = float(np.round(mse, 4))\n\n    return X_hat_rounded, mse_rounded\n```"}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a 1-D gradient\u2013boosting model with decision stumps and predict on *x_query*.\n    \n    The function follows the algorithm described in the task \u2013 it keeps adding the\n    stump (one split, constant on each side) that most reduces the residual sum of\n    squares (RSS) until the RSS drops below *epsilon*.\n    \"\"\"\n    \n    # ---- preparation --------------------------------------------------------\n    x = np.asarray(x_train, dtype=float).ravel()\n    y = np.asarray(y_train, dtype=float).ravel()\n    if x.size == 0:\n        raise ValueError(\"Training set must contain at least one sample.\")\n    if x.size != y.size:\n        raise ValueError(\"x_train and y_train must have the same length.\")\n    \n    # order the data by the single feature\n    order          = np.argsort(x)\n    x_sorted       = x[order]\n    y_sorted       = y[order]\n    n              = x_sorted.size\n    \n    # if every x is identical, the best we can do is the global mean\n    if np.allclose(x_sorted, x_sorted[0]):\n        return round(float(np.mean(y_sorted)), 4)\n    \n    # candidate split positions: mid-points between *different* consecutive x's\n    cand_indices   = [i for i in range(n - 1) if x_sorted[i] != x_sorted[i + 1]]\n    splits         = (x_sorted[cand_indices] + x_sorted[np.array(cand_indices) + 1]) / 2.0\n    \n    # storage for learnt stumps: list of tuples (split, c1, c2)\n    stumps: list[tuple[float, float, float]] = []\n    \n    # current residuals (initial prediction is 0 everywhere)\n    residuals = y_sorted.copy()\n    rss       = np.sum(residuals ** 2)\n    \n    max_iters = 1000                       # hard safety guard\n    it        = 0\n    while rss > epsilon and it < max_iters:\n        # prefix sums needed for O(#splits) search of the best stump\n        prefix_sum     = np.cumsum(residuals)\n        prefix_sum_sq  = np.cumsum(residuals ** 2)\n        total_sum      = prefix_sum[-1]\n        total_sum_sq   = prefix_sum_sq[-1]\n        \n        best_rss  = np.inf\n        best_c1   = best_c2 = best_split = None\n        best_idx  = -1\n        \n        for k, idx in enumerate(cand_indices):     # idx is the last index of the left part\n            n_left   = idx + 1\n            n_right  = n - n_left\n            if n_left == 0 or n_right == 0:\n                continue                            # should not happen, but stay safe\n            # sums on each side\n            sum_left   = prefix_sum[idx]\n            sum_right  = total_sum - sum_left\n            # means (stump constants)\n            c1 = sum_left  / n_left\n            c2 = sum_right / n_right\n            # RSS on each side:  \u03a3r^2 \u2212 (\u03a3r)^2 / n\n            rss_left  = prefix_sum_sq[idx]                 - (sum_left  ** 2) / n_left\n            rss_right = (total_sum_sq - prefix_sum_sq[idx]) - (sum_right ** 2) / n_right\n            candidate_rss = rss_left + rss_right\n            \n            if candidate_rss < best_rss:\n                best_rss   = candidate_rss\n                best_split = splits[k]\n                best_c1, best_c2 = c1, c2\n                best_idx   = idx\n        \n        # add the best stump found\n        stumps.append( (best_split, best_c1, best_c2) )\n        \n        # update residuals: r \u2190 r \u2212 f_best\n        # (because everything is sorted, we can use slicing instead of boolean masks)\n        residuals[:best_idx + 1]  -= best_c1\n        residuals[best_idx + 1:]  -= best_c2\n        \n        # new RSS\n        rss = np.sum(residuals ** 2)\n        it += 1\n    \n    # ---- prediction for the query point -------------------------------------\n    pred = 0.0\n    for s, c1, c2 in stumps:\n        pred += c1 if x_query <= s else c2\n    \n    return round(float(pred), 4)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\ndef check_data(a, b):\n   Validate and convert the two inputs so that both are numeric.\n\n    Accepts ints, floats, or numeric strings.  Any string is converted with\n    `float()`, while existing numeric values are left unchanged.  The cleaned\n    values are returned as a tuple (a, b) in the original order.\n       def _coerce(value):\n        # Leave native numeric types untouched\n        if isinstance(value, (int, float)):\n            return value\n        # Convert strings that represent numbers\n        if isinstance(value, str):\n            return float(value)\n        # Let float() raise the appropriate error for unsupported types\n        return float(value)\n\n    return _coerce(a), _coerce(b)\n\n\n# Do NOT change this decorator \u2013 it is required by the grader.\ndef validate_input(function):\n    def wrapper(a, b):\n        a, b = check_data(a, b)\n        return function(a, b)\n\n    return wrapper\n\n\n@validate_input\ndef sum_numbers(a, b):\n   Return the arithmetic sum of *a* and *b* after validation.\n\n    The result is returned as an int when it represents a whole number;\n    otherwise it is returned as a float.\n       result = a + b\n    # Convert to int if the float has no fractional part\n    if isinstance(result, float) and result.is_integer():\n        return int(result)\n    return result\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"\n    Generate a NumPy tensor whose entries are 0.0 or 1.0.\n\n    Parameters\n    ----------\n    shape : tuple[int, ...]\n        Desired shape of the output array.\n    sparsity : float, optional\n        Probability that any given entry is 1.0.\n        Must lie in the closed interval [0, 1].  Default is 0.5.\n    seed : int | None, optional\n        Seed for NumPy\u2019s random number generator.  If provided, the result\n        is deterministic.  Default is None (non-deterministic).\n\n    Returns\n    -------\n    np.ndarray\n        A NumPy array of floating-point zeros and ones when `sparsity`\n        is valid, otherwise the integer -1.\n\n    Notes\n    -----\n    The function obeys the following rules required by the task statement:\n    1. Validates that `sparsity` is in [0, 1]; returns -1 if not.\n    2. Calls `np.random.seed(seed)` when `seed` is supplied.\n    3. Each entry is 1.0 with probability equal to `sparsity`\n       and 0.0 otherwise.\n    4. The output dtype is a floating type (np.float64 by default).\n    \"\"\"\n    # Rule 1: validate sparsity\n    if not 0.0 <= sparsity <= 1.0:\n        return -1\n\n    # Rule 3: deterministic behaviour when a seed is supplied\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Generate uniform random numbers in [0, 1)\n    random_values = np.random.rand(*shape)\n\n    # Map to {0.0, 1.0} using the sparsity threshold\n    tensor = (random_values < sparsity).astype(float)\n\n    return tensor\n```\n"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nfrom typing import Any\n\n\nclass node:  # noqa: N801 \u2013 keep the original name used by the problem description\n    \"\"\"A minimal tree node for decision-tree-like structures.\n\n    Attributes\n    ----------\n    fea : int\n        Index of the feature to split on.  A value of -1 marks a leaf node.\n    res : Any | None\n        Label stored in the leaf.  `None` for internal nodes.\n    child : dict[int, node]\n        Mapping from feature value to the corresponding sub-tree.\n    \"\"\"\n\n    def __init__(\n        self,\n        fea: int = -1,\n        res: Any | None = None,\n        child: dict[Any, \"node\"] | None = None,\n    ) -> None:\n        self.fea = fea\n        self.res = res\n        self.child = child or {}\n\n\ndef classify(root: node, sample: list[Any]) -> Any:  # noqa: ANN401 \u2013 keep signature simple\n    \"\"\"Traverse a decision-tree represented by *node* objects and return a label.\n\n    Parameters\n    ----------\n    root : node\n        Root of the decision tree.\n    sample : list[Any]\n        A single data point expressed as an ordered list of feature values.\n\n    Returns\n    -------\n    Any\n        \u2022 The label stored in the leaf reached by following the path dictated\n          by *sample*.\n        \u2022 -1 if the traversal cannot proceed at any step (missing feature index\n          or unseen feature value).\n    \"\"\"\n    if root is None:  # Defensive guard; not required by specification.\n        return -1\n\n    current = root\n    while current.fea != -1:  # while **not** a leaf\n        # 1. Check that the required feature index exists in the sample\n        if current.fea >= len(sample) or current.fea < 0:\n            return -1\n\n        value = sample[current.fea]\n\n        # 2. Follow the edge corresponding to that value\n        if value not in current.child:\n            return -1\n\n        current = current.child[value]\n\n        # In malformed trees `current` could be None or not a `node`; guard against it\n        if not isinstance(current, node):\n            return -1\n\n    # 3. `current` is a leaf \u21d2 return its stored result\n    return current.res\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nfrom typing import List, Optional\n\nclass Node:\n    \"\"\"Binary decision-tree node (provided).\n\n    Non-leaf node:   rule = (feature_index, threshold)\n    Leaf node:       rule = (None, prediction_value)\n    \"\"\"\n\n    def __init__(self, left: Optional['Node'], right: Optional['Node'], rule: tuple):\n        self.left = left\n        self.right = right\n        self.feature = rule[0]\n        self.threshold = rule[1]\n\n\ndef predict_tree(root: Optional[Node], X: List[List[float]]) -> List[float]:\n    \"\"\"\n    Evaluate a binary decision tree on a batch of samples.\n\n    Parameters\n    ----------\n    root : Node or None\n        Root node of the decision tree.  If None, the tree is empty.\n    X : list of list of float\n        Input samples.  X[i][j] is the j-th feature of the i-th sample.\n\n    Returns\n    -------\n    list of float\n        Predictions in the same order as the input samples.\n    \"\"\"\n    # Empty tree \u2192 no predictions\n    if root is None:\n        return []\n\n    predictions: List[float] = []\n\n    for sample in X:\n        node = root\n        # Walk down the tree until a leaf is reached\n        while not (node.left is None and node.right is None):   # while not a leaf\n            # Decide which child to follow based on the split rule\n            if sample[node.feature] <= node.threshold:\n                node = node.left\n            else:\n                node = node.right\n        # At a leaf: its `threshold` stores the prediction value\n        predictions.append(node.threshold)\n\n    return predictions\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import cycle\n\ndef build_kd_tree(data: np.ndarray) -> dict | None:\n    \"\"\"Build a kd-tree out of *data* (see task description).\"\"\"\n    # ------------------------------------------------------------------\n    # safety checks\n    # ------------------------------------------------------------------\n    if data is None or data.size == 0:\n        return None                     # empty input \u2192 empty tree\n\n    n_samples, n_cols = data.shape\n    m = n_cols - 1                      # number of spatial dimensions\n\n    # ------------------------------------------------------------------\n    # global (shared) splitting-dimension generator\n    # ------------------------------------------------------------------\n    splitter = cycle(range(m))          # 0, 1, \u2026, m-1, 0, 1, \u2026\n\n    # ------------------------------------------------------------------\n    # recursive builder\n    # ------------------------------------------------------------------\n    def _build(sub: np.ndarray) -> dict | None:\n        if sub.shape[0] == 0:           # empty subset \u2192 leaf = None\n            return None\n\n        d = next(splitter)              # next global splitting dimension\n\n        # sort by the chosen dimension and pick the median row\n        idx_sorted   = sub[:, d].argsort(kind=\"mergesort\")   # stable\n        sorted_sub   = sub[idx_sorted]\n        median_index = len(sorted_sub) // 2\n        median_row   = sorted_sub[median_index]\n\n        # prepare current node\n        node = {\n            \"point\": median_row[:m].tolist(),\n            \"label\": median_row[m].item()  # convert to plain Python type\n        }\n\n        # recursively build sub-trees\n        node[\"left\"]  = _build(sorted_sub[:median_index])\n        node[\"right\"] = _build(sorted_sub[median_index + 1:])\n\n        return node\n\n    # ------------------------------------------------------------------\n    # kick-off\n    # ------------------------------------------------------------------\n    return _build(data)\n```\n\nHow it works\n1. A single `cycle(range(m))` object (`splitter`) is created once and then shared by every recursive call.  \n   Each node consumes exactly one `next(splitter)` so the dimension sequence is truly global.\n2. For the selected dimension `d` the subset is stably sorted (`mergesort`) and the median row is extracted.\n3. Points that lie *before* the median form the `left` subset, those *after* form the `right` subset; recursion continues on both.\n4. An empty subset immediately returns `None`, turning the recursion into a leaf.  \n5. An original empty input (`size == 0`) returns `None` straight away."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linucb_select_arm(context: np.ndarray,\n                      A: list[list[list[float]]],\n                      b: list[list[float]],\n                      alpha: float) -> int:\n   Select an arm according to the LinUCB rule (disjoint model).    _, n_arms = context.shape\n    scores = np.empty(n_arms, dtype=float)\n\n    for a in range(n_arms):\n        Aa = np.asarray(A[a], dtype=float)          # (D, D)\n        ba = np.asarray(b[a], dtype=float)          # (D,)\n        ca = context[:, a]                          # (D,)\n\n        # \u03b8\u0302_a = A_a\u207b\u00b9 b_a  (solve instead of explicit inverse)\n        theta_hat = np.linalg.solve(Aa, ba)\n\n        # s = A_a\u207b\u00b9 c_a       ->  c_a\u1d40 A_a\u207b\u00b9 c_a = c_a\u1d40 s\n        s = np.linalg.solve(Aa, ca)\n\n        exploit = ca @ theta_hat\n        explore = alpha * np.sqrt(ca @ s)\n\n        scores[a] = exploit + explore\n\n    return int(np.argmax(scores))\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n          \u2022 pivot is the row index whose value in column *d* is the median\n            element (k = n//2);\n          \u2022 left  is a list of row indices with strictly smaller values;\n          \u2022 right is a list of row indices with strictly larger  values.\n        Both lists are returned in increasing order.\n    \"\"\"\n    # --- sanity checks ------------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError(\"data must be a 2-D array\")\n    n_samples, n_features = data.shape\n    if not (0 <= d < n_features):\n        raise ValueError(\"column index d is out of bounds\")\n\n    col = data[:, d]\n    k = n_samples // 2                     # position of the median element\n\n    # --- O(n) median selection via argpartition -----------------------------\n    # After argpartition, the element at index *k* is the same as if the whole\n    # array was sorted (but the remaining elements are only partially ordered).\n    part_idx = np.argpartition(col, k)\n    pivot = int(part_idx[k])              # row index of the median sample\n    median_val = col[pivot]\n\n    # --- gather indices -----------------------------------------------------\n    # np.flatnonzero returns the indices in ascending order \u2192 deterministic.\n    left  = np.flatnonzero(col < median_val).tolist()\n    right = np.flatnonzero(col > median_val).tolist()\n\n    return pivot, left, right\n```\n\nExplanation of the key steps\n1. `np.argpartition(col, k)` re-orders the indices such that the element that\n   would be at position `k` in a fully-sorted array is placed at position `k`\n   \u2013 in O(n) time.\n2. That index is taken as the pivot.  \n3. Boolean comparisons (`<` / `>`) followed by `np.flatnonzero` supply all\n   indices strictly left and right of the pivot value; the returned arrays are\n   already sorted, so the output is deterministic without extra work."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames (view, no copy).\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One\u2013dimensional input signal of length *N*.\n    frame_width : int\n        Length (in samples) of each frame.\n    stride : int\n        Hop size \u2013 distance (in samples) between starts of consecutive frames.\n    writeable : bool, default=False\n        When False the returned array is read-only.  When True it is writeable\n        iff the source array *x* was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        View with shape (n_frames, frame_width) where\n        n_frames = (len(x) - frame_width) // stride + 1\n    \"\"\"\n    # ------------------ sanity checks ------------------\n    assert x.ndim == 1,          \"x must be one-dimensional\"\n    assert stride >= 1,          \"stride must be a positive integer\"\n    assert frame_width >= 1,     \"frame_width must be a positive integer\"\n    assert len(x) >= frame_width, \"frame_width larger than signal\"\n\n    # ----------------- number of frames ----------------\n    n_frames = (len(x) - frame_width) // stride + 1\n\n    # --------------- build the strided view ------------\n    item_stride = x.strides[0]                           # byte step between neighbouring samples\n    frame_stride = stride * item_stride                  # byte step between neighbouring frames\n    view = as_strided(\n        x,\n        shape=(n_frames, frame_width),\n        strides=(frame_stride, item_stride)\n    )\n\n    # --------------- handle writability ---------------\n    if not writeable:\n        # Force read-only view\n        view = view.view()            # make sure we own the flags object\n        view.flags.writeable = False\n    # else: inherit writability from the source array (default behaviour)\n\n    return view\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(X_data: list[list[float]], y_data: list, \n                X: list[list[float]], k: int = 5) -> list:\n    \"\"\"Predict labels for the query samples *X* using the k-Nearest Neighbours rule.\n\n    Parameters\n    ----------\n    X_data : list[list[float]]\n        Training data \u2013 each inner list is a feature vector.\n    y_data : list\n        Labels corresponding to *X_data*.\n    X : list[list[float]]\n        Query samples to classify.\n    k : int, optional\n        Number of neighbours to look at (default is 5).\n\n    Returns\n    -------\n    list\n        Predicted labels for each query sample in *X*.\n    \"\"\"\n    if not X_data:\n        raise ValueError(\"Training data must not be empty.\")\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n\n    # Convert data to NumPy arrays for fast vectorised distance computation\n    X_train = np.asarray(X_data, dtype=float)\n    X_query = np.asarray(X,      dtype=float)\n\n    n_train = len(X_train)\n    k_eff   = min(k, n_train)          # use all samples if k > n_train\n\n    predictions = []\n    for q in X_query:\n        # Squared Euclidean distances (no square-root needed for ranking)\n        diff   = X_train - q\n        dist2  = np.einsum('ij,ij->i', diff, diff)\n\n        # Indices of k smallest distances\n        idx_k  = np.argpartition(dist2, k_eff - 1)[:k_eff]\n        neigh_labels = (y_data[i] for i in idx_k)\n\n        # Majority vote with deterministic tie-breaking\n        counts     = Counter(neigh_labels)\n        max_count  = max(counts.values())\n        winners    = [lbl for lbl, c in counts.items() if c == max_count]\n        predictions.append(min(winners))     # lexicographically smallest winner\n\n    return predictions\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsle(actual: list[float], predicted: list[float]) -> float:\n   Compute the Root Mean Squared Logarithmic Error (RMSLE).\n\n    Parameters\n    ----------\n    actual : list[float]\n        Non-negative ground-truth values.\n    predicted : list[float]\n        Non-negative values predicted by a model.\n\n    Returns\n    -------\n    float\n        The RMSLE rounded to 4 decimal places, or -1 if the input is invalid.\n       # -------- Input validation --------\n    # Both must be lists/sequence of the same, non-zero length\n    if not isinstance(actual, (list, tuple)) or not isinstance(predicted, (list, tuple)):\n        return -1\n    if len(actual) == 0 or len(actual) != len(predicted):\n        return -1\n\n    # Convert to NumPy arrays for vectorised computations\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except (ValueError, TypeError):\n        # Conversion to float failed (e.g., strings present)\n        return -1\n\n    # All values must be \u2265 0\n    if np.any(a < 0) or np.any(p < 0):\n        return -1\n\n    # -------- RMSLE computation --------\n    # log1p(x) == ln(x + 1)\n    log_diff = np.log1p(p) - np.log1p(a)\n    rmsle_value = np.sqrt(np.mean(log_diff ** 2))\n\n    # -------- Return rounded value --------\n    return float(round(rmsle_value, 4))\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef tanh_activation(x, derivative: bool = False) -> list[float]:\n    \"\"\"Compute tanh or its derivative element-wise without using np.tanh.\n\n    Args:\n        x: A 1-D list or NumPy array of numbers.\n        derivative: If True, return the gradient of tanh; otherwise return tanh.\n\n    Returns:\n        A Python list with every element rounded to 4 decimals.\n    \"\"\"\n    # Convert input to a NumPy 1-D array\n    x_arr = np.asarray(x, dtype=float)\n\n    # ----- forward tanh -----\n    # tanh(x) = 2 / (1 + e^{-2x}) - 1\n    tanh_x = 2.0 / (1.0 + np.exp(-2.0 * x_arr)) - 1.0\n\n    if derivative:\n        # d/dx tanh(x) = 1 - tanh(x)^2\n        out = 1.0 - tanh_x ** 2\n    else:\n        out = tanh_x\n\n    # Round to 4 decimal places and convert back to list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef autocorrelate_1d(x: list | np.ndarray) -> list:\n   Compute the non-negative-lag autocorrelation of a 1-D real signal.\n\n    Parameters\n    ----------\n    x : list | np.ndarray\n        A one-dimensional sequence of numeric (int/float) values of length N.\n\n    Returns\n    -------\n    list\n        A list [a_0, a_1, \u2026, a_{N-1}] where\n\n            a_k = \u03a3_{n=0}^{N-k-1} x_{n+k} \u00b7 x_n,   k = 0 \u2026 N-1\n\n        For N == 0 an empty list is returned.\n\n    Notes\n    -----\n    \u2022 O(N\u00b2) direct computation is used as required.\n    \u2022 No call is made to np.correlate, scipy, or FFT-based routines.\n       # Convert input to a 1-D NumPy array for convenience/consistency\n    arr = np.asarray(x)\n\n    # Basic validation: must be 1-D\n    if arr.ndim != 1:\n        raise ValueErrorInput must be a 1-D sequence    N = arr.size\n    if N == 0:                       # Corner case\n        return []\n\n    # Allocate result list\n    ac = [0.0] * N\n\n    # Direct double-sum formulation (O(N\u00b2))\n    for k in range(N):\n        s = 0.0\n        # Sum over n = 0 \u2026 N-k-1\n        for n in range(N - k):\n            s += arr[n] * arr[n + k]\n        ac[k] = s\n\n    return ac\n```\n\nExample\n--------\n```python\n>>> import numpy as np\n>>> x = np.array([1, 2, 3])\n>>> autocorrelate_1d(x)\n[14.0, 8.0, 3.0]\n```\n\nExplanation of the result (matches the task description):\na\u2080 = 1\u00b71 + 2\u00b72 + 3\u00b73 = 14  \na\u2081 = 2\u00b71 + 3\u00b72 = 8  \na\u2082 = 3\u00b71 = 3"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] |np.ndarray -> bool:\n   Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every entry is a probability\n    (0 \u2264 p \u2264 1) and each row sums to 1 (within a small numerical tolerance).\n\n    Parameters\n    ----------\n    X : list of lists | numpy.ndarray\n        Matrix to be tested.\n\n    Returns\n    -------\n    bool\n        True  \u2013 X satisfies the stochastic conditions;\n        False \u2013 any requirement is violated.\n       # 1. Convert to a numeric NumPy array, fail quietly on error\n    try:\n        A = np.asarray(X, dtype=float)\n    except Exception:          # non-numeric entries, ragged lists, etc.\n        return False\n\n    # 2. Must be a genuine 2-D array with at least one row and one column\n    if A.ndim != 2 or min(A.shape) == 0:\n        return False\n\n    # 3. Every element has to lie inside the closed interval [0, 1]\n    if not np.all((A >= 0) & (A <= 1)):\n        return False\n\n    # 4. Each row must sum to 1 within numerical tolerance\n    if not np.allclose(A.sum(axis=1), 1.0, atol=1e-8, rtol=0.0):\n        return False\n\n    return True\n```"}
{"task_id": 49, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\n\ndef _move_up(heap: List[Number], idx: int) -> None:\n    \"\"\"Bubble the item at *idx* up until the max-heap property is restored.\"\"\"\n    while idx:\n        parent = (idx - 1) // 2\n        if heap[idx] > heap[parent]:\n            heap[idx], heap[parent] = heap[parent], heap[idx]\n            idx = parent\n        else:\n            break\n\n\ndef _move_down(heap: List[Number], idx: int) -> None:\n    \"\"\"Push the item at *idx* down until the max-heap property is restored.\"\"\"\n    n = len(heap)\n    while True:\n        left = 2 * idx + 1\n        right = left + 1\n        largest = idx\n\n        if left < n and heap[left] > heap[largest]:\n            largest = left\n        if right < n and heap[right] > heap[largest]:\n            largest = right\n\n        if largest != idx:\n            heap[idx], heap[largest] = heap[largest], heap[idx]\n            idx = largest\n        else:\n            break\n\n\ndef k_smallest(nums: List[Number], k: int) -> List[Number]:\n    \"\"\"Return the k smallest numbers contained in *nums*.\n\n    The function scans *nums* once, maintaining a manual max-heap of\n    size at most *k*.  When the scan finishes, the heap contains exactly\n    the k smallest elements (in arbitrary order); they are then sorted\n    and returned.  If *k* is invalid (\u22640 or > len(nums)), return -1.\n    \"\"\"\n    if k <= 0 or k > len(nums):\n        return -1\n\n    heap: List[Number] = []        # our manual max-heap\n\n    for value in nums:\n        if len(heap) < k:\n            # Still room: push and bubble up.\n            heap.append(value)\n            _move_up(heap, len(heap) - 1)\n        elif value < heap[0]:\n            # New value is smaller than current largest \u2192 replace root.\n            heap[0] = value\n            _move_down(heap, 0)\n        # else: value is larger than the k-th smallest so far \u2192 ignore.\n\n    return sorted(heap)\n```\n\nExplanation of the approach\n1. Validate *k*; return \u20131 on illegal values.\n2. Iterate through the list once.\n   \u2022 If the heap hasn\u2019t reached size *k*, push the new element and restore the max-heap property via `_move_up`.  \n   \u2022 Otherwise compare against the root (current largest among stored items).  \n     \u2013 If the new value is smaller, replace the root and restore the heap via `_move_down`.  \n     \u2013 If it is not smaller, discard it.\n3. After the single pass, the heap holds the *k* smallest elements, albeit unordered. Sort it once and return.\n\nTime complexity: O(n log k)  \nSpace complexity: O(k)"}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef newton(X: np.ndarray, y: np.ndarray, epsilon: float = 1e-6, max_iter: int = 1000) -> list[list[float]]:\n   Newton\u2019s method for 2-D linear least-squares.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Design matrix of shape (n, 2)\n    y : np.ndarray\n        Target vector of shape (n,) or (n, 1)\n    epsilon : float\n        Stopping threshold on the gradient 2-norm\n    max_iter : int\n        Maximum number of Newton iterations\n    \n    Returns\n    -------\n    list[list[float]]\n        2\u00d71 list of lists with weights rounded to 4 decimals\n       # Ensure proper shapes and dtypes\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).reshape(-1)\n\n    n = X.shape[0]                           # number of samples\n    if X.shape[1] != 2:\n        raise ValueErrorX must have exactly 2 columns    w = np.ones(2, dtype=float)              # initial guess  [1, 1]\n\n    for _ in range(max_iter):\n        residual = X @ w - y                 # (n,)\n        grad = (2.0 / n) * (X.T @ residual)  # (2,)\n        if np.linalg.norm(grad) < epsilon:   # stopping criterion\n            break\n\n        hess = (2.0 / n) * (X.T @ X)         # (2, 2)\n        hess_inv = np.linalg.pinv(hess)      # robust inverse (handles singularity)\n\n        w -= hess_inv @ grad                 # Newton update\n\n    # Return as 2\u00d71 list of lists rounded to 4 decimals\n    return [[round(float(w[0]), 4)], [round(float(w[1]), 4)]]\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport string\nfrom typing import List\n\n# A reasonably comprehensive list of English stop-words\n_STOP_WORDS: set[str] = {\n    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an',\n    'and', 'any', 'are',aren't 'as', 'at', 'be', 'because', 'been',\n    'before', 'being', 'below', 'between', 'both', 'but', 'by',can't    'cannot', 'could',couldn't 'did',didn't 'do', 'does',doesn't    'doing',don't 'down', 'during', 'each', 'few', 'for', 'from',\n    'further', 'had',hadn't 'has',hasn't 'have',haven't 'having',\n    'he',he'dhe'llhe's 'her', 'here',here's 'hers',\n    'herself', 'him', 'himself', 'his', 'how',how's 'i',i'di'll   i'mi've 'if', 'in', 'into', 'is',isn't 'it',it's 'its',\n    'itself',let's 'me', 'more', 'most',mustn't 'my', 'myself', 'no',\n    'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought',\n    'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same',shan't    'she',she'dshe'llshe's 'should',shouldn't 'so', 'some',\n    'such', 'than', 'that',that's 'the', 'their', 'theirs', 'them',\n    'themselves', 'then', 'there',there's 'these', 'they',they'd   they'llthey'rethey've 'this', 'those', 'through', 'to', 'too',\n    'under', 'until', 'up', 'very', 'was',wasn't 'we',we'dwe'll   we'rewe've 'were',weren't 'what',what's 'when',when's    'where',where's 'which', 'while', 'who',who's 'whom', 'why',\n   why's 'with',won't 'would',wouldn't 'you',you'dyou'll   you'reyou've 'your', 'yours', 'yourself', 'yourselves'\n}\n\n\ndef tokenize_whitespace(\n    line: str,\n    lowercase: bool = True,\n    filter_stopwords: bool = True,\n    filter_punctuation: bool = True,\n    **kwargs\n) -> List[str]:\n   Tokenize *line* by whitespace with optional lower-casing, punctuation\n    stripping and stop-word removal.\n\n    Args:\n        line: Input text string.\n        lowercase: If True, convert text to lower-case before tokenisation.\n        filter_stopwords: If True, remove common English stop-words.\n        filter_punctuation: If True, strip punctuation from each token.\n        **kwargs: Ignored \u2013 kept for forward compatibility.\n\n    Returns:\n        List of processed tokens in their original order.\n       # Early exit for empty or whitespace-only strings\n    if not line or line.isspace():\n        return []\n\n    # Step 1 \u2013 optional lower-casing of the whole line\n    if lowercase:\n        line = line.lower()\n\n    # Step 2 \u2013 whitespace tokenisation (split on any consecutive whitespace)\n    raw_tokens = line.split()\n\n    # Step 3 \u2013 per-token processing\n    tokens: list[str] = []\n    punct_tbl = string.punctuation\n\n    for tok in raw_tokens:\n        # Optional punctuation stripping (only leading/trailing punctuation)\n        if filter_punctuation:\n            tok = tok.strip(punct_tbl)\n\n        # Discard tokens that became empty after stripping\n        if not tok:\n            continue\n\n        # Optional stop-word filtering (case-insensitive)\n        if filter_stopwords and tok.lower() in _STOP_WORDS:\n            continue\n\n        tokens.append(tok)\n\n    return tokens\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\"\"\"\n    # ensure float64 for numerical stability\n    X = X.astype(np.float64, copy=False)\n    y = y.astype(np.float64, copy=False)\n\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features, dtype=np.float64)   # parameter vector\n    s = np.zeros(n_features, dtype=np.float64)   # running average of squared gradients\n\n    # Convenience for the \u201cfull-gradient\u201d used in the stopping criterion\n    def full_gradient(weights: np.ndarray) -> np.ndarray:\n        return (X.T @ (X @ weights - y)) / n_samples\n\n    # If the whole data set fits in one batch, keep references\n    full_batch = (n_samples <= batch_size)\n\n    for it in range(max_iter):\n        # --------- mini-batch selection (with wrap-around) ----------\n        if full_batch:\n            X_b, y_b = X, y\n        else:\n            start = (it * batch_size) % n_samples\n            end   = start + batch_size\n            if end <= n_samples:\n                X_b = X[start:end]\n                y_b = y[start:end]\n            else:  # wrap around\n                X_b = np.vstack((X[start:], X[: end - n_samples]))\n                y_b = np.concatenate((y[start:], y[: end - n_samples]))\n        m = X_b.shape[0]  # actual batch size\n\n        # ------------------- gradient & update ----------------------\n        g = (X_b.T @ (X_b @ w - y_b)) / m           # batch gradient\n        s = rho * s + (1.0 - rho) * (g ** 2)        # running average\n        w -= eta * g / (np.sqrt(s) + eps_station)   # parameter update\n\n        # ------------------ early stopping test --------------------\n        if np.linalg.norm(full_gradient(w)) < epsilon:\n            break\n\n    # return rounded to 4 decimal places\n    return [round(float(val), 4) for val in w]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"\n    Numerically-stable SoftPlus activation (and its gradient).\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s).\n    deriv : bool, optional\n        If False (default) returns SoftPlus(x);\n        if True returns the element-wise derivative (sigmoid).\n\n    Returns\n    -------\n    float | list\n        Rounded result (4 decimals).  A float is returned for a scalar input,\n        otherwise a Python list is returned.\n    \"\"\"\n    # convert to ndarray for vectorised maths, keep note if scalar\n    arr = np.asarray(x, dtype=float)\n    is_scalar = np.isscalar(x) or arr.ndim == 0\n\n    if deriv:          # -------- sigmoid / gradient --------\n        # numerically-stable sigmoid\n        pos_mask = arr >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(arr)\n\n        # For x >= 0 : 1 / (1 + exp(-x))\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n        # For x  < 0 : exp(x) / (1 + exp(x))\n        exp_x = np.exp(arr[neg_mask])\n        out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    else:              # -------- SoftPlus --------\n        # SoftPlus(x) = log(1 + e^x)\n        # stable formulation\n        pos_mask = arr > 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(arr)\n\n        # For x > 0 : x + log1p(exp(-x))\n        out[pos_mask] = arr[pos_mask] + np.log1p(np.exp(-arr[pos_mask]))\n        # For x <= 0 : log1p(exp(x))\n        out[neg_mask] = np.log1p(np.exp(arr[neg_mask]))\n\n    # round to 4 decimals\n    out = np.round(out, 4)\n\n    # Return according to original input type\n    if is_scalar:\n        return float(out)          # scalar python float\n    return out.tolist()            # python list for vector input\n```\n\nExample usage\n```\n>>> softplus(5)\n5.0067\n>>> softplus([-100, -1, 0, 1, 100])\n[0.0, 0.3133, 0.6931, 1.3133, 100.0]\n>>> softplus([0, 1, 2], deriv=True)\n[0.5, 0.7311, 0.8808]\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n   Return the total log-probability (natural logarithm) of *sequence*\n    under the unsmoothed MLE N-gram model trained on *corpus*.\n\n    The result is rounded to 4 decimal places.  If any required count is\n    zero the function returns ``float('-inf')``.\n       # ---------- Helper ------------------------------------------------------\n    def extract_ngrams(tokens: list[str], n: int):\n       Generate all length-n n-grams as tuples from *tokens*        for i in range(len(tokens) - n + 1):\n            yield tuple(tokens[i:i + n])\n\n    # ---------- Build the required count tables ----------------------------\n    if N == 1:\n        unigram_counts = Counter(corpus)\n        corpus_len = len(corpus)\n    else:\n        ngram_counts     = Counter(extract_ngrams(corpus, N))\n        prefix_counts = Counter(extract_ngrams(corpus, N - 1))\n\n    # ---------- Accumulate log-probability over the sequence ---------------\n    total_log_prob = 0.0\n    seq_len = len(sequence)\n\n    # empty sum (sequence shorter than N) \u21d2 probability 1 \u21d2 log-prob 0\n    if seq_len < N:\n        return 0.0\n\n    for i in range(seq_len - N + 1):\n        ngram = tuple(sequence[i:i + N])\n\n        if N == 1:                             # ------- Unigram case -------\n            count_ngram = unigram_counts.get(ngram[0], 0)\n            if count_ngram == 0 or corpus_len == 0:\n                return float('-inf')\n            prob = count_ngram / corpus_len\n\n        else:                                  # ------- N > 1 --------------\n            count_ngram  = ngram_counts.get(ngram, 0)\n            prefix       = ngram[:-1]\n            count_prefix = prefix_counts.get(prefix, 0)\n\n            if count_ngram == 0 or count_prefix == 0:\n                return float('-inf')\n            prob = count_ngram / count_prefix\n\n        total_log_prob += np.log(prob)\n\n    return round(total_log_prob, 4)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_impurity(y, criterion):\n   Compute a node's impurity.\n\n    Args:\n        y (numpy.ndarray): 1-D array with the target values.\n        criterion (str): One ofentropyginimse.\n\n    Returns:\n        float: Impurity value rounded to 4 decimal places.\n       # Ensure y is a NumPy 1-D array\n    y = np.asarray(y).ravel()\n\n    # Empty node \u2192 zero impurity\n    if y.size == 0:\n        return 0.0\n\n    criterion = criterion.lower()\n    \n    if criterion ==mse        mean = y.mean()\n        impurity = np.mean((y - mean) ** 2)\n\n    elif criterion inentropygini        # Compute class probabilities\n        _, counts = np.unique(y, return_counts=True)\n        p = counts / counts.sum()\n\n        if criterion ==entropy            # Avoid log(0) by masking zero probabilities\n            non_zero = p > 0\n            impurity = -np.sum(p[non_zero] * np.log2(p[non_zero]))\n        else:  #gini            impurity = 1.0 - np.sum(p ** 2)\n\n    else:\n        raise ValueError('criterion must be one ofentropyginimse')\n\n    return round(float(impurity), 4)\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef grad(X, y, W):\n    \"\"\"Return the gradient of the MSE loss for linear regression.\n\n    Parameters\n    ----------\n    X : list[list[float]] | np.ndarray\n        The design matrix with shape (n_samples, n_features).\n    y : list[float] | list[list[float]] | np.ndarray\n        The target vector with shape (n_samples,) or (n_samples, 1).\n    W : list[float] | list[list[float]] | np.ndarray\n        The weight vector with shape (n_features,) or (n_features, 1).\n\n    Returns\n    -------\n    list[list[float]]\n        The gradient vector of shape (n_features, 1), rounded to 4 decimal\n        places, or -1 if the input shapes are incompatible.\n    \"\"\"\n    # Convert all inputs to NumPy arrays\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    W = np.asarray(W, dtype=float)\n\n    # X must be 2-D\n    if X.ndim != 2:\n        return -1\n\n    # Automatically reshape 1-D y and W into column vectors\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    if W.ndim == 1:\n        W = W.reshape(-1, 1)\n\n    # After possible reshaping both y and W must be 2-D column vectors\n    if y.ndim != 2 or y.shape[1] != 1:\n        return -1\n    if W.ndim != 2 or W.shape[1] != 1:\n        return -1\n\n    n_samples, n_features = X.shape\n\n    # Dimensionality checks\n    if y.shape[0] != n_samples:\n        return -1\n    if W.shape[0] != n_features:\n        return -1\n\n    # Gradient: (X\u1d40(XW \u2013 y)) / N   (vectorised computation)\n    gradient = X.T.dot(X.dot(W) - y) / n_samples\n\n    # Round to 4 decimal places and convert to nested list\n    return np.round(gradient, 4).tolist()\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef lasso_regression(\n    X: list[float] | \"np.ndarray\",\n    y: list[float] | \"np.ndarray\",\n    degree: int,\n    reg_factor: float,\n    n_iterations: int = 1000,\n    tol: float = 1e-6,\n) -> list[float]:\n    \"\"\"Univariate Lasso regression with polynomial features (coordinate descent).\"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Convert inputs to 1-D NumPy arrays\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X, dtype=float).ravel()\n    y = np.asarray(y, dtype=float).ravel()\n    m = y.size\n    if X.size != m:\n        raise ValueError(\"X and y must have the same length.\")\n\n    # ------------------------------------------------------------------ #\n    # 2. Build the design matrix  [1, x, x\u00b2, \u2026, x\u1d48]\n    # ------------------------------------------------------------------ #\n    # shape -> (m, degree+1)\n    Z = np.ones((m, degree + 1))\n    if degree:  # avoid unnecessary power when degree == 0\n        powers = np.arange(1, degree + 1)\n        Z[:, 1:] = X[:, None] ** powers  # broadcast powers\n\n    # Special-case degree == 0 (only bias term, no regularisation)\n    if degree == 0:\n        return [round(float(np.mean(y)), 4)]\n\n    # ------------------------------------------------------------------ #\n    # 3. Coordinate-descent initialisation\n    # ------------------------------------------------------------------ #\n    w = np.zeros(degree + 1)  # [w0 \u2026 wd]\n    # Pre-compute \u2016x\u2c7c\u2016\u00b2 for j \u2265 1 (bias column not needed)\n    col_norm2 = np.sum(Z[:, 1:] ** 2, axis=0)\n\n    def soft_threshold(rho: float, lam: float) -> float:\n        \"\"\"Soft-thresholding operator.\"\"\"\n        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n\n    # ------------------------------------------------------------------ #\n    # 4. Main optimisation loop\n    # ------------------------------------------------------------------ #\n    for _ in range(n_iterations):\n        w_old = w.copy()\n\n        # ---- 4.1 Update bias exactly ---------------------------------- #\n        w[0] = float(np.mean(y - Z[:, 1:] @ w[1:]))\n\n        # ---- 4.2 Update the remaining coefficients -------------------- #\n        for j in range(1, degree + 1):\n            x_j = Z[:, j]\n\n            # rho = x\u2c7c\u1d40 (y - (Z\u00b7w) + w\u2c7c x\u2c7c)\n            residual = y - (Z @ w) + w[j] * x_j\n            rho = float(np.dot(x_j, residual))\n\n            if reg_factor == 0.0:\n                # Ordinary least squares update\n                w[j] = rho / col_norm2[j - 1] if col_norm2[j - 1] != 0 else 0.0\n            else:\n                w[j] = soft_threshold(rho, reg_factor) / col_norm2[j - 1]\n\n        # ---- 4.3 Convergence check ------------------------------------ #\n        if np.max(np.abs(w - w_old)) < tol:\n            break\n\n    # ------------------------------------------------------------------ #\n    # 5. Return rounded coefficients\n    # ------------------------------------------------------------------ #\n    return [round(float(coeff), 4) for coeff in w]\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], \n                  B: list[list[float]], \n                  obs: list[int], \n                  t: int) -> list[float]:\n    \"\"\"\n    Compute the backward probability vector \u03b2_t for a discrete Hidden Markov Model.\n\n    Parameters\n    ----------\n    A   : list[list[float]]\n        State-transition matrix of size (N, N) where A[i][j] = P(q_{t+1}=j | q_t=i).\n    B   : list[list[float]]\n        Emission matrix of size (N, M) where B[i][k] = P(o_t = k | q_t = i).\n    obs : list[int]\n        Observation sequence encoded as symbol indices (length = T).\n    t   : int\n        Time index for which the backward vector \u03b2_t is required (0 \u2264 t < T).\n\n    Returns\n    -------\n    list[float]\n        The backward probability vector \u03b2_t (length = N) rounded to 4 decimals.\n    \"\"\"\n    # --- convert inputs to NumPy arrays for vectorised computation ---\n    A = np.asarray(A, dtype=float)          # shape (N, N)\n    B = np.asarray(B, dtype=float)          # shape (N, M)\n    obs = np.asarray(obs, dtype=int)        # shape (T,)\n    \n    T = len(obs)\n    N = A.shape[0]\n\n    # \u03b2_{T-1}(i) = 1  for all i\n    beta = np.ones(N, dtype=float)\n\n    # Propagate backward from time T-2 down to the desired t\n    # Each step: \u03b2_k(i) = \u03a3_j a_{ij} * b_j(o_{k+1}) * \u03b2_{k+1}(j)\n    for k in range(T - 2, t - 1, -1):\n        emission_next = B[:, obs[k + 1]]       # b_j(o_{k+1}) for all j\n        beta = A.dot(emission_next * beta)     # vectorised computation\n\n    # Round to 4 decimal places and convert to ordinary Python list\n    return np.round(beta, 4).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft(frame: np.ndarray, positive_only: bool = True) -> list:\n    \"\"\"Compute the Discrete Fourier Transform (DFT) of a 1-D signal.\n\n    Args:\n        frame: 1-D NumPy array with the signal samples (real or complex).\n        positive_only: If True, keep only the non\u2013negative-frequency\n            coefficients (first N//2+1 terms).\n\n    Returns\n    -------\n        list[complex]: DFT coefficients rounded to 4 decimals.  Length is\n        N//2+1 when ``positive_only`` is True and N otherwise.\n    \"\"\"\n    # --- Preparation ------------------------------------------------------\n    frame = np.asarray(frame, dtype=np.complex128).ravel()   # ensure 1-D, complex\n    N = frame.size\n    if N == 0:                       # empty input -> empty output\n        return []\n\n    # --- Build the N\u00d7N DFT matrix (na\u00efve, O(N\u00b2)) --------------------------\n    n = np.arange(N)                       # sample indices\n    k = n[:, None]                         # column vector of frequency indices\n    W = np.exp(-2j * np.pi * k * n / N)    # DFT kernel  (k,n) grid\n\n    # --- Spectrum ---------------------------------------------------------\n    coeffs = W.dot(frame)                  # length-N spectrum\n\n    if positive_only:                      # keep only non-negative freqs\n        coeffs = coeffs[: N // 2 + 1]\n\n    # --- Rounding ---------------------------------------------------------\n    rounded = []\n    for c in coeffs:\n        r = float(np.round(c.real, 4))\n        i = float(np.round(c.imag, 4))\n        # eliminate signed zeros so repr is cleaner (optional)\n        if r == -0.0:\n            r = 0.0\n        if i == -0.0:\n            i = 0.0\n        rounded.append(complex(r, i))\n\n    return rounded\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], \n                  pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden-Markov-Model backward algorithm.\n\n    Parameters\n    ----------\n    A   : state-to-state transition matrix  (N \u00d7 N)\n    B   : emission matrix                   (N \u00d7 M)\n    pi  : initial state distribution        (N,)\n    obs : observation sequence              (T,)\n\n    Returns\n    -------\n    float\n        P(obs | \u03bb) rounded to 6 decimals.  If any input list is empty, 0.0\n        is returned.\n    \"\"\"\n    # -------- guard clauses -------------------------------------------------\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    # -------- convert the lists to numpy arrays -----------------------------\n    A  = np.asarray(A,  dtype=float)          # (N, N)\n    B  = np.asarray(B,  dtype=float)          # (N, M)\n    pi = np.asarray(pi, dtype=float)          # (N,)\n    obs = np.asarray(obs, dtype=int)          # (T,)\n\n    N        = A.shape[0]                     # number of states\n    T        = len(obs)                       # sequence length\n    beta     = np.zeros((T, N), dtype=float)  # backward variable table\n\n    # -------- initialization -------------------------------------------------\n    beta[-1] = 1.0                            # \u03b2_{T-1}(i) = 1\n\n    # -------- recursion ------------------------------------------------------\n    # \u03b2_t(i) = \u03a3_j  A_ij * B_j,o_{t+1} * \u03b2_{t+1}(j)\n    for t in range(T - 2, -1, -1):\n        beta[t] = A.dot(B[:, obs[t + 1]] * beta[t + 1])\n\n    # -------- termination ----------------------------------------------------\n    prob = np.sum(pi * B[:, obs[0]] * beta[0])\n\n    # -------- result ---------------------------------------------------------\n    return round(float(prob), 6)\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import accumulate          #  (accumulate is not needed, but was required in the stub)\nimport numpy as np\nfrom itertools import accumulate          #  duplicate import \u2013 harmless but keeps the given header\n\nTOL = 1e-8                                # numerical tolerance used everywhere\n\n\ndef _valid_prob_vector(vec, *, tol=TOL) -> bool:\n   Return True iff *vec* is a proper probability vector    if not vec:                                   # empty\n        return False\n    if any((x < 0) or (x > 1) for x in vec):      # negative or >1 entries\n        return False\n    return abs(sum(vec) - 1.0) <= tol\n\n\ndef _sample_from(p: list[float]) -> int:\n       Sample a single index from probability vector *p*.\n    A tiny numerical residue can leave `sum(p) < 1` by ~1e-16,\n    so the final `return len(p)-1` is a safe guard.\n       r = np.random.rand()\n    cum = 0.0\n    for i, prob in enumerate(p):\n        cum += prob\n        if r < cum:\n            return i\n    return len(p) - 1                         # should only be triggered by rounding error\n\n\ndef gen_hmm_data(\n    S: list[float],\n    A: list[list[float]],\n    B: list[list[float]],\n    n_sample: int,\n    seed: int | None = None,\n) -> tuple[list[int], list[int]] | int:\n   Generate a sequence of hidden states and observations from a Hidden Markov Model.\n\n    If any probability distribution is invalid the function immediately returns -1.\n       # Basic size checks --------------------------------------------------------\n    if n_sample < 1:\n        return -1\n    N = len(S)                               # number of hidden states\n    if N == 0:\n        return -1\n\n    # Check shapes of A and B --------------------------------------------------\n    if (len(A) != N) or (len(B) != N):\n        return -1\n    if any(len(row) != N for row in A):      # A must be N\u00d7N\n        return -1\n    M = len(B[0])                            # number of observation symbols\n    if any(len(row) != M for row in B):      # B must be N\u00d7M (rectangular allowed)\n        return -1\n\n    # Validate all probability distributions ----------------------------------\n    if not _valid_prob_vector(S):\n        return -1\n    if any(not _valid_prob_vector(row) for row in A):\n        return -1\n    if any(not _valid_prob_vector(row) for row in B):\n        return -1\n\n    # Optional seeding ---------------------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Sampling -----------------------------------------------------------------\n    states: list[int] = []\n    obs:    list[int] = []\n\n    # initial hidden state & observation\n    current_state = _sample_from(S)\n    states.append(current_state)\n    obs.append(_sample_from(B[current_state]))\n\n    # subsequent steps\n    for _ in range(1, n_sample):\n        current_state = _sample_from(A[current_state])\n        states.append(current_state)\n        obs.append(_sample_from(B[current_state]))\n\n    return states, obs\n```"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sgd_momentum_update(\n    w: np.ndarray,\n    grad: np.ndarray,\n    learning_rate: float = 0.01,\n    momentum: float = 0.0,\n    prev_update: np.ndarray | None = None\n) -> tuple[list, list]:\n   Performs one SGD optimisation step with momentum.\n\n    Args:\n        w: Current parameters (NumPy array).\n        grad: Gradient of the loss with respect to *w*.\n        learning_rate: Learning rate controlling the update magnitude.\n        momentum: Momentum factor in the interval [0, 1].\n        prev_update: Previous momentum update / velocity. If ``None`` a zero\n            tensor of the same shape as *w* is used.\n\n    Returns:\n        A tuple ``(updated_w, new_update)`` where each element is converted to\n        a Python ``list`` and rounded to 4 decimal places.\n       \n    # Initialise previous update (velocity) to zeros if not provided\n    if prev_update is None:\n        prev_update = np.zeros_like(w)\n    else:\n        prev_update = np.asarray(prev_update, dtype=w.dtype)\n\n    # Compute the new momentum update (velocity)\n    new_update = momentum * prev_update + (1.0 - momentum) * grad\n\n    # Update the parameters by moving opposite to the gradient direction\n    updated_w = w - learning_rate * new_update\n\n    # Round to 4 decimal places and convert to plain Python lists\n    updated_w_list = np.round(updated_w, 4).tolist()\n    new_update_list = np.round(new_update, 4).tolist()\n\n    return updated_w_list, new_update_list\n```"}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # ----------------------------------------------------------\n    # 1. Basic dimension / emptiness checks\n    # ----------------------------------------------------------\n    if not observations:                         # empty sequence\n        return -1\n\n    n = len(S)                                  # number of states\n    if n == 0:                                  # no states\n        return -1\n\n    # A must be n\u00d7n\n    if len(A) != n or any(len(row) != n for row in A):\n        return -1\n\n    # B must be n\u00d7m and all rows equally long\n    if len(B) != n:\n        return -1\n    m = len(B[0])                               # number of symbols\n    if m == 0 or any(len(row) != m for row in B):\n        return -1\n\n    # Observation indices must lie in [0, m-1]\n    if any(o < 0 or o >= m for o in observations):\n        return -1\n\n    # ----------------------------------------------------------\n    # 2. Convert to numpy arrays for vectorised maths\n    # ----------------------------------------------------------\n    S = np.asarray(S, dtype=float)              # shape (n,)\n    A = np.asarray(A, dtype=float)              # shape (n, n)\n    B = np.asarray(B, dtype=float)              # shape (n, m)\n\n    # ----------------------------------------------------------\n    # 3. Forward algorithm\n    # ----------------------------------------------------------\n    # \u03b1_0(i) = S_i * B_i(o_0)\n    alpha = S * B[:, observations[0]]\n\n    # \u03b1_t(j) = B_j(o_t) * \u03a3_i \u03b1_{t-1}(i) * A_{i,j}\n    for obs in observations[1:]:\n        alpha = (alpha @ A) * B[:, obs]         # (1\u00d7n) @ (n\u00d7n) \u2192 (n,) then element-wise *\n\n    likelihood = float(alpha.sum())\n\n    # ----------------------------------------------------------\n    # 4. Return rounded result\n    # ----------------------------------------------------------\n    return round(likelihood, 4)\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\nfrom __future__ import annotations\nfrom collections import Counter\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass FPNode:\n   A node in an FP-tree    item: str | None          # `None` for the root\n    parent: 'FPNode' | None   # root\u2019s parent is None\n    count: int = 0            # support of the path that reaches this node\n    link: 'FPNode' | None = None          # next node with the same `item`\n    children: dict[str, 'FPNode'] = None  # child nodes keyed by item name\n\n    def __post_init__(self):\n        if self.children is None:\n            self.children = {}\n\n\n# --------------------------------------------------------------------------- #\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n   Discover every frequent item-set in *transactions* with FP-Growth.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        Each inner list is a transaction that contains the bought items.\n    min_support : int\n        Minimum number of transactions an item-set has to appear in.\n\n    Returns\n    -------\n    list[list[str]]\n        All frequent item-sets.  Items inside an item-set are sorted\n        alphabetically; the outer list is sorted by (len(itemset), itemset).\n       # ------------------------------- helpers -------------------------------- #\n    def build_tree(weighted_transactions: list[tuple[list[str], int]]\n                   ) -> tuple[FPNode | None, dict[str, list]]:\n               Build an FP-tree from *weighted_transactions* and return\n        (root, header_table).  The header table maps an item to\n        [support, first_node_with_that_item].  If no item is frequent,\n        (None, None) is returned.\n               # (1) frequency of single items ------------------------------------- #\n        freq = Counter()\n        for trx, w in weighted_transactions:\n            for itm in trx:\n                freq[itm] += w\n        freq = {itm: sup for itm, sup in freq.items() if sup >= min_support}\n        if not freq:                                     # nothing is frequent\n            return None, None\n\n        # A deterministic, descending-frequency + lexicographic ordering ---- #\n        order = sorted(freq.items(), key=lambda t: (-t[1], t[0]))\n        rank = {itm: i for i, (itm, _) in enumerate(order)}\n\n        # (2) construct the tree ------------------------------------------- #\n        root = FPNode(None, None)\n        header: dict[str, list] = {itm: [sup, None] for itm, sup in freq.items()}\n\n        for trx, w in weighted_transactions:\n            items = [i for i in trx if i in freq]\n            if not items:\n                continue\n            items.sort(key=lambda i: rank[i])            # deterministic order\n            cur = root\n            for itm in items:\n                nxt = cur.children.get(itm)\n                if nxt:\n                    nxt.count += w\n                else:\n                    nxt = FPNode(itm, cur, w)\n                    cur.children[itm] = nxt\n                    # link into header table\n                    head = header[itm][1]\n                    if head is None:\n                        header[itm][1] = nxt\n                    else:\n                        while head.link is not None:\n                            head = head.link\n                        head.link = nxt\n                cur = nxt\n        return root, header\n\n    # --------------------------------------------------------------------- #\n    def conditional_pattern_base(node: FPNode) -> list[tuple[list[str], int]]:\n       Return the conditional pattern base of *node.item*        patterns: list[tuple[list[str], int]] = []\n        while node is not None:\n            path: list[str] = []\n            par = node.parent\n            while par and par.item is not None:          # root has item None\n                path.append(par.item)\n                par = par.parent\n            if path:\n                patterns.append((path, node.count))\n            node = node.link\n        return patterns\n\n    # --------------------------------------------------------------------- #\n    def mine_tree(root: FPNode, header: dict[str, list],\n                  prefix: list[str], out: set[tuple[str, ...]]):\n       Recursively grow frequent patterns        for itm in sorted(header):                       # lexicographic order\n            new_pattern = sorted(prefix + [itm])         # keep inside sorted\n            out.add(tuple(new_pattern))\n\n            # build conditional FP-tree\n            cond_base = conditional_pattern_base(header[itm][1])\n            if not cond_base:\n                continue\n            cond_root, cond_header = build_tree(cond_base)\n            if cond_root:                                # there are freq items\n                mine_tree(cond_root, cond_header, new_pattern, out)\n\n    # ========================== driver starts here ========================= #\n    # corner cases ---------------------------------------------------------- #\n    if not transactions or min_support < 1:\n        return []\n\n    # build the main FP-tree ------------------------------------------------ #\n    weighted_tx = [(trx, 1) for trx in transactions]\n    root, header = build_tree(weighted_tx)\n    if root is None:                                     # nothing is frequent\n        return []\n\n    # mine all frequent item-sets ------------------------------------------ #\n    results: set[tuple[str, ...]] = set()\n    mine_tree(root, header, [], results)\n\n    # turn into the required representation -------------------------------- #\n    frequent_itemsets = [list(tpl) for tpl in results]\n    frequent_itemsets.sort(key=lambda x: (len(x), x))    # deterministic order\n    return frequent_itemsets\n```\n\nExplanation of the main steps\n\n1. First pass \u2013 frequency filtering  \n   Only items that reach `min_support` survive, guaranteeing the tree stays compact.\n\n2. FP-tree construction  \n   \u2022 Items inside every transaction are ordered by descending local frequency (ties\n   broken lexicographically).  \n   \u2022 A header table keeps, for each item, its total support and a linked list that\n   connects all nodes bearing that item \u2013 essential for quick traversal of\n   conditional pattern bases.\n\n3. Mining (recursive)  \n   For each item (taken in lexicographic order to keep the output deterministic)  \n     \u2022 the current prefix plus this item is a new frequent pattern;  \n     \u2022 its conditional pattern base is collected;  \n     \u2022 an FP-tree is built from that base and mined in turn.\n\n4. Output formatting  \n   Internally item-sets are stored as tuples inside a set to guard against possible\n   duplicates.  Before returning, they are converted into lists and sorted by\n   `(len(itemset), itemset)` \u2013 therefore calling `sort` again with the same key\n   would leave the list unchanged, satisfying the determinism requirement."}
{"task_id": 71, "completion_id": 0, "completion": "```python\ndef label_uniq_cnt(data: list[list]) -> dict:\n    \"\"\"Count how many times each label appears in *data*.\n\n    The last element of every inner list is considered the class label.\n    The function iterates once over *data* and builds a frequency map.\n\n    Args:\n        data: List of samples where each sample is a list.  The final\n              element of each sample is its label.\n\n    Returns:\n        dict: {label: count, ...}.  If *data* is empty, returns {}.\n    \"\"\"\n    label_counts: dict = {}\n\n    # single pass over the dataset\n    for sample in data:\n        if not sample:        # optionally guard against empty rows\n            continue\n        label = sample[-1]    # last element is the label\n        label_counts[label] = label_counts.get(label, 0) + 1\n\n    return label_counts\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef variance_reduction(y: np.ndarray,\n                       y_left: np.ndarray,\n                       y_right: np.ndarray) -> float:\n    \"\"\"\n    Compute the variance-reduction achieved by splitting a parent node that\n    contains the target values `y` into two child nodes with targets\n    `y_left` and `y_right`.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Target values of the parent node (shape: n_samples x n_targets or\n        just n_samples for a 1-D target).\n    y_left : np.ndarray\n        Target values that would go to the left child after the split.\n    y_right : np.ndarray\n        Target values that would go to the right child after the split.\n\n    Returns\n    -------\n    float\n        Variance reduction rounded to 4 decimal places.\n\n    Notes\n    -----\n    \u2022 Population variance (ddof=0) is used.\n    \u2022 For multi-dimensional targets the variances of all target dimensions\n      are summed so that the function always returns a scalar.\n    \"\"\"\n    # Number of samples in each node\n    n        = y.shape[0]\n    n_left   = y_left.shape[0]\n    n_right  = y_right.shape[0]\n\n    # Helper : summed (across targets) population variance of a node\n    def _summed_var(a: np.ndarray) -> float:\n        # axis=0 \u2192 variance for every target dimension, then sum\n        return np.var(a, axis=0, ddof=0).sum()\n\n    # Variances\n    var_parent = _summed_var(y)\n    var_left   = _summed_var(y_left)\n    var_right  = _summed_var(y_right)\n\n    # Variance-reduction formula\n    vr = var_parent - (n_left / n) * var_left - (n_right / n) * var_right\n\n    # Return rounded to 4 decimal places\n    return round(float(vr), 4)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef magnitude_spectrum(frames:np.ndarray ->list[list[float]]   Compute the positive-frequency magnitude spectrum for each real frame.\n\n    Parameters\n    ----------\n    frames : numpy.ndarray\n        \u2022 2-D shape (M, N): each row is a frame\n        \u2022 1-D shape (N,): treated as a single frame\n\n    Returns\n    -------\n    list[list[float]]\n        Nested list whose inner length is N//2 + 1, containing the\n        magnitudes rounded to four decimals for every input frame.\n       # Ensure `frames` is a NumPy array and at least 2-D (row-wise frames)\n    frames = np.asarray(frames, dtype=float)\n    frames_2d = np.atleast_2d(frames)\n\n    # Real FFT along last axis \u2192 shape (M, N//2 + 1)\n    spectrum = np.fft.rfft(frames_2d, axis=-1)\n\n    # Magnitude and rounding\n    magnitudes = np.round(np.abs(spectrum), 4)\n\n    # Return Python nested list\n    return magnitudes.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef knn_predict(\n        X_train: list[list[float]],\n        y_train: list[int],\n        X_test: list[list[float]],\n        k: int\n    ) -> list[int] | int:\n   k-Nearest Neighbour classifier (majority voting, Euclidean distance).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list[int]\n        Corresponding class labels.\n    X_test : list[list[float]]\n        Test feature vectors to classify.\n    k : int\n        Number of neighbours that take part in the vote.\n\n    Returns\n    -------\n    list[int] | int\n        Predicted labels for every row in `X_test`, or -1 when `k`\n        is invalid.\n       n_train = len(X_train)\n    # ---- parameter sanity check ------------------------------------------------\n    if not isinstance(k, int) or k < 1 or k > n_train:\n        return -1\n\n    # ---- convert to NumPy for fast vectorised distance computation -------------\n    X_train_np = np.asarray(X_train, dtype=float)\n    X_test_np  = np.asarray(X_test,  dtype=float)\n    y_train_np = np.asarray(y_train, dtype=int)\n\n    predictions: list[int] = []\n\n    for x in X_test_np:\n        # squared Euclidean distances to all training samples\n        dists = np.sum((X_train_np - x) ** 2, axis=1)\n\n        # indices of the k smallest distances\n        knn_idx = np.argpartition(dists, k-1)[:k]\n\n        # majority vote among the k neighbours\n        neighbour_labels = y_train_np[knn_idx]\n        vote_counter = Counter(neighbour_labels)\n        highest_vote = max(vote_counter.values())\n\n        # labels that have the maximum vote (handle ties)\n        candidates = [label for label, cnt in vote_counter.items()\n                      if cnt == highest_vote]\n\n        predictions.append(int(min(candidates)))  # smallest label wins the tie\n\n    return predictions\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n   Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n       # Convert inputs to NumPy arrays with float dtype\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Machine epsilon for numerical stability\n    eps = np.finfo(float).eps\n\n    # Cross-entropy computation\n    loss = -np.sum(y * np.log(y_pred + eps))\n\n    # Return rounded to 4 decimal places\n    return round(loss, 4)\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]):\n   Forward propagation for an L-layer neural network (ReLU \u2026 ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary that contains the weight matrices W1 \u2026 WL and the bias\n        vectors b1 \u2026 bL.\n\n    Returns\n    -------\n    tuple[list[list[float]], list]\n        A two-element tuple\n            ( AL_as_python_list , caches )\n        AL_as_python_list  \u2013 the output of the network rounded to 4 decimals\n                             and converted to a plain Python list;\n        caches             \u2013 a list with the per-layer caches that would be\n                             needed during back-propagation.\n       # ------------------------------------------------------------------ #\n    # helper functions                                                    #\n    # ------------------------------------------------------------------ #\n    def linear_forward(A_prev, W, b):\n       Linear part: Z = W\u00b7A_prev + b        Z = W @ A_prev + b\n        return Z, (A_prev, W, b)\n\n    def linear_activation_forward(A_prev, W, b, activation: str):\n       Linear step + activation (ReLU or Sigmoid).        Z, linear_cache = linear_forward(A_prev, W, b)\n\n        if activation ==relu            A = np.maximum(0, Z)\n        elif activation ==sigmoid            A = 1.0 / (1.0 + np.exp(-Z))\n        else:\n            raise ValueError(fUnsupported activation: {activation        # We store both caches so that a backward pass would be possible.\n        cache = (linear_cache, Z)\n        return A, cache\n\n    # ------------------------------------------------------------------ #\n    # forward propagation through all layers                              #\n    # ------------------------------------------------------------------ #\n    caches: list = []\n    A = X\n    L = len(parameters) // 2  # number of layers\n\n    # 1 \u2026 L-1  (hidden layers, ReLU)\n    for l in range(1, L):\n        A_prev = A\n        W = parameters[f{l}        b = parameters[fb{l}        A, cache = linear_activation_forward(A_prev, W, b, activationrelu        caches.append(cache)\n\n    # layer L  (output layer, Sigmoid)\n    WL = parameters[f{L}    bL = parameters[fb{L}    AL, cache = linear_activation_forward(A, WL, bL, activationsigmoid    caches.append(cache)\n\n    # ------------------------------------------------------------------ #\n    # post-processing of the network output                               #\n    # ------------------------------------------------------------------ #\n    AL_list = np.round(AL, 4).tolist()\n\n    return AL_list, caches\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adamax_step(params: list[float],\n                grads:  list[float],\n                m:      list[float],\n                u:      list[float],\n                t:      int,\n                learning_rate: float = 0.002,\n                beta1:         float = 0.9,\n                beta2:         float = 0.999,\n                epsilon:       float = 1e-8) -> tuple[list[float], list[float], list[float], int]:\n    \"\"\"Perform one Adamax update step (all operations element-wise).\n\n    Args:\n        params:          Current parameter vector.\n        grads:           Current gradient vector.\n        m:               First-moment running averages.\n        u:               Exponentially weighted infinity norms.\n        t:               Current time-step counter (starts at 1).\n        learning_rate:   \u03b7.\n        beta1:           \u03b2\u2081, decay for the first moment.\n        beta2:           \u03b2\u2082, decay for the infinity norm.\n        epsilon:         Small constant to avoid division by zero.\n\n    Returns:\n        (updated_params, updated_m, updated_u, updated_t) \u2013 every float rounded\n        to 6 decimal places, with updated_t = t + 1.\n    \"\"\"\n\n    # Convert inputs to numpy arrays of float64 for vectorised math\n    params_arr = np.asarray(params, dtype=np.float64)\n    grads_arr  = np.asarray(grads,  dtype=np.float64)\n    m_arr      = np.asarray(m,      dtype=np.float64)\n    u_arr      = np.asarray(u,      dtype=np.float64)\n\n    # 1. First-moment update\n    m_t = beta1 * m_arr + (1.0 - beta1) * grads_arr\n\n    # 2. Infinity-norm update\n    u_t = np.maximum(beta2 * u_arr, np.abs(grads_arr))\n\n    # 3. Bias-corrected learning rate\n    lr_t = learning_rate / (1.0 - beta1 ** t)\n\n    # 4. Parameter step\n    step = lr_t * m_t / (u_t + epsilon)\n    params_t = params_arr - step\n\n    # 5. Increment time-step\n    new_t = t + 1\n\n    # Helper: round every element to 6 decimals and convert back to regular lists\n    round6 = lambda arr: np.round(arr, 6).tolist()\n\n    return (\n        round6(params_t),  # updated_params\n        round6(m_t),       # updated_m\n        round6(u_t),       # updated_u\n        new_t              # updated_t\n    )\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef adadelta_update(w: list[float] | np.ndarray,\n                    grad_w: list[float] | np.ndarray,\n                    rho: float = 0.95,\n                    eps: float = 1e-6) -> list[float]:\n    \"\"\"Perform **one** Adadelta optimisation step (first step, running\n    averages still zero).  The updated weights are returned rounded to\n    4 decimals as a regular Python list.\n    \"\"\"\n    # Convert inputs to NumPy arrays for element-wise maths\n    w       = np.asarray(w,       dtype=np.float64)\n    grad_w  = np.asarray(grad_w,  dtype=np.float64)\n\n    # Running averages are still zero  \u279c  E_grad = (1-\u03c1)\u00b7g\u00b2\n    E_grad  = (1.0 - rho) * grad_w**2\n\n    # Root-mean-squares\n    RMS_grad  = np.sqrt(E_grad + eps)\n    RMS_delta = np.sqrt(eps)                # because E_delta = 0 at step 0\n\n    # Element-wise adaptive learning rate and update\n    adaptive_lr = RMS_delta / RMS_grad\n    delta_w     = adaptive_lr * grad_w      # \u0394w\n\n    # (Optional for completeness: new E_delta = (1-\u03c1)\u00b7\u0394w\u00b2)\n    # E_delta = (1.0 - rho) * delta_w**2\n\n    # Parameter update\n    w_new = w - delta_w\n\n    # Round to 4 decimals and return as plain Python list\n    return np.round(w_new, 4).tolist()\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n   Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n       # Ensure both arrays are 1-D and have identical shape\n    A2 = np.ravel(A2)\n    Y  = np.ravel(Y)\n\n    # Numerical stability: clip predictions to avoid log(0)\n    eps = 1e-15\n    A2_clipped = np.clip(A2, eps, 1.0 - eps)\n\n    # Binary cross-entropy (negative log-likelihood)\n    cost = -np.mean(Y * np.log(A2_clipped) + (1 - Y) * np.log(1.0 - A2_clipped))\n\n    # Round to six decimal places and return as Python float\n    return float(round(cost, 6))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef epsilon_soft(Q: list[float] |np.ndarray epsilon: float) -> list[float]:\n   Return the \u03b5-soft action\u2013selection probabilities.\n\n    Parameters\n    ----------\n    Q : 1-D sequence (list or np.ndarray)\n        Estimated action values for a single state.\n    epsilon : float               (0 \u2264 \u03b5 \u2264 1)\n        Exploration parameter.\n\n    Returns\n    -------\n    list[float]\n        Probabilities for selecting each action, rounded to 4 decimals.\n        The probabilities sum to 1 up to 1\u00d710\u207b\u00b9\u00b2 numerical tolerance.\n       # --- basic checks --------------------------------------------------------\n    if not (0.0 <= epsilon <= 1.0):\n        raise ValueErrorepsilon must satisfy 0 \u2264 \u03b5 \u2264 1    # convert to 1-D numpy array\n    q = np.asarray(Q, dtype=float).ravel()\n    n_actions = q.size\n    if n_actions == 0:\n        raise ValueErrorQ must contain at least one action value    # --- \u03b5-soft distribution --------------------------------------------------\n    greedy_idx = int(np.argmax(q))                 # first max in case of ties\n    base_prob = epsilon / n_actions\n\n    probs = np.full(n_actions, base_prob, dtype=float)\n    probs[greedy_idx] += (1.0 - epsilon)           # add the greedy surplus\n\n    # --- rounding to 4 decimals while keeping the sum 1 ----------------------\n    rounded = np.round(probs, 4)\n    diff = 1.0 - rounded.sum()\n\n    # Numerically adjust the greedy action so that \u03a3p = 1 (within tolerance)\n    if abs(diff) > 1e-12:\n        rounded[greedy_idx] = np.round(rounded[greedy_idx] + diff, 4)\n\n        # In very rare cases the second rounding may still leave a tiny error\n        # (because we were forced to round again).  Fix that numerically.\n        final_diff = 1.0 - rounded.sum()\n        if abs(final_diff) > 1e-12:\n            rounded[greedy_idx] += final_diff      # no extra rounding\n\n    return rounded.tolist()\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse_line_search(y: np.ndarray, y_pred: np.ndarray, h_pred: np.ndarray) -> float:\n   Compute the optimal step size \u03b3 for an MSE gradient-boosting update.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True target values.\n    y_pred : np.ndarray\n        Current model predictions.\n    h_pred : np.ndarray\n        Predictions of the new base learner.\n\n    Returns\n    -------\n    float\n        Optimal step size \u03b3 rounded to four decimal places.\n        Returns 1.0 if h_pred is a zero vector (perfect residual fit).\n       # Residuals\n    residual = y - y_pred\n\n    # Numerator and denominator of the closed-form solution\n    numerator = np.dot(residual, h_pred)\n    denominator = np.dot(h_pred, h_pred)          # \u2211 h_i\u00b2\n\n    # If the learner contributes nothing (all h_i == 0) we take \u03b3 = 1\n    if np.isclose(denominator, 0.0):\n        return 1.0\n\n    gamma = numerator / denominator\n\n    # Round to 4 decimal places as required\n    return float(np.round(gamma, 4))\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax_regression_step(X: np.ndarray, Y: np.ndarray, W: np.ndarray, lr: float) -> list[list[float]]:\n    \"\"\"Perform one gradient\u2013descent update step for multi\u2013class Softmax regression.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Training samples, shape (N, D).\n    Y : np.ndarray\n        One-hot encoded target labels, shape (N, C).\n    W : np.ndarray\n        Current weight matrix, shape (D, C).\n    lr : float\n        Learning-rate scalar.\n\n    Returns\n    -------\n    list[list[float]]\n        Updated weight matrix rounded to four decimal places.\n    \"\"\"\n    # Early-exit: no learning or no information in X\n    if lr == 0 or np.all(X == 0):\n        return np.round(W, 4).tolist()\n\n    N = X.shape[0]                          # number of samples -----------------\n    # ------------------------ forward pass ------------------------------------\n    Z = X @ W                               # logits, shape (N, C)\n\n    # numerically-stable softmax\n    Z_max = Z.max(axis=1, keepdims=True)    # subtract max for stability\n    exp_Z = np.exp(Z - Z_max)\n    Y_hat = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n\n    # ------------------------ gradient ----------------------------------------\n    grad = (X.T @ (Y_hat - Y)) / N          # shape (D, C)\n\n    # ------------------------ weight update -----------------------------------\n    W_new = W - lr * grad\n\n    # ------------------------ rounding / return -------------------------------\n    return np.round(W_new, 4).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, List, Union\n\n\ndef aggregate_random_forest_votes(\n    predictions: List[List[Union[int, float, str]]]\n) -> List[Any]:\n   Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions\n        produced by *one* decision tree for **all** samples. All inner lists\n        must have the same length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting.  If several labels obtain the same (highest) number of votes\n        the smallest label is returned (numeric value or lexicographic order).\n       # No trees \u2192 nothing to aggregate\n    if not predictions:\n        return []\n\n    n_samples = len(predictions[0])\n    if n_samples == 0:                       # empty dataset\n        return []\n\n    # Sanity-check: all trees predicted for the same number of samples\n    for tree_pred in predictions:\n        if len(tree_pred) != n_samples:\n            raise ValueError(\n               All inner lists in `predictions` must have the same length            )\n\n    final_predictions: list[Any] = []\n\n    # Iterate over samples (column-wise)\n    for col in range(n_samples):\n        # Collect the votes of every tree for the current sample\n        votes = (tree_pred[col] for tree_pred in predictions)\n        counts = Counter(votes)\n\n        # Determine the highest vote count\n        max_votes = max(counts.values())\n\n        # All labels with this count (handles ties)\n        winners = [lbl for lbl, cnt in counts.items() if cnt == max_votes]\n\n        # Tie-break: pick the smallest label\n        final_predictions.append(min(winners))\n\n    return final_predictions\n```\n\nExample\n-------\n```python\npreds = [\n    [0, 1, 1, 2],      # tree 1\n    [1, 1, 0, 2],      # tree 2\n    [1, 0, 1, 2],      # tree 3\n]\n\nprint(aggregate_random_forest_votes(preds))  # \u2192 [1, 1, 1, 2]\n```\n\nThe function counts votes for every sample, selects the label with the most votes, and resolves ties by returning the smallest label\u2014exactly how a Random Forest majority vote works."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef layer_sizes(X: np.ndarray, Y: np.ndarray) -> tuple[int, int, int]:\n    \"\"\"Return the sizes of the input, hidden, and output layers.\n\n    Args:\n        X: 2-D NumPy array of shape (n_x, m) containing the training inputs.\n        Y: 2-D NumPy array of shape (n_y, m) containing the labels.\n\n    Returns:\n        Tuple (n_x, n_h, n_y) where n_h is fixed to 10.\n    \"\"\"\n    n_x = X.shape[0]   # number of input features\n    n_h = 10           # fixed hidden layer size\n    n_y = Y.shape[0]   # number of output units\n    return n_x, n_h, n_y\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n   Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n       # Convert the input to a NumPy array for unified processing\n    arr = np.asarray(z)\n\n    # Numerically stable softplus: ln(1 + e**z) == logaddexp(0, z)\n    result = np.logaddexp(0.0, arr)\n\n    # Round to four decimal places\n    result = np.round(result, 4)\n\n    # Return type depends on the original input\n    if np.isscalar(z):\n        # Ensure a Python float (not a NumPy scalar)\n        return float(result)\n    else:\n        # Convert any array-like result to pure Python lists\n        return result.tolist()\n```"}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (left intentionally, as given in the stub)\n\ndef linear_activation_forward(A_prev: np.ndarray,\n                              W: np.ndarray,\n                              b: np.ndarray,\n                              activation: str):\n    \"\"\"Forward propagation for a single neural-network layer.\n\n    Parameters\n    ----------\n    A_prev : np.ndarray\n        Activations from the previous layer, shape (n_{l-1}, m).\n    W : np.ndarray\n        Weight matrix for the current layer, shape (n_l, n_{l-1}).\n    b : np.ndarray\n        Bias vector for the current layer, shape (n_l, 1).\n    activation : str\n        The activation to be used: either \"relu\" or \"sigmoid\".\n\n    Returns\n    -------\n    tuple\n        (A, cache) where  \n        \u2022 A is the layer output **rounded to 4 decimals** and converted to\n          a plain Python list.  \n        \u2022 cache is ((A_prev, W, b), Z) needed for back-prop.\n    \"\"\"\n    \n    # ----- Linear step -------------------------------------------------------\n    Z = np.dot(W, A_prev) + b        # broadcasting handles b\n    \n    # ----- Non-linear activation --------------------------------------------\n    if activation == \"relu\":\n        A = np.maximum(0, Z)\n    elif activation == \"sigmoid\":\n        A = 1.0 / (1.0 + np.exp(-Z))\n    else:\n        raise ValueError(\"Unsupported activation; choose 'relu' or 'sigmoid'.\")\n    \n    # ----- Shape check -------------------------------------------------------\n    assert A.shape == (W.shape[0], A_prev.shape[1]), \\\n        f\"Output shape {A.shape} does not match expected {(W.shape[0], A_prev.shape[1])}\"\n    \n    # ----- Preparing outputs -------------------------------------------------\n    A_rounded_list = A.round(4).tolist()\n    cache = ((A_prev, W, b), Z)\n    \n    return A_rounded_list, cache\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\n\ndef _fetch(obj: Any, key: str):\n       Helper: obtain `obj.key` if it exists, otherwise `obj[key]`\n    (works for both objects and dictionaries).\n       if isinstance(obj, dict):\n        return obj.get(key, None)\n    return getattr(obj, key, None)\n\n\ndef mse(bandit: Any, policy: Any) -> float:\n   Mean-squared error between a policy\u2019s estimates and true arm values.\n\n    Parameters\n    ----------\n    bandit : Any\n        Must expose the iterable `arm_evs` (tuple/list/np.ndarray) that\n        contains the true expected reward of each arm.\n    policy : Any\n        Must expose the dict `ev_estimates` mapping an arm index (int) to\n        the policy\u2019s current estimate of that arm\u2019s expectation.\n\n    Returns\n    -------\n    float\n        Mean-squared error rounded to 4 decimal places.  If the policy\n        provides no estimates (missing attribute or empty dictionary)\n        ``numpy.nan`` is returned.\n       # ------------------------------------------------------------------ #\n    # 1. Pull out the data we need\n    # ------------------------------------------------------------------ #\n    true_evs = _fetch(bandit,arm_evs    est_dict = _fetch(policy,ev_estimates    # If the policy does not provide any estimates \u2192 return NaN\n    if not est_dict:        # catches None and an empty dict alike\n        return np.nan\n\n    if true_evs is None:\n        raise AttributeError`bandit` must contain the field/attribute                            arm_evs` with the true arm expectations    # ------------------------------------------------------------------ #\n    # 2. Compute the squared error arm-by-arm, matching indices\n    # ------------------------------------------------------------------ #\n    # Sort arm indices so that the *i*-th element matches arm_evs[i]\n    indices = sorted(est_dict)\n    try:\n        sq_errors = [(est_dict[i] - true_evs[i]) ** 2 for i in indices]\n    except IndexError:\n        raise IndexErrorMismatch between number of true arms and                        policy indices    # ------------------------------------------------------------------ #\n    # 3. Mean-squared error and rounding\n    # ------------------------------------------------------------------ #\n    mse_val = float(np.mean(sq_errors))          # ensure plain python float\n    return round(mse_val, 4)\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_backward(dA: list[list[int | float]],\n                  activation_cache: list[list[int | float]]\n                  ) -> list[list[int | float]] | int:\n   Backward pass of the ReLU activation function.\n\n    Parameters\n    ----------\n    dA : list[list[int | float]]\n        Upstream gradient from the next layer.\n    activation_cache : list[list[int | float]]\n        Cached pre-activation values (Z).\n\n    Returns\n    -------\n    list[list[int | float]] | int\n        The gradient with respect to Z (same shape as dA) or\n        -1 if the input shapes do not match.\n       # Convert inputs to NumPy arrays for vectorised operations\n    dA_arr = np.asarray(dA, dtype=float)\n    Z_arr  = np.asarray(activation_cache, dtype=float)\n\n    # Shape check\n    if dA_arr.shape != Z_arr.shape:\n        return -1\n\n    # Element-wise gradient: keep dA where Z > 0, else 0\n    dZ = dA_arr * (Z_arr > 0)\n\n    # Convert back to (nested) Python list\n    return dZ.tolist()\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport numbers\n\ndef is_number(a) -> bool:\n   Check whether the input value is numeric.\n\n    A value is considered numeric if it is an instance of ``numbers.Number``\n    (int, float, complex, Fraction, Decimal, etc.) but **not** a boolean.\n\n    Args:\n        a: Any Python object.\n\n    Returns:\n        bool: True if ``a`` is numeric and not a bool, otherwise False.\n       # `bool` is a subclass of `int`, so we must explicitly exclude it.\n    return isinstance(a, numbers.Number) and not isinstance(a, bool)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu(Z):\n    \"\"\"Compute the element-wise Rectified Linear Unit (ReLU) of *Z*.\n\n    Args:\n        Z (array-like): A NumPy array, Python scalar or (nested) list of\n            numbers representing the pre-activation values.\n\n    Returns:\n        tuple[list, list]: A tuple ``(A, cache)`` where\n            * ``A``     \u2013 ReLU(Z)         (as a Python list)\n            * ``cache`` \u2013 the original Z  (as a Python list)\n\n        Both outputs are obtained via NumPy\u2019s ``tolist()`` method so they are\n        JSON-serialisable.\n    \"\"\"\n    # Convert input to a NumPy array (no data copied unless necessary)\n    Z_arr = np.asarray(Z)\n\n    # Vectorised ReLU: max(0, Z)\n    A_arr = np.maximum(0, Z_arr)\n\n    # Convert to plain Python objects\n    A_out     = A_arr.tolist()\n    cache_out = Z_arr.tolist()\n\n    # Ensure that even scalar inputs are returned as *lists*\n    # (tolist() on a 0-D array returns a scalar, not a list)\n    if np.ndim(Z_arr) == 0:\n        A_out     = [A_out]\n        cache_out = [cache_out]\n\n    return A_out, cache_out\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Hashable, TypeVar, Sequence\n\nT = TypeVarT bound=Hashable)          # any hashable (and thus comparable) type\n\n\ndef knn_majority_vote(neighbors_targets: Sequence[T]) -> T:\n   Return the majority class label among k-NN neighbours.\n\n    Args:\n        neighbors_targets: A non-empty sequence with the class labels of the\n            k nearest neighbours.\n\n    Returns:\n        The label that occurs most frequently.  If several labels share the\n        highest frequency, the smallest label according to Python's default\n        ordering (numerical for numbers, lexicographic for strings, \u2026) is\n        returned.\n\n    Raises:\n        ValueError: if *neighbors_targets* is empty.\n       if not neighbors_targets:\n        raise ValueErrorneighbors_targets must contain at least one label    counts = Counter(neighbors_targets)           # 1. count frequencies\n    max_freq = max(counts.values())               #    highest frequency\n    candidates = [lbl for lbl, freq in counts.items() if freq == max_freq]\n    return min(candidates)                        # 2./3. majority or smallest tie\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\"\"\"\n    \n    # ---------- build the training vocabulary ----------\n    vocab: set[str] = set()\n    tokenised_corpus: list[list[str]] = []\n    for sent in corpus:\n        tokens = sent.split()\n        vocab.update(tokens)\n        tokenised_corpus.append(tokens)\n\n    vocab.add(\"<UNK>\")                       # explicit unknown token\n    V = len(vocab)                           # vocabulary size (with <UNK>)\n    \n    # ---------- collect all n-gram counts (1 \u2026 N) ----------\n    ngram_counts: dict[int, Counter] = {n: Counter() for n in range(1, N + 1)}\n    \n    for tokens in tokenised_corpus:\n        L = len(tokens)\n        for n in range(1, N + 1):\n            if L < n:\n                continue\n            for i in range(L - n + 1):\n                ngram = tuple(tokens[i : i + n])\n                ngram_counts[n][ngram] += 1\n\n    total_unigrams = sum(ngram_counts[1].values())        # needed for the unigram model\n    \n    # ---------- prepare the target sequence ----------\n    seq_tokens = [t if t in vocab else \"<UNK>\" for t in sequence.split()]\n    if len(seq_tokens) < N:               # no N-grams can be extracted\n        return 0.0\n    \n    # ---------- probability of every contiguous N-gram ----------\n    log_prob = 0.0\n    for i in range(len(seq_tokens) - N + 1):\n        ngram = tuple(seq_tokens[i : i + N])\n\n        if N == 1:\n            # unigram model: context is \u201cempty\u201d\n            num = ngram_counts[1][ngram] + K\n            den = total_unigrams + K * V\n        else:\n            context = ngram[:-1]                          # first N-1 tokens\n            num = ngram_counts[N][ngram] + K\n            den = ngram_counts[N - 1][context] + K * V   # context count + smoothing mass\n\n        prob = num / den\n        log_prob += math.log(prob)\n    \n    return round(log_prob, 4)\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef drelu(Z):\n    \"\"\"Return the element-wise derivative of the ReLU activation.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Input that can be a scalar, list (any depth), or ndarray.\n\n    Returns\n    -------\n    float | list\n        ReLU derivative with the same shape layout as *Z*\n        (float if *Z* is a scalar, otherwise a Python list).\n    \"\"\"\n    # Scalar fast-path\n    if np.isscalar(Z):\n        return 1.0 if Z > 0 else 0.0\n\n    # Convert any array-like structure to an ndarray\n    arr = np.asarray(Z)\n\n    # Derivative: 1.0 for strictly positive entries, else 0.0\n    deriv = (arr > 0).astype(float)\n\n    # Convert back to Python list to satisfy the requirement\n    return deriv.tolist()\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_(Z):\n    \"\"\"Apply the element-wise Rectified Linear Unit (ReLU) activation.\n\n    The container type of the returned object matches the input:\n    scalar \u2192 scalar, list / list-of-lists \u2192 list / list-of-lists,\n    NumPy array \u2192 NumPy array.\n\n    Parameters\n    ----------\n    Z : int | float | list | numpy.ndarray\n        Scalar, 1-D / 2-D list, or NumPy array.\n\n    Returns\n    -------\n    Same as type(Z)\n        ReLU-transformed data (all negatives clipped to 0).\n    \"\"\"\n    # ---------- scalar --------------------------------------------------\n    if isinstance(Z, (int, float)):\n        return max(0, Z)\n\n    # ---------- NumPy array ---------------------------------------------\n    if isinstance(Z, np.ndarray):\n        # np.maximum is vectorised and preserves dtype/shape\n        return np.maximum(0, Z)\n\n    # ---------- Python list / list-of-lists -----------------------------\n    if isinstance(Z, list):\n        # Convert once to ndarray, apply ReLU vectorised, then convert back\n        return np.maximum(0, np.asarray(Z)).tolist()\n\n    # ---------- unsupported type ----------------------------------------\n    raise TypeError(\n        \"relu_ supports int, float, list (1-D/2-D) and numpy.ndarray inputs.\"\n    )\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef classification_error(actual, predicted):\n   Compute the proportion of incorrect predictions.\n\n    Parameters\n    ----------\n    actual : list | tuple | numpy.ndarray\n        Ground-truth labels.\n    predicted : list | tuple | numpy.ndarray\n        Predicted labels from a classifier.\n\n    Returns\n    -------\n    float | int\n        The classification error rate rounded to four decimals, or \u22121 when\n        the input lengths are not identical.\n       # Convert inputs to 1-D NumPy arrays\n    actual_arr    = np.asarray(actual).ravel()\n    predicted_arr = np.asarray(predicted).ravel()\n\n    # Check that both inputs have the same length\n    if actual_arr.size != predicted_arr.size:\n        return -1\n\n    # Count mismatches and compute error rate\n    mismatches = np.sum(actual_arr != predicted_arr)\n    error_rate = mismatches / actual_arr.size\n\n    # Return value rounded to four decimal places\n    return round(error_rate, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x: np.ndarray, axis: int = 1) -> list:\n    \"\"\"Apply the softmax activation function along a specified axis.\n\n    Args:\n        x: NumPy ndarray with **at least two dimensions**.\n        axis: Axis along which to apply the softmax (negative indices allowed).\n\n    Returns:\n        A (nested) Python list containing the soft-maxed values, each rounded\n        to 4 decimal places.\n\n    Raises:\n        ValueError: If ``x`` is 1-D (``x.ndim == 1``).\n    \"\"\"\n    # ---- Validation ---------------------------------------------------------\n    if x.ndim == 1:\n        raise ValueError(\"Cannot apply softmax to a tensor that is 1D\")\n\n    # NumPy automatically normalises negative axes, so no extra work needed\n    # other than letting potential `AxisError` propagate for invalid indices.\n    # ---- Numerically-stable softmax ----------------------------------------\n    # Subtract the max for numerical stability\n    max_along_axis = np.max(x, axis=axis, keepdims=True)\n    exp_shifted     = np.exp(x - max_along_axis)\n    sum_exp         = np.sum(exp_shifted, axis=axis, keepdims=True)\n    softmax_vals    = exp_shifted / sum_exp\n\n    # ---- Post-processing ----------------------------------------------------\n    softmax_rounded = np.round(softmax_vals, 4)\n\n    return softmax_rounded.tolist()\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef polynomial_regression_predict(\n    x: list[float], y: list[float], degree: int, x_pred: list[float]\n) -> list[float]:\n   Fit a polynomial regression model and return predictions.\n\n    Parameters\n    ----------\n    x : list[float]\n        Training input values.\n    y : list[float]\n        Training target values.\n    degree : int\n        Degree of the polynomial to be fitted (must be >= 0).\n    x_pred : list[float]\n        Values at which the fitted model should be evaluated.\n\n    Returns\n    -------\n    list[float]\n        Predicted values for *x_pred* rounded to 4 decimal places, or -1 if the\n        model cannot be fitted (e.g. not enough data, inconsistent input).\n       # ------------ sanity checks ------------------------------------------------\n    if (\n        not isinstance(degree, int)\n        or degree < 0\n        or not isinstance(x, list)\n        or not isinstance(y, list)\n        or len(x) != len(y)\n        or len(x) == 0\n        or len(x) < degree + 1  # not enough points to fit the polynomial\n    ):\n        return -1\n\n    try:\n        # Convert to NumPy arrays of float\n        x_arr = np.asarray(x, dtype=float)\n        y_arr = np.asarray(y, dtype=float)\n        x_pred_arr = np.asarray(x_pred, dtype=float)\n\n        # ----------- build design / feature matrices --------------------------\n        # Rows: samples, Columns: x^0, x^1, ..., x^degree\n        X = np.vander(x_arr, N=degree + 1, increasing=True)\n        X_pred = np.vander(x_pred_arr, N=degree + 1, increasing=True)\n\n        # ----------- solve ordinary least squares -----------------------------\n        # np.linalg.lstsq returns (weights, residuals, rank, s)\n        w = np.linalg.lstsq(X, y_arr, rcond=None)[0]\n\n        # ----------- generate predictions -------------------------------------\n        y_hat = X_pred.dot(w)\n\n        # ----------- rounding & return ----------------------------------------\n        return np.round(y_hat, 4).tolist()\n\n    except Exception:\n        # Any numerical / conversion error \u2192 fitting impossible\n        return -1\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef information_gain(y: np.ndarray, splits: list[np.ndarray]) -> float:\n   Computes the information gain of a proposed split.\n\n    Args:\n        y: 1-D NumPy array containing the class labels of the parent node.\n        splits: List of 1-D NumPy arrays. Each array contains the labels that\n                fall into one child node produced by the split.\n\n    Returns:\n        Information gain rounded to 4 decimal places.\n       def entropy(labels: np.ndarray) -> float:\n       Shannon entropy (base-2) of a 1-D label array        if labels.size == 0:                 # empty split \u2192 no contribution\n            return 0.0\n        _, counts = np.unique(labels, return_counts=True)\n        probs = counts / labels.size         # class probabilities\n        return -np.sum(probs * np.log2(probs))\n\n    # Entropy of the parent node\n    parent_entropy = entropy(y)\n\n    # Weighted entropy of all children\n    total = y.size\n    weighted_children_entropy = 0.0\n    for child in splits:\n        weight = child.size / total\n        weighted_children_entropy += weight * entropy(child)\n\n    # Information gain\n    ig = parent_entropy - weighted_children_entropy\n    return round(float(ig), 4)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_pdf(x, mean, sigma):\n    \"\"\"Compute the Gaussian probability density for each element in *x*.\n\n    The function returns a list of floats rounded to 5 decimal places. If *sigma*\n    is not strictly positive the function must return -1.\n\n    Args:\n        x (list | tuple | np.ndarray | float | int): Values at which to evaluate the PDF.\n        mean (float | int): Distribution mean (\u03bc).\n        sigma (float | int): Distribution standard deviation (\u03c3).\n\n    Returns:\n        list[float] | int: The PDF values or -1 when \u03c3 \u2264 0.\n    \"\"\"\n    # 1. Validate sigma\n    if sigma <= 0:\n        return -1\n\n    # 2. Convert *x* to a NumPy array (works for scalars, lists, tuples, arrays)\n    arr = np.asarray(x, dtype=float)\n\n    # 3. Compute the Gaussian PDF\n    coeff = 1.0 / (np.sqrt(2 * np.pi) * sigma)\n    exponent = np.exp(-((arr - mean) ** 2) / (2 * sigma ** 2))\n    pdf_vals = coeff * exponent\n\n    # 4. Round to five decimals\n    pdf_vals = np.round(pdf_vals, 5)\n\n    # 5. Return as a Python list (ensure scalar input becomes a one-element list)\n    return pdf_vals.tolist() if pdf_vals.shape != () else [float(pdf_vals)]\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_step(theta: np.ndarray,\n              grad: np.ndarray,\n              m_prev: np.ndarray,\n              v_prev: np.ndarray,\n              t: int,\n              alpha: float = 0.01,\n              beta1: float = 0.9,\n              beta2: float = 0.99,\n              epsilon: float = 1e-9) -> tuple[list, list, list, int]:\n    \"\"\"Perform one iteration of the Adam optimisation algorithm\n    and return rounded Python lists.\n\n    All floating-point outputs are rounded to 6 decimal places.\n    \"\"\"\n    # --- 1. Update the biased first and second moment estimates ---\n    m_t = beta1 * m_prev + (1.0 - beta1) * grad\n    v_t = beta2 * v_prev + (1.0 - beta2) * (grad ** 2)\n\n    # --- 2. Bias-correct the moment estimates ---\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n\n    # --- 3. Parameter update ---\n    theta_new = theta - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # --- 4. Round everything to 6 decimal places and convert to lists ---\n    theta_new = np.round(theta_new, 6).tolist()\n    m_t       = np.round(m_t,       6).tolist()\n    v_t       = np.round(v_t,       6).tolist()\n\n    return theta_new, m_t, v_t, t + 1\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)            # deterministic initialisation\n    N, M = X.shape\n\n    # Random Gaussian initialisation of factor matrices\n    W = np.random.randn(N, K)\n    H = np.random.randn(K, M)\n\n    # Helper to compute regularised Frobenius loss\n    def loss(W, H):\n        resid = X - W @ H\n        return np.sum(resid ** 2) + alpha * (np.sum(W ** 2) + np.sum(H ** 2))\n\n    best_W, best_H = W.copy(), H.copy()\n    best_loss = loss(W, H)\n\n    for _ in range(max_iter):\n        # ---------- Update H (solve K\u00d7K system once, many RHS) ----------\n        gram_W = W.T @ W + alpha * np.eye(K)\n        WtX = W.T @ X                         # shape (K, M)\n        # Solve gram_W * H = WtX  ->  H = gram_W^{-1} WtX\n        H = np.linalg.solve(gram_W, WtX)\n\n        # ---------- Update W ----------\n        gram_H = H @ H.T + alpha * np.eye(K)\n        XHt = X @ H.T                         # shape (N, K)\n        # Solve W * gram_H^T = XHt   -> W = XHt * gram_H^{-1}\n        # Using solve with RHS transposed for stability\n        W = np.linalg.solve(gram_H.T, XHt.T).T\n\n        # ---------- Check loss / keep best ----------\n        current_loss = loss(W, H)\n        if current_loss < best_loss:\n            best_loss = current_loss\n            best_W, best_H = W.copy(), H.copy()\n\n        if current_loss <= tol:\n            break\n\n    X_hat = (best_W @ best_H).round(4).tolist()\n    return X_hat\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef _squared_euclidean(a: np.ndarray, b: np.ndarray) -> float:\n   Squared Euclidean distance between two 1-D arrays    return float(np.sum((a - b) ** 2))\n\n\ndef _kmeans_plus_plus_init(X: np.ndarray, K: int) -> np.ndarray:\n       Returns K initial centroids chosen with the K-Means++ strategy.\n       m, _ = X.shape\n    centroids: list[np.ndarray] = []\n\n    # 1. pick the first centroid completely at random\n    first_idx = random.randrange(m)\n    centroids.append(X[first_idx])\n\n    # 2. pick the remaining K-1 centroids\n    for _ in range(1, K):\n        # distances to the closest already\u2013chosen centroid\n        d2 = np.array(\n            [min(_squared_euclidean(x, c) for c in centroids) for x in X],\n            dtype=float,\n        )\n\n        total = d2.sum()\n        if total == 0:                           # all samples coincide\n            remaining = [i for i in range(m) if not any(np.all(X[i] == c) for c in centroids)]\n            pick_idx = random.choice(remaining) if remaining else random.randrange(m)\n            centroids.append(X[pick_idx])\n            continue\n\n        # pick according to the probability distribution \u221d distance\u00b2\n        probs = d2 / total\n        r = random.random()\n        pick_idx = np.searchsorted(np.cumsum(probs), r)\n        centroids.append(X[pick_idx])\n\n    return np.asarray(centroids, dtype=float)\n\n\ndef kmeans(\n    X: np.ndarray,\n    K: int,\n    max_iter: int = 100,\n    random_state: int | None = None,\n) -> list[list[float]]:\n   Perform K-Means clustering with K-Means++ initialisation (see doc-string).    if X.ndim != 2:\n        raise ValueError`X` must be a 2-D NumPy array    m, n = X.shape\n    if not 1 <= K <= m:\n        raise ValueError`K` must satisfy 1 \u2264 K \u2264 number of samples    # ------------------------------------------------------------------\n    # 1. deterministic behaviour (both RNGs!)\n    # ------------------------------------------------------------------\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    # ------------------------------------------------------------------\n    # 2. K-Means++ initialisation\n    # ------------------------------------------------------------------\n    centroids = _kmeans_plus_plus_init(X, K)\n\n    # ------------------------------------------------------------------\n    # 3. Lloyd refinement (assign-update loop)\n    # ------------------------------------------------------------------\n    labels = np.full(m, -1, dtype=int)  # previous assignment\n    for _ in range(max_iter):\n        # 3a. Assignment step\n        #    compute squared distances (broadcasting) \u2013 shape (m, K)\n        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n        new_labels = np.argmin(dists, axis=1)\n\n        # stop if nothing changed\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # 3b. Update step\n        for k in range(K):\n            members = X[labels == k]\n            if len(members):                       # non-empty cluster\n                centroids[k] = members.mean(axis=0)\n            # else keep centroid unchanged (as required)\n\n    # ------------------------------------------------------------------\n    # 4. rounding & sorting\n    # ------------------------------------------------------------------\n    centroids_rounded = [[round(float(val), 4) for val in c] for c in centroids]\n    centroids_sorted = sorted(centroids_rounded, key=lambda c: tuple(c))\n    return centroids_sorted\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (second import harmless but keeps the given header)\n\nTIME_STEPS = 20      # fixed length expected by the model\n\n\ndef string_to_int(text: str,\n                  time_steps: int,\n                  vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"\n    Encode `text` as a list of integer token-ids of length `time_steps`.\n\n    \u2022 Unknown characters \u2192 0\n    \u2022 If the text is shorter than `time_steps` \u2192 pad with 0 on the right\n    \u2022 If it is longer \u2192 truncate\n    \"\"\"\n    encoded = []\n\n    # encode each character (or use 0 if not in vocab)\n    for ch in text[:time_steps]:\n        encoded.append(vocabulary.get(ch, 0))\n\n    # right-pad with zeros if necessary\n    if len(encoded) < time_steps:\n        encoded.extend([0] * (time_steps - len(encoded)))\n\n    return encoded\n\n\ndef int_to_string(indices,\n                  inverse_vocab: dict[int, str]) -> str:\n    \"\"\"\n    Decode a sequence of ids back to a string.\n\n    \u2022 Padding tokens (0) are skipped.\n    \"\"\"\n    chars = []\n    for idx in indices:\n        if idx == 0:       # ignore padding\n            continue\n        # silently skip completely unknown ids\n        ch = inverse_vocab.get(int(idx))\n        if ch is not None:\n            chars.append(ch)\n    return ''.join(chars)\n\n\ndef run_example(model,\n                input_vocabulary: dict[str, int],\n                inv_output_vocabulary: dict[int, str],\n                text: str) -> str:\n    \"\"\"\n    End-to-end helper:\n      1. encode `text`\n      2. model.predict\n      3. argmax to obtain ids\n      4. decode back to string\n    \"\"\"\n    # 1) encode -------------------------------------------------------------\n    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)       # list[int]\n\n    # model expects a batch \u2192 shape (1, TIME_STEPS)\n    batch_input = np.asarray([encoded], dtype=np.int64)\n\n    # 2) run the model ------------------------------------------------------\n    predictions = model.predict(batch_input)     # shape (1, T, vocab_size)\n\n    # 3) choose the most likely id for every time step ---------------------\n    pred_ids = np.argmax(predictions, axis=-1)   # shape (1, T)\n    pred_ids = pred_ids[0]                       # drop batch dim \u2192 (T,)\n\n    # 4) decode -------------------------------------------------------------\n    decoded_text = int_to_string(pred_ids, inv_output_vocabulary)\n    return decoded_text\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\nimport re\nfrom collections import Counter\n\nclass Token:\n    def __init__(self, word):\n        self.count = 0\n        self.word = word\n\n    def __repr__(self):\n        return \"Token(word='{}', count={})\".format(self.word, self.count)\n\ndef tokenize_and_count(text: str) -> list[Token]:\n    \"\"\"Convert *text* into a list of Token objects with their frequencies.\n\n    1. Only contiguous A\u2013Z / a\u2013z sequences are considered words.\n    2. Comparison is case-insensitive; words are stored in lowercase.\n    3. Returned list is sorted by descending frequency, then lexicographically.\n    4. If no words are found, an empty list is returned.\n    \"\"\"\n    # 1. Extract words (alphabetic sequences) ignoring everything else.\n    words = re.findall(r\"[A-Za-z]+\", text)\n    if not words:                       # 5. No words -> empty list\n        return []\n\n    # 2. Normalise to lowercase and count frequencies.\n    counts = Counter(w.lower() for w in words)\n\n    # 3. Sort by (-frequency, word) as required.\n    sorted_items = sorted(counts.items(), key=lambda item: (-item[1], item[0]))\n\n    # 4. Build the list of Token objects.\n    tokens: list[Token] = []\n    for word, freq in sorted_items:\n        tok = Token(word)\n        tok.count = freq\n        tokens.append(tok)\n\n    return tokens\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function (or any callable) that receives a single input string and\n        returns the corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 this helper itself does not use it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused here but\n        kept for API consistency.\n    examples : iterable[str] or None\n        Collection of input strings.  If *None*, the function falls back to the\n        global constant ``EXAMPLES``.\n\n    Returns\n    -------\n    list[str]\n        All model predictions in the same order as the inputs.\n    \"\"\"\n    # Fall back to a global EXAMPLES constant when *examples* is None\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError as exc:  # pragma: no cover\n            raise NameError(\n                \"`examples` was None and global `EXAMPLES` is not defined.\"\n            ) from exc\n\n    predictions = []\n\n    # Iterate through every example, run the model, display and collect results\n    for example in examples:\n        # Call the helper that actually runs the model on one example\n        output_chars = run_example(\n            model,\n            input_vocabulary,\n            inv_output_vocabulary,\n            example,\n        )\n        # `run_example` is expected to return a list of characters.\n        # Concatenate them to form the full output string.\n        predicted_str = \"\".join(output_chars)\n\n        # Print the input / output pair for quick inspection\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_str}\")\n\n        # Keep track of the prediction\n        predictions.append(predicted_str)\n\n    return predictions\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef selu(x: np.ndarray, derivative: bool = False) -> list:\n   Scaled Exponential Linear Unit (SELU).\n\n    Applies SELU activation or its derivative element-wise to *x*.\n\n    Args:\n        x: A NumPy ndarray with real values.\n        derivative: If False (default) compute SELU; if True compute d(SELU)/dx.\n\n    Returns:\n        Nested Python list mirroring *x*'s shape with all values rounded to\n        six decimal places.\n       # SELU constants (from the original paper)\n    alpha  = 1.6732632423543772848170429916717\n    lam    = 1.0507009873554804934193349852946\n\n    if derivative:                                   # d(SELU)/dx\n        result = np.where(x >= 0, lam,\n                          lam * alpha * np.exp(x))\n    else:                                            # SELU activation\n        result = np.where(x >= 0, lam * x,\n                          lam * alpha * (np.exp(x) - 1))\n\n    # Round to six decimals and convert to nested Python lists\n    return np.round(result, 6).tolist()\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (kept to follow the given stub)\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\"\"\"\n    \n    # Ensure proper 2-D shapes:  (m, 1) for y and (n, 1) for w\n    y = y.reshape(-1, 1)\n    w = w.reshape(-1, 1)\n    \n    # Forward pass \u2013 probabilities\n    z = X @ w                               # (m, 1)\n    p = 1.0 / (1.0 + np.exp(-z))            # \u03c3(z)\n    \n    # Numerical safety for the logs\n    eps = 1e-20\n    p_safe = np.clip(p, eps, 1.0 - eps)\n    \n    # Average binary cross-entropy loss\n    loss = -np.mean(y * np.log(p_safe) + (1 - y) * np.log(1 - p_safe))\n    loss = round(float(loss), 4)            # convert to Python float & round\n    \n    # Gradient\n    m = X.shape[0]\n    grad = (X.T @ (p - y)) / m              # (n, 1)\n    grad = np.round(grad, 4).tolist()       # list of lists, 4-dp\n    \n    return loss, grad\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse_criterion(y: np.ndarray, splits: list[np.ndarray]) -> float:\n   Calculate the reduction in mean-squared error (variance) produced by a split.\n    \n    Args:\n        y: 1-D NumPy array with the parent-node target values.\n        splits: List of 1-D NumPy arrays \u2013 the target values that end up in each\n                child after the split.\n    \n    Returns:\n        \u0394MSE (parent MSE \u2212 weighted child MSE) rounded to 4 decimal places.\n       if y.size == 0:\n        return 0.0                           # nothing to split\n    \n    # Parent node MSE (population variance)\n    parent_mse = np.var(y, ddof=0)\n    \n    # Weighted MSE of all children\n    N = y.size\n    children_mse = 0.0\n    for s in splits:\n        if s.size == 0:                      # empty child: contributes nothing\n            continue\n        weight = s.size / N\n        children_mse += weight * np.var(s, ddof=0)\n    \n    delta_mse = parent_mse - children_mse\n    return round(float(delta_mse), 4)\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train:  np.ndarray,\n    X_test:  np.ndarray,\n    n_clf:   int = 5\n) -> list[int]:\n    \"\"\"AdaBoost with decision stumps (1-level trees).\n\n    Parameters\n    ----------\n    X_train : (m, n) array\n        Training features.\n    y_train : (m,) array\n        Labels \u2013 must be -1 or 1.\n    X_test  : (k, n) array\n        Samples to classify.\n    n_clf   : int, default=5\n        Number of weak learners to build (min=1).\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 / 1) for every row of `X_test`.\n    \"\"\"\n    # -----------  preparation ------------------------------------------------\n    if n_clf < 1:\n        n_clf = 1\n    m, n_features = X_train.shape\n    # sample weights\n    D = np.full(m, 1.0 / m)\n\n    # will hold (feature, threshold, polarity, alpha)\n    stumps: list[tuple[int, float, int, float]] = []\n\n    # -----------  boosting rounds -------------------------------------------\n    for _ in range(n_clf):\n        best_feature   = None\n        best_threshold = None\n        best_polarity  = 1\n        best_error     = np.inf\n        best_pred      = None\n\n        # search the best decision stump\n        for j in range(n_features):\n            col = X_train[:, j]\n            thresholds = np.unique(col)\n            for thr in thresholds:\n                for polarity in (1, -1):\n                    preds = np.ones(m, dtype=int)\n                    # polarity * x < polarity * thr  -> predict  1\n                    preds[polarity * col >= polarity * thr] = -1\n                    err = np.sum(D[preds != y_train])\n\n                    if err < best_error:\n                        best_error     = err\n                        best_feature   = j\n                        best_threshold = thr\n                        best_polarity  = polarity\n                        best_pred      = preds\n\n        # numerical stability\n        eps   = 1e-10\n        error = max(best_error, eps)\n        alpha = 0.5 * np.log((1 - error) / error)\n\n        # store the stump\n        stumps.append((best_feature, best_threshold, best_polarity, alpha))\n\n        # update sample weights\n        D *= np.exp(-alpha * y_train * best_pred)\n        D /= D.sum()          # normalise\n\n    # -----------  prediction on X_test --------------------------------------\n    agg = np.zeros(X_test.shape[0])\n    for feat, thr, pol, a in stumps:\n        pred = np.ones(X_test.shape[0], dtype=int)\n        pred[pol * X_test[:, feat] >= pol * thr] = -1\n        agg += a * pred\n\n    y_hat = np.where(agg >= 0, 1, -1)\n\n    return y_hat.tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sgd_update(params: list, grads: list, alpha: float = 0.01) -> list:\n   One step of Stochastic Gradient Descent.\n\n    Parameters\n    ----------\n    params : list[np.ndarray]\n        Current model parameters.\n    grads : list[np.ndarray]\n        Gradients for each parameter.\n    alpha : float, optional\n        Learning-rate, by default 0.01.\n\n    Returns\n    -------\n    list\n        List of updated parameters (Python lists) rounded to 4 decimals.\n       # Basic validity checks\n    if not params or not grads or len(params) != len(grads):\n        return []\n\n    updated_params = []\n    for p, g in zip(params, grads):\n        # Compute the SGD update (create a NEW array \u2013 no in-place modification)\n        new_p = p - alpha * g\n\n        # Round to 4 decimal places and convert to Python list\n        new_p = np.round(new_p, 4).tolist()\n        updated_params.append(new_p)\n\n    return updated_params\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef linear_regression(X: list[list[int | float]],\n                      y: list[int | float],\n                      n_iterations: int,\n                      learning_rate: float) -> list[float]:\n    \"\"\"Learn linear regression weights with batch gradient descent.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # 1. Basic dimensionality checks\n    # ------------------------------------------------------------------ #\n    m = len(X)\n    if m == 0 or m != len(y):\n        return -1  # incompatible number of rows\n\n    # ------------------------------------------------------------------ #\n    # 2. Convert input lists to numpy arrays (ensure 2-D for X)\n    # ------------------------------------------------------------------ #\n    try:\n        X_mat = np.asarray(X, dtype=float)\n        if X_mat.ndim == 1:                      # single feature given as flat list\n            X_mat = X_mat.reshape(-1, 1)\n        y_vec = np.asarray(y, dtype=float).reshape(-1, 1)\n    except Exception:                            # non-numeric entries etc.\n        return -1\n\n    if X_mat.shape[0] != m:                      # further safety check\n        return -1\n\n    # ------------------------------------------------------------------ #\n    # 3. Add bias column  (column of ones as first column)\n    # ------------------------------------------------------------------ #\n    X_aug = np.hstack((np.ones((m, 1)), X_mat))\n    n_features_aug = X_aug.shape[1]\n\n    # ------------------------------------------------------------------ #\n    # 4. Random weight initialisation in [-1/\u221aN , 1/\u221aN]\n    # ------------------------------------------------------------------ #\n    limit = 1 / math.sqrt(n_features_aug)\n    w = np.random.uniform(-limit, limit, size=(n_features_aug, 1))\n\n    # ------------------------------------------------------------------ #\n    # 5. Batch Gradient Descent\n    # ------------------------------------------------------------------ #\n    for _ in range(max(0, n_iterations)):        # allow n_iterations == 0\n        error = X_aug @ w - y_vec                # (m \u00d7 1)\n        grad  = (X_aug.T @ error) / m            # (N \u00d7 1)\n        w -= learning_rate * grad\n\n    # ------------------------------------------------------------------ #\n    # 6. Prepare and return the result\n    # ------------------------------------------------------------------ #\n    w_rounded = np.round(w.flatten(), 4)         # round to 4 decimals\n    return [float(val) for val in w_rounded]\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_weights(\n    X: list[list[int | float]], \n    y: list[int | float]\n) -> list[float]:\n   Return the ordinary least-squares weight vector for Linear Regression.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        2-D list where each inner list contains the feature values for one sample.\n    y : list[int | float]\n        1-D list with the target value corresponding to each sample.\n\n    Returns\n    -------\n    list[float]\n        Weight vector [w0, w1, \u2026, wd] rounded to 4 decimal places,\n        where w0 is the intercept term.\n       # 1. Convert input lists to NumPy arrays (float dtype)\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float).ravel()        # Flatten y to 1-D\n\n    # 2. Augment X with a leading column of ones (intercept)\n    ones = np.ones((X_arr.shape[0], 1), dtype=float)\n    X_aug = np.hstack((ones, X_arr))\n\n    # 3. Compute optimal weights using the Moore\u2013Penrose pseudo-inverse\n    w = np.linalg.pinv(X_aug) @ y_arr                 # Shape: (d+1,)\n\n    # 4. Round each weight to 4 decimal places and convert back to Python list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one_hot_encoding(y: np.ndarray) -> list[list[int]]:\n    \"\"\"Convert a 1-D array of categorical values to a one-hot matrix.\n\n    The unique categories are detected automatically, sorted in\n    ascending/alphabetical order, and each element of *y* is represented\n    by a binary row vector.\n\n    Args\n    ----\n    y : np.ndarray (or array-like)\n        One-dimensional sequence of categorical values (ints, strings, \u2026).\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded matrix containing only 0 and 1 as plain Python\n        lists.\n    \"\"\"\n    # Convert to a NumPy array and make sure it is one-dimensional\n    y_arr = np.asarray(y)\n    if y_arr.ndim != 1:\n        raise ValueError(\"Input y must be one-dimensional\")\n\n    # Handle the trivial empty input early\n    if y_arr.size == 0:\n        return []\n\n    # Automatically find and sort the unique categories\n    categories = np.unique(y_arr)            # already sorted\n    n_classes  = len(categories)\n    n_samples  = y_arr.size\n\n    # Map each category to its column index in the one-hot matrix\n    cat_to_index = {cat: idx for idx, cat in enumerate(categories)}\n\n    # Build the one-hot encoded result\n    one_hot = [[0] * n_classes for _ in range(n_samples)]\n    for row, value in enumerate(y_arr):\n        col = cat_to_index[value]\n        one_hot[row][col] = 1\n\n    return one_hot\n```\n\nExample usage\n-------------\n```python\n>>> y = np.array(['dog', 'cat', 'bird', 'cat'])\n>>> one_hot_encoding(y)\n[[0, 0, 1],   # bird\n [0, 1, 0],   # cat\n [1, 0, 0],   # dog\n [0, 1, 0]]   # cat again\n```\n\nThe function uses only the standard NumPy library for convenience; the returned value is an ordinary Python list-of-lists containing integers 0 and 1, satisfying the stated constraints."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef polynomial_kernel(X: list[list[int | float]],\n                      Y: list[list[int | float]] | None = None,\n                      d: int = 3,\n                      gamma: float | None = None,\n                      c0: float = 1) -> list[list[float]]:\n   Compute the degree-d polynomial kernel between all rows of X and Y.\n\n    Args:\n        X: First data matrix as a list-of-lists with shape (N, C).\n        Y: Optional second data matrix; if None, defaults to X.\n        d: Degree of the polynomial kernel.\n        gamma: Scaling factor; if None, defaults to 1 / C.\n        c0: Bias term added inside the polynomial.\n\n    Returns:\n        Gram matrix as a nested Python list rounded to 4 decimals.\n       # Convert inputs to NumPy arrays of dtype float\n    X_np = np.asarray(X, dtype=float)\n    Y_np = X_np if Y is None else np.asarray(Y, dtype=float)\n\n    # Number of features (columns)\n    _, C = X_np.shape\n\n    # Default gamma\n    if gamma is None:\n        gamma = 1.0 / C\n\n    # Compute the dot-product matrix\n    dot_products = X_np @ Y_np.T                # Shape: (N, M)\n\n    # Apply the polynomial kernel formula\n    K = (gamma * dot_products + c0) ** d\n\n    # Round to 4 decimals and convert back to nested Python list\n    return np.round(K, 4).tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\ndef confusion_matrix(y_true: list, y_pred: list) -> list:\n   Build a confusion matrix for the given true and predicted labels.\n\n    Parameters\n    ----------\n    y_true : list\n        Ground-truth class labels.\n    y_pred : list\n        Predicted class labels. Must have the same length as `y_true`.\n\n    Returns\n    -------\n    list\n        2-D list representing the confusion matrix.  The element at row *i*\n        and column *j* is the number of instances with\n        true class equal to the *i-th* sorted unique label and\n        predicted class equal to the *j-th* sorted unique label.\n\n        If the input lengths differ the function returns -1.\n       # -------------------------------------------------------------\n    # 1. Sanity check: both sequences must have the same length.\n    # -------------------------------------------------------------\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # -------------------------------------------------------------\n    # 2. Determine all distinct class labels and sort them.\n    # -------------------------------------------------------------\n    classes = sorted(set(y_true).union(y_pred))       # list of unique labels\n    n_classes = len(classes)\n\n    # Fast lookup from label value -> matrix index.\n    label_to_index = {label: idx for idx, label in enumerate(classes)}\n\n    # -------------------------------------------------------------\n    # 3. Build an empty C \u00d7 C matrix filled with zeros.\n    # -------------------------------------------------------------\n    matrix = [[0 for _ in range(n_classes)] for _ in range(n_classes)]\n\n    # -------------------------------------------------------------\n    # 4. Populate the matrix.\n    # -------------------------------------------------------------\n    for t, p in zip(y_true, y_pred):\n        i = label_to_index[t]      # row   index (true label)\n        j = label_to_index[p]      # column index (predicted label)\n        matrix[i][j] += 1\n\n    # -------------------------------------------------------------\n    # 5. Return the plain Python list of lists.\n    # -------------------------------------------------------------\n    return matrix\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Fisher Linear Discriminant Analysis (LDA).\n\n    Parameters\n    ----------\n    X : (n_samples, n_features) ndarray\n        Input data.\n    y : (n_samples,) ndarray\n        Integer class labels.\n    n_components : int\n        Number of discriminant directions to keep.\n\n    Returns\n    -------\n    list[list[float]]\n        Projected data (rounded to 4 decimals) or an empty list.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # ------------------------------------------------------------------ #\n    # 0. Trivial case\n    # ------------------------------------------------------------------ #\n    if n_components <= 0:\n        return []\n\n    # ------------------------------------------------------------------ #\n    # 1. Compute class means, global mean, and within-class scatter S_W\n    # ------------------------------------------------------------------ #\n    classes        = np.unique(y)\n    n_classes      = classes.size\n    global_mean    = X.mean(axis=0)\n\n    S_W = np.zeros((n_features, n_features), dtype=float)\n    S_B = np.zeros((n_features, n_features), dtype=float)\n\n    for cls in classes:\n        X_c   = X[y == cls]\n        mean_c = X_c.mean(axis=0)\n        # Within-class scatter\n        X_centered = X_c - mean_c\n        S_W += X_centered.T @ X_centered\n        # Between-class scatter\n        n_c  = X_c.shape[0]\n        mean_diff = (mean_c - global_mean).reshape(-1, 1)        # column\n        S_B += n_c * (mean_diff @ mean_diff.T)\n\n    # ------------------------------------------------------------------ #\n    # 2. Generalised eigen-problem  pinv(S_W) S_B  w = \u03bb w\n    # ------------------------------------------------------------------ #\n    SW_inv = np.linalg.pinv(S_W)\n    A      = SW_inv @ S_B\n\n    # Force exact symmetry for numerical stability (optional but safe)\n    A = 0.5 * (A + A.T)\n\n    eigvals, eigvecs = np.linalg.eigh(A)          # ascending order\n    sorted_idx = eigvals.argsort()[::-1]          # descending\n    eigvals  = eigvals[sorted_idx]\n    eigvecs  = eigvecs[:, sorted_idx]\n\n    # Keep only the requested number of components\n    n_components = min(n_components, n_features, eigvecs.shape[1])\n    if n_components == 0:\n        return []\n\n    W = eigvecs[:, :n_components]\n\n    # ------------------------------------------------------------------ #\n    # 3. Deterministic sign: flip each eigenvector if first non-zero\n    #    element is negative\n    # ------------------------------------------------------------------ #\n    for k in range(n_components):\n        w = W[:, k]\n        # Find first non-zero entry\n        nz_idx = np.flatnonzero(w)\n        if nz_idx.size and w[nz_idx[0]] < 0:\n            W[:, k] = -w\n\n    # ------------------------------------------------------------------ #\n    # 4. Project the data and round\n    # ------------------------------------------------------------------ #\n    X_proj = X @ W\n    X_proj = np.round(X_proj, 4)\n\n    return X_proj.tolist()\n```"}
{"task_id": 129, "completion_id": 0, "completion": "```python\nfrom itertools import islice, cycle\n\ndef cycle_sequence(sequence: list|tuple, samples: int) -> list:\n    \"\"\"Return the first *samples* items from an infinite cycle over *sequence*.\n\n    Args:\n        sequence (list | tuple): Finite input sequence.\n        samples (int): Number of items to return from the infinite cycle.\n\n    Returns:\n        list: A list containing *samples* items collected by looping over\n              *sequence* repeatedly. If *sequence* is empty or *samples*\n              is not positive, an empty list is returned.\n    \"\"\"\n    # Guard-clauses for the two trivial \u201cnothing to do\u201d situations\n    if samples <= 0 or not sequence:\n        return []\n\n    # islice limits how many elements we read from the infinite cycle\n    return list(islice(cycle(sequence), samples))\n```\n\nExplanation (concise):\n1. If the caller requests zero/negative samples or provides an empty sequence, the function immediately returns an empty list.\n2. Otherwise, `itertools.cycle` creates an endless iterator that repeats the original sequence.\n3. `itertools.islice` limits that endless iterator to the first `samples` items.\n4. The sliced items are collected into a list and returned, satisfying the output-type requirement."}
{"task_id": 130, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, List, Tuple\n\ndef outcome_probs(transitions: List[Tuple[Any, Any, Any, Any]],\n                  state: Any,\n                  action: Any) -> List[List[float]]:\n    \"\"\"Return empirical outcome probabilities for a given (state, action).\n\n    transitions: list of (state, action, reward, next_state) tuples.\n    state:       queried state.\n    action:      queried action.\n\n    The function returns a list of [next_state, probability] pairs sorted by\n    next_state.  Probabilities are rounded to 4 decimal places.  If the\n    pair never occurred, an empty list is returned.\n    \"\"\"\n    # count how often each next_state followed the queried (state, action)\n    counts: Counter = Counter(\n        next_state\n        for s, a, _, next_state in transitions\n        if s == state and a == action\n    )\n\n    total = sum(counts.values())\n    if total == 0:                      # pair never occurred\n        return []\n\n    # build probability list and sort by next_state\n    distribution = [\n        [ns, round(cnt / total, 4)]     # probability rounded to 4 decimals\n        for ns, cnt in counts.items()\n    ]\n    distribution.sort(key=lambda x: x[0])\n    return distribution\n```"}
{"task_id": 131, "completion_id": 0, "completion": "```python\ndef get_index(uid: int, i: int):\n    \"\"\"Return the element at position *i* of the sequence identified by *uid*.\n\n    The global list ``_SHARED_SEQUENCES`` collects all sequences that may be\n    accessed concurrently by different parts of the program.\n\n    If *uid* or *i* is invalid the function returns ``None`` instead of raising\n    an exception.\n\n    Args:\n        uid: Identifier of the desired sequence (must be a non-negative index\n             inside ``_SHARED_SEQUENCES``).\n        i:   Position inside the selected sequence (supports the usual negative\n             indexing semantics of Python).\n\n    Returns:\n        The requested element, or ``None`` if the access is invalid.\n    \"\"\"\n    # 1) Validate *uid* first.  Negative *uid* values must be rejected \u2013\n    #    we are *not* supposed to use Python\u2019s negative indexing here.\n    try:\n        if uid < 0:\n            return None\n        seq = _SHARED_SEQUENCES[uid]\n    except (IndexError, NameError):\n        #   - IndexError: uid outside the range of the shared list\n        #   - NameError : _SHARED_SEQUENCES not defined (paranoia guard)\n        return None\n\n    # 2) Normalise *i* exactly like Python does for lists and check bounds.\n    seq_len = len(seq)\n    if i < 0:\n        i += seq_len\n\n    if i < 0 or i >= seq_len:\n        return None\n\n    # 3) The access is valid \u2013 return the requested element.\n    return seq[i]\n```"}
{"task_id": 132, "completion_id": 0, "completion": "```python\nimport numbers\nimport numpy as np\n\ndef linear(z):\n    \"\"\"Linear (identity) activation function.\n\n    Args:\n        z: Scalar (int/float), list, tuple or NumPy ndarray.\n\n    Returns:\n        The same value(s) as *z*, obeying the rules:\n            \u2022 Scalars are returned unchanged.\n            \u2022 Non-scalar inputs are returned as a NumPy ndarray.\n    \"\"\"\n    # Scalar (plain Python or NumPy scalar)\n    if isinstance(z, numbers.Number) or np.isscalar(z):\n        return z\n\n    # Already a NumPy array \u2192 return the very same object (no copy)\n    if isinstance(z, np.ndarray):\n        return z\n\n    # Python list / tuple \u2192 convert to ndarray\n    if isinstance(z, (list, tuple)):\n        return np.asarray(z)\n\n    # Anything else is outside the contract\n    raise TypeError(\n        \"Unsupported type for linear(): expected scalar, list, tuple, or np.ndarray.\"\n    )\n```"}
{"task_id": 133, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one_hot_targets(X_train: list[list[int]], vocab_length: int) -> list[list[list[int]]]:\n    \"\"\"Convert integer-encoded sequences into a 3-D one-hot representation.\n\n    Parameters\n    ----------\n    X_train : list[list[int]]\n        A batch of sequences where each element is an integer token index.\n    vocab_length : int\n        The size of the vocabulary (number of different tokens).\n\n    Returns\n    -------\n    list[list[list[int]]]\n        A nested (pure-Python) list with shape\n        (batch_size, time_steps, vocab_length) containing the one-hot\n        representations, or -1 if an invalid token index is encountered.\n    \"\"\"\n    # ---- basic sanity checks -------------------------------------------------\n    if not isinstance(vocab_length, int) or vocab_length <= 0:\n        return -1\n\n    # Pre-build an identity matrix once; each row is a one-hot vector.\n    eye = np.eye(vocab_length, dtype=int)\n\n    Y_train = []\n    try:\n        for seq in X_train:\n            # Convert current sequence to a NumPy array to use vectorised checks\n            seq_arr = np.asarray(seq, dtype=int)\n\n            # Validate indices; if any are out of range, abort with -1\n            if (seq_arr < 0).any() or (seq_arr >= vocab_length).any():\n                return -1\n\n            # Fancy-indexing directly gives us the one-hot rows for this sequence\n            Y_train.append(eye[seq_arr].tolist())\n\n    except Exception:                     # covers wrong dtypes or nested levels\n        return -1\n\n    return Y_train\n```"}
{"task_id": 134, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_arm(payoff_probs: list[float]) -> tuple[float, int]:\n   Find the arm with the highest expected reward in a Bernoulli bandit.\n\n    Parameters\n    ----------\n    payoff_probs : list[float]\n        A list in which element k is the success probability p_k of arm k.\n        Every probability must lie in the closed interval [0, 1].\n\n    Returns\n    -------\n    tuple\n        (max_expected_reward, best_arm_index)\n\n        * max_expected_reward : float\n            The largest probability found in `payoff_probs`.\n        * best_arm_index : int\n            The (0-based) index of the arm achieving that probability.  If\n            several arms share the maximum probability, the smallest index\n            is returned.\n\n        If the input list is empty, or if any probability is outside the\n        interval [0, 1], the function returns (-1.0, -1).\n       # ---------- Validation --------------------------------------------------\n    if not payoff_probs:                     # empty list\n        return -1.0, -1\n\n    for p in payoff_probs:                   # range and type check\n        # Reject non-numeric values or values outside [0, 1]\n        if not isinstance(p, (int, float)) or p < 0 or p > 1:\n            return -1.0, -1\n\n    # ---------- Locate best arm --------------------------------------------\n    # `max` is stable: the first occurrence of the maximum is chosen,\n    # which gives the smallest index when several arms tie.\n    best_idx = max(range(len(payoff_probs)), key=payoff_probs.__getitem__)\n    best_p   = float(payoff_probs[best_idx])\n\n    return best_p, best_idx\n```"}
{"task_id": 135, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_predict(\n        X_train: list[list[int | float]],\n        y_train: list[int | float],\n        X_test: list[list[int | float]]) -> list[float]:\n    \"\"\"Fit an OLS model with an intercept and return rounded predictions.\n       If (X\u1d40X) is singular, return -1.\n    \"\"\"\n    # Convert to float NumPy arrays\n    X_tr = np.asarray(X_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=float)\n    X_te = np.asarray(X_test,  dtype=float)\n\n    # Add intercept term (column of ones)\n    X_tr = np.hstack((np.ones((X_tr.shape[0], 1)), X_tr))\n    X_te = np.hstack((np.ones((X_te.shape[0], 1)), X_te))\n\n    # Compute (X\u1d40X) and check for singularity\n    XtX = X_tr.T @ X_tr\n    if np.linalg.matrix_rank(XtX) < XtX.shape[0]:   # singular / non-invertible\n        return -1\n\n    # Solve (X\u1d40X)\u03b8 = X\u1d40y  for \u03b8  using a stable linear solver\n    theta = np.linalg.solve(XtX, X_tr.T @ y_tr)\n\n    # Predict on test data\n    y_pred = X_te @ theta\n\n    # Round to 4 decimal places and return as Python list\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 136, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(z: np.ndarray, axis: int = -1) -> list:\n   Compute the numerically-stable softmax of *z* along *axis*.\n\n    Steps\n    -----\n    1. Shift by subtracting the maximum value along the chosen axis.\n    2. Exponentiate the shifted values.\n    3. Normalize by the sum of exponentials along the same axis.\n    4. Round every probability to four decimal places.\n    5. Return the result as a (nested) Python list.\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array of raw scores.\n    axis : int, default -1\n        Axis along which to apply softmax.\n\n    Returns\n    -------\n    list\n        Softmax probabilities (same shape as *z*), rounded to 4 decimals.\n       # 1. Ensure axis is valid (supports negative indexing like NumPy)\n    axis = axis if axis >= 0 else z.ndim + axis\n\n    # 2. Numerically-stable shift: subtract the max along *axis*\n    z_max = np.max(z, axis=axis, keepdims=True)\n    z_shifted = z - z_max\n\n    # 3. Exponentiate the shifted values\n    exp_z = np.exp(z_shifted)\n\n    # 4. Sum of exponentials along *axis*\n    sum_exp = np.sum(exp_z, axis=axis, keepdims=True)\n\n    # 5. Normalize\n    softmax_vals = exp_z / sum_exp\n\n    # 6. Round to four decimal places\n    softmax_vals = np.round(softmax_vals, 4)\n\n    # 7. Convert to Python list\n    return softmax_vals.tolist()\n```"}
{"task_id": 138, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true: list, y_pred: list) -> float:\n    \"\"\"Compare y_true to y_pred and return the classification accuracy.\n\n    The function must:\n    \u2022 Return -1 if the two input sequences are not of the same non-zero length.\n    \u2022 Otherwise compute the proportion of positions in which the corresponding\n      elements are equal and round the result to four decimal places.\n\n    Args:\n        y_true (list): Ground-truth labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: Accuracy rounded to four decimals, or -1 for invalid inputs.\n    \"\"\"\n    # Validate lengths\n    if not y_true or not y_pred or len(y_true) != len(y_pred):\n        return -1\n\n    # Count matching positions\n    matches = sum(1 for true_val, pred_val in zip(y_true, y_pred) if true_val == pred_val)\n\n    # Compute accuracy\n    accuracy = matches / len(y_true)\n\n    # Round to four decimal places\n    return round(accuracy, 4)\n```"}
{"task_id": 139, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_activation(z):\n    \"\"\"Compute the hyperbolic tangent (tanh) for every element of the input.\n\n    Args:\n        z (list[int | float] | np.ndarray): A one-dimensional iterable of\n            numeric values.\n\n    Returns:\n        list[float]: Tanh applied element-wise, rounded to four decimals.\n    \"\"\"\n    # Convert to NumPy array for convenient, element-wise computation\n    arr = np.asarray(z, dtype=float)\n\n    # Handle empty input early\n    if arr.size == 0:\n        return []\n\n    # Numerically stable tanh: tanh(z) = 2 / (1 + exp(-2z)) - 1\n    tanh_vals = 2.0 / (1.0 + np.exp(-2.0 * arr)) - 1.0\n\n    # Round to four decimals and return as regular Python list\n    return np.round(tanh_vals, 4).tolist()\n```"}
{"task_id": 140, "completion_id": 0, "completion": "```python\nfrom collections import deque\nimport math\n\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Return the minimum number of minutes needed to reach the lower-right\n    corner of a square maze while avoiding a spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        n strings of length n; characters are\n        '.' \u2013 free cell\n        '#' \u2013 wall\n        'F' \u2013 fire source present at minute 0\n\n    Returns\n    -------\n    int\n        Minimum travel time, or -1 if the goal can\u2019t be reached safely.\n    \"\"\"\n    n = len(grid)\n    if n == 0:                       # empty input\n        return -1\n\n    # -----------------------------------------------------------------\n    # 1. Multi-source BFS: earliest minute at which fire reaches a cell\n    # -----------------------------------------------------------------\n    INF = math.inf\n    fire_t = [[INF] * n for _ in range(n)]\n    fq = deque()\n\n    for r in range(n):\n        for c in range(n):\n            if grid[r][c] == 'F':\n                fire_t[r][c] = 0\n                fq.append((r, c))\n\n    DIR = ((1, 0), (-1, 0), (0, 1), (0, -1))\n\n    while fq:\n        r, c = fq.popleft()\n        for dr, dc in DIR:\n            nr, nc = r + dr, c + dc\n            if (\n                0 <= nr < n\n                and 0 <= nc < n\n                and grid[nr][nc] != '#'\n                and fire_t[nr][nc] == INF\n            ):\n                fire_t[nr][nc] = fire_t[r][c] + 1\n                fq.append((nr, nc))\n\n    # -----------------------------------------------------------------\n    # 2. Quick impossibility checks\n    # -----------------------------------------------------------------\n    start_blocked = grid[0][0] != '.' or fire_t[0][0] == 0\n    goal_blocked  = grid[n - 1][n - 1] == '#' or grid[n - 1][n - 1] == 'F'\n\n    if start_blocked or goal_blocked:\n        return -1\n\n    # single-cell maze\n    if n == 1:\n        return 0\n\n    # -----------------------------------------------------------------\n    # 3. BFS for the agent\n    #    Agent arrives at minute t after the fire has already spread t times.\n    #    So a move to (r,c) at minute t is legal only if t < fire_t[r][c].\n    # -----------------------------------------------------------------\n    aq = deque([(0, 0, 0)])          # row, col, current minute\n    seen = [[False] * n for _ in range(n)]\n    seen[0][0] = True\n\n    while aq:\n        r, c, t = aq.popleft()\n\n        for dr, dc in DIR:\n            nr, nc = r + dr, c + dc\n            nt = t + 1               # agent would arrive at this minute\n\n            if not (0 <= nr < n and 0 <= nc < n):\n                continue\n            if grid[nr][nc] != '.':\n                continue\n            if seen[nr][nc]:\n                continue\n            if nt >= fire_t[nr][nc]:  # cell already burning (or ignites now)\n                continue\n\n            if nr == n - 1 and nc == n - 1:\n                return nt            # reached goal safely\n            seen[nr][nc] = True\n            aq.append((nr, nc, nt))\n\n    return -1                         # impossible\n```\n\nExplanation of the algorithm\n1. Spread of fire  \n   \u2022 Treat every initial \u2018F\u2019 as a source and perform one multi-source BFS.  \n   \u2022 `fire_t[r][c]` stores the first minute the fire occupies `(r,c)`.  \n   \u2022 Walls (`#`) are never entered, their time stays `\u221e`.\n\n2. Agent\u2019s movement  \n   \u2022 A second BFS explores the agent\u2019s positions minute by minute.  \n   \u2022 At minute `t` the fire has already spread `t` times, therefore a cell is usable only if `t < fire_t[cell]` (strict inequality because fire moves first each minute).  \n   \u2022 When the goal cell is dequeued the corresponding `t` is the minimal travel time.\n\nBoth BFS traversals visit each cell at most once, giving overall time complexity `O(n\u00b2)` and memory `O(n\u00b2)`."}
{"task_id": 141, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np                 # (kept \u2013 required by the statement)\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n   k-Nearest Neighbours classifier.\n\n    Parameters\n    ----------\n    X_train : (n_train, n_features) ndarray\n        Training patterns.\n    y_train : (n_train,) ndarray\n        Training labels.\n    X_test  : (n_test , n_features) ndarray\n        Unlabelled patterns to classify.\n    k : int\n        Number of neighbours.\n    metric : {'euclidean', 'manhattan', 'cosine'}, optional\n        Distance metric. Any other value falls back to Euclidean.\n\n    Returns\n    -------\n    (n_test,) ndarray\n        Predicted labels for X_test.\n       # ------------------------------------------------------------------ helpers\n    def _euclidean(a, b):\n        # ||x-y|| = sqrt(||x||\u00b2+||y||\u00b2-2x\u00b7y)\n        a2 = np.sum(a * a, axis=1)            # (n_test,)\n        b2 = np.sum(b * b, axis=1)            # (n_train,)\n        # broadcasting -> (n_test, n_train)\n        d2 = a2[:, None] + b2[None, :] - 2.0 * a @ b.T\n        np.maximum(d2, 0, out=d2)             # numerical safety\n        return np.sqrt(d2, out=d2)\n\n    def _manhattan(a, b):\n        # |x-y|\u2081\n        return np.sum(np.abs(a[:, None, :] - b[None, :, :]), axis=2)\n\n    def _cosine(a, b):\n        # 1 - (x\u00b7y)/(||x||\u00b7||y||)\n        a_norm = np.linalg.norm(a, axis=1)\n        b_norm = np.linalg.norm(b, axis=1)\n        a_safe = np.where(a_norm == 0, 1, a_norm)\n        b_safe = np.where(b_norm == 0, 1, b_norm)\n        a_n = a / a_safe[:, None]\n        b_n = b / b_safe[:, None]\n        cos_sim = a_n @ b_n.T                       # (n_test, n_train)\n        return 1.0 - cos_sim\n\n    def _distance(a, b, m):\n        if m == 'euclidean':\n            return _euclidean(a, b)\n        if m == 'manhattan':\n            return _manhattan(a, b)\n        if m == 'cosine':\n            return _cosine(a, b)\n        # fallback\n        return _euclidean(a, b)\n\n    def _majority(labels):\n       Return the label occurring most often \u2013 smallest label breaks ties        uniq, counts = np.unique(labels, return_counts=True)\n        winners = uniq[counts == counts.max()]\n        return winners.min()                     # smallest label in case of tie\n    # ------------------------------------------------------------------ body\n    if k <= 0:\n        raise ValueError('k must be positive')\n    if k > len(y_train):\n        k = len(y_train)                         # use all samples if k is large\n\n    # Ensure float computations\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test , dtype=float)\n\n    # Distance matrix (n_test, n_train)\n    dist = _distance(X_test, X_train, metric)\n\n    # Indices of k nearest neighbours for every test sample\n    # argpartition is O(n) and keeps the k smallest in front (unordered),\n    # which is enough for voting.\n    knn_idx = np.argpartition(dist, kth=k-1, axis=1)[:, :k]   # (n_test, k)\n\n    # Majority voting\n    preds = np.empty(X_test.shape[0], dtype=y_train.dtype)\n    for i, neighbours in enumerate(knn_idx):\n        neighbour_labels = y_train[neighbours]\n        preds[i] = _majority(neighbour_labels)\n\n    return preds\n```"}
{"task_id": 143, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(x, alpha: float = 0.2, derivative: bool = False):\n   Compute the Leaky ReLU activation or its derivative.\n\n    Parameters\n    ----------\n    x : numpy.ndarray | list | tuple\n        Input data of arbitrary shape. If a Python sequence is provided it will\n        be converted to a NumPy array.\n    alpha : float, optional\n        Negative-slope coefficient (default 0.2).\n    derivative : bool, optional\n        If False (default) returns the Leaky ReLU activation.\n        If True returns the derivative with respect to *x*.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of the same shape as *x* containing the computed values.\n       x = np.asarray(x)\n\n    if derivative:\n        # 1 for x >= 0, else alpha\n        return np.where(x >= 0, 1.0, alpha)\n    else:\n        # x for x >= 0, else alpha * x\n        return np.where(x >= 0, x, alpha * x)\n```"}
{"task_id": 144, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mean_absolute_error(actual, predicted):\n    \"\"\"Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence containing the true values.\n    predicted : list | tuple | np.ndarray\n        Sequence containing the predicted values. Must have the same length\n        (and shape for multi-dimensional inputs) as ``actual``.\n\n    Returns\n    -------\n    float\n        The MAE rounded to 4 decimal places if inputs have the same shape,\n        otherwise -1.\n    \"\"\"\n    # Convert inputs to NumPy arrays (force float dtype for arithmetic safety)\n    try:\n        actual_arr = np.asarray(actual, dtype=float)\n        pred_arr   = np.asarray(predicted, dtype=float)\n    except Exception:                # Non-numeric data or conversion error\n        return -1\n\n    # Ensure both arrays have identical shapes\n    if actual_arr.shape != pred_arr.shape:\n        return -1\n\n    # Compute MAE: mean(|actual - predicted|)\n    mae = np.mean(np.abs(actual_arr - pred_arr))\n\n    # Return value rounded to 4 decimal places\n    return float(np.round(mae, 4))\n```"}
{"task_id": 146, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n   k\u2013Nearest Neighbours classifier (from scratch).    # ------------- basic sanity checks --------------------------------------\n    if X.ndim != 2 or X_test.ndim != 2:\n        raise ValueError('X and X_test must be 2-D arrays')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must contain the same number of samples')\n    if k < 1:\n        raise ValueError('k must be a positive integer')\n\n    n_train = X.shape[0]\n    k = min(k, n_train)                 # use all samples if k is too large\n    metric = metric.lower()\n\n    # ------------- distance matrix ------------------------------------------\n    if metric == 'euclidean':\n        # \u2016a \u2212 b\u2016\u2082 = sqrt(sum((a-b)\u00b2))\n        diff = X_test[:, None, :] - X[None, :, :]          # shape (m, n, f)\n        dists = np.sqrt(np.sum(diff ** 2, axis=2))         # shape (m, n)\n\n    elif metric == 'manhattan':\n        # \u2016a \u2212 b\u2016\u2081 = sum(|a-b|)\n        diff = X_test[:, None, :] - X[None, :, :]\n        dists = np.sum(np.abs(diff), axis=2)\n\n    elif metric == 'cosine':\n        # 1 - (a\u00b7b)/(\u2016a\u2016\u2016b\u2016)\n        eps = 1e-12\n        train_norms = np.linalg.norm(X, axis=1) + eps              # (n,)\n        test_norms  = np.linalg.norm(X_test, axis=1) + eps         # (m,)\n        sim = (X_test @ X.T) / (test_norms[:, None] * train_norms[None, :])\n        dists = 1.0 - sim                                          # (m, n)\n    else:\n        raise ValueErrormetric must be 'euclidean', 'manhattan', or 'cosine'    # ------------- k-nearest neighbours & majority vote ---------------------\n    preds = []\n    for row in dists:                           # iterate over test samples\n        nn_idx = np.argpartition(row, k-1)[:k]  # indices of k smallest distances\n        neighbour_labels = y[nn_idx]\n\n        labels, counts = np.unique(neighbour_labels, return_counts=True)\n        max_count = counts.max()\n        # candidates with the highest vote\n        candidates = labels[counts == max_count]\n        preds.append(candidates.min())          # tie-break: smallest label\n\n    return np.asarray(preds).tolist()\n```"}
{"task_id": 147, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    learning_rate: float = 0.1,\n    epochs: int = 10_000,\n) -> list[int]:\n    \"\"\"Binary Logistic Regression (batch GD, NumPy only).\"\"\"\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 0.  Small sanity checks\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if X_train.ndim != 2 or X_test.ndim != 2:\n        raise ValueError(\"X_train and X_test must be 2-D arrays\")\n    if y_train.ndim != 1 or len(y_train) != len(X_train):\n        raise ValueError(\"y_train must be a 1-D array with same length as X_train\")\n    if not np.array_equal(np.unique(y_train), np.array([0])) and \\\n       not np.array_equal(np.unique(y_train), np.array([1])) and \\\n       not np.array_equal(np.unique(y_train), np.array([0, 1])):\n        raise ValueError(\"y_train must contain only 0 and/or 1\")\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 1.  Trivial single-class case\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    unique_labels = np.unique(y_train)\n    if unique_labels.size == 1:\n        constant_label = int(unique_labels[0])\n        return [constant_label] * X_test.shape[0]\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 2.  Helper: numerically stable sigmoid\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        # Stable formulation to avoid overflow for large |z|\n        pos_mask = z >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(z, dtype=np.float64)\n\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        exp_z = np.exp(z[neg_mask])\n        out[neg_mask] = exp_z / (1.0 + exp_z)\n        return out\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 3.  Initialise parameters\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    n_samples, n_features = X_train.shape\n    w = np.zeros(n_features, dtype=np.float64)\n    b = 0.0\n\n    X = X_train.astype(np.float64)\n    y = y_train.astype(np.float64)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 4.  Batch gradient descent\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    for _ in range(epochs):\n        # Forward pass\n        z = X @ w + b\n        p = _sigmoid(z)\n\n        # Gradients of negative log-likelihood\n        error = p - y                                   # shape: (n_samples,)\n        grad_w = X.T @ error / n_samples                # shape: (n_features,)\n        grad_b = error.mean()                           # scalar\n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # 5.  Predict on unseen data\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    z_test = X_test.astype(np.float64) @ w + b\n    probs  = _sigmoid(z_test)\n    preds  = (probs >= 0.5).astype(int)\n\n    return preds.tolist()\n```\n\nExplanation of the main steps\n1. A numerically stable sigmoid is defined so the code does not overflow when the linear term `w\u00b7x + b` becomes large in magnitude.  \n2. Parameters are initialised at zero and refined using full-batch gradient descent on the negative log-likelihood.  \n   \u2022 `grad_w = X\u1d40(p\u2212y) / n`, `grad_b = mean(p\u2212y)`  \n3. After training, a simple 0.5 probability threshold converts predicted probabilities to class labels.  \n4. When the training set contains only a single class, the optimiser is skipped and that constant label is returned for every test sample, as requested."}
{"task_id": 148, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\ndef exp_smoothing(data: List[float], alpha: float) -> Union[List[float], int]:\n   Compute the exponential moving average (EMA) of a numeric sequence.\n\n    Parameters\n    ----------\n    data : List[float]\n        Sequence of numeric observations.\n    alpha : float\n        Smoothing factor (0 \u2264 alpha \u2264 1).\n\n    Returns\n    -------\n    List[float] | int\n        List of smoothed values rounded to 4 decimal places\n        or -1 for invalid input.\n       # \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if not data or not (0.0 <= alpha <= 1.0):\n        return -1\n\n    n = len(data)\n\n    # \u2500\u2500 Special-case shortcuts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if alpha == 0:\n        first_val = round(float(data[0]), 4)\n        return [first_val] * n\n\n    if alpha == 1:\n        return [round(float(x), 4) for x in data]\n\n    # \u2500\u2500 General EMA computation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    smoothed: List[float] = [0.0] * n\n    smoothed[0] = float(data[0])            # s\u2080 = x\u2080\n\n    for k in range(1, n):\n        smoothed[k] = (1 - alpha) * smoothed[k - 1] + alpha * float(data[k])\n\n    # Round all results to 4 decimal places\n    return [round(val, 4) for val in smoothed]\n```"}
{"task_id": 150, "completion_id": 0, "completion": "```python\nfrom heapq import heappush, heappop\nfrom typing import List, Tuple, Union\nfrom heapq import heappush, heappop     # (duplicated in the template on purpose)\n\ndef find_shortest_path(grid: List[List[int]]) -> Union[int, List[Tuple[int, int]]]:\n    \"\"\"Find a shortest path from the top-left to the bottom-right corner in a\n    4-connected grid using the A* algorithm (Manhattan heuristic).\n\n    Returns\n    -------\n    list[(int,int)]\n        A shortest path including start and goal as (row, col) tuples, or\n    -1\n        if the goal is unreachable.\n    \"\"\"\n    if not grid or not grid[0]:\n        return -1               # No cells at all (degenerate input).\n\n    m, n = len(grid), len(grid[0])\n    start, goal = (0, 0), (m - 1, n - 1)\n\n    # If start or goal blocked \u2192 impossible.\n    if grid[0][0] == 1 or grid[-1][-1] == 1:\n        return -1\n\n    # Trivial 1\u00d71 case.\n    if start == goal:\n        return [start]\n\n    # Manhattan-distance heuristic.\n    def h(r: int, c: int) -> int:\n        return abs(r - goal[0]) + abs(c - goal[1])\n\n    # open_set = priority queue ordered by f = g + h\n    open_heap: List[Tuple[int, int, int, int]] = []\n    heappush(open_heap, (h(*start), 0, start[0], start[1]))  # (f, g, r, c)\n\n    # Best cost found so far for each visited cell.\n    g_cost = {start: 0}\n\n    # Parent pointers for path reconstruction.\n    parent: dict[Tuple[int, int], Tuple[int, int]] = {}\n\n    # Closed set (optional \u2013 you can also rely only on g_cost checks).\n    closed: set[Tuple[int, int]] = set()\n\n    while open_heap:\n        f, g, r, c = heappop(open_heap)\n        current = (r, c)\n\n        if current in closed:\n            continue            # Stale entry with higher cost.\n\n        # Goal reached: reconstruct the path.\n        if current == goal:\n            path = [current]\n            while current in parent:        # Walk backwards to the start.\n                current = parent[current]\n                path.append(current)\n            path.reverse()\n            return path\n\n        closed.add(current)\n\n        for dr, dc in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n            nr, nc = r + dr, c + dc\n            neighbor = (nr, nc)\n\n            # Skip out-of-bounds or blocked cells.\n            if not (0 <= nr < m and 0 <= nc < n) or grid[nr][nc] == 1:\n                continue\n\n            tentative_g = g + 1\n            if tentative_g < g_cost.get(neighbor, float('inf')):\n                g_cost[neighbor] = tentative_g\n                parent[neighbor] = current\n                heappush(open_heap, (tentative_g + h(nr, nc), tentative_g, nr, nc))\n\n    # No path found\n    return -1\n```"}
{"task_id": 151, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# ---------------------------------------------------------------------------\n# Helper functions \u2013 forward activations\n# ---------------------------------------------------------------------------\ndef _sigmoid_forward(x: np.ndarray) -> np.ndarray:\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef _tanh_forward(x: np.ndarray) -> np.ndarray:\n    return np.tanh(x)\n\n\ndef _relu_forward(x: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, x)\n\n\ndef _softmax_forward(x: np.ndarray) -> np.ndarray:\n    # numerically-stable softmax along the last axis\n    shift = x - np.max(x, axis=-1, keepdims=True)\n    exps  = np.exp(shift)\n    return exps / np.sum(exps, axis=-1, keepdims=True)\n\n\n# ---------------------------------------------------------------------------\n# Helper functions \u2013 local derivatives (they receive the ALREADY ACTIVATED output)\n# ---------------------------------------------------------------------------\ndef _sigmoid_grad(a: np.ndarray) -> np.ndarray:\n    return a * (1.0 - a)\n\n\ndef _tanh_grad(a: np.ndarray) -> np.ndarray:\n    return 1.0 - a ** 2\n\n\ndef _relu_grad(a: np.ndarray) -> np.ndarray:\n    return (a > 0).astype(a.dtype)\n\n\ndef _softmax_grad(a: np.ndarray) -> np.ndarray:\n    # acts like an identity when multiplied element-wise with upstream gradient\n    return np.ones_like(a)\n\n\n# ---------------------------------------------------------------------------\n# Public helper that combines the forward pass and the local backward pass\n# ---------------------------------------------------------------------------\ndef activation_forward_backward(\n    X: np.ndarray,\n    activation: str,\n    upstream_grad: np.ndarray\n) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Apply an activation function and compute its local gradient.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Raw pre-activation values.\n    activation : str\n        One of: \"sigmoid\", \"tanh\", \"relu\", \"softmax\" (case-insensitive).\n    upstream_grad : np.ndarray\n        Gradient from the layer above (same shape as X).\n\n    Returns\n    -------\n    (A, grad) : tuple[np.ndarray, np.ndarray]\n        A     \u2013 activated output (rounded to 4 decimals)\n        grad  \u2013 upstream_grad \u2299 local_derivative (rounded to 4 decimals)\n    \"\"\"\n    if upstream_grad.shape != X.shape:\n        raise ValueError(\"`upstream_grad` must have the same shape as `X`.\")\n\n    act = activation.lower()\n    # forward pass\n    if act == \"sigmoid\":\n        A = _sigmoid_forward(X)\n        dA = _sigmoid_grad(A)\n    elif act == \"tanh\":\n        A = _tanh_forward(X)\n        dA = _tanh_grad(A)\n    elif act == \"relu\":\n        A = _relu_forward(X)\n        dA = _relu_grad(A)\n    elif act == \"softmax\":\n        A = _softmax_forward(X)\n        dA = _softmax_grad(A)\n    else:\n        raise ValueError(f\"Unsupported activation: '{activation}'\")\n\n    # local backward (element-wise)\n    grad = upstream_grad * dA\n\n    # round to 4 decimal places as requested\n    A    = np.round(A,    4)\n    grad = np.round(grad, 4)\n\n    return A, grad\n```"}
{"task_id": 152, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n   Calculate the proportion of correctly classified samples.\n\n    Parameters\n    ----------\n    y_true : list | numpy.ndarray\n        The true class labels.\n    y_pred : list | numpy.ndarray\n        The predicted class labels.\n\n    Returns\n    -------\n    float | int\n        The accuracy rounded to four decimal places. If the two inputs do\n        not have the same length or are empty, the function returns -1.\n       # Convert inputs to NumPy arrays (works for lists and ndarrays alike)\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check for 1-D compatibility: same length and non-empty\n    if y_true.size == 0 or y_true.size != y_pred.size:\n        return -1\n\n    # Count correct predictions\n    correct = np.sum(y_true == y_pred)\n\n    # Compute accuracy\n    accuracy = correct / y_true.size\n\n    # Return value rounded to 4 decimal places\n    return round(float(accuracy), 4)\n```"}
{"task_id": 154, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: np.ndarray, Y: np.ndarray, gamma: float = 0.1) -> list[list[float]]:\n    \"\"\"Return the RBF kernel matrix between two sets of vectors.\n\n    Args:\n        X: First input array of shape (n_samples, n_features) or (n_features,).\n        Y: Second input array of shape (m_samples, n_features) or (n_features,).\n        gamma: Positive scalar controlling the width of the kernel.\n\n    Returns:\n        A nested Python list containing the RBF kernel matrix rounded to six\n        decimal places.\n    \"\"\"\n    # --------- Input checks / reshaping ------------------------------------\n    if gamma <= 0:\n        raise ValueError(\"gamma must be a positive number.\")\n\n    # Treat 1-D arrays as a single sample\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    if Y.ndim == 1:\n        Y = Y.reshape(1, -1)\n\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            \"X and Y must have the same number of features (columns).\"\n        )\n\n    # --------- Pair-wise squared Euclidean distances -----------------------\n    # \u2016x\u2016\u00b2 for each row in X  \u2192 shape (n,)\n    X_norm = np.sum(X ** 2, axis=1)\n\n    # \u2016y\u2016\u00b2 for each row in Y  \u2192 shape (m,)\n    Y_norm = np.sum(Y ** 2, axis=1)\n\n    # Using (x - y)\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2\u00b7x\u00b7y\n    dist_sq = X_norm[:, None] + Y_norm[None, :] - 2.0 * X @ Y.T\n\n    # Numerical stability: any tiny negative values to zero\n    dist_sq = np.maximum(dist_sq, 0.0)\n\n    # --------- Apply RBF kernel formula ------------------------------------\n    K = np.exp(-gamma * dist_sq)\n\n    # --------- Rounding and conversion -------------------------------------\n    K = np.round(K, 6)\n\n    return K.tolist()\n```"}
{"task_id": 155, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    Architecture:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D state vector.\n        weights (dict): Dictionary containing the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector, each value rounded to 4 decimals.\n    \"\"\"\n    # Ensure state is a 1-D NumPy array\n    x = np.asarray(state, dtype=np.float64)\n\n    # ----- First layer -----\n    z1 = x @ weights[\"W1\"] + weights[\"b1\"]      # affine\n    a1 = np.maximum(0.0, z1)                    # ReLU\n\n    # ----- Second layer ----\n    z2 = a1 @ weights[\"W2\"] + weights[\"b2\"]\n    a2 = np.maximum(0.0, z2)                    # ReLU\n\n    # ----- Output layer ----\n    z3 = a2 @ weights[\"W3\"] + weights[\"b3\"]\n    action = np.tanh(z3)                        # tanh keeps values in [-1, 1]\n\n    # Round to 4 decimals and convert to Python list\n    return np.round(action, 4).tolist()\n```"}
{"task_id": 157, "completion_id": 0, "completion": "```python\ndef accuracy_score(y_true, y_pred):\n   Return the classification accuracy between *y_true* and *y_pred*.\n\n    The function must:\n      \u2022 return \u20111 if the two containers have different lengths;\n      \u2022 return 0 when both containers are empty;\n      \u2022 otherwise return the proportion of matching elements rounded to\n        4 decimal places.\n\n    Args:\n        y_true (list | tuple): Ground-truth class labels.\n        y_pred (list | tuple): Predicted class labels.\n\n    Returns:\n        float | int: The accuracy (rounded to 4 decimals) or \u20111.\n       # 1. Length mismatch -----------------------------------------------\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # 2. Both containers are empty --------------------------------------\n    total = len(y_true)\n    if total == 0:                # y_true and y_pred are both empty here\n        return 0\n\n    # 3. Compute accuracy ----------------------------------------------\n    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n    accuracy = correct / total\n\n    # 4. Round to four decimal places -----------------------------------\n    return round(accuracy, 4)\n```"}
{"task_id": 158, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(X: list[list[int | float]],\n                     y: list[int | float],\n                     alpha: float = 1.0,\n                     fit_intercept: bool = True) -> list[float]:\n    \"\"\"Compute Ridge Regression coefficients.\n\n    Args:\n        X: Nested list representing the feature matrix (shape N\u00d7M).\n        y: List of target values (length N).\n        alpha: Non-negative regularisation strength.\n        fit_intercept: Whether to add an unregularised intercept term.\n\n    Returns:\n        List of regression coefficients rounded to four decimals.  When\n        *fit_intercept* is True the first element is the intercept.\n    \"\"\"\n    # Convert to NumPy arrays (float64 for numeric stability)\n    X_mat = np.asarray(X, dtype=float)\n    y_vec = np.asarray(y, dtype=float)\n\n    # Optionally augment with an unregularised intercept column\n    if fit_intercept:\n        ones = np.ones((X_mat.shape[0], 1), dtype=float)\n        X_mat = np.hstack((ones, X_mat))\n\n    # Dimensions\n    n_features = X_mat.shape[1]\n\n    # Regularisation matrix A\n    A = np.eye(n_features) * alpha\n    if fit_intercept:           # Do NOT regularise the intercept\n        A[0, 0] = 0.0\n\n    # Closed-form solution: \u03b2\u0302 = (X\u1d40X + A)\u207b\u00b9 X\u1d40 y\n    XtX = X_mat.T @ X_mat\n    Xt_y = X_mat.T @ y_vec\n    beta = np.linalg.solve(XtX + A, Xt_y)\n\n    # Round to 4 decimals and convert to Python list\n    beta_rounded = np.round(beta, 4).tolist()\n    return beta_rounded\n```"}
{"task_id": 159, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_gaussian_pdf(x_i: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> float:\n   Compute the log-density of a multivariate Gaussian.\n\n    log N(x | \u00b5, \u03a3) = -\u00bd [ d ln(2\u03c0) + ln det \u03a3 + (x-\u00b5)\u1d40 \u03a3\u207b\u00b9 (x-\u00b5) ]\n\n    Parameters\n    ----------\n    x_i : np.ndarray  (d,)\n        Sample vector.\n    mu  : np.ndarray  (d,)\n        Mean vector.\n    sigma : np.ndarray  (d,d)\n        Positive\u2013definite covariance matrix.\n\n    Returns\n    -------\n    float\n        Log probability rounded to 4 decimal places.\n       # Ensure inputs are numpy arrays\n    x_i = np.asarray(x_i, dtype=float)\n    mu  = np.asarray(mu,  dtype=float)\n    sigma = np.asarray(sigma, dtype=float)\n\n    d = x_i.size                       # dimensionality\n    diff = x_i - mu                    # (x - \u00b5)\n\n    # ln det \u03a3 in a numerically stable way\n    sign, logdet = np.linalg.slogdet(sigma)\n    if sign <= 0:\n        raise ValueErrorCovariance matrix must be positive definite    # Solve \u03a3 y = (x - \u00b5) for y instead of computing \u03a3\u207b\u00b9 directly\n    mahal = diff.T @ np.linalg.solve(sigma, diff)   # (x-\u00b5)^T \u03a3\u207b\u00b9 (x-\u00b5)\n\n    log_prob = -0.5 * (d * np.log(2 * np.pi) + logdet + mahal)\n    return round(float(log_prob), 4)\n```"}
{"task_id": 160, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _soft_threshold(a: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S(a, \u03c4).\"\"\"\n    if a >  tau:\n        return a - tau\n    if a < -tau:\n        return a + tau\n    return 0.0\n\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Lasso (L1) linear regression fitted with coordinate-descent.\n    \n    Parameters\n    ----------\n    X : ndarray, shape (m, n)\n        Design matrix.\n    y : ndarray, shape (m,)\n        Target values.\n    lambda_param : float, default=1.0\n        Regularisation strength \u03bb (non-negative).\n    max_iters : int, default=100\n        Number of full passes over all coordinates.\n    fit_intercept : bool, default=True\n        Whether to learn an intercept term (not penalised).\n    \n    Returns\n    -------\n    (weights, bias)\n        weights : list length n, rounded to 4 decimals\n        bias    : float, rounded to 4 decimals\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n    m, n = X.shape\n\n    # ------------------------------------------------------------------\n    # 1. add column for the intercept (always the first column)\n    # ------------------------------------------------------------------\n    if fit_intercept:\n        intercept_col = np.ones((m, 1), dtype=float)\n    else:\n        intercept_col = np.zeros((m, 1), dtype=float)\n\n    X_aug = np.hstack([intercept_col, X])          # shape (m, n+1)\n    p = n + 1                                      # number of parameters\n\n    # Pre-compute column-wise squared norms (needed for denominator)\n    col_norm_sq = (X_aug ** 2).sum(axis=0)         # shape (p,)\n\n    # ------------------------------------------------------------------\n    # 2. initialise parameters to zero\n    # ------------------------------------------------------------------\n    w = np.zeros(p, dtype=float)                   # w[0] == intercept\n\n    # ------------------------------------------------------------------\n    # 3. outer iterations\n    # ------------------------------------------------------------------\n    for _ in range(max_iters):\n        # ---- update / recompute intercept --------------------------------\n        if fit_intercept:\n            residual = y - X_aug @ w\n            w[0] += residual.mean()               # mean residual as described\n\n        # ---- coordinate loop for the True weights ------------------------\n        for j in range(1, p):                     # skip intercept (j == 0)\n            if col_norm_sq[j] == 0.0:\n                w[j] = 0.0\n                continue\n\n            w_j_old = w[j]\n            w[j] = 0.0                            # \u201ctemporarily set\u201d to 0\n\n            # Partial residual r_j = y \u2212 X w_{\u00acj}\n            r_j = y - X_aug @ w                   # uses w with w_j == 0\n\n            # Inner product <x_j, r_j>\n            rho = (X_aug[:, j] * r_j).sum()\n\n            # Soft-threshold update\n            w[j] = _soft_threshold(rho, lambda_param * m) / col_norm_sq[j]\n\n    # ------------------------------------------------------------------\n    # 4. prepare return values (original n weights + bias)\n    # ------------------------------------------------------------------\n    bias = round(float(w[0]), 4)\n    weights = [round(float(v), 4) for v in w[1:]]\n\n    return weights, bias\n```"}
{"task_id": 161, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef factorization_machine_predict(X, w0, w, V):\n    \"\"\"\n    Binary prediction of a second\u2013order Factorization Machine.\n\n    Parameters\n    ----------\n    X  : ndarray, shape (m, n)\n        Design matrix with m samples and n features.\n    w0 : float\n        Global bias.\n    w  : ndarray, shape (n,)\n        Linear feature weights.\n    V  : ndarray, shape (n, k)\n        Latent factor matrix (k is the embedding dimension).\n\n    Returns\n    -------\n    list\n        Predicted class labels (+1 or -1) for the m samples.\n    \"\"\"\n    # --- make sure everything is an ndarray of compatible shape ----------\n    X = np.asarray(X)\n    w = np.asarray(w)\n    V = np.asarray(V)\n\n    # ---------------- linear part ----------------------------------------\n    linear_term = X @ w                      # shape: (m,)\n\n    # ---------------- pair-wise interaction part -------------------------\n    # Trick: \u00bd \u03a3_f [ (X V)_f\u00b2 - (X\u00b2 V\u00b2)_f ]\n    XV   = X @ V               # (m, k)\n    X2V2 = (X**2) @ (V**2)     # (m, k)\n    interaction_term = 0.5 * np.sum(XV**2 - X2V2, axis=1)  # (m,)\n\n    # ---------------- total score ----------------------------------------\n    scores = w0 + linear_term + interaction_term\n\n    # ---------------- convert to {-1, +1} labels -------------------------\n    labels = np.where(scores >= 0, 1, -1)\n\n    return labels.tolist()\n```"}
{"task_id": 162, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef cross_entropy(Y_hat: np.ndarray, Y: np.ndarray, epsilon: float = 1e-20) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute average cross-entropy loss of a batch and its gradient.\n\n    Parameters\n    ----------\n    Y_hat : np.ndarray\n        Soft-max probabilities with shape (batch_size, num_classes).\n    Y : np.ndarray\n        One-hot encoded ground-truth labels with the same shape as *Y_hat*.\n    epsilon : float, optional\n        Small constant added for numerical stability before taking the log.\n\n    Returns\n    -------\n    tuple[float, list[list[float]]]\n        The scalar loss and the gradient (as a nested Python list),\n        both rounded to 4 decimal places.\n    \"\"\"\n    # 1. shape check\n    assert Y_hat.shape == Y.shape, \"Shapes of Y_hat and Y must match\"\n    \n    m = Y_hat.shape[0]                                    # batch size\n    \n    # 2. loss\n    loss = -np.sum(Y * np.log(Y_hat + epsilon)) / m\n    loss = float(np.round(loss, 4))                       # scalar & rounded\n    \n    # 3. gradient\n    grad = (Y_hat - Y) / m\n    grad = np.round(grad, 4).tolist()                     # to nested list\n    \n    return loss, grad\n```"}
{"task_id": 163, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef elastic_net_regression(\n    X: list[float],\n    y: list[float],\n    degree: int,\n    reg_factor: float,\n    l1_ratio: float,\n    learning_rate: float,\n    n_iterations: int,\n) -> list[float]:\n   Elastic-Net / Ridge regression (single feature, polynomial expansion).    # ------------------------------------------------------------------ #\n    # 0. Convert input lists to NumPy arrays\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n_samples = X.shape[0]\n\n    # ------------------------------------------------------------------ #\n    # 1. Build the design matrix with polynomial features (incl. bias)\n    #    \u03a6 = [1, x, x\u00b2, \u2026 , x^degree ]\n    # ------------------------------------------------------------------ #\n    powers = np.arange(degree + 1)                    # [0, 1, 2, \u2026, degree]\n    Phi = np.column_stack([X ** p for p in powers])   # shape (n_samples, degree+1)\n\n    n_features = degree + 1  # includes the bias column\n\n    # ------------------------------------------------------------------ #\n    # 2. Fit the coefficients \u03b2\n    # ------------------------------------------------------------------ #\n    if l1_ratio == 0.0:  # ----------- pure Ridge: closed-form solution\n        # Regularisation matrix that does NOT penalise the bias term \u03b2\u2080\n        R = np.diag([0.0] + [1.0] * (n_features - 1))\n\n        # Normal-equation system:  (\u03a6\u1d40\u03a6 + n\u00b7\u03bb/2 \u00b7 R) \u03b2 = \u03a6\u1d40y\n        A = Phi.T @ Phi\n        if reg_factor > 0.0:\n            A = A + (n_samples * reg_factor / 2.0) * R\n\n        b = Phi.T @ y\n\n        # Solve; fall back to pseudo-inverse if the matrix is (near) singular\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.pinv(A) @ b\n\n    else:            # --------------- Elastic Net (general case): GD\n        beta = np.zeros(n_features)  # initialisation\n\n        alpha = l1_ratio\n        one_minus_alpha = 1.0 - alpha\n        lr = learning_rate\n\n        for _ in range(max(1, n_iterations)):\n            # Predictions and residuals\n            y_pred = Phi @ beta\n            residual = y_pred - y\n\n            # Gradient of the squared error part: 2/n \u03a6\u1d40(residual)\n            grad = (2.0 / n_samples) * (Phi.T @ residual)\n\n            # Add L2 part (\u03b2\u2080 excluded)\n            if reg_factor != 0.0 and one_minus_alpha != 0.0:\n                grad[1:] += reg_factor * one_minus_alpha * beta[1:]\n\n            # Add L1 part (\u03b2\u2080 excluded) \u2013 sub-gradient sign(\u03b2)\n            if reg_factor != 0.0 and alpha != 0.0:\n                grad[1:] += reg_factor * alpha * np.sign(beta[1:])\n\n            # Parameter update\n            beta -= lr * grad\n\n    # ------------------------------------------------------------------ #\n    # 3. Predictions on the training points, rounded to 4 decimals\n    # ------------------------------------------------------------------ #\n    predictions = Phi @ beta\n    return [round(float(p), 4) for p in predictions]\n```"}
{"task_id": 164, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_log_loss(actual, predicted):\n       Compute the binary classification log-loss (cross-entropy loss).\n\n    Parameters\n    ----------\n    actual : 1-D array-like of int (0 or 1)\n        Ground-truth binary labels.\n    predicted : 1-D array-like of float\n        Predicted probabilities for the positive class, expected in [0, 1].\n\n    Returns\n    -------\n    float\n        Mean log-loss, rounded to six decimal places.\n       # Convert inputs to NumPy arrays\n    y_true = np.asarray(actual, dtype=np.float64)\n    y_pred = np.asarray(predicted, dtype=np.float64)\n\n    # Basic validation\n    if y_true.shape != y_pred.shape:\n        raise ValueError`actual` and `predicted` must have the same length    if y_true.ndim != 1:\n        raise ValueErrorInputs must be 1-D sequences    # Clip predictions to avoid log(0) and log(1)\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n\n    # Log-loss computation\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n    return round(float(loss), 6)\n```"}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function returns \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays for unified processing\n    x_arr = np.asarray(x)\n    y_arr = np.asarray(y)\n\n    # If shapes differ, the vectors are not comparable\n    if x_arr.shape != y_arr.shape:\n        return -1\n\n    # Ensure we are working with 1-D data\n    x_flat = x_arr.ravel()\n    y_flat = y_arr.ravel()\n\n    # Length check (needed if original inputs were higher-dimensional)\n    if x_flat.size != y_flat.size:\n        return -1\n\n    n = x_flat.size\n    if n == 0:            # both vectors are empty\n        return 0.0\n\n    # Count differing positions\n    diff_cnt = np.count_nonzero(x_flat != y_flat)\n\n    # Normalized Hamming distance\n    distance = diff_cnt / n\n\n    return round(float(distance), 4)\n```"}
{"task_id": 166, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef perceptron_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, max_iter: int = 1000) -> list[int]:\n    \"\"\"Classic binary Perceptron (learning-rate = 1).\"\"\"\n    \n    # Make sure the inputs are two-dimensional / one-dimensional as expected\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test,  dtype=float)\n    y_train = np.asarray(y_train, dtype=float).ravel()\n    \n    n_samples, n_features = X_train.shape\n    \n    # 1. initialise parameters\n    w = np.zeros(n_features, dtype=float)\n    b = 0.0\n    \n    # 2. training loop\n    for _ in range(max_iter):\n        errors = 0\n        for xi, yi in zip(X_train, y_train):\n            margin = yi * (np.dot(w, xi) + b)\n            if margin <= 0.0:              # misclassified (or on the margin)\n                w += yi * xi              # update weight vector\n                b += yi                   # update bias\n                errors += 1\n        if errors == 0:                   # 3. early stopping if converged\n            break\n    \n    # 4. inference on test data\n    scores = X_test @ w + b              # raw scores\n    preds  = np.where(scores >= 0.0, 1, -1)\n    \n    # Return as ordinary Python ints\n    return preds.astype(int).tolist()\n```"}
{"task_id": 167, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_cross_entropy(y_hat, y, epsilon: float = 1e-10):\n    \"\"\"Compute the Binary Cross Entropy loss and its gradient.\n\n    Args:\n        y_hat (np.ndarray): Predicted probabilities in [0, 1].\n        y (np.ndarray): Ground-truth binary labels (0 or 1).\n        epsilon (float, optional): Small constant to avoid log(0). Defaults to 1e-10.\n\n    Returns:\n        tuple[float, list[float]]: (rounded_loss, rounded_gradient_list)\n    \"\"\"\n    # Ensure NumPy arrays\n    y_hat = np.asarray(y_hat, dtype=np.float64)\n    y     = np.asarray(y,     dtype=np.float64)\n\n    # 1. Clip predictions to avoid log(0)\n    y_hat = np.clip(y_hat, epsilon, 1.0 - epsilon)\n\n    # Number of samples\n    m = y.shape[0]\n\n    # 2. Mean BCE loss\n    loss = -np.mean(y * np.log(y_hat) + (1.0 - y) * np.log(1.0 - y_hat))\n\n    # 3. Gradient w.r.t. y_hat\n    grad = (y_hat - y) / m\n\n    # 4. Round to 4 decimal places\n    loss_rounded = float(np.round(loss, 4))\n    grad_rounded = np.round(grad, 4).tolist()\n\n    return loss_rounded, grad_rounded\n```"}
{"task_id": 168, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]]\n) -> list[int]:\n   Gaussian Naive Bayes classifier (from scratch, NumPy only).    # ------------------------------------------------------------------\n    # Convert incoming Python lists to NumPy arrays\n    # ------------------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    X_test  = np.asarray(X_test,  dtype=float)\n\n    n_samples, n_features = X_train.shape\n\n    # ------------------------------------------------------------------\n    # Extract unique class labels, keep them in the order given by np.unique\n    # ------------------------------------------------------------------\n    classes = np.unique(y_train)\n    n_classes = len(classes)\n\n    # ------------------------------------------------------------------\n    # Allocate arrays for the per\u2013class mean, variance and prior\n    # ------------------------------------------------------------------\n    means  = np.zeros((n_classes, n_features), dtype=float)\n    vars_  = np.zeros((n_classes, n_features), dtype=float)\n    priors = np.zeros(n_classes,             dtype=float)\n\n    eps = 1e-6          # variance smoothing term to avoid division by zero\n\n    # ------------------------------------------------------------------\n    # Maximum\u2013likelihood estimates of mean, variance and class prior\n    # ------------------------------------------------------------------\n    for idx, c in enumerate(classes):\n        X_c = X_train[y_train == c]          # samples belonging to class c\n        priors[idx] = X_c.shape[0] / n_samples\n        means[idx]  = X_c.mean(axis=0)\n        vars_[idx]  = X_c.var(axis=0) + eps  # add smoothing term\n\n    # ------------------------------------------------------------------\n    # Pre\u2013compute parts of the log\u2013gaussian that do not depend on the sample\n    # log N(x; \u03bc, \u03c3\u00b2) = -\u00bd [ log(2\u03c0\u03c3\u00b2) + (x-\u03bc)\u00b2 / \u03c3\u00b2 ]\n    # ------------------------------------------------------------------\n    log_priors = np.log(priors)                      # shape (C,)\n    const_part = -0.5 * np.sum(np.log(2.0 * np.pi * vars_), axis=1)  # shape (C,)\n\n    # ------------------------------------------------------------------\n    # Vectorised computation of log\u2013posterior for every test sample\n    # ------------------------------------------------------------------\n    # diff: shape (n_test, C, n_features)\n    diff = X_test[:, None, :] - means[None, :, :]\n    # quadratic term: shape (n_test, C)\n    quad = -0.5 * np.sum((diff ** 2) / vars_[None, :, :], axis=2)\n\n    # Total log\u2013posterior: shape (n_test, C)\n    log_posterior = quad + const_part + log_priors\n\n    # ------------------------------------------------------------------\n    # Pick the class with the highest posterior probability\n    # ------------------------------------------------------------------\n    predictions = classes[np.argmax(log_posterior, axis=1)]\n    return predictions.tolist()\n```"}
{"task_id": 169, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef generate_window(\n    window: str,\n    N: int,\n    coefficients: list[float] | None = None\n) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        One of \"hamming\", \"hann\", \"blackman_harris\", \"generalized_cosine\".\n    N : int\n        Number of samples (must be a positive integer).\n    coefficients : list[float] | None\n        Cosine\u2013series coefficients, required **only** for\n        \"generalized_cosine\".\n\n    Returns\n    -------\n    list[float]\n        N window coefficients rounded to 4 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If *N* < 1, the window name is unknown, or the coefficient\n        list is missing when required.\n    \"\"\"\n    # ----------------------------- sanity checks -----------------------------\n    if not isinstance(N, int) or N < 1:\n        raise ValueError(\"N must be a positive integer.\")\n\n    win = window.lower()\n\n    if N == 1:                     # universal convention\n        return [1.0]\n\n    if win not in {\n        \"hamming\",\n        \"hann\",\n        \"blackman_harris\",\n        \"generalized_cosine\",\n    }:\n        raise ValueError(f\"Unknown window: {window!r}\")\n\n    # sample index 0 \u2026 N-1 and the common divisor (N-1)\n    n = np.arange(N, dtype=float)\n    div = N - 1                    # never 0 here because N > 1\n    two_pi_n = 2.0 * np.pi * n / div\n\n    # --------------------------- concrete formulas ---------------------------\n    if win == \"hamming\":\n        w = 0.54 - 0.46 * np.cos(two_pi_n)\n\n    elif win == \"hann\":\n        w = 0.5 - 0.5 * np.cos(two_pi_n)\n\n    elif win == \"blackman_harris\":\n        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = (\n            a0\n            - a1 * np.cos(two_pi_n)\n            + a2 * np.cos(2 * two_pi_n)\n            - a3 * np.cos(3 * two_pi_n)\n        )\n\n    elif win == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"coefficients must be supplied for generalized_cosine.\")\n        # start with zeros then add each cosine term\n        w = np.zeros_like(n, dtype=float)\n        for k, a_k in enumerate(coefficients):\n            w += a_k * np.cos(2 * np.pi * k * n / div)\n\n    # ------------------------------- rounding --------------------------------\n    # Round to 4 decimals and convert to regular Python list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 171, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set (threshold 0.5).\"\"\"\n\n    # -------------------- Helper functions -------------------- #\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        \"\"\"Numerically stable sigmoid.\"\"\"\n        pos_mask = z >= 0\n        neg_mask = ~pos_mask\n        out = np.empty_like(z, dtype=float)\n\n        out[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        exp_z = np.exp(z[neg_mask])\n        out[neg_mask] = exp_z / (1.0 + exp_z)\n        return out\n\n    # -------------------- Basic checks -------------------- #\n    y_arr = np.asarray(y_train)\n    unique_labels = np.unique(y_arr)\n\n    # Special case \u2013 all training labels identical\n    if unique_labels.size == 1:\n        return [int(unique_labels[0]) for _ in range(len(X_test))]\n\n    if unique_labels.size != 2:\n        raise ValueError(\n            \"y_train must contain exactly two distinct numeric labels.\"\n        )\n\n    # -------------------- Label mapping -------------------- #\n    label_to_bin = {unique_labels[0]: 0, unique_labels[1]: 1}\n    bin_to_label = {0: unique_labels[0], 1: unique_labels[1]}\n    y_bin = np.vectorize(label_to_bin.get)(y_arr).astype(float)\n\n    # -------------------- Prepare design matrices -------------------- #\n    X_train_np = np.asarray(X_train, dtype=float)\n    X_test_np = np.asarray(X_test, dtype=float)\n\n    # Add intercept / bias term (column of ones at index 0)\n    X_train_np = np.hstack((np.ones((X_train_np.shape[0], 1)), X_train_np))\n    X_test_np = np.hstack((np.ones((X_test_np.shape[0], 1)), X_test_np))\n\n    n_samples, n_features = X_train_np.shape\n\n    # -------------------- Parameter initialization -------------------- #\n    rng = np.random.default_rng()\n    w = np.zeros(n_features, dtype=float)  # simple zero initialization\n\n    # -------------------- Mini-batch Gradient Descent -------------------- #\n    for _ in range(epochs):\n        # Shuffle indices for each epoch\n        indices = rng.permutation(n_samples)\n\n        for start in range(0, n_samples, batch_size):\n            end = start + batch_size\n            batch_idx = indices[start:end]\n\n            X_b = X_train_np[batch_idx]\n            y_b = y_bin[batch_idx]\n\n            # Forward pass\n            preds = _sigmoid(X_b @ w)\n\n            # Gradient of negative log-likelihood\n            grad = (X_b.T @ (preds - y_b)) / y_b.size\n\n            # Parameter update\n            w -= learning_rate * grad\n\n    # -------------------- Inference on test set -------------------- #\n    test_probs = _sigmoid(X_test_np @ w)\n    test_preds_bin = (test_probs >= 0.5).astype(int)\n\n    # Map predictions back to original label space\n    test_preds_orig = [int(bin_to_label[int(b)]) for b in test_preds_bin]\n\n    return test_preds_orig\n```"}
{"task_id": 172, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_grad(z):\n   Derivative of the hyper-bolic tangent (tanh) activation.\n\n    The derivative is computed element-wise as 1 \u2212 tanh(z)**2.\n\n    Args:\n        z: A scalar, Python list or NumPy ndarray of numbers.\n\n    Returns:\n        float | list: If `z` is scalar a float is returned, otherwise a\n        Python list with each element rounded to 4 decimals.\n       # Compute derivative using NumPy\n    grad = 1 - np.tanh(z) ** 2          # element-wise derivative\n    grad = np.round(grad, 4)            # round to four decimals\n\n    # Decide on return type based on the original input\n    if np.isscalar(z):                  # scalar input \u2192 float\n        return float(grad)\n    else:                               # array-like input \u2192 list\n        return grad.tolist()\n```"}
{"task_id": 173, "completion_id": 0, "completion": "```python\ndef alphabetical_distance(candidate: str, target: str) -> int:\n    \"\"\"Calculate the alphabetical (ASCII) distance between two equal-length strings.\n\n    Parameters\n    ----------\n    candidate : str\n        The string produced by the genetic algorithm.\n    target : str\n        The desired target string.\n\n    Returns\n    -------\n    int\n        Sum of absolute ASCII differences between corresponding characters.\n        If the strings differ in length, returns -1.\n    \"\"\"\n    # Rule 1: Both strings must be the same length.\n    if len(candidate) != len(target):\n        return -1\n\n    # Rule 2 (and general case): sum the per-character absolute differences.\n    return sum(abs(ord(c) - ord(t)) for c, t in zip(candidate, target))\n```"}
{"task_id": 174, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (left intentionally \u2013 harmless duplication)\n\ndef gan_discriminator_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[float, float]:\n   Calculate categorical cross-entropy loss & accuracy for a GAN discriminator.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Soft-max probabilities with shape (n_samples, n_classes).\n    labels : np.ndarray\n        One-hot encoded true labels with the same shape as *predictions*.\n\n    Returns\n    -------\n    tuple[float, float]\n        (loss, accuracy) \u2013 both rounded to 4 decimal places.\n       # -------- Input sanity checks (optional but helpful) -------------------\n    if predictions.shape != labels.shape:\n        raise ValueError`predictions` and `labels` must have identical shapes    if predictions.ndim != 2:\n        raise ValueErrorInputs must be 2-D arrays of shape (batch, classes).    # -------- Categorical cross-entropy ------------------------------------\n    eps = 1e-12                                 # numerical stability\n    clipped_preds = np.clip(predictions, eps, 1.0 - eps)\n    # Cross-entropy per sample: -\u03a3 y_true * log(y_pred)\n    sample_losses = -np.sum(labels * np.log(clipped_preds), axis=1)\n    mean_loss = np.mean(sample_losses)\n\n    # -------- Accuracy ------------------------------------------------------\n    pred_classes  = np.argmax(predictions, axis=1)\n    true_classes  = np.argmax(labels,       axis=1)\n    accuracy      = np.mean(pred_classes == true_classes)\n\n    # -------- Round to 4 decimal places -------------------------------------\n    return round(float(mean_loss), 4), round(float(accuracy), 4)\n```"}
{"task_id": 175, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_penalty(weights: list | np.ndarray, C: float) -> float:\n   Compute the L2 regularization penalty.\n\n    Args:\n        weights (list | np.ndarray): 1-D iterable containing the model weights.\n        C (float): Non-negative regularization strength.\n\n    Returns:\n        float: Penalty value rounded to 4 decimal places.\n       # Convert the input to a NumPy array (makes lists, tuples, etc. work uniformly)\n    w = np.asarray(weights, dtype=float).ravel()   # ensure 1-D\n    \n    # Guard against empty arrays or zero regularization strength\n    if w.size == 0 or C == 0:\n        return 0.0\n    \n    # Compute C * \u03a3 w_i\u00b2\n    penalty = float(C) * float(np.dot(w, w))       # np.dot(w, w) == \u03a3 w_i\u00b2\n    \n    # Round to 4 decimal places\n    return round(penalty, 4)\n```"}
{"task_id": 176, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test:  list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"AdaBoost with decision-stumps (1-level trees) built completely from\n    scratch.  The function returns predicted labels (0/1) for *X_test*.\n    \"\"\"\n    # ----------------------- preparation ----------------------------------\n    X_tr = np.asarray(X_train, dtype=float)\n    X_te = np.asarray(X_test,  dtype=float)\n    y_tr = np.asarray(y_train, dtype=int)\n\n    if X_tr.ndim == 1:                      # make sure we have 2-D matrices\n        X_tr = X_tr[:, None]\n        X_te = X_te[:, None]\n\n    n_samples, n_features = X_tr.shape\n    # encode {0,1} -> {-1,+1}\n    y = 2*y_tr - 1\n\n    # initial sample weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # to store the ensemble\n    stumps: list[tuple[int, float, int]] = []        # (feature, threshold, polarity)\n    alphas: list[float] = []\n\n    # ----------------------- training loop --------------------------------\n    EPS = 1e-10\n    for _ in range(n_estimators):\n        best_err = np.inf\n        best_stump: tuple[int, float, int] | None = None\n        best_pred = None\n\n        for f in range(n_features):                   # deterministic order\n            column = X_tr[:, f]\n            thresholds = np.unique(column)\n\n            for thr in thresholds:                    # ascending order\n                for polarity in (1, -1):             # 1 before -1\n                    # stump prediction\n                    if polarity == 1:\n                        pred = np.where(column < thr, 1, -1)\n                    else:  # polarity == -1\n                        pred = np.where(column >= thr, 1, -1)\n\n                    err = np.sum(w[pred != y])\n\n                    # strict improvement?\n                    if err + 1e-12 < best_err:\n                        best_err = err\n                        best_stump = (f, float(thr), polarity)\n                        best_pred = pred\n                    # tie \u2013 keep the earlier one (because of the loop order)\n                    # so nothing to do\n\n        # perfect classifier \u2013 can stop early\n        if best_err < EPS:\n            alpha_t = 0.5 * np.log((1.0 - EPS) / EPS)\n            stumps.append(best_stump)          # type: ignore[arg-type]\n            alphas.append(alpha_t)\n            break\n\n        # compute alpha\n        alpha_t = 0.5 * np.log((1.0 - best_err) / (best_err + EPS))\n\n        # update weights\n        w *= np.exp(-alpha_t * y * best_pred)\n        w /= w.sum()\n\n        # remember learner\n        stumps.append(best_stump)              # type: ignore[arg-type]\n        alphas.append(alpha_t)\n\n    # ------------------------- prediction ---------------------------------\n    agg = np.zeros(len(X_te))\n\n    for (f, thr, polarity), alpha in zip(stumps, alphas):\n        col = X_te[:, f]\n        if polarity == 1:\n            pred = np.where(col < thr, 1, -1)\n        else:\n            pred = np.where(col >= thr, 1, -1)\n        agg += alpha * pred\n\n    y_hat = np.where(agg >= 0, 1, 0)\n    return y_hat.tolist()\n```"}
{"task_id": 177, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Union\n\ndef tanh(z: Union[int, float, np.ndarray, list]) -> Union[float, list]:\n    \"\"\"Compute the hyper-bolic tangent without using ``numpy.tanh``.\n    \n    Supports scalars, (nested) Python lists and ``numpy.ndarray`` of any\n    shape.  All results are rounded to 4 decimal places.\n    \"\"\"\n    \n    # ---------- helper for a *single* scalar --------------------------------\n    def _scalar_tanh(x: float) -> float:\n        \"\"\"Numerically stable tanh for one scalar value.\"\"\"\n        if x >= 0.0:\n            #  tanh(x) = 1 \u2212 2/(e^{2x}+1)            (good for x >= 0)\n            res = 1.0 - 2.0 / (np.exp(2.0 * x) + 1.0)\n        else:\n            #  tanh(x) = 2/(e^{\u22122x}+1) \u2212 1            (good for x  < 0)\n            res = 2.0 / (np.exp(-2.0 * x) + 1.0) - 1.0\n        return round(float(res), 4)                   # built-in float\n    \n    # ------------------------------------------------------------------------\n    # If the input is already a (Python) scalar \u2192 return a scalar\n    if isinstance(z, (int, float, np.integer, np.floating)):\n        return _scalar_tanh(float(z))\n    \n    # ------------------------------------------------------------------------\n    # Otherwise treat the input as an array-like object\n    arr = np.asanyarray(z, dtype=np.float64)          # copy-on-write if needed\n    out = np.empty_like(arr)\n    \n    # Vectorised, numerically stable implementation\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    \n    # x >= 0 : tanh(x) = 1 \u2212 2/(e^{2x}+1)\n    if np.any(pos_mask):\n        x = arr[pos_mask]\n        out[pos_mask] = 1.0 - 2.0 / (np.exp(2.0 * x) + 1.0)\n    \n    # x < 0  : tanh(x) = 2/(e^{\u22122x}+1) \u2212 1\n    if np.any(neg_mask):\n        x = arr[neg_mask]\n        out[neg_mask] = 2.0 / (np.exp(-2.0 * x) + 1.0) - 1.0\n    \n    # Round to 4 decimals and convert back to a *pure* Python list\n    return np.round(out, 4).tolist()\n```\n\nExplanation of the numerically stable formulae  \nFor large positive numbers `x`, `exp(x)` overflows quickly, while `exp(-x)` underflows.  \nUsing the identities  \n\n\u2022 tanh(x) = 1 \u2212 2 / (e^{2x} + 1)\u2003\u2003 (for x \u2265 0)  \n\u2022 tanh(x) = 2 / (e^{\u22122x} + 1) \u2212 1\u2003 (for x < 0)  \n\nkeeps every exponential in a safe range, so the implementation remains stable for very large positive or negative inputs (e.g. \u00b11 000)."}
{"task_id": 178, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n   Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or \u20111 on\n        invalid input.\n       # Validate input\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n\n    # Constants\n    w, c1, c2 = 0.5, 1.5, 1.5\n    lower, upper = -1.0, 1.0\n\n    rng = np.random.default_rng(seed)\n\n    # Initial particle positions and velocities\n    positions = rng.uniform(lower, upper, size=(num_particles, n_dims))\n    velocities = np.zeros_like(positions)\n\n    # Personal bests\n    p_best_positions = positions.copy()\n    p_best_values = np.sum(p_best_positions ** 2, axis=1)\n\n    # Global best\n    g_best_idx = np.argmin(p_best_values)\n    g_best_position = p_best_positions[g_best_idx].copy()\n    g_best_value = p_best_values[g_best_idx]\n\n    for _ in range(num_iterations):\n        r1 = rng.random(size=(num_particles, n_dims))\n        r2 = rng.random(size=(num_particles, n_dims))\n\n        # Velocity update\n        velocities = (w * velocities\n                      + c1 * r1 * (p_best_positions - positions)\n                      + c2 * r2 * (g_best_position - positions))\n\n        # Position update with clamping to bounds\n        positions = np.clip(positions + velocities, lower, upper)\n\n        # Evaluate objective function (Sphere)\n        obj_values = np.sum(positions ** 2, axis=1)\n\n        # Update personal bests\n        better = obj_values < p_best_values\n        if np.any(better):\n            p_best_positions[better] = positions[better]\n            p_best_values[better] = obj_values[better]\n\n            # Possibly update global best\n            best_idx = np.argmin(p_best_values)\n            if p_best_values[best_idx] < g_best_value:\n                g_best_value = p_best_values[best_idx]\n                g_best_position = p_best_positions[best_idx].copy()\n\n    return round(float(g_best_value), 4)\n```"}
{"task_id": 179, "completion_id": 0, "completion": "```python\nimport re\nimport string\n\n\n# Translation table for quickly stripping ASCII punctuation\n_PUNCT_TRANSLATOR = str.maketrans(\"\", \"\", string.punctuation)\n\n\ndef tokenize_chars(line: str, lowercase: bool = True, filter_punctuation: bool = True) -> list[str]:\n    \"\"\"Tokenise a text string at the character level.\n\n    Args:\n        line: Input text. If *None*, an empty list is returned.\n        lowercase: When *True*, convert *line* to lower-case before tokenisation.\n        filter_punctuation: When *True*, remove every character that is present in\n            ``string.punctuation`` (i.e. the 32 standard ASCII punctuation marks).\n\n    Returns:\n        A list of single-character strings after the selected preprocessing\n        steps have been applied. Consecutive spaces are collapsed into a single\n        space, and leading/trailing spaces are removed. If no characters remain\n        after preprocessing, an empty list is returned.\n    \"\"\"\n    # 1. Handle *None* input early.\n    if line is None:\n        return []\n\n    text = line\n\n    # 2. Lower-case conversion (optional).\n    if lowercase:\n        text = text.lower()\n\n    # 3. Punctuation filtering (optional).\n    if filter_punctuation:\n        text = text.translate(_PUNCT_TRANSLATOR)\n\n    # 4. Collapse runs of \u22652 spaces and strip leading/trailing spaces.\n    #    Only ordinary spaces are considered here, per the task description.\n    text = re.sub(r\" {2,}\", \" \", text).strip()\n\n    # 5. If nothing remains, return an empty list.\n    if not text:\n        return []\n\n    # 6. Return a list of single-character strings.\n    return list(text)\n```"}
{"task_id": 180, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef knn_predict(\n    X_train: list[list[float]],\n    y_train: list,\n    X_test: list[list[float]],\n    k: int = 5,\n    task: str =classification) -> list:\n   k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, defaultclassification        Eitherclassification for majority voting orregression for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n       # --- basic checks --------------------------------------------------------\n    if task not inclassificationregression:\n        raise ValueError('`task` must be eitherclassification orregression')\n\n    if len(X_train) == 0:\n        raise ValueError`X_train` must contain at least one sample    # --- prepare data --------------------------------------------------------\n    X_train_np = np.asarray(X_train, dtype=float)\n    X_test_np = np.asarray(X_test, dtype=float)\n    y_train_list = list(y_train)  # just in case some other sequence type is supplied\n\n    n_train = X_train_np.shape[0]\n\n    # effective k\n    k_eff = n_train if (k <= 0 or k > n_train) else k\n\n    predictions: list = []\n\n    # --- core loop -----------------------------------------------------------\n    for x in X_test_np:\n        # Euclidean distances to every training point\n        distances = np.linalg.norm(X_train_np - x, axis=1)\n\n        # indices of the k nearest neighbours\n        knn_indices = np.argsort(distances)[:k_eff]\n\n        neighbour_targets = [y_train_list[idx] for idx in knn_indices]\n\n        if task ==classification            counts = Counter(neighbour_targets)\n            max_count = max(counts.values())\n            # Resolve ties by natural Python ordering (min)\n            candidates = [label for label, cnt in counts.items() if cnt == max_count]\n            predicted = min(candidates)\n        else:  # regression\n            mean_val = float(np.mean(neighbour_targets))\n            predicted = round(mean_val, 4)\n\n        predictions.append(predicted)\n\n    return predictions\n```"}
{"task_id": 181, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\n\ndef knn_predict(\n    X_train: list[list[float]],\n    y_train: list,\n    X_test: list[list[float]],\n    k: int = 5,\n    problem: str =classify) -> list:\n   Predict targets of *X_test* using k-Nearest Neighbours    # Convert to NumPy for convenient vector arithmetic\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n\n    n_train = len(X_train)\n    if n_train == 0:\n        raise ValueErrorX_train must contain at least one sample    k_eff = min(max(1, k), n_train)  # ensure 1 \u2264 k_eff \u2264 n_train\n\n    preds = []\n\n    for x in X_test:\n        # Euclidean distances to all training points\n        dists = np.linalg.norm(X_train - x, axis=1)\n\n        # Indices of k closest neighbours\n        nn_idx = np.argsort(dists)[:k_eff]\n        nn_targets = [y_train[i] for i in nn_idx]\n\n        if problem ==classify            counts = Counter(nn_targets)\n            max_cnt = max(counts.values())\n            # Candidates with highest frequency; break ties by natural order\n            majority = sorted([lab for lab, cnt in counts.items() if cnt == max_cnt])[0]\n            preds.append(majority)\n\n        elif problem ==regress            mean_val = sum(nn_targets) / k_eff\n            preds.append(round(mean_val, 4))\n\n        else:\n            raise ValueErrorproblem must be either 'classify' or 'regress'    return preds\n```"}
{"task_id": 182, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef polynomial_kernel(X, Y, degree=2):\n    \"\"\"\n    Compute the polynomial-kernel (Gram) matrix  K = (X\u00b7Y\u1d40)^degree.\n\n    Parameters\n    ----------\n    X : 2-D list or np.ndarray   (shape = [n, p])\n    Y : 2-D list or np.ndarray   (shape = [m, p])\n    degree : int, default=2\n        Polynomial degree (must be a positive integer \u2265 1).\n\n    Returns\n    -------\n    list\n        Nested Python list containing the kernel matrix rounded to four\n        decimal places, or \u22121 if any validation check fails.\n    \"\"\"\n    # ---------- validation --------------------------------------------------\n    try:\n        X_arr = np.asarray(X, dtype=float)\n        Y_arr = np.asarray(Y, dtype=float)\n    except (TypeError, ValueError):\n        return -1                                   # non-numeric or malformed input\n\n    if X_arr.ndim != 2 or Y_arr.ndim != 2:          # must be 2-D\n        return -1\n    if X_arr.shape[1] != Y_arr.shape[1]:            # same #features\n        return -1\n    if not isinstance(degree, (int, np.integer)) or degree < 1:\n        return -1\n\n    # ---------- kernel computation ------------------------------------------\n    try:\n        K = (X_arr @ Y_arr.T) ** degree             # (X\u00b7Y\u1d40)^d\n    except ValueError:                              # broadcast / shape error\n        return -1\n\n    # ---------- post-processing ---------------------------------------------\n    K = np.round(K, 4)                              # 4-decimal rounding\n    return K.tolist()\n```"}
{"task_id": 183, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef he_uniform(weight_shape):\n    \"\"\"Return a NumPy ndarray initialised with He uniform distribution.\n\n    Parameters\n    ----------\n    weight_shape : tuple | list\n        Shape of the weight tensor. Must be of length 2 (dense layer) or 4\n        (2-D convolutional kernel).\n\n    Returns\n    -------\n    np.ndarray\n        Array of the given shape with values drawn from \ud835\udcb0[\u2212limit, limit] where\n        limit = sqrt(6 / fan_in).\n    \"\"\"\n    # Convert to tuple to make indexing safer\n    shape = tuple(weight_shape)\n\n    if len(shape) == 2:                        # Fully-connected layer\n        fan_in = shape[0]\n    elif len(shape) == 4:                      # 2-D convolutional kernel\n        fan_in = shape[0] * shape[1] * shape[2]\n    else:\n        raise ValueError(\"weight_shape must have length 2 (dense) or 4 (conv2d).\")\n\n    # He uniform limit\n    limit = np.sqrt(6.0 / fan_in)\n\n    # Sample and return the weights\n    return np.random.uniform(-limit, limit, size=shape)\n```"}
{"task_id": 184, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n    # ------------------------------------------------------------------ #\n    # helpers\n    # ------------------------------------------------------------------ #\n    class Node:  # simple tree container\n        __slots__ = (\"feature\", \"thr\", \"left\", \"right\", \"pred\")\n\n        def __init__(\n            self,\n            feature: int | None = None,\n            thr: float | None = None,\n            left: \"Node\" | None = None,\n            right: \"Node\" | None = None,\n            pred: int | None = None,\n        ):\n            self.feature = feature  # split feature index\n            self.thr = thr  # threshold\n            self.left = left\n            self.right = right\n            self.pred = pred  # class held by leaf (None for internal)\n\n        @property\n        def is_leaf(self) -> bool:\n            return self.pred is not None\n\n    # ----- impurity, majority, etc. ---------------------------------- #\n    n_classes: int = int(y_train.max()) + 1  # labels are non-negative ints\n\n    def gini_from_counts(class_counts: np.ndarray) -> float:\n        \"\"\"Gini impurity given absolute class counts.\"\"\"\n        total = class_counts.sum()\n        if total == 0:\n            return 0.0\n        probs = class_counts / total\n        return 1.0 - np.sum(probs * probs)\n\n    def majority_label(labels: np.ndarray) -> int:\n        \"\"\"Most frequent label (ties -> lowest label).\"\"\"\n        counts = np.bincount(labels, minlength=n_classes)\n        return int(np.flatnonzero(counts == counts.max())[0])\n\n    # ------------------------------------------------------------------ #\n    # training\n    # ------------------------------------------------------------------ #\n    X, y = X_train, y_train\n    n_samples, n_features = X.shape\n    unlimited_depth = float(\"inf\") if max_depth is None else max_depth\n\n    # Pre-compute sort orders for each feature once (saves time)\n    sort_idx_per_feature: list[np.ndarray] = [\n        np.argsort(X[:, f]) for f in range(n_features)\n    ]\n\n    def build(indices: np.ndarray, depth: int) -> Node:\n        \"\"\"Recursively build the decision tree, returning its root Node.\"\"\"\n        node_labels = y[indices]\n        # Stopping rule 1: pure node\n        if np.all(node_labels == node_labels[0]):\n            return Node(pred=int(node_labels[0]))\n        # Stopping rule 2: depth limit\n        if depth >= unlimited_depth:\n            return Node(pred=majority_label(node_labels))\n\n        # Parent impurity\n        parent_counts = np.bincount(node_labels, minlength=n_classes)\n        parent_gini = gini_from_counts(parent_counts)\n\n        best_gain = 0.0\n        best_feat: int | None = None\n        best_thr: float | None = None\n        best_left_idx: np.ndarray | None = None\n        best_right_idx: np.ndarray | None = None\n\n        # evaluate every (feature, threshold) candidate\n        for f in range(n_features):\n            # sort the current node's samples by this feature\n            sorted_global_idx = sort_idx_per_feature[f]\n            # Select only indices that are inside current node\n            mask = np.isin(sorted_global_idx, indices, assume_unique=False)\n            sorted_idx = sorted_global_idx[mask]\n\n            if sorted_idx.size < 2:  # can't split\n                continue\n\n            feat_values = X[sorted_idx, f]\n            labels_sorted = y[sorted_idx]\n\n            # cumulative class counts to build left/right fast\n            total_counts = np.bincount(labels_sorted, minlength=n_classes)\n            left_counts = np.zeros(n_classes, dtype=int)\n            right_counts = total_counts.copy()\n\n            # iterate over possible split points\n            for i in range(sorted_idx.size - 1):\n                lbl = labels_sorted[i]\n                left_counts[lbl] += 1\n                right_counts[lbl] -= 1\n\n                # skip if identical consecutive feature values\n                if feat_values[i] == feat_values[i + 1]:\n                    continue\n\n                left_size = i + 1\n                right_size = sorted_idx.size - left_size\n                if left_size == 0 or right_size == 0:\n                    continue\n\n                g_left = gini_from_counts(left_counts)\n                g_right = gini_from_counts(right_counts)\n                weighted_gini = (\n                    left_size * g_left + right_size * g_right\n                ) / sorted_idx.size\n                gain = parent_gini - weighted_gini\n\n                if gain > best_gain + 1e-15:\n                    best_gain = gain\n                    best_feat = f\n                    best_thr = (feat_values[i] + feat_values[i + 1]) / 2.0\n                    # prepare left/right indices for recursion\n                    left_mask = X[indices, f] <= best_thr\n                    best_left_idx = indices[left_mask]\n                    best_right_idx = indices[~left_mask]\n\n        # No split improving impurity or produced empty child\n        if (\n            best_feat is None\n            or best_left_idx.size == 0\n            or best_right_idx.size == 0\n            or best_gain <= 0.0\n        ):\n            return Node(pred=majority_label(node_labels))\n\n        # recursive children\n        left_child = build(best_left_idx, depth + 1)\n        right_child = build(best_right_idx, depth + 1)\n        return Node(feature=best_feat, thr=best_thr, left=left_child, right=right_child)\n\n    # construct the tree\n    root = build(np.arange(n_samples), 0)\n\n    # ------------------------------------------------------------------ #\n    # inference\n    # ------------------------------------------------------------------ #\n    def predict_one(x_row: np.ndarray) -> int:\n        node = root\n        while not node.is_leaf:\n            if x_row[node.feature] <= node.thr:\n                node = node.left\n            else:\n                node = node.right\n        return node.pred\n\n    return [predict_one(x) for x in X_test]\n```"}
{"task_id": 185, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef dataset_shape(X):\n   Inspect the input data and return *(n_samples, n_features)*.\n\n    Parameters\n    ----------\n    X : array-like\n        Feature data. If *X* is one-dimensional it is treated as a single\n        sample, otherwise the first axis counts the samples.\n\n    Returns\n    -------\n    tuple of two ints\n        *(n_samples, n_features)* extracted from *X* following the same logic\n        used by the `_setup_input` method in the supplied code snippet.\n       # 1. Convert to a NumPy array (without altering the original object)\n    X = np.asarray(X)\n\n    # 2. Reject empty inputs\n    if X.size == 0:\n        raise ValueErrorGot an empty matrix    # 3. Determine the shape\n    if X.ndim == 1:\n        # One-dimensional input \u2192 single sample\n        n_samples = 1\n        n_features = int(len(X))\n    else:\n        # Two or more dimensions \u2192 first axis stores samples\n        n_samples = int(X.shape[0])\n        # Product of the remaining axes gives the number of features\n        n_features = int(np.prod(X.shape[1:], dtype=int))\n\n    return (n_samples, n_features)\n```"}
{"task_id": 186, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef baseline_mean_predict(X_train, y_train, X_test):\n    \"\"\"Baseline mean regressor.\n    \n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features)\n        Training data (ignored here).\n    y_train : array-like, shape (n_samples,)\n        Target values.\n    X_test : array-like, shape (m_samples, n_features)\n        Samples to be predicted.\n    \n    Returns\n    -------\n    list of float\n        A list of length *m_samples* where every element equals the\n        arithmetic mean of *y_train*, rounded to 4 decimal places.\n    \"\"\"\n    # 1. Compute mean of y_train\n    y_arr = np.asarray(y_train, dtype=float)\n    if y_arr.size == 0:\n        raise ValueError(\"`y_train` must contain at least one element.\")\n    mean_val = round(float(np.mean(y_arr)), 4)          # round to 4 decimals\n    \n    # 2. Determine number of rows in X_test\n    if X_test is None:\n        n_rows = 0\n    elif hasattr(X_test, \"shape\"):                      # numpy array / pandas object / scipy sparse ...\n        # Treat a 1-D array as one sample\n        n_rows = 1 if X_test.ndim == 1 else X_test.shape[0]\n    else:                                               # list-like\n        n_rows = len(X_test)\n    \n    # 3. Return the repeated mean (or an empty list if no rows)\n    if n_rows == 0:\n        return []\n    \n    return [mean_val] * n_rows\n```"}
{"task_id": 188, "completion_id": 0, "completion": "```python\nimport math\ndef epsilon_decay(max_epsilon: float, min_epsilon: float, decay_rate: float, step: int) -> float:\n    \"\"\"Compute the exponentially decayed \u03b5 (epsilon) for \u03b5-greedy exploration.\n\n    \u03b5(t) = \u03b5_min + (\u03b5_max \u2212 \u03b5_min) * exp(\u2212decay_rate * t)\n\n    Args:\n        max_epsilon (float): Initial exploration rate \u03b5_max at step 0.\n        min_epsilon (float): Lower bound \u03b5_min that \u03b5 should never fall below.\n        decay_rate (float):  Positive decay rate \u03bb.\n        step (int):          Current time-step.\n\n    Returns:\n        float: \u03b5(t) rounded to 4 decimal places, or \u22121 if the parameters are invalid.\n    \"\"\"\n    # ---------------------- input validation ---------------------- #\n    # 0 \u2264 \u03b5_min < \u03b5_max\n    if not (0 <= min_epsilon < max_epsilon):\n        return -1\n    # \u03bb > 0\n    if decay_rate <= 0:\n        return -1\n    # t \u2265 0 and an integer\n    if not (isinstance(step, int) and step >= 0):\n        return -1\n\n    # --------------------- \u03b5-decay computation -------------------- #\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * math.exp(-decay_rate * step)\n\n    # Guard against numerical underflow\n    epsilon = max(epsilon, min_epsilon)\n\n    # ------------------------- rounding --------------------------- #\n    return round(epsilon, 4)\n```"}
{"task_id": 189, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef batch_resample(X, new_dim, modebilinear   Resample a batch of images to a new spatial resolution.\n\n    Parameters\n    ----------\n    X : numpy.ndarray, shape (n_ex, in_rows, in_cols, in_channels)\n        Input batch of images.\n    new_dim : tuple[int, int]\n        Target dimension (out_rows, out_cols).\n    mode :bilinearneighbor defaultbilinear        Interpolation method.\n\n    Returns\n    -------\n    numpy.ndarray\n        Resampled batch with shape (n_ex, out_rows, out_cols, in_channels),\n        values rounded to 4 decimal digits.\n       if mode not inbilinearneighbor        raise NotImplementedError(fUnrecognized resampling mode: {mode    # ------------------------------------------------------------------ #\n    # basic shapes and target grid\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X)\n    n_ex, in_rows, in_cols, n_ch = X.shape\n    out_rows, out_cols = new_dim\n\n    # coordinates in the *input* image that each output pixel maps to\n    # linspace ensures that the first/last pixels line-up (corner alignment)\n    row_idx = np.linspace(0, in_rows - 1, out_rows) if out_rows > 1 else np.array([0.])\n    col_idx = np.linspace(0, in_cols - 1, out_cols) if out_cols > 1 else np.array([0.])\n\n    # ------------------------------------------------------------------ #\n    # nearest\u2013neighbour\n    # ------------------------------------------------------------------ #\n    if mode ==neighbor        r_nn = np.clip(np.rint(row_idx).astype(int), 0, in_rows - 1)\n        c_nn = np.clip(np.rint(col_idx).astype(int), 0, in_cols - 1)\n\n        # fancy indexing broadcasts automatically\n        out = X[:, r_nn[:, None], c_nn[None, :], :]\n\n    # ------------------------------------------------------------------ #\n    # bilinear\n    # ------------------------------------------------------------------ #\n    else:\n        # 2-D grids\n        r, c = np.meshgrid(row_idx, col_idx, indexingij      # (out_rows, out_cols)\n\n        r0 = np.floor(r).astype(int)\n        c0 = np.floor(c).astype(int)\n        r1 = np.clip(r0 + 1, 0, in_rows - 1)\n        c1 = np.clip(c0 + 1, 0, in_cols - 1)\n\n        wy = r - r0                     # fractional row distance\n        wx = c - c0                     # fractional col distance\n\n        wa = (1 - wy) * (1 - wx)        # top-left\n        wb = (1 - wy) * wx              # top-right\n        wc = wy * (1 - wx)              # bottom-left\n        wd = wy * wx                    # bottom-right\n\n        # make sure we work in float for interpolation\n        Xf = X.astype(float)\n\n        Ia = Xf[:, r0, c0, :]           # gather four neighbours\n        Ib = Xf[:, r0, c1, :]\n        Ic = Xf[:, r1, c0, :]\n        Id = Xf[:, r1, c1, :]\n\n        # weighted sum (broadcasting: (1, H, W, 1) with (N, H, W, C))\n        out = (\n            wa[None, ..., None] * Ia +\n            wb[None, ..., None] * Ib +\n            wc[None, ..., None] * Ic +\n            wd[None, ..., None] * Id\n        )\n\n    return np.round(out, 4)\n```"}
{"task_id": 190, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n   Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        (best_feature_index, best_threshold_value, best_gini) with `best_gini`\n        rounded to 4 decimals.  If no valid split exists the function returns\n        (-1, None, round(G_whole_dataset, 4)).\n       # ------------------------------------------------------------------ helpers\n    def gini_from_counts(counts):\n       Gini impurity from an integer count vector        tot = counts.sum()\n        if tot == 0:\n            return 0.0\n        return 1.0 - (counts.astype(float) ** 2).sum() / (tot * tot)\n\n    # ------------------------------------------------------------------ ingest\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n\n    if X.ndim != 2:\n        raise ValueErrorX must be 2-dimensional    if y.ndim != 1 or y.shape[0] != X.shape[0]:\n        raise ValueErrory must be 1-D and have the same number of rows as X    n_samples, n_features = X.shape\n    if n_samples == 0:\n        raise ValueErrorEmpty input    # ---------------------------------------------------------------- classes\n    classes, y_enc = np.unique(y, return_inverse=True)\n    n_classes = len(classes)\n    total_counts = np.bincount(y_enc, minlength=n_classes)\n\n    # ---------------------------------------------------------------- root gini\n    G_root = gini_from_counts(total_counts)\n    # If the node is already pure, no split is necessary / allowed\n    if G_root < 1e-12:\n        return (-1, None, round(G_root, 4))\n\n    # ---------------------------------------------------------------- search\n    best_feature = -1\n    best_threshold = None\n    best_gini = np.inf\n    n_total = n_samples\n    eps = 1e-12  # tie tolerance\n\n    for j in range(n_features):\n        col = X[:, j]\n\n        # Sort samples by the current feature\n        order = np.argsort(col, kind='mergesort')  # stable sort for reproducible ties\n        sorted_x = col[order]\n        sorted_y = y_enc[order]\n\n        # Running class counts for the left node\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # Iterate over all positions, but only evaluate after each distinct value\n        for i in range(n_total):\n            cls = sorted_y[i]\n            left_counts[cls] += 1\n\n            # If next value is the same, we are still on the same threshold\n            if i + 1 < n_total and sorted_x[i] == sorted_x[i + 1]:\n                continue  # wait until the last occurrence of this value\n\n            # Determine sizes\n            n_left = i + 1\n            n_right = n_total - n_left\n            if n_right == 0:        # skip empty right child\n                continue\n\n            # Calculate Gini for children\n            G_left = gini_from_counts(left_counts)\n            right_counts = total_counts - left_counts\n            G_right = gini_from_counts(right_counts)\n\n            G_split = (n_left / n_total) * G_left + (n_right / n_total) * G_right\n            threshold_val = sorted_x[i]\n\n            # --------------------------------------------------- tie-breaking\n            update = False\n            if G_split + eps < best_gini:\n                update = True\n            elif abs(G_split - best_gini) < eps:\n                if best_feature == -1 or j < best_feature:\n                    update = True\n                elif j == best_feature and threshold_val < best_threshold:\n                    update = True\n\n            if update:\n                best_gini = G_split\n                best_feature = j\n                best_threshold = threshold_val\n\n    # ----------------------------------------------------------- return\n    if best_feature == -1:   # no valid split found\n        return (-1, None, round(G_root, 4))\n\n    return (best_feature, best_threshold, round(best_gini, 4))\n```"}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef metric(name: str):\n   Factory producing a validated distance function    \n    name = (name or '').lower()          # normalise once\n    supported = {'euclidean', 'manhattan', 'chebyshev', 'cosine'}\n\n    def _validate_and_cast(a, b):\n       Return the two vectors as 1-D float np.ndarray or None on failure        # correct containers?\n        if not isinstance(a, (list, tuple, np.ndarray)):\n            return None, None\n        if not isinstance(b, (list, tuple, np.ndarray)):\n            return None, None\n\n        # cast to float\n        try:\n            a = np.asarray(a, dtype=float)\n            b = np.asarray(b, dtype=float)\n        except Exception:\n            return None, None\n\n        # 1-D, same length, non-empty\n        if a.ndim != 1 or b.ndim != 1:\n            return None, None\n        if a.size == 0 or b.size == 0 or a.size != b.size:\n            return None, None\n\n        return a, b\n\n    def distance(u, v):\n        # Unsupported metric name \u2192 always \u20131\n        if name not in supported:\n            return -1\n\n        # Validate and get numpy vectors\n        u, v = _validate_and_cast(u, v)\n        if u is None:\n            return -1\n\n        # Specific extra rule for cosine\n        if name == 'cosine':\n            nu, nv = np.linalg.norm(u), np.linalg.norm(v)\n            if nu == 0 or nv == 0:\n                return -1\n\n        # Compute the requested distance\n        if name == 'euclidean':\n            val = np.linalg.norm(u - v)\n        elif name == 'manhattan':\n            val = np.sum(np.abs(u - v))\n        elif name == 'chebyshev':\n            val = np.max(np.abs(u - v))\n        elif name == 'cosine':\n            val = 1 - np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n        else:                              # should never reach here\n            return -1\n\n        return round(float(val), 4)\n\n    return distance\n```"}
{"task_id": 193, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softsign(z):\n    \"\"\"Compute the Softsign activation function.\n\n    Softsign(z) = z / (1 + |z|)\n\n    Parameters\n    ----------\n    z : int, float, list, or np.ndarray\n        Scalar or 1-D array-like input.\n\n    Returns\n    -------\n    float or list\n        For a scalar input, a single float rounded to 4 decimals.\n        For an array-like input, a list of floats (same 1-D shape) each\n        rounded to 4 decimals.\n    \"\"\"\n    # ---------- Scalar branch ----------\n    if np.isscalar(z):\n        result = z / (1 + abs(z))\n        return round(float(result), 4)\n\n    # ---------- Array-like branch ----------\n    arr = np.asarray(z, dtype=float)\n\n    # Empty input \u2192 empty list\n    if arr.size == 0:\n        return []\n\n    # Compute Softsign element-wise, round, convert to list\n    result = arr / (1 + np.abs(arr))\n    result = np.round(result, 4)\n    return result.tolist()\n```"}
{"task_id": 194, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adagrad_update(weights: list[float],\n                   gradients: list[float],\n                   G: list[float] | None = None,\n                   learning_rate: float = 0.01,\n                   eps: float = 1e-8) -> tuple[list[float], list[float]]:\n    \"\"\"Performs one Adagrad optimisation step.\n\n    Args:\n        weights: Current parameter vector.\n        gradients: Current gradient vector of the loss w.r.t. *weights*.\n        G: Running sum of squared gradients (None on the very first step).\n        learning_rate: Global learning-rate (\u03b7).\n        eps: Small constant to avoid division by zero.\n\n    Returns:\n        (new_weights, new_G) \u2013 both rounded to 6 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorised arithmetic\n    w  = np.asarray(weights,   dtype=float)\n    g  = np.asarray(gradients, dtype=float)\n\n    # Basic shape check\n    if w.shape != g.shape:\n        raise ValueError(\"weights and gradients must have the same length\")\n\n    # Initialise/convert accumulated squared gradients\n    G_prev = np.zeros_like(w) if G is None else np.asarray(G, dtype=float)\n    if G_prev.shape != w.shape:\n        raise ValueError(\"G must be the same length as weights\")\n\n    # 1. Accumulate squared gradients\n    G_new = G_prev + g ** 2\n\n    # 2. Parameter update\n    w_new = w - learning_rate * g / np.sqrt(G_new + eps)\n\n    # Round to 6 decimal places as required\n    w_new = np.round(w_new, 6).tolist()\n    G_new = np.round(G_new, 6).tolist()\n\n    return w_new, G_new\n```"}
{"task_id": 197, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\n\ndef _is_discrete(space: Any) -> bool:\n    \"\"\"Return True if *space* looks like a discrete Gym-like space.\"\"\"\n    return hasattr(space, \"n\")\n\n\ndef _is_continuous(space: Any) -> bool:\n    \"\"\"Return True if *space* looks like a continuous Gym-like space.\"\"\"\n    return hasattr(space, \"shape\")\n\n\ndef action_stats(\n    env: Any, md_action: bool, cont_action: bool\n) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], List[int], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env:  Environment instance exposing an ``action_space`` attribute mimicking\n              OpenAI Gym.\n        md_action:  ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if at least one dimension is continuous.\n\n    Returns:\n        (n_actions_per_dim, action_ids, action_dim)\n    \"\"\"\n    action_space = env.action_space\n\n    # --------------------------------------------------------------------- #\n    # Multi\u2013dimensional action spaces\n    # --------------------------------------------------------------------- #\n    if md_action:\n        # Gym's `spaces.Tuple` or `spaces.MultiDiscrete`-like container\n        sub_spaces = getattr(action_space, \"spaces\", None)\n        if sub_spaces is None:\n            raise ValueError(\n                \"Multi-dimensional flag is set, but the environment does not expose \"\n                \"an `action_space.spaces` attribute.\"\n            )\n\n        n_actions_per_dim: List[Union[int, float]] = []\n        has_continuous = False\n\n        # Collect per-dimension statistics\n        for sp in sub_spaces:\n            if _is_discrete(sp):\n                n_actions_per_dim.append(int(sp.n))\n            elif _is_continuous(sp):\n                n_actions_per_dim.append(np.inf)\n                has_continuous = True\n            else:\n                raise TypeError(\"Unsupported sub-space encountered in action_space.\")\n\n        action_dim = len(n_actions_per_dim)\n\n        # ---------------------------------------------------------------- #\n        # Build the list of all valid discrete actions (cartesian product)\n        # ---------------------------------------------------------------- #\n        if has_continuous:\n            action_ids = None\n        else:\n            ranges = [range(int(n)) for n in n_actions_per_dim]  # type: ignore[arg-type]\n            action_ids = list(product(*ranges))\n\n        return n_actions_per_dim, action_ids, action_dim\n\n    # --------------------------------------------------------------------- #\n    # 1-D action spaces\n    # --------------------------------------------------------------------- #\n    if _is_discrete(action_space):\n        # Purely discrete \u2013 a simple range of integers\n        n = int(action_space.n)\n        n_actions_per_dim = [n]\n        action_ids = list(range(n))\n        action_dim = 1\n        return n_actions_per_dim, action_ids, action_dim\n\n    if _is_continuous(action_space):\n        # Purely continuous \u2013 infinite possibilities\n        shape = action_space.shape\n        # Gym's Box typically returns shape=(k,), meaning *k* scalar dimensions.\n        action_dim = shape[0] if len(shape) > 0 else 1\n        n_actions_per_dim = [np.inf] * action_dim\n        action_ids = None\n        return n_actions_per_dim, action_ids, action_dim\n\n    # --------------------------------------------------------------------- #\n    # Unknown / unsupported space                                                                   #\n    # --------------------------------------------------------------------- #\n    raise TypeError(\"The provided action_space is not recognised as discrete or continuous.\")\n```"}
{"task_id": 198, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document.  The array belonging to\n        document *d* has shape (N_d, T) and stores the current values of the\n        variational parameter \u03d5 for this document.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size (number of rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list.  Each column sums to one\n        (up to rounding in the 4th decimal place) and every number is rounded\n        to four decimals.\n    \"\"\"\n    # --------------------------------------------------------------------- #\n    # 0.  Basic sanity checks and dimensions\n    # --------------------------------------------------------------------- #\n    if len(phi) != len(corpus):\n        raise ValueError(\"`phi` and `corpus` must contain the same number of documents.\")\n    if not phi:                       # empty corpus \u2013 return a V\u00d70 \u201cmatrix\u201d\n        return [[ ] for _ in range(V)]\n\n    T = phi[0].shape[1]               # number of topics\n    # --------------------------------------------------------------------- #\n    # 1.  Accumulate the un-normalised counts  \u03a3_d \u03a3_n \u03d5_{dn,t} \u00b7 [w_{dn}=v]\n    # --------------------------------------------------------------------- #\n    counts = np.zeros((V, T), dtype=np.float64)\n\n    for doc_words, phi_d in zip(corpus, phi):\n        if len(doc_words) != phi_d.shape[0]:\n            raise ValueError(\"Length of document and number of \u03c6-rows differ.\")\n\n        # add the whole document in one go:\n        # counts[word_index, :] += phi_d[token_index, :]\n        np.add.at(counts, np.asarray(doc_words, dtype=int), phi_d)\n\n    # --------------------------------------------------------------------- #\n    # 2.  Column-normalise to obtain probabilities\n    # --------------------------------------------------------------------- #\n    column_sums = counts.sum(axis=0)                  # shape (T,)\n\n    # A column with sum==0 should not occur in a sensible LDA run; we guard\n    # against it nevertheless by leaving such a column at zero (it will stay\n    # zero after normalisation, which keeps untouched rows at 0 as required).\n    # The specification demands columns to sum to one, so we only perform\n    # the division where we do have positive mass.\n    positive = column_sums > 0\n    counts[:, positive] /= column_sums[positive]\n\n    beta = counts                                     # now already normalised\n\n    # --------------------------------------------------------------------- #\n    # 3.  Round to 4 decimal places\n    # --------------------------------------------------------------------- #\n    beta = np.round(beta, 4)\n\n    # --------------------------------------------------------------------- #\n    # 4.  Convert to the requested Python list-of-lists format\n    # --------------------------------------------------------------------- #\n    return beta.tolist()\n```"}
{"task_id": 199, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef split(X: np.ndarray, y: np.ndarray, value: float) -> tuple[list[int | float], list[int | float]]:\n    \"\"\"Split the label array `y` into two parts based on a threshold applied to `X`.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        1-D array of feature values.\n    y : np.ndarray\n        1-D array of labels, same length as `X`.\n    value : float\n        Threshold used for the split.\n\n    Returns\n    -------\n    tuple[list[int | float], list[int | float]]\n        (y_left, y_right) where\n          \u2022 y_left  contains labels with `X < value`.\n          \u2022 y_right contains labels with `X >= value`.\n    \"\"\"\n    # Ensure the inputs are 1-D and of equal length\n    if X.ndim != 1 or y.ndim != 1:\n        raise ValueError(\"X and y must be one-dimensional arrays.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same length.\")\n\n    # Boolean masks for the split\n    left_mask  = X < value\n    right_mask = ~left_mask       # equivalent to (X >= value)\n\n    # Apply masks and convert to Python lists\n    y_left  = y[left_mask].tolist()\n    y_right = y[right_mask].tolist()\n\n    return (y_left, y_right)\n```"}
{"task_id": 200, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_l1_penalty(weights, C):\n   Compute the L1 regularization penalty.\n\n    Args:\n        weights: Sequence or np.ndarray containing numeric weights (any shape).\n        C: Regularization strength (non-negative float).\n\n    Returns:\n        L1 penalty (float) rounded to 4 decimal places.\n       # Convert input to NumPy array to handle any dimensionality\n    try:\n        arr = np.asarray(weights, dtype=float)\n    except Exception:\n        # Fallback for possible ragged Python sequences\n        def _flatten(seq):\n            for item in seq:\n                if isinstance(item, (list, tuple, np.ndarray)):\n                    yield from _flatten(item)\n                else:\n                    yield item\n        arr = np.fromiter(_flatten(weights), dtype=float)\n\n    # Empty container \u2192 zero penalty\n    if arr.size == 0:\n        return 0.0\n\n    l1_penalty = C * np.sum(np.abs(arr))\n    return round(float(l1_penalty), 4)\n```"}
{"task_id": 201, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef max_norm(p, m=2.0, axis=0):\n    \"\"\"Clip the L2 norm of slices of *p* along *axis* so that they do not exceed *m*.\n\n    Parameters\n    ----------\n    p : np.ndarray\n        Input tensor (weights) of arbitrary shape.\n    m : float, optional\n        Maximum allowed L2 norm for each slice. Default is 2.0.\n    axis : int | None, optional\n        Axis along which to compute the norm. If None, the whole tensor is\n        considered a single vector. Default is 0.\n\n    Returns\n    -------\n    list\n        Tensor with the same shape as the input where every slice along *axis*\n        has been norm-clipped. The returned value is a (nested) Python list and\n        every number is rounded to 4 decimal places.\n    \"\"\"\n    p = np.asarray(p, dtype=float)           # work with a float array\n    eps = 1e-12                              # numerical-stability term\n\n    if axis is None:\n        # treat p as a single vector\n        norm = np.linalg.norm(p)\n        scale = min(1.0, m / (norm + eps))\n        clipped = p * scale\n    else:\n        # make sure negative axes work\n        axis = axis % p.ndim\n        # compute L2 norms along the chosen axis, keepdims=True for broadcasting\n        norms = np.linalg.norm(p, axis=axis, keepdims=True)\n        scales = np.minimum(1.0, m / (norms + eps))\n        clipped = p * scales                 # element-wise broadcasting\n\n    # round to 4 decimals and convert to Python lists\n    return np.round(clipped, 4).tolist()\n```"}
{"task_id": 202, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _pairwise_distances(X: np.ndarray) -> np.ndarray:\n    \"\"\"Return the full pair-wise Euclidean distance matrix.\"\"\"\n    diff = X[:, None, :] - X[None, :, :]\n    return np.sqrt(np.sum(diff ** 2, axis=2))\n\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Deterministic Partitioning Around Medoids (PAM).\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_samples, n_features)\n        Input data.\n    k : int\n        Number of clusters (1 \u2264 k \u2264 n_samples).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels (length == n_samples, values 0 \u2026 k-1).\n    \"\"\"\n    n_samples = X.shape[0]\n    if not (1 <= k <= n_samples):\n        raise ValueError(\"k must satisfy 1 \u2264 k \u2264 n_samples\")\n\n    # Pre-compute all pair-wise distances once (symmetric matrix).\n    D = _pairwise_distances(X)\n\n    # ------------------------------------------------------------------\n    # 1.  Initial medoids: the first k samples (indices 0 \u2026 k-1)\n    # ------------------------------------------------------------------\n    medoids = list(range(k))         # keeps positional order (0 \u2026 k-1)\n    medoid_set = set(medoids)        # fast membership test\n\n    # Helper that returns current cost, closest distances and positions\n    def _compute_cost_and_assignments(meds):\n        dist_to_meds = D[:, meds]             # (n, k)\n        closest_pos = np.argmin(dist_to_meds, axis=1)  # 0 \u2026 k-1\n        closest_dist = dist_to_meds[np.arange(n_samples), closest_pos]\n        total_cost = float(np.sum(closest_dist))\n        return total_cost, closest_dist, closest_pos\n\n    cost, closest_dist, closest_pos = _compute_cost_and_assignments(medoids)\n\n    # ------------------------------------------------------------------\n    # 2.  Iterative improvement by single swaps\n    # ------------------------------------------------------------------\n    improved = True\n    while improved:\n        best_improvement = 0.0\n        best_swap = None\n\n        # Try every (medoid_position, non-medoid_index) pair\n        for m_pos, m_idx in enumerate(medoids):\n            for h_idx in range(n_samples):\n                if h_idx in medoid_set:      # skip current medoids\n                    continue\n\n                # Build new candidate medoid list (copy then replace)\n                candidate_medoids = medoids.copy()\n                candidate_medoids[m_pos] = h_idx\n\n                # Compute cost for the candidate set\n                new_cost, _, _ = _compute_cost_and_assignments(candidate_medoids)\n                improvement = cost - new_cost  # positive if better\n\n                # Keep the best (largest positive) improvement found so far\n                if improvement > best_improvement + 1e-12:  # strictly better\n                    best_improvement = improvement\n                    best_swap = (m_pos, m_idx, h_idx)\n\n        # Apply the best swap if it really lowers the cost\n        if best_improvement > 0 and best_swap is not None:\n            m_pos, old_idx, new_idx = best_swap\n            medoids[m_pos] = new_idx\n            medoid_set.remove(old_idx)\n            medoid_set.add(new_idx)\n\n            # Re-compute cost and assignments for the new configuration\n            cost, closest_dist, closest_pos = _compute_cost_and_assignments(medoids)\n        else:\n            improved = False   # no further decrease possible -> converge\n\n    # ------------------------------------------------------------------\n    # 3.  Final labelling\n    # ------------------------------------------------------------------\n    final_labels = closest_pos.tolist()   # already 0 \u2026 k-1 in order of medoids\n    return final_labels\n```"}
{"task_id": 203, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef build_alias_table(probs: list[float] | np.ndarray) -> tuple[list[float], list[int]] | int:\n   Build the alias\u2010method probability and alias tables.\n\n    Args\n    ----\n    probs : 1-D list or np.ndarray\n        Non-negative numbers that must sum to 1 (within 1e-12).\n\n    Returns\n    -------\n    (prob_table, alias_table)  \u2013 both Python lists, prob_table rounded to 4 decimals.\n    If the input is invalid, returns -1.\n       # ---------- validation & normalisation ----------\n    probs = np.asarray(probs, dtype=np.float64)          # convert to NumPy array\n\n    if probs.ndim != 1 or probs.size == 0:               # must be 1-D and non-empty\n        return -1\n    if np.any(probs < 0):                                # no negative values\n        return -1\n    if not np.isclose(probs.sum(), 1.0, atol=1e-12):     # must sum to 1\n        return -1\n\n    N = probs.size\n    scaled_probs = probs * N                             # step 1\n\n    # ---------- step 2 : split into 'small' and 'large' ----------\n    small, large = [], []\n    for idx, val in enumerate(scaled_probs):\n        (small if val < 1.0 else large).append(idx)\n\n    # prep output tables\n    prob_table  = np.zeros(N, dtype=np.float64)\n    alias_table = np.zeros(N, dtype=np.int64)\n\n    # ---------- step 3 : main loop ----------\n    while small and large:\n        l = small.pop()      # last element from small\n        g = large.pop()      # last element from large\n\n        prob_table[l]  = scaled_probs[l]\n        alias_table[l] = g\n\n        scaled_probs[g] = scaled_probs[g] + scaled_probs[l] - 1.0\n\n        if scaled_probs[g] < 1.0:\n            small.append(g)\n        else:\n            large.append(g)\n\n    # ---------- step 4 : remaining entries get probability 1 ----------\n    for idx in large + small:\n        prob_table[idx] = 1.0\n        # alias_table[idx] is already 0 from initialisation\n\n    # ---------- convert to Python lists & round ----------\n    prob_table_list  = [round(float(x), 4) for x in prob_table]\n    alias_table_list = alias_table.tolist()\n\n    return prob_table_list, alias_table_list\n```\n\nThe function follows the exact construction rules:\n\n1. Probabilities are scaled by `N`.\n2. Indices are split into `small` (<1) and `large` (\u22651) lists while preserving ascending order.\n3. The loop pops the last element from each list, fills the tables, updates the large probability and re-queues the index.\n4. Remaining indices receive probability `1`.\n\nThe final tables are returned as Python lists, with the probability table rounded to four decimal places. If the input is invalid (negative values or the probabilities do not sum to 1 within `1e-12`), the function returns `-1`."}
{"task_id": 204, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef num_missing(data) -> int:\n    \"\"\"Count missing values (None or NaN) in a 1-D iterable.\"\"\"\n    missing = 0\n\n    # Turn the input into something we can iterate over safely.\n    # If data is already an iterable, this is essentially a no-op;\n    # if it is empty it will simply yield nothing.\n    for x in list(data):\n        if x is None:\n            missing += 1\n        else:\n            # math.isnan works for regular Python floats and all NumPy\n            # floating scalar types; it raises TypeError for everything else.\n            try:\n                if math.isnan(x):\n                    missing += 1\n            except (TypeError, ValueError):\n                # x is not a floating-point number \u2192 not missing\n                pass\n\n    return int(missing)\n```"}
{"task_id": 205, "completion_id": 0, "completion": "```python\ndef get_gym_environs(registry: list) -> list:\n    \"\"\"Extract unique environment ids from a registry.\n\n    Args:\n        registry (list): A list whose elements can be any Python object. When an\n            element is a dictionary **and** contains the key \"id\", the value of\n            that key is considered an environment id.\n\n    Returns:\n        list: A list of unique ids in the order they first appear in *registry*.\n    \"\"\"\n    unique_ids = []          # Result list preserving appearance order\n    seen = set()             # Helper set for O(1) duplicate checks\n\n    for item in registry:    # Scan registry from left to right\n        if isinstance(item, dict) and \"id\" in item:\n            env_id = item[\"id\"]\n            if env_id not in seen:    # First time we see this id\n                seen.add(env_id)\n                unique_ids.append(env_id)\n\n    return unique_ids\n```"}
{"task_id": 206, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef absolute_error(actual, predicted):\n   Calculate the Mean Absolute Error (MAE) between two sequences.\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        A one-dimensional sequence of ground-truth numeric values.\n    predicted : list | tuple | np.ndarray\n        A one-dimensional sequence of predicted numeric values.\n\n    Returns\n    -------\n    float | int\n        The MAE rounded to four decimal places. If the two sequences are not\n        of equal length, are not one-dimensional, or contain non-numeric data,\n        return -1.\n       # --------------------------------------------------------------------- #\n    # 1. Convert inputs to NumPy arrays, forcing numeric dtype (float).\n    #    If conversion fails (e.g., due to non-numeric elements), exit early.\n    # --------------------------------------------------------------------- #\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except (TypeError, ValueError):\n        return -1\n\n    # --------------------------------------------------------------------- #\n    # 2. Basic validations: one-dimensionality, equal length, non-empty.\n    # --------------------------------------------------------------------- #\n    if a.ndim != 1 or p.ndim != 1:\n        return -1\n    if a.size == 0 or a.size != p.size:\n        return -1\n\n    # --------------------------------------------------------------------- #\n    # 3. Vectorised MAE computation.\n    # --------------------------------------------------------------------- #\n    mae = np.mean(np.abs(a - p))\n\n    # --------------------------------------------------------------------- #\n    # 4. Return result rounded to four decimal places.\n    # --------------------------------------------------------------------- #\n    return round(float(mae), 4)\n```"}
{"task_id": 207, "completion_id": 0, "completion": "```python\ndef convolution_shape(img_height: int,\n                      img_width: int,\n                      filter_shape: tuple[int, int],\n                      stride: tuple[int, int],\n                      padding: tuple[int, int]) -> tuple[int, int] | int:\n   Compute the spatial dimensions of the output produced by a 2-D convolution.\n\n    Args:\n        img_height (int): Height of the input image.\n        img_width  (int): Width  of the input image.\n        filter_shape (tuple[int, int]): (kernel_height, kernel_width).\n        stride        (tuple[int, int]): (stride_height, stride_width).\n        padding       (tuple[int, int]): (pad_height, pad_width).\n\n    Returns\n    -------\n        tuple[int, int] | int\n            (output_height, output_width) when the configuration is valid;\n            \u22121 otherwise.\n       # ------------------------------------------------------------------ #\n    # 1. Basic sanity-checks on the arguments                            #\n    # ------------------------------------------------------------------ #\n    # All five scalars must be integers\n    for name, value in [\n       img_height img_height),img_width img_width),\n       kernel_height filter_shape[0]),kernel_width  filter_shape[1]),\n       stride_height  stride[0]),     stride_width  stride[1]),\n       pad_height     padding[0]),    pad_width     padding[1]),\n    ]:\n        if not isinstance(value, int):\n            return -1\n\n    H,  W  = img_height, img_width\n    KH, KW = filter_shape\n    SH, SW = stride\n    PH, PW = padding\n\n    # Dimensions and strides must be strictly positive, paddings \u2265 0\n    if not (H > 0 and W > 0 and KH > 0 and KW > 0 and SH > 0 and SW > 0):\n        return -1\n    if PH < 0 or PW < 0:\n        return -1\n\n    # ------------------------------------------------------------------ #\n    # 2. Compute the (possibly non-integer) output sizes                 #\n    # ------------------------------------------------------------------ #\n    num_h = H + 2 * PH - KH          # numerator for height\n    num_w = W + 2 * PW - KW          # numerator for width\n\n    # The numerators must be non-negative to give at least one position\n    if num_h < 0 or num_w < 0:\n        return -1\n\n    # Must be divisible by the corresponding stride to yield an integer\n    if num_h % SH != 0 or num_w % SW != 0:\n        return -1\n\n    OH = num_h // SH + 1\n    OW = num_w // SW + 1\n\n    # Final guarantee: both outputs must be positive\n    if OH <= 0 or OW <= 0:\n        return -1\n\n    return OH, OW\n```"}
{"task_id": 208, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef one_hot_encode(labels: list[str], categories: list[str] | None = None) -> list[list[int]]:\n   Convert categorical labels to a one-hot encoded matrix.\n\n    Args:\n        labels: Sequence of length N containing the categorical labels.\n        categories: Optional sequence with the complete set of categories\n                    (length C). If None, it is inferred from *labels* in the\n                    order of first appearance.\n\n    Returns:\n        A list of N lists, each of length C, with 0/1 entries that form the\n        one-hot encoded matrix.  \n        If *categories* is supplied and an unknown label is encountered, -1\n        is returned instead.\n       # ------------------------------------------------------------------\n    # 1. Determine (or validate) the list of categories\n    # ------------------------------------------------------------------\n    if categories is None:\n        # Preserve order of first appearance\n        seen = {}\n        for lbl in labels:\n            if lbl not in seen:\n                seen[lbl] = None\n        categories = list(seen.keys())\n    else:\n        # Ensure lookup table for constant-time membership testing\n        categories = list(categories)                      # keep original order\n        category_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n        # Validate that every label exists in the supplied categories\n        for lbl in labels:\n            if lbl not in category_to_idx:\n                return -1\n    # When categories were inferred we still need an index map\n    if 'category_to_idx' not in locals():\n        category_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n\n    # ------------------------------------------------------------------\n    # 2. Build the one-hot matrix\n    # ------------------------------------------------------------------\n    N, C = len(labels), len(categories)\n    one_hot = np.zeros((N, C), dtype=int)\n\n    for row, lbl in enumerate(labels):\n        col = category_to_idx[lbl]\n        one_hot[row, col] = 1\n\n    # ------------------------------------------------------------------\n    # 3. Return the result as a (nested) Python list\n    # ------------------------------------------------------------------\n    return one_hot.tolist()\n```\n\nUsage example\n```python\nlabels      = ['red', 'green', 'blue', 'green']\ncategories  = ['red', 'green', 'blue']\n\nprint(one_hot_encode(labels, categories))\n# [[1, 0, 0],\n#  [0, 1, 0],\n#  [0, 0, 1],\n#  [0, 1, 0]]\n```"}
{"task_id": 209, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef analyze_bandit(payoffs: list[list[int | float]],\n                   payoff_probs: list[list[float]]\n                   ) -> tuple[list[float], int, float] | int:\n   Validate a multi-armed-bandit description and return the expected values.\n\n    Returns\n    -------\n    (expected_values, best_arm, best_ev)\n        expected_values : list of the arms\u2019 expected payoffs, rounded to 4 dp\n        best_arm        : smallest 0-based index whose rounded EV is maximal\n        best_ev         : that maximal EV, rounded to 4 dp\n    \u22121\n        if the specification is invalid\n       tol = 1e-8                                # tolerance for probability sum\n\n    # ---- top-level shape check ------------------------------------------------\n    if not isinstance(payoffs, (list, tuple)) \\\n       or not isinstance(payoff_probs, (list, tuple)) \\\n       or len(payoffs) != len(payoff_probs):\n        return -1\n\n    expected_values: list[float] = []\n\n    # ---- per-arm validation + EV computation ----------------------------------\n    for arm_payoffs, arm_probs in zip(payoffs, payoff_probs):\n\n        # Lengths must match\n        if len(arm_payoffs) != len(arm_probs):\n            return -1\n\n        try:\n            vals  = np.asarray(arm_payoffs, dtype=float)\n            probs = np.asarray(arm_probs,    dtype=float)\n        except Exception:                     # non-numeric entries\n            return -1\n\n        # Probabilities must form a valid distribution\n        if probs.ndim != 1 \\\n           or np.any(probs < 0) \\\n           or np.any(probs > 1 + tol) \\\n           or abs(probs.sum() - 1.0) > tol:\n            return -1\n\n        # Expected value for this arm\n        ev = float((vals * probs).sum())\n        expected_values.append(round(ev, 4))\n\n    # No arms at all?\n    if not expected_values:\n        return -1\n\n    # ---- identify best arm (break ties by smallest index) ---------------------\n    best_ev  = max(expected_values)\n    best_arm = next(i for i, ev in enumerate(expected_values) if ev == best_ev)\n\n    return expected_values, best_arm, best_ev\n```"}
{"task_id": 210, "completion_id": 0, "completion": "```python\nimport functools\nfrom typing import Callable, Any\n\n\ndef doublewrap(decorator_func: Callable[..., Callable[..., Any]]) -> Callable[..., Any]:\n   Allow *decorator_func* to be used with **or** without its own\n    parentheses.\n\n    The returned object behaves like a normal decorator when applied as\n        @decorator_func\n    (the wrapped function is handed in immediately) **and** like a decorator\n    factory when used as\n        @decorator_func(...)\n    (its arguments are collected first and the function to wrap is supplied\n    later by the ``@`` machinery).\n\n    All metadata of *decorator_func* are preserved.\n       @functools.wraps(decorator_func)\n    def _wrapper(*dargs, **dkwargs):\n        # Case 1 \u2013 used *without* parentheses:\n        #   @my_deco\n        # Python has already passed the function to be decorated as the first\n        # and only positional argument and supplied no keyword arguments.\n        if dargs and callable(dargs[0]) and len(dargs) == 1 and not dkwargs:\n            target_func = dargs[0]\n            return decorator_func(target_func)\n\n        # Case 2 \u2013 used *with* its own (optional) arguments:\n        #   @my_deco(...)\n        # Collect those arguments now and return the real decorator that will\n        # later receive the function to be wrapped.\n        def _real_decorator(target_func: Callable[..., Any]) -> Callable[..., Any]:\n            return decorator_func(target_func, *dargs, **dkwargs)\n\n        return _real_decorator\n\n    return _wrapper\n```"}
{"task_id": 211, "completion_id": 0, "completion": "```python\ndef accuracy(actual, predicted):\n    \"\"\"Return the classification accuracy between two sequences of labels.\n\n    Parameters\n    ----------\n    actual : Sequence\n        The ground-truth labels.\n    predicted : Sequence\n        The predicted labels.\n\n    Returns\n    -------\n    float\n        Accuracy rounded to four decimal places, or -1 if the inputs\n        are empty or of unequal length.\n    \"\"\"\n\n    # Helper: computes the classification error\n    def classification_error(a, p):\n        # Both must be sequences of the same, non-zero length\n        if len(a) == 0 or len(a) != len(p):\n            return None                     # signals invalid input\n        mismatches = sum(1 for ai, pi in zip(a, p) if ai != pi)\n        return mismatches / len(a)\n\n    err = classification_error(actual, predicted)\n    if err is None:                         # invalid input\n        return -1\n\n    acc = 1.0 - err\n    return round(acc, 4)\n```"}
{"task_id": 212, "completion_id": 0, "completion": "```python\nfrom collections import deque\n\ndef extract_realtime_model(graph: dict[str, list[str]]) -> list[str]:\n   Return one shortest, lexicographically-smallest path from 'input'\n    to 'output_realtime' in the directed graph given as an adjacency list.\n       start =input    target =output_realtime    # Trivial (degenerate) case ------------------------------------------------\n    if start == target:\n        return [start] if start in graph or any(start in v for v in graph.values()) else []\n\n    # Nothing to do if the start node is obviously absent ----------------------\n    if start not in graph and all(start not in v for v in graph.values()):\n        return []\n\n    # Breadth-first search ------------------------------------------------------\n    q = deque()\n    q.append((start, [start]))          # (current node, path so far)\n    seen_distance: dict[str, int] = {start: 0}\n\n    while q:\n        node, path = q.popleft()\n\n        # Goal reached\n        if node == target:\n            return path\n\n        next_dist = len(path)\n\n        # Iterate neighbours in lexicographic order so that the queue\n        # always contains paths ordered first by length, then lexicographically.\n        for neigh in sorted(graph.get(node, [])):\n            # Push neighbour only if this is the first time we reach it\n            # with the current minimal distance.\n            if neigh not in seen_distance:\n                seen_distance[neigh] = next_dist\n                q.append((neigh, path + [neigh]))\n\n    # No path found ------------------------------------------------------------\n    return []\n```"}
{"task_id": 213, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef he_normal(shape: tuple[int, ...]) -> list:\n   Return weights initialised with He Normal strategy.\n\n    Args:\n        shape: A tuple that represents the desired tensor shape.\n\n    Returns:\n        Nested Python list containing the initialised weights.\n       # If any dimension is zero, nothing can be initialised\n    if len(shape) == 0 or any(dim == 0 for dim in shape):\n        return []\n\n    # ------------------------------------------------------------------\n    # 1. Compute fan_in according to the common (PyTorch / TensorFlow) rule set\n    # ------------------------------------------------------------------\n    n_dims = len(shape)\n\n    if n_dims == 1:                       # Vector\n        fan_in = shape[0]\n\n    elif n_dims == 2:                     # Matrix [in_features, out_features]\n        fan_in = shape[0]\n\n    else:                                 # Conv-kernels etc.\n        in_channels = shape[1]\n        receptive_field_size = 1\n        for dim in shape[2:]:\n            receptive_field_size *= dim\n        fan_in = in_channels * receptive_field_size\n\n    # Guard against a possible fan_in of zero\n    if fan_in == 0:\n        return []\n\n    # ------------------------------------------------------------------\n    # 2. Draw samples from \ud835\udcdd(0, \u221a(2 / fan_in))\n    # ------------------------------------------------------------------\n    rng = np.random.default_rng(seed=42)  # fixed seed \u2192 reproducible\n    std = np.sqrt(2.0 / fan_in)\n    weights = rng.normal(loc=0.0, scale=std, size=shape)\n\n    # ------------------------------------------------------------------\n    # 3. Return as nested Python list\n    # ------------------------------------------------------------------\n    return weights.tolist()\n```"}
{"task_id": 214, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mean_squared_error(actual, predicted):\n   Calculate the Mean Squared Error (MSE) between two numerical sequences.\n\n    The function returns -1 if the two input sequences do not share the\n    same length/shape. Otherwise, it returns the MSE rounded to 4 decimal\n    places. Both Python lists and NumPy arrays are supported.\n\n    Args:\n        actual (list[int | float] | np.ndarray): Ground-truth values.\n        predicted (list[int | float] | np.ndarray): Predicted values.\n\n    Returns:\n        float | int: The rounded MSE, or -1 when the inputs are incompatible.\n       # Convert inputs to NumPy arrays for unified processing\n    actual_arr     = np.asarray(actual)\n    predicted_arr  = np.asarray(predicted)\n\n    # Check that the two arrays have exactly the same shape\n    if actual_arr.shape != predicted_arr.shape:\n        return -1\n\n    # Compute the Mean Squared Error\n    mse = np.mean((actual_arr - predicted_arr) ** 2)\n\n    # Round to 4 decimal places and return as a plain Python float\n    return float(np.round(mse, 4))\n```"}
{"task_id": 216, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef elastic_net_regression(x_train, y_train, x_test,\n                           degree=1, reg_factor=0.05, l1_ratio=0.5,\n                           n_iterations=3000, learning_rate=0.01):\n   Elastic-Net linear regression (batch gradient-descent).    # ------------- helpers --------------------------------------------------\n    def _as_2d(a):\n        a = np.asarray(a, dtype=float)\n        if a.ndim == 1:\n            a = a.reshape(-1, 1)\n        return a\n\n    def _poly_features(X, deg):\n        if deg == 1:\n            return X\n        feats = [X]\n        for d in range(2, deg + 1):\n            feats.append(X ** d)\n        return np.hstack(feats)\n\n    # ------------- input checking ------------------------------------------\n    Xtr = _as_2d(x_train)\n    Xte = _as_2d(x_test)\n    ytr = np.asarray(y_train, dtype=float).ravel()\n\n    if (Xtr.shape[0] != ytr.shape[0] or\n        Xtr.shape[1] != Xte.shape[1] or\n        n_iterations < 1):\n        return -1  # invalid shapes / iterations\n\n    # ------------- feature engineering -------------------------------------\n    # polynomial expansion (no cross terms, only powers)\n    Xtr_poly = _poly_features(Xtr, degree)\n    Xte_poly = _poly_features(Xte, degree)\n\n    # standardisation (training stats only)\n    mean = Xtr_poly.mean(axis=0)\n    std = Xtr_poly.std(axis=0)\n    std[std == 0] = 1.0           # avoid division by zero\n\n    Xtr_std = (Xtr_poly - mean) / std\n    Xte_std = (Xte_poly - mean) / std\n\n    # prepend bias column\n    Xtr_std = np.hstack([np.ones((Xtr_std.shape[0], 1)), Xtr_std])\n    Xte_std = np.hstack([np.ones((Xte_std.shape[0], 1)), Xte_std])\n\n    m, n_features = Xtr_std.shape\n    w = np.zeros(n_features)\n\n    # factors that stay constant\n    l1_alpha = reg_factor * l1_ratio\n    l2_alpha = reg_factor * (1.0 - l1_ratio)\n\n    # ------------- gradient-descent loop -----------------------------------\n    for _ in range(int(n_iterations)):\n        y_pred = Xtr_std.dot(w)\n        error = y_pred - ytr\n        grad = (2.0 / m) * Xtr_std.T.dot(error)\n\n        # regularisation gradient (skip bias term index 0)\n        l2_grad = 2.0 * l2_alpha * w\n        l1_grad = l1_alpha * np.sign(w)\n        l2_grad[0] = l1_grad[0] = 0.0     # no reg. on bias\n        grad += l2_grad + l1_grad\n\n        w -= learning_rate * grad\n\n    # ------------- prediction ----------------------------------------------\n    preds = Xte_std.dot(w)\n    return np.round(preds, 4).tolist()\n```"}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    Args:\n        actual:    1-D container (list, tuple, or np.ndarray) with values +1 or \u22121.\n        predicted: 1-D container (list, tuple, or np.ndarray) with raw scores.\n\n    Returns:\n        Tuple of three lists:\n            1. gradient  (\u2202\u2113/\u2202score) for every observation,\n            2. Hessian   (\u2202\u00b2\u2113/\u2202score\u00b2) for every observation,\n            3. probability = sigmoid(score) for every observation,\n       with every number rounded to 6 decimal places.\n    \"\"\"\n    # --- Convert inputs to NumPy arrays -------------------------------------------------\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n\n    # --- Basic validation ---------------------------------------------------------------\n    if a.shape != p.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same length.\")\n    if a.ndim != 1:\n        raise ValueError(\"`actual` and `predicted` must be one-dimensional.\")\n\n    # --- Sigmoid helper -----------------------------------------------------------------\n    sigmoid = lambda z: 1.0 / (1.0 + np.exp(-z))\n\n    # --- Probability (sigmoid of raw score) --------------------------------------------\n    prob = sigmoid(p)                                 # prob_i\n\n    # --- Gradient -----------------------------------------------------------------------\n    grad = a * sigmoid(-a * p)                        # grad_i\n\n    # --- Hessian ------------------------------------------------------------------------\n    hess = prob * (1.0 - prob)                        # hess_i\n\n    # --- Round to six decimal places ----------------------------------------------------\n    grad  = np.round(grad,  6)\n    hess  = np.round(hess,  6)\n    prob  = np.round(prob,  6)\n\n    # --- Return plain Python lists ------------------------------------------------------\n    return grad.tolist(), hess.tolist(), prob.tolist()\n```"}
{"task_id": 218, "completion_id": 0, "completion": "```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n   Evaluate the outcome of a finished round of Blackjack.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float: -1, 0, 1 or 1.5\n       def score(hand: list[int]) -> int:\n       Return the best (highest \u2264 21) score of the hand, or the raw sum if bust        total = sum(hand)\n        # usable ace?\n        if 1 in hand and total + 10 <= 21:\n            total += 10\n        return total\n\n    def is_natural(hand: list[int]) -> bool:\n       True iff the hand is a natural blackjack (exactly two cards: Ace + 10-value).        return len(hand) == 2 and 1 in hand and 10 in hand\n\n    player_score = score(player)\n    dealer_score = score(dealer)\n\n    player_bust = player_score > 21\n    dealer_bust = dealer_score > 21\n    player_natural = is_natural(player)\n\n    # 1. Player busts\n    if player_bust:\n        return -1.0\n\n    # 2. Dealer busts\n    if dealer_bust:\n        return 1.5 if player_natural else 1.0\n\n    # 3. Compare scores\n    if player_score > dealer_score:\n        return 1.5 if player_natural else 1.0\n    if player_score < dealer_score:\n        return -1.0\n\n    # 4. Equal scores \u2192 draw\n    return 0.0\n```"}
{"task_id": 220, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef estimate_lda_params(C_wt, C_dt, alpha, beta):\n    \"\"\"Estimate smoothed word-topic (phi) and document-topic (theta)\n    distributions for an LDA model.\n\n    Parameters\n    ----------\n    C_wt : np.ndarray\n        Word\u2013topic count matrix of shape (V, T).\n    C_dt : np.ndarray\n        Document\u2013topic count matrix of shape (D, T).\n    alpha : float\n        Symmetric Dirichlet prior for theta.\n    beta : float\n        Symmetric Dirichlet prior for phi.\n\n    Returns\n    -------\n    (list[list[float]], list[list[float]])\n        Phi and theta, each rounded to four decimals and given\n        as nested Python lists.\n    \"\"\"\n    # ---------- phi (word\u2013topic) ----------\n    if C_wt.size == 0:\n        phi_out = []\n    else:\n        V, T = C_wt.shape          # vocabulary size and #topics\n        numer = C_wt + beta                                # V \u00d7 T\n        denom = np.sum(C_wt, axis=0) + V * beta            #     T\n        phi   = numer / denom                              # V \u00d7 T\n        phi_out = np.round(phi, 4).tolist()\n\n    # ---------- theta (document\u2013topic) ----------\n    if C_dt.size == 0:\n        theta_out = []\n    else:\n        D, T = C_dt.shape          # #documents and #topics\n        numer = C_dt + alpha                              # D \u00d7 T\n        denom = np.sum(C_dt, axis=1, keepdims=True) + T * alpha  # D \u00d7 1\n        theta = numer / denom                            # D \u00d7 T\n        theta_out = np.round(theta, 4).tolist()\n\n    return phi_out, theta_out\n```"}
{"task_id": 221, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef squared_log_error(actual, predicted):\n    \"\"\"\n    Element-wise squared logarithmic error.\n\n    Parameters\n    ----------\n    actual, predicted : array-like\n        Sequences of true / predicted values (must be broadcastable to\n        the same shape and non-negative).\n\n    Returns\n    -------\n    np.ndarray\n        Array with the squared log errors for every element.\n    \"\"\"\n    actual = np.asarray(actual, dtype=float)\n    predicted = np.asarray(predicted, dtype=float)\n\n    # log1p(x) == log(1 + x)  \u2192  vectorised & numerically stable\n    return np.square(np.log1p(actual) - np.log1p(predicted))\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Compute the Mean Squared Logarithmic Error (MSLE).\n\n    Returns the MSLE rounded to 4 decimal places, or \u20111 on invalid input.\n    \"\"\"\n    # --- Convert to NumPy arrays ------------------------------------------------\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except (TypeError, ValueError):\n        # Non-numeric data or impossible cast\n        return -1\n\n    # --- Validation ------------------------------------------------------------\n    if a.shape != p.shape:                 # same length / shape?\n        return -1\n    if np.any(a < 0) or np.any(p < 0):     # all values non-negative?\n        return -1\n\n    # --- MSLE computation ------------------------------------------------------\n    sq_log_err = squared_log_error(a, p)\n    msle = float(np.mean(sq_log_err))      # Python float\n    return round(msle, 4)\n```"}
{"task_id": 222, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n   Compute the element-wise sigmoid (logistic) function.\n\n    Works for a scalar, Python list or NumPy array (1-D/2-D) and is\n    numerically stable for very large positive or negative inputs.\n\n    Args\n    ----\n    z : int | float | list | np.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    float | list\n        Sigmoid-transformed value(s) rounded to 4 decimal places.\n        A scalar input yields a float; an array-like input yields a\n        Python list with the same nested structure.\n       # Detect if the input is a scalar before converting to ndarray\n    is_scalar = np.isscalar(z)\n\n    # Convert to float64 ndarray for vectorised processing\n    z_arr = np.asarray(z, dtype=np.float64)\n\n    # Allocate result array\n    result = np.empty_like(z_arr, dtype=np.float64)\n\n    # Numerically stable computation:\n    # For non-negative values: 1 / (1 + exp(-z))\n    # For negative values:      exp(z) / (1 + exp(z))\n    pos_mask = z_arr >= 0\n    neg_mask = ~pos_mask\n\n    # Handle z >= 0\n    result[pos_mask] = 1.0 / (1.0 + np.exp(-z_arr[pos_mask]))\n    # Handle z  < 0\n    exp_z = np.exp(z_arr[neg_mask])\n    result[neg_mask] = exp_z / (1.0 + exp_z)\n\n    # Round to 4 decimal places\n    result = np.round(result, 4)\n\n    # Return according to the required type rule\n    if is_scalar:\n        # result is a 0-d ndarray; convert to Python float\n        return float(result)\n    else:\n        return result.tolist()\n```"}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n   Apply the Leaky ReLU activation element-wise.\n\n    Parameters\n    ----------\n    z : scalar | list | np.ndarray\n        Input value(s) \u2013 can be a Python scalar, a (nested) list, or a NumPy array\n        of any shape / dimension.\n    a : float, optional\n        Slope for the negative part (must satisfy 0 \u2264 a < 1).  Default is 0.01.\n\n    Returns\n    -------\n    list | scalar\n        The activated values having the same nested structure as *z*.\n        Arrays and lists are returned as (nested) Python lists; a lone scalar\n        is returned as a scalar.\n       # Simple guard for the slope parameter\n    if not (0 <= a < 1):\n        raise ValueErrorParameter 'a' must be in the interval [0, 1).    # Case 1 \u2013 NumPy array: use vectorised operations then convert to list\n    if isinstance(z, np.ndarray):\n        return np.where(z >= 0, z, a * z).tolist()\n\n    # Case 2 \u2013 list / tuple: recurse element-wise, preserving structure\n    if isinstance(z, (list, tuple)):\n        return [leaky_relu(elem, a) for elem in z]\n\n    # Case 3 \u2013 scalar (Python or NumPy scalar)\n    res = z if z >= 0 else a * z\n    # Convert NumPy scalar to native Python scalar if necessary\n    if isinstance(res, np.generic):\n        return res.item()\n    return res\n```"}
{"task_id": 225, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hz2mel(hz, formula: str =htk -> list:\n   Convert a set of frequencies from Hertz to the mel scale.\n\n    Parameters\n    ----------\n    hz : array-like of float\n        One- or multi-dimensional collection of frequencies in Hertz.\n    formula :htkslaney optional\n        \u2022htk    \u2013 2595 * log10(1 + hz / 700)   (default)  \n        \u2022slaney \u2013 1127 *  ln (1 + hz / 700)\n\n    Returns\n    -------\n    list of float\n        Mel values (same shape as *hz*), rounded to 4 decimals.\n       # Convert input to NumPy array for vectorised math\n    hz = np.asarray(hz, dtype=float)\n\n    # Normalise and validate formula argument\n    formula = formula.lower()\n    assert formula inhtkslaneyformula must be 'htk' or 'slaney'    # Apply the chosen conversion\n    if formula ==htk        mel = 2595.0 * np.log10(1.0 + hz / 700.0)\n    else:  #slaney        mel = 1127.0 * np.log1p(hz / 700.0)  # log1p(x) == ln(1 + x)\n\n    # Round to 4 decimals and return as nested Python list\n    return np.round(mel, 4).tolist()\n```\n\nExample\n--------\n```python\nimport numpy as np\nprint(hz2mel(np.array([0, 6300]), formulahtk# \u2794 [0.0, 2595.0]\n```"}
{"task_id": 226, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(\n    X_train: np.ndarray,\n    y_train:  np.ndarray,\n    X_test:  np.ndarray,\n    n_clf: int = 5\n) -> list[int]:\n    \"\"\"AdaBoost with decision stumps (depth-1 trees).\n\n    Parameters\n    ----------\n    X_train : (n_samples , n_features) array\n    y_train : (n_samples,) array, labels \u22121 / 1\n    X_test  : (m_samples , n_features) array\n    n_clf   : number of stumps in the ensemble\n\n    Returns\n    -------\n    list of length m_samples containing only \u22121 or 1\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Helper: predict of one stump\n    # polarity ==  1 :  x < thresh  -> -1 ,  else 1\n    # polarity == -1 :  x < thresh  ->  1 ,  else -1\n    def stump_predict(X, feature_idx, thresh, polarity):\n        col = X[:, feature_idx]\n        preds = np.ones_like(col, dtype=float)\n        if polarity == 1:\n            preds[col < thresh] = -1\n        else:  # polarity == -1\n            preds[col < thresh] = 1\n            preds[col >= thresh] = -1\n        return preds\n    # ------------------------------------------------------------------ #\n\n    n_samples, n_features = X_train.shape\n    w = np.full(n_samples, 1.0 / n_samples)          # sample weights\n\n    # To store the learnt ensemble\n    stumps   = []     # tuples (feature, threshold, polarity)\n    alphas   = []     # learner weights\n\n    # ------------------------------------------------------------------ #\n    # Training phase\n    # ------------------------------------------------------------------ #\n    for _ in range(n_clf):\n        best_feature   = None\n        best_threshold = None\n        best_polarity  = None\n        best_error     = np.inf\n\n        # Search the best decision stump under current sample weights\n        for feature_idx in range(n_features):\n            X_col  = X_train[:, feature_idx]\n            sorted_idx = np.argsort(X_col)\n            sorted_x   = X_col[sorted_idx]\n            sorted_y   = y_train[sorted_idx]\n            sorted_w   = w[sorted_idx]\n\n            # cumulative weighted counts for quick error computation\n            pos_total = np.sum(sorted_w[sorted_y == 1])\n            neg_total = np.sum(sorted_w[sorted_y == -1])\n\n            # cumulative sums up to i (inclusive)\n            cum_pos = np.cumsum(sorted_w * (sorted_y == 1))\n            cum_neg = np.cumsum(sorted_w * (sorted_y == -1))\n\n            # iterate over possible split positions (0 .. n_samples-1)\n            for i in range(n_samples):\n                # weighted error if polarity =  1\n                err_pol1 = cum_pos[i] + (neg_total - cum_neg[i])\n                # weighted error if polarity = -1\n                err_polm1 = cum_neg[i] + (pos_total - cum_pos[i])\n\n                # pick better polarity\n                if err_pol1 < err_polm1:\n                    err, pol = err_pol1, 1\n                else:\n                    err, pol = err_polm1, -1\n\n                if err < best_error:\n                    best_error   = err\n                    best_feature = feature_idx\n                    best_polarity = pol\n                    # threshold between current value and next (no training pt exactly on thresh)\n                    if i < n_samples - 1:\n                        thr = (sorted_x[i] + sorted_x[i + 1]) / 2.0\n                    else:  # last point \u2013 put threshold slightly above max\n                        thr = sorted_x[i] + 1e-10\n                    best_threshold = thr\n\n        # Obtain predictions of the best stump found\n        predictions = stump_predict(\n            X_train, best_feature, best_threshold, best_polarity\n        )\n\n        # Weighted error (safeguard to be consistent with training set)\n        error = np.sum(w[predictions != y_train])\n\n        # Learner weight\n        alpha = 0.5 * math.log((1.0 - error) / (error + 1e-10))\n\n        # Update sample weights\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= np.sum(w)                # normalise\n\n        # Store the stump\n        stumps.append((best_feature, best_threshold, best_polarity))\n        alphas.append(alpha)\n\n    # ------------------------------------------------------------------ #\n    # Prediction phase\n    # ------------------------------------------------------------------ #\n    agg = np.zeros(X_test.shape[0], dtype=float)\n    for (feature, thresh, pol), a in zip(stumps, alphas):\n        agg += a * stump_predict(X_test, feature, thresh, pol)\n\n    final_preds = np.sign(agg)\n    # replace zeros (can happen if agg == 0) with 1\n    final_preds[final_preds == 0] = 1\n\n    return final_preds.astype(int).tolist()\n```"}
{"task_id": 227, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef value_network_forward(state: list[float],\n                          W1: list[list[float]],\n                          W2: list[list[float]],\n                          W3: list[list[float]] | list[float]) -> float:\n    \"\"\"Forward pass of a 2-hidden-layer value network using tanh activations.\n\n    Returns the scalar value rounded to 4 decimals, or \u22121 if any shape is wrong.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Convert inputs to NumPy arrays and perform shape verifications\n    # ------------------------------------------------------------------ #\n    try:\n        s  = np.asarray(state, dtype=float)\n        W1 = np.asarray(W1,    dtype=float)\n        W2 = np.asarray(W2,    dtype=float)\n        W3 = np.asarray(W3,    dtype=float)\n    except Exception:\n        return -1                       # non-numeric entries, ragged lists, etc.\n\n    # ---- basic dimensionality checks --------------------------------- #\n    if s.ndim != 1 or W1.ndim != 2 or W2.ndim != 2 or W3.ndim not in (1, 2):\n        return -1\n\n    N        = s.shape[0]\n    N_W1, H1 = W1.shape\n    H1_W2, H2 = W2.shape\n\n    # ---- check consistency across layers ----------------------------- #\n    if N_W1 != N or H1_W2 != H1:                # state\u2192H1 and H1\u2192H2 sizes\n        return -1\n\n    if W3.ndim == 1:\n        if W3.shape[0] != H2:                   # shape (H2,)\n            return -1\n    else:                                       # 2-D case: must be (H2, 1)\n        if W3.shape != (H2, 1):\n            return -1\n\n    # ------------------------------------------------------------------ #\n    # 2. Forward pass\n    # ------------------------------------------------------------------ #\n    h1 = np.tanh(s @ W1)          # (H1,)\n    h2 = np.tanh(h1 @ W2)         # (H2,)\n    v  = (h2 @ W3).item()         # scalar\n\n    # ------------------------------------------------------------------ #\n    # 3. Round to 4 decimals and return\n    # ------------------------------------------------------------------ #\n    return float(round(v, 4))\n```"}
{"task_id": 228, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pairwise_l2_distances(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n    \"\"\"Return the matrix of pair-wise Euclidean (L2) distances between rows of *X* and *Y*.\n\n    If *Y* is ``None`` the distances are computed between the rows of *X* itself\n    (i.e. *Y* is implicitly taken to be *X*).\n\n    All distances are rounded to 4 decimal places and returned as a regular Python\n    nested list (not a NumPy array).\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (N, C)\n        First set of N row-vectors of dimensionality C.\n    Y : np.ndarray | None, shape (M, C), optional\n        Second set of M row-vectors.  If ``None`` the function uses ``X`` itself.\n\n    Returns\n    -------\n    list[list[float]]\n        An N \u00d7 M matrix (as a list of lists) whose (i, j) entry contains the\n        Euclidean distance between ``X[i]`` and ``Y[j]`` rounded to 4 decimals.\n    \"\"\"\n    # If Y is not provided, compare X with itself\n    if Y is None:\n        Y = X\n\n    # Sanity-check: both inputs must have the same feature dimension\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f\"Incompatible shapes: X has {X.shape[1]} columns, \"\n            f\"but Y has {Y.shape[1]} columns.\"\n        )\n\n    # Compute squared \u21132 norms of each row\n    x2 = np.sum(X ** 2, axis=1)           # (N,)\n    y2 = np.sum(Y ** 2, axis=1)           # (M,)\n\n    # Pair-wise \u2016x \u2212 y\u2016\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2\u00b7x\u1d40y\n    dist2 = x2[:, None] + y2[None, :] - 2 * (X @ Y.T)\n\n    # Numerical errors can produce tiny negative numbers, clip them to zero\n    dist2 = np.maximum(dist2, 0.0)\n\n    # Final Euclidean distances\n    dist = np.sqrt(dist2)\n\n    # Round to 4 decimals and convert to nested Python list\n    return np.round(dist, 4).tolist()\n```"}
{"task_id": 230, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train, y_train, X_test):\n       Gaussian Naive Bayes for binary classification, written from scratch.\n    Parameters\n    ----------\n    X_train : ndarray of shape (n_samples, n_features)\n    y_train : ndarray of shape (n_samples,), values 0 or 1\n    X_test  : ndarray of shape (m_samples, n_features)\n\n    Returns\n    -------\n    list of length m_samples with predicted labels (0 or 1)\n       # ---- constants ---------------------------------------------------------\n    eps = 1e-9                              # variance smoothing\n    classes = np.array([0, 1], dtype=int)   # the two possible labels\n    n_features = X_train.shape[1]\n\n    # ---- allocate arrays to hold class-wise statistics ---------------------\n    means     = np.zeros((2, n_features), dtype=float)\n    variances = np.zeros((2, n_features), dtype=float)\n    log_priors = np.zeros(2, dtype=float)\n\n    # ---- estimate parameters per class ------------------------------------\n    for c in classes:\n        mask = (y_train == c)\n        n_c = np.sum(mask)\n\n        if n_c == 0:                  # class not present in training data\n            # Put dummy stats; log prior will be -inf so class will never win\n            means[c]     = 0.0\n            variances[c] = 1.0\n            log_priors[c] = -np.inf\n            continue\n\n        X_c = X_train[mask]\n        means[c]     = X_c.mean(axis=0)\n        variances[c] = X_c.var(axis=0) + eps       # population variance + smoothing\n        log_priors[c] = np.log(n_c / len(y_train)) # prior\n\n    # ---- pre-compute constants used in the Gaussian log-pdf ----------------\n    log_coeff = -0.5 * np.log(2.0 * np.pi * variances)  # shape: (2, n_features)\n\n    # ---- classify each test sample ----------------------------------------\n    preds = []\n    for x in X_test:                                   # x shape: (n_features,)\n        # Broadcast: (2, n_features) over features\n        diff = x - means                               # shape: (2, n_features)\n        log_likelihood = log_coeff - 0.5 * (diff ** 2) / variances\n        log_posterior  = log_priors + log_likelihood.sum(axis=1)  # sum over features\n\n        pred_label = int(np.argmax(log_posterior))     # 0 or 1\n        preds.append(pred_label)\n\n    return preds\n```"}
{"task_id": 231, "completion_id": 0, "completion": "```python\ndef is_natural(hand: list[int]) -> bool:\n       Determine whether a given Blackjack hand is a natural (blackjack).\n\n    A natural occurs only when:\n        * the hand contains exactly two cards, and\n        * one of those cards is an Ace (encoded as 1), and\n        * the other card is any ten-valued card (encoded as 10).\n\n    Parameters\n    ----------\n    hand : list[int]\n        The hand to evaluate, encoded as integers.\n\n    Returns\n    -------\n    bool\n        True  \u2013 if the hand is a natural blackjack (Ace + 10).  \n        False \u2013 for every other hand.\n       # A hand is a natural iff it has exactly two distinct cards {1, 10}.\n    return len(hand) == 2 and set(hand) == {1, 10}\n```"}
{"task_id": 232, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef is_one_hot(x: np.ndarray) -> bool:\n    \"\"\"Check if *x* is a valid one-hot encoded NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A NumPy array to verify.\n\n    Returns\n    -------\n    bool\n        True if *x* is one-hot encoded, otherwise False.\n    \"\"\"\n    # Rule 1: must be a 2-D NumPy array\n    if not isinstance(x, np.ndarray) or x.ndim != 2:\n        return False\n\n    # Empty matrices (no rows or no columns) cannot be valid one-hot encodings\n    if x.shape[0] == 0 or x.shape[1] == 0:\n        return False\n\n    # Rule 2: all values must be 0 or 1 (allowing bools, ints, floats with exact 0/1)\n    if not np.logical_or(x == 0, x == 1).all():\n        return False\n\n    # Rule 3: each row must contain exactly one 1\n    if not np.all(x.sum(axis=1) == 1):\n        return False\n\n    return True\n```"}
{"task_id": 234, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef thompson_sampling_beta_binomial(successes: list[int],\n                                    failures: list[int],\n                                    prior_alpha: float = 1.0,\n                                    prior_beta:  float = 1.0,\n                                    seed: int | None = None) -> int:\n   Single-step Thompson sampling for a Bernoulli multi-armed bandit    # Basic sanity checks -----------------------------------------------------\n    if (not successes) or (not failures) or (len(successes) != len(failures)):\n        return -1\n\n    # Cast to NumPy arrays (float so we can add the priors)\n    try:\n        suc = np.asarray(successes, dtype=float)\n        fail = np.asarray(failures,  dtype=float)\n    except Exception:\n        return -1\n\n    # Negative counts are not admissible\n    if np.any(suc < 0) or np.any(fail < 0):\n        return -1\n\n    # Posterior parameters for every arm -------------------------------------\n    alpha = prior_alpha + suc\n    beta  = prior_beta  + fail\n\n    # Beta-distribution requires strictly positive shape parameters\n    if np.any(alpha <= 0) or np.any(beta <= 0):\n        return -1\n\n    # Deterministic behaviour (optional) -------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Thompson sampling: one draw from every posterior ------------------------\n    samples = np.random.beta(alpha, beta)\n\n    # Pick the arm with the highest sample (ties resolved by lowest index) ----\n    return int(np.argmax(samples))\n```"}
{"task_id": 235, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_one_hot(labels: np.ndarray, n_classes: int | None = None) -> np.ndarray:\n    \"\"\"Convert integer class labels to one-hot encodings.\"\"\"\n    # Ensure NumPy array\n    labels = np.asarray(labels)\n\n    # 1. Validate dimensionality\n    if labels.ndim != 1:\n        raise ValueError(\"`labels` must be a 1-D array; \"\n                         f\"got array with ndim={labels.ndim}.\")\n\n    # 2. Special-case: empty input ------------------------------------------\n    if labels.size == 0:\n        if n_classes is None:\n            raise ValueError(\"Cannot infer the number of classes from an \"\n                             \"empty array; please supply `n_classes`.\")\n        return np.empty((0, n_classes), dtype=float)\n\n    # 3. Basic label checks --------------------------------------------------\n    if np.any(labels < 0):\n        raise ValueError(\"Class labels must be non-negative integers.\")\n    if not np.issubdtype(labels.dtype, np.integer):\n        raise ValueError(\"`labels` must contain integers.\")\n\n    # 4. Determine number of classes ----------------------------------------\n    max_label = int(labels.max())\n    if n_classes is None:\n        C = max_label + 1\n    else:\n        if n_classes < max_label + 1:\n            raise ValueError(\"`n_classes` must be at least max(labels)+1 \"\n                             f\"({max_label + 1}); got {n_classes}.\")\n        C = n_classes\n\n    # 5. Build one-hot matrix ------------------------------------------------\n    N = labels.shape[0]\n    one_hot = np.zeros((N, C), dtype=float)\n    one_hot[np.arange(N), labels] = 1.0\n    return one_hot\n```"}
{"task_id": 236, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef minibatch(X: np.ndarray, batchsize: int = 256, shuffle: bool = True):\n   Create a generator that yields index mini-batches.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (N, *). The first axis holds the samples.\n    batchsize : int, default = 256\n        Desired mini-batch size. The last batch may be smaller.\n    shuffle : bool, default = True\n        Whether to shuffle sample indices before splitting.\n\n    Returns\n    -------\n    tuple\n        (mb_generator, n_batches)\n        mb_generator : generator yielding NumPy integer arrays \u2013 the indices\n            of each mini-batch in the order they are processed.\n        n_batches : int \u2013 total number of mini-batches.\n       if batchsize <= 0:\n        raise ValueError`batchsize` must be a positive integer    N = int(X.shape[0])                        # number of samples\n    n_batches = (N + batchsize - 1) // batchsize  # ceil division\n\n    # Build the index array and shuffle in-place if requested\n    indices = np.arange(N, dtype=np.int64)\n    if shuffle:\n        np.random.shuffle(indices)             # in-place shuffle\n\n    # Single-pass generator\n    def _generator():\n        for start in range(0, N, batchsize):\n            yield indices[start:start + batchsize]\n\n    return _generator(), n_batches\n```"}
{"task_id": 237, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef score(hand: List[str]) -> int:\n   Return the Blackjack score of *hand* or 0 if the hand is bust.\n\n    The function uses standard Blackjack counting rules:\n    - 2\u201310 are worth their face value.\n    - J, Q, K are worth 10.\n    - A can be worth 11 or 1, whichever keeps the total highest\n      without going over 21.\n\n    Args:\n        hand: list of card labels ('2'-'10', 'J', 'Q', 'K', 'A').\n\n    Returns:\n        Integer 0\u201321.  Returns 0 if no assignment of Ace values\n        prevents the hand from busting (>21).\n       total = 0           # running total without Aces\n    aces = 0            # number of Aces encountered\n\n    # First count non-Aces and remember how many Aces we have\n    for card in hand:\n        if card == 'A':\n            aces += 1\n        elif card in {'J', 'Q', 'K'}:\n            total += 10\n        else:                           # numeric card '2'\u2013'10'\n            total += int(card)\n\n    # Treat every Ace as 11 to begin with\n    total += aces * 11\n\n    # Downgrade Ace(s) from 11 to 1 as needed to avoid bust\n    while total > 21 and aces:\n        total -= 10      # changing one Ace 11\u21921 lowers total by 10\n        aces -= 1\n\n    return total if total <= 21 else 0\n```"}
{"task_id": 239, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import Tuple\n\ndef determine_padding(\n    filter_shape: Tuple[int, int],\n    output_shape: str = \"same\"\n) -> Tuple[Tuple[int, int], Tuple[int, int]]:\n    \"\"\"Calculate the zero\u2013padding required at each border for a 2-D convolution\n    with stride = 1.\n\n    Parameters\n    ----------\n    filter_shape : tuple[int, int]\n        (filter_height, filter_width).\n    output_shape : {\"same\", \"valid\"}, default \"same\"\n        \"same\"  \u2013 pad so that the output spatial dimensions equal the input.\n        \"valid\" \u2013 no padding.\n\n    Returns\n    -------\n    ((pad_top, pad_bottom), (pad_left, pad_right)) : tuple[tuple[int, int], tuple[int, int]]\n        Non\u2013negative integers specifying the amount of padding for every side.\n    \"\"\"\n    # Normalise flag to lower-case to tolerate \"Same\"/\"VALID\"/\u2026\n    output_shape = output_shape.lower()\n\n    if output_shape == \"valid\":\n        return ((0, 0), (0, 0))\n\n    if output_shape != \"same\":\n        raise ValueError('output_shape must be either \"same\" or \"valid\".')\n\n    fh, fw = filter_shape\n\n    # Total padding needed in each dimension (stride = 1)\n    pad_h = fh - 1          # vertical\n    pad_w = fw - 1          # horizontal\n\n    # Distribute the padding: smaller part goes to the \"top/left\"\n    pad_top    = pad_h // 2\n    pad_bottom = pad_h - pad_top\n\n    pad_left   = pad_w // 2\n    pad_right  = pad_w - pad_left\n\n    return ((pad_top, pad_bottom), (pad_left, pad_right))\n```"}
{"task_id": 240, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections.abc import Mapping\n\ndef _invert_mapping(mapping: Mapping, name: str) -> dict:\n   Return the inverse of a one-to-one mapping, raising if it is invalid    inv = {}\n    for k, v in mapping.items():\n        if v in inv:\n            raise ValueError(\n                fDuplicate numeric code {v!r} detected in {name}.               Every symbol must map to a unique integer            )\n        inv[v] = k\n    return inv\n\n\ndef _build_from_list(items: list, name: str) -> dict:\n   Enumerate a list into {symbol: index} while checking uniqueness    # ``dict.fromkeys`` keeps first occurrence order \u2013 handy for checking dups.\n    if len(items) != len(dict.fromkeys(items)):\n        raise ValueError(fDuplicate identifiers detected in {name}.    return {sym: idx for idx, sym in enumerate(items)}\n\n\ndef create_encoding_dicts(\n    obs_ids: list,\n    action_ids: list,\n    obs_encoder: dict | None = None,\n    act_encoder: dict | None = None,\n) -> dict:\n   Generate forward and backward mappings for observations and actions.\n\n    Args\n    ----\n    obs_ids     : list            \u2013 all distinct observation identifiers.\n    action_ids  : list            \u2013 all distinct action identifiers.\n    obs_encoder : dict | None     \u2013 optional predefined obs\u2192number mapping.\n    act_encoder : dict | None     \u2013 optional predefined act\u2192number mapping.\n\n    Returns\n    -------\n    dict with the four keys:\n        'obs2num', 'num2obs', 'act2num', 'num2act'\n       # ------------------------------------------------------------------\n    # Forward mappings (symbol \u2192 number)\n    # ------------------------------------------------------------------\n    if obs_encoder is not None:\n        if not isinstance(obs_encoder, Mapping):\n            raise TypeErrorobs_encoder must be a mapping or None        obs2num = dict(obs_encoder)\n    else:\n        obs2num = _build_from_list(obs_ids,obs_ids    if act_encoder is not None:\n        if not isinstance(act_encoder, Mapping):\n            raise TypeErroract_encoder must be a mapping or None        act2num = dict(act_encoder)\n    else:\n        act2num = _build_from_list(action_ids,action_ids    # ------------------------------------------------------------------\n    # Backward mappings (number \u2192 symbol)\n    # ------------------------------------------------------------------\n    num2obs = _invert_mapping(obs2num,obs2num if obs2num else {}\n    num2act = _invert_mapping(act2num,act2num if act2num else {}\n\n    # ------------------------------------------------------------------\n    # Final consistency check (optional runtime safeguard)\n    # ------------------------------------------------------------------\n    for sym in obs2num:\n        assert num2obs[obs2num[sym]] == sym,Inconsistent observation maps    for act in act2num:\n        assert num2act[act2num[act]] == act,Inconsistent action maps    # ------------------------------------------------------------------\n    return {\n       obs2num obs2num,\n       num2obs num2obs,\n       act2num act2num,\n       num2act num2act,\n    }\n```\n\nHow it works\n------------\n\n1. If an explicit encoder is supplied, the function copies it verbatim (after\n   checking that it is a proper mapping). Otherwise it enumerates the\n   identifiers in the incoming list, thereby guaranteeing the requested order.\n\n2. Helper `_invert_mapping` produces the inverse dictionary and simultaneously\n   verifies that every numeric code is unique (one-to-one property).\n\n3. When no identifiers are available **and** no encoder is given, the\n   corresponding dictionaries are simply empty, satisfying the \u201cspecial\n   cases\u201d requirement.\n\n4. Two `assert` statements give an additional runtime guarantee that\n   `num2obs[obs2num[x]] == x` and\n   `num2act[act2num[a]] == a` for every key.\n\nExample\n-------\n\n```\n>>> obs  = ['left', 'right', 'front', 'back']\n>>> acts = ['move', 'turn']\n>>> create_encoding_dicts(obs, acts)\n{'obs2num': {'left': 0, 'right': 1, 'front': 2, 'back': 3},\n 'num2obs': {0: 'left', 1: 'right', 2: 'front', 3: 'back'},\n 'act2num': {'move': 0, 'turn': 1},\n 'num2act': {0: 'move', 1: 'turn'}}\n```\n\nThe output matches the specification exactly."}
{"task_id": 241, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix (Saxe et al., 2014).\n\n    Args:\n        shape: Desired output shape (length \u2265 2).\n        scale: Scalar multiplier for the orthogonal matrix.\n\n    Returns:\n        A Python list representing the orthogonally-initialised tensor\n        (rows or columns\u2014whichever are fewer\u2014are orthonormal up to `scale`).\n        Returns -1 if `shape` has fewer than two dimensions.\n    \"\"\"\n    # At least two dimensions are required\n    if len(shape) < 2:\n        return -1\n\n    d0 = shape[0]                             # first dimension\n    d_rest = int(np.prod(shape[1:]))          # product of the remaining dims\n\n    # 1 \u2013 2.  Draw a random Gaussian matrix A\n    A = np.random.normal(0.0, 1.0, size=(d0, d_rest))\n\n    # 3. SVD: A = U \u03a3 V\u1d40   (economy/compact SVD)\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n\n    # 4. Pick the factor that has the same shape as A\n    Q = U if U.shape == A.shape else Vt\n\n    # 5. Reshape back and scale\n    Q = Q.reshape(shape) * scale\n\n    # Return as a (nested) Python list\n    return Q.tolist()\n```\nThe function follows the exact steps dictated by Saxe et al. and returns a plain Python list whose first two axes form an orthogonal (up to `scale`) matrix."}
{"task_id": 242, "completion_id": 0, "completion": "```python\nimport math\ndef ucb1_select(ev_estimates: list[float], pull_counts: list[int], current_step: int, C: float = 1.0) -> int:\n   Return the arm index that UCB1 would pick next.\n\n    PARAMETERS\n    ----------\n    ev_estimates : list[float]\n        Current empirical mean reward for every arm.\n    pull_counts  : list[int]\n        How many times each arm has been pulled so far (same length as\n        ``ev_estimates``).\n    current_step : int\n        The time-step about to be executed.  The quantity *T* in the UCB1\n        formula equals ``current_step + 1``.\n    C : float, default 1.0\n        Exploration parameter.  ``C = 1.0`` recovers the original UCB1.\n\n    RETURNS\n    -------\n    int\n        Index (0-based) of the arm selected by UCB1.\n       # Try each arm at least once: return the first unseen arm\n    for idx, pulls in enumerate(pull_counts):\n        if pulls == 0:\n            return idx\n\n    # All arms have been seen at least once: compute UCB scores\n    t = current_step + 1               # Total number of past pulls (1-based)\n    log_t = math.log(t)                # ln(T)\n    best_idx = -1\n    best_score = -math.inf\n\n    for idx, (mean, pulls) in enumerate(zip(ev_estimates, pull_counts)):\n        exploration = C * math.sqrt(2.0 * log_t / pulls)\n        score = mean + exploration\n        if score > best_score:\n            best_score = score\n            best_idx = idx\n\n    return best_idx\n```"}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n   Forward pass of the tiny fully-connected Actor\u2013Critic network.\n\n    Architecture (all weights = 1.0, all biases = 0.0):\n        1. Dense 3  \u2192 4, ReLU\n        2. Dense 4  \u2192 4, ReLU\n        3. Dense 4  \u2192 4, ReLU\n        4. Actor head 4 \u2192 3, Softmax\n        5. Critic head 4 \u2192 1, linear\n    \n    Args\n    ----\n    state : list[float]\n        A list of exactly three floats [s0, s1, s2].\n\n    Returns\n    -------\n    tuple\n        ( [p0, p1, p2], value ) where the three action probabilities and\n        the scalar state value are all rounded to 4 decimal places.\n       # --- preparation & input check ------------------------------------------\n    s = np.asarray(state, dtype=float)\n    if s.shape != (3,):\n        raise ValueErrorState must be a list of exactly three floats    \n    # --- shared feature extractor -------------------------------------------\n    # Layer 1 -------------------------------------------------\n    z1_comp = max(0.0, s.sum())          # each of 4 neurons after first ReLU\n    \n    # Layer 2 -------------------------------------------------\n    z2_comp = 4.0 * z1_comp              # still identical across 4 neurons\n    \n    # Layer 3 -------------------------------------------------\n    z3_comp = 4.0 * z2_comp              # = 16 * z1_comp\n    \n    # --- heads ----------------------------------------------\n    # Actor head (softmax over three identical logits) -------\n    logits_comp = 4.0 * z3_comp          # = 64 * z1_comp\n    logits      = np.full(3, logits_comp)\n    \n    # softmax \u2013 stable implementation although all equal\n    exp_logits      = np.exp(logits - logits.max())\n    action_probs_np = exp_logits / exp_logits.sum()\n    \n    # Critic head -------------------------------------------\n    state_value = 4.0 * z3_comp          # = 64 * z1_comp\n    \n    # --- rounding & output ---------------------------------\n    action_probs = [round(float(p), 4) for p in action_probs_np]\n    state_value  = round(float(state_value), 4)\n    \n    return action_probs, state_value\n```\n\nThe computation follows the exact analytical simplifications for this \u201call-ones\u201d network:\n\u2022 z\u2081 component = max(0, s\u2080+s\u2081+s\u2082)  \n\u2022 z\u2082 component = 4\u00b7z\u2081, z\u2083 component = 16\u00b7z\u2081  \n\u2022 logits = 64\u00b7z\u2081 (identical for the three actions \u21d2 softmax \u2192 \u2153 each)  \n\u2022 state value = 64\u00b7z\u2081  \n\nBoth probabilities and value are finally rounded to four decimal places before returning."}
{"task_id": 244, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple, sparsity: float = 0.5) -> list:\n    \"\"\"Generate a random binary tensor.\n\n    Parameters\n    ----------\n    shape : tuple\n        Desired tensor shape.  Must be a *tuple of positive integers*.\n    sparsity : float, optional\n        Probability that any entry equals 1.0.  Must lie in the closed\n        interval [0, 1].  Defaults to 0.5.\n\n    Returns\n    -------\n    list\n        Nested Python list with the same structure as the requested tensor,\n        containing only the floats 0.0 and 1.0.\n        Returns -1 if either argument is invalid.\n    \"\"\"\n\n    # ----------------- argument validation -----------------\n    # shape: must be a tuple of positive ints\n    if (not isinstance(shape, tuple) or len(shape) == 0 or\n        not all(isinstance(d, int) and d > 0 for d in shape)):\n        return -1\n\n    # sparsity: must be real number in [0, 1]\n    if (not isinstance(sparsity, (int, float, np.floating)) or\n        sparsity < 0.0 or sparsity > 1.0):\n        return -1\n\n    # ----------------- tensor generation ------------------\n    # Draw uniform random numbers in [0, 1)\n    rnd = np.random.rand(*shape)\n\n    # Convert to binary: 1.0 with probability `sparsity`\n    # Threshold is 1 - sparsity, as explained in the task description.\n    binary_tensor = (rnd >= (1.0 - sparsity)).astype(float)\n\n    # Return as nested Python list\n    return binary_tensor.tolist()\n```"}
{"task_id": 247, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gaussian_bandit_oracle(payoff_dists: list[tuple[float, float]],\n                           payoff_probs: list[float]) -> tuple[float, int] | int:\n   Return (best_ev, best_arm) for a Gaussian bandit or \u20131 if input is invalid.\n\n    best_ev  \u2013 maximum expected value (p_k * \u03bc_k) rounded to 4 decimals\n    best_arm \u2013 0-based index of the arm that attains best_ev (smallest index if\n               several arms tie)\n\n    Invalid input rules\n    -------------------\n    1. payoff_dists and payoff_probs must have the same non-zero length\n    2. Every variance \u03c3\u00b2 must be strictly positive\n    3. Every probability p_k must lie in the closed interval [0, 1]\n       # ---------- basic checks -------------------------------------------------\n    if not isinstance(payoff_dists, list) or not isinstance(payoff_probs, list):\n        return -1\n\n    if len(payoff_dists) == 0 or len(payoff_dists) != len(payoff_probs):\n        return -1\n\n    # ---------- extract parameters & validate --------------------------------\n    mus, vars_, ps = [], [], []\n    for dist, p in zip(payoff_dists, payoff_probs):\n        # dist must be a 2-tuple (\u03bc, \u03c3\u00b2)\n        if (not isinstance(dist, tuple)) or len(dist) != 2:\n            return -1\n        mu, var = dist\n\n        # convert to float (catches strings, None, etc.)\n        try:\n            mu = float(mu)\n            var = float(var)\n            p = float(p)\n        except (TypeError, ValueError):\n            return -1\n\n        # validity checks\n        if np.isnan(var) or np.isnan(p) or np.isnan(mu):\n            return -1\n        if var <= 0:\n            return -1\n        if p < 0 or p > 1:\n            return -1\n\n        mus.append(mu)\n        vars_.append(var)\n        ps.append(p)\n\n    # ---------- compute expected values --------------------------------------\n    mus_arr = np.asarray(mus, dtype=float)\n    ps_arr = np.asarray(ps, dtype=float)\n    expected_values = mus_arr * ps_arr       # element-wise product\n\n    # ---------- select the best arm ------------------------------------------\n    best_arm = int(np.argmax(expected_values))             # first index of max\n    best_ev_raw = float(expected_values[best_arm])\n    best_ev = round(best_ev_raw, 4)\n\n    return best_ev, best_arm\n```"}
{"task_id": 248, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef baseline_prob_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[float]:\n    \"\"\"Return a constant probability equal to the fraction of positive labels.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix. (Not used by the baseline.)\n    y_train : np.ndarray\n        One-dimensional array of 0/1 labels.\n    X_test : np.ndarray\n        Feature matrix whose rows need probability predictions.\n\n    Returns\n    -------\n    list[float]\n        One probability per row of `X_test`, all identical and rounded to\n        four decimal places.  If `X_test` has zero rows an empty list is\n        returned.\n    \"\"\"\n    # If there is nothing to predict for, return an empty list immediately.\n    n_test = X_test.shape[0] if X_test.ndim > 0 else 0\n    if n_test == 0:\n        return []\n\n    # Avoid division-by-zero if y_train is empty.\n    if y_train.size == 0:\n        positive_fraction = 0.0\n    else:\n        # Mean of 0/1 labels = fraction of ones.\n        positive_fraction = float(np.mean(y_train))\n\n    # Round to four decimal places.\n    p = round(positive_fraction, 4)\n\n    # Produce one probability per test instance.\n    return [p] * n_test\n```"}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _relu(x: np.ndarray) -> np.ndarray:\n    \"\"\"Re-LU activation (inlined for clarity).\"\"\"\n    return np.maximum(0, x)\n\n\ndef _softmax(x: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable soft-max.\"\"\"\n    x = x - np.max(x)           # shift for numerical stability\n    exp_x = np.exp(x)\n    return exp_x / exp_x.sum()\n\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    Network:\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU\n              \u251c\u2500 actor head  (Dense \u2192 soft-max) \u2192 action probabilities\n              \u2514\u2500 critic head (Dense)            \u2192 state value\n    \"\"\"\n    # -------- 1. convert inputs to NumPy --------\n    x = np.asarray(state, dtype=np.float64)\n\n    W1, b1 = np.asarray(params[\"W1\"]), np.asarray(params[\"b1\"])\n    W2, b2 = np.asarray(params[\"W2\"]), np.asarray(params[\"b2\"])\n    W3, b3 = np.asarray(params[\"W3\"]), np.asarray(params[\"b3\"])\n\n    Wa, ba = np.asarray(params[\"Wa\"]), np.asarray(params[\"ba\"])\n    Wc, bc = np.asarray(params[\"Wc\"]), np.asarray(params[\"bc\"])\n\n    # -------- 2. shared trunk (3 \u00d7 Dense + ReLU) --------\n    h = _relu(x @ W1 + b1)\n    h = _relu(h @ W2 + b2)\n    h = _relu(h @ W3 + b3)\n\n    # -------- 3. actor head --------\n    logits = h @ Wa + ba\n    probs = _softmax(logits)\n\n    # -------- 4. critic head --------\n    value = float(h @ Wc + bc)\n\n    # -------- 5. rounding --------\n    probs_rounded = [round(float(p), 4) for p in probs]\n    value_rounded = round(value, 4)\n\n    return probs_rounded, value_rounded\n```"}
{"task_id": 251, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _round_output(out):\n       Round every numeric entry to 4 decimals while preserving the\n    requested output structure (scalar \u2192 scalar, vector \u2192 np.ndarray).\n       if np.isscalar(out):\n        # Python\u2019s round returns a float when ndigits is supplied\n        return round(float(out), 4)\n    else:\n        return np.round(out, 4)\n\n\ndef sigmoid(x):\n   Element-wise logistic sigmoid    if np.isscalar(x):\n        return _round_output(1.0 / (1.0 + np.exp(-x)))\n    x = np.asarray(x, dtype=float)\n    return _round_output(1.0 / (1.0 + np.exp(-x)))\n\n\ndef tanh(x):\n   Element-wise hyperbolic tangent    if np.isscalar(x):\n        return _round_output(np.tanh(x))\n    x = np.asarray(x, dtype=float)\n    return _round_output(np.tanh(x))\n\n\ndef relu(x):\n   Element-wise ReLU    if np.isscalar(x):\n        return _round_output(max(0.0, x))\n    x = np.asarray(x, dtype=float)\n    return _round_output(np.maximum(0.0, x))\n\n\ndef leaky_relu(x, alpha: float = 0.01):\n   Element-wise Leaky-ReLU with slope *alpha* for x < 0    if np.isscalar(x):\n        return _round_output(x if x >= 0 else alpha * x)\n    x = np.asarray(x, dtype=float)\n    return _round_output(np.where(x >= 0, x, alpha * x))\n\n\ndef softmax(x):\n   Numerically-stable softmax over a 1-D input    if np.isscalar(x):\n        # Softmax of a single value is 1.0\n        return _round_output(1.0)\n\n    x = np.asarray(x, dtype=float).ravel()          # ensure 1-D\n    shift_x = x - np.max(x)                         # stability trick\n    exp_x = np.exp(shift_x)\n    softmax_x = exp_x / np.sum(exp_x)\n    return _round_output(softmax_x)\n\n\ndef get_activation(name):\n       Retrieve an activation function by name.\n\n    Parameters\n    ----------\n    name : str\n        One of {'sigmoid', 'tanh', 'relu', 'leaky_relu', 'softmax'}.\n\n    Returns\n    -------\n    Callable\n        The corresponding activation function.\n\n    Raises\n    ------\n    ValueError\n        If *name* is not recognised.\n       mapping = {\n       sigmoid sigmoid,\n       tanh tanh,\n       relu relu,\n       leaky_relu leaky_relu,\n       softmax softmax,\n    }\n    try:\n        return mapping[name]\n    except KeyError:  # unknown activation name\n        raise ValueErrorInvalid activation function from None\n```"}
{"task_id": 252, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef least_squares_loss(actual: np.ndarray, predicted: np.ndarray) -> tuple[list[float], list[float]]:\n   Compute the gradient and (diagonal) Hessian of the least-squares loss.\n\n    Least-squares loss: 0.5 * ||actual \u2212 predicted||\u00b2\n\n    For every coordinate i:\n        gradient_i = actual[i] \u2212 predicted[i]\n        hessian_i  = 1\n\n    Args:\n        actual (np.ndarray): 1-D array of true targets.\n        predicted (np.ndarray): 1-D array of model predictions.\n\n    Returns:\n        tuple[list[float], list[float]]: (grad, hess)\n            grad  \u2013 list of gradients for each prediction element.\n            hess  \u2013 list of diagonal Hessian entries (all ones).\n       # Gradient: element-wise difference\n    grad = (actual - predicted).tolist()\n\n    # Hessian: ones with the same length\n    hess = [1.0] * len(actual)\n\n    return grad, hess\n```"}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n   Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w (list | np.ndarray): 1-D weight vector.\n        alpha (float): Overall regularisation strength (\u03b1 \u2265 0).\n        l1_ratio (float): L1 component fraction (0 \u2264 l1_ratio \u2264 1).\n        gradient (bool): If True, return \u2207R(w); otherwise return R(w).\n\n    Returns:\n        float | list[float]: Rounded penalty value or gradient.\n       # convert to numpy array of type float\n    w = np.asarray(w, dtype=float)\n\n    # Elastic-Net components\n    l1_part = np.sum(np.abs(w))                 # \u2016w\u2016\u2081\n    l2_part = 0.5 * np.dot(w, w)                # 0.5\u00b7w\u1d40w\n\n    if not gradient:\n        # Penalty value\n        penalty = alpha * (l1_ratio * l1_part + (1 - l1_ratio) * l2_part)\n        return round(float(np.round(penalty, 4)), 4)\n\n    # Gradient\n    grad = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n    return [float(v) for v in np.round(grad, 4)]\n```"}
{"task_id": 255, "completion_id": 0, "completion": "```python\ndef usable_ace(hand: list[int]) -> bool:\n    \"\"\"Determine whether a Blackjack hand has a usable ace.\n\n    A *usable ace* is an ace that can be counted as 11 (rather than 1) without\n    the hand's total value exceeding 21.\n\n    Args:\n        hand: List of integers where each integer represents the value of a card\n              in a Blackjack hand (ace is 1, face cards are 10, etc.).\n\n    Returns:\n        True if the hand contains a usable ace, otherwise False.\n    \"\"\"\n    # Sum the hand with all aces counted as 1\n    total = sum(hand)\n\n    # Check whether at least one ace is present\n    have_ace = 1 in hand\n\n    # If we have an ace, see if treating ONE ace as 11 (i.e., +10 more)\n    # keeps the total at 21 or less.\n    return have_ace and total + 10 <= 21\n```\n\nExplanation of the key idea:\n\n1. Count every ace as 1 when computing the raw total (`total = sum(hand)`).\n2. If the hand contains at least one ace (`have_ace`), imagine turning exactly one of those aces from 1 into 11 \u2015 that adds 10 more to the total (`total + 10`).\n3. If the adjusted total is still `<= 21`, then that ace is *usable*, so return `True`; otherwise return `False`."}
{"task_id": 256, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom copy import deepcopy          # noqa: F401  (kept because the statement appeared in the stub)\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n   Numerically estimate the gradient of an RNN parameter using centred finite\n    differences (double\u2013precision).\n\n    The function perturbs one entry of the selected weight / bias at a time,\n    runs a *full* forward pass over ``n_t`` time-steps for the \u201c+\u03b5\u2019\u2019 and \u201c\u2013\u03b5\u2019\u2019\n    variants and stores\n\n            ( L(\u03b8+\u03b5) \u2013 L(\u03b8\u2013\u03b5) ) / (2\u03b5)\n\n    in the corresponding position of the returned array (*transposed* before\n    returning).\n\n    See the doc-string in the task description for detailed explanations of the\n    arguments.\n       # ----------------------------------------------------------------------- #\n    # 1. Handle the special parameter names                                   #\n    # ----------------------------------------------------------------------- #\n    if param_name inXy              # non-trainable \u2192 nothing to do\n        return None\n\n    key = param_name.lower() if param_name inBaBx else param_name\n\n    if key not in model.parameters:           # defensive programming\n        raise ValueError(fParameter \u201c{param_name}\u201d not found in model    # ----------------------------------------------------------------------- #\n    # 2. Make sure we work in float64 precision                               #\n    # ----------------------------------------------------------------------- #\n    theta = model.parameters[key]\n    if theta.dtype != np.float64:\n        # Replace the parameter inside the model by a float64 copy so that all\n        # subsequent computations (and in-place perturbations) are done with\n        # higher numerical precision.\n        theta = theta.astype(np.float64)\n        model.parameters[key] = theta\n\n    grads = np.zeros_like(theta, dtype=np.float64)      # accumulator\n\n    # ----------------------------------------------------------------------- #\n    # 3. Helper: run one complete forward pass and compute the loss           #\n    # ----------------------------------------------------------------------- #\n    def _forward_sequence() -> float:\n       Run the model for n_t steps and return the scalar loss        model.flush_gradients()          # reset anything stored in the model\n        preds = []\n        for t in range(n_t):\n            x_t = X[:, :, t].astype(np.float64)\n            preds.append(model.forward(x_t))\n        return float(loss_func(preds))   # ensure a python/float64 scalar\n\n    # ----------------------------------------------------------------------- #\n    # 4. Centred finite-difference loop                                       #\n    # ----------------------------------------------------------------------- #\n    # Iterate over every scalar entry of the parameter tensor\n    it = np.nditer(theta, flagsmulti_index op_flagsreadwrite    for _ in it:\n        idx = it.multi_index\n        original_val = theta[idx]           # cache the unperturbed value\n\n        # \u03b8 + \u03b5\n        theta[idx] = original_val + epsilon\n        loss_plus = _forward_sequence()\n\n        # \u03b8 \u2013 \u03b5\n        theta[idx] = original_val - epsilon\n        loss_minus = _forward_sequence()\n\n        # Restore the original value\n        theta[idx] = original_val\n\n        # Numerical gradient\n        grads[idx] = (loss_plus - loss_minus) / (2.0 * epsilon)\n\n    # ----------------------------------------------------------------------- #\n    # 5. Return the *transposed* gradient as required by the task             #\n    # ----------------------------------------------------------------------- #\n    return grads.T.astype(np.float64)\n```"}
{"task_id": 257, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray,\n                     X_test:  np.ndarray, n_clf: int = 5) -> list[int]:\n   AdaBoost with decision stumps (no external ML libraries).    # ------------------------------------------------------------------ #\n    # Helper \u2013 single decision stump                                     #\n    # ------------------------------------------------------------------ #\n    class Stump:\n        __slots__ =featurethresholdpolarity        def __init__(self, feature: int, threshold: float, polarity: int):\n            self.feature   = feature     # column index\n            self.threshold = threshold   # split value\n            self.polarity  = polarity    #  1 or -1\n\n        # vectorised prediction for a 2-D array\n        def predict(self, X: np.ndarray) -> np.ndarray:\n            col = X[:, self.feature]\n            if self.polarity == 1:\n                #  predict  1  if value < threshold else -1\n                return np.where(col < self.threshold,  1, -1)\n            else:  # polarity == -1\n                #  predict -1  if value < threshold else  1\n                return np.where(col < self.threshold, -1,  1)\n\n    # ------------------------------------------------------------------ #\n    # Input checks / initialisation                                      #\n    # ------------------------------------------------------------------ #\n    if n_clf < 1:\n        n_clf = 1\n    m, n = X_train.shape\n    w = np.full(m, 1.0 / m)                      # initial sample weights\n\n    stumps, alphas = [], []\n    eps = 1e-10                                   # numerical safety\n\n    # ================================================================== #\n    #                         Boosting loop                              #\n    # ================================================================== #\n    for _ in range(n_clf):\n        best_stump   = None\n        best_error   = np.inf\n\n        # -------------------------------------------------------------- #\n        #           search best decision stump for this round            #\n        # -------------------------------------------------------------- #\n        for feature in range(n):                                  # each column\n            idx_sorted = np.argsort(X_train[:, feature])\n            x_sorted   = X_train[idx_sorted, feature]\n            y_sorted   = y_train[idx_sorted]\n            w_sorted   = w[idx_sorted]\n\n            # cumulative sums of positive / negative weights\n            pos_cum = np.concatenate(([0.0],\n                       np.cumsum(w_sorted * (y_sorted == 1))))\n            neg_cum = np.concatenate(([0.0],\n                       np.cumsum(w_sorted * (y_sorted == -1))))\n            total_pos, total_neg = pos_cum[-1], neg_cum[-1]\n\n            # errors if LEFT side gets label  1  (polarity =  1)\n            err_p1 = neg_cum + (total_pos - pos_cum)          # w_left_neg + w_right_pos\n            # errors if LEFT side gets label -1  (polarity = -1)\n            err_m1 = pos_cum + (total_neg - neg_cum)          # w_left_pos + w_right_neg\n\n            # locate minimums\n            min_idx_p1 = np.argmin(err_p1)\n            min_idx_m1 = np.argmin(err_m1)\n            min_err_p1 = err_p1[min_idx_p1]\n            min_err_m1 = err_m1[min_idx_m1]\n\n            # choose polarity that really minimises the error for this feature\n            if min_err_p1 <= min_err_m1:\n                idx_thr   = min_idx_p1\n                err       = min_err_p1\n                polarity  = 1\n            else:\n                idx_thr   = min_idx_m1\n                err       = min_err_m1\n                polarity  = -1\n\n            # compute a threshold value for the chosen index\n            if idx_thr == 0:\n                threshold = x_sorted[0] - 1e-12\n            elif idx_thr == m:\n                threshold = x_sorted[-1] + 1e-12\n            else:\n                threshold = (x_sorted[idx_thr - 1] + x_sorted[idx_thr]) / 2.0\n\n            # keep the global best\n            if err < best_error - 1e-12:                     # tiny tolerance\n                best_error = err\n                best_stump = Stump(feature, threshold, polarity)\n\n        # -------------------------------------------------------------- #\n        #            Break if we cannot find a better stump             #\n        # -------------------------------------------------------------- #\n        # (Should not normally happen because polarity search guarantees\n        #  error <= 0.5, but for completeness:)\n        if best_error >= 0.5 - eps:\n            break\n\n        # -------------------------------------------------------------- #\n        #              compute stump weight  (alpha_t)                  #\n        # -------------------------------------------------------------- #\n        alpha_t = 0.5 * np.log((1.0 - best_error + eps) / (best_error + eps))\n\n        # -------------------------------------------------------------- #\n        #                   update sample weights                       #\n        # -------------------------------------------------------------- #\n        pred = best_stump.predict(X_train)\n        w   *= np.exp(-alpha_t * y_train * pred)      # move mass to wrong ones\n        w   /= w.sum()                                # re-normalise\n\n        # store\n        stumps.append(best_stump)\n        alphas.append(alpha_t)\n\n        # optional early stop: perfect classification\n        if best_error <= eps:\n            break\n\n    # ================================================================== #\n    #                       prediction on X_test                         #\n    # ================================================================== #\n    agg = np.zeros(X_test.shape[0])\n    for stump, alpha in zip(stumps, alphas):\n        agg += alpha * stump.predict(X_test)\n\n    final_pred = np.sign(agg)\n    final_pred[final_pred == 0] = 1                  # tie \u2192 class 1\n\n    return final_pred.astype(int).tolist()\n```"}
{"task_id": 258, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef hinge_loss(actual, predicted):\n   Compute the average hinge loss for binary classification.\n\n    Parameters\n    ----------\n    actual : list[int | float] | numpy.ndarray\n        True class labels encoded as -1 or 1.\n    predicted : list[int | float] | numpy.ndarray\n        Real-valued prediction scores.\n\n    Returns\n    -------\n    float | int\n        The mean hinge loss rounded to four decimal places, or -1 if the\n        two inputs have different lengths.\n       # 1. Length check\n    if len(actual) != len(predicted):\n        return -1\n\n    # 2. Convert to NumPy arrays (flatten to 1-D in case higher-dim inputs)\n    y_true = np.asarray(actual, dtype=float).ravel()\n    y_pred = np.asarray(predicted, dtype=float).ravel()\n\n    # 3. Vectorised hinge-loss computation\n    losses = np.maximum(0.0, 1.0 - y_true * y_pred)\n\n    # 4. Mean loss and rounding\n    mean_loss = losses.mean()\n    return float(round(mean_loss, 4))\n```"}
{"task_id": 259, "completion_id": 0, "completion": "```python\nimport random\n\ndef sample_replay_memory(memory: list, batch_size: int) -> list:\n   Randomly sample *batch_size* distinct transitions from *memory*.\n\n    The sampling is deterministic because the random number generator\n    is re-seeded with 42 on every call.  \n    If *batch_size* is larger than the current buffer length the\n    function returns -1.\n       # Not enough elements to satisfy the request\n    if batch_size > len(memory):\n        return -1\n\n    # Deterministic sampling\n    random.seed(42)\n    return random.sample(memory, batch_size)\n```"}
{"task_id": 260, "completion_id": 0, "completion": "```python\nfrom itertools import zip_longest\nfrom math import floor\n\ndef tileswrap(ihtORsize: int,\n              numtilings: int,\n              floats: list[float],\n              wrapwidths: list[int | None],\n              ints: list[int] | None = None,\n              readonly: bool = False) -> list[int]:\n   Return tile indices for the given floats/ints using wrapping tile coding.\n\n    The algorithm is a compact re-implementation of Sutton\u2019s `tileswrap`\n    (with a deterministic polynomial hash and a plain integer table size).\n\n    Args\n    ----\n    ihtORsize : int\n        Size of the hash table (must be > 0).\n    numtilings : int\n        Number of tilings to overlay.\n    floats : list[float]\n        Continuous input variables.\n    wrapwidths : list[int | None]\n        Wrap-width for every float dimension.  A value \u2264 0 or None disables\n        wrapping for that dimension.\n    ints : list[int] | None\n        Additional integer variables to append to the coordinate list.\n    readonly : bool\n        Ignored \u2013 included only for API compatibility.\n\n    Returns\n    -------\n    list[int]\n        One hash index per tiling (length == numtilings).\n       if numtilings <= 0 or ihtORsize <= 0:\n        raise ValueErrornumtilings and ihtORsize must be positive    if ints is None:\n        ints = []\n\n    # 1. Quantise all floats once.\n    q_vals = [floor(f * numtilings) for f in floats]\n\n    # 2. Produce coordinates / indices for every tiling.\n    indices: list[int] = []\n    for t in range(numtilings):\n        coords = [t]          # 2a. start with the tiling number\n        b = t                 # running offset\n\n        # 2b. process every float dimension\n        for q_i, wrap in zip_longest(q_vals, wrapwidths, fillvalue=None):\n            c = (q_i + (b % numtilings)) // numtilings\n            if wrap is not None and wrap > 0:\n                coords.append(c % wrap)\n            else:\n                coords.append(c)\n            b += t * 2        # update b after each float dim\n\n        # 2c. append any extra integer variables\n        coords.extend(ints)\n\n        # 3. deterministic polynomial hash into [0, ihtORsize)\n        index = 0\n        for coord in coords:\n            index = (index * 31 + coord) % ihtORsize\n\n        indices.append(index)\n\n    return indices\n```\n\nThe implementation follows the exact steps demanded in the task description:\n\n1. Each float is quantised once (`q_i = floor(f_i * numtilings)`).\n2. For every tiling\n   \u2022 the coordinate list starts with the tiling number,  \n   \u2022 each float coordinate is offset/wrapped as specified,  \n   \u2022 extra integer variables are appended unchanged.\n3. The coordinate list is mapped deterministically to an index via the polynomial\n   hash `index = (index*31 + coord) % ihtORsize`.\n\nAll returned indices are guaranteed to lie in `[0, ihtORsize)`."}
{"task_id": 261, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n   Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Desired array shape. Must contain at least two dimensions.\n\n    Returns:\n        np.ndarray: Randomly-initialised tensor with mean 0 and variance\n        2 / (fan_in + fan_out).\n\n    Raises:\n        ValueError: If `shape` has fewer than two dimensions.\n       if len(shape) < 2:\n        raise ValueErrorGlorot initialisation requires at least 2 dimensions    if len(shape) == 2:                       # Typical dense layer weight matrix\n        fan_in, fan_out = shape\n    else:                                     # Convolutional / higher-dim. kernel\n        receptive_field = int(np.prod(shape[2:]))  # product of spatial dims\n        fan_in  = shape[1] * receptive_field\n        fan_out = shape[0] * receptive_field\n\n    std = np.sqrt(2.0 / (fan_in + fan_out))\n\n    # Draw samples without disturbing global RNG state beyond normal use.\n    return np.random.normal(loc=0.0, scale=std, size=shape)\n```"}
{"task_id": 263, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef feedforward_predict(x, weights, activations):\n    \"\"\"Runs a forward pass through a fully-connected neural network.\n\n    Parameters\n    ----------\n    x : list[float] | numpy.ndarray\n        Input feature vector (bias term **not** included).\n    weights : list[list[list[float]]]\n        List of weight matrices; each inner list represents one 2-D weight\n        matrix where the first column contains the bias weights.\n    activations : list[str]\n        Activation functions per layer.  Allowed values: \"sigmoid\", \"relu\",\n        \"linear\".\n\n    Returns\n    -------\n    float | list[float]\n        Network output rounded to 4 decimal places.  A scalar is returned for\n        a single-neuron output layer, otherwise a list is returned.\n    \"\"\"\n    # Ensure the input is a 1-D NumPy array of floats\n    a = np.asarray(x, dtype=float).ravel()\n\n    # Helper that maps a string to the actual activation computation\n    def _activate(z, kind):\n        if kind == \"sigmoid\":\n            return 1.0 / (1.0 + np.exp(-z))\n        elif kind == \"relu\":\n            return np.maximum(0.0, z)\n        elif kind == \"linear\":\n            return z\n        else:\n            raise ValueError(f\"Unsupported activation: {kind}\")\n\n    # Forward pass through all layers\n    for W, act in zip(weights, activations):\n        W = np.asarray(W, dtype=float)\n        # prepend bias term\n        a_with_bias = np.concatenate(([1.0], a))\n        # pre-activation\n        z = W @ a_with_bias\n        # activation\n        a = _activate(z, act)\n\n    # Round the final output\n    a = np.round(a, 4)\n\n    # Return scalar for single output neuron, otherwise a list\n    if a.size == 1:\n        return float(a.item())\n    return a.tolist()\n```"}
{"task_id": 265, "completion_id": 0, "completion": "Here is a direct implementation that follows the four cases exactly and returns the result rounded to four decimal places:\n\n```python\ndef king_loss_fn(x: int | float) -> float:\n   Compute King\u2019s piece-wise loss for a scalar value.\n\n    Segments:\n      \u2022 x \u2264 250:            L(x) = \u20130.25 * x + 82.50372665317208\n      \u2022 250 < x \u2264 600:      L(x) = 20.00372665317208\n      \u2022 600 < x \u2264 700:      L(x) = \u20130.2 * x + 140.00372665317207\n      \u2022 x > 700:            L(x) = 0.003726653172066108\n\n    The result is rounded to 4 decimal places.\n       if x <= 250:\n        loss = -0.25 * x + 82.50372665317208\n    elif x <= 600:                     # 250 < x \u2264 600\n        loss = 20.00372665317208\n    elif x <= 700:                     # 600 < x \u2264 700\n        loss = -0.2 * x + 140.00372665317207\n    else:                              # x > 700\n        loss = 0.003726653172066108\n\n    return round(loss, 4)\n```"}
{"task_id": 266, "completion_id": 0, "completion": "```python\nfrom typing import Any, List, Tuple\n\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n   Convert an undirected graph (V,E) to an adjacency-list representation.\n\n    Parameters\n    ----------\n    V : List[Any]\n        List of unique vertex identifiers.  Order must be preserved.\n    E : List[Tuple[Any, Any]]\n        List of 2-tuples (u,v) describing undirected edges.\n\n    Returns\n    -------\n    List[List[Any]]\n        Adjacency list G such that\n            \u2022 len(G) == len(V) and G[i] lists the neighbours of V[i]\n            \u2022 neighbours appear only once\n            \u2022 neighbours are ordered according to their order in V\n       # -----------------------------------------------\n    # 1. helper map: vertex -> its position in V\n    # -----------------------------------------------\n    pos = {vertex: i for i, vertex in enumerate(V)}\n\n    # -----------------------------------------------\n    # 2. collect neighbours in a set (to remove dups)\n    # -----------------------------------------------\n    neighbours = [set() for _ in V]          # one set per vertex\n\n    for u, v in E:\n        if u not in pos or v not in pos:     # ignore edges with unknown vertices\n            continue\n        neighbours[pos[u]].add(v)\n        neighbours[pos[v]].add(u)            # undirected graph\n\n    # -----------------------------------------------\n    # 3. convert each set into a list, ordered by V\n    # -----------------------------------------------\n    G: List[List[Any]] = []\n    for s in neighbours:\n        # sort neighbours by their index in V to keep output deterministic\n        G.append(sorted(s, key=pos.__getitem__))\n\n    return G\n```\n\nHow it works\n1. Build a dictionary `pos` that remembers where every vertex resides inside the given list `V`.\n2. Use one `set` per vertex to gather all its neighbours while iterating through the edges.  \n   \u2022 Sets automatically discard parallel/duplicate edges.  \n   \u2022 Each undirected edge `(u,v)` is stored both at `u` and `v`.\n3. Finally, turn every set into a list, sorting the elements by `pos[...]` so the inner order follows the original vertex order in `V`.\n4. Vertices without incident edges keep the empty list `[]`.\n\nThe algorithm touches every vertex and edge only a constant number of times:\n\u2022 Time complexity: \ud835\udc42(|V| + |E|)  \n\u2022 Memory: \ud835\udc42(|V| + |E|) (in the worst case)."}
{"task_id": 267, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n\n    n_samples, n_features = X.shape\n\n    # assure correct dtypes\n    X            = X.astype(float, copy=False)\n    y            = y.astype(int,   copy=False)\n    sample_weights = sample_weights.astype(float, copy=False)\n\n    best_err       = np.inf\n    best_feature   = 0\n    best_threshold = 0.0\n    best_polarity  = 1\n\n    for j in range(n_features):\n        feature_values = X[:, j]\n\n        # sort samples by the j-th feature\n        order      = np.argsort(feature_values)\n        v_sorted   = feature_values[order]\n        y_sorted   = y[order]\n        w_sorted   = sample_weights[order]\n\n        # masks for positive / negative labels\n        pos_mask = (y_sorted == 1)\n        neg_mask = ~pos_mask\n\n        # cumulative weighted sums\n        cum_pos = np.cumsum(w_sorted * pos_mask)          # \u03a3 w_i   for y_i =  1  , up to (and incl.) index i\n        cum_neg = np.cumsum(w_sorted * neg_mask)          # \u03a3 w_i   for y_i = -1\n        total_pos = cum_pos[-1]\n        total_neg = cum_neg[-1]\n\n        # indices where a new (unique) feature value starts\n        unique_vals, first_idx = np.unique(v_sorted, return_index=True)\n\n        # prefix sums *before* the current unique value\n        #   (prefix 0 for the very first value)\n        pref_pos_before = np.concatenate(([0.0], cum_pos[first_idx[:-1]]))\n        pref_neg_before = np.concatenate(([0.0], cum_neg[first_idx[:-1]]))\n\n        # cumulative sums *including* current value (needed for right side of threshold)\n        cum_pos_at_val = cum_pos[first_idx]\n        cum_neg_at_val = cum_neg[first_idx]\n\n        # --- polarity  +1  --------------------------------------------------\n        # predict  +1  for x < \u03b8 , -1  otherwise\n        # error =  (negatives left)  +  (positives right)\n        err_p1 = pref_neg_before + (total_pos - cum_pos_at_val)\n\n        # --- polarity  -1  --------------------------------------------------\n        # predict  -1  for x < \u03b8 , +1  otherwise\n        # error =  (positives left)  +  (negatives right)\n        err_p_1 = pref_pos_before + (total_neg - cum_neg_at_val)\n\n        # Combine the two error arrays and inspect the minima\n        for k in range(len(unique_vals)):\n            # polarity +1\n            if err_p1[k] < best_err - 1e-12:        # the small epsilon avoids floating glitches\n                best_err       = err_p1[k]\n                best_feature   = j\n                best_threshold = float(unique_vals[k])\n                best_polarity  = 1\n\n            # polarity -1\n            if err_p_1[k] < best_err - 1e-12:\n                best_err       = err_p_1[k]\n                best_feature   = j\n                best_threshold = float(unique_vals[k])\n                best_polarity  = -1\n\n    return {\n        \"feature_index\" : int(best_feature),\n        \"threshold\"     : round(best_threshold, 4),\n        \"polarity\"      : int(best_polarity),\n        \"weighted_error\": round(best_err, 4)\n    }\n```"}
{"task_id": 268, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations, permutations   # permutations is imported only because\n                                                   # the exercise header mentions it\n                                                   # (it is not required inside the code)\n\ndef random_unweighted_graph(n_vertices: int,\n                            edge_prob: float = 0.5,\n                            directed: bool = False) -> list[list[int]]:\n   Generate an Erd\u0151s\u2013R\u00e9nyi G(n,p) random graph and return its adjacency matrix.\n\n    Parameters\n    ----------\n    n_vertices : int\n        Number of vertices (labelled 0 \u2026 n_vertices-1).\n    edge_prob : float, optional\n        Probability p that an admissible edge is present (0 \u2264 p \u2264 1).\n    directed : bool, optional\n        If True, build a directed graph, otherwise an undirected one.\n\n    Returns\n    -------\n    list[list[int]]\n        Nested list representing the adjacency matrix A (shape n\u00d7n) with\n        integer entries 0 or 1.\n\n    Notes\n    -----\n    \u2022 Self-loops are not allowed (A[i][i] == 0 for every i).\n    \u2022 In the undirected case A is symmetric.\n    \u2022 The global NumPy RNG is used, therefore callers can reproduce results\n      via `np.random.seed(seed_value)`.\n       # ---------- basic sanity checks -------------------------------------------------\n    if n_vertices < 1:\n        raise ValueErrorn_vertices must be a positive integer (\u2265 1).    if not (0.0 <= edge_prob <= 1.0):\n        raise ValueErroredge_prob must lie in the closed interval [0, 1].    # ---------- trivial one-vertex graph -------------------------------------------\n    if n_vertices == 1:\n        return [[0]]\n\n    # ---------- initialise an empty (all-zero) adjacency matrix --------------------\n    A = np.zeros((n_vertices, n_vertices), dtype=int)\n\n    # ---------- add edges -----------------------------------------------------------\n    if directed:\n        # Consider every ordered pair (i, j) with i != j\n        for i in range(n_vertices):\n            for j in range(n_vertices):\n                if i == j:\n                    continue                              # no self-loops\n                if np.random.rand() <= edge_prob:\n                    A[i, j] = 1\n    else:\n        # Undirected: consider each unordered pair once\n        for i, j in combinations(range(n_vertices), 2):\n            if np.random.rand() <= edge_prob:\n                A[i, j] = 1\n                A[j, i] = 1                              # keep symmetry\n\n    # ---------- return a *pure-Python* nested list (required by the task) ----------\n    return A.tolist()\n```"}
{"task_id": 269, "completion_id": 0, "completion": "```python\ndef count_trainable_params(state_dimensions: int, action_dimensions: int) -> int:\n   Return the total number of trainable parameters of the two-layer DQN.\n\n    Parameters\n    ----------\n    state_dimensions : int\n        Size of the input state vector.\n    action_dimensions : int\n        Number of discrete actions (size of the output layer).\n\n    Returns\n    -------\n    int\n        Total number of trainable parameters (weights + biases).\n       # First fully-connected layer: state_dimensions \u2192 164 neurons\n    first_layer_params = (state_dimensions * 164) + 164  # weights + biases\n\n    # Second fully-connected layer: 164 neurons \u2192 action_dimensions\n    second_layer_params = (164 * action_dimensions) + action_dimensions  # weights + biases\n\n    # Total trainable parameters\n    return first_layer_params + second_layer_params\n```"}
{"task_id": 270, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oracle_payoff(context: np.ndarray, thetas: np.ndarray) -> tuple[float, int]:\n   Return the oracle\u2019s best (rounded) expected reward and its arm index.\n\n    Parameters\n    ----------\n    context : np.ndarray, shape (D, K)\n        Column k is the D-dimensional context vector x_{k}.\n    thetas  : np.ndarray, shape (D, K)\n        Column k is the unknown parameter vector \u03b8_{k}.\n\n    Returns\n    -------\n    (best_reward, best_arm) : (float, int)\n        best_reward is the maximal expected reward rounded to four decimals;\n        best_arm    is the smallest arm index attaining that rounded reward.\n       # 1. Expected rewards for all K arms:  x_k\u1d40 \u03b8_k = \u03a3_d x_{d,k} \u03b8_{d,k}\n    rewards = np.sum(context * thetas, axis=0)          # shape (K,)\n\n    # 2. Round each reward to four decimals\n    rounded = np.round(rewards, 4)\n\n    # 3. Identify maximal rounded reward and the smallest index that achieves it\n    best_reward = float(rounded.max())\n    best_arm    = int(np.where(rounded == best_reward)[0][0])  # first (smallest) index\n\n    return best_reward, best_arm\n```\n\nThe function:\n1. Computes every arm\u2019s expected reward via element-wise multiplication followed by a column-wise sum.\n2. Rounds each reward to four decimal places.\n3. Selects the maximum rounded value and, in case of ties, returns the smallest corresponding arm index."}
{"task_id": 271, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef xgboost_split_gain(\n    left_true: list[float],\n    left_pred: list[float],\n    right_true: list[float],\n    right_pred: list[float],\n) -> float:\n   Compute XGBoost split gain for squared-error loss    # --- convert to NumPy arrays and basic checks ---------------------------\n    lt = np.asarray(left_true, dtype=float)\n    lp = np.asarray(left_pred, dtype=float)\n    rt = np.asarray(right_true, dtype=float)\n    rp = np.asarray(right_pred, dtype=float)\n\n    if lt.size != lp.size or rt.size != rp.size:\n        raise ValueErrortrue and pred lists must have the same length per side    if lt.size == 0 or rt.size == 0:\n        raise ValueErrorboth left and right sides must be non-empty    # --- gradients (g_i = y_pred - y_true) and hessians (h_i = 1) -----------\n    g_left  = (lp - lt).sum()          # G_L\n    g_right = (rp - rt).sum()          # G_R\n\n    h_left  = float(lt.size)           # H_L\n    h_right = float(rt.size)           # H_R\n\n    g_parent = g_left + g_right        # G_P\n    h_parent = h_left + h_right        # H_P\n\n    # --- gain ---------------------------------------------------------------\n    gain = 0.5 * (\n        (g_left ** 2)  / h_left\n        + (g_right ** 2) / h_right\n        - (g_parent ** 2) / h_parent\n    )\n\n    # --- rounding / +0.0 vs. -0.0 ------------------------------------------\n    gain_rounded = round(gain, 4)\n    return 0.0 if gain_rounded == 0 else gain_rounded\n```"}
{"task_id": 273, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\n\ndef _entropy(counter: Counter, size: int) -> float:\n    \"\"\"Shannon entropy of a multiset that is already counted.\"\"\"\n    if size == 0:\n        return 0.0\n    ent = 0.0\n    for cnt in counter.values():\n        p = cnt / size\n        ent -= p * math.log2(p)\n    return ent\n\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert to ordinary Python lists (works for both list and numpy array)\n    feat = list(feature)\n    y    = list(target)\n\n    n = len(feat)\n    if n == 0 or n != len(y):\n        return (None, 0.0)           # empty input or length mismatch\n\n    # Sort data by the feature value\n    pairs = sorted(zip(feat, y), key=lambda p: p[0])\n    sorted_feat, sorted_y = zip(*pairs)          # each is a tuple, len == n\n\n    # Overall (parent) entropy\n    parent_entropy = _entropy(Counter(sorted_y), n)\n\n    # If the parent node is pure, no split can improve it\n    if parent_entropy == 0.0:\n        return (None, 0.0)\n\n    # Prepare counters for an O(n) sweep over possible split positions\n    left_counter  = Counter()\n    right_counter = Counter(sorted_y)\n\n    best_gain = 0.0\n    best_thr  = None\n\n    for i in range(n - 1):               # last index cannot be a split border\n        label = sorted_y[i]\n        # Move current sample from right side to left side\n        left_counter[label]  += 1\n        right_counter[label] -= 1\n        if right_counter[label] == 0:\n            del right_counter[label]\n\n        # Skip if the next feature value is identical \u2013 not a legal threshold\n        if sorted_feat[i] == sorted_feat[i + 1]:\n            continue\n\n        # Candidate threshold \u2013 mid-point between consecutive distinct values\n        threshold = 0.5 * (sorted_feat[i] + sorted_feat[i + 1])\n\n        left_size  = i + 1\n        right_size = n - left_size\n\n        h_left  = _entropy(left_counter,  left_size)\n        h_right = _entropy(right_counter, right_size)\n\n        info_gain = (\n            parent_entropy\n            - (left_size  / n) * h_left\n            - (right_size / n) * h_right\n        )\n\n        # Update the best split (smallest threshold wins the tie)\n        if (info_gain > best_gain + 1e-12) or (\n            abs(info_gain - best_gain) <= 1e-12 and (best_thr is None or threshold < best_thr)\n        ):\n            best_gain = info_gain\n            best_thr  = threshold\n\n    # Nothing useful found\n    if best_thr is None or best_gain <= 0.0:\n        return (None, 0.0)\n\n    # Round as requested\n    return (round(best_thr, 4), round(best_gain, 4))\n```"}
{"task_id": 274, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef small_norm_clip(values):\n    \"\"\"Clip each element of *values* to the closed interval [-5, 5].\n\n    Args:\n        values (list[int | float] | np.ndarray): One-dimensional numeric data.\n\n    Returns:\n        list[float]: A Python list where every element has been limited to the\n            range [-5, 5] while preserving the original order.\n    \"\"\"\n    # Accept either a NumPy array or any (one-dimensional) iterable such as a list\n    if isinstance(values, np.ndarray):\n        # np.clip performs element-wise clipping efficiently\n        return np.clip(values, -5, 5).tolist()\n    else:\n        # Fallback for plain Python lists (or any iterable)\n        # max(-5, x) forces the lower bound, min(..., 5) forces the upper bound\n        return [max(-5, min(5, x)) for x in values]\n```\n\nExample usage:\n```python\n>>> small_norm_clip([-10, -3, 0, 4.2, 8])\n[-5, -3, 0, 4.2, 5]\n\n>>> small_norm_clip(np.array([6.7, -9.1, 2]))\n[5.0, -5.0, 2.0]\n```"}
{"task_id": 276, "completion_id": 0, "completion": "```python\nimport math\nimport string\nfrom collections import Counter\n\ndef additive_ngram_log_prob(corpus: list[str], sentence: str, N: int,\n                            k: float = 1.0, unk: bool = True) -> float:\n   Additive-smoothed N\u2013gram sentence log-probability (base 10).    # ---------- helpers -----------------------------------------------------\n    punct = set(string.punctuation)\n\n    def _tokenise(line: str) -> list[str]:\n       Lower-case, whitespace split, strip leading/trailing punctuation        out = []\n        for tok in line.lower().split():\n            tok = tok.strip(string.punctuation)\n            if tok:                       # skip empty strings\n                out.append(tok)\n        return out\n\n    # ---------- train: vocabulary & counts ----------------------------------\n    vocab: set[str] = set()\n    ngram_counts: Counter = Counter()          # N-gram counts\n    hist_counts: Counter = Counter()           # (N-1)-gram counts\n    total_tokens = 0                           # only needed for unigrams\n\n    for line in corpus:\n        toks = _tokenise(line)\n        vocab.update(toks)\n        seq = ['<bol>'] + toks + ['<eol>']     # padding\n        total_tokens += len(seq)\n\n        # collect N-gram and history counts\n        for i in range(len(seq) - N + 1):\n            ng = tuple(seq[i:i + N])\n            ngram_counts[ng] += 1\n            if N > 1:\n                hist_counts[ng[:-1]] += 1\n\n    # add the special symbols to the vocabulary\n    vocab.update({'<bol>', '<eol>'})\n    if unk:\n        vocab.add('<unk>')\n\n    V = len(vocab)                             # |V|\n\n    # ---------- prepare target sentence -------------------------------------\n    test_toks = _tokenise(sentence)\n    if unk:\n        test_toks = [w if w in vocab else '<unk>' for w in test_toks]\n\n    test_seq = ['<bol>'] + test_toks + ['<eol>']\n\n    # ---------- probability computation -------------------------------------\n    log_prob = 0.0\n\n    for i in range(len(test_seq) - N + 1):\n        ng = tuple(test_seq[i:i + N])\n        count_ng = ngram_counts.get(ng, 0)\n        numerator = count_ng + k\n\n        if N == 1:\n            denominator = total_tokens + k * V\n        else:\n            hist = ng[:-1]\n            hist_count = hist_counts.get(hist, 0)\n            denominator = hist_count + k * V\n\n        prob = numerator / denominator\n        log_prob += math.log10(prob)\n\n    return round(log_prob, 4)\n```"}
{"task_id": 277, "completion_id": 0, "completion": "```python\ndef is_tuple(env: dict) -> tuple:\n    \"\"\"Determine whether the *action* and *observation* spaces contained in\n    ``env`` are composite (tuple or dictionary).\n\n    A *composite* space is defined as a built-in ``tuple`` or ``dict``.\n\n    Args:\n        env (dict): A dictionary that **must** contain the keys\n            ``\"action_space\"`` and ``\"observation_space\"``.\n\n    Returns:\n        tuple: Two booleans ``(tuple_action, tuple_obs)`` indicating whether\n            each space is composite.\n    \"\"\"\n    # Retrieve the two spaces\n    action_space = env[\"action_space\"]\n    observation_space = env[\"observation_space\"]\n\n    # Check for tuple/dict\n    tuple_action = isinstance(action_space, (tuple, dict))\n    tuple_obs = isinstance(observation_space, (tuple, dict))\n\n    return tuple_action, tuple_obs\n```"}
{"task_id": 278, "completion_id": 0, "completion": "```python\ndef evaluate_decision_tree(tree: dict, sample: list[float]):\n    \"\"\"Traverse a binary decision-tree represented by nested dictionaries.\n\n    Args:\n        tree (dict): Root node of the decision tree. Internal nodes contain\n            'feature_i', 'threshold', 'true_branch', 'false_branch'.\n            Leaf nodes contain only 'value'.\n        sample (list[float]): Feature vector that will be classified/regressed.\n\n    Returns:\n        The value stored in the reached leaf (int, float, or str).\n    \"\"\"\n    node = tree\n    while True:\n        # Leaf node \u21d2 return prediction\n        if 'value' in node:\n            return node['value']\n\n        # Internal node \u21d2 decide which branch to follow\n        feature_idx = node['feature_i']\n        threshold   = node['threshold']\n\n        try:\n            feature_val = sample[feature_idx]\n        except IndexError as e:\n            raise IndexError(\n                f\"Sample has no feature at index {feature_idx}\"\n            ) from e\n\n        # Follow the corresponding branch\n        node = node['true_branch'] if feature_val <= threshold else node['false_branch']\n```"}
{"task_id": 279, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error (MSE), its gradient and 0.5-threshold accuracy.\n\n    Parameters\n    ----------\n    y_true : list[int] | numpy.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : list[float] | numpy.ndarray\n        Model predictions.\n\n    Returns\n    -------\n    tuple\n        (mse_value, gradient_list, accuracy) \u2013 each rounded to 4 decimals.\n    \"\"\"\n    # --- convert inputs to NumPy arrays -------------------------------\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    n = y_true.size                     # number of samples\n\n    # --- 1) Mean-Squared Error ---------------------------------------\n    mse = np.mean((y_pred - y_true) ** 2)\n\n    # --- 2) Analytic gradient w.r.t. predictions ----------------------\n    grad = (2.0 / n) * (y_pred - y_true)      # NumPy array\n    grad_list = [round(float(g), 4) for g in grad]\n\n    # --- 3) Accuracy with 0.5 threshold ------------------------------\n    pred_class = (y_pred >= 0.5).astype(float)\n    accuracy = np.mean(pred_class == y_true)\n\n    # --- rounding scalars --------------------------------------------\n    mse = round(float(mse), 4)\n    accuracy = round(float(accuracy), 4)\n\n    return mse, grad_list, accuracy\n```"}
{"task_id": 280, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_regularization(w: np.ndarray, alpha: float, return_grad: bool = False):\n    \"\"\"Compute the L2 regularization value or its gradient.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Weight vector or matrix of arbitrary shape.\n    alpha : float\n        Non-negative regularization strength.\n    return_grad : bool, default=False\n        If True, return the gradient (alpha\u00b7w) instead of the scalar value.\n\n    Returns\n    -------\n    float or np.ndarray\n        \u2022 Scalar float (0.5\u00b7alpha\u00b7\u03a3 w_i\u00b2) when *return_grad* is False.  \n        \u2022 NumPy array (alpha\u00b7w) with the same shape as *w* when\n          *return_grad* is True.\n    \"\"\"\n    # Ensure input is a NumPy array of type float for safe arithmetic\n    w = np.asarray(w, dtype=float)\n\n    if return_grad:\n        # Gradient: alpha * w\n        grad = alpha * w\n        return np.round(grad, 4)          # ndarray, same shape as w\n    else:\n        # Regularization value: 0.5 * alpha * sum(w^2)\n        reg_val = 0.5 * alpha * np.sum(w ** 2)\n        return float(np.round(reg_val, 4))  # Python float\n```\n\nExplanation of key points\n\u2022 Uses `np.asarray` to guarantee that `w` is a floating-point NumPy array.  \n\u2022 Regularization value formula: 0.5 \u00b7 \u03b1 \u00b7 \u03a3 w\u1d62\u00b2.  \n\u2022 Gradient formula: \u03b1 \u00b7 w (element-wise).  \n\u2022 `np.round(..., 4)` enforces the required four-decimal precision.  \n\u2022 The scalar case is explicitly cast to `float` so the caller receives a true Python `float` rather than a 0-D NumPy array."}
{"task_id": 281, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (kept to match the supplied skeleton)\n\ndef L1Regularization(alpha: float):\n    \"\"\"Factory that creates a callable L1-regularisation object.\"\"\"\n\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative.\")\n\n    class _L1Regularization:\n        __slots__ = (\"_alpha\",)\n\n        def __init__(self, alpha):\n            self._alpha = float(alpha)\n\n        # ------------------------------------------------------------------ #\n        # 1.  Value of the penalty                                           #\n        # ------------------------------------------------------------------ #\n        def __call__(self, w):\n            \"\"\"\n            Parameters\n            ----------\n            w : np.ndarray (any shape)\n\n            Returns\n            -------\n            float\n                J_L1(w) = alpha * ||w||_1\n            \"\"\"\n            if self._alpha == 0.0:\n                return 0.0\n            # np.abs followed by np.sum works for arrays of any shape/dtype\n            return float(self._alpha * np.sum(np.abs(w)))\n\n        # ------------------------------------------------------------------ #\n        # 2.  Gradient of the penalty                                        #\n        # ------------------------------------------------------------------ #\n        def grad(self, w):\n            \"\"\"\n            Parameters\n            ----------\n            w : np.ndarray (any shape)\n\n            Returns\n            -------\n            np.ndarray\n                Gradient of the L1 penalty w.r.t w:\n                alpha * sign(w)   (with sign(0) == 0)\n            \"\"\"\n            if self._alpha == 0.0:\n                # Fast path when the penalty is switched off\n                return np.zeros_like(w, dtype=float)\n\n            # np.sign returns -1, 0, +1 element-wise; multiply by alpha\n            return self._alpha * np.sign(w)\n\n        # Make repr useful for debugging / printing\n        def __repr__(self):\n            return f\"L1Regularization(alpha={self._alpha})\"\n\n    # Return an *instance* that captures the chosen alpha\n    return _L1Regularization(alpha)\n```\n\nUsage example\n--------------\n```python\n>>> l1 = L1Regularization(alpha=0.5)\n>>> w = np.array([[1.0, -3.0], [0.0, 2.0]])\n>>> l1(w)                         # penalty value\n3.0\n>>> l1.grad(w)                    # gradient\narray([[ 0.5, -0.5],\n       [ 0. ,  0.5]])\n```"}
{"task_id": 282, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import Union, Tuple, List\n\nNumber = Union[int, float]\n\ndef running_stats(data: List[Number]) -> Tuple[float, float, float]:\n   Return sample mean, variance and standard deviation (rounded to 4 dp)\n    using Welford\u2019s online algorithm.\n\n    If `data` is empty the triple (0.0, 0.0, 0.0) is returned.\n       n = 0                 # number of samples seen so far\n    mean = 0.0            # running mean\n    ss = 0.0              # running sum of squared deviations from the mean\n\n    for x in data:                  # single-pass update\n        n += 1\n        delta = x - mean\n        mean += delta / n\n        ss += delta * (x - mean)    # uses new mean on RHS\n\n    if n == 0:                      # empty input\n        return 0.0, 0.0, 0.0\n\n    variance = ss / (n - 1) if n > 1 else 0.0   # sample variance\n    std = math.sqrt(variance)\n\n    # round to 4 decimal places as required\n    return round(mean, 4), round(variance, 4), round(std, 4)\n```"}
{"task_id": 283, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_log_loss_metrics(actual, predicted, regularization=1.0):\n    \"\"\"Compute the approximate Newton leaf value and split gain for binary\n    cross-entropy (logistic) loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        1-D array with true labels (0 or 1).\n    predicted : np.ndarray\n        1-D array with raw prediction scores (log-odds).\n    regularization : float, default 1.0\n        L2 regularisation term \u03bb used in the formulas.\n\n    Returns\n    -------\n    list[float]\n        [leaf_value, gain] \u2013 both rounded to 4 decimals.\n    \"\"\"\n    # Ensure numpy arrays of float dtype\n    y = np.asarray(actual, dtype=float)\n    f = np.asarray(predicted, dtype=float)\n\n    # Numerically stable sigmoid\n    p = np.empty_like(f)\n    pos = f >= 0\n    neg = ~pos\n    p[pos] = 1.0 / (1.0 + np.exp(-f[pos]))\n    exp_f = np.exp(f[neg])\n    p[neg] = exp_f / (1.0 + exp_f)\n\n    # First and second derivatives\n    g = p - y                  # gradient\n    h = p * (1.0 - p)          # Hessian\n\n    sum_g = g.sum()\n    sum_h = h.sum()\n\n    denom = sum_h + regularization\n\n    leaf_value = sum_g / denom\n    gain       = 0.5 * (sum_g ** 2) / denom\n\n    # Round to 4 decimal places as required\n    return [round(leaf_value, 4), round(gain, 4)]\n```"}
{"task_id": 284, "completion_id": 0, "completion": "```python\ndef iht_get_index(\n    obj: object,\n    size: int,\n    table: dict,\n    overfull_count: list[int],\n    readonly: bool = False,\n) -> int | None:\n    \"\"\"Map *obj* to a stable integer index using a bounded index hash table.\n\n    Args:\n        obj:               Hashable object that should receive an index.\n        size:              Maximum number of *stored* entries.\n        table:             Mutable dictionary holding the current mapping.\n        overfull_count:    One-element list whose integer counts collisions.\n        readonly:          If True, never modify *table* or *overfull_count*.\n\n    Returns:\n        The integer index for *obj*, or None when *readonly* is True and the\n        object is not yet in *table*.\n    \"\"\"\n    # 1.  Object already known \u2192 just return its index.\n    if obj in table:\n        return table[obj]\n\n    # 2.  In read-only mode we must not touch anything.\n    if readonly:\n        return None\n\n    # 3.  Table full \u2192 use collision scheme (no insertion).\n    if len(table) >= size:\n        index = hash(obj) % size            # collision index (always 0 \u2026 size-1)\n        overfull_count[0] += 1              # count the collision\n        return index\n\n    # 4.  Table not full \u2192 insert the new object with the next free index.\n    index = len(table)                      # keeps indices consecutive (0,1,2,\u2026)\n    table[obj] = index\n    return index\n```"}
{"task_id": 285, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(x: np.ndarray, y: np.ndarray) -> list | int:\n   Compute the linear kernel (Gram matrix) between two data sets.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        First input data. Can be 1-D (single sample) or 2-D (multiple samples).\n    y : np.ndarray\n        Second input data. Can be 1-D (single sample) or 2-D (multiple samples).\n\n    Returns\n    -------\n    list | int\n        \u2022 A nested Python list containing the Gram matrix (x @ y.T)  \n        \u2022 -1 if the two inputs have different feature dimensions\n       # Ensure inputs are NumPy arrays (in case the caller passes e.g. lists)\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    # If a vector is supplied, turn it into a (1, d) matrix\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    if y.ndim == 1:\n        y = y.reshape(1, -1)\n\n    # Check that both inputs are now 2-D\n    if x.ndim != 2 or y.ndim != 2:     # outside the problem scope\n        return -1\n\n    # Feature-dimension compatibility check\n    if x.shape[1] != y.shape[1]:\n        return -1\n\n    # Compute the Gram matrix and convert to a (nested) Python list\n    return (x @ y.T).tolist()\n```"}
{"task_id": 286, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Callable, Tuple, List\n\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n   Returns a matrix filled with zeros of the requested shape    rows, cols = shape\n    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n   Returns a matrix filled with ones of the requested shape    rows, cols = shape\n    return [[1.0 for _ in range(cols)] for _ in range(rows)]\n\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n   Returns a matrix with uniformly distributed random numbers.\n\n    The random numbers are drawn from the half-open interval [low, high].\n    A local Random instance is used so that seeding remains self-contained.\n       rng = random.Random(seed)\n    rows, cols = shape\n    return [[rng.uniform(low, high) for _ in range(cols)] for _ in range(rows)]\n\n\ndef get_initializer(name: str) -> Callable:\n   Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g.zeros_init    Returns:\n        A callable initializer.\n\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n       init_fn = globals().get(name, None)\n\n    if init_fn is None or not callable(init_fn):\n        raise ValueErrorInvalid initialization function    return init_fn\n```"}
{"task_id": 287, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef average_pool2d(\n    X: np.ndarray,\n    pool_shape: tuple[int, int],\n    stride: tuple[int, int],\n    accum_grad: np.ndarray\n) -> tuple[list, list]:\n   Forward & backward passes of a 2-D average-pool layer.\n\n    Parameters\n    ----------\n    X : ndarray           \u2013 shape (N, C, H, W), input feature-maps.\n    pool_shape : (p_h, p_w)\n    stride : (s_h, s_w)\n    accum_grad : ndarray  \u2013 gradients arriving from the next layer,\n                            shape == pooled output.\n\n    Returns\n    -------\n    pooled_output, grad_input  \u2013 both rounded to 4 decimals and converted\n                                 to regular Python lists.\n       N, C, H, W = X.shape\n    p_h, p_w   = pool_shape\n    s_h, s_w   = stride\n\n    # Compute output spatial dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n\n    # Forward pass -----------------------------------------------------------\n    pooled = np.empty((N, C, out_h, out_w), dtype=X.dtype)\n\n    for n in range(N):\n        for c in range(C):\n            for oh in range(out_h):\n                h_start = oh * s_h\n                h_end   = h_start + p_h\n                for ow in range(out_w):\n                    w_start = ow * s_w\n                    w_end   = w_start + p_w\n\n                    window = X[n, c, h_start:h_end, w_start:w_end]\n                    pooled[n, c, oh, ow] = window.mean()\n\n    # Back-propagation -------------------------------------------------------\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    coeff = 1.0 / (p_h * p_w)          # each element gets equal share\n\n    for n in range(N):\n        for c in range(C):\n            for oh in range(out_h):\n                h_start = oh * s_h\n                h_end   = h_start + p_h\n                for ow in range(out_w):\n                    w_start = ow * s_w\n                    w_end   = w_start + p_w\n\n                    share = accum_grad[n, c, oh, ow] * coeff\n                    grad_input[n, c, h_start:h_end, w_start:w_end] += share\n\n    # Round to 4 decimals and convert to Python lists ------------------------\n    pooled_rounded = np.round(pooled, 4).tolist()\n    grad_rounded   = np.round(grad_input, 4).tolist()\n\n    return pooled_rounded, grad_rounded\n```"}
{"task_id": 288, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef squared_error(actual, predicted):\n    \"\"\"Calculate the squared error (scalar inputs) or mean-squared error\n    (one-dimensional iterable inputs).\n\n    Rules\n    -----\n    \u2022 If *actual* and *predicted* are both scalars \u2192 return (actual-predicted)**2  \n    \u2022 If they are 1-D iterables of identical non-zero length \u2192 return the mean\n      of the squared element-wise differences.  \n    \u2022 Any length mismatch, empty iterable, different dimensionality or\n      scalar/iterable mix \u2192 return \u20111.\n\n    The final value is rounded to 4 decimal places.\n    \"\"\"\n    # ---------- scalar case -------------------------------------------------\n    if np.isscalar(actual) and np.isscalar(predicted):\n        try:\n            return round((float(actual) - float(predicted)) ** 2, 4)\n        except Exception:\n            return -1\n\n    # ---------- iterable case ----------------------------------------------\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except Exception:            # non-numeric contents, etc.\n        return -1\n\n    # Must both be 1-D, non-empty and of identical length\n    if a.ndim != 1 or p.ndim != 1:\n        return -1\n    if a.size == 0 or p.size == 0 or a.size != p.size:\n        return -1\n\n    mse = np.mean((a - p) ** 2)\n    return round(float(mse), 4)\n```"}
{"task_id": 289, "completion_id": 0, "completion": "```python\ndef format_entries(entries: list[tuple[str, str]]) -> str:\n   Formats a list of (field, value) tuples.\n\n    Each tuple is converted into a line of the form:\n        field:value    All lines are joined together by a single newline character. No extra\n    newline is added at the end.\n\n    Args:\n        entries: List of tuples where the first element is the field name and\n                 the second element is the corresponding value.\n\n    Returns:\n        A single string containing all formatted lines separated by a newline,\n        or an empty string if *entries* is empty.\n       # Create the list of formatted strings\n    formatted = [f'{field}:value' for field, value in entries]\n    \n    # Join them with '\\n'; join on an empty list yields an empty string.\n    returnnjoin(formatted)\n```"}
{"task_id": 290, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom dataclasses import dataclass\nimport math\n\n@dataclass\nclass Leaf:\n   A terminal node that stores a prediction value    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n   An internal decision-tree node    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\n\n_ABS_TOL = 1e-8          # numerical tolerance required by the task\n\n\ndef _leaves_equal(val_a, val_b) -> bool:\n   Compare two prediction values with the prescribed tolerance    return np.allclose(\n        np.asarray(val_a),           # convert to ndarray (works for scalars, lists, arrays)\n        np.asarray(val_b),\n        atol=_ABS_TOL,\n        rtol=0.0                     # pure absolute tolerance\n    )\n\n\ndef compare_trees(tree_a, tree_b):\n   Recursively checks whether *tree_a* and *tree_b* are equivalent    # Both are leaves -------------------------------------------------------\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return _leaves_equal(tree_a.value, tree_b.value)\n\n    # Both are internal nodes ----------------------------------------------\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        return (\n            tree_a.feature == tree_b.feature and\n            math.isclose(tree_a.threshold, tree_b.threshold, abs_tol=_ABS_TOL) and\n            compare_trees(tree_a.left,  tree_b.left) and     # recurse on left\n            compare_trees(tree_a.right, tree_b.right)        # recurse on right\n        )\n\n    # One is a Node and the other a Leaf (or different types) --------------\n    return False\n```"}
{"task_id": 291, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(X: np.ndarray, n_components: int) -> list[list[float]]:\n   Project *X* onto its first *n_components* principal components.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input data of shape (m, d).\n    n_components : int\n        Number of principal components to retain (1 \u2264 n_components \u2264 d).\n\n    Returns\n    -------\n    list[list[float]]\n        Data projected on the first *n_components* principal components,\n        rounded to 4 decimals.  If *n_components* is invalid, returns -1.\n       # ------------------------------------------------------------------\n    # 1. Basic validation\n    # ------------------------------------------------------------------\n    if not isinstance(X, np.ndarray) or X.ndim != 2:\n        raise ValueErrorX must be a 2-D NumPy array    d = X.shape[1]\n    if not isinstance(n_components, int) or n_components < 1 or n_components > d:\n        return -1\n\n    # ------------------------------------------------------------------\n    # 2. Covariance matrix\n    # ------------------------------------------------------------------\n    cov = np.cov(X, rowvar=False)\n\n    # ------------------------------------------------------------------\n    # 3. Eigen-decomposition\n    # ------------------------------------------------------------------\n    eigvals, eigvecs = np.linalg.eig(cov)\n    eigvals, eigvecs = eigvals.real, eigvecs.real   # guard against tiny imaginaries\n\n    # ------------------------------------------------------------------\n    # 4. Sort eigenpairs (descending eigenvalues)\n    # ------------------------------------------------------------------\n    order = np.argsort(eigvals)[::-1]\n    eigvecs = eigvecs[:, order]\n\n    # ------------------------------------------------------------------\n    # 5. Keep the first n_components and fix sign deterministically\n    # ------------------------------------------------------------------\n    V = eigvecs[:, :n_components]\n    for i in range(n_components):\n        if V[0, i] < 0:      # flip if first element is negative\n            V[:, i] *= -1\n\n    # ------------------------------------------------------------------\n    # 6. Project the data\n    # ------------------------------------------------------------------\n    X_transformed = X @ V\n\n    # ------------------------------------------------------------------\n    # 7. Round and convert to plain Python list\n    # ------------------------------------------------------------------\n    return np.round(X_transformed, 4).tolist()\n```"}
{"task_id": 292, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]] | int:\n   Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix (list of lists).\n        parent2: Second parent weight matrix (same shape as *parent1*).\n        cutoff : Column index at which crossover starts (inclusive).\n\n    Returns\n    -------\n        Tuple (child1, child2) \u2013 each a list of lists obtained with\n        ``numpy.ndarray.tolist``.  \n        If the parents do not share the same rectangular shape, ``-1`` is\n        returned instead.\n       # Convert to NumPy arrays for convenient, fast slicing\n    try:\n        p1 = np.asarray(parent1, dtype=float)\n        p2 = np.asarray(parent2, dtype=float)\n    except Exception:          # conversion failed \u21d2 shapes definitely differ\n        return -1\n\n    # Check shape compatibility\n    if p1.shape != p2.shape or p1.ndim != 2:\n        return -1\n\n    n_rows, n_cols = p1.shape\n\n    # A valid cutoff is in [0, n_cols]; anything else is an error.\n    if not (0 <= cutoff <= n_cols):\n        raise ValueError(fcutoff must be between 0 and {n_cols}, got {cutoff    # Perform single-point column crossover (no in-place modification)\n    # Columns [0 .. cutoff-1] stay, columns [cutoff .. end] are swapped.\n    child1 = np.hstack((p1[:, :cutoff], p2[:, cutoff:]))\n    child2 = np.hstack((p2[:, :cutoff], p1[:, cutoff:]))\n\n    # Convert back to ordinary Python lists\n    return child1.tolist(), child2.tolist()\n```"}
{"task_id": 293, "completion_id": 0, "completion": "```python\ndef map_agent_environment(agent_name: str):\n   Return the environment name and access type that should be used for a given RL agent.\n    \n    The mapping is case-insensitive and ignores leading/trailing spaces. If the\n    agent name is not recognised, the function returns -1.\n\n    Args:\n        agent_name: Name of the RL agent (e.g.dqnA3C etc.).\n\n    Returns:\n        Tuple[str, str] if the agent is known, or -1 otherwise.\n       # Normalise the incoming name: strip white-space and force lower case\n    key = agent_name.strip().lower()\n\n    # Pre-defined agent \u2192 (environment, access_type) mapping\n    mapping = {\n       dqn      CartPole-v0unwrapped       ddpg     Pendulum-v0env       a3c      Pendulum-v0unwrapped       a2c      CartPole-v0env       a2c_multiCartPole-v0raw       trpo     Pendulum-v0unwrapped    }\n\n    return mapping.get(key, -1)\n```"}
{"task_id": 294, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List, Tuple\n\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom\u2010graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        The custom graph with the keys\n            * 'is_directed' \u2013 bool\n            * '_V2I'        \u2013 dict mapping vertex label -> index\n            * '_G'          \u2013 list of adjacency lists.  Each edge is a\n                              tuple (source_label, target_label, weight)\n\n    Returns\n    -------\n    dict\n        Maps every vertex label to a list of (neighbour, weight) tuples.\n        In an undirected graph every edge is stored exactly once in\n        each direction.  Neighbour lists are sorted lexicographically.\n    \"\"\"\n    # 1.  Handle empty graph -------------------------------------------------\n    v2i: Dict[Any, int] = G.get(\"_V2I\", {})\n    if not v2i:                        # no vertices present\n        return {}\n\n    is_directed: bool = G.get(\"is_directed\", False)\n    raw: Dict[Any, Dict[Any, float]] = {v: {} for v in v2i}   # v -> {nbr: w}\n\n    # 2.  Collect edges ------------------------------------------------------\n    for edges_of_v in G.get(\"_G\", []):\n        for edge in edges_of_v:\n            if len(edge) != 3:     # ignore malformed edges silently\n                continue\n            u, v, w = edge\n\n            # make sure completely isolated vertices that only appear in\n            # edges are created in `raw`\n            raw.setdefault(u, {})\n            raw.setdefault(v, {})\n\n            # add u -> v\n            if v not in raw[u]:\n                raw[u][v] = w\n\n            # add v -> u for undirected graphs\n            if not is_directed and u not in raw[v]:\n                raw[v][u] = w\n\n    # 3.  Produce the final adjacency dict ----------------------------------\n    adj: Dict[Any, List[Tuple[Any, float]]] = {}\n    for u, nbrs in raw.items():\n        # sort neighbours alphabetically (lexicographically by label)\n        adj[u] = sorted(nbrs.items(), key=lambda item: item[0])\n\n    return adj\n```"}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        True  if all elements are numerically 0 or 1,\n        False otherwise.\n    \"\"\"\n    # Convert the input to a NumPy array (no copy if already ndarray)\n    arr = np.asarray(x)\n\n    # Reject complex dtypes right away\n    if np.iscomplexobj(arr):\n        return False\n\n    # Elements are binary when they equal 0 or 1\n    try:\n        ok = (arr == 0) | (arr == 1)   # boolean mask\n    except Exception:                  # e.g. non-numeric object dtype\n        return False\n\n    # Convert NumPy bool_ to plain Python bool\n    return bool(ok.all())\n```"}
{"task_id": 297, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef min_max_scale(data: list[list[int | float]],\n                  feature_range: tuple = (0, 1)) -> list[list[float]]:\n   Scale every feature column of *data* to a given interval.\n\n    Parameters\n    ----------\n    data : list[list[int | float]]\n        Two-dimensional numerical data set where each inner list is a sample.\n    feature_range : tuple, optional\n        Desired range for the transformed data given as ``(lower, upper)``.\n        The default is ``(0, 1)``.\n\n    Returns\n    -------\n    list[list[float]]\n        Scaled data rounded to four decimal places.\n        Returns ``-1`` if the supplied ``feature_range`` is invalid\n        (i.e. lower >= upper).\n       # ------------------------------- #\n    # 1. Validate feature_range\n    # ------------------------------- #\n    if (\n        not isinstance(feature_range, tuple)\n        or len(feature_range) != 2\n        or feature_range[0] >= feature_range[1]\n    ):\n        return -1\n\n    a, b = map(float, feature_range)\n\n    # ------------------------------- #\n    # 2. Convert input to NumPy array\n    # ------------------------------- #\n    if len(data) == 0:\n        return []                           # empty data set\n    X = np.asarray(data, dtype=float)\n\n    # ------------------------------- #\n    # 3. Compute per-column min / max\n    # ------------------------------- #\n    col_min = np.min(X, axis=0)\n    col_max = np.max(X, axis=0)\n\n    # Range: if constant feature, use 1\n    col_range = col_max - col_min\n    col_range[col_range == 0] = 1.0        # requirement (3)\n\n    # ------------------------------- #\n    # 4. Apply Min-Max scaling\n    # ------------------------------- #\n    scaled = (X - col_min) / col_range        # in [0,1]\n    scaled = scaled * (b - a) + a             # rescale to [a,b]\n\n    # ------------------------------- #\n    # 5. Post-processing\n    # ------------------------------- #\n    scaled = np.nan_to_num(scaled, nan=0.0, posinf=0.0, neginf=0.0)  # requirement (5)\n    scaled = np.round(scaled, 4)                                      # requirement (4)\n\n    return scaled.tolist()\n```"}
{"task_id": 298, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom typing import List, Tuple\n\n\ndef _ngrams(seq: List[str], n: int):\n   Yield all consecutive n-grams of *seq* as tuples    for i in range(len(seq) - n + 1):\n        yield tuple(seq[i : i + n])\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n   Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (whitespace-separated tokens).\n    query : str\n        Sentence whose probability is required.\n    N : int\n        Order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        Sum of natural logs of all consecutive N-gram probabilities in *query*\n        (rounded to 4 decimals).  If any needed N-gram is unseen in *corpus*\n        or the query is too short, returns float('-inf').\n       if N < 1:\n        raise ValueErrorN must be at least 1    n_counts: Counter = Counter()               # counts of N-grams\n    prefix_counts: Counter = Counter()          # counts of (N-1)-grams\n    total_tokens = 0                            # only used for unigrams\n\n    # -------------------  Training  -------------------\n    for sent in corpus:\n        tokens = sent.split()\n\n        if N == 1:\n            n_counts.update(tokens)\n            total_tokens += len(tokens)\n            continue\n\n        # padding for N > 1\n        padded =<bol> * (N - 1) + tokens +<eol>        n_counts.update(_ngrams(padded, N))\n        prefix_counts.update(_ngrams(padded, N - 1))\n\n    # If we trained a unigram model but have no tokens, every query is impossible\n    if N == 1 and total_tokens == 0:\n        return floatinf    # -------------------  Evaluation  -------------------\n    query_tokens = query.split()\n\n    # Too short?\n    if N == 1:\n        if not query_tokens:\n            return floatinf        padded_query = query_tokens                  # no padding for unigrams\n    else:\n        if not query_tokens:                         # an empty sentence is \u201ctoo short\u201d\n            return floatinf        padded_query =<bol> * (N - 1) + query_tokens +<eol>    if len(padded_query) < N:\n        return floatinf    log_prob_sum = 0.0\n\n    for ngram in _ngrams(padded_query, N):\n        if N == 1:\n            count = n_counts[ngram[0]]\n            denom = total_tokens\n        else:\n            count = n_counts[ngram]\n            denom = prefix_counts[ngram[:-1]]\n\n        # unseen N-gram or missing context \u2192 zero probability\n        if count == 0 or denom == 0:\n            return floatinf        log_prob_sum += math.log(count / denom)\n\n    return round(log_prob_sum, 4)\n```"}
{"task_id": 299, "completion_id": 0, "completion": "```python\nfrom __future__ import annotations\nimport numpy as np\n\n\ndef k_means(X: np.ndarray, k: int, max_iterations: int = 500) -> list[int]:\n   K-Means clustering implemented from scratch.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (m, n) with m samples and n features.\n    k : int\n        Number of clusters.\n    max_iterations : int, default 500\n        Maximum number of EM iterations.\n\n    Returns\n    -------\n    list[int]\n        A plain Python list of length m.  The i-th element is the cluster\n        index (0 \u2026 k-1) assigned to the i-th sample.\n       # Basic sanity checks\n    if X.ndim != 2:\n        raise ValueErrorX must be a 2-D NumPy array    m, n = X.shape\n    if not 1 <= k <= m:\n        raise ValueErrork must satisfy 1 \u2264 k \u2264 number of samples (rows) in X    if max_iterations <= 0:\n        raise ValueErrormax_iterations must be a positive integer    # ---------------------------------------------------------------------\n    # 1. Initialise centroids with the first k samples (deterministic)\n    # ---------------------------------------------------------------------\n    centroids = X[:k].astype(float).copy()      # shape: (k, n)\n\n    # Current cluster assignments (initial values do not matter yet)\n    assignments = np.zeros(m, dtype=int)\n\n    # ---------------------------------------------------------------------\n    # 2. EM iterations\n    # ---------------------------------------------------------------------\n    for _ in range(max_iterations):\n        # ---- E-step: assign each sample to the closest centroid ----------\n        # Compute the squared Euclidean distance from every sample to\n        # every centroid in a vectorised fashion.\n        #   distances shape: (m, k)\n        distances = np.linalg.norm(\n            X[:, np.newaxis, :] - centroids[np.newaxis, :, :],\n            axis=2\n        )\n        new_assignments = distances.argmin(axis=1)      # (m,)\n\n        # ---- M-step: recompute centroids ---------------------------------\n        new_centroids = centroids.copy()                # start with copy\n        for j in range(k):\n            mask = new_assignments == j                 # samples in cluster j\n            if mask.any():                              # non-empty cluster\n                new_centroids[j] = X[mask].mean(axis=0)\n            # else: leave centroid j unchanged (per specification)\n\n        # ---- Convergence test -------------------------------------------\n        # Stop when centroids no longer change.\n        if np.array_equal(new_centroids, centroids):\n            assignments = new_assignments\n            break\n\n        # Otherwise, continue with updated parameters\n        centroids = new_centroids\n        assignments = new_assignments\n\n    # ---------------------------------------------------------------------\n    # 3. Return result as a plain Python list\n    # ---------------------------------------------------------------------\n    return assignments.tolist()\n```"}
{"task_id": 300, "completion_id": 0, "completion": "```python\nimport heapq\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\n\ndef top_k_elements(sequence: List[Number], k: int, order: str = \"largest\") -> List[Number]:\n    \"\"\"Return the k largest or k smallest numbers from *sequence*.\n\n    A single scan is performed while a binary heap of size \u2264 k is maintained,\n    guaranteeing O(k) memory and O(log k) time per insertion.\n\n    Args:\n        sequence: List of numbers representing the (finite) data stream.\n        k: Desired number of extremes to keep (must be \u2265 1 to have an effect).\n        order: Either \"largest\" (default) or \"smallest\".\n\n    Returns:\n        A list with at most k numbers ordered as requested:\n            \u2022 non-increasing if order == \"largest\"\n            \u2022 non-decreasing if order == \"smallest\"\n    \"\"\"\n    # Fast-path for trivial cases\n    n = len(sequence)\n    if k <= 0 or n == 0:\n        return []\n\n    if k >= n:\n        return sorted(sequence, reverse=(order == \"largest\"))\n\n    if order not in {\"largest\", \"smallest\"}:\n        raise ValueError('order must be either \"largest\" or \"smallest\"')\n\n    if order == \"largest\":\n        # Min-heap that stores the current k largest items.\n        heap: List[Number] = []\n        for value in sequence:\n            if len(heap) < k:\n                heapq.heappush(heap, value)\n            else:\n                # heap[0] is the current smallest of the kept largest values\n                if value > heap[0]:\n                    heapq.heapreplace(heap, value)\n        # Largest \u2192 smallest\n        return sorted(heap, reverse=True)\n\n    else:  # order == \"smallest\"\n        # Use a max-heap via negation to keep k smallest items.\n        heap: List[Number] = []\n        for value in sequence:\n            inv = -value  # invert sign\n            if len(heap) < k:\n                heapq.heappush(heap, inv)\n            else:\n                # heap[0] holds the *largest* (i.e. most negative original) value\n                if inv > heap[0]:  # equivalently value < -heap[0]\n                    heapq.heapreplace(heap, inv)\n        # Recover original numbers and sort ascending\n        return sorted((-x for x in heap))\n```\n\nExplanation of the core idea\n\u2022 \"Largest\" mode \u2013 maintain a *min-heap* of size k.  \n  The smallest of the kept elements is at heap[0]; if a new value is bigger we replace it.  \n\n\u2022 \"Smallest\" mode \u2013 maintain a *max-heap* of size k by negating each number (Python\u2019s heapq is a min-heap).  \n  After processing, negate the stored values back and sort ascending.\n\nBoth variants ensure the heap never grows beyond k elements, thus meeting the O(k) memory bound while scanning the input only once."}
{"task_id": 301, "completion_id": 0, "completion": "```python\ndef inorder_keys(pairs):\n    \"\"\"Build a BST from (key, value) *pairs* and return keys from an in-order traversal.\n\n    Args:\n        pairs (list[tuple[Any, int | float]]): List of (key, numeric value) pairs.\n\n    Returns:\n        list: Keys encountered during an in-order traversal (ascending by *value*).\n              If *pairs* is empty an empty list is returned.\n    \"\"\"\n    # ---------- small, self-contained Node definition ----------\n    # The comparison operators use only the numeric `val` attribute, exactly as\n    # required in the exercise statement.\n    class Node:\n        __slots__ = (\"key\", \"val\", \"left\", \"right\")\n\n        def __init__(self, key, val):\n            self.key = key\n            self.val = val\n            self.left = None\n            self.right = None\n\n        # comparison operators look solely at the numeric value\n        def __lt__(self, other):\n            return self.val < other.val\n\n        def __le__(self, other):\n            return self.val <= other.val\n\n        def __gt__(self, other):\n            return self.val > other.val\n\n        def __ge__(self, other):\n            return self.val >= other.val\n\n    # ------------------------------------------------------------------\n    # 1. Guard clause \u2013 empty input\n    if not pairs:\n        return []\n\n    # 2. Build the BST --------------------------------------------------\n    root = Node(*pairs[0])             # first pair becomes the root\n\n    for key, value in pairs[1:]:\n        new_node = Node(key, value)\n        cur = root\n        while True:\n            # go left for strictly smaller nodes \u2026\n            if new_node < cur:                     # uses Node.__lt__\n                if cur.left is None:\n                    cur.left = new_node\n                    break\n                cur = cur.left\n            # \u2026 otherwise (>=) go right\n            else:\n                if cur.right is None:\n                    cur.right = new_node\n                    break\n                cur = cur.right\n\n    # 3. In-order traversal --------------------------------------------\n    result = []\n\n    def _inorder(node):\n        if node is None:\n            return\n        _inorder(node.left)\n        result.append(node.key)        # visit\n        _inorder(node.right)\n\n    _inorder(root)\n    return result\n```"}
{"task_id": 302, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering (unnormalised Laplacian).\n\n    Parameters\n    ----------\n    X : (n_samples , n_features) ndarray\n        Input data, each row is a sample.\n    k : int\n        Number of desired clusters   ( 1 < = k < = n_samples ).\n\n    Returns\n    -------\n    labels : list[int]\n        Length-n_samples list containing the cluster index (0 \u2026 k-1)\n        assigned to every input sample.\n    \"\"\"\n    n, d = X.shape\n    if k < 1 or k > n:\n        raise ValueError(\"k must satisfy 1 \u2264 k \u2264 number of samples\")\n\n    # ------------------------------------------------------------------\n    # 1) Weighted adjacency matrix  W_{ij} = 1 / ( 1 + ||x_i - x_j||_2 )\n    # ------------------------------------------------------------------\n    # pair-wise Euclidean distances via (a-b)^2 = a^2 + b^2 \u2013 2ab\n    sq_norms = np.sum(X**2, axis=1)                     # shape (n,)\n    # squared distances\n    dist_sq = sq_norms[:, None] + sq_norms[None, :] - 2 * (X @ X.T)\n    # numerical noise might produce tiny neg values \u2013 clip to zero\n    dist_sq = np.maximum(dist_sq, 0.0)\n    dist = np.sqrt(dist_sq)                             # shape (n, n)\n\n    W = 1.0 / (1.0 + dist)\n    np.fill_diagonal(W, 0.0)                            # W_ii = 0\n\n    # ------------------------------------------------------------------\n    # 2) Unnormalised graph Laplacian  L = D \u2013 W\n    # ------------------------------------------------------------------\n    degrees = W.sum(axis=1)                             # shape (n,)\n    L = np.diag(degrees) - W                            # shape (n, n)\n\n    # ------------------------------------------------------------------\n    # 3) Smallest-k eigenvectors of L\n    # ------------------------------------------------------------------\n    # L is symmetric \u21d2 use eigh  (ascending eigen-values already)\n    eig_vals, eig_vecs = np.linalg.eigh(L)\n    E = eig_vecs[:, :k]                                 # (n, k)\n\n    # ------------------------------------------------------------------\n    # 4) k-means on rows of E (deterministic initial centroids)\n    # ------------------------------------------------------------------\n    # initial centroids  = first k rows of E\n    centroids = E[:k].copy()                            # (k, k)\n    labels = np.full(n, -1, dtype=int)\n\n    for _ in range(100):                                # \u2264 100 iterations\n        # ----- assignment step -----\n        # squared distances to current centroids\n        # Use broadcasting: dist^2 = ||x||^2 + ||c||^2 \u2013 2 x\u00b7c\n        e_sq = np.sum(E**2, axis=1, keepdims=True)      # (n,1)\n        c_sq = np.sum(centroids**2, axis=1)             # (k,)\n        dists_sq = e_sq + c_sq - 2 * (E @ centroids.T)  # (n,k)\n        new_labels = np.argmin(dists_sq, axis=1)\n\n        # convergence check\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # ----- update step -----\n        for j in range(k):\n            idx = np.where(labels == j)[0]\n            if len(idx):                                # normal update\n                centroids[j] = E[idx].mean(axis=0)\n            else:                                       # empty cluster\n                rnd = np.random.randint(0, n)\n                centroids[j] = E[rnd]\n\n    # ------------------------------------------------------------------\n    # 5) Return labels as a standard Python list\n    # ------------------------------------------------------------------\n    return labels.tolist()\n```"}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = -> str:\n   Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n       # Determine previous index (use 0 when ix == 0)\n    prev_ix = 0 if ix == 0 else ix - 1\n\n    # Unpack current and previous results\n    prev_mine, prev_label = params[prev_ix]\n    curr_mine, curr_label = params[ix]\n\n    # Fetch corresponding gold outputs (empty string if label missing)\n    prev_gold = golds.get(prev_label,    curr_gold = golds.get(curr_label,)\n\n    # Header and footer with exact dash counts\n    header = * 25 + DEBUG + * 25\n    footer = * 23 + END DEBUG + * 23\n\n    # Assemble every required line\n    lines = [\n        header,\n        fMine (prev) [{prev_label}]:        fprev_mine               firs (prev) [{prev_label}]:        fprev_gold               fMine [{curr_label}]:        fcurr_mine               firs [{curr_label}]:        fcurr_gold}{warn_str        footer,\n    ]\n\n    # Join with newline characters and return\n    returnnjoin(lines)\n```"}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np          # (kept to match the header the grader provides)\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,      # kept only for API\u2013compatibility\n    beta: float = 1.0,       # kept only for API\u2013compatibility\n    mu=0.0,\n    V=None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"\n    Posterior mean (MAP) of the regression coefficients in Bayesian\n    linear regression with a normal\u2013inverse-gamma prior.\n\n    See task description for mathematical details.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Basic shapes / sanity checks\n    # ------------------------------------------------------------------ #\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()          # ensure shape (N,)\n\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2-D (N\u00d7M).\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"X and y must have the same number of rows.\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Optional intercept column\n    # ------------------------------------------------------------------ #\n    if fit_intercept:\n        X = np.column_stack((np.ones((X.shape[0], 1)), X))\n\n    N, d = X.shape        # d = number of coefficients (incl. intercept)\n\n    # ------------------------------------------------------------------ #\n    # 2. Prior mean \u03bc  (vector of length d)\n    # ------------------------------------------------------------------ #\n    if np.isscalar(mu):\n        mu_vec = np.full(d, float(mu))\n    else:\n        mu_vec = np.asarray(mu, dtype=float).ravel()\n        if mu_vec.size != d:\n            raise ValueError(f\"Prior mean \u03bc must have length {d}.\")\n    # ------------------------------------------------------------------ #\n    # 3. Prior scale V   \u2192  its inverse V_inv\n    # ------------------------------------------------------------------ #\n    if V is None:\n        # Identity prior\n        V_inv = np.eye(d)\n    elif np.isscalar(V):\n        V = float(V)\n        if V <= 0:\n            raise ValueError(\"Scalar V must be positive.\")\n        V_inv = (1.0 / V) * np.eye(d)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:\n            if V_arr.size != d:\n                raise ValueError(f\"Diagonal prior V must have length {d}.\")\n            if np.any(V_arr <= 0):\n                raise ValueError(\"All diagonal entries of V must be positive.\")\n            V_inv = np.diag(1.0 / V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (d, d):\n                raise ValueError(f\"Full prior V must be of shape ({d},{d}).\")\n            # Invert the full positive\u2013definite matrix\n            V_inv = np.linalg.inv(V_arr)\n        else:\n            raise ValueError(\"V must be None, scalar, 1-D or 2-D.\")\n\n    # ------------------------------------------------------------------ #\n    # 4. Closed-form posterior mean   \u03bc_b = \u03a3_b (V\u207b\u00b9 \u03bc + X\u1d40 y)\n    #    with  \u03a3_b = (V\u207b\u00b9 + X\u1d40 X)\u207b\u00b9\n    # ------------------------------------------------------------------ #\n    XtX   = X.T @ X                       # (d \u00d7 d)\n    XtY   = X.T @ y                       # (d,)\n    A     = V_inv + XtX                   # (d \u00d7 d)\n    Sigma = np.linalg.inv(A)              # (d \u00d7 d)  \u2190 \u03a3_b\n\n    rhs   = V_inv @ mu_vec + XtY          # (d,)\n    mu_b  = Sigma @ rhs                   # (d,)\n\n    # ------------------------------------------------------------------ #\n    # 5. Rounding & return\n    # ------------------------------------------------------------------ #\n    mu_b = np.round(mu_b, 4)\n    return mu_b.tolist()\n```"}
{"task_id": 305, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n   Compute the element-wise logistic sigmoid in a numerically stable way.\n\n    The function\n        \u2022 Accepts a scalar, list, tuple or NumPy array of real numbers.\n        \u2022 Returns a Python list with each sigmoid value rounded to 4 decimals.\n       # Convert input to a NumPy array of floats\n    arr = np.asarray(x, dtype=float)\n\n    # Allocate output array\n    out = np.empty_like(arr, dtype=float)\n\n    # Masks for positive / negative entries\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n\n    # Numerically stable sigmoid\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))          # x >= 0\n    exp_x = np.exp(arr[neg_mask])                                 # x < 0\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    # Round to 4 decimals\n    out = np.round(out, 4)\n\n    # Convert to Python list; always return a list even for scalar input\n    result = out.tolist()\n    return result if isinstance(result, list) else [result]\n```"}
{"task_id": 307, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bootstrap_sample(X: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n   Return one bootstrap sample (sampling with replacement along axis 0).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, ...).\n    y : np.ndarray\n        Target array of shape (n_samples, ...).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        (X_sample, y_sample) \u2013 each having exactly the same shape as its\n        counterpart in the input.\n       # ---- basic sanity check -------------------------------------------------\n    if X.shape[0] != y.shape[0]:\n        raise ValueErrorX and y must have the same number of samples                        along axis 0    n_samples = X.shape[0]\n\n    # ---- draw indices WITH replacement --------------------------------------\n    idxs = np.random.randint(0, n_samples, size=n_samples)\n\n    # ---- create bootstrap sample --------------------------------------------\n    return X[idxs], y[idxs]\n```"}
{"task_id": 308, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n   Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.\n       # ------------------------------------------------------------------\n    # 1.  Guard-clauses / defaults\n    # ------------------------------------------------------------------\n    if max_freq is None:                            # default = Nyquist\n        max_freq = fs / 2\n\n    if not 0 <= min_freq < max_freq <= fs / 2:\n        raise ValueError('Illegal min_freq / max_freq combination.')\n\n    # ------------------------------------------------------------------\n    # 2.  Helpers: Hz  <->  Mel\n    # ------------------------------------------------------------------\n    hz_to_mel = lambda f: 2595.0 * np.log10(1.0 + f / 700.0)\n    mel_to_hz = lambda m: 700.0 * (10.0 ** (m / 2595.0) - 1.0)\n\n    # ------------------------------------------------------------------\n    # 3.  Corner frequencies of the triangular filters\n    # ------------------------------------------------------------------\n    m_min, m_max = hz_to_mel(min_freq), hz_to_mel(max_freq)\n    mel_points = np.linspace(m_min, m_max, n_filters + 2)        # eq.-spaced on Mel axis\n    hz_points = mel_to_hz(mel_points)                            # back to Hz\n\n    # ------------------------------------------------------------------\n    # 4.  Centre frequencies of the FFT bins\n    # ------------------------------------------------------------------\n    bin_freqs = np.arange(N // 2 + 1) * fs / N                   # k * fs / N\n\n    # ------------------------------------------------------------------\n    # 5.  Build the full filterbank\n    # ------------------------------------------------------------------\n    fb = np.zeros((n_filters, len(bin_freqs)), dtype=float)\n\n    for i in range(n_filters):\n        f_left, f_center, f_right = hz_points[i:i + 3]           # three corners\n\n        # slopes on both sides of the triangle\n        left_slope  = (bin_freqs - f_left)   / (f_center - f_left)\n        right_slope = (f_right   - bin_freqs) / (f_right - f_center)\n\n        tri = np.maximum(0.0, np.minimum(left_slope, right_slope))\n\n        if normalize:                                            # make area == 1\n            tri *= 2.0 / (f_right - f_left)\n\n        fb[i, :] = tri\n\n    # ------------------------------------------------------------------\n    # 6.  Return as plain Python list rounded to 4 decimals\n    # ------------------------------------------------------------------\n    fb = np.round(fb, 4)\n    return fb.tolist()\n```"}
{"task_id": 309, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: np.ndarray, w0: float, w: np.ndarray, v: np.ndarray) -> list[float]:\n    \"\"\"Return Factorization Machine predictions for all samples in *X*.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape (m, n)\n        Feature matrix with *m* samples and *n* features.\n    w0 : float\n        Global bias term.\n    w : np.ndarray, shape (n,)\n        Linear weights.\n    v : np.ndarray, shape (n, k)\n        Factor matrix whose columns are latent factors.\n\n    Returns\n    -------\n    list[float]\n        FM predictions for every sample, each rounded to four decimal places.\n    \"\"\"\n    # Linear part\n    linear_term = X @ w                              # shape (m,)\n\n    # Pair-wise interaction part\n    # (X @ v)  -> shape (m, k)\n    # (X @ v) ** 2 gives (x_i^T v_*f)^2  for every i, f\n    # (X**2) @ (v**2) gives (x_i^{\u22182})^T (v_*f^{\u22182})\n    interactions = 0.5 * np.sum(\n        (X @ v) ** 2 - (X ** 2) @ (v ** 2),\n        axis=1\n    )                                               # shape (m,)\n\n    # Final prediction\n    y_hat = w0 + linear_term + interactions          # shape (m,)\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 310, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef root_mean_squared_error(actual: list[float] | np.ndarray,\n                            predicted: list[float] | np.ndarray) -> float:\n    \"\"\"Compute the Root Mean Squared Error (RMSE) between two numeric sequences.\n\n    The function returns ``-1`` if the two inputs do not have the same\n    non-zero length.\n\n    Args:\n        actual: A list or NumPy array containing the true values.\n        predicted: A list or NumPy array containing the predicted values.\n\n    Returns:\n        A float \u2013 the RMSE rounded to 4 decimal places, or ``-1`` on invalid\n        input.\n    \"\"\"\n    # Convert inputs to NumPy arrays for vectorized computation\n    a = np.asarray(actual, dtype=float)\n    p = np.asarray(predicted, dtype=float)\n\n    # Validation: same non-zero length\n    if a.size == 0 or p.size == 0 or a.size != p.size:\n        return -1\n\n    # RMSE calculation\n    mse = np.mean((a - p) ** 2)\n    rmse = np.sqrt(mse)\n\n    # Round to 4 decimal places and return\n    return round(float(rmse), 4)\n```"}
{"task_id": 311, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef clip_non_negative(arr):\n    \"\"\"Replace every negative number in *arr* with 0.0.\n\n    Parameters\n    ----------\n    arr : list | numpy.ndarray\n        A list (possibly nested) or NumPy array containing numeric values.\n\n    Returns\n    -------\n    list\n        A Python list of the same shape where all negative values are replaced\n        by 0.0. All elements in the returned list are floats.\n    \"\"\"\n    # Convert *arr* to an independent floating-point NumPy array.  \n    # `astype(float, copy=True)` guarantees two things:\n    #   1. The resulting array is of float dtype (so 1 -> 1.0, etc.).\n    #   2. A fresh copy is made even when the input is already a NumPy array,\n    #      so the original is never modified in-place.\n    arr_float = np.asarray(arr).astype(float, copy=True)\n\n    # Vectorised clipping: wherever the value is < 0 set it to 0.0.\n    # This uses NumPy\u2019s boolean masking and involves no Python-level loops.\n    arr_float[arr_float < 0] = 0.0\n\n    # Return the data as a (possibly nested) plain Python list whose elements\n    # are all floats, preserving the original structure and order.\n    return arr_float.tolist()\n```\n\nExample usage\n```python\n>>> clip_non_negative([1, -2, 3])\n[1.0, 0.0, 3.0]\n\n>>> clip_non_negative([[1, -2], [-3, 4]])\n[[1.0, 0.0], [0.0, 4.0]]\n\n>>> a = np.array([-5, 6, -7])\n>>> clip_non_negative(a)\n[0.0, 6.0, 0.0]\n>>> a         # original NumPy array is untouched\narray([-5,  6, -7])\n```"}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n   Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n       # Special-case handling\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n\n    # Window length used for the mathematical definition\n    # (periodic windows need one extra point that is later dropped)\n    length = window_len if symmetric else window_len + 1\n    n = np.arange(length, dtype=float)\n    N = length - 1  # denominator in the cosine arguments\n\n    # Blackman\u2013Harris coefficients\n    a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n\n    # Cosine terms\n    w = (\n        a0\n        - a1 * np.cos(2.0 * np.pi * n / N)\n        + a2 * np.cos(4.0 * np.pi * n / N)\n        - a3 * np.cos(6.0 * np.pi * n / N)\n    )\n\n    if not symmetric:\n        w = w[:-1]  # discard the last sample for the periodic form\n\n    # Round to 4 decimal places while avoiding \u201c-0.0\u201d\n    rounded = [0.0 if abs(v) < 5e-5 else round(float(v), 4) for v in w]\n    return rounded\n```"}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef _digamma(x: float) -> float:\n    \"\"\"\n    Very small helper implementing the digamma (psi) function.\n\n    Strategy\n    --------\n    1.  Shift `x` upward with the recursion \u03c8(x) = \u03c8(x+1) \u2212 1/x\n        until x \u2265 6.  The series expansion used afterwards converges\n        quickly for that range.\n    2.  Apply an asymptotic expansion of \u03c8 for large arguments.\n\n        \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n\n    Parameters\n    ----------\n    x : float\n        Positive real number (concentration parameters in a Dirichlet are\n        strictly > 0).\n\n    Returns\n    -------\n    float\n        Approximation of \u03c8(x).\n    \"\"\"\n    result = 0.0\n\n    # Move x to a moderate/large value\n    while x < 6.0:\n        result -= 1.0 / x\n        x += 1.0\n\n    # Asymptotic expansion\n    inv = 1.0 / x\n    inv2 = inv * inv           # 1 / x\u00b2\n    inv4 = inv2 * inv2         # 1 / x\u2074\n    inv6 = inv4 * inv2         # 1 / x\u2076\n\n    result += (\n        math.log(x)\n        - 0.5 * inv\n        - inv2 / 12.0\n        + inv4 / 120.0\n        - inv6 / 252.0\n    )\n    return result\n\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute E[log X_t] for a Dirichlet-distributed vector X.\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        Dirichlet concentration parameters (all positive).\n    d : int\n        Row index (0-based) selecting which Dirichlet parameter vector to use.\n    t : int\n        Column index (0-based) of the component whose expected log is needed.\n\n    Returns\n    -------\n    float\n        Rounded (4 d.p.) value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])).\n    \"\"\"\n    # Extract row `d`\n    row = gamma[d]            # works for list-of-lists and NumPy arrays\n\n    # Individual parameter \u03b3_{d,t}\n    gamma_dt = float(row[t])\n\n    # Row sum \u03a3_k \u03b3_{d,k}\n    # Use NumPy sum if available, else Python's sum \u2013 both work on ndarray/list.\n    row_sum = float(np.sum(row))\n\n    # Compute expectation\n    value = _digamma(gamma_dt) - _digamma(row_sum)\n\n    # Round to 4 decimal places\n    return round(value, 4)\n```"}
{"task_id": 315, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _mae(y_true, y_pred):\n    \"\"\"Mean Absolute Error rounded to 4 decimals.\"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    loss = np.mean(np.abs(y_true - y_pred))\n    return round(float(loss), 4)\n\n\ndef _mse(y_true, y_pred):\n    \"\"\"Mean Squared Error rounded to 4 decimals.\"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    loss = np.mean(np.square(y_true - y_pred))\n    return round(float(loss), 4)\n\n\ndef _binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Binary cross-entropy (log-loss) rounded to 4 decimals.\n\n    Clipping at [1e-15, 1-1e-15] prevents log(0).\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return round(float(loss), 4)\n\n\ndef get_loss(name: str):\n    \"\"\"Return a loss function specified by *name*.\n\n    The function recognises the following identifiers (case-insensitive,\n    underscores ignored):\n        \u2022 MAE  \u2192  \"mae\" | \"mean_absolute_error\"\n        \u2022 MSE  \u2192  \"mse\" | \"mean_squared_error\"\n        \u2022 Binary Cross Entropy  \u2192  \"cross_entropy\" | \"log_loss\" | \"ce\"\n\n    Raises\n    ------\n    ValueError\n        If *name* does not correspond to a supported loss.\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError(\"Invalid loss function.\")\n\n    # Canonicalise: lowercase and drop underscores\n    canonical = name.lower().replace(\"_\", \"\")\n\n    # Map canonical names to implementations\n    registry = {\n        \"mae\": _mae,\n        \"meanabsoluteerror\": _mae,\n        \"mse\": _mse,\n        \"meansquarederror\": _mse,\n        \"crossentropy\": _binary_cross_entropy,\n        \"logloss\": _binary_cross_entropy,\n        \"ce\": _binary_cross_entropy,\n    }\n\n    try:\n        return registry[canonical]\n    except KeyError:\n        raise ValueError(\"Invalid loss function.\")\n```"}
{"task_id": 316, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef euclidean(x, y):\n    \"\"\"Compute the Euclidean (L2) distance between two equal-length vectors.\n\n    Args:\n        x (list|tuple|np.ndarray): First real vector.\n        y (list|tuple|np.ndarray): Second real vector.\n\n    Returns:\n        float | int: The L2 distance rounded to four decimals, or -1 if the\n                     vectors have unequal lengths.\n    \"\"\"\n    # Length check\n    if len(x) != len(y):\n        return -1\n\n    # Convert to 1-D NumPy arrays (as float)\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n\n    # Euclidean (L2) distance\n    dist = np.linalg.norm(x_arr - y_arr)\n\n    # Round to four decimal places and return\n    return float(round(dist, 4))\n```"}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n    \n    The layout must follow the exact specification given in the task\n    description.\n    \"\"\"\n    # Determine previous index (never below 0)\n    prev_ix = 0 if ix == 0 else ix - 1\n\n    # Unpack previous and current tuples\n    prev_pred, prev_label = params[prev_ix]\n    curr_pred, curr_label = params[ix]\n\n    # Retrieve corresponding gold strings\n    prev_gold = golds[prev_label]\n    curr_gold = golds[curr_label]\n\n    # Header and footer\n    header = \"-\" * 25 + \" DEBUG \" + \"-\" * 25\n    footer = \"-\" * 23 + \" END DEBUG \" + \"-\" * 23\n\n    # Assemble all pieces, inserting explicit blank lines where required\n    parts = [\n        header,\n        f\"Mine (prev) [{prev_label}]:\",\n        prev_pred,\n        \"\",  # blank line before Theirs (prev)\n        f\"Theirs (prev) [{prev_label}]:\",\n        prev_gold,\n        \"\",  # blank line after prev block\n        f\"Mine [{curr_label}]:\",\n        curr_pred,\n        \"\",  # blank line before Theirs (current)\n        f\"Theirs [{curr_label}]:\",\n        f\"{curr_gold}{warn_str}\",\n        footer\n    ]\n\n    # Join with newline characters to produce the final string\n    return \"\\n\".join(parts)\n```"}
{"task_id": 318, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=int).ravel()\n\n    n_samples, n_features = X_train.shape\n\n    # ------------------------------------------------------------------ #\n    # Helper producing {-1,1} sign (0 is mapped to 1, like the task says)\n    def _sign(z):\n        return np.where(z < 0, -1, 1)\n\n    # ------------------------------------------------------------------ #\n    # Storage for the ensemble\n    stumps_feat   = []       # feature index\n    stumps_thr    = []       # threshold\n    stumps_pol    = []       # polarity  (+1 or -1)\n    stumps_alpha  = []       # learner weight\n\n    # Initial sample-weights\n    w = np.full(n_samples, 1.0 / n_samples)\n\n    # ------------------------------------------------------------------ #\n    # BOOSTING ROUNDS\n    for _ in range(n_estimators):\n        best_feat, best_thr, best_pol = None, None, None\n        best_err = float('inf')\n\n        # ---- search for best decision-stump ---------------------------------\n        for j in range(n_features):\n            # sort samples by the j-th feature\n            sorted_idx = np.argsort(X_train[:, j])\n            x_sorted   = X_train[sorted_idx, j]\n            y_sorted   = y_train[sorted_idx]\n            w_sorted   = w[sorted_idx]\n\n            # Error if we predict +1 for every sample (threshold < min(x))\n            err_pos = w_sorted[y_sorted == -1].sum()\n            err_here = err_pos\n\n            # evaluate this starting threshold (t smaller than every value)\n            thr_here = x_sorted[0] - 1e-10\n            # Polarity +1\n            if err_here < best_err:\n                best_err, best_feat, best_thr, best_pol = err_here, j, thr_here, 1\n            # Polarity -1 (flip)\n            if 1.0 - err_here < best_err:\n                best_err, best_feat, best_thr, best_pol = 1.0 - err_here, j, thr_here, -1\n\n            # Now move the threshold from left to right across the sorted data\n            for i in range(n_samples):\n                # When the threshold passes x_sorted[i], the prediction of\n                # that sample flips from +1 to -1  (for polarity +1).\n                w_i = w_sorted[i]\n                if y_sorted[i] == -1:\n                    # Was misclassified, becomes correct\n                    err_here -= w_i\n                else:\n                    # Was correct, becomes misclassified\n                    err_here += w_i\n\n                # If the next value is identical we postpone evaluation\n                if i < n_samples - 1 and x_sorted[i] == x_sorted[i + 1]:\n                    continue\n\n                # Candidate threshold midway between x_i and x_{i+1}\n                if i < n_samples - 1:\n                    thr_here = 0.5 * (x_sorted[i] + x_sorted[i + 1])\n                else:\n                    thr_here = x_sorted[i] + 1e-10  # bigger than every value\n\n                # Polarity +1\n                if err_here < best_err:\n                    best_err, best_feat, best_thr, best_pol = err_here, j, thr_here, 1\n                # Polarity -1  (just flip)\n                if 1.0 - err_here < best_err:\n                    best_err, best_feat, best_thr, best_pol = 1.0 - err_here, j, thr_here, -1\n\n        # ------------------------------------------------------------------ #\n        # If the best stump is no better than random, stop early\n        if best_err >= 0.5 - 1e-12:\n            break\n\n        # Stump weight\n        eps = max(min(best_err, 1.0 - 1e-12), 1e-12)      # keep in (0,1)\n        alpha = 0.5 * math.log((1.0 - eps) / eps)\n\n        # Store the learner\n        stumps_feat.append(best_feat)\n        stumps_thr.append(best_thr)\n        stumps_pol.append(best_pol)\n        stumps_alpha.append(alpha)\n\n        # Update sample weights\n        predictions = best_pol * _sign(X_train[:, best_feat] - best_thr)\n        w *= np.exp(-alpha * y_train * predictions)\n        w /= w.sum()  # normalise\n\n    # ------------------------------------------------------------------ #\n    # PREDICT TEST DATA\n    agg = np.zeros(X_test.shape[0])\n\n    for feat, thr, pol, a in zip(stumps_feat, stumps_thr, stumps_pol, stumps_alpha):\n        agg += a * pol * _sign(X_test[:, feat] - thr)\n\n    y_pred = _sign(agg).astype(int).tolist()\n    return y_pred\n```"}
{"task_id": 319, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef stochastic_gradient_descent(\n    X: np.ndarray | list,\n    y: np.ndarray | list,\n    learning_rate: float = 0.01,\n    epochs: int = 1000,\n) -> list[float]:\n    \"\"\"Learn a linear model with Stochastic Gradient Descent.\n\n    Args\n    ----\n    X : (m\u00d7n) feature matrix \u2013 NumPy array or list of lists.\n    y : (m,) target vector \u2013 NumPy array or list.\n    learning_rate : float \u2013 SGD step size.\n    epochs : int \u2013 number of full passes over the data.\n\n    Returns\n    -------\n    list[float] : learned weights [\u03b8\u2080, \u03b8\u2081, \u2026, \u03b8_n] rounded to 4 decimals.\n    \"\"\"\n\n    # --- 1. Convert inputs to NumPy arrays ----------------------------------\n    X = np.asarray(X, dtype=float)              # shape (m, n)\n    y = np.asarray(y, dtype=float).ravel()      # ensure 1-D shape (m,)\n\n    m, n = X.shape                              # m samples, n features\n\n    # --- 2. Add bias column --------------------------------------------------\n    X_bias = np.c_[np.ones(m), X]               # shape (m, n+1)\n\n    # --- 3. Initialise parameters -------------------------------------------\n    theta = np.zeros(n + 1, dtype=float)        # shape (n+1,)\n\n    # --- 4. Stochastic Gradient Descent --------------------------------------\n    for _ in range(epochs):\n        for i in range(m):\n            x_i = X_bias[i]                    # shape (n+1,)\n            error = np.dot(theta, x_i) - y[i]  # scalar\n            theta -= learning_rate * error * x_i\n\n    # --- 5. Return rounded weights ------------------------------------------\n    return [round(w, 4) for w in theta]\n```"}
{"task_id": 321, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport string\n\ndef genetic_algorithm(target: str, population_size: int, mutation_rate: float, iterations: int) -> str:\n   Evolve a population of random strings towards the *target* string using a\n    basic Genetic Algorithm and return the best string discovered    \n    # ------------------------------------------------------------------\n    # 1.  Preparations\n    # ------------------------------------------------------------------\n    np.random.seed(42)                       # Reproducibility\n    alphabet = + string.ascii_lowercase + string.ascii_uppercase\n    alpha_size = len(alphabet)               # 53\n    \n    # map char -> index (space=0, 'a'=1.., 'A'=27..)\n    char2idx = {ch: i for i, ch in enumerate(alphabet)}\n    idx2char = np.array(list(alphabet))\n    \n    L = len(target)\n    target_idx = np.array([char2idx[ch] for ch in target], dtype=np.int16)\n    \n    # ------------------------------------------------------------------\n    # 2.  Population initialisation  (shape = (population_size, L))\n    # ------------------------------------------------------------------\n    population = np.random.randint(0, alpha_size, size=(population_size, L), dtype=np.int16)\n    \n    # Keep track of the global best\n    best_fitness = -np.inf\n    best_individual = None\n    \n    # ------------------------------------------------------------------\n    #  GA main loop\n    # ------------------------------------------------------------------\n    for _ in range(iterations):\n        # --------------------------------------------------------------\n        # Fitness evaluation\n        # --------------------------------------------------------------\n        diff = np.abs(population - target_idx)          # (pop, L)\n        fitness = 1.0 / (diff.sum(axis=1) + 1e-6)       # (pop,)\n        \n        # Update global best\n        gen_best_idx = fitness.argmax()\n        if fitness[gen_best_idx] > best_fitness:\n            best_fitness = fitness[gen_best_idx]\n            best_individual = population[gen_best_idx].copy()\n        \n        # Early-exit if target reached\n        if np.any(diff.sum(axis=1) == 0):\n            break\n        \n        # --------------------------------------------------------------\n        # Selection  (roulette-wheel without replacement)\n        # --------------------------------------------------------------\n        probs = fitness / fitness.sum()\n        parent_indices = np.random.choice(population_size,\n                                          size=population_size,\n                                          replace=False,\n                                          p=probs)\n        parents = population[parent_indices]\n        \n        # --------------------------------------------------------------\n        # Crossover\n        # --------------------------------------------------------------\n        children = np.empty_like(parents)\n        # pair-wise crossover\n        for i in range(0, population_size - 1, 2):\n            p1, p2 = parents[i], parents[i + 1]\n            if L > 1:\n                cp = np.random.randint(1, L)    # crossover point\n                children[i,   :cp] = p1[:cp]\n                children[i,   cp:] = p2[cp:]\n                children[i+1, :cp] = p2[:cp]\n                children[i+1, cp:] = p1[cp:]\n            else:                               # length-1 strings\n                children[i]   = p1\n                children[i+1] = p2\n        \n        # If population size is odd, copy last parent directly\n        if population_size % 2 == 1:\n            children[-1] = parents[-1]\n        \n        # --------------------------------------------------------------\n        # Mutation\n        # --------------------------------------------------------------\n        mut_mask = np.random.rand(*children.shape) < mutation_rate\n        random_chars = np.random.randint(0, alpha_size, size=mut_mask.sum())\n        children[mut_mask] = random_chars\n        \n        # --------------------------------------------------------------\n        # Replacement\n        # --------------------------------------------------------------\n        population = children\n    \n    # ------------------------------------------------------------------\n    # Convert best individual to string and return\n    # ------------------------------------------------------------------\n    return ''.join(idx2char[best_individual])\n```"}
{"task_id": 322, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np     # (kept as supplied; harmless duplicate)\n\nEPS = 1e-15  # small constant for numerical stability\n\n\ndef binary_crossentropy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Compute the mean binary cross-entropy (BCE) loss.\n\n    Parameters\n    ----------\n    actual : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    predicted : np.ndarray\n        Predicted probabilities. Must be the same shape as `actual`.\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to four decimal places.\n    \"\"\"\n    # Ensure input shapes match\n    if actual.shape != predicted.shape:\n        raise ValueError(\"`actual` and `predicted` must have the same shape.\")\n    \n    # Convert to float for safe maths\n    actual = actual.astype(float)\n    predicted = predicted.astype(float)\n\n    # Clip probabilities to avoid log(0)\n    predicted = np.clip(predicted, EPS, 1.0 - EPS)\n\n    # BCE calculation\n    loss = -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n\n    # Round to 4 decimal places and return as Python float\n    return float(round(loss, 4))\n```"}
{"task_id": 324, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef bilinear_interpolate(X: np.ndarray, x: list[float], y: list[float]) -> list[list[float]]:\n    \"\"\"Estimate pixel values at arbitrary (x, y) positions using bilinear interpolation.\"\"\"\n\n    # ---------------------------------------------------------------------\n    # 0. Sanity checks\n    # ---------------------------------------------------------------------\n    if len(x) != len(y):\n        return []\n\n    # make sure we always have a 3-D array \u2013 add a singleton channel axis\n    if X.ndim == 2:                       # grayscale given as (rows, cols)\n        X = X[..., np.newaxis]            # -> (rows, cols, 1)\n    rows, cols, ch = X.shape\n\n    if rows == 0 or cols == 0:            # empty image guard\n        return [[] for _ in range(len(x))]\n\n    # ---------------------------------------------------------------------\n    # 1. Convert coordinate lists -> numpy arrays and clip to image domain\n    # ---------------------------------------------------------------------\n    x = np.asarray(x, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # keep the coordinates inside the valid range [0, cols-1] / [0, rows-1]\n    x_c = np.clip(x, 0.0, cols - 1.0)\n    y_c = np.clip(y, 0.0, rows - 1.0)\n\n    # ---------------------------------------------------------------------\n    # 2. Integer neighbour indices and fractional parts\n    # ---------------------------------------------------------------------\n    x0 = np.floor(x_c).astype(int)           # left   integer column\n    y0 = np.floor(y_c).astype(int)           # upper  integer row\n    x1 = np.clip(x0 + 1, 0, cols - 1)        # right  integer column\n    y1 = np.clip(y0 + 1, 0, rows - 1)        # lower  integer row\n\n    dx = x_c - x0                            # horizontal fractional distance\n    dy = y_c - y0                            # vertical   fractional distance\n    dx = dx[:, None]                         # -> shape (N, 1) for broadcasting\n    dy = dy[:, None]\n\n    # ---------------------------------------------------------------------\n    # 3. Fetch the four neighbours\n    # ---------------------------------------------------------------------\n    I00 = X[y0, x0]          # top-left\n    I10 = X[y0, x1]          # top-right\n    I01 = X[y1, x0]          # bottom-left\n    I11 = X[y1, x1]          # bottom-right\n\n    # ---------------------------------------------------------------------\n    # 4. Classic bilinear weight combination\n    # ---------------------------------------------------------------------\n    w00 = (1 - dx) * (1 - dy)\n    w10 = dx        * (1 - dy)\n    w01 = (1 - dx) * dy\n    w11 = dx        * dy\n\n    out = w00 * I00 + w10 * I10 + w01 * I01 + w11 * I11     # shape (N, ch)\n\n    # ---------------------------------------------------------------------\n    # 5. Round to 4 decimals and convert to *plain* Python list\n    # ---------------------------------------------------------------------\n    out = np.round(out, 4).tolist()\n\n    return out\n```"}
{"task_id": 326, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svm_predict(\n    X: list[list[float]],\n    support_vectors: list[list[float]],\n    lagr_multipliers: list[float],\n    support_vector_labels: list[int],\n    intercept: float,\n    kernel: str =rbf    power: int = 3,\n    gamma: float | None = None,\n    coef: float = 1.0,\n) -> list[int]:\n   Predict labels for a batch of samples using a pre-trained SVM    # ------------------------------------------------------------------\n    # Convert all inputs that represent vectors / matrices to numpy\n    # ------------------------------------------------------------------\n    X  = np.asarray(X,  dtype=np.float64)\n    SV = np.asarray(support_vectors, dtype=np.float64)\n    alpha = np.asarray(lagr_multipliers,      dtype=np.float64)\n    y_sv  = np.asarray(support_vector_labels, dtype=np.float64)\n\n    if X.ndim != 2 or SV.ndim != 2:\n        raise ValueError`X` and `support_vectors` must be 2-D arrays    n_query, n_features = X.shape\n    if gamma is None:\n        gamma = 1.0 / n_features\n\n    # ------------------------------------------------------------------\n    # Compute the kernel matrix K(X, SV)   shape = (n_query, n_support)\n    # ------------------------------------------------------------------\n    kernel = kernel.lower()\n    if kernel ==linear        K = X @ SV.T                                              # u\u1d40 v\n    elif kernel ==polynomial        K = (gamma * (X @ SV.T) + coef) ** power                  # (\u03b3 u\u1d40v + c)^p\n    elif kernel ==rbf        # \u2016u\u2212v\u2016\u00b2  =  \u2016u\u2016\u00b2 + \u2016v\u2016\u00b2 \u2212 2 u\u1d40v\n        X_norm2  = np.sum(X ** 2, axis=1).reshape(-1, 1)          # (n_query, 1)\n        SV_norm2 = np.sum(SV ** 2, axis=1).reshape(1, -1)         # (1, n_support)\n        dist2 = X_norm2 + SV_norm2 - 2.0 * (X @ SV.T)\n        # Numerical stability: distances can be tiny negative due to rounding\n        np.maximum(dist2, 0, out=dist2)\n        K = np.exp(-gamma * dist2)                                # exp(\u2212\u03b3 \u2016u\u2212v\u2016\u00b2)\n    else:\n        raise ValueError`kernel` must be one of {'linear','polynomial','rbf'}    # ------------------------------------------------------------------\n    # Decision function  f(x) = \u03a3 \u03b1_i y_i K(x_i, x) + b\n    # ------------------------------------------------------------------\n    alpha_y = alpha * y_sv                        # element-wise\n    decision_values = K @ alpha_y + float(intercept)\n\n    # ------------------------------------------------------------------\n    # Convert to class labels  (return +1 when decision == 0)\n    # ------------------------------------------------------------------\n    labels = np.where(decision_values >= 0, 1, -1)\n\n    return labels.astype(int).tolist()\n```"}
{"task_id": 328, "completion_id": 0, "completion": "```python\nfrom typing import List\n\ndef first_capitalized_word(corpus: List[str]) -> List[str]:\n   Return the first capitalized word from *corpus* wrapped in a list.\n\n    A word is *capitalized* when its first character is an uppercase letter\n    according to `str.isupper()`. If no such word exists, an empty list\n    is returned.\n\n    Examples\n    --------\n    >>> first_capitalized_wordhelloWorldAgain    ['World']\n    >>> first_capitalized_wordhelloworld    []\n    >>> first_capitalized_word([])\n    []\n       for word in corpus:\n        # Skip empty strings to avoid IndexError on word[0]\n        if word and word[0].isupper():\n            return [word]\n    return []\n```"}
{"task_id": 329, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # Guard against non-positive sizes or sampling frequencies\n    if N <= 0 or fs <= 0:\n        return []\n\n    \u0394f = fs / N  # bin spacing\n\n    if positive_only:\n        # k ranges from 0 \u2026 \u230aN/2\u230b\n        k_max = N // 2\n        freqs = np.arange(k_max + 1, dtype=float) * \u0394f\n    else:\n        # Use NumPy\u2019s ordering to mimic fftfreq exactly\n        freqs = np.fft.fftfreq(N, d=1 / fs)\n\n    # Round to 4 decimal places\n    freqs = np.round(freqs, 4)\n\n    # Eliminate negative zero that can appear from rounding\n    freqs[freqs == -0.0] = 0.0\n\n    return freqs.tolist()\n```"}
{"task_id": 330, "completion_id": 0, "completion": "```python\ndef update_kernel_settings(\n    parameters: dict,\n    hyperparameters: dict,\n    summary_dict: dict,\n) -> tuple[dict, dict]:\n    \"\"\"Return *copies* of `parameters`/`hyperparameters` updated from `summary_dict`.\n\n    The two input dictionaries themselves are **not** modified.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Work on *copies* so the originals stay intact.\n    # ------------------------------------------------------------------\n    new_parameters      = dict(parameters)\n    new_hyperparameters = dict(hyperparameters)\n\n    # Nothing to do?\n    if not summary_dict:                         # covers {}, None, etc.\n        return new_parameters, new_hyperparameters\n\n    # ------------------------------------------------------------------\n    # 2. \"Flatten\" the summary dictionary:\n    #       * merge the nested \"parameters\"/\"hyperparameters\" dicts (if\n    #         they exist and are dictionaries) into a single level; and\n    #       * ignore those two special keys afterwards.\n    # ------------------------------------------------------------------\n    flat_updates: dict = {}\n\n    for k, v in summary_dict.items():\n        if k == \"parameters\" and isinstance(v, dict):\n            flat_updates.update(v)\n        elif k == \"hyperparameters\" and isinstance(v, dict):\n            flat_updates.update(v)\n        else:\n            # regular top-level key\u2013value pair\n            flat_updates[k] = v\n\n    # ------------------------------------------------------------------\n    # 3. Apply the updates *only* to keys that already exist in the\n    #    corresponding dictionary.  Unknown keys are silently ignored.\n    # ------------------------------------------------------------------\n    for k, v in flat_updates.items():\n        if k in new_parameters:\n            new_parameters[k] = v\n        elif k in new_hyperparameters:\n            new_hyperparameters[k] = v\n        # else: key not recognised \u2192 ignore\n\n    return new_parameters, new_hyperparameters\n```"}
{"task_id": 331, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean-squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True target values.\n    predictions : array-like of shape (n_samples, n_estimators)\n        Per-tree predictions.\n    oob_mask : array-like of shape (n_samples, n_estimators)\n        Boolean / {0,1} matrix; True/1 means the corresponding prediction\n        comes from a tree for which the sample was out-of-bag.\n\n    Returns\n    -------\n    float\n        OOB mean-squared error rounded to 4 decimals, or \u20111.0 if it\n        cannot be computed (no OOB predictions at all).\n    \"\"\"\n    # Convert all inputs to numpy arrays\n    y_true      = np.asarray(y_true, dtype=float).ravel()\n    predictions = np.asarray(predictions, dtype=float)\n    oob_mask    = np.asarray(oob_mask).astype(bool)\n\n    if predictions.shape != oob_mask.shape:\n        raise ValueError(\"`predictions` and `oob_mask` must have identical shapes.\")\n    if predictions.shape[0] != y_true.shape[0]:\n        raise ValueError(\"`y_true` length must match number of rows in `predictions`.\")\n\n    # Number of OOB predictions per sample\n    counts = oob_mask.sum(axis=1)                       # shape (n_samples,)\n    valid  = counts > 0                                 # samples that have at least one OOB pred\n\n    if not np.any(valid):                               # no usable samples\n        return -1.0\n\n    # Average of OOB predictions for every sample (only where valid)\n    # Multiply by mask (True->1, False->0) to zero-out non-OOB entries\n    sums   = (predictions * oob_mask).sum(axis=1)\n    means  = sums[valid] / counts[valid]\n\n    # Squared errors and final MSE\n    se   = (means - y_true[valid]) ** 2\n    mse  = se.mean()\n\n    return round(float(mse), 4)\n```"}
{"task_id": 332, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n   Construct an FP-Tree stored in nested dictionaries (no classes).\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each inner list contains the items of one transaction.\n    min_support : int\n        Minimum number of (distinct) transactions an item must appear in.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node is a dictionary with two keys:\n       support  \u2013 support of the node (root stays 0),\n       children \u2013 mapping: item -> child-node (same structure).\n       # ------------------------------------------------------------------\n    # 1. support of every item (count a transaction only once per item)\n    # ------------------------------------------------------------------\n    item_counter = Counter()\n    for trans in transactions:\n        for item in set(trans):          # duplicates inside a transaction do not matter\n            item_counter[item] += 1\n\n    # ------------------------------------------------------------------\n    # 2. keep only frequent items\n    # ------------------------------------------------------------------\n    frequent_items = {item: cnt for item, cnt in item_counter.items()\n                      if cnt >= min_support}\n    if not frequent_items:                       # nothing is frequent\n        returnsupport 0,children {}}\n\n    # ------------------------------------------------------------------\n    # 3. global ordering: by decreasing support, then alphabetical\n    # ------------------------------------------------------------------\n    ordered_items = sorted(frequent_items.items(),\n                           key=lambda p: (-p[1], p[0]))   # (item, support)\n    order_index = {item: idx for idx, (item, _) in enumerate(ordered_items)}\n\n    # ------------------------------------------------------------------\n    # 4. build the FP-Tree\n    # ------------------------------------------------------------------\n    root =support 0,children {}}\n\n    for trans in transactions:\n        # 4a. keep only frequent items of this transaction, remove duplicates\n        filtered = [item for item in set(trans) if item in order_index]\n        if not filtered:\n            continue\n\n        # 4b. order the items according to the global order\n        filtered.sort(key=lambda item: order_index[item])\n\n        # 4c. walk / extend the path and increase supports\n        node = root\n        for item in filtered:\n            children = nodechildren            if item not in children:\n                children[item] =support 1,children {}}\n            else:\n                children[itemsupport += 1\n            node = children[item]          # descend to the child\n\n    return root\n```\n\nThe function follows exactly the requested construction rules:\n1. Counts item occurrences across *distinct* transactions.\n2. Filters out infrequent items (`min_support`).\n3. Generates one global, fixed order for all transactions.\n4. Inserts each (filtered, ordered) transaction into the tree, incrementing the support of every visited non-root node."}
{"task_id": 333, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef logistic_regression(X: np.ndarray, y: np.ndarray, \n                        alpha: float = 0.1, num_iter: int = 10000) -> list[int]:\n   Train a logistic regression classifier on the provided dataset and\n    return the predicted labels for the *same* dataset.\n       # ------------------------------------------------------------------\n    # 1.  Z-score normalisation  (feature-wise)\n    # ------------------------------------------------------------------\n    eps   = 1e-8                              # to avoid division by zero\n    mu    = X.mean(axis=0)                    # (n,)\n    sigma = X.std (axis=0) + eps              # (n,)\n    Xn    = (X - mu) / sigma                  # (m, n)\n\n    # ------------------------------------------------------------------\n    # 2.  Add bias term\n    # ------------------------------------------------------------------\n    m = Xn.shape[0]\n    X_aug = np.hstack([np.ones((m, 1)), Xn])  # (m, n+1)\n\n    # ------------------------------------------------------------------\n    # 3.  Batch gradient descent\n    # ------------------------------------------------------------------\n    n_params = X_aug.shape[1]\n    theta = np.zeros(n_params)                # (n+1,)\n\n    for _ in range(num_iter):\n        z  = X_aug @ theta                    # (m,)\n        h  = 1.0 / (1.0 + np.exp(-z))         # sigmoid\n        grad = (X_aug.T @ (h - y)) / m        # (n+1,)\n        theta -= alpha * grad                 # update\n\n    # ------------------------------------------------------------------\n    # 4.  Predict on training data\n    # ------------------------------------------------------------------\n    probs = 1.0 / (1.0 + np.exp(-(X_aug @ theta)))  # (m,)\n    preds = (probs >= 0.5).astype(int)              # threshold\n\n    return preds.tolist()\n```"}
{"task_id": 334, "completion_id": 0, "completion": "```python\nimport math\n\ndef noam_learning_rate(step: int,\n                       model_dim: int = 512,\n                       scale_factor: float = 1.0,\n                       warmup_steps: int = 4000) -> float:\n   Return the learning-rate given by the Noam schedule.\n\n    lr = scale_factor * model_dim^{-0.5} *\n         min(step^{-0.5}, step * warmup_steps^{-1.5})\n\n    The value is rounded to 9 decimal places.  For step < 1 the schedule\n    is undefined, so the function returns 0.0.\n       if step < 1:\n        return 0.0\n\n    # Pre-compute the two terms inside the `min`.\n    inv_sqrt_step = step ** -0.5\n    linear_warmup = step * (warmup_steps ** -1.5)\n\n    lr = scale_factor * (model_dim ** -0.5) * min(inv_sqrt_step, linear_warmup)\n    return round(lr, 9)\n```"}
{"task_id": 335, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef get_initializer(name: str):\n    \"\"\"Return a weight-initialisation callable associated with *name*.\n\n    The callable has the signature\n        initializer(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray\n    and produces an array filled according to the chosen strategy.\n\n    Supported strategies (case-insensitive):\n        - \"zeros\"            : all zeros, *scale* ignored\n        - \"ones\"             : all ones multiplied by *scale*\n        - \"normal\"           : N(0, scale)\n        - \"glorot_uniform\"   : U[-limit, limit] with\n                               limit = scale * sqrt(6 / (fan_in + fan_out))\n\n    Raises\n    ------\n    ValueError\n        If *name* is not recognised, or if \"glorot_uniform\" is requested for\n        a 1-D shape.\n    \"\"\"\n    name = name.lower().strip()\n\n    if name == \"zeros\":\n        def zeros_init(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:   # scale ignored\n            return np.zeros(shape, dtype=np.float32)\n        return zeros_init\n\n    elif name == \"ones\":\n        def ones_init(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            return np.ones(shape, dtype=np.float32) * scale\n        return ones_init\n\n    elif name == \"normal\":\n        def normal_init(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            return np.random.normal(loc=0.0, scale=scale, size=shape).astype(np.float32)\n        return normal_init\n\n    elif name == \"glorot_uniform\":\n        def glorot_uniform_init(shape: tuple[int, ...], scale: float = 1.0) -> np.ndarray:\n            if len(shape) < 2:\n                raise ValueError(\"Glorot uniform initialisation requires at least 2-D shape.\")\n            fan_in, fan_out = shape[0], shape[1]\n            limit = scale * np.sqrt(6.0 / (fan_in + fan_out))\n            return np.random.uniform(low=-limit, high=limit, size=shape).astype(np.float32)\n        return glorot_uniform_init\n\n    else:\n        raise ValueError(f\"Unknown initialisation strategy '{name}'.\")\n```"}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef linear_kernel(\n    X: list[list[int | float]] |np.ndarray    Y: list[list[int | float]] |np.ndarray | None = None,\n    c0: int | float = 0,\n) -> list[list[float]] | int:\n   Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function sets Y = X.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix rounded to 4 decimals, or \u20111 if X and Y have\n        incompatible shapes (different number of columns).\n       # --- Convert the inputs to 2-D floating-point NumPy arrays ----------------\n    X_arr = np.asarray(X, dtype=float)\n\n    # Special case: empty X \u21d2 return empty list immediately\n    if X_arr.size == 0:\n        return []\n\n    # Ensure X is 2-D\n    if X_arr.ndim == 1:\n        X_arr = X_arr.reshape(1, -1)\n\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        Y_arr = np.asarray(Y, dtype=float)\n        if Y_arr.ndim == 1:\n            # Handle a single row passed as a 1-D array/list\n            Y_arr = Y_arr.reshape(1, -1)\n\n        # Special case: empty Y \u21d2 return matrix with correct (N, 0) shape\n        if Y_arr.size == 0:\n            return [[] for _ in range(X_arr.shape[0])]\n\n    # --- Validate dimensional compatibility ----------------------------------\n    if X_arr.shape[1] != Y_arr.shape[1]:\n        return -1  # different numbers of columns (features)\n\n    # --- Compute the linear kernel matrix ------------------------------------\n    K = X_arr @ Y_arr.T  # dot products\n    if c0 != 0:\n        K = K + c0        # additive constant\n\n    # --- Round to 4 decimal places and convert to Python lists ---------------\n    K = np.round(K, 4).tolist()\n    return K\n```"}
{"task_id": 337, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nn_interpolate_2D(X: 'np.ndarray', x: list[float | int], y: list[float | int]) -> list:\n    \"\"\"Nearest-Neighbour interpolation on a 2-D regular grid.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Image of shape (rows, cols, channels).\n    x : list[float | int]\n        *x* (column) coordinates.\n    y : list[float | int]\n        *y* (row) coordinates.\n\n    Returns\n    -------\n    list\n        List with shape (k, channels) containing the nearest-neighbour\n        pixel values.\n    \"\"\"\n    # Nothing to do?\n    if not x or not y:\n        return []\n\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length.\")\n\n    # Convert to NumPy arrays\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # 1. Banker's rounding (np.around) and convert to int\n    x_idx = np.around(x_arr).astype(int, copy=False)\n    y_idx = np.around(y_arr).astype(int, copy=False)\n\n    # 2. Clip to the valid image domain\n    n_rows, n_cols = X.shape[:2]\n    x_idx = np.clip(x_idx, 0, n_cols - 1)\n    y_idx = np.clip(y_idx, 0, n_rows - 1)\n\n    # 3. Fetch the pixels for all channels\n    pixels = X[y_idx, x_idx]          # shape -> (k, channels)\n\n    # Return as a regular Python list\n    return pixels.tolist()\n```"}
{"task_id": 338, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(X, y, reg_factor):\n       Closed-form Ridge (L2-regularised) linear regression.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature/design matrix.\n    y : array-like, shape (n_samples,)\n        Target values.\n    reg_factor : float, >= 0\n        Regularisation strength (\u03b1). If 0, reduces to ordinary least squares.\n\n    Returns\n    -------\n    list\n        Weight vector [bias, w1, w2, \u2026] rounded to 4 decimal places.\n       # -- Input sanitising ----------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()\n\n    if X.ndim != 2:\n        raise ValueErrorX must be a 2-D array    if y.ndim != 1:\n        raise ValueErrory must be a 1-D array    if X.shape[0] != y.shape[0]:\n        raise ValueErrorNumber of rows in X must equal length of y    if reg_factor < 0:\n        raise ValueErrorreg_factor must be non-negative    n_samples, n_features = X.shape\n\n    # -- Augment X with bias column ------------------------------------------\n    X_bar = np.hstack((np.ones((n_samples, 1)), X))   # shape: (n, d+1)\n\n    # -- Closed-form solution -------------------------------------------------\n    I = np.eye(n_features + 1)                        # (d+1)\u00d7(d+1)\n    A = X_bar.T @ X_bar + reg_factor * I\n    b = X_bar.T @ y\n\n    # Solve Aw = b; use solve if possible, fall back to pinv for robustness\n    try:\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        w = np.linalg.pinv(A) @ b\n\n    # -- Round and return -----------------------------------------------------\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 339, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Any, Optional\n\ndef most_common_label(y: Iterable[Any]) -> Optional[Any]:\n    \"\"\"Return the most frequent label in *y*.\n\n    If *y* is empty, the function returns ``None``.\n    In case of a tie, the label whose first appearance comes earliest\n    in *y* is returned.\n\n    Args:\n        y: An iterable containing hashable elements (e.g., strings, numbers).\n\n    Returns:\n        The label that occurs most frequently in *y*, or ``None`` if *y* is\n        empty.\n    \"\"\"\n    # Convert to list to allow multiple passes and index operations\n    y = list(y)\n    if not y:                          # 1. Empty input\n        return None\n\n    counts = Counter(y)                # Frequency of each label\n    best_count = -1\n    best_label = None\n\n    # 2. Iterate in the original order so first-appearance priority is preserved\n    for label in y:\n        c = counts[label]\n        if c > best_count:             # Found a higher frequency\n            best_count = c\n            best_label = label\n            # No need to check for equal count: the first occurrence\n            # encountered stays as best_label in a tie situation.\n\n    return best_label\n```"}
{"task_id": 340, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Optimiser time-step counter (starts at 1).\n        m:     First-moment estimate from the previous step.\n        v:     Second-moment estimate from the previous step.\n        lr:    Learning rate \u03b1.\n        beta1: Exponential decay rate for the first moment.\n        beta2: Exponential decay rate for the second moment.\n        eps:   Small constant to avoid division by zero.\n\n    Returns\n    -------\n        (updated_param, new_m, new_v) as python lists,\n        each value rounded to 8 decimal places.\n    \"\"\"\n\n    # Update biased first and second moment estimates\n    m_t = beta1 * m + (1.0 - beta1) * grad\n    v_t = beta2 * v + (1.0 - beta2) * (grad ** 2)\n\n    # Bias-corrected moments\n    m_hat = m_t / (1.0 - beta1 ** t)\n    v_hat = v_t / (1.0 - beta2 ** t)\n\n    # Parameter update\n    param_t = param - lr * m_hat / (np.sqrt(v_hat) + eps)\n\n    # Round to 8 decimal places and convert to native python lists\n    param_out = np.round(param_t, 8).tolist()\n    m_out     = np.round(m_t,     8).tolist()\n    v_out     = np.round(v_t,     8).tolist()\n\n    return (param_out, m_out, v_out)\n```"}
{"task_id": 342, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef manhattan(x: list[float | int], y: list[float | int]) -> float:\n   Compute the Manhattan (L1) distance between two real vectors.\n\n    Args:\n        x: List of numbers representing the first vector.\n        y: List of numbers representing the second vector.\n\n    Returns:\n        The L1 distance rounded to 4 decimal places as a float. If the\n        vectors have different lengths, returns -1.\n       # Length check\n    if len(x) != len(y):\n        return -1\n\n    # Convert to numpy arrays for vectorized operations\n    x_arr = np.asarray(x, dtype=float)\n    y_arr = np.asarray(y, dtype=float)\n\n    # Compute L1 distance and round to 4 decimals\n    distance = np.abs(x_arr - y_arr).sum()\n    return round(float(distance), 4)\n```"}
{"task_id": 343, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                         rewards: list[float] | np.ndarray,\n                         retain_prcnt: float) -> tuple[list[float], list[float]]:\n   Compute the updated mean and variance for CEM    \n    # Convert rewards to a NumPy array for easy indexing / sorting\n    rewards = np.asarray(rewards)\n    N = theta_samples.shape[0]\n    \n    # How many elite samples to keep\n    k = int(retain_prcnt * N)\n    k = max(k, 1)                 # (Guarantee at least one; spec says valid anyway)\n    \n    # Indices of the top-k rewards (largest rewards arebest    elite_idx = np.argsort(rewards)[-k:]          # ascending sort, take last k (highest)\n    \n    # Elite / retained samples\n    elite_samples = theta_samples[elite_idx]\n    \n    # Per-dimension statistics\n    mean_vec = elite_samples.mean(axis=0)\n    var_vec  = elite_samples.var(axis=0)          # population variance (ddof=0)\n    \n    # Round to 4 decimals and convert to Python lists\n    mean_list = [round(x, 4) for x in mean_vec]\n    var_list  = [round(v, 4) for v in var_vec]\n    \n    return mean_list, var_list\n```"}
{"task_id": 344, "completion_id": 0, "completion": "```python\nimport heapq\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass _Node:\n   Internal helper class for a Huffman-tree node    freq: int\n    token: str                     # the token itself; marks an internal node\n    left: OptionalNode = None\n    right: OptionalNode = None\n\n\ndef huffman_encode(tokens: List[str]) -> Dict[str, str]:\n   Generate a prefix-free (Huffman) binary code for `tokens`.\n\n    The returned dictionary maps each distinct token to a string consisting\n    only of the characters '0' and '1'.\n       # ------------------------------------------------------------------ #\n    # 1.  Trivial cases                                                   #\n    # ------------------------------------------------------------------ #\n    if not tokens:\n        return {}\n\n    freq = Counter(tokens)\n\n    # Only one unique symbol \u2192 give it the single-bit code \u201c0\u201d.\n    if len(freq) == 1:\n        sole_token = next(iter(freq))\n        return {sole_token:0    # ------------------------------------------------------------------ #\n    # 2.  Build the Huffman tree using a min-heap                         #\n    # ------------------------------------------------------------------ #\n    heap = []\n    unique_counter = 0                 # guarantees total ordering in heap\n\n    # Insert one leaf per distinct token\n    for token, f in freq.items():\n        node = _Node(f, token)\n        heapq.heappush(heap, (f, token, unique_counter, node))\n        unique_counter += 1\n\n    # Classical bottom-up construction\n    while len(heap) > 1:\n        f1, label1, _, n1 = heapq.heappop(heap)\n        f2, label2, _, n2 = heapq.heappop(heap)\n\n        merged = _Node(f1 + f2, left=n1, right=n2)\n        heapq.heappush(heap, (merged.freq, unique_counter, merged))\n        unique_counter += 1\n\n    # The remaining node is the root of the finished Huffman tree\n    root = heap[0][3]\n\n    # ------------------------------------------------------------------ #\n    # 3.  Traverse the tree to create the codes                           #\n    # ------------------------------------------------------------------ #\n    codes: Dict[str, str] = {}\n\n    def _walk(node: _Node, code: str) -> None:\n        if node.left is None and node.right is None:         # leaf\n            codes[node.token] = code\n            return\n        if node.left:\n            _walk(node.left, code +0        if node.right:\n            _walk(node.right, code +1    _walk(root,)\n\n    return codes\n```"}
{"task_id": 345, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        mu0: np.ndarray,\n        omega0: np.ndarray) -> list[float]:\n    \"\"\"Predict targets for X_test using Bayesian linear regression.\n\n    The function adds an intercept, computes the posterior mean of the\n    weight vector with a Normal prior (\u03bc\u2080, \u03a9\u2080), and returns the predictions\n    for every sample in X_test rounded to four decimals.\n    \"\"\"\n    # ---------- 1. Add intercept ----------\n    X_train_i = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n    X_test_i  = np.c_[np.ones((X_test.shape[0], 1)),  X_test]\n\n    # ---------- 2. Posterior mean ----------\n    # Ensure y is a 1-D vector\n    y_vec = y_train.ravel()\n\n    # Compute the posterior precision and mean\n    XtX = X_train_i.T @ X_train_i\n    A   = XtX + omega0                         # posterior precision\n    b   = X_train_i.T @ y_vec + omega0 @ mu0   # posterior \u201cinformation\u201d vector\n    mu_n = np.linalg.solve(A, b)               # posterior mean (solves A\u00b7\u03bc\u2099 = b)\n\n    # ---------- 3. Predictions ----------\n    y_pred = X_test_i @ mu_n\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 346, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef entropy(y):\n    \"\"\"Compute the base-2 Shannon entropy of a vector of integer labels.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        One-dimensional iterable containing non-negative integer class labels.\n\n    Returns\n    -------\n    float\n        The entropy value rounded to 4 decimal places.\n    \"\"\"\n    # Convert to a 1-D NumPy array for convenience\n    y = np.asarray(y).ravel()\n    if y.size == 0:\n        raise ValueError(\"Input 'y' must contain at least one label.\")\n\n    # Count occurrences of each unique label\n    _, counts = np.unique(y, return_counts=True)\n\n    # Empirical probabilities\n    probs = counts / counts.sum()\n\n    # Shannon entropy (base-2). Guard against log2(0) by multiplying only\n    # probabilities greater than 0 \u2013 though np.unique guarantees counts>0.\n    H = -np.sum(probs * np.log2(probs))\n\n    return round(float(H), 4)\n```"}
{"task_id": 347, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef wgan_gp_loss(\n        Y_fake: np.ndarray,\n        module: str,\n        Y_real: np.ndarray | None = None,\n        gradInterp: np.ndarray | None = None,\n        lambda_: float = 10.0) -> float:\n    \"\"\"Compute the WGAN-GP loss for the critic or the generator.\n\n    Args:\n        Y_fake (np.ndarray): Critic outputs on fake samples (shape (n, \u2026)).\n        module (str): \"C\" for critic loss, \"G\" for generator loss.\n        Y_real (np.ndarray | None, optional): Critic outputs on real samples.\n            Required when *module* == \"C\".\n        gradInterp (np.ndarray | None, optional): Gradients of the critic\n            outputs on the interpolated samples wrt. the samples themselves.\n            Required when *module* == \"C\".\n        lambda_ (float, optional): Gradient-penalty coefficient \u03bb. Defaults to 10.\n\n    Returns\n    -------\n        float: Requested loss rounded to four decimals.\n\n    Raises\n    ------\n        ValueError: If the inputs are inconsistent with *module*.\n    \"\"\"\n\n    module = module.upper().strip()\n    if module not in {\"C\", \"G\"}:\n        raise ValueError('module must be either \"C\" or \"G\".')\n\n    # Ensure numeric numpy arrays\n    Y_fake = np.asarray(Y_fake, dtype=np.float64).reshape(-1)\n\n    if module == \"G\":\n        # L_G = -E[D(x_fake)]\n        loss = -np.mean(Y_fake)\n        return round(float(loss), 4)\n\n    # --- Critic loss ---------------------------------------------------------\n    # Check required inputs\n    if Y_real is None or gradInterp is None:\n        raise ValueError(\"Y_real and gradInterp must be provided when module=='C'.\")\n\n    Y_real = np.asarray(Y_real, dtype=np.float64).reshape(-1)\n    gradInterp = np.asarray(gradInterp, dtype=np.float64)\n\n    # First two terms: Wasserstein distance estimate\n    loss_wasserstein = np.mean(Y_fake) - np.mean(Y_real)\n\n    # Gradient penalty\n    # Flatten all but batch dimension to compute L2-norm per sample\n    grad_flat = gradInterp.reshape(gradInterp.shape[0], -1)\n    grad_norm = np.linalg.norm(grad_flat, axis=1)          # \u2016\u2207D\u2016\u2082 per example\n    gp = np.mean((grad_norm - 1.0) ** 2)                   # E[(\u2016\u2207D\u2016\u2082 \u2212 1)\u00b2]\n\n    loss = loss_wasserstein + lambda_ * gp\n    return round(float(loss), 4)\n```"}
{"task_id": 348, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric_positive_definite(X: np.ndarray) -> bool:\n   Check whether a real matrix is symmetric and positive-definite.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A two-dimensional NumPy array representing the matrix to test.\n\n    Returns\n    -------\n    bool\n        True if *X* is symmetric and positive-definite, otherwise False.\n       # 1. Must be a square, real-valued, 2-D array\n    if X.ndim != 2 or X.shape[0] != X.shape[1]:\n        return False\n    if not np.isrealobj(X):\n        return False\n\n    # 2. Symmetry test (within numerical tolerance)\n    if not np.allclose(X, X.T, atol=1e-8, rtol=0.0):\n        return False\n\n    # 3. Positive-definiteness test via eigenvalues\n    try:\n        eigvals = np.linalg.eigvalsh(X)  # for symmetric/hermitian matrices\n    except np.linalg.LinAlgError:\n        return False  # numerical failure \u21d2 treat as not PD\n\n    return np.all(eigvals > 1e-8)\n```"}
{"task_id": 350, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef polynomial_ridge_regression(X: list[float | int],\n                                y: list[float | int],\n                                degree: int,\n                                reg_factor: float) -> list[float]:\n   Fit a polynomial ridge regression model and return its coefficients.\n\n    Parameters\n    ----------\n    X : list of float or int\n        One-dimensional feature vector.\n    y : list of float or int\n        Target values.\n    degree : int\n        Highest polynomial degree (>=0).\n    reg_factor : float\n        Non-negative ridge regularisation factor (\u03bb).\n\n    Returns\n    -------\n    list[float]\n        The (degree+1) learned coefficients, rounded to 4 decimal places.\n       # Convert to numpy arrays (column vector for y)\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    # Construct the design / Vandermonde matrix\n    # Each row: [1, x, x^2, ..., x^degree]\n    Phi = np.vander(X, N=degree + 1, increasing=True)\n\n    # Closed-form ridge solution:  w = (Phi\u1d40 Phi + \u03bbI)\u207b\u00b9 Phi\u1d40 y\n    I = np.eye(degree + 1)\n    A = Phi.T @ Phi + reg_factor * I\n    b = Phi.T @ y\n\n    # Solve the linear system instead of explicitly inverting A\n    w = np.linalg.solve(A, b)\n\n    # Round to 4 decimal places and return as a list\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 351, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef max_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: int) -> list:\n    \"\"\"Perform 2-D max pooling over a 4-D minibatch of images.\n\n    Args:\n        X: A 4-D NumPy array with shape (N, C, H, W).\n        pool_shape: (p_h, p_w) \u2013 height and width of the pooling window.\n        stride: Number of pixels the window moves vertically and horizontally.\n\n    Returns:\n        Python (nested) list with the pooled result, shape (N, C, out_h, out_w).\n    \"\"\"\n    # Unpack shapes / hyper-parameters\n    p_h, p_w = pool_shape\n    N, C, H, W = X.shape\n\n    # Output spatial dimensions (guaranteed integers)\n    out_h = (H - p_h) // stride + 1\n    out_w = (W - p_w) // stride + 1\n\n    # ------------------------------------------------------------\n    # Build a strided *view* onto X whose last two axes are the\n    # p_h \u00d7 p_w windows and whose preceding two axes enumerate all\n    # window positions (out_h, out_w).\n    # ------------------------------------------------------------\n    sN, sC, sH, sW = X.strides  # strides in bytes\n    window_shape   = (N, C, out_h, out_w, p_h, p_w)\n    window_strides = (\n        sN,                      # move to next image\n        sC,                      #   \"   next channel\n        sH * stride,             # move down by `stride` pixels\n        sW * stride,             # move right by `stride` pixels\n        sH,                      # inside a window: down 1 pixel\n        sW                       # inside a window: right 1 pixel\n    )\n\n    windows = as_strided(\n        X, shape=window_shape, strides=window_strides, writeable=False\n    )\n\n    # Max over the window\u2019s height & width => (N, C, out_h, out_w)\n    pooled = windows.max(axis=(-1, -2))\n\n    # Return as a nested Python list\n    return pooled.tolist()\n```"}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n   Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, generator, \u2026)\n                whose elements are hashable.\n\n    Returns:\n        float: Entropy rounded to five decimal places.\n       # Materialise the (possibly one-pass) iterable\n    labels_list = list(labels)\n\n    # Special case: empty input\n    n = len(labels_list)\n    if n == 0:\n        return 0.0\n\n    # Count occurrences of each label\n    counts = Counter(labels_list)\n\n    # Special case: only one unique label  -> entropy 0\n    if len(counts) == 1:\n        return 0.0\n\n    # Shannon entropy\n    h = 0.0\n    for c in counts.values():\n        p = c / n\n        # Skip p == 0; mathematically it contributes 0\n        h -= p * math.log2(p)\n\n    return round(h, 5)\n```"}
{"task_id": 354, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _as_4tuple(x):\n    \"\"\"Return a padding specification (top, bottom, left, right).\"\"\"\n    if isinstance(x, str) and x.lower() == \"same\":       # will be dealt with later\n        return x\n    if isinstance(x, int):\n        return (x, x, x, x)\n    if len(x) == 2:                                      # (pr, pc)\n        pr, pc = x\n        return (pr, pr, pc, pc)\n    if len(x) == 4:                                      # already `(pr1, pr2, pc1, pc2)`\n        return tuple(x)\n    raise ValueError(\"Invalid padding specification\")\n\n\ndef conv2D(X: \"np.ndarray\",\n           W: \"np.ndarray\",\n           stride: int,\n           pad,\n           dilation: int = 0):\n    \"\"\"Performs a 2-D (cross-correlation) convolution through the classic\n    *im2col + GEMM* strategy and returns the result as a Python list.\"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Basic shapes & derived constants                                #\n    # ------------------------------------------------------------------ #\n    n_ex, in_r, in_c, in_ch     = X.shape\n    k_r,  k_c,  _,     out_ch   = W.shape\n    dil                          = dilation if dilation >= 0 else 0\n    kr_eff                       = k_r + (k_r - 1) * dil           # effective size\n    kc_eff                       = k_c + (k_c - 1) * dil\n    s                            = stride\n\n    # ------------------------------------------------------------------ #\n    # 1. Work out the padding to apply                                   #\n    # ------------------------------------------------------------------ #\n    pad_spec = _as_4tuple(pad)\n\n    if pad_spec == \"same\":                             # smallest symmetric padding\n        # general formula for \"same\" (works for any stride)\n        out_r = int(np.ceil(in_r / s))\n        out_c = int(np.ceil(in_c / s))\n        pad_needed_r = max((out_r - 1) * s + kr_eff - in_r, 0)\n        pad_needed_c = max((out_c - 1) * s + kc_eff - in_c, 0)\n        pad_top    = pad_needed_r // 2\n        pad_bottom = pad_needed_r - pad_top\n        pad_left   = pad_needed_c // 2\n        pad_right  = pad_needed_c - pad_left\n    else:\n        pad_top, pad_bottom, pad_left, pad_right = pad_spec\n        # corresponding spatial output size\n        out_r = (in_r + pad_top + pad_bottom - kr_eff) // s + 1\n        out_c = (in_c + pad_left + pad_right   - kc_eff) // s + 1\n\n    # ------------------------------------------------------------------ #\n    # 2. Pad the input                                                   #\n    # ------------------------------------------------------------------ #\n    X_pad = np.pad(\n        X,\n        pad_width=((0, 0),\n                   (pad_top, pad_bottom),\n                   (pad_left, pad_right),\n                   (0, 0)),\n        mode=\"constant\",\n        constant_values=0\n    )\n    n_ex, R, C, _ = X_pad.shape         # updated spatial size\n\n    # ------------------------------------------------------------------ #\n    # 3. im2col with pure NumPy / as_strided                             #\n    # ------------------------------------------------------------------ #\n    # desired window view: (n_ex, out_r, out_c, k_r, k_c, in_ch)\n    # strides:\n    s_n, s_r, s_c, s_ch = X_pad.strides\n    stride_row_out      = s_r * s                     # move one output step\n    stride_col_out      = s_c * s\n    stride_row_k        = s_r * (dil + 1)             # inside the window\n    stride_col_k        = s_c * (dil + 1)\n\n    new_shape   = (n_ex, out_r, out_c, k_r, k_c, in_ch)\n    new_strides = (s_n,\n                   stride_row_out,\n                   stride_col_out,\n                   stride_row_k,\n                   stride_col_k,\n                   s_ch)\n\n    patches = np.lib.stride_tricks.as_strided(\n        X_pad,\n        shape=new_shape,\n        strides=new_strides,\n        writeable=False\n    )\n\n    # flatten each patch -> (n_ex, out_r*out_c, k_r*k_c*in_ch)\n    X_cols = patches.reshape(n_ex, out_r * out_c, -1)\n\n    # ------------------------------------------------------------------ #\n    # 4. Prepare the kernels & do GEMM                                   #\n    # ------------------------------------------------------------------ #\n    W_cols = W.reshape(-1, out_ch)                     # (k_r*k_c*in_ch, out_ch)\n    out = X_cols @ W_cols                              # (n_ex, out_r*out_c, out_ch)\n\n    # ------------------------------------------------------------------ #\n    # 5. Re-shape back to NHWC and return as list                        #\n    # ------------------------------------------------------------------ #\n    out = out.reshape(n_ex, out_r, out_c, out_ch)\n    return out.tolist()\n```"}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef pca_transform(\n    data: np.ndarray,\n    n_components: int,\n    solver: str = \"svd\",\n) -> tuple[list[list[float]], list[float]]:\n    \"\"\"Principal Component Analysis with deterministic sign fixing.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (samples \u00d7 features).\n    n_components : int\n        Number of principal directions to keep (\u2265 1).\n    solver : {\"svd\", \"eigen\"}, default=\"svd\"\n        Algorithm used to obtain the principal directions.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 Scores (projected samples) \u2013 list of lists, rounded to 4 decimals.  \n        \u2022 Explained-variance ratios for the kept components \u2013 list, rounded.\n    \"\"\"\n    # --------------- basic checks -------------------------------------------------\n    if n_components < 1:\n        raise ValueError(\"n_components must be at least 1\")\n    if solver not in {\"svd\", \"eigen\"}:\n        raise ValueError('solver must be either \"svd\" or \"eigen\"')\n\n    X = np.asarray(data, dtype=float)\n    n_samples, n_features = X.shape\n\n    # --------------- 1. mean-centre ------------------------------------------------\n    mean = X.mean(axis=0)\n    Xc = X - mean  # centred data\n\n    # --------------- 2. obtain directions & eigenvalues ---------------------------\n    if solver == \"svd\":\n        # Economy SVD\n        U, S, VT = np.linalg.svd(Xc, full_matrices=False)\n        directions = VT.T                                   # columns = PCs\n        eigen_vals = (S**2) / (n_samples - 1)               # sample covariance\n    else:  # eigen\n        cov = np.cov(Xc, rowvar=False, ddof=1)\n        eig_vals, eig_vecs = np.linalg.eigh(cov)\n        order = eig_vals.argsort()[::-1]                    # descending\n        eigen_vals = eig_vals[order]\n        directions = eig_vecs[:, order]\n\n    # --------------- 3. sort & trim ------------------------------------------------\n    k = min(n_components, directions.shape[1])\n    directions = directions[:, :k]\n    eigen_vals = eigen_vals[:k]\n    total_variance = eigen_vals.sum()\n\n    # --------------- 4. deterministic sign fix ------------------------------------\n    scores = Xc @ directions                                # project\n\n    for j in range(k):                                      # each component\n        comp = directions[:, j]\n        # first non-zero loading\n        for val in comp:\n            if abs(val) > 1e-12:\n                if val < 0:                                # flip if negative\n                    directions[:, j] *= -1\n                    scores[:, j] *= -1\n                break\n\n    # --------------- 5. final rounding --------------------------------------------\n    scores_rounded = np.round(scores, 4).tolist()\n    expl_var_ratio = np.round(eigen_vals / total_variance, 4).tolist()\n\n    return scores_rounded, expl_var_ratio\n```"}
{"task_id": 356, "completion_id": 0, "completion": "```python\nclass Leaf:\n   Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n       def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf:Leaf classifier: bool):\n   Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` whose `value` attribute is either a sequence of class\n              probabilities (for classification) or a scalar (for regression).\n        classifier: If True, interpret the leaf as classification; otherwise\n                    as regression.\n\n    Returns\n    -------\n        int | float:  Class index (int) for classification, or the stored\n                      scalar for regression.\n       # Classification: pick the index of the largest probability\n    if classifier:\n        probs = leaf.value\n\n        # Try the fast path when `value` is a NumPy array.\n        try:\n            import numpy as np\n            if isinstance(probs, np.ndarray):\n                # numpy.argmax already resolves ties toward the smaller index.\n                return int(probs.argmax())\n        except ModuleNotFoundError:\n            # NumPy not available \u2013 fall back to pure Python below.\n            pass\n\n        # Generic Python sequence: find first occurrence of the maximum.\n        max_idx = 0\n        max_val = probs[0]\n        for idx, val in enumerate(probs):\n            if val > max_val:          # strict '>' keeps first max on ties\n                max_val = val\n                max_idx = idx\n        return int(max_idx)\n\n    # Regression: just return the stored scalar.\n    return leaf.value\n```"}
{"task_id": 357, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n   Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    K : int\n        Number of clusters.\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Centroids (sorted, rounded to 4 decimals).\n       # Ensure a 2-D float array\n    data = np.asarray(data, dtype=float)\n    n_samples, n_feats = data.shape\n\n    # Special-case K == 1\n    if K == 1:\n        centroid = np.round(data.mean(axis=0), 4)\n        return [centroid.tolist()]\n\n    # 1. Deterministic initial centroids: first K samples\n    centroids = data[:K].copy()\n\n    for _ in range(max_iters):\n        # 2a. Assign each sample to closest centroid (Euclidean)\n        #     distances shape: (n_samples, K)\n        distances = np.linalg.norm(data[:, None, :] - centroids[None, :, :], axis=2)\n        labels = distances.argmin(axis=1)          # (n_samples,)\n\n        # 2b. Recompute centroids\n        new_centroids = centroids.copy()\n        for k in range(K):\n            members = data[labels == k]\n            if members.size:                       # keep unchanged if empty\n                new_centroids[k] = members.mean(axis=0)\n\n        # 2c. Stopping criterion (movement \u2264 1e-6)\n        if np.allclose(new_centroids, centroids, atol=1e-6, rtol=0):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # 3. Sort lexicographically by feature 0, then 1, \u2026\n    sort_keys = [centroids[:, i] for i in reversed(range(n_feats))]\n    order = np.lexsort(sort_keys)\n    centroids = centroids[order]\n\n    # 4. Round to 4 decimals and return as list of lists\n    return np.round(centroids, 4).tolist()\n```\n\nThe implementation follows the specification exactly:\n\n1. Initial centroids are the first `K` rows (deterministic).  \n2. Each iteration:  \n   \u2022 Assign samples to the nearest centroid (Euclidean distance).  \n   \u2022 Re-compute centroids as the mean of their assigned samples, keeping any empty-cluster centroid unchanged.  \n   \u2022 Stop early when every centroid moves less than 1 \u00d7 10\u207b\u2076.  \n3. After convergence (or `max_iters`), centroids are sorted lexicographically, every coordinate is rounded to four decimals, and the result is returned as a plain Python list of lists."}
{"task_id": 358, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy(y_true, y_pred):\n    \"\"\"Compute classification accuracy between two equal-length sequences.\n\n    Args:\n        y_true (list | np.ndarray): Ground-truth class labels.\n        y_pred (list | np.ndarray): Predicted class labels.\n\n    Returns:\n        float | int: Accuracy rounded to 4 decimals, or \u20111 if lengths differ.\n    \"\"\"\n    # 1. Check length mismatch\n    if len(y_true) != len(y_pred):\n        return -1\n\n    # 2. Handle empty input (avoid division-by-zero)\n    if len(y_true) == 0:\n        return 0.0\n\n    # 3. Convert to numpy arrays for element-wise comparison\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # 4. Compute proportion of exactly correct predictions\n    correct = np.sum(y_true == y_pred)\n    acc = correct / len(y_true)\n\n    # 5. Return accuracy rounded to four decimal places\n    return round(float(acc), 4)\n```"}
{"task_id": 359, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy(y, y_pred, derivative=False):\n   Cross-entropy loss (sum) and its gradient.\n\n    Parameters\n    ----------\n    y : numpy.ndarray\n        One-hot encoded true labels (n_samples, n_classes).\n    y_pred : numpy.ndarray\n        Predicted class probabilities (same shape as y).\n    derivative : bool, default False\n        If True, return \u2202L/\u2202z (y_pred \u2212 y); else return the loss.\n\n    Returns\n    -------\n    float | list[list[float]]\n        Scalar loss or gradient matrix, each value rounded to 4 decimals.\n       # ensure numpy arrays\n    y = np.asarray(y, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    eps = np.finfo(float).eps       # tiny constant to avoid log(0)\n\n    if derivative:                  # gradient branch\n        grad = y_pred - y\n        return np.round(grad, 4).tolist()\n\n    # loss branch\n    loss = -np.sum(y * np.log(y_pred + eps))\n    return round(loss, 4)\n```"}
{"task_id": 360, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dct(frame: np.ndarray | list[float], orthonormal: bool = True) -> list[float]:\n   Compute the 1-D DCT-II of *frame*.\n\n    Parameters\n    ----------\n    frame : 1-D array-like\n        Real-valued input samples x[0 \u2026 N-1].\n    orthonormal : bool, default=True\n        If True, apply the usual DCT-II orthonormal scaling.\n\n    Returns\n    -------\n    list[float]\n        The N DCT-II coefficients rounded to four decimals.\n       # --- prepare input -------------------------------------------------------\n    x = np.asarray(frame, dtype=float).ravel()            # ensure 1-D float array\n    if x.ndim != 1:\n        raise ValueError`frame` must be one-dimensional    N = x.size\n    if N == 0:\n        return []\n\n    # --- naive DCT-II --------------------------------------------------------\n    n = np.arange(N)\n    coeffs = np.empty(N, dtype=float)\n    for k in range(N):\n        theta = (np.pi * k * (2 * n + 1)) / (2 * N)       # vector of angles\n        coeffs[k] = 2.0 * np.sum(x * np.cos(theta))       # 2 * \u03a3 x_n cos(\u2026)\n    \n    # --- optional orthonormal scaling ---------------------------------------\n    if orthonormal:\n        # scale[0] = 2*sqrt(1/(4N));  scale[k\u22651] = 2*sqrt(1/(2N))\n        scale = np.full(N, 2.0 * np.sqrt(1.0 / (2 * N)), dtype=float)\n        scale[0] = 2.0 * np.sqrt(1.0 / (4 * N))\n        coeffs *= scale\n\n    # --- rounding & output ---------------------------------------------------\n    return np.round(coeffs, 4).tolist()\n```"}
{"task_id": 362, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef row_stochastic_matrix(\n    data: list[list[int | float]] |np.ndarray) -> list[list[float]] | int:\n   Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation. If a row has a sum of\n    0 the function returns -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The row-normalised matrix (rounded to 4 decimals) or -1 when\n        normalisation is impossible.\n       # 1. Convert to a float NumPy array\n    arr = np.asarray(data, dtype=float)\n\n    # Sanity-check: must be 2-D\n    if arr.ndim != 2:\n        raise ValueErrorInput must be a 2-dimensional structure    # 2. Compute row sums\n    row_sums = arr.sum(axis=1)\n\n    # 3. Check for zero rows\n    if np.any(row_sums == 0):\n        return -1\n\n    # 4. Normalise rows (broadcasting divides each row by its own sum)\n    stochastic = arr / row_sums[:, np.newaxis]\n\n    # 5. Round to 4 decimals and convert back to native Python list\n    return np.round(stochastic, 4).tolist()\n```"}
{"task_id": 363, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n   Tiny environment that only stores two spaces    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n   Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its immediate\n    children are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite (`Tuple` / `Dict`).\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` with the continuity of each space.\n       def _composite_is_continuous(space: Space) -> bool:\n       Return True if a composite space (Tuple/Dict) is fully Box        if isinstance(space, Tuple):\n            return all(isinstance(s, Box) for s in space.spaces)\n        if isinstance(space, Dict):\n            return all(isinstance(s, Box) for s in space.spaces.values())\n        # Fallback \u2013 not a recognised composite space.\n        return False\n\n    # ----- Action space -----\n    if tuple_action:\n        # For composite spaces check all members,\n        # otherwise fall back to the simple check.\n        if isinstance(env.action_space, Box):\n            cont_action = True\n        else:\n            cont_action = _composite_is_continuous(env.action_space)\n    else:\n        cont_action = isinstance(env.action_space, Box)\n\n    # ----- Observation space -----\n    if tuple_obs:\n        if isinstance(env.observation_space, Box):\n            cont_obs = True\n        else:\n            cont_obs = _composite_is_continuous(env.observation_space)\n    else:\n        cont_obs = isinstance(env.observation_space, Box)\n\n    return cont_action, cont_obs\n```"}
{"task_id": 364, "completion_id": 0, "completion": "```python\ndef sign(x: int | float) -> int:\n   Return the sign of *x*.\n\n    The function returns:\n        1  if x > 0\n        0  if x == 0\n       -1  if x < 0\n\n    Args:\n        x: A real number (int or float).\n\n    Returns:\n        An integer: -1, 0, or 1.\n       if x > 0:\n        return 1\n    if x < 0:\n        return -1\n    return 0\n```"}
{"task_id": 366, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l2_distance(X: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute the pair-wise squared Euclidean distance matrix.\n\n    Args:\n        X (np.ndarray): A 2-D array with shape (n_samples, n_features).\n\n    Returns:\n        list[list[float]]: The n\u00d7n squared distance matrix.  The element (i, j)\n            equals the squared \u2113\u2082 distance between samples i and j, and all\n            diagonal entries are exactly 0.0.\n    \"\"\"\n    # Ensure 2-D and use floating-point arithmetic\n    X = np.atleast_2d(X).astype(float, copy=False)\n    n_samples = X.shape[0]\n\n    # Trivial case: only one sample\n    if n_samples == 1:\n        return [[0.0]]\n\n    # Squared norms of each row \u2016x_i\u2016\u00b2, shape (n_samples, 1)\n    sq_norms = np.sum(X * X, axis=1, keepdims=True)\n\n    # Pair-wise squared distances using the identity\n    # \u2016x_i \u2212 x_j\u2016\u00b2 = \u2016x_i\u2016\u00b2 + \u2016x_j\u2016\u00b2 \u2212 2 x_i\u00b7x_j\n    dists = sq_norms + sq_norms.T - 2.0 * X @ X.T\n\n    # Numerical guard: clamp very small negative values to 0\n    dists = np.maximum(dists, 0.0)\n\n    # Ensure exact zeros on the diagonal\n    np.fill_diagonal(dists, 0.0)\n\n    return dists.tolist()\n```"}
{"task_id": 367, "completion_id": 0, "completion": "```python\nimport ast\nimport re\nfrom typing import Any, Dict, Optional, Type\n\nclass KernelBase:\n   Minimal base class for all kernels.  Do NOT modify this class    def __init__(self, **kwargs: Any) -> None:\n        self.params: Dict[str, Any] = kwargs\n\n    def set_params(self, state: Dict[str, Any]):\n       (Re)sets parameters from a previously stored summary dict        hp = state.gethyperparameters {})\n        self.params.update({k: v for k, v in hp.items() if k !=id        return self\n\n    def summary(self) -> Dict[str, Any]:\n       Dictionary description that can be fed back to *initialize_kernel*        returnhyperparametersid self.__class__.__name__, **self.params}}\n\n\nclass LinearKernel(KernelBase):\n    def __init__(self):\n        super().__init__()\n\n\nclass PolynomialKernel(KernelBase):\n    def __init__(self, degree: int = 3, coef0: float = 1.0):\n        super().__init__(degree=degree, coef0=coef0)\n\n\nclass RBFKernel(KernelBase):\n    def __init__(self, gamma: float = 1.0):\n        super().__init__(gamma=gamma)\n\n\n# --------------------------------------------------------------------------- #\n#                             initialize_kernel                               #\n# --------------------------------------------------------------------------- #\ndef initialize_kernel(param: Optional[Any] = None) -> KernelBase:\n   Factory that converts various representations into a kernel instance    # ------------------------- helper / common data ------------------------- #\n    _name2class: Dict[str, Type[KernelBase]] = {\n       linear LinearKernel,\n       linearkernel LinearKernel,\n       polynomial PolynomialKernel,\n       polynomialkernel PolynomialKernel,\n       poly PolynomialKernel,\n       rbf RBFKernel,\n       rbfkernel RBFKernel,\n    }\n\n    def _cls_from_name(name: str) -> Type[KernelBase]:\n       Return kernel class from textual identifier or raise        key = name.lower()\n        if key not in _name2class:\n            raise NotImplementedError(fUnknown kernel '{name}'.        return _name2class[key]\n\n    def _parse_value(text: str) -> Any:\n       Try to convert a textual literal to Python object, fallback to str        try:\n            return ast.literal_eval(text)\n        except (ValueError, SyntaxError):\n            return text  # leave as raw string if it cannot be evaluated\n\n    # ---------------------------- main dispatch ----------------------------- #\n    # 1. None  -> default LinearKernel\n    if param is None:\n        return LinearKernel()\n\n    # 2. Already a kernel instance -> return unchanged\n    if isinstance(param, KernelBase):\n        return param\n\n    # 3. String description\n    if isinstance(param, str):\n        # pattern:  kernelName ( arg1=val1 , arg2=val2 )\n        match = re.match(r^\\s*([A-Za-z_]\\w*)\\s*(?:\\((.*)\\))?\\s*$ param)\n        if not match:\n            raise ValueError(fCannot parse kernel specification '{param}'.        name, arg_block = match.groups()\n        cls = _cls_from_name(name)\n\n        # no arguments given\n        if not arg_block:\n            return cls()\n\n        # split arguments safely on commas (no nested parentheses expected)\n        kwargs: Dict[str, Any] = {}\n        for item in re.split(rs*,\\s arg_block.strip()):\n            if not item:\n                continue\n            if not in item:\n                raise ValueError(\n                    fExpected key=value syntax inside parentheses: '{item}'                )\n            key, val_txt = map(str.strip, item.split 1))\n            kwargs[key] = _parse_value(val_txt)\n        try:\n            return cls(**kwargs)\n        except TypeError:\n            # fall back to default construction then patch params\n            inst = cls()\n            inst.params.update(kwargs)\n            return inst\n\n    # 4. Dictionary representation (as produced by KernelBase.summary())\n    if isinstance(param, dict):\n        ifhyperparameters not in param:\n            raise ValueErrorDictionary must contain a 'hyperparameters' key        hp: Dict[str, Any] = paramhyperparameters        ifid not in hp:\n            raise ValueErrorMissing 'id' entry inside 'hyperparameters'.        cls = _cls_from_name(hpid        ctor_kwargs = {k: v for k, v in hp.items() if k !=id        try:\n            return cls(**ctor_kwargs)\n        except TypeError:\n            inst = cls()\n            inst.params.update(ctor_kwargs)\n            return inst\n\n    # Any other input type is unsupported\n    raise TypeErrorUnsupported type for kernel initialization```"}
{"task_id": 368, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minkowski(x: np.ndarray | list, y: np.ndarray | list, p: float) -> float:\n   Compute the Minkowski\u2013p distance between two vectors.\n\n    Args:\n        x (np.ndarray | list): First one-dimensional vector.\n        y (np.ndarray | list): Second one-dimensional vector of the same length.\n        p (float): Order of the norm (must be > 0).\n\n    Returns:\n        float: Minkowski distance rounded to 4 decimal places, or \u20111 if the\n               inputs are invalid.\n       # ---------- Validation ----------\n    if not isinstance(p, (int, float)) or p <= 0:\n        return -1\n\n    try:\n        xa = np.asarray(x, dtype=float)\n        ya = np.asarray(y, dtype=float)\n    except Exception:          # non-convertible to numeric array\n        return -1\n\n    # Must be 1-D and have identical shape\n    if xa.ndim != 1 or ya.ndim != 1 or xa.shape != ya.shape:\n        return -1\n\n    # ---------- Minkowski distance ----------\n    diff = np.abs(xa - ya)          # |x_i \u2212 y_i|\n    powered = diff ** p             # |x_i \u2212 y_i|^p\n    distance = (powered.sum()) ** (1 / p)\n\n    return round(float(distance), 4)\n```"}
{"task_id": 369, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gradient_boosting_predict(\n    updates: list[np.ndarray], learning_rate: float, regression: bool\n) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is ``(n_samples,)``;\n        for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers (predicted class labels).\n    \"\"\"\n    if len(updates) == 0:\n        raise ValueError(\"`updates` must contain at least one tree output.\")\n\n    # ------------------------------------------------------------------\n    # 1. Aggregate all tree updates (running prediction starts at zero).\n    #    running_pred = 0  -  learning_rate * \u03a3 updates\n    # ------------------------------------------------------------------\n    # Summation is fully vectorised; no per-sample loops are used.\n    total_update = np.sum(updates, axis=0)\n    running_pred = -learning_rate * total_update\n\n    # ------------------------------------------------------------------\n    # 2. Regression \u2013 just return the aggregated value (rounded).\n    # ------------------------------------------------------------------\n    if regression:\n        return np.round(running_pred.astype(float), 4)\n\n    # ------------------------------------------------------------------\n    # 3. Classification \u2013 convert logits to probabilities via soft-max,\n    #    then take the arg-max to obtain predicted class labels.\n    # ------------------------------------------------------------------\n    # Soft-max in a numerically stable way.\n    logits = running_pred\n    max_per_row = np.max(logits, axis=1, keepdims=True)\n    exps = np.exp(logits - max_per_row)\n    probs = exps / np.sum(exps, axis=1, keepdims=True)\n\n    return np.argmax(probs, axis=1)\n```"}
{"task_id": 370, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multivariate_gaussian(X: np.ndarray, mu: np.ndarray, cov: np.ndarray) -> list[float]:\n       Compute the multivariate normal (Gaussian) probability density values for\n    each sample in X under N(mu, cov).\n\n    Parameters\n    ----------\n    X   : (m, n) array_like\n          Each row is an n-dimensional data point.\n    mu  : (n,) array_like\n          Mean vector of the distribution.\n    cov : (n, n) array_like\n          Positive-definite covariance matrix.\n\n    Returns\n    -------\n    list[float]\n          PDF value for every row in X, rounded to 4 decimal places.\n       # Make sure inputs are arrays with correct shapes\n    X   = np.atleast_2d(X)\n    mu  = np.asarray(mu).ravel()\n    cov = np.asarray(cov)\n\n    n = mu.size                     # dimensionality\n    diff = X - mu                   # (m, n)\n\n    # Pre-compute inverse and determinant of covariance\n    inv_cov = np.linalg.inv(cov)\n    det_cov = np.linalg.det(cov)\n\n    # Normalization constant\n    norm_const = np.sqrt((2 * np.pi) ** n * det_cov)\n\n    # Quadratic form for each sample: (x-\u03bc)^T \u03a3^{-1} (x-\u03bc)\n    exponent = -0.5 * np.sum(diff @ inv_cov * diff, axis=1)\n\n    pdf_vals = np.exp(exponent) / norm_const\n\n    # Round to 4 decimal places and return as Python list\n    return [round(v, 4) for v in pdf_vals.tolist()]\n```"}
{"task_id": 371, "completion_id": 0, "completion": "```python\nimport math\nimport numbers\nimport numpy as np\n\n\ndef _compute_padding(l_in: int,\n                     k_eff: int,\n                     stride: int,\n                     pad):\n       Helper that turns *pad* (int / tuple /same into a concrete\n    (p_left, p_right) pair and also returns the output length that\n    will result from the convolution.\n       #  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  explicit padding  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if isinstance(pad, tuple):\n        if len(pad) != 2 or not all(isinstance(x, numbers.Integral) for x in pad):\n            raise ValueError('pad tuple must have two integer entries')\n        p_left, p_right = pad\n    elif isinstance(pad, numbers.Integral):\n        p_left = p_right = int(pad)\n    #  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  TensorFlow-style \u201cSAME\u201d padding  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    elif isinstance(pad, str) and pad.lower() == 'same':\n        # out_len = ceil(l_in / stride)\n        out_len = math.ceil(l_in / stride)\n        total_pad = max(0, (out_len - 1) * stride + k_eff - l_in)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n        return p_left, p_right, out_len\n    else:\n        raise ValueError('pad must be int, 2-tuple orsame')\n\n    # With explicit padding the output length follows the usual formula\n    out_len = (l_in + p_left + p_right - k_eff) // stride + 1\n    if out_len <= 0:\n        raise ValueError('Chosen parameters lead to empty output')\n    return p_left, p_right, out_len\n\n\ndef conv1D(X: np.ndarray,\n           W: np.ndarray,\n           stride: int,\n           pad,\n           dilation: int = 0) -> list:\n   Perform a 1-D cross-correlation between *X* and *W*.\n\n    Args\n    ----\n    X : ndarray, shape (n_examples, signal_length, in_channels)\n    W : ndarray, shape (kernel_width, in_channels, out_channels)\n    stride : positive integer\n    pad : int, (int,int) tuple or the stringsame    dilation : non-negative integer, the number of zeros inserted\n               between neighbouring kernel elements (0 \u21d2 standard)\n\n    Returns\n    -------\n    Nested Python list with shape\n    (n_examples, output_length, out_channels)\n       # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 sanity checks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if X.ndim != 3:\n        raise ValueError('X must have shape (batch, length, in_channels)')\n    if W.ndim != 3:\n        raise ValueError('W must have shape (kernel_width, in_channels, out_channels)')\n    if X.shape[2] != W.shape[1]:\n        raise ValueError('Number of input channels mismatch between X and W')\n    if stride <= 0:\n        raise ValueError('stride must be a positive integer')\n    if dilation < 0:\n        raise ValueError('dilation must be non-negative')\n\n    N, L_in, C_in = X.shape\n    K, C_in2, C_out = W.shape\n    del C_in2  # already checked equal to C_in\n\n    # Effective kernel size when dilation is present\n    k_eff = (K - 1) * (dilation + 1) + 1\n\n    # Determine explicit padding and output length\n    p_left, p_right, L_out = _compute_padding(L_in, k_eff, stride, pad)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 zero padding \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    X_pad = np.pad(X,\n                   pad_width=((0, 0), (p_left, p_right), (0, 0)),\n                   mode='constant')\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 allocate output tensor \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    out_dtype = np.result_type(X.dtype, W.dtype)\n    Y = np.zeros((N, L_out, C_out), dtype=out_dtype)\n\n    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 convolution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    hop = dilation + 1   # distance between neighbouring kernel taps\n    for i_out in range(L_out):\n        start = i_out * stride\n        # accumulate contribution of every kernel tap\n        for k in range(K):\n            x_idx = start + k * hop\n            # X_slice : (N, C_in)\n            X_slice = X_pad[:, x_idx, :]             # sample for every example\n            # W_slice : (C_in, C_out)\n            W_slice = W[k, :, :]\n            # (N, C_in)  @  (C_in, C_out)  \u21d2  (N, C_out)\n            Y[:, i_out, :] += X_slice @ W_slice\n\n    return Y.tolist()\n```"}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    # Convert input to a NumPy 1-D array (this also copies Python lists)\n    y = np.asarray(y).ravel()\n\n    # Empty sequence \u2192 impurity 0.0 (by convention)\n    n_samples = y.size\n    if n_samples == 0:\n        return 0.0\n\n    # Count occurrences of each class\n    _, counts = np.unique(y, return_counts=True)\n\n    # Relative frequencies p_c\n    probs = counts / n_samples\n\n    # Gini impurity\n    gini_value = 1.0 - np.sum(probs ** 2)\n\n    # Round to 4 decimal places as requested\n    return round(float(gini_value), 4)\n```"}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(\n        X: np.ndarray,\n        y: np.ndarray,\n        learning_rate: float = 0.01,\n        n_iterations: int = 10000\n    ) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n).\n    y : np.ndarray\n        Target vector of shape (m,) or (m, 1).\n    learning_rate : float, optional\n        Step size for gradient descent, by default 0.01.\n    n_iterations : int, optional\n        Number of gradient\u2013descent iterations, by default 10000.\n    \n    Returns\n    -------\n    tuple[list[float], float]\n        (weights_list, bias)   \u2013 rounded to 4 decimal places.\n        If the numbers of samples in X and y disagree, returns -1.\n    \"\"\"\n\n    # -------------------- Input validation --------------------\n    if X.ndim == 1:                       # allow 1-D feature (m,) \u2192 (m,1)\n        X = X.reshape(-1, 1)\n    if y.ndim > 1:                        # squeeze possible (m,1)\n        y = y.reshape(-1)\n        \n    m, n = X.shape\n    if m != y.shape[0]:                   # inconsistent sample count\n        return -1\n    \n    # -------------------- Initialisation ----------------------\n    theta = np.zeros(n, dtype=float)      # weight vector \u03b8\n    b = 0.0                               # bias\n    \n    # -------------------- Gradient descent --------------------\n    for _ in range(n_iterations):\n        y_pred = X @ theta + b            # predictions (m,)\n        error  = y_pred - y               # residuals   (m,)\n        \n        # gradients\n        dw = (2.0 / m) * (X.T @ error)    # (n,)\n        db = (2.0 / m) * np.sum(error)    # scalar\n        \n        # parameter update\n        theta -= learning_rate * dw\n        b     -= learning_rate * db\n    \n    # -------------------- Rounding & return -------------------\n    theta_rounded = [round(w, 4) for w in theta.tolist()]\n    b_rounded     = round(b, 4)\n    \n    return (theta_rounded, b_rounded)\n```"}
{"task_id": 375, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calc_pad_dims_2D(X_shape: tuple,\n                     out_dim: tuple,\n                     kernel_shape: tuple,\n                     stride: int,\n                     dilation: int = 0) -> tuple:\n    \"\"\"Return the amount of zero-padding needed on each side of a 4-D tensor.\n\n    Parameters\n    ----------\n    X_shape : tuple\n        Shape of the input tensor ``(n_ex, in_rows, in_cols, in_ch)``.\n    out_dim : tuple\n        Desired output height and width ``(out_rows, out_cols)``.\n    kernel_shape : tuple\n        Height and width of the convolution kernel ``(fr, fc)``.\n    stride : int\n        Stride used by the convolution (must be a strictly positive integer).\n    dilation : int, optional (default=0)\n        Number of zeros inserted between two neighbouring kernel elements\n        (must be a non-negative integer).\n\n    Returns\n    -------\n    tuple\n        A 4-tuple ``(top, bottom, left, right)`` with the number of rows/columns\n        that have to be padded on each side of the input tensor.\n\n    Raises\n    ------\n    ValueError\n        If one of the arguments has a wrong type/value or if the requested\n        output size cannot be obtained with zero-padding.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. Basic type / value checks\n    # ------------------------------------------------------------------ #\n    def _check_shape(arg, name, length):\n        if not (isinstance(arg, tuple) and len(arg) == length):\n            raise ValueError(f\"{name} must be a tuple of length {length}.\")\n        if not all(isinstance(x, int) and x > 0 for x in arg):\n            raise ValueError(f\"All elements of {name} must be positive ints.\")\n\n    _check_shape(X_shape,      \"X_shape\",      4)\n    _check_shape(out_dim,      \"out_dim\",      2)\n    _check_shape(kernel_shape, \"kernel_shape\", 2)\n\n    if not (isinstance(stride, int) and stride > 0):\n        raise ValueError(\"stride must be a positive integer.\")\n    if not (isinstance(dilation, int) and dilation >= 0):\n        raise ValueError(\"dilation must be a non-negative integer.\")\n\n    # ------------------------------------------------------------------ #\n    # 2. Extract sizes\n    # ------------------------------------------------------------------ #\n    _, in_rows, in_cols, _ = X_shape\n    out_rows, out_cols      = out_dim\n    fr, fc                  = kernel_shape\n\n    # ------------------------------------------------------------------ #\n    # 3. Effective kernel size with the given dilation\n    #    (number of \u201creal\u201d positions covered by the dilated kernel)\n    # ------------------------------------------------------------------ #\n    eff_fr = fr * (dilation + 1) - dilation   # _fr\n    eff_fc = fc * (dilation + 1) - dilation   # _fc\n\n    # ------------------------------------------------------------------ #\n    # 4. Total padding required (top+bottom, left+right)\n    #    pad_total = (out\u22121)\u00b7s \u2212 in + eff_filter\n    #    If pad_total < 0 we can never reach the requested output size.\n    # ------------------------------------------------------------------ #\n    pad_rows_total = (out_rows - 1) * stride - in_rows + eff_fr\n    pad_cols_total = (out_cols - 1) * stride - in_cols + eff_fc\n\n    if pad_rows_total < 0 or pad_cols_total < 0:\n        raise ValueError(\"Requested output size is impossible with zero-padding.\")\n\n    # ------------------------------------------------------------------ #\n    # 5. Split the total padding between the two sides\n    #    (extra pixel goes to bottom / right, TensorFlow behaviour)\n    # ------------------------------------------------------------------ #\n    up_pad    = pad_rows_total // 2\n    down_pad  = pad_rows_total - up_pad     # extra row (if any) -> bottom\n\n    left_pad  = pad_cols_total // 2\n    right_pad = pad_cols_total - left_pad   # extra col (if any) -> right\n\n    # All returned values are guaranteed to be non-negative integers\n    return (up_pad, down_pad, left_pad, right_pad)\n```"}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose\n    within a numerical tolerance of 1 \u00d7 10\u207b\u2078.\n\n    Args:\n        X: Matrix represented as a list of lists (or any array-like) \n           containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert to a NumPy array (this also works if `X` is already an ndarray)\n    A = np.asarray(X, dtype=float)\n\n    # Must be two-dimensional and square\n    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n        return False\n\n    # Compare the matrix with its transpose using NumPy\u2019s allclose\n    return np.allclose(A, A.T, atol=1e-8)\n```"}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n       Compute the (negative) gradient that Gradient Boosting uses for the\n    logistic (binomial deviance) loss in binary classification.\n\n    Parameters\n    ----------\n    y : list or 1-D array-like\n        Binary class labels encoded as 0 or 1.\n    f : list or 1-D array-like\n        Current prediction scores (any real numbers).\n\n    Returns\n    -------\n    list\n        The negative gradients g rounded to four decimal places.\n        If `y` contains values other than 0 or 1, the function returns -1.\n       # Convert inputs to NumPy arrays\n    y_arr = np.asarray(y)\n    f_arr = np.asarray(f, dtype=float)\n\n    # Check that y only contains 0 or 1\n    if not np.all(np.isin(y_arr, [0, 1])):\n        return -1\n\n    # Convert labels {0,1} \u2192 {-1,1}\n    y_prime = 2 * y_arr - 1        # y' = 2y \u2212 1\n\n    # Compute g = y' / (1 + exp(y' * f))\n    z = y_prime * f_arr\n    with np.errstate(overignore      # suppress overflow warnings in exp\n        g = y_prime / (1.0 + np.exp(z))\n\n    # Round to four decimals and return as Python list\n    return np.round(g, 4).tolist()\n```"}
{"task_id": 378, "completion_id": 0, "completion": "```python\ndef indicator(flag: int) -> int:\n    \"\"\"Return 1 if *flag* is 1, otherwise return 0.\n\n    Args:\n        flag (int): An integer value.\n\n    Returns:\n        int: 1 if flag == 1, else 0.\n    \"\"\"\n    return 1 if flag == 1 else 0\n```\n"}
{"task_id": 379, "completion_id": 0, "completion": "```python\ndef sort_priority_nodes(nodes: list[dict]) -> list[str]:\n   Sort the given priority-queue nodes and return their keys.\n\n    The sort order is:\n      1. Increasing ``priority`` value.\n      2. For equal priorities, increasing ``entry_id`` value.\n\n    The original ``nodes`` list is left untouched.\n\n    Args:\n        nodes: A list of mapping objects each containing the keys\n               ``'key'``, ``'val'``, ``'priority'`` and ``'entry_id'``.\n\n    Returns:\n        A list with the ``key`` values ordered according to the rules\n        above.  If *nodes* is empty, an empty list is returned.\n       # ``sorted`` creates a new list, so the input is not mutated.\n    ordered_nodes = sorted(nodes, key=lambda n: (n['priority'], n['entry_id']))\n    # Extract just the ``key`` field in the determined order.\n    return [node['key'] for node in ordered_nodes]\n```"}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# SELU recommended constants (paper values)\n_ALPHA  = 1.6732632423543772848170429916717\n_SCALE  = 1.0507009873554804934193349852946\n\n\ndef _selu_base(z):\n   SELU proper    return np.where(z > 0,\n                    _SCALE * z,\n                    _SCALE * _ALPHA * (np.exp(z) - 1.0))\n\n\ndef _selu_first(z):\n   First derivative of SELU    return np.where(z > 0,\n                    _SCALE * 1.0,\n                    _SCALE * _ALPHA * np.exp(z))\n\n\ndef _selu_second(z):\n   Second derivative of SELU    return np.where(z > 0,\n                    0.0,\n                    _SCALE * _ALPHA * np.exp(z))\n\n\ndef selu(x, order: int = 0):\n       Compute the SELU activation or its analytical first / second derivative.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input values (scalar or any-shape array-like).\n    order : {0, 1, 2}, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 d(SELU)/dx\n        2 \u2192 d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        Result rounded to 4 decimals.  Arrays are returned\n        as (possibly nested) Python lists, scalars as float.\n       if order not in (0, 1, 2):\n        raise ValueErrororder must be 0, 1 or 2    z = np.asarray(x, dtype=float)       # works for scalar & array inputs\n\n    if   order == 0:\n        out = _selu_base(z)\n    elif order == 1:\n        out = _selu_first(z)\n    else:                                # order == 2\n        out = _selu_second(z)\n\n    out = np.round(out, 4)               # element-wise rounding\n\n    # Preserve original 'scalar' vs 'array' nature\n    return out.item() if out.shape == () else out.tolist()\n```"}
{"task_id": 382, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef gaussian_naive_bayes_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray) -> list:\n    \"\"\"Train a Gaussian Naive Bayes model on *X_train*, *y_train* and predict\n    the labels of *X_test*.\n\n    Notes\n    -----\n    \u2022 Each feature is modelled by an independent Gaussian distribution\n      per class.  \n    \u2022 An \u03b5 = 1e-4 is added to every variance to avoid division by zero.  \n    \u2022 All computations are carried out in the log\u2013domain to prevent\n      numerical under-/overflow.  Tie-breaking follows `np.argmax`\n      which is consistent with the statement.\n    \"\"\"\n    eps = 1e-4                          # numerical stabiliser\n    X_train = np.asarray(X_train, float)\n    y_train = np.asarray(y_train)\n    X_test  = np.asarray(X_test,  float)\n\n    classes, counts = np.unique(y_train, return_counts=True)\n    n_classes = classes.size\n    n_features = X_train.shape[1]\n\n    # ------------------------------------------------------------\n    # 1.  Estimate class-conditional Gaussians: mean and variance\n    # ------------------------------------------------------------\n    means     = np.zeros((n_classes, n_features), dtype=float)\n    variances = np.zeros((n_classes, n_features), dtype=float)\n    priors    = counts / counts.sum()          # P(Y=c)\n\n    for idx, c in enumerate(classes):\n        X_c = X_train[y_train == c]\n        means[idx]     = X_c.mean(axis=0)\n        variances[idx] = X_c.var(axis=0)       # population variance (ddof=0)\n\n    # ------------------------------------------------------------\n    # 2.  Compute log-posteriors for every test sample and class\n    # ------------------------------------------------------------\n    # Pre-compute constant parts of the log-likelihood\n    log_coeff = -0.5 * np.log(2.0 * math.pi * variances + eps)   # shape (C, F)\n\n    # Container for the final predictions\n    predictions = []\n\n    for x in X_test:                                           # iterate over rows\n        # Broadcast x (shape F,) against means/variances (C, F)\n        diff     = x - means                                   # (C, F)\n        log_lik  = log_coeff - (diff ** 2) / (2.0 * variances + eps)\n        log_post = np.log(priors) + log_lik.sum(axis=1)        # (C,)\n\n        # Pick the class with the largest posterior\n        predictions.append(classes[int(np.argmax(log_post))])\n\n    return predictions\n```"}
{"task_id": 383, "completion_id": 0, "completion": "```python\ndef find_bounds(y_i: int, y_j: int, alpha_i: float, alpha_j: float, C: float) -> list[float]:\n    \"\"\"Return the feasible interval [L, H] for alpha_j in SMO.\n\n    Parameters\n    ----------\n    y_i : int\n        Label of the i-th example (\u22121 or 1).\n    y_j : int\n        Label of the j-th example (\u22121 or 1).\n    alpha_i : float\n        Current Lagrange multiplier for the i-th example.\n    alpha_j : float\n        Current Lagrange multiplier for the j-th example.\n    C : float\n        Regularisation constant of the SVM.\n\n    Returns\n    -------\n    list[float]\n        A two-element list [L, H] with both values rounded to 4 decimals.\n    \"\"\"\n    # Different labels\n    if y_i != y_j:\n        L = max(0.0, alpha_j - alpha_i)\n        H = min(C, C - alpha_i + alpha_j)\n    # Same labels\n    else:\n        L = max(0.0, alpha_i + alpha_j - C)\n        H = min(C, alpha_i + alpha_j)\n\n    # Round to four decimals as required\n    return [round(L, 4), round(H, 4)]\n```"}
{"task_id": 384, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef get_eigvecs(L: list[list[int | float]] | np.ndarray, k: int):\n    \"\"\"Return the k Laplacian eigenvectors that correspond to the k\n    smallest eigenvalues.\n\n    Parameters\n    ----------\n    L : 2-D list/np.ndarray\n        Square (n\u00d7n) real Laplacian matrix.\n    k : int\n        Number of eigenvectors wanted.\n\n    Returns\n    -------\n    list[list[float]] | int\n        A (n\u00d7k) list whose columns are the required eigenvectors, rounded\n        to 4 decimals, or \u22121 if k is invalid.\n    \"\"\"\n    # -- basic checks -------------------------------------------------------\n    A = np.asarray(L, dtype=float)\n    if A.ndim != 2 or A.shape[0] != A.shape[1] or k <= 0 or k > A.shape[0]:\n        return -1\n    n = A.shape[0]\n\n    # -- eigen-decomposition ------------------------------------------------\n    # Laplacians are symmetric \u21d2 eigh is ideal; it already sorts eigenvalues\n    try:\n        eigvals, eigvecs = np.linalg.eigh(A)\n    except np.linalg.LinAlgError:\n        # Fallback if eigh fails (should not happen for proper Laplacians)\n        eigvals, eigvecs = np.linalg.eig(A)\n        eigvals = eigvals.real\n        eigvecs = eigvecs.real\n        order = np.argsort(eigvals)\n        eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n\n    # Take the k eigenvectors for the k smallest eigenvalues\n    V = eigvecs[:, :k].real  # shape (n, k)\n\n    # -- normalise, fix signs ----------------------------------------------\n    for j in range(k):\n        v = V[:, j]\n        # L2 normalisation\n        norm = np.linalg.norm(v)\n        if norm != 0:\n            v /= norm\n        # Deterministic sign: make first non-zero entry positive\n        nz = np.flatnonzero(v)\n        if nz.size and v[nz[0]] < 0:\n            v *= -1\n        V[:, j] = v\n\n    # -- rounding & conversion ---------------------------------------------\n    V = np.round(V, 4)\n    return V.tolist()\n```"}
{"task_id": 385, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef lda_transform(X: list[list[int | float]], y: list[int], k: int) -> list[list[float]]:\n   Project data onto the first k Linear Discriminant directions.\n\n    Parameters\n    ----------\n    X : list[list[int|float]]\n        Input data of shape (n_samples, n_features).\n    y : list[int]\n        Class labels for each sample (length n_samples).\n    k : int\n        Number of LDA components to keep.\n\n    Returns\n    -------\n    list[list[float]]\n        Data projected onto the first k LDA directions,\n        rounded to 4 decimals.\n       # --- 1. Convert inputs to numpy arrays ----------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n    n_samples, n_features = X.shape\n\n    # --- 2. Basic class information ----------------------------------------\n    classes = np.unique(y)\n    c = len(classes)\n    if c < 2:                          # LDA needs at least two classes\n        raise ValueErrorNeed at least two distinct classes for LDA    max_components = min(c - 1, n_features)\n    k = max(1, min(k, max_components)) # enforce 1 \u2264 k \u2264 c-1 (and \u2264 n_features)\n\n    # --- 3. Compute class means and overall mean ---------------------------\n    overall_mean = X.mean(axis=0)\n    Sw = np.zeros((n_features, n_features), dtype=float)  # within-class scatter\n    Sb = np.zeros((n_features, n_features), dtype=float)  # between-class scatter\n\n    for cls in classes:\n        X_c = X[y == cls]\n        n_c = X_c.shape[0]\n        mean_c = X_c.mean(axis=0)\n        # within-class scatter\n        centered = X_c - mean_c\n        Sw += centered.T @ centered\n        # between-class scatter\n        mean_diff = (mean_c - overall_mean).reshape(-1, 1)\n        Sb += n_c * (mean_diff @ mean_diff.T)\n\n    # --- 4. Generalised eigenvalue problem ---------------------------------\n    # Use pseudo-inverse in case Sw is singular\n    Sw_inv = np.linalg.pinv(Sw)\n    mat = Sw_inv @ Sb\n\n    eigvals, eigvecs = np.linalg.eig(mat)          # columns of eigvecs are eigenvectors\n    # Sort by descending eigenvalues (real parts)\n    sorted_idx = np.argsort(-eigvals.real)\n    eigvecs = eigvecs[:, sorted_idx]\n\n    # --- 5. Pick first k eigenvectors and post-process ----------------------\n    W = []\n    count = 0\n    for vec in eigvecs.T:               # iterate in sorted order\n        if count == k:\n            break\n        # Normalise to unit length\n        vec = vec.real\n        norm = np.linalg.norm(vec)\n        if norm == 0:                   # skip zero vectors (shouldn't happen in theory)\n            continue\n        vec = vec / norm\n        # Deterministic orientation: first element non-negative\n        if vec[0] < 0:\n            vec = -vec\n        W.append(vec)\n        count += 1\n\n    W = np.stack(W, axis=1)             # shape (n_features, k)\n\n    # --- 6. Project data ----------------------------------------------------\n    projected = X @ W                   # shape (n_samples, k)\n\n    # --- 7. Round to 4 decimals and return plain Python list ---------------\n    projected = np.round(projected, 4).tolist()\n    return projected\n```"}
{"task_id": 386, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (harmless duplicate kept to respect starter code)\n\ndef binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross-Entropy (BCE) loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        1-D array of true labels (0 or 1).\n    y_pred : np.ndarray\n        1-D array of predicted probabilities (between 0 and 1).\n\n    Returns\n    -------\n    float\n        The average BCE loss rounded to 4 decimal places.\n    \"\"\"\n    # Ensure 1-D matching shapes\n    y_true = np.ravel(y_true).astype(float)\n    y_pred = np.ravel(y_pred).astype(float)\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # 1. Clip predictions to avoid log(0)\n    eps = 1e-15\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    \n    # 2. Compute BCE\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    # 3. Round to 4 decimal places and return as Python float\n    return float(np.round(loss, 4))\n```"}
{"task_id": 387, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float],\n                         n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Very small 1-D gradient boosting regressor (decision stumps only).\"\"\"\n    # ---- preparation -------------------------------------------------------\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = y.size\n\n    # initial prediction \u2013 mean target\n    y_pred = np.full(n, y.mean())\n\n    # ---- boosting rounds ---------------------------------------------------\n    for _ in range(n_estimators):\n        # negative gradient of squared-error \u21d2 residuals\n        r = y - y_pred\n\n        # sort once by the (single) feature\n        order = np.argsort(x)\n        xs, rs = x[order], r[order]\n\n        # cumulative statistics for O(N) split search\n        csum_r  = np.cumsum(rs)            # \u03a3 r\n        csum_r2 = np.cumsum(rs ** 2)       # \u03a3 r\u00b2\n        tot_r, tot_r2 = csum_r[-1], csum_r2[-1]\n\n        best_sse = np.inf\n        best_idx = None\n\n        # examine every gap with a different neighbour value\n        for i in range(n - 1):\n            if xs[i] == xs[i + 1]:\n                continue          # cannot split inside identical values\n\n            nL = i + 1\n            nR = n - nL\n\n            sumL, sumSqL = csum_r[i],  csum_r2[i]\n            sumR, sumSqR = tot_r - sumL,  tot_r2 - sumSqL\n\n            # SSE  = \u03a3 r\u00b2 \u2013 n\u00b7(mean)\u00b2  = \u03a3 r\u00b2 \u2013 (\u03a3 r)\u00b2 / n\n            sseL = sumSqL - (sumL ** 2) / nL\n            sseR = sumSqR - (sumR ** 2) / nR\n            sse  = sseL + sseR\n\n            if sse < best_sse:\n                best_sse = sse\n                best_idx = i\n\n        # ---- build stump and update prediction -----------------------------\n        if best_idx is None:        # all feature values identical\n            stump_pred = np.full(n, r.mean())\n        else:\n            # split threshold halfway between the two distinct neighbours\n            t_star = 0.5 * (xs[best_idx] + xs[best_idx + 1])\n\n            nL = best_idx + 1\n            sumL = csum_r[best_idx]\n            meanL = sumL / nL\n            meanR = (tot_r - sumL) / (n - nL)\n\n            stump_pred = np.where(x <= t_star, meanL, meanR)\n\n        # shrinkage update\n        y_pred += learning_rate * stump_pred\n\n    # ---- return rounded in-sample predictions ------------------------------\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 389, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_laplacian(W):\n   Compute the (unnormalised) Laplacian matrix of an undirected graph.\n\n    Parameters\n    ----------\n    W : list[list[int | float]] | np.ndarray\n        Square weighted adjacency matrix of the graph.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The Laplacian matrix rounded to 4 decimal places and converted to a\n        Python list of lists. Returns -1 if the input is not a square matrix.\n       # Convert the input into a NumPy array (keeps a copy only if necessary)\n    W = np.asarray(W, dtype=float)\n\n    # Validate that the matrix is 2-D and square\n    if W.ndim != 2 or W.shape[0] != W.shape[1]:\n        return -1\n\n    # Degree matrix: diagonal entries are the row sums of W\n    row_sums = W.sum(axis=1)\n    D = np.diag(row_sums)\n\n    # Laplacian: L = D - W\n    L = D - W\n\n    # Round to 4 decimal places\n    L = np.round(L, 4)\n\n    # Return as a (nested) Python list\n    return L.tolist()\n```"}
{"task_id": 391, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nn_interpolate_1D(X: np.ndarray, t: list[float]) -> list[list[float]]:\n    \"\"\"Nearest-neighbour interpolation of a 1-D (possibly multi-channel) signal.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array of shape (in_length, in_channels) holding the original samples.\n        If X is one-dimensional it is treated as having exactly one channel.\n    t : list[float] | np.ndarray\n        Query positions (may be non-integers).\n\n    Returns\n    -------\n    list[list[float]]\n        Interpolated samples with shape (len(t), in_channels), returned as a\n        plain Python list obtained via ``tolist()``.\n    \"\"\"\n    # Ensure input is an ndarray\n    X = np.asarray(X)\n\n    # Length along the time/sample axis\n    in_length = X.shape[0]\n\n    # 1. Round each query position to the nearest integer (NumPy's around rules)\n    idx = np.around(t).astype(int)\n\n    # 2. Clip indices to the valid range [0, in_length-1]\n    idx = np.clip(idx, 0, in_length - 1)\n\n    # 3. Gather the requested samples\n    Y = X[idx]\n\n    # If X had a single channel, expand the dimension so that every element\n    # becomes a list of length 1 (as required by the specification).\n    if Y.ndim == 1:\n        Y = Y[:, np.newaxis]\n\n    return Y.tolist()\n```"}
{"task_id": 392, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef preemphasis(x:list[float] | np.ndarray alpha: float) -> list[float]:\n   Apply a first-order pre-emphasis filter to a 1-D signal.\n\n    Args:\n        x: Input signal (array-like).  It may have any shape; it is first\n           flattened so the filter is applied to the raw sample sequence.\n        alpha: Pre-emphasis coefficient, 0 \u2264 alpha < 1.\n\n    Returns:\n        List of floats: the filtered signal, every value rounded to four\n        decimal places.\n       # ---- sanity checks ----------------------------------------------------\n    if not (0 <= alpha < 1):\n        raise ValueErroralpha must satisfy 0 \u2264 alpha < 1    \n    # ---- convert to 1-D numpy array ---------------------------------------\n    x = np.asarray(x, dtype=float).ravel()        # flatten in case x is >1-D\n    n = x.size\n    if n == 0:\n        return []                                 # empty input \u2192 empty output\n    \n    # ---- core pre-emphasis computation ------------------------------------\n    y = np.empty_like(x)\n    y[0]  = x[0]                                  # first sample unchanged\n    if n > 1:\n        y[1:] = x[1:] - alpha * x[:-1]            # vectorised recurrence\n    \n    # ---- round to 4 decimals & return Python list -------------------------\n    return np.round(y, 4).tolist()\n```"}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` is used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists and rounded to four decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays (makes the function tolerant to Python lists)\n    w = np.asarray(w, dtype=float)\n    grad = np.asarray(grad, dtype=float)\n\n    # 1. Initialise Eg if necessary\n    if Eg is None:\n        Eg = np.zeros_like(grad, dtype=float)\n    else:\n        Eg = np.asarray(Eg, dtype=float)\n\n    # 2. Compute the new running average of the squared gradients\n    Eg_next = rho * Eg + (1.0 - rho) * np.square(grad)\n\n    # 3. Update parameters using the RMSprop rule\n    eps = 1e-8  # numerical stability term\n    w_next = w - learning_rate * grad / np.sqrt(Eg_next + eps)\n\n    # 4. Round to 4 decimal places and convert to Python lists\n    w_next_list = np.round(w_next, 4).tolist()\n    Eg_next_list = np.round(Eg_next, 4).tolist()\n\n    return w_next_list, Eg_next_list\n```"}
{"task_id": 395, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef print_backward_result(beta: np.ndarray, b_prob: float) -> str:\n    \"\"\"Return a formatted string with a \u03b2-matrix and its probability.\n\n    The string must look like:\n        **************************************************\n        Beta:\n        <beta as str(beta)>\n        Probability of sequence: <b_prob>\n\n    Args:\n        beta: A 2-D NumPy array of \u03b2-values.\n        b_prob: Probability of the observation sequence.\n\n    Returns:\n        The formatted multi-line string (do not print inside this function).\n    \"\"\"\n    # Build each required part and concatenate using '\\n'\n    header = \"*\" * 50                       # 50 asterisks\n    beta_label = \"Beta:\"\n    prob_line = f\"Probability of sequence: {b_prob}\"\n\n    # Assemble the final multi-line string\n    formatted = f\"{header}\\n{beta_label}\\n{str(beta)}\\n{prob_line}\"\n\n    return formatted\n```"}
{"task_id": 396, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef tanh_with_derivatives(x):\n   Compute tanh and its first two derivatives element-wise.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        1-D collection of real numbers.\n\n    Returns\n    -------\n    list[list[float]]\n        [\n            tanh(x),\n            d/dx tanh(x),\n            d\u00b2/dx\u00b2 tanh(x)\n        ]\n        Every numeric value is rounded to 6 decimal places.\n       # Convert to a NumPy array without altering the original input\n    x_arr = np.asarray(x, dtype=float)\n\n    # tanh(x)\n    t = np.tanh(x_arr)\n\n    # First derivative: 1 - tanh(x)^2\n    first = 1.0 - t**2\n\n    # Second derivative: -2 * tanh(x) * (1 - tanh(x)^2)\n    second = -2.0 * t * first\n\n    # Round to 6 decimal places and convert to plain Python lists\n    t_list       = np.round(t,      6).tolist()\n    first_list   = np.round(first,  6).tolist()\n    second_list  = np.round(second, 6).tolist()\n\n    return [t_list, first_list, second_list]\n```"}
{"task_id": 397, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef random_one_hot_matrix(n_examples: int, n_classes: int):\n    \"\"\"Create a random one-hot matrix.\n\n    Parameters\n    ----------\n    n_examples : int\n        Number of rows (examples) to generate. May be 0.\n    n_classes : int\n        Number of distinct classes (>0).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of shape (n_examples, n_classes) where each row is a one-hot\n        vector chosen uniformly at random from the `n_classes` possibilities.\n    \"\"\"\n    # Handle the trivial empty-matrix case explicitly so the shape is correct.\n    if n_examples == 0:\n        return np.zeros((0, n_classes), dtype=float)\n\n    # Pick a random target class for every example, uniformly and independently.\n    class_indices = np.random.randint(0, n_classes, size=n_examples)\n\n    # Build the one-hot matrix.\n    one_hot = np.zeros((n_examples, n_classes), dtype=float)\n    one_hot[np.arange(n_examples), class_indices] = 1.0\n    return one_hot\n```"}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n   Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numeric values.\n        alpha: Positive constant that controls the value for negative saturation.\n        derivative: If True, return the gradient; otherwise, return the activation.\n\n    Returns:\n        Python list of floats rounded to 4 decimal places.\n       # Convert input to NumPy array for vectorized operations\n    x_arr = np.asarray(x, dtype=float)\n\n    if derivative:\n        # ELU'(x) = 1 for x >= 0, else alpha * exp(x)\n        result = np.where(x_arr >= 0, 1.0, alpha * np.exp(x_arr))\n    else:\n        # ELU(x) = x for x >= 0, else alpha * (exp(x) - 1)\n        result = np.where(x_arr >= 0, x_arr, alpha * (np.exp(x_arr) - 1))\n\n    # Round to 4 decimal places and return as Python list\n    return [round(val, 4) for val in result]\n```"}
{"task_id": 399, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pairwise_distances(X, Y):\n    \"\"\"Compute the Euclidean distance between every pair of points in X and Y.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        A 2-D array of shape (m, d) where each row is a point.\n    Y : numpy.ndarray\n        A 2-D array of shape (n, d) where each row is a point.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The m\u00d7n matrix of distances rounded to four decimals as a list of\n        lists, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # --- Validation ---------------------------------------------------------\n    if not isinstance(X, np.ndarray) or not isinstance(Y, np.ndarray):\n        return -1\n    if X.ndim != 2 or Y.ndim != 2:\n        return -1\n    if X.shape[1] != Y.shape[1]:\n        return -1\n\n    # --- Pair-wise squared Euclidean distances ------------------------------\n    # ||x - y||\u00b2 = ||x||\u00b2 + ||y||\u00b2 \u2212 2\u00b7x\u00b7y\n    X_sq = (X ** 2).sum(axis=1).reshape(-1, 1)        # (m, 1)\n    Y_sq = (Y ** 2).sum(axis=1).reshape(1, -1)        # (1, n)\n    dist_sq = X_sq + Y_sq - 2 * X @ Y.T               # (m, n)\n    dist_sq = np.clip(dist_sq, 0.0, None)             # numerical safety\n\n    # --- Convert to Euclidean distance, round, and cast to list -------------\n    D = np.sqrt(dist_sq)\n    D = np.round(D, 4)\n    return D.tolist()\n```"}
{"task_id": 400, "completion_id": 0, "completion": "```python\nimport math\n\ndef gaussian_cdf(x: float, mean: float, variance: float) -> float:\n   Compute \u03a6(x; \u03bc, \u03c3\u00b2) \u2013 the Gaussian CDF evaluated at *x*.\n\n    Parameters\n    ----------\n    x : float\n        The point at which to evaluate the CDF.\n    mean : float\n        The mean (\u03bc) of the Gaussian distribution.\n    variance : float\n        The variance (\u03c3\u00b2) of the Gaussian distribution. Must be non-negative.\n\n    Returns\n    -------\n    float\n        The probability that a Gaussian random variable with given parameters\n        does not exceed *x*, rounded to six decimal places.\n       # Guard against invalid (negative) variance\n    if variance < 0.0:\n        raise ValueErrorVariance must be non-negative    # Degenerate (point-mass) case: variance == 0\n    if variance == 0.0:\n        if x < mean:\n            cdf = 0.0\n        elif math.isclose(x, mean, rel_tol=0.0, abs_tol=1e-12):\n            cdf = 0.5\n        else:  # x > mean\n            cdf = 1.0\n    else:\n        sigma = math.sqrt(variance)\n        z = (x - mean) / (sigma * math.sqrt(2.0))\n        cdf = 0.5 * (1.0 + math.erf(z))\n        # Numerical safety: confine result to [0, 1]\n        cdf = min(max(cdf, 0.0), 1.0)\n\n    return round(cdf, 6)\n```"}
{"task_id": 401, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _is_one_hot(arr: np.ndarray) -> bool:\n   Return True if *arr* looks like a one-hot matrix    if arr.ndim != 2 or arr.shape[1] == 1:          # must be 2-D and wider than 1\n        return False\n    # Every row must contain exactly one `1` and the remainder `0`\n    row_sum = arr.sum(axis=1)\n    if not np.allclose(row_sum, 1):\n        return False\n    # All values must be (close to) 0 or 1\n    return np.all((arr == 0) | (arr == 1))\n\n\ndef to_one_hot(y: np.ndarray, num_classes: int | None = None) -> np.ndarray:\n   Convert integer class labels to a one-hot encoded matrix.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        Label vector of shape (N,) or (N, 1), or an already one-hot matrix\n        of shape (N, C) whose rows sum to 1.\n    num_classes : int, optional\n        Number of classes C.  If ``None`` it is inferred as\n        ``max(y) + 1`` when *y* is a label vector.  When *y* is already\n        one-hot its second dimension is taken as *C* and this argument\n        is ignored.\n\n    Returns\n    -------\n    np.ndarray\n        One-hot encoded array of shape (N, C) and dtype ``float`` that\n        contains only 0.0 and 1.0.\n       # Make sure we work with a numpy array without touching the caller\u2019s data\n    y_arr = np.asarray(y)\n\n    # ---------------------------------------------------------------------\n    # Case 1: *y* already looks like a one-hot matrix, just return a float\n    #         copy (or view) so the caller cannot modify the original.\n    # ---------------------------------------------------------------------\n    if _is_one_hot(y_arr):\n        return y_arr.astype(float, copy=False)\n\n    # ---------------------------------------------------------------------\n    # Case 2: *y* is a label vector (shape (N,) or (N, 1)) \u2013 build one-hot\n    # ---------------------------------------------------------------------\n    if y_arr.ndim == 2 and y_arr.shape[1] == 1:\n        y_arr = y_arr.ravel()                # (N, 1) \u2192 (N,)\n\n    if y_arr.ndim != 1:\n        raise ValueErrorInput array must be 1-D, (N, 1), or already one-hot    if not np.issubdtype(y_arr.dtype, np.integer):\n        raise TypeErrorLabel vector must contain integers    if np.any(y_arr < 0):\n        raise ValueErrorClass labels must be non-negative integers    # Determine number of classes\n    inferred_classes = int(y_arr.max()) + 1\n    if num_classes is None:\n        num_classes = inferred_classes\n    elif num_classes < inferred_classes:\n        raise ValueError(\n            fnum_classes` (={num_classes}) is smaller than            fmax label + 1 (={inferred_classes}).        )\n\n    # Build one-hot matrix using NumPy indexing trick\n    one_hot = np.zeros((y_arr.shape[0], num_classes), dtype=float)\n    one_hot[np.arange(y_arr.shape[0]), y_arr] = 1.0\n    return one_hot\n```"}
{"task_id": 402, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_nb_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test:  np.ndarray,\n    eps:     float = 1e-6\n) -> np.ndarray:\n    \"\"\"Predict labels for X_test using a Gaussian Naive Bayes classifier.\n\n    Parameters\n    ----------\n    X_train : np.ndarray of shape (N_train, M)\n        Training feature matrix.\n    y_train : np.ndarray of shape (N_train,)\n        Training labels.\n    X_test : np.ndarray of shape (N_test, M)\n        Test feature matrix whose labels are to be predicted.\n    eps : float, default=1e-6\n        Small value added to variances to avoid division by zero.\n\n    Returns\n    -------\n    np.ndarray of shape (N_test,)\n        Predicted labels for each row in X_test.\n    \"\"\"\n    # ---------------------------------------------------------------------\n    # 1.  Gather basic dataset information\n    # ---------------------------------------------------------------------\n    classes          = np.unique(y_train)           # distinct class labels\n    n_classes        = classes.size\n    n_features       = X_train.shape[1]\n    n_total_samples  = X_train.shape[0]\n\n    # ---------------------------------------------------------------------\n    # 2.  Estimate class-conditional means, variances and class priors\n    # ---------------------------------------------------------------------\n    means      = np.zeros((n_classes, n_features))\n    variances  = np.zeros((n_classes, n_features))\n    priors     = np.zeros(n_classes)\n\n    for idx, c in enumerate(classes):\n        mask          = (y_train == c)\n        X_c           = X_train[mask]                 # samples of class c\n        priors[idx]   = X_c.shape[0] / n_total_samples\n        means[idx]    = X_c.mean(axis=0)\n        # variance with denominator N_c (maximum-likelihood estimate)\n        variances[idx] = X_c.var(axis=0) + eps        # add eps for stability\n\n    # ---------------------------------------------------------------------\n    # 3.  Compute log-posterior for every test sample and class\n    # ---------------------------------------------------------------------\n    # Pre-compute constants that do not depend on an individual test sample\n    log_priors = np.log(priors)                      # shape (n_classes,)\n    log_var    = np.log(variances)                   # shape (n_classes, n_features)\n    inv_var    = 1.0 / variances                     # shape (n_classes, n_features)\n\n    # Bring dimensions in line for vectorised broadcasting\n    X_test_exp = X_test[:, None, :]                  # (N_test, 1,         M)\n    means_exp  = means[None, :, :]                   # (1,       n_classes, M)\n    log_var_exp= log_var[None, :, :]                 # (1,       n_classes, M)\n    inv_var_exp= inv_var[None, :, :]                 # (1,       n_classes, M)\n\n    # Gaussian log-likelihood part (excluding constant log(2\u03c0))\n    sq_diff     = (X_test_exp - means_exp) ** 2\n    ll_part1    = -0.5 * np.sum(log_var_exp, axis=2)           # (N_test, n_classes)\n    ll_part2    = -0.5 * np.sum(sq_diff * inv_var_exp, axis=2) # (N_test, n_classes)\n\n    # Add constant \u22120.5*M*log(2\u03c0)\n    const = -0.5 * n_features * np.log(2.0 * np.pi)\n    log_likelihood = const + ll_part1 + ll_part2                # (N_test, n_classes)\n\n    # Log-posterior \u221d log prior + log likelihood\n    log_post = log_likelihood + log_priors                      # broadcasting\n\n    # ---------------------------------------------------------------------\n    # 4.  Predict the class with the highest log-posterior\n    # ---------------------------------------------------------------------\n    preds = classes[np.argmax(log_post, axis=1)]\n\n    return preds\n```"}
{"task_id": 405, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport re\n\n\n_LEAKY_RELU_RE = re.compile(\n    r\"\"\"\n    ^\\s*leaky\\s*relu              # keyword\n    (?:                           # optional \"(alpha=<float>)\"\n        \\s*\\(\\s*alpha\\s*=\\s*\n        (?P<alpha>[+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)   # float literal\n        \\s*\\)\\s*\n    )?$\n    \"\"\",\n    re.IGNORECASE | re.VERBOSE,\n)\n\n_AFFINE_RE = re.compile(\n    r\"\"\"\n    ^\\s*affine\n    \\s*\\(\\s*\n        slope\\s*=\\s*\n        (?P<slope>[+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\n        \\s*,\\s*\n        intercept\\s*=\\s*\n        (?P<intercept>[+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\n    \\s*\\)\\s*$\n    \"\"\",\n    re.IGNORECASE | re.VERBOSE,\n)\n\n\ndef _sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Numerically stable logistic\u2010sigmoid.\n    \"\"\"\n    # Split by sign to avoid overflow.\n    pos_mask = z >= 0\n    neg_mask = ~pos_mask\n    out = np.empty_like(z)\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n    exp_z = np.exp(z[neg_mask])\n    out[neg_mask] = exp_z / (1.0 + exp_z)\n    return out\n\n\ndef apply_activation(x, param=None):\n    \"\"\"Apply a chosen activation to the input vector.\n\n    Args:\n        x (list[float] | np.ndarray): Input data. Anything that can be\n            converted to a NumPy array of floats is accepted.\n        param (str | None, optional): Description of the activation. See the\n            task specification for the accepted values. If *None* the\n            identity function is applied.\n\n    Returns:\n        list[float]: Result of applying the activation element-wise, rounded\n            to six decimals and converted to a standard Python *list*.\n    \"\"\"\n    # Convert input to float array\n    arr = np.asarray(x, dtype=float)\n\n    # ------------------------\n    # Identity (default)\n    # ------------------------\n    if param is None or (isinstance(param, str) and param.strip().lower() == \"identity\"):\n        activated = arr\n\n    # ------------------------\n    # Simple named activations\n    # ------------------------\n    elif isinstance(param, str) and param.strip().lower() in {\"relu\", \"tanh\", \"sigmoid\"}:\n        name = param.strip().lower()\n        if name == \"relu\":\n            activated = np.maximum(0.0, arr)\n        elif name == \"tanh\":\n            activated = np.tanh(arr)\n        else:  # \"sigmoid\"\n            activated = _sigmoid(arr)\n\n    # ------------------------\n    # Leaky-ReLU\n    # ------------------------\n    elif isinstance(param, str) and _LEAKY_RELU_RE.match(param):\n        m = _LEAKY_RELU_RE.match(param)\n        alpha = float(m.group(\"alpha\")) if m.group(\"alpha\") is not None else 0.01\n        activated = np.where(arr > 0, arr, alpha * arr)\n\n    # ------------------------\n    # Affine\n    # ------------------------\n    elif isinstance(param, str) and _AFFINE_RE.match(param):\n        m = _AFFINE_RE.match(param)\n        slope = float(m.group(\"slope\"))\n        intercept = float(m.group(\"intercept\"))\n        activated = slope * arr + intercept\n\n    # ------------------------\n    # Unknown activation\n    # ------------------------\n    else:\n        raise ValueError(f\"Unknown activation: {param}\")\n\n    # Round results and convert to Python list\n    rounded = np.round(activated, 6)\n\n    # Preserve original dimensionality except ensure list for scalars\n    if rounded.ndim == 0:\n        return [rounded.item()]\n    return rounded.tolist()\n```"}
{"task_id": 406, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_predict(X_train:np.ndarray y_train:np.ndarray                X_test:np.ndarray k: int = 5) -> list[int]:\n   k-Nearest Neighbors classifier (majority vote, Euclidean distance).\n\n    Parameters\n    ----------\n    X_train : np.ndarray, shape (n_train, n_features)\n        Training samples.\n    y_train : np.ndarray, shape (n_train,)\n        Non-negative integer class labels for `X_train`.\n    X_test  : np.ndarray, shape (n_test, n_features)\n        Samples to classify.\n    k : int, default=5\n        Number of neighbors used for voting.\n\n    Returns\n    -------\n    list[int]\n        Predicted label for each sample in `X_test` (same order).\n       # Ensure `k` does not exceed the number of available training samples\n    k = min(k, len(y_train))\n    predictions: list[int] = []\n\n    # For every test sample compute distances, find k nearest neighbors,\n    # then decide the majority label.\n    for x in X_test:\n        # 1. Euclidean distances to all training points\n        #    (squared distances are enough for ranking)\n        diff = X_train - x          # shape (n_train, n_features)\n        dists = np.einsumij,ij->i diff, diff)  # squared L2 norm, shape (n_train,)\n\n        # 2. Indices of the k smallest distances\n        idx = np.argpartition(dists, k - 1)[:k]\n\n        # 3. Corresponding labels\n        neighbor_labels = y_train[idx]\n\n        # 4. Majority vote (ties broken toward smaller label via argmax)\n        vote = int(np.bincount(neighbor_labels).argmax())\n\n        # 5. Store prediction\n        predictions.append(vote)\n\n    return predictions\n```"}
{"task_id": 410, "completion_id": 0, "completion": "```python\nimport re\n\n# reusable numeric-literal pattern:  3   -2   0.5   -.75   +4.\n_NUM = r'[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)'\n\ndef parse_activation(act_str: str):\n   Convert an activation-function description string into a dict.\n\n    Legal forms (no leading / trailing blanks):\n        \u2022ReLU        \u2022Sigmoid        \u2022Tanh        \u2022Affine(slope=<number>, intercept=<number>)        \u2022Leaky ReLU(alpha=<number>)    All returned numeric values are floats; otherwise \u22121 is returned.\n       # 1) Plain, non-parameterised activations -------------------------------\n    if act_str inReLUSigmoidTanh:\n        returntype act_str}\n\n    # 2) Affine(slope=\u2026, intercept=\u2026) --------------------------------------\n    affine_pat = rf'''\n        ^Affine\\(                       # literal header\n        \\s*slope\\s*=\\s*({_NUM})        # slope\n        \\s*,\\s*                        # comma separator\n        intercept\\s*=\\s*({_NUM})       # intercept\n        \\s*\\)$                         # closing parenthesis\n    '''\n    m = re.fullmatch(affine_pat, act_str, re.VERBOSE)\n    if m:\n        slope, intercept = map(float, m.groups())\n        returntypeAffineslope slope,intercept intercept}\n\n    # 3) Leaky ReLU(alpha=\u2026) -----------------------------------------------\n    leaky_pat = rf'''\n        ^Leaky\\ +ReLU\\(                #Leaky ReLU        \\s*alpha\\s*=\\s*({_NUM})        # alpha\n        \\s*\\)$                         #    '''\n    m = re.fullmatch(leaky_pat, act_str, re.VERBOSE)\n    if m:\n        (alpha,) = m.groups()\n        returntypeLeaky ReLUalpha float(alpha)}\n\n    # ----------------------------------------------------------------------\n    return -1\n```"}
{"task_id": 411, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections.abc import Iterable\n\ndef _is_scalar(x) -> bool:\n   Return True if *x* should be treated as a scalar    return np.isscalar(x)\n\ndef _non_integer_float(x) -> bool:\n   Return True when *x* is a float that is NOT an integer    if np.issubdtype(type(x), np.floating):\n        return not float(x).is_integer()\n    return False\n\n\ndef _process(values: list):\n       Inspect *values* (actions OR observations) and return\n    the requested statistics for this single list.\n       if len(values) == 0:\n        raise ValueErrorInput lists must not be empty    # ------------------------------------------------------------------\n    # 1) tuple_flag  ----------------------------------------------------\n    # ------------------------------------------------------------------\n    tuple_flag = any(\n        isinstance(v, (list, tuple, np.ndarray)) and not _is_scalar(v) for v in values\n    )\n\n    # ------------------------------------------------------------------\n    # 2) dimensionality -------------------------------------------------\n    # ------------------------------------------------------------------\n    if tuple_flag:\n        # first iterable element defines dimensionality\n        for v in values:\n            if isinstance(v, (list, tuple, np.ndarray)) and not _is_scalar(v):\n                dim = len(v)\n                break\n    else:  # scalar data\n        dim = 1\n\n    multidim_flag = dim > 1\n\n    # ------------------------------------------------------------------\n    # 3) unique ids per dimension --------------------------------------\n    # ------------------------------------------------------------------\n    unique_per_dim = [set() for _ in range(dim)]\n    continuous_flag = False\n\n    for v in values:\n        if _is_scalar(v):\n            comp = [v]            # length 1\n        else:                     # iterable -> convert to list\n            comp = v.tolist() if isinstance(v, np.ndarray) else list(v)\n\n        for d, val in enumerate(comp):\n            if _non_integer_float(val):\n                continuous_flag = True\n            unique_per_dim[d].add(val)\n\n    ids_per_dim = [sorted(list(s)) for s in unique_per_dim]\n    n_per_dim   = [len(s) for s in unique_per_dim]\n\n    return {\n       tuple_flag  tuple_flag,\n       multidim    multidim_flag,\n       continuous  continuous_flag,\n       n_per_dim   n_per_dim,\n       dim         dim,\n       ids_per_dim ids_per_dim,\n    }\n\n\ndef env_stats(actions: list, observations: list) -> dict:\n   Compute statistics for collections of actions and observations    a = _process(actions)\n    o = _process(observations)\n\n    return {\n       tuple_actions          atuple_flag       tuple_observations     otuple_flag       multidim_actions       amultidim       multidim_observations  omultidim       continuous_actions     acontinuous       continuous_observationsocontinuous       n_actions_per_dim      an_per_dim       action_dim             adim       n_obs_per_dim          on_per_dim       obs_dim                odim       action_ids             aids_per_dim       obs_ids                oids_per_dim    }\n```\n\nThe helper `_process` does the heavy lifting for one list (actions or observations):\n\n1. Detects whether at least one element is an iterable (`tuple_flag`).\n2. Determines dimensionality (`dim`) and whether it is multi-dimensional (`multidim`).\n3. Collects unique values per dimension, counts them, and decides if any of them are genuine non-integer floats (`continuous`).\n\n`env_stats` simply applies this helper to both input lists and assembles the dictionary using the exact keys required by the specification."}
{"task_id": 412, "completion_id": 0, "completion": "```python\ndef get_metric(name: str):\n    \"\"\"Return a binary-classification evaluation metric by *name*.\n\n    Supported names:\n        - 'accuracy'\n        - 'precision'\n        - 'recall'\n        - 'f1'\n\n    If *name* is not one of the above, return -1.\n\n    Each metric function returned accepts two equal-length iterables\n    (y_true, y_pred) containing only 0s and 1s and returns the metric\n    value rounded to 4 decimal places.\n    \"\"\"\n\n    # ---------- metric helpers ----------\n    def _basic_counts(y_true, y_pred):\n        \"\"\"Return TP, FP, FN, TN counts.\"\"\"\n        tp = fp = fn = tn = 0\n        for t, p in zip(y_true, y_pred):\n            if t == 1 and p == 1:\n                tp += 1\n            elif t == 0 and p == 1:\n                fp += 1\n            elif t == 1 and p == 0:\n                fn += 1\n            else:                       # t == 0 and p == 0\n                tn += 1\n        return tp, fp, fn, tn\n\n    # ---------- metric definitions ----------\n    def accuracy(y_true, y_pred):\n        total = len(y_true)\n        correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n        return round(correct / total, 4)\n\n    def precision(y_true, y_pred):\n        tp, fp, _, _ = _basic_counts(y_true, y_pred)\n        denom = tp + fp\n        score = 0 if denom == 0 else tp / denom\n        return round(score, 4)\n\n    def recall(y_true, y_pred):\n        tp, _, fn, _ = _basic_counts(y_true, y_pred)\n        denom = tp + fn\n        score = 0 if denom == 0 else tp / denom\n        return round(score, 4)\n\n    def f1(y_true, y_pred):\n        tp, fp, fn, _ = _basic_counts(y_true, y_pred)\n        prec_denom = tp + fp\n        rec_denom = tp + fn\n\n        prec = 0 if prec_denom == 0 else tp / prec_denom\n        rec = 0 if rec_denom == 0 else tp / rec_denom\n\n        if prec == 0 and rec == 0:\n            score = 0\n        else:\n            score = 2 * prec * rec / (prec + rec)\n        return round(score, 4)\n\n    # ---------- name \u2192 function mapping ----------\n    metric_map = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n    return metric_map.get(name, -1)\n```"}
{"task_id": 413, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gbdt_regression(\n        X_train: np.ndarray, \n        y_train: np.ndarray, \n        X_test:  np.ndarray, \n        n_estimators: int = 10, \n        learning_rate: float = 0.1, \n        max_depth: int = 3) -> list[float]:\n    \"\"\"Very small-scale Gradient Boosting Decision Tree regressor written\n    with nothing but NumPy.  It uses squared\u2013error loss, CART-style binary\n    splits and simple greedy tree growth.\"\"\"\n    \n    # ------------------------------------------------------------------ #\n    # ----------------------- helper: build tree ----------------------- #\n    # ------------------------------------------------------------------ #\n    def build_tree(idxs, depth):\n        \"\"\"Recursively grow a regression tree on the residuals.\"\"\"\n        y_node = y_res[idxs]\n        # Leaf stopping conditions\n        if depth == max_depth or len(idxs) <= 1 or np.all(y_node == y_node[0]):\n            return {'is_leaf': True, 'value': y_node.mean()}\n        \n        # Current (parent) SSE\n        sse_parent = ((y_node - y_node.mean())**2).sum()\n        best_feat, best_thr, best_sse = None, None, sse_parent\n        \n        # Enumerate all candidate splits\n        for feat in range(n_features):\n            X_col   = X_train[idxs, feat]\n            unique  = np.unique(X_col)\n            if unique.size == 1:          # can\u2019t split on constant feature\n                continue\n            thresholds = (unique[:-1] + unique[1:]) / 2.0  # mid-points\n            \n            for thr in thresholds:\n                left_mask  = X_col <= thr\n                right_mask = ~left_mask\n                if not left_mask.any() or not right_mask.any():\n                    continue\n                left_y  = y_node[left_mask]\n                right_y = y_node[right_mask]\n                \n                # Sum of squared errors of children\n                sse_left  = ((left_y  - left_y.mean())**2).sum()\n                sse_right = ((right_y - right_y.mean())**2).sum()\n                sse_total = sse_left + sse_right\n                \n                if sse_total < best_sse - 1e-7:   # tiny tolerance\n                    best_sse     = sse_total\n                    best_feat    = feat\n                    best_thr     = thr\n                    \n        # If no split improved the loss, make this a leaf\n        if best_feat is None:\n            return {'is_leaf': True, 'value': y_node.mean()}\n        \n        # Otherwise create internal node\n        left_idxs  = idxs[X_train[idxs, best_feat] <= best_thr]\n        right_idxs = idxs[X_train[idxs, best_feat] >  best_thr]\n        \n        return {\n            'is_leaf': False,\n            'feature': best_feat,\n            'threshold': best_thr,\n            'left':  build_tree(left_idxs,  depth + 1),\n            'right': build_tree(right_idxs, depth + 1)\n        }\n    \n    # ------------------------------------------------------------------ #\n    # ----------------------- helper: predict -------------------------- #\n    # ------------------------------------------------------------------ #\n    def predict_tree(tree, X):\n        \"\"\"Vectorised prediction for one tree.\"\"\"\n        out = np.empty(X.shape[0], dtype=float)\n        for i, x in enumerate(X):\n            node = tree\n            while not node['is_leaf']:\n                if x[node['feature']] <= node['threshold']:\n                    node = node['left']\n                else:\n                    node = node['right']\n            out[i] = node['value']\n        return out\n    \n    # ------------------------------------------------------------------ #\n    # ----------------------   boosting loop   ------------------------- #\n    # ------------------------------------------------------------------ #\n    n_samples, n_features = X_train.shape\n    \n    # Initial prediction: mean of targets\n    init_pred = y_train.mean()\n    train_pred = np.full(n_samples, init_pred, dtype=float)\n    test_pred  = np.full(X_test.shape[0], init_pred, dtype=float)\n    \n    for _ in range(n_estimators):\n        # 1. Negative gradient / residuals\n        y_res = y_train - train_pred\n        \n        # 2. Fit regression tree to residuals\n        idx_all = np.arange(n_samples)\n        tree = build_tree(idx_all, depth=0)\n        \n        # 3. Update predictions\n        update_train = predict_tree(tree, X_train)\n        update_test  = predict_tree(tree, X_test)\n        \n        train_pred += learning_rate * update_train\n        test_pred  += learning_rate * update_test\n    \n    # ------------------------------------------------------------------ #\n    # --------------------------  output  ------------------------------ #\n    # ------------------------------------------------------------------ #\n    return [round(float(p), 4) for p in test_pred]\n```\nThe routine follows the requested steps:\n\n1. Starts with the mean of `y_train`.\n2. Each iteration computes residuals, fits a depth-limited CART tree (using greedy\n   variance reduction) with NumPy only, and adds the tree\u2019s prediction scaled by\n   `learning_rate`.\n3. After `n_estimators` rounds the final predictions for `X_test` are returned,\n   each rounded to four decimal places and packed in a regular Python list."}
{"task_id": 414, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef exponential_scheduler(step: int,\n                          initial_lr: float = 0.01,\n                          stage_length: int = 500,\n                          staircase: bool = False,\n                          decay: float = 0.1) -> float:\n   Return the exponentially decayed learning-rate for a given optimisation step    \n    # -----------------------------  sanity checks  -----------------------------\n    # step must be a non-negative integer\n    if not isinstance(step, int) or step < 0:\n        return -1\n    \n    # stage_length must be a strictly positive integer\n    if not isinstance(stage_length, int) or stage_length <= 0:\n        return -1\n    \n    # decay must be strictly positive\n    if decay <= 0:\n        return -1\n    \n    # ---------------------------  compute curr_stage  --------------------------\n    if staircase:\n        # integer (floor) stage index\n        curr_stage = step // stage_length\n    else:\n        # smooth / fractional stage index\n        curr_stage = step / stage_length\n    \n    # ----------------------------  learning-rate  ------------------------------\n    lr = initial_lr * (decay ** curr_stage)\n    \n    # round to 6 decimal places as required\n    return round(lr, 6)\n```"}
{"task_id": 415, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dct2(frame: list[int | float]) -> list[float]:\n    \"\"\"\n    Compute the un-normalised, one\u2013dimensional type-II Discrete Cosine Transform\n    (DCT-II) for a real-valued input frame.\n\n    Parameters\n    ----------\n    frame : list[int | float]\n        Input samples (length N).  May contain ints and/or floats.\n\n    Returns\n    -------\n    list[float]\n        List of length N containing the DCT-II coefficients rounded to\n        four decimal places.  If `frame` is empty, an empty list is returned.\n    \"\"\"\n    # Convert to NumPy array of floats\n    x = np.asarray(frame, dtype=float)\n    N = x.size\n\n    # Empty input \u2192 empty output\n    if N == 0:\n        return []\n\n    # Indices\n    n = np.arange(N)                         # shape (N,)\n    k = np.arange(N)                         # shape (N,)\n\n    # Basis matrix:  cos( \u03c0/N \u00b7 (n + 0.5) \u00b7 k )\n    # Shape (N, N)  \u2013 rows: n,  columns: k\n    basis = np.cos((np.pi / N) * (n[:, None] + 0.5) * k[None, :])\n\n    # Dot-product:  X_k = \u03a3_n x_n * basis[n, k]\n    coeffs = x @ basis                       # shape (N,)\n\n    # Round to 4 decimal places and convert to Python list\n    return np.round(coeffs, 4).tolist()\n```"}
{"task_id": 416, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Compute the probability-density values of a multivariate normal distribution.\n\n    Parameters\n    ----------\n    X    : array-like, shape (n_samples, n_features)  or  (n_samples,)\n           Points at which the pdf is evaluated. If a 1-D array is supplied it\n           is interpreted as n_samples points in one dimension.\n    mean : array-like, length n_features  or  scalar for 1-D case\n           Mean vector \u03bc of the distribution.\n    cov  : array-like, shape (n_features, n_features)\n           Positive-definite covariance matrix \u03a3.\n\n    Returns\n    -------\n    list\n        List whose i-th element is the pdf value for X[i] rounded\n        to four decimal places.\n    \"\"\"\n    # --- Normalise the inputs -------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    mean = np.asarray(mean, dtype=float)\n    cov = np.asarray(cov,  dtype=float)\n\n    # If X is one-dimensional treat it as (n_samples, 1)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    # When mean is a scalar (1-D case) turn it into length-1 vector\n    if mean.ndim == 0:\n        mean = np.array([mean], dtype=float)\n    mean = mean.ravel()                       # ensure shape (n_features,)\n\n    n_samples, d = X.shape\n\n    # --- Pre-compute constant part of the formula -----------------------------\n    det_cov = np.linalg.det(cov)\n    if det_cov <= 0:\n        raise ValueError(\"Covariance matrix must be positive-definite.\")\n\n    inv_cov = np.linalg.inv(cov)\n\n    prefactor = 1.0 / math.sqrt((2 * math.pi) ** d * det_cov)\n\n    # --- Mahalanobis distance for each sample ---------------------------------\n    diff = X - mean            # shape (n_samples, d)\n    # quadratic form (x-\u03bc)^T \u03a3^{-1} (x-\u03bc)  for all samples\n    quad = np.sum(diff.dot(inv_cov) * diff, axis=1)\n\n    # --- Final pdf values -----------------------------------------------------\n    pdf_vals = prefactor * np.exp(-0.5 * quad)\n\n    # Round to 4 decimal places and return as plain Python list\n    return np.round(pdf_vals, 4).tolist()\n```\n\nExplanation of the main steps\n1. The inputs are converted to NumPy arrays.  \n2. A one-dimensional `X` is reshaped into `(n_samples, 1)` so that the same code works for both univariate and multivariate cases.  \n3. The constant front factor  \n      1 / \u221a[(2\u03c0)^d det(\u03a3)]  \n   is computed once.  \n4. For each sample the quadratic Mahalanobis term  \n      (x \u2212 \u03bc)^T \u03a3\u207b\u00b9 (x \u2212 \u03bc)  \n   is evaluated in vectorised form.  \n5. The final pdf values are obtained, rounded to four decimal places, and returned as a Python list."}
{"task_id": 417, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int = 1) -> tuple[list[list[float]], list[list[float]]]:\n   Perform Principal Component Analysis and return the projected data and its reconstruction.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int, default=1\n        Number of principal components to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[list[float]]]\n        * First element  \u2013 the data expressed in the *k*-dimensional PCA space.\n        * Second element \u2013 the data reconstructed back in the original space.\n\n        Both matrices are rounded to 4 decimal places.\n       # ---- 1. input checks & basic shapes ------------------------------------\n    X = np.asarray(data, dtype=float)\n    n_samples, n_features = X.shape\n    k = int(k)\n    if k < 1:\n        raise ValueErrork must be a positive integer    # ---- 2. mean-centering --------------------------------------------------\n    mean = X.mean(axis=0)\n    Xc = X - mean\n\n    # ---- 3. unbiased covariance matrix -------------------------------------\n    if n_samples > 1:\n        C = (Xc.T @ Xc) / (n_samples - 1)\n    else:                                   # degenerate case: only one sample\n        C = np.zeros((n_features, n_features), dtype=float)\n\n    # ---- 4. eigen-decomposition & sorting ----------------------------------\n    eigvals, eigvecs = np.linalg.eigh(C)    # ascending order\n    idx = np.argsort(eigvals)[::-1]         # descending\n    eigvecs = eigvecs[:, idx]\n\n    # ---- 5. keep first k eigenvectors --------------------------------------\n    k = min(k, n_features)\n    W = eigvecs[:, :k].copy()               # (features \u00d7 k)\n\n    # ---- 6. deterministic orientation --------------------------------------\n    for i in range(k):\n        v = W[:, i]\n        nz = np.flatnonzero(v)\n        if nz.size:                         # at least one non-zero component\n            if v[nz[0]] < 0:                # first non-zero component negative\n                W[:, i] = -v\n\n    # ---- 7. projection & reconstruction ------------------------------------\n    Z = Xc @ W                              # low-dimensional representation\n    X_hat = Z @ W.T + mean                  # reconstruction in original space\n\n    # ---- 8. rounding to 4 decimal places -----------------------------------\n    Z_round = np.round(Z, 4)\n    X_hat_round = np.round(X_hat, 4)\n\n    # ---- 9. return as Python lists -----------------------------------------\n    return Z_round.tolist(), X_hat_round.tolist()\n```"}
{"task_id": 418, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef calculate_covariance_matrix(\n    X: np.ndarray | list[list[int | float]]\n) -> list[list[float]]:\n   Compute and return the sample covariance matrix of *X*.\n\n    Args:\n        X: A 2-D NumPy array or a nested Python list where rows correspond to\n           samples and columns correspond to features.\n\n    Returns:\n        The sample covariance matrix as a nested Python list, with every entry\n        rounded to 4 decimal places.\n       # ------------------------------------------------------------------\n    # 1. Convert input to a NumPy array of type float for safe arithmetic\n    # ------------------------------------------------------------------\n    X = np.asarray(X, dtype=float)\n\n    # Handle the corner-case that a 1-D array was supplied (single feature)\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    # ------------------------------------------------------------------\n    # 2. Deal with an insufficient number of samples (n < 2)\n    # ------------------------------------------------------------------\n    n_samples, n_features = X.shape if X.size else (0, 0)\n    if n_samples < 2:\n        return np.zeros((n_features, n_features)).round(4).tolist()\n\n    # ------------------------------------------------------------------\n    # 3. Center the data and compute sample covariance (divide by n-1)\n    # ------------------------------------------------------------------\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n\n    # ------------------------------------------------------------------\n    # 4. Round to 4 decimal places and return as nested Python list\n    # ------------------------------------------------------------------\n    return np.round(cov_matrix, 4).tolist()\n```"}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (kept twice to match the stub)\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,          # ignored for MAP but kept for signature\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n   Return MAP predictive means for every row in `X_new`.\n\n    Parameters follow the specification given in the task description.\n       # --- 1. Design matrices -------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    X_new = np.asarray(X_new, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()          # ensure shape (N,)\n\n    if fit_intercept:\n        ones_train = np.ones((X.shape[0], 1))\n        ones_test  = np.ones((X_new.shape[0], 1))\n        X_design   = np.hstack([ones_train, X])\n        X_new_d    = np.hstack([ones_test,  X_new])\n    else:\n        X_design = X\n        X_new_d  = X_new\n\n    N, D = X_design.shape\n\n    # --- 2. Prior mean ------------------------------------------------------\n    if np.isscalar(mu):\n        mu_vec = np.full(D, float(mu))\n    else:\n        mu_arr = np.asarray(mu, dtype=float).ravel()\n        if mu_arr.size == 1:                         # treat length-1 array as scalar\n            mu_vec = np.full(D, float(mu_arr.item()))\n        elif mu_arr.size == D:\n            mu_vec = mu_arr\n        else:\n            raise ValueErrorPrior mean `mu` has incompatible length    # --- 3. Prior covariance matrix V --------------------------------------\n    if V is None:\n        V_mat = np.eye(D)\n    elif np.isscalar(V):\n        V_mat = np.eye(D) * float(V)\n    else:\n        V_arr = np.asarray(V, dtype=float)\n        if V_arr.ndim == 1:                          # diagonal specified\n            if V_arr.size != D:\n                raise ValueErrorDiagonal prior `V` has wrong length            V_mat = np.diag(V_arr)\n        elif V_arr.ndim == 2:\n            if V_arr.shape != (D, D):\n                raise ValueErrorPrior matrix `V` has wrong shape            V_mat = V_arr\n        else:\n            raise ValueError`V` must be scalar, 1-D or 2-D    # --- 4. Posterior parameters (MAP) -------------------------------------\n    # Compute inverse(V) once\n    V_inv = np.linalg.inv(V_mat)\n\n    XtX = X_design.T @ X_design\n    Sigma_p_inv = V_inv + XtX            # (D x D)\n\n    # Instead of inverting again, solve the linear system for \u03bc_p\n    rhs = V_inv @ mu_vec + X_design.T @ y      # (D,)\n    mu_p = np.linalg.solve(Sigma_p_inv, rhs)   # (D,)\n\n    # --- 5. Predictive mean for new data -----------------------------------\n    y_pred = X_new_d @ mu_p                    # (n_new,)\n\n    # --- 6. Return rounded list --------------------------------------------\n    return [round(float(v), 4) for v in y_pred]\n```"}
{"task_id": 420, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_split_mask(X: np.ndarray, column: int, value: float) -> tuple[list[bool], list[bool]]:\n   Generate Boolean masks that split rows of *X* on a given threshold.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    column : int\n        Index of the column (feature) used for the split.\n    value : float\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple[list[bool], list[bool]]\n        A pair *(left_mask, right_mask)* where `left_mask[i]` is `True` iff\n        `X[i, column] < value` and `right_mask[i]` is `True` iff\n        `X[i, column] >= value`.  If *column* is out of bounds, two empty\n        lists are returned.\n       # Guard against invalid column indices\n    if not (0 <= column < X.shape[1]):\n        return [], []\n\n    # Compute boolean masks\n    left_mask_arr = X[:, column] < value\n    right_mask_arr = X[:, column] >= value   # could also be ~left_mask_arr\n\n    # Convert ndarray masks to plain Python lists\n    return left_mask_arr.tolist(), right_mask_arr.tolist()\n```"}
{"task_id": 421, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n   Density-Based Spatial Clustering (DBSCAN) implemented from scratch.\n\n    Args\n    ----\n    data : ndarray, shape (n_samples, n_features)\n        Data set to cluster.\n    eps : float\n        Maximum distance for two points to be considered neighbours.\n    min_samples : int\n        Minimum number of points in an `eps`-neighbourhood to turn a point into\n        a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points are marked **-1**,\n        clusters are numbered 1, 2, 3 \u2026\n       if data.ndim != 2:\n        raise ValueError`data` must be a 2-D array (n_samples, n_features).    n_samples = data.shape[0]\n    # 0  \u2026 unvisited / unlabelled\n    # -1 \u2026 noise\n    # 1,2,\u2026 cluster indices\n    labels = np.zeros(n_samples, dtype=int)\n\n    def region_query(idx: int) -> np.ndarray:\n               Return indices of all points whose distance to point *idx*\n        is \u2264 eps (including *idx* itself).\n               # Euclidean distance; broadcasting keeps the implementation short\n        diff = data - data[idx]                     # (n_samples, n_features)\n        dist = np.sqrt(np.sum(diff * diff, axis=1)) # (n_samples,)\n        return np.where(dist <= eps)[0]\n\n    cluster_id = 0\n\n    for idx in range(n_samples):\n        # Skip already processed points\n        if labels[idx] != 0:\n            continue\n\n        # Find neighbours of point *idx*\n        neighbours = region_query(idx)\n\n        # Not enough points to form a core \u2192 mark as noise\n        if neighbours.size < min_samples:\n            labels[idx] = -1\n            continue\n\n        # Found a new cluster\n        cluster_id += 1\n        labels[idx] = cluster_id\n\n        # All direct neighbours to be processed (queue / stack)\n        # We copy to list to allow `.pop()` in O(1)\n        seeds = neighbours.tolist()\n        seeds.remove(idx)           # idx already labelled\n\n        # Expand cluster\n        while seeds:\n            current = seeds.pop()\n\n            # Previously labelled as noise \u2192 now part of the cluster\n            if labels[current] == -1:\n                labels[current] = cluster_id\n\n            # Already assigned to any cluster \u2192 nothing to do\n            if labels[current] != 0:\n                continue\n\n            # Assign to current cluster\n            labels[current] = cluster_id\n\n            # If *current* is a core point, add its neighbours\n            current_neigh = region_query(current)\n            if current_neigh.size >= min_samples:\n                # Only points that are still unvisited (label == 0 or -1)\n                # are appended to `seeds`\n                unlabelled = [p for p in current_neigh\n                               if labels[p] in (0, -1)]  # 0: unvisited, -1: noise\n                seeds.extend(unlabelled)\n\n    return labels.tolist()\n```\n\nHow it works\n-------------\n1. `region_query(idx)` finds all indices within `eps` of point `idx` using\n   NumPy vectorisation (no third-party ML tools).\n2. The main loop scans the data once, keeping cluster numbers in the order\n   in which core points are first met.\n3. Unvisited points are\n   \u2022 declared **noise** (`-1`) if they are not core points,  \n   \u2022 otherwise a **new cluster** is born and grown breadth-first by repeatedly\n     adding all density-reachable points.\n4. The resulting `labels` array is converted to `list[int]` to match the\n   required return type.\n\nComplexity\n----------\nA neighbour search is performed whenever a point is examined; the na\u00efve\nimplementation shown is `O(n\u00b2)` in time and `O(1)` in additional memory.  This\nkeeps the code short and dependency-free; for very large data sets one would\nreplace `region_query` with a spatial index (k-d tree, ball tree, \u2026)."}
{"task_id": 422, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x: \"np.ndarray | list | tuple | float | int\", order: int = 0):\n    \"\"\"SoftPlus activation function and its first two derivatives.\n\n    Parameters\n    ----------\n    x : float | int | list | tuple | np.ndarray\n        Input data on which SoftPlus or its derivatives will be computed.\n    order : int, optional\n        0  -> SoftPlus(x)\n        1  -> SoftPlus'(x)\n        2  -> SoftPlus''(x)\n        Any other value makes the function return -1.\n\n    Returns\n    -------\n    float | list\n        Result rounded to 4 decimal places (Python native types).\n        If the order is invalid, returns -1.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Validate `order`\n    # ------------------------------------------------------------------ #\n    if order not in (0, 1, 2):\n        return -1\n\n    # ------------------------------------------------------------------ #\n    # Convert the input to a NumPy array for vectorised computation\n    # ------------------------------------------------------------------ #\n    x_arr = np.asarray(x, dtype=float)          # keeps shape (incl. scalar 0-D)\n\n    # ------------------------------------------------------------------ #\n    # Core computations\n    # ------------------------------------------------------------------ #\n    if order == 0:\n        # Numerically-stable SoftPlus:\n        #   softplus(x) = log1p(exp(-|x|)) + max(x, 0)\n        result = np.where(\n            x_arr > 0,\n            x_arr + np.log1p(np.exp(-x_arr)),\n            np.log1p(np.exp(x_arr))\n        )\n\n    elif order == 1:\n        # First derivative \u2013 logistic sigmoid (stable formulation)\n        result = 1.0 / (1.0 + np.exp(-x_arr))\n\n    else:  # order == 2\n        # Second derivative \u2013 sigmoid * (1 - sigmoid)\n        sigmoid = 1.0 / (1.0 + np.exp(-x_arr))\n        result = sigmoid * (1.0 - sigmoid)\n\n    # ------------------------------------------------------------------ #\n    # Round to 4 decimals and cast back to native Python types\n    # ------------------------------------------------------------------ #\n    result = np.round(result, 4)\n\n    # If the original input was a scalar, return a scalar float\n    if np.isscalar(x) or result.shape == ():\n        return float(result)\n\n    # Otherwise, return a (nested) list with the same shape\n    return result.tolist()\n```"}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"\n    Compute the Root Mean Squared Logarithmic Error (RMSLE) between two equally-sized\n    sequences of non-negative numbers.\n    \n    Parameters\n    ----------\n    actual : list[float]\n        Ground-truth target values (must be non-negative).\n    predicted : list[float]\n        Model-predicted values (must be non-negative).\n    \n    Returns\n    -------\n    float\n        RMSLE rounded to four decimal places, or \u22121 if the inputs are invalid.\n    \"\"\"\n    # 1. Basic validation: same length\n    if len(actual) != len(predicted) or len(actual) == 0:\n        return -1\n\n    # 2. Convert to NumPy arrays for efficient computations\n    try:\n        a = np.asarray(actual, dtype=float)\n        p = np.asarray(predicted, dtype=float)\n    except (TypeError, ValueError):\n        return -1               # Non-numeric data present\n    \n    # 3. Non-negativity check\n    if (a < 0).any() or (p < 0).any():\n        return -1\n    \n    # 4. Compute RMSLE\n    log_diff = np.log1p(p) - np.log1p(a)   # log1p(x) = log(1 + x)\n    rmsle = np.sqrt(np.mean(np.square(log_diff)))\n    \n    # 5. Round to four decimal places\n    return round(float(rmsle), 4)\n```"}
{"task_id": 424, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef entropy(y: list[int]) -> float:\n    \"\"\"Compute the Shannon entropy of a label sequence.\n\n    Args:\n        y: A non-empty list of non-negative integers representing class labels.\n\n    Returns:\n        The Shannon entropy (in bits) of the sequence, rounded\n        to 4 decimal places.\n    \"\"\"\n    # Convert to NumPy array for vectorised operations\n    y_arr = np.asarray(y)\n    \n    # Count occurrences of each distinct label\n    _, counts = np.unique(y_arr, return_counts=True)\n    \n    # Relative frequencies (probabilities)\n    probs = counts / counts.sum()\n    \n    # Shannon entropy:  -\u03a3 p * log2(p)\n    entropy_val = -np.sum(probs * np.log2(probs))\n    \n    # Round to 4 decimal places and return as a Python float\n    return float(np.round(entropy_val, 4))\n```"}
{"task_id": 425, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef f_entropy(p: list[int] | np.ndarray) -> float:\n       Calculate the natural-log Shannon entropy of a one-dimensional collection\n    of non-negative integers.\n\n    Parameters\n    ----------\n    p : list[int] | np.ndarray\n        A 1-D sequence of non-negative integers (Python list or NumPy array).\n\n    Returns\n    -------\n    float\n        Shannon entropy H  rounded to 4 decimal places.  If the input contains\n        only a single unique symbol (or is empty) the function returns 0.0.\n       # Convert the input to a NumPy array (this works for lists as well)\n    x = np.asarray(p)\n\n    # Handle empty input explicitly\n    if x.size == 0:\n        return 0.0\n\n    # Obtain counts of each unique symbol\n    _, counts = np.unique(x, return_counts=True)\n\n    # If only one unique symbol, entropy is zero\n    if counts.size == 1:\n        return 0.0\n\n    # Convert counts to probabilities\n    probs = counts / counts.sum()\n\n    # Compute entropy using natural logarithm\n    entropy = -np.sum(probs * np.log(probs))\n\n    # Round to 4 decimal places and return as a regular Python float\n    return round(float(entropy), 4)\n```"}
{"task_id": 426, "completion_id": 0, "completion": "```python\ndef classification_accuracy(y_true: list, y_pred: list) -> float:\n   Compute the accuracy between the true and predicted class labels.\n\n    Args:\n        y_true (list): Ground-truth class labels.\n        y_pred (list): Predicted class labels produced by a model.\n\n    Returns:\n        float: Accuracy rounded to 4 decimal places, or -1 for invalid input.\n       # Validate inputs\n    if (\n        not isinstance(y_true, (list, tuple)) or\n        not isinstance(y_pred, (list, tuple)) or\n        len(y_true) != len(y_pred) or\n        len(y_true) == 0               # covers theempty requirement\n    ):\n        return -1\n\n    # Count correct predictions\n    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n\n    # Compute accuracy\n    accuracy = correct / len(y_true)\n\n    # Round to 4 decimal places\n    return round(accuracy, 4)\n```"}
{"task_id": 427, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef identity_activation(z):\n    \"\"\"Compute identity activation and its derivative.\n\n    Args:\n        z (list | tuple | numpy.ndarray): Input data of any numeric type and\n            shape.\n\n    Returns:\n        tuple[list, list]: A tuple (activation, derivative) where both elements\n            are Python lists matching the shape of the input.\n    \"\"\"\n    # Convert the input to a NumPy array (no copy if already ndarray)\n    arr = np.asarray(z)\n\n    # Identity activation: the output is identical to the input\n    activation = arr.copy()           # copy() to avoid side-effects\n\n    # Derivative of identity: ones with the same shape as the input\n    derivative = np.ones_like(arr)\n\n    # Convert results back to regular Python lists\n    return activation.tolist(), derivative.tolist()\n```\n\nThe function:\n\n1. Accepts lists, tuples, or NumPy arrays.\n2. Converts the input to a NumPy array for vectorised processing.\n3. Calculates:\n   \u2022 The identity activation (`activation`) \u2013 identical to the input.  \n   \u2022 The derivative (`derivative`) \u2013 an array of ones matching the input\u2019s shape.\n4. Converts both results to standard Python lists with `.tolist()` and returns them as a tuple `(activation, derivative)`."}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n   Generate a Hann (Hanning) window rounded to 4 decimal places.\n\n    Args:\n        window_len (int): Desired number of coefficients.\n        symmetric (bool, optional): ``True``  \u2013 classical symmetric window\n                                    ``False`` \u2013 periodic window (for FFT).\n                                    Default is ``False``.\n\n    Returns:\n        list[float]: Hann-window coefficients rounded to 4 dp.\n                     Returns an empty list when *window_len* \u2264 0.\n       # ---------- special-case handling ----------\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        # For a length-1 Hann window the lone sample is always 1.0\n        return [1.0]\n\n    # ---------- select length for underlying computation ----------\n    # Periodic window is obtained by computing a symmetric window\n    # one sample longer and discarding the last sample.\n    N = window_len if symmetric else window_len + 1\n\n    # ---------- compute the Hann window ----------\n    n = np.arange(N)\n    window = 0.5 - 0.5 * np.cos(2 * math.pi * n / (N - 1))\n\n    if not symmetric:               # drop the extra sample\n        window = window[:-1]\n\n    # ---------- round to 4 decimal places and convert to list ----------\n    window = np.round(window, 4)\n    return window.tolist()\n```"}
{"task_id": 429, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef oracle_payoff(context_probs: np.ndarray, context: np.ndarray) -> tuple[float, int]:\n   Oracle that returns the optimal expected reward and the corresponding arm.\n\n    Parameters\n    ----------\n    context_probs : np.ndarray\n        A 2-D array of shape (D, K) where each row represents a context and each\n        column an arm. Entry (i, j) is the success probability of arm *j* when\n        the active context is *i*.\n    context : np.ndarray\n        A one-hot vector of length *D* identifying the current context.\n\n    Returns\n    -------\n    tuple[float, int]\n        (optimal_rwd, optimal_arm): the best achievable expected reward (rounded\n        to four decimals) and the index of the arm that achieves it.  If the\n        input context is invalid the function returns (-1, \u20111).\n       # Ensure we are working with NumPy arrays\n    context_probs = np.asarray(context_probs)\n    context        = np.asarray(context)\n\n    # ---------- sanity checks for the context ----------\n    if (\n        context_probs.ndim != 2               # context_probs must be 2-D\n        or context.ndim != 1                  # context must be 1-D\n        or context.shape[0] != context_probs.shape[0]  # lengths must match\n    ):\n        return (-1, -1)\n\n    # A valid one-hot vector must contain exactly one 1 and the rest 0s\n    if np.count_nonzero(context == 1) != 1 or np.any((context != 0) & (context != 1)):\n        return (-1, -1)\n\n    # ---------- identify the active context ----------\n    ctx_idx: int = int(np.flatnonzero(context)[0])     # unique index where context == 1\n    row          = context_probs[ctx_idx]              # probabilities for this context\n\n    # ---------- compute optimal arm and reward ----------\n    optimal_arm  = int(np.argmax(row))                 # first argmax = smallest index tie-breaker\n    optimal_rwd  = float(np.round(row[optimal_arm], 4))\n\n    return (optimal_rwd, optimal_arm)\n```"}
{"task_id": 430, "completion_id": 0, "completion": "```python\nfrom typing import List, Dict, Set, Tuple, Optional\n\n\ndef topological_sort(vertices: list, edges: list[tuple]) -> list | None:\n    \"\"\"Return a deterministic topological order of a DAG or ``None`` if a cycle\n    exists.\n\n    The algorithm performs a depth-first search (DFS).  Exploration is\n    *deterministic* because\n        \u2022 roots are taken from the input list `vertices` from left to right;\n        \u2022 every outgoing neighbour list is traversed in that very same order.\n\n    Args\n    ----\n    vertices : list\n        The ordered list of vertices.\n    edges    : list[tuple]\n        List of directed edges (u, v) meaning u \u2192 v.\n\n    Returns\n    -------\n    list | None\n        A list with the vertices in topological order, or ``None`` when\n        a directed cycle is detected.\n    \"\"\"\n\n    # ------------------------------------------------------------\n    # 1. Pre-processing: adjacency list whose neighbours respect the\n    #    order induced by the original `vertices` sequence.\n    # ------------------------------------------------------------\n    pos: dict = {v: i for i, v in enumerate(vertices)}          # position map\n    adj: Dict = {v: [] for v in vertices}                       # adjacency list\n\n    for u, v in edges:\n        # Ignore edges that involve unknown vertices.  Alternatively,\n        # they could be added automatically \u2013 the spec only talks about\n        # the given `vertices` list, so we stay inside that universe.\n        if u in adj and v in pos:\n            adj[u].append(v)\n\n    # Sort each neighbour list according to the left-to-right order of `vertices`\n    for u in adj:\n        adj[u].sort(key=pos.__getitem__)\n\n    # ------------------------------------------------------------\n    # 2. DFS with colour-marking to detect back edges (cycles)\n    # ------------------------------------------------------------\n    WHITE, GREY, BLACK = 0, 1, 2\n    colour: Dict = {v: WHITE for v in vertices}\n    topo: list = []                                             # finishing order\n\n    def dfs(u) -> bool:\n        \"\"\"Returns False on cycle, True otherwise.\"\"\"\n        colour[u] = GREY\n        for v in adj[u]:\n            if colour[v] == GREY:          # back edge \u21d2 cycle\n                return False\n            if colour[v] == WHITE and not dfs(v):\n                return False\n        colour[u] = BLACK\n        topo.append(u)                     # vertex finished\n        return True\n\n    # ------------------------------------------------------------\n    # 3. Run DFS from every still-white vertex in the prescribed order\n    # ------------------------------------------------------------\n    for v in vertices:\n        if colour[v] == WHITE:\n            if not dfs(v):\n                return None\n\n    # Finishing times are the *reverse* of a topological order\n    topo.reverse()\n    return topo\n```"}
{"task_id": 431, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dist(x1, x2):\n   Compute the Euclidean distance between two vectors.\n\n    The vectors may be supplied as Python lists, tuples, or NumPy arrays. If the\n    vectors are of unequal length, the function returns -1.\n\n    Parameters\n    ----------\n    x1 : list | tuple | numpy.ndarray\n        First 1-D vector containing numeric (int/float) values.\n    x2 : list | tuple | numpy.ndarray\n        Second 1-D vector containing numeric (int/float) values.\n\n    Returns\n    -------\n    float\n        Euclidean distance rounded to 4 decimal places, or -1 if the input\n        vectors are of different lengths.\n       # Convert inputs to NumPy arrays (1-D) of type float\n    v1 = np.asarray(x1, dtype=float).ravel()\n    v2 = np.asarray(x2, dtype=float).ravel()\n\n    # Length check\n    if v1.size != v2.size:\n        return -1\n\n    # Euclidean (L2) distance\n    diff = v1 - v2\n    dist_val = np.sqrt(np.dot(diff, diff))       # same as np.linalg.norm(diff)\n\n    # Round to 4 decimal places\n    return round(float(dist_val), 4)\n```"}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n   Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Args:\n        shape: Sequence of integers \u2265 2 long.  For a 4-D convolution kernel\n               the layout is (out_channels, in_channels, k_height, k_width).\n\n    Returns:\n        (fan_in, fan_out) as floats.\n       if len(shape) == 4:                                       # 2-D convolution\n        out_channels, in_channels, k_h, k_w = shape\n        receptive_field = k_h * k_w\n        fan_in  = in_channels  * receptive_field\n        fan_out = out_channels * receptive_field\n    else:                                                     # dense / other\n        fan_in, fan_out = shape[0], shape[1]\n\n    return float(fan_in), float(fan_out)\n```"}
{"task_id": 434, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(logits: np.ndarray) -> list[list[float]]:\n    \"\"\"Compute a numerically-stable, row-wise softmax.\n    \n    Parameters\n    ----------\n    logits : np.ndarray\n        A 1-D array of shape (n,) or a 2-D array of shape (m, n)\n        containing the un-normalized scores.\n    \n    Returns\n    -------\n    list[list[float]]\n        Row-wise softmax probabilities, each rounded to 6 decimals.\n    \"\"\"\n    # Ensure we are working with a NumPy array of type float64\n    x = np.asarray(logits, dtype=np.float64)\n    \n    # Treat a 1-D input as a single row\n    if x.ndim == 1:\n        x = x.reshape(1, -1)\n    \n    # Subtract the row-wise maximum for numerical stability\n    shifted = x - np.max(x, axis=1, keepdims=True)\n    \n    # Exponentiate and normalize\n    exp_shifted = np.exp(shifted)\n    probs = exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n    \n    # Round to 6 decimal places and convert to (nested) Python lists\n    return np.round(probs, 6).tolist()\n```"}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n   Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear-regression weak learners and squared-error loss.\n    All returned numbers are rounded to 4 decimal places and packed into a\n    plain Python list.\n       # ------------------------------------------------------------------\n    # 1. Convert inputs to NumPy float arrays\n    # ------------------------------------------------------------------\n    X_arr = np.asarray(X, dtype=float)\n    y_arr = np.asarray(y, dtype=float).ravel()          #  shape (m,)\n    m = y_arr.shape[0]\n\n    # Edge cases:   no boosting at all  \u2192  constant predictor\n    if n_estimators <= 0 or learning_rate == 0.0 or m == 0:\n        constant_pred = np.full(m, y_arr.mean() if m else 0.0)\n        return np.round(constant_pred, 4).tolist()\n\n    # ------------------------------------------------------------------\n    # 2. Initial prediction (mean of targets)\n    # ------------------------------------------------------------------\n    y_pred = np.full(m, y_arr.mean(), dtype=float)\n\n    # ------------------------------------------------------------------\n    # 3. Boosting iterations\n    # ------------------------------------------------------------------\n    # Design matrix with intercept term,     shape  (m, d + 1)\n    ones = np.ones((m, 1), dtype=float)\n    X_design = np.hstack((ones, X_arr))\n\n    for _ in range(n_estimators):\n        # Residuals\n        residuals = y_arr - y_pred\n\n        # Fit OLS to residuals: beta = (X\u1d40X)^\u207b\u00b9 X\u1d40 r\n        beta = np.linalg.pinv(X_design) @ residuals          #  shape (d + 1,)\n\n        # Weak-learner prediction of residuals\n        h_pred = X_design @ beta                             #  shape (m,)\n\n        # Update overall prediction\n        y_pred += learning_rate * h_pred\n\n    # ------------------------------------------------------------------\n    # 4. Round and return as plain list\n    # ------------------------------------------------------------------\n    return np.round(y_pred, 4).tolist()\n```"}
{"task_id": 437, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data.\n        order (int, optional): 0=function value, 1=1st derivative,\n            2=2nd derivative. Defaults to 0.\n\n    Returns:\n        float | list | int: Result rounded to 4 decimals (float for scalar\n            input, list for list/ndarray input). Returns -1 for invalid *order*.\n    \"\"\"\n    # ---------- sanity-check for the requested order ----------\n    if order not in (0, 1, 2):\n        return -1\n\n    # ---------- keep track of whether the original input is scalar ----------\n    is_scalar = np.isscalar(x)\n\n    # ---------- convert everything to a NumPy array for vectorised maths ----------\n    arr = np.asarray(x, dtype=float)\n\n    # ---------- numerically-stable sigmoid --------------------\n    # For large negative inputs plain exp(-x) can overflow; the branch\n    # below avoids that by rewriting the expression.\n    pos_mask = arr >= 0\n    neg_mask = ~pos_mask\n    sigma = np.empty_like(arr)\n\n    sigma[pos_mask] = 1.0 / (1.0 + np.exp(-arr[pos_mask]))\n    exp_a = np.exp(arr[neg_mask])\n    sigma[neg_mask] = exp_a / (1.0 + exp_a)\n\n    # ---------- choose the required order ----------\n    if order == 0:\n        out = sigma\n    elif order == 1:\n        out = sigma * (1.0 - sigma)\n    else:  # order == 2\n        out = sigma * (1.0 - sigma) * (1.0 - 2.0 * sigma)\n\n    # ---------- round to 4 decimals ----------\n    out = np.round(out, 4)\n\n    # ---------- return in the requested format ----------\n    if is_scalar:\n        return float(out)\n    else:\n        return out.tolist()\n```"}
{"task_id": 438, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef to_categorical(\n    y: list[int] | np.ndarray, num_classes: int | None = None\n) -> list[list[int]] | int:\n    \"\"\"\n    Convert class labels to a one-hot encoded representation.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes.  If ``None`` it is inferred as\n        ``max(y) + 1``.\n\n    Returns\n    -------\n    list[list[int]] | int\n        A 2-D list of lists containing the one-hot encoded matrix or\n        ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # ---- Normalise the input ------------------------------------------------\n    y_arr = np.asarray(y, dtype=int).ravel()          # ensure 1-D int array\n    n_samples = y_arr.shape[0]\n\n    # Empty `y` edge-case\n    if n_samples == 0:\n        # Cannot infer number of classes from empty input\n        if num_classes is None:\n            return []\n        # Return an empty matrix with `num_classes` columns\n        return np.empty((0, num_classes), dtype=int).tolist()\n\n    # ---- Determine / validate `num_classes` ---------------------------------\n    required_classes = int(y_arr.max()) + 1           # largest label + 1\n\n    if num_classes is None:\n        num_classes = required_classes\n    elif num_classes < required_classes:\n        return -1                                     # impossible to encode\n\n    # ---- Build the one-hot matrix ------------------------------------------\n    one_hot = np.zeros((n_samples, num_classes), dtype=int)\n    one_hot[np.arange(n_samples), y_arr] = 1\n\n    # ---- Return as plain Python lists --------------------------------------\n    return one_hot.tolist()\n```"}
{"task_id": 439, "completion_id": 0, "completion": "```python\nimport itertools\nfrom collections import Counter\nfrom typing import Hashable, Iterable, List, Tuple\n\n\ndef _to_sorted_tuple(items: Iterable[Hashable]) -> Tuple[Hashable, ...]:\n    \"\"\"Convert a set/frozenset to a sorted tuple.\"\"\"\n    return tuple(sorted(items))\n\n\ndef apriori_frequent_itemsets(\n    transactions: List[List[Hashable]], min_sup: float\n) -> List[Tuple[Hashable, ...]]:\n    \"\"\"Find *all* frequent item-sets with the classical Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        The transactional data set.  Every sub-list is a transaction.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (0 < min_sup \u2264 1).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent item-sets ordered by increasing size and then lexicographically.\n    \"\"\"\n    # ----------------------- basic validation ------------------------------\n    if not (0.0 < min_sup <= 1.0):\n        raise ValueError(\"`min_sup` must be in the interval (0, 1].\")\n\n    n_transactions = len(transactions)\n    if n_transactions == 0:            # Nothing to mine\n        return []\n\n    # Materialise transactions as frozensets for O(1) subset checks\n    transactions = [frozenset(t) for t in transactions]\n\n    # ----------------------------------------------------------------------\n    # 1. Frequent 1-itemsets\n    # ----------------------------------------------------------------------\n    item_counter = Counter()\n    for t in transactions:\n        item_counter.update(t)\n\n    freq_k: set[frozenset] = {\n        frozenset([item])\n        for item, cnt in item_counter.items()\n        if cnt / n_transactions >= min_sup\n    }\n\n    all_frequents: list[frozenset] = list(freq_k)   # collect everything here\n    k = 2                                           # length of next candidates\n\n    # ----------------------------------------------------------------------\n    # 2. Breadth-first candidate generation & counting\n    # ----------------------------------------------------------------------\n    while freq_k:\n        # 2a. Self-join L(k-1) \u00d7 L(k-1)  -->  C(k)\n        candidates: set[frozenset] = set()\n        freq_k_list = sorted(freq_k, key=lambda s: tuple(sorted(s)))\n\n        for a, b in itertools.combinations(freq_k_list, 2):\n            union = a | b\n            if len(union) != k:\n                continue\n\n            # 2b. Apriori pruning: all (k-1) subsets must themselves be frequent\n            if all(\n                frozenset(sub) in freq_k\n                for sub in itertools.combinations(union, k - 1)\n            ):\n                candidates.add(union)\n\n        if not candidates:                          # no more candidates \u21d2 stop\n            break\n\n        # 2c. Count candidate supports\n        support_counter = {c: 0 for c in candidates}\n        for t in transactions:\n            for c in candidates:\n                if c.issubset(t):\n                    support_counter[c] += 1\n\n        # 2d. Keep only the frequent ones to obtain L(k)\n        freq_k = {\n            c\n            for c, cnt in support_counter.items()\n            if cnt / n_transactions >= min_sup\n        }\n\n        if not freq_k:                              # nothing frequent at size k\n            break\n\n        all_frequents.extend(freq_k)\n        k += 1\n\n    # ----------------------------------------------------------------------\n    # 3. Final formatting & sorting\n    # ----------------------------------------------------------------------\n    result = [_to_sorted_tuple(fs) for fs in all_frequents]\n    result.sort(key=lambda tup: (len(tup), tup))\n\n    return result\n```\n\nExplanatory notes\n-----------------\n1. Every transaction is converted to a `frozenset` to allow fast `issubset`\n   checks.\n2. Supports are compared as exact *fractions* (`cnt / n_transactions`) so the\n   function behaves correctly for every possible `min_sup`.\n3. Candidate generation follows the classical Apriori recipe:\n   \u2022 self-join frequent sets of size `k-1`;  \n   \u2022 prune any candidate that contains an infrequent (k-1)-subset.\n4. All item-sets are finally returned\n   \u2022 as tuples whose items appear in ascending (lexicographic) order,  \n   \u2022 with the whole list ordered first by length and then lexicographically,\n     exactly as required."}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n   Aggregate per-tree class\u2013probability predictions in an ensemble.\n\n    Parameters\n    ----------\n    predictions\n        Three-level nested python list with shape\n        (n_estimators, n_samples, n_classes).\n\n    Returns\n    -------\n    tuple\n        1) Averaged class probabilities (list of lists) rounded to 4 decimals.\n        2) Final class indices (list[int]) obtained via arg-max on the rounded\n           averages.  Ties are broken by the smallest index, matching NumPy\u2019s\n           default behaviour.\n       # Convert to a NumPy array for vectorised arithmetic.\n    arr = np.asarray(predictions, dtype=float)\n\n    if arr.ndim != 3:\n        raise ValueError`predictions` must be a 3-level nested list                        with shape (n_estimators, n_samples, n_classes).    # 1. Average over estimators  -> shape: (n_samples, n_classes)\n    avg_probs = arr.mean(axis=0)\n\n    # 2. Round to 4 decimal places.\n    avg_probs_rounded = np.round(avg_probs, 4)\n\n    # 3. Arg-max to get the predicted class per sample.\n    pred_labels = np.argmax(avg_probs_rounded, axis=1)\n\n    # Convert back to plain python lists.\n    return avg_probs_rounded.tolist(), pred_labels.tolist()\n```"}
{"task_id": 441, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a one\u2013dimensional Hamming window.\n\n    Parameters\n    ----------\n    window_len : int\n        Number of samples to return.\n    symmetric : bool, default False\n        If True, return the symmetric window (length = window_len).\n        If False, return the periodic window that is meant to be used\n        for overlap-add FFT processing.\n\n    Returns\n    -------\n    list[float]\n        Hamming-window samples, each rounded to 4 decimal places.\n    \"\"\"\n    # -------- edge cases -------------------------------------------------\n    if window_len == 0:            # empty window\n        return []\n    if window_len == 1:            # the single sample is always 1\n        return [1.0]\n\n    # -------- choose denominator -----------------------------------------\n    # symmetric  : denominator = N-1\n    # periodic   : denominator = N\n    denom = window_len - 1 if symmetric else window_len\n\n    # -------- compute window ---------------------------------------------\n    n = np.arange(window_len, dtype=float)\n    w = 0.54 - 0.46 * np.cos(2.0 * np.pi * n / denom)\n\n    # -------- round to 4 decimals and convert to list --------------------\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 442, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport heapq\n\n\ndef knn_kdtree(X_train: np.ndarray, y_train: np.ndarray,\n               X_test:  np.ndarray, k: int) -> list[int]:\n   k-Nearest Neighbours classification accelerated by a hand-written KD-Tree    # ------------------------------------------------------------------\n    # 1. --- Build a balanced KD-Tree ----------------------------------\n    # ------------------------------------------------------------------\n    #\n    # Each node is stored as an (immutable) tuple:\n    #   ('leaf' , index_array)                             \u2013\u2013 leaf\n    #   ('node' , split_dim , split_val , left , right)    \u2013\u2013 inner node\n    #\n    # No classes are used, so everything is plain, hashable Python data.\n    #\n    leaf_size = 10                      # threshold when to stop splitting\n    n_features = X_train.shape[1]\n\n    def _build(idx: np.ndarray) -> tuple:\n       Recursively build a (balanced) KD-Tree and return its root        if idx.size <= leaf_size:                     # turn into leaf\n            return ('leaf', idx)\n\n        # choose the dimension with the largest variance\n        variances = X_train[idx].var(axis=0)\n        split_dim = int(np.argmax(variances))\n\n        # split at the median along that dimension\n        idx_sorted = idx[np.argsort(X_train[idx, split_dim])]\n        mid = idx_sorted.size // 2\n        split_val = float(X_train[idx_sorted[mid], split_dim])\n\n        left  = _build(idx_sorted[:mid])\n        right = _build(idx_sorted[mid:])              # keeps the median\n        return ('node', split_dim, split_val, left, right)\n\n    root = _build(np.arange(X_train.shape[0]))        # build once\n\n\n\n    # ------------------------------------------------------------------\n    # 2. --- KD-Tree k-NN query ----------------------------------------\n    # ------------------------------------------------------------------\n    #\n    # A max-heap of size \u2264 k is maintained (implemented as a min-heap on\n    # negative squared distances).  It stores tuples (-dist, index).\n    #\n    # While descending the tree the algorithm\n    #   \u2022 first explores the side that contains the query point,\n    #   \u2022 afterwards explores the other side only if the hypersphere\n    #     around the query with radius = current worst distance touches\n    #     the splitting hyper-plane.\n    #\n\n    def _query_single(x: np.ndarray) -> int:\n       Return the predicted label for a single query vector *x*        heap: list[tuple[float, int]] = []            # max-heap on distance\n\n        def _maybe_add(idx: int, dist2: float) -> None:\n           Put training index *idx* with squared distance *dist2* into heap            item = (-dist2, idx)                      # negative for max-heap\n            if len(heap) < k:\n                heapq.heappush(heap, item)\n            elif item[0] > heap[0][0]:                # distance smaller?\n                heapq.heapreplace(heap, item)\n\n        def _search(node: tuple) -> None:\n           Recursive depth-first tree traversal with pruning            if node[0] == 'leaf':                     # ------- leaf\n                leaf_idx = node[1]\n                diff = X_train[leaf_idx] - x\n                dists2 = np.einsum('ij,ij->i', diff, diff)   # fast row-wise dot\n                for i, d2 in zip(leaf_idx, dists2):\n                    _maybe_add(int(i), float(d2))\n                return\n\n            # ---------------------------- inner node\n            _, dim, val, left, right = node\n            go_left_first = x[dim] <= val\n\n            first, second = (left, right) if go_left_first else (right, left)\n            _search(first)\n\n            # Decide whether we have to visit the other side\n            worst = np.inf if len(heap) < k else -heap[0][0]     # largest dist\u00b2 so far\n            axis_diff2 = (x[dim] - val) ** 2\n            if axis_diff2 < worst:                               # sphere intersects plane\n                _search(second)\n\n        _search(root)\n\n        # Majority vote on the k stored neighbour indices\n        neighbour_idx = [idx for _, idx in heap]\n        pred_label = int(np.argmax(np.bincount(y_train[neighbour_idx])))\n        return pred_label\n\n\n\n    # ------------------------------------------------------------------\n    # 3. --- Predict all test samples ----------------------------------\n    # ------------------------------------------------------------------\n    predictions = [ _query_single(sample) for sample in X_test ]\n    return predictions\n```"}
{"task_id": 443, "completion_id": 0, "completion": "```python\nimport numbers\nimport numpy as np\n\n\ndef divide_on_feature(X: np.ndarray, feature_i: int, threshold):\n   Split *X* into two NumPy matrices according to feature *feature_i* and *threshold*.\n\n    A row is sent to the *left* subset iff\n    \u2022 threshold is numeric   ->  X[row, feature_i] >= threshold\n    \u2022 threshold is non-numeric -> X[row, feature_i] == threshold\n\n    Args:\n        X: 2-D NumPy array. Each row is a sample, each column a feature.\n        feature_i: Index of the column that is tested.\n        threshold: Value that determines the split (numeric or categorical).\n\n    Returns\n    -------\n        (left_subset, right_subset): tuple of NumPy arrays with the same\n        number of columns as *X*.\n       # --- 1.  Determine whether *threshold* is treated as \u201cnumeric\u201d ------------\n    # bool is a subclass of int, but must be handled as *non-numeric* here\n    is_numeric = (\n        isinstance(threshold, numbers.Number)\n        and not isinstance(threshold, (bool, np.bool_))\n    )\n\n    # --- 2.  Build a boolean mask for the *left* branch -----------------------\n    column = X[:, feature_i]\n\n    if is_numeric:\n        mask = column >= threshold\n    else:\n        mask = column == threshold\n\n    # Ensure mask is a NumPy boolean array (needed when X has dtype=object)\n    mask = np.asarray(mask, dtype=bool)\n\n    # --- 3.  Slice and return -------------------------------------------------\n    left_subset = X[mask]\n    right_subset = X[~mask]\n\n    # Guarantee shapes even when one side is empty\n    # (X[mask] already has correct shape, including 0 rows if necessary)\n    return left_subset, right_subset\n```\n\nThe function\n\n1. Decides whether the supplied `threshold` should be compared numerically or categorically  \n   (explicitly excluding `bool` from the numeric branch).\n2. Creates a boolean mask that selects rows for the left subset.\n3. Returns the left and right subsets as NumPy arrays, preserving the original column order and guaranteeing a shape of `(0, X.shape[1])` when a subset is empty."}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n   Compute the Radial Basis Function (RBF) kernel matrix.\n\n    Parameters\n    ----------\n    X : list of shape (N, C)\n        First set of N C-dimensional samples.\n    Y : list of shape (M, C) or None, optional\n        Second set of M samples.  If None, Y is taken to be X.\n    sigma : float, list of floats or None, optional\n        Scale (band-width) parameter(s):\n            * positive float  -> isotropic kernel\n            * list/1-D array  -> anisotropic kernel (length must be C)\n            * None            -> default sqrt(C/2)\n\n    Returns\n    -------\n    list[list[float]] | int\n        Kernel matrix rounded to 4 decimals, or \u22121 when the input is invalid.\n       # ------------- Helper : return -1 on any failure -----------------\n    def _fail() -> int:\n        return -1\n    # -----------------------------------------------------------------\n\n    # ---------- Convert X to numpy array and validate -----------------\n    try:\n        X_arr = np.asarray(X, dtype=float)\n    except Exception:\n        return _fail()\n\n    if X_arr.ndim != 2:\n        return _fail()\n    N, C = X_arr.shape\n\n    # ---------------------- Handle Y  --------------------------------\n    if Y is None:\n        Y_arr = X_arr\n    else:\n        try:\n            Y_arr = np.asarray(Y, dtype=float)\n        except Exception:\n            return _fail()\n        if Y_arr.ndim != 2 or Y_arr.shape[1] != C:\n            return _fail()\n\n    # --------------------- Validate sigma ----------------------------\n    isotropic = True  # assume scalar until proven otherwise\n\n    if sigma is None:                          # default value\n        sig_val = np.sqrt(C / 2.0)\n    elif np.isscalar(sigma) and not isinstance(sigma, bool):\n        sig_val = float(sigma)\n        if sig_val <= 0:\n            return _fail()\n    else:                                      # list / array\n        try:\n            sig_vec = np.asarray(sigma, dtype=float)\n        except Exception:\n            return _fail()\n        if sig_vec.ndim != 1 or sig_vec.size != C or np.any(sig_vec <= 0):\n            return _fail()\n        isotropic = False\n\n    # ------------------ Compute kernel matrix ------------------------\n    if isotropic:                              # same \u03c3 for every feature\n        sig2 = sig_val ** 2\n        X_norm = np.sum(X_arr ** 2, axis=1)\n        Y_norm = np.sum(Y_arr ** 2, axis=1)\n        # pairwise squared Euclidean distances\n        dist2 = X_norm[:, None] + Y_norm[None, :] - 2.0 * (X_arr @ Y_arr.T)\n        dist2 = np.maximum(dist2, 0.0)         # numerical safety\n        K = np.exp(-0.5 * dist2 / sig2)\n    else:                                      # feature-wise \u03c3\n        X_scaled = X_arr / sig_vec\n        Y_scaled = Y_arr / sig_vec\n        X_norm = np.sum(X_scaled ** 2, axis=1)\n        Y_norm = np.sum(Y_scaled ** 2, axis=1)\n        dist2 = X_norm[:, None] + Y_norm[None, :] - 2.0 * (X_scaled @ Y_scaled.T)\n        dist2 = np.maximum(dist2, 0.0)\n        K = np.exp(-0.5 * dist2)\n\n    # -------------------- Round & return -----------------------------\n    return np.round(K, 4).tolist()\n```"}
{"task_id": 446, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find the best information-gain decision stump.\n\n    See the long task description for the exact requirements.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # convert input to numpy arrays and collect some useful statistics\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y)\n\n    n_samples, n_features = X.shape\n    if n_samples == 0 or n_features == 0:\n        raise ValueError(\"Empty X given.\")\n\n    # all (sorted) distinct class labels found in y\n    classes = np.array(sorted(set(y)))\n    n_classes = len(classes)\n    lbl2idx = {lbl: i for i, lbl in enumerate(classes)}\n\n    # helper: majority label from a vector of class counts\n    def majority_label(class_counts: np.ndarray) -> int:\n        max_count = class_counts.max()\n        # indices (in `classes`) that are tied for max frequency\n        ties = np.where(class_counts == max_count)[0]\n        # `classes` is sorted, so the first index is the smallest label\n        return int(classes[ties[0]])\n\n    # helper: Shannon entropy given absolute class counts\n    def entropy(counts: np.ndarray, total: int) -> float:\n        if total == 0:\n            return 0.0\n        probs = counts.astype(float) / total\n        nz = probs > 0\n        return -np.sum(probs[nz] * np.log2(probs[nz]))\n\n    # class-frequency vector of the (root) parent node\n    parent_counts = np.zeros(n_classes, dtype=int)\n    for lbl in y:\n        parent_counts[lbl2idx[lbl]] += 1\n    parent_entropy = entropy(parent_counts, n_samples)\n\n    # ------------------------------------------------------------------ #\n    # variables that will hold the best split found so far\n    best_gain = -np.inf\n    best_feature = -1\n    best_threshold = None\n    best_left_major = None\n    best_right_major = None\n\n    # ------------------------------------------------------------------ #\n    # iterate over all features and all admissible thresholds\n    for j in range(n_features):\n        col = X[:, j]\n\n        # sort feature column (and the corresponding labels)\n        order = np.argsort(col, kind=\"mergesort\")      # stable sort\n        col_sorted = col[order]\n        y_sorted = y[order]\n\n        # cumulative counts for the left subset\n        left_counts = np.zeros(n_classes, dtype=int)\n\n        # scan once through the sorted values \u2013 every position where\n        # col_sorted[i] != col_sorted[i+1] proposes a threshold\n        for i in range(n_samples - 1):\n            lbl_idx = lbl2idx[y_sorted[i]]\n            left_counts[lbl_idx] += 1\n\n            # only consider a threshold between two *different* values\n            if col_sorted[i] == col_sorted[i + 1]:\n                continue\n\n            n_left = i + 1\n            n_right = n_samples - n_left\n            right_counts = parent_counts - left_counts\n\n            H_left = entropy(left_counts, n_left)\n            H_right = entropy(right_counts, n_right)\n\n            gain = parent_entropy - (n_left / n_samples) * H_left \\\n                                   - (n_right / n_samples) * H_right\n\n            thr = (col_sorted[i] + col_sorted[i + 1]) / 2.0\n            left_major = majority_label(left_counts)\n            right_major = majority_label(right_counts)\n\n            # ---------------------------------------------------------- #\n            # check if this split is better according to the tie rules\n            better_split = False\n            eps = 1e-12  # numerical tolerance\n\n            if gain > best_gain + eps:\n                better_split = True\n            elif abs(gain - best_gain) <= eps:\n                if best_feature == -1 or j < best_feature:\n                    better_split = True\n                elif j == best_feature and thr < best_threshold - eps:\n                    better_split = True\n\n            if better_split:\n                best_gain = gain\n                best_feature = j\n                best_threshold = thr\n                best_left_major = left_major\n                best_right_major = right_major\n\n    # ------------------------------------------------------------------ #\n    # no split improved the (root) entropy? -> return stumpless result\n    if best_gain <= 1e-12:\n        majority = majority_label(parent_counts)\n        return (-1, None, majority, majority)\n\n    # otherwise return the best split (threshold rounded to 4 decimals)\n    return (\n        best_feature,\n        round(best_threshold, 4),\n        best_left_major,\n        best_right_major,\n    )\n```"}
{"task_id": 447, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef one(shape, **kwargs):\n   Create a nested Python list filled with ones.\n\n    Parameters\n    ----------\n    shape : int | tuple[int, ...] | list[int]\n        Desired shape of the output list. Works exactly as the first argument of\n        ``numpy.ones``.\n    **kwargs : dict\n        Optional keyword arguments forwarded to ``numpy.ones`` (e.g. ``dtype``).\n\n    Returns\n    -------\n    list\n        A list (possibly nested) containing only ones that mirrors the provided\n        shape.\n       # Let NumPy do the heavy lifting and validation; then convert to a pure list.\n    return np.ones(shape, **kwargs).tolist()\n```"}
{"task_id": 448, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mean_baseline_predict(X_train, y_train, X_test):\n    \"\"\"Baseline regressor that predicts the mean of the training targets.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples_train, n_features_train).\n        Only its first dimension is used.\n    y_train : np.ndarray\n        One-dimensional array of training targets of length n_samples_train.\n    X_test : np.ndarray\n        Test feature matrix of shape (n_samples_test, n_features_test).\n\n    Returns\n    -------\n    list[float]\n        A list containing the predictions for every row in ``X_test`` rounded to\n        4 decimal places. If ``y_train`` is empty, an empty list is returned.\n    \"\"\"\n    # If no training targets are provided, we cannot compute a mean.\n    if y_train.size == 0:\n        return []\n\n    # 1. Compute the arithmetic mean of y_train.\n    mean_value = float(np.mean(y_train))\n\n    # 2. Create an array of size equal to the number of test samples\n    #    and fill it with the mean value.\n    preds = np.full(X_test.shape[0], mean_value)\n\n    # 3. Round every prediction to the nearest 4th decimal place.\n    preds = np.round(preds, 4)\n\n    # 4. Return predictions as a plain Python list.\n    return preds.tolist()\n```"}
{"task_id": 449, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef gda(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n   Train a (simplified) Gaussian Discriminant Analysis model and predict labels.\n\n    The learning / prediction rule follows the specification verbatim.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Shape (n_samples, n_features) \u2013 training patterns.\n    y_train : np.ndarray\n        Shape (n_samples,) \u2013 binary class labels (0 or 1).\n    X_test : np.ndarray\n        Shape (k_samples, n_features) \u2013 patterns to be classified.\n\n    Returns\n    -------\n    list[int]\n        Predicted class labels (0 or 1) for every row of `X_test`.\n       # ------------------------------------------------------------------\n    # 1. Split the training data by class\n    # ------------------------------------------------------------------\n    X0 = X_train[y_train == 0]\n    X1 = X_train[y_train == 1]\n\n    # ------------------------------------------------------------------\n    # 2. Compute class means\n    # ------------------------------------------------------------------\n    Mu0 = X0.mean(axis=0)\n    Mu1 = X1.mean(axis=0)\n\n    # ------------------------------------------------------------------\n    # 3. (Unused) common covariance estimate \u2013 included for faithfulness\n    # ------------------------------------------------------------------\n    X_sub_Mu = np.vstack([X0 - Mu0, X1 - Mu1])\n    m = X_train.shape[1]                     # number of *features* (note!)\n    Sigma = (1.0 / m) * (X_sub_Mu.T @ X_sub_Mu)  # noqa: F841 ( intentionally unused )\n\n    # ------------------------------------------------------------------\n    # 4. Normal-vector of the separating hyper-plane\n    # ------------------------------------------------------------------\n    normal_vec = Mu1 - Mu0\n    norm = np.sqrt((normal_vec ** 2).sum())\n    # Guard against the degenerate case where both means coincide\n    if norm == 0:\n        # Means identical \u21d2 model cannot separate \u21d2 always predict 0\n        return [0] * len(X_test)\n    w = normal_vec / norm\n\n    # ------------------------------------------------------------------\n    # 5. Bias term of the hyper-plane\n    # ------------------------------------------------------------------\n    b = -np.dot(w, (Mu0 + Mu1) / 2.0)\n\n    # ------------------------------------------------------------------\n    # 6. Determine on which side class-1 lies in the *training* data\n    # ------------------------------------------------------------------\n    sign = int(np.dot(w, Mu1) + b > 0)\n\n    # ------------------------------------------------------------------\n    # 7. Prediction on the test set\n    # ------------------------------------------------------------------\n    preds = ((X_test @ w) + b > 0).astype(int) * sign\n\n    # Convert to plain Python list of ints\n    return preds.tolist()\n```"}
{"task_id": 450, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(preds: np.ndarray) -> list[int]:\n    \"\"\"Aggregate ensemble predictions using majority voting.\n\n    Parameters\n    ----------\n    preds : np.ndarray\n        A 2-D NumPy array of shape ``(n_samples, n_estimators)`` where each entry\n        is an *integer* class label predicted by one estimator for one sample.\n\n    Returns\n    -------\n    list[int]\n        A plain Python list containing the chosen class for every sample.  If\n        multiple classes have the same maximal count (a tie), the smallest class\n        label is returned (NumPy\u2019s ``argmax`` on the result of ``np.bincount``\n        guarantees exactly this behaviour).\n    \"\"\"\n    # Safety check \u2013 the function is defined only for 2-D inputs\n    if preds.ndim != 2:\n        raise ValueError(\"`preds` must be a 2-D NumPy array.\")\n\n    # For every sample (row) count how many times each class appears,\n    # then pick the index (class label) with the highest count.\n    winners = [int(np.argmax(np.bincount(row))) for row in preds]\n\n    return winners\n```\n\nExplanation of the single line in the list-comprehension:\n\n1. `row` contains all predictions of the `n_estimators` for one single sample.\n2. `np.bincount(row)` builds a frequency vector: index \u21d2 class label, value \u21d2 times that label appears.\n3. `np.argmax(...)` returns the index of the maximum frequency, breaking ties in favour of the smallest index (exactly the rule we need).\n4. We cast to `int` and collect the results into a standard Python list."}
{"task_id": 451, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\nfrom collections import Counter\nfrom typing import Optional, List, Tuple\nimport numpy as np\n\n\n# --------------------------------------------------------------------------- #\n#  KD-Tree infrastructure                                                     #\n# --------------------------------------------------------------------------- #\n\nclass _KDNode(tuple):\n    \"\"\"\n    A very small, immutable container for a KD-Tree node.\n\n    Contents (tuple order):\n        0: splitting axis      (int)\n        1: index of the point  (int)\n        2: left  child node    (_KDNode | None)\n        3: right child node    (_KDNode | None)\n    \"\"\"\n    __slots__ = ()\n\n    @property\n    def axis(self):   return self[0]\n\n    @property\n    def idx(self):    return self[1]\n\n    @property\n    def left(self):   return self[2]\n\n    @property\n    def right(self):  return self[3]\n\n\ndef _build_kdtree(X: np.ndarray,\n                  indices: np.ndarray,\n                  depth: int = 0) -> Optional[_KDNode]:\n    \"\"\"\n    Recursively build a KD-Tree that stores only indices into *X*.\n    \"\"\"\n    if indices.size == 0:\n        return None\n\n    axis = depth % X.shape[1]\n    # Median by value along *axis*\n    median_pos = indices.size // 2\n    # argpartition avoids a full sort (O(n))\n    median_in_indices = np.argpartition(X[indices, axis], median_pos)[median_pos]\n    median_index = indices[median_in_indices]\n\n    # Split the remaining points\n    left_mask = X[indices, axis] < X[median_index, axis]\n    left_indices = indices[left_mask]\n    right_indices = indices[~left_mask]\n    # remove the median itself from right_indices if it got in there\n    right_indices = right_indices[right_indices != median_index]\n\n    return _KDNode((\n        axis,\n        median_index,\n        _build_kdtree(X, left_indices, depth + 1),\n        _build_kdtree(X, right_indices, depth + 1),\n    ))\n\n\n# --------------------------------------------------------------------------- #\n#  k-nearest search                                                           #\n# --------------------------------------------------------------------------- #\n\ndef _search_knn(node: Optional[_KDNode],\n                X_train: np.ndarray,\n                query: np.ndarray,\n                k: int,\n                heap: List[Tuple[float, int]],\n                labels: np.ndarray) -> None:\n    \"\"\"\n    Recursively search the *node* subtree, maintaining a *max*-heap\n    (using negative distances, because ``heapq`` is a min-heap) of length \u2264 k.\n    \"\"\"\n    if node is None:\n        return\n\n    idx = node.idx\n    axis = node.axis\n\n    # Distance (squared) to current point\n    dist2 = float(np.sum((query - X_train[idx]) ** 2))\n\n    # Push current point into heap\n    if len(heap) < k:\n        heapq.heappush(heap, (-dist2, idx))\n    else:\n        # heap[0] is the *farthest* neighbour stored so far\n        if -dist2 > heap[0][0]:\n            heapq.heapreplace(heap, (-dist2, idx))\n\n    # Decide which side to visit first\n    diff = query[axis] - X_train[idx, axis]\n    first, second = (node.left, node.right) if diff < 0 else (node.right,\n                                                              node.left)\n\n    _search_knn(first, X_train, query, k, heap, labels)\n\n    # Could points on the other side of the splitting plane\n    # be nearer than our worst current neighbour?\n    need_second = len(heap) < k or diff * diff < -heap[0][0]\n    if need_second:\n        _search_knn(second, X_train, query, k, heap, labels)\n\n\n# --------------------------------------------------------------------------- #\n#  Public function                                                            #\n# --------------------------------------------------------------------------- #\n\ndef knn_kdtree_predict(\n        X_train: np.ndarray,\n        y_train,\n        X_test: np.ndarray,\n        k: int\n) -> list[int]:\n    \"\"\"Classify *X_test* using a k-NN classifier accelerated by a KD-Tree.\"\"\"\n    # Basic sanity checks\n    if k <= 0 or X_train.size == 0:\n        return []\n\n    X_train = np.asarray(X_train, dtype=float)\n    X_test = np.asarray(X_test, dtype=float)\n    y_train = np.asarray(y_train)\n\n    n_samples = X_train.shape[0]\n    k = min(k, n_samples)             # you cannot take more neighbours than\n                                      # there are samples\n\n    # ------------------------------------------------------------------ #\n    # Build the KD-Tree                                                  #\n    # ------------------------------------------------------------------ #\n    indices = np.arange(n_samples, dtype=int)\n    kdtree_root = _build_kdtree(X_train, indices)\n\n    # ------------------------------------------------------------------ #\n    # Answer each query                                                  #\n    # ------------------------------------------------------------------ #\n    predictions: List[int] = []\n\n    for query in X_test:\n        heap: List[Tuple[float, int]] = []     # (negative dist2, index)\n\n        _search_knn(kdtree_root, X_train, query, k, heap, y_train)\n\n        # Collect the labels of the k closest neighbours\n        neighbour_labels = [int(y_train[idx]) for _, idx in heap]\n\n        # Majority vote with deterministic tie-breaking (pick smallest label)\n        counts = Counter(neighbour_labels)\n        # max by (count, -label) so that smaller label wins on ties\n        predicted = min(\n            (-cnt, lbl) for lbl, cnt in counts.items()\n        )[1]\n\n        predictions.append(predicted)\n\n    return predictions\n```"}
{"task_id": 452, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List\nimport numbers\n\n\ndef _as_list(subset) -> List[list]:\n    \"\"\"Helper \u2013 convert *subset* to a regular Python list of lists.\"\"\"\n    if not subset:                       # empty subset\n        return []\n    # ndarray.tolist() guarantees deep conversion to pure Python objects\n    return np.asarray(subset, dtype=object).tolist()\n\n\ndef divide_on_feature(\n    X: np.ndarray | List[list], feature_i: int, threshold\n) -> List[list]:\n    \"\"\"Split *X* into two subsets according to *feature_i* and *threshold*.\n\n    For a numeric threshold (int / float / numpy number) the first subset\n    contains the samples whose value at *feature_i* is **>= threshold**.\n    For any other threshold type (categorical) the first subset contains the\n    samples whose value at *feature_i* is **== threshold**.\n    The second subset holds the remaining samples.\n    Both subsets preserve the original order and are returned as pure\n    Python lists (list-of-lists).\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # Determine whether we are dealing with a \"numeric\" threshold.\n    # Treat plain bools separately: `bool` is a subclass of `int`, but for\n    # splitting purposes it is far more natural to handle them\n    # categorically.\n    # ------------------------------------------------------------------ #\n    numeric_threshold = (\n        isinstance(threshold, numbers.Real)\n        and not isinstance(threshold, (bool, np.bool_))\n    )\n\n    # ------------------------------------------------------------------ #\n    # Build the two subsets while preserving the original order.\n    # ------------------------------------------------------------------ #\n    subset_1, subset_2 = [], []\n\n    # We iterate exactly in the order the data appear in *X*.\n    for sample in X:\n        value = sample[feature_i]\n\n        if numeric_threshold:\n            in_first = value >= threshold\n        else:  # categorical comparison\n            in_first = value == threshold\n\n        if in_first:\n            subset_1.append(sample)\n        else:\n            subset_2.append(sample)\n\n    # ------------------------------------------------------------------ #\n    # Convert the subsets to regular Python lists and return them.\n    # ------------------------------------------------------------------ #\n    return [_as_list(subset_1), _as_list(subset_2)]\n```"}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # --------------- basic validation -----------------\n    # both Q-matrices must be 2-D and of identical shape\n    if not (isinstance(Q, np.ndarray) and isinstance(Q_next, np.ndarray)):\n        return -1\n    if Q.ndim != 2 or Q_next.ndim != 2 or Q.shape != Q_next.shape:\n        return -1\n\n    batch_size, n_actions = Q.shape\n\n    # lists must all have length == batch_size\n    if not (len(actions) == len(rewards) == len(dones) == batch_size):\n        return -1\n\n    # gamma must be within [0, 1]\n    if not (0.0 <= gamma <= 1.0):\n        return -1\n\n    # every action index must be valid\n    if any((a < 0 or a >= n_actions) for a in actions):\n        return -1\n\n    # --------------- build targets --------------------\n    targets = Q.copy()  # do **not** overwrite input\n    for i in range(batch_size):\n        if dones[i]:\n            target_value = rewards[i]\n        else:\n            target_value = rewards[i] + gamma * np.max(Q_next[i])\n        targets[i, actions[i]] = target_value\n\n    # round to 4 decimals and return as (nested) plain Python list\n    return np.round(targets, 4).tolist()\n```"}
{"task_id": 454, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef generate_dcgan_labels(batch_size: int) -> tuple[list[list[int]], list[list[int]], list[list[int]]]:\n    \"\"\"Generate fixed one-hot label arrays for DCGAN training.\n    \n    Args:\n        batch_size: Positive, even integer (\u22652) \u2013 total samples in a batch.\n    \n    Returns:\n        (valid_half, fake_half, valid_full), each converted to Python lists.\n    \"\"\"\n    # Sanity-check (optional but helpful during development)\n    if batch_size <= 0 or batch_size % 2 != 0:\n        raise ValueError(\"batch_size must be a positive, even integer.\")\n    \n    half = batch_size // 2\n    \n    # Build the three required label arrays\n    valid_half = np.tile([1, 0], (half, 1))      # shape (n//2, 2) \u2192 all [1, 0]\n    fake_half  = np.tile([0, 1], (half, 1))      # shape (n//2, 2) \u2192 all [0, 1]\n    valid_full = np.tile([1, 0], (batch_size, 1))  # shape (n, 2)   \u2192 all [1, 0]\n    \n    # Convert to regular Python lists before returning\n    return valid_half.tolist(), fake_half.tolist(), valid_full.tolist()\n```"}
{"task_id": 455, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\nimport numpy as np\nfrom collections import Counter\n\n\ndef _majority_vote(labels):\n    \"\"\"Return the most common label, ties \u2192 smallest label.\"\"\"\n    cnt = Counter(labels)\n    max_freq = max(cnt.values())\n    best = [lab for lab, fr in cnt.items() if fr == max_freq]\n    return min(best)\n\n\n# -----------------------------------------------------------------------------  \n#  KD-TREE HELPERS\n# -----------------------------------------------------------------------------\nclass _KDNode:\n    __slots__ = (\"idx\", \"axis\", \"left\", \"right\", \"leaf_idx\")\n\n    # internal node: idx (pivot), axis, children;  leaf: leaf_idx (array of\n    # sample indices) and idx == None\n    def __init__(self, *, idx=None, axis=None, left=None, right=None,\n                 leaf_idx=None):\n        self.idx = idx\n        self.axis = axis\n        self.left = left\n        self.right = right\n        self.leaf_idx = leaf_idx\n\n\ndef _build_kdtree(X, indices, depth=0, leaf_size=10):\n    \"\"\"Return root of KD-tree built on X[indices].\"\"\"\n    if len(indices) <= leaf_size:\n        return _KDNode(leaf_idx=indices)\n\n    axis = depth % X.shape[1]\n    # sort indices by splitting axis and take median as pivot\n    sorted_idx = indices[np.argsort(X[indices, axis])]\n    median_pos = len(sorted_idx) // 2\n    pivot = sorted_idx[median_pos]\n\n    left_child = _build_kdtree(X, sorted_idx[:median_pos], depth + 1,\n                               leaf_size)\n    right_child = _build_kdtree(X, sorted_idx[median_pos + 1:], depth + 1,\n                                leaf_size)\n\n    return _KDNode(idx=pivot, axis=axis, left=left_child, right=right_child)\n\n\ndef _kdtree_query(root, X, q, k):\n    \"\"\"Return labels of k nearest neighbours of q using the given KD-tree.\"\"\"\n    heap = []        # max-heap of size \u2264 k, stores (-dist, label)\n\n    def visit(node):\n        nonlocal heap\n        if node is None:\n            return\n\n        # Leaf ----------------------------------------------------------------\n        if node.leaf_idx is not None:\n            for i in node.leaf_idx:\n                dist = np.sum((q - X[i]) ** 2)\n                if len(heap) < k:\n                    heapq.heappush(heap, (-dist, i))\n                elif dist < -heap[0][0]:\n                    heapq.heapreplace(heap, (-dist, i))\n            return\n\n        # Internal node --------------------------------------------------------\n        # 1) check pivot\n        dist = np.sum((q - X[node.idx]) ** 2)\n        if len(heap) < k:\n            heapq.heappush(heap, (-dist, node.idx))\n        elif dist < -heap[0][0]:\n            heapq.heapreplace(heap, (-dist, node.idx))\n\n        # 2) choose subtree order\n        diff = q[node.axis] - X[node.idx, node.axis]\n        first, second = (node.left, node.right) if diff <= 0 else \\\n                        (node.right, node.left)\n\n        visit(first)\n\n        worst = -heap[0][0] if heap else math.inf\n        if len(heap) < k or diff * diff < worst:      # may contain closer one\n            visit(second)\n\n    visit(root)\n    return [idx for _, idx in heap]\n\n\n# -----------------------------------------------------------------------------  \n#  MAIN ENTRY\n# -----------------------------------------------------------------------------\ndef knn_classifier(X_train, y_train, X_query, k, method=\"naive\"):\n    \"\"\"k-Nearest-Neighbours classifier supporting three search strategies.\"\"\"\n    if k <= 0:\n        raise ValueError(\"k must be positive.\")\n    # numpy arrays (copy = False when already ndarray)\n    X_train = np.asarray(X_train, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n\n    n_train = X_train.shape[0]\n    if k > n_train:\n        k = n_train  # silently cap\n\n    preds = []\n\n    # ---------------------------------------------------------------------  \n    #  NA\u00cfVE \u2013 full distance computation + partial sort\n    # ---------------------------------------------------------------------\n    if method == \"naive\":\n        for q in X_query:\n            dists = np.sum((X_train - q) ** 2, axis=1)\n            idx_k = np.argpartition(dists, k - 1)[:k]\n            preds.append(_majority_vote(y_train[idx_k]))\n        return preds\n\n    # ---------------------------------------------------------------------  \n    #  HEAP \u2013 keep k smallest distances while scanning training set\n    # ---------------------------------------------------------------------\n    if method == \"heap\":\n        for q in X_query:\n            heap = []                                  # max-heap on (-dist, lab)\n            for xi, lab in zip(X_train, y_train):\n                dist = np.sum((q - xi) ** 2)\n                if len(heap) < k:\n                    heapq.heappush(heap, (-dist, lab))\n                elif dist < -heap[0][0]:\n                    heapq.heapreplace(heap, (-dist, lab))\n            preds.append(_majority_vote([lab for _, lab in heap]))\n        return preds\n\n    # ---------------------------------------------------------------------  \n    #  KD-TREE \u2013 build once, then query each sample\n    # ---------------------------------------------------------------------\n    if method == \"kdtree\":\n        # build tree once\n        indices = np.arange(n_train)\n        root = _build_kdtree(X_train, indices)\n        for q in X_query:\n            neighbour_idx = _kdtree_query(root, X_train, q, k)\n            preds.append(_majority_vote(y_train[neighbour_idx]))\n        return preds\n\n    # ---------------------------------------------------------------------\n    raise ValueError(\"Unknown method '{}'. Choose 'naive', 'heap' or 'kdtree'.\".format(method))\n```"}
{"task_id": 456, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_covariance_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list[list[float]]:\n   Compute the sample covariance or cross-covariance matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        (n \u00d7 p) data matrix.\n    Y : np.ndarray | None, optional\n        (n \u00d7 m) data matrix. If None, Y is taken as X.\n\n    Returns\n    -------\n    list[list[float]] | int\n        Covariance matrix rounded to 4 decimals, or -1 if the input is invalid.\n       # Validate X\n    if X.ndim != 2 or X.shape[0] < 2:\n        return -1\n\n    n = X.shape[0]\n\n    # Handle Y and validate compatibility\n    if Y is None:\n        Y = X\n    else:\n        if Y.ndim != 2 or Y.shape[0] != n:\n            return -1\n\n    # Center the matrices\n    X_centered = X - X.mean(axis=0, keepdims=True)\n    Y_centered = Y - Y.mean(axis=0, keepdims=True)\n\n    # Un-biased (n-1) denominator\n    cov_matrix = (X_centered.T @ Y_centered) / (n - 1)\n\n    # Round to 4 decimals and convert to list of lists\n    return np.round(cov_matrix, 4).tolist()\n```"}
{"task_id": 457, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_penalty(weights, alpha):\n    \"\"\"Compute the Elastic-Net penalty for a set of weights.\n\n    Parameters\n    ----------\n    weights : list | tuple | numpy.ndarray\n        1-D collection of numerical weights.\n    alpha : float\n        Mixing parameter in the closed interval [0, 1].\n        \u2022 alpha = 1   -> purely 0.5 * L2 penalty\n        \u2022 alpha = 0   -> purely L1 penalty\n\n    Returns\n    -------\n    float\n        Elastic-Net penalty rounded to 4 decimal places,\n        or \u20111 if *alpha* is outside [0, 1].\n    \"\"\"\n    # ---- validate alpha ----------------------------------------------------\n    try:                       # also guards against non-numeric alpha\n        if not (0.0 <= float(alpha) <= 1.0):\n            return -1\n    except (TypeError, ValueError):\n        return -1\n\n    # ---- convert weights to ndarray & flatten ------------------------------\n    w = np.asarray(weights, dtype=float).ravel()\n\n    # ---- compute penalty ----------------------------------------------------\n    l2_part = 0.5 * alpha * np.sum(w ** 2)       # 0.5 * \u03b1 * \u03a3 w_i\u00b2\n    l1_part = (1.0 - alpha) * np.sum(np.abs(w))  # (1\u2212\u03b1) * \u03a3 |w_i|\n    penalty = l2_part + l1_part\n\n    # ---- round & return -----------------------------------------------------\n    return float(round(penalty, 4))\n```"}
{"task_id": 458, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n   Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes.  If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of *labels*, or **-1** if the\n        input is invalid (contains negative labels or an insufficient\n        ``num_classes``).\n       # -------- 1. Normalise input to a plain Python list -----------------\n    try:\n        labels_list = list(labels)\n    except TypeError:               # e.g. labels is not iterable\n        return -1\n\n    # Edge-case: empty input\n    if not labels_list:\n        if num_classes is None:\n            return -1                      # cannot infer the width\n        return []                          # empty but valid\n\n    # -------- 2. Validate labels ----------------------------------------\n    max_label = -1\n    for lbl in labels_list:\n        # Accept built-in int or any NumPy integer subclass\n        if not isinstance(lbl, (int, np.integer)):\n            return -1\n        if lbl < 0:                        # negative labels not allowed\n            return -1\n        if lbl > max_label:\n            max_label = int(lbl)\n\n    # -------- 3. Determine / validate num_classes -----------------------\n    inferred_classes = max_label + 1\n    if num_classes is None:\n        num_classes = inferred_classes\n    elif num_classes < inferred_classes:\n        return -1\n\n    # -------- 4. Build one-hot matrix -----------------------------------\n    # Pre-allocate a zero-filled matrix\n    one_hot = [[0] * num_classes for _ in range(len(labels_list))]\n    for row_idx, lbl in enumerate(labels_list):\n        one_hot[row_idx][int(lbl)] = 1     # ensure plain int 1\n\n    return one_hot\n```"}
{"task_id": 459, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport heapq\n\ndef knn_validate_heap(X_train:np.ndarray                      Y_train:np.ndarray                      X_val:np.ndarray                      Y_val:np.ndarray                      k: int) -> tuple[float, list[int]]:\n   Validate a k-NN classifier using a fixed-size max-heap.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples, shape (n_train, n_features).\n    Y_train : np.ndarray\n        Integer class labels for the training samples.\n    X_val : np.ndarray\n        Validation samples, shape (n_val, n_features).\n    Y_val : np.ndarray\n        True labels for the validation samples.\n    k : int\n        Number of nearest neighbours to consider (1 \u2264 k \u2264 n_train).\n\n    Returns\n    -------\n    tuple\n        (accuracy, predicted_labels) where accuracy is rounded to 4 decimals\n        and predicted_labels is a list of ints.\n       n_val = X_val.shape[0]\n    predicted_labels: list[int] = []\n\n    for i in range(n_val):\n        sample = X_val[i]\n\n        # Vectorised distance computation (Euclidean)\n        dists = np.linalg.norm(X_train - sample, axis=1)\n\n        # Fixed-size max-heap that stores the k smallest distances seen so far\n        heap: list[tuple[float, int]] = []        # elements are (-dist, idx)\n\n        for idx, dist in enumerate(dists):\n            neg_dist = -dist                       # negate to turn min-heap \u2192 max-heap\n\n            if len(heap) < k:\n                heapq.heappush(heap, (neg_dist, idx))\n            elif neg_dist > heap[0][0]:            # current dist < largest dist in heap\n                heapq.heapreplace(heap, (neg_dist, idx))\n\n        # Extract neighbour indices and their labels\n        neighbour_indices = [idx for (_, idx) in heap]\n        neighbour_labels = Y_train[neighbour_indices]\n\n        # Majority vote (ties resolved automatically by argmax \u2192 smallest label wins)\n        label_counts = np.bincount(neighbour_labels)\n        pred_label = int(label_counts.argmax())\n        predicted_labels.append(pred_label)\n\n    # Accuracy on the validation set\n    accuracy = np.mean(np.array(predicted_labels) == Y_val)\n    accuracy = round(float(accuracy), 4)\n\n    return accuracy, predicted_labels\n```"}
{"task_id": 460, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X: np.ndarray, Y: np.ndarray | None = None) -> list | int:\n    \"\"\"Compute the Pearson correlation matrix between the columns of *X* and *Y*\n    using the population definition (divide by *n*, not *n \u2013 1*).\n\n    If *Y* is omitted the function returns \u03c1(X, X).  \n    If any column of *X* or *Y* has zero population standard deviation the\n    correlation is undefined and the function returns -1.\n\n    Parameters\n    ----------\n    X : (n_samples, n_features_X) ndarray\n    Y : (n_samples, n_features_Y) ndarray, optional\n\n    Returns\n    -------\n    list[list[float]]  \u2013 correlation matrix rounded to 4 decimals\n    or\n    int                \u2013 the number -1 when the computation is impossible\n    \"\"\"\n    # --- basic shape checks --------------------------------------------------\n    if X.ndim != 2:\n        raise ValueError(\"X must be a two-dimensional array.\")\n    n_samples = X.shape[0]\n\n    if Y is None:\n        Y = X                     # fall back to \u03c1(X, X)\n    else:\n        if Y.ndim != 2:\n            raise ValueError(\"Y must be a two-dimensional array.\")\n        if Y.shape[0] != n_samples:\n            raise ValueError(\"X and Y must have the same number of rows.\")\n\n    # --- centre the columns --------------------------------------------------\n    X = X.astype(float, copy=False)\n    Y = Y.astype(float, copy=False)\n\n    X_mean = X.mean(axis=0)\n    Y_mean = Y.mean(axis=0)\n\n    X_centered = X - X_mean\n    Y_centered = Y - Y_mean\n\n    # --- population (\u03c3) standard deviations ----------------------------------\n    #   \u03c3 = sqrt( \u03a3(x \u2013 \u0233)\u00b2 / n )\n    X_std = np.sqrt((X_centered ** 2).sum(axis=0) / n_samples)\n    Y_std = np.sqrt((Y_centered ** 2).sum(axis=0) / n_samples)\n\n    # --- zero-variance guard --------------------------------------------------\n    if np.any(X_std == 0) or np.any(Y_std == 0):\n        return -1\n\n    # --- covariance matrix ----------------------------------------------------\n    #   cov = (X\u1d40 Y) / n\n    cov = (X_centered.T @ Y_centered) / n_samples     # shape: (p, q)\n\n    # --- correlation matrix ---------------------------------------------------\n    corr = cov / (X_std[:, None] * Y_std[None, :])\n\n    # --- round & convert ------------------------------------------------------\n    corr_rounded = np.round(corr, 4).tolist()\n    return corr_rounded\n```"}
{"task_id": 461, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n   Gradient Boosting with decision stumps for 1-D regression    \n    # --- basic checks / array conversion ------------------------------------\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n_samples = y.size\n    if n_samples == 0:                         # nothing to fit\n        return []\n    \n    # --- start with the mean -------------------------------------------------\n    y_hat = np.full(n_samples, y.mean())\n    if n_estimators <= 0:                      # special-case: no boosting\n        return np.round(y_hat, 4).tolist()\n    \n    # ------------------------------------------------------------------------\n    for _ in range(n_estimators):\n        # residuals\n        residuals = y - y_hat\n        if np.allclose(residuals, 0.0):        # early stop if perfect fit\n            break\n        \n        # ---------- find the best split (depth-1 tree) -----------------------\n        uniq = np.unique(x)\n        possible_splits = uniq[:-1]            # drop the greatest value\n        best_sse = np.inf\n        best_split = None\n        best_left_val = best_right_val = 0.0\n        \n        for t in possible_splits:\n            left_mask  = x <= t\n            right_mask = ~left_mask\n            if not left_mask.any() or not right_mask.any():\n                continue                       # ignore invalid splits\n            \n            # mean residuals on both sides\n            v_left  = residuals[left_mask].mean()\n            v_right = residuals[right_mask].mean()\n            \n            # Sum of squared errors for this split\n            sse  = np.sum((residuals[left_mask]  - v_left )**2)\n            sse += np.sum((residuals[right_mask] - v_right)**2)\n            \n            if sse < best_sse:                 # take first split in ties\n                best_sse = sse\n                best_split = t\n                best_left_val, best_right_val = v_left, v_right\n        \n        # ---------- stump prediction ----------------------------------------\n        if best_split is None:                 # no valid split -> constant\n            stump_pred = np.full(n_samples, residuals.mean())\n        else:\n            stump_pred = np.where(x <= best_split,\n                                  best_left_val,\n                                  best_right_val)\n        \n        # ---------- update ensemble prediction ------------------------------\n        y_hat += learning_rate * stump_pred\n    \n    # ------------------------------------------------------------------------\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 462, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cepstral_lifter(mfccs: np.ndarray, D: int) -> np.ndarray:\n    \"\"\"Apply a sinusoidal cepstral lifter to a matrix of MFCC coefficients.\n\n    Args:\n        mfccs: NumPy array of shape (G, C) containing MFCC coefficients.\n        D: Non-negative integer lifter parameter.  A value of 0 (or 1) returns\n           the input unchanged.\n\n    Returns:\n        NumPy array with the same shape and dtype as *mfccs* containing the\n        liftered coefficients.\n\n    Raises:\n        ValueError: If D is negative.\n    \"\"\"\n    if D < 0:\n        raise ValueError(\"The lifter parameter D must be non-negative.\")\n\n    # When D is 0 or 1, the lifter equals 1 for all n, so return the input.\n    if D in (0, 1):\n        return mfccs.copy()\n\n    G, C = mfccs.shape\n\n    # Indices of the cepstral coefficients (0 \u2026 C-1)\n    n = np.arange(C, dtype=np.float64)\n\n    # Sinusoidal lifter: 1 + (D/2) * sin(pi * n / D)\n    lifter = 1.0 + (D / 2.0) * np.sin(np.pi * n / D)\n\n    # Broadcast lifter across all frames (rows) and apply it.\n    lifted = mfccs * lifter[None, :]\n\n    # Ensure the output dtype matches the input dtype.\n    if lifted.dtype != mfccs.dtype:\n        lifted = lifted.astype(mfccs.dtype, copy=False)\n\n    return lifted\n```"}
{"task_id": 463, "completion_id": 0, "completion": "```python\nimport math\n\ndef compute_ball_radius(centroid: list[float], X: list[list[float]]) -> float:\n   Return the Ball-tree node radius, i.e. the largest Euclidean distance\n    between *centroid* and any point in *X*, rounded to 4 decimals.\n    If *X* is empty the radius is 0.0.\n       if not X:                      # handle empty node\n        return 0.0\n\n    # track the largest distance encountered\n    max_dist = 0.0\n    for point in X:\n        # Euclidean distance between the centroid and current point\n        dist = math.dist(centroid, point)\n        if dist > max_dist:\n            max_dist = dist\n\n    # round to the nearest 4th decimal and return\n    return round(max_dist, 4)\n```"}
{"task_id": 464, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef knn_validate(X_train: np.ndarray,\n                 Y_train: np.ndarray,\n                 X_val: np.ndarray,\n                 Y_val: np.ndarray,\n                 k: int) -> tuple[float, list[int]]:\n    \"\"\"Basic k-Nearest Neighbours validation helper.\"\"\"\n    # Ensure everything is NumPy arrays of the expected dtype\n    X_train = np.asarray(X_train, dtype=float)\n    X_val   = np.asarray(X_val,   dtype=float)\n    Y_train = np.asarray(Y_train, dtype=int)\n    Y_val   = np.asarray(Y_val,   dtype=int)\n\n    # -----------------------------------------------------------\n    # 1. Compute the (squared) Euclidean distances\n    #    dist[i, j] = || X_val[i] - X_train[j] ||\u00b2\n    # -----------------------------------------------------------\n    diff  = X_val[:, None, :] - X_train[None, :, :]\n    dists = np.sum(diff * diff, axis=2)                   # shape (n_val, n_train)\n\n    # -----------------------------------------------------------\n    # 2. For every validation sample grab indices of k-nearest\n    #    neighbours using argpartition (O(n) per row instead of\n    #    full sort).                                           \n    # -----------------------------------------------------------\n    knn_idx = np.argpartition(dists, kth=k-1, axis=1)[:, :k]  # shape (n_val, k)\n\n    # Corresponding neighbour labels\n    knn_labels = Y_train[knn_idx]                             # shape (n_val, k)\n\n    # -----------------------------------------------------------\n    # 3. Majority vote for each validation sample\n    # -----------------------------------------------------------\n    predictions: list[int] = []\n    for row in knn_labels:\n        # bincount gives vote counts per label; argmax returns\n        # the smallest label in case of ties (rule 2).\n        pred_label = int(np.argmax(np.bincount(row)))\n        predictions.append(pred_label)\n\n    # -----------------------------------------------------------\n    # 4. Compute accuracy\n    # -----------------------------------------------------------\n    accuracy = round(float(np.mean(np.asarray(predictions) == Y_val)), 4)\n\n    return accuracy, predictions\n```\n\nHow it works\n1. Distance matrix `dists` stores the squared Euclidean distance from every validation point to every training point (no square-root needed for ranking).\n2. `np.argpartition` efficiently fetches indices of the `k` nearest neighbours for every validation example.\n3. For each group of neighbours, `np.bincount` counts label occurrences; `np.argmax` picks the label with the highest count. In a tie, the smallest label is chosen automatically.\n4. The fraction of correct predictions is rounded to 4 decimals and returned together with the full list of predicted labels."}
{"task_id": 465, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef rbf_kernel(gamma: float, **kwargs):\n    \"\"\"Create and return a Gaussian RBF kernel function.\n\n    The returned function takes two 1-D vectors and computes the Gaussian\n    Radial Basis Function (RBF) similarity between them.\n\n    Args:\n        gamma: A non-negative real number that controls the width of the RBF\n               kernel. A larger gamma makes the kernel decay faster.\n        **kwargs: Accepted only for interface compatibility.\n\n    Returns:\n        Callable f(x1, x2) that returns exp(-gamma * ||x1 \u2013 x2||\u00b2) rounded\n        to four decimal places.\n    \"\"\"\n    # --- Basic validation of gamma -----------------------------------------\n    if gamma < 0:\n        raise ValueError(\"gamma must be non-negative.\")\n    gamma = float(gamma)          # ensure native float for speed\n\n    # ---------------------- inner kernel function ---------------------------\n    def _kernel(x1, x2):\n        \"\"\"Compute the RBF similarity between two 1-D vectors.\"\"\"\n        # Convert inputs to 1-D NumPy arrays\n        v1 = np.asarray(x1, dtype=float).ravel()\n        v2 = np.asarray(x2, dtype=float).ravel()\n\n        if v1.shape != v2.shape:\n            raise ValueError(\"Input vectors must have the same length.\")\n\n        # Special-case gamma == 0 (similarity is 1 for any inputs)\n        if gamma == 0:\n            return 1.0\n\n        # Squared Euclidean distance\n        diff = v1 - v2\n        sq_norm = np.dot(diff, diff)          # faster than np.linalg.norm**2\n\n        # RBF similarity\n        sim = np.exp(-gamma * sq_norm)\n\n        # Round to four decimal places and return\n        return float(np.round(sim, 4))\n\n    return _kernel\n```"}
{"task_id": 466, "completion_id": 0, "completion": "```python\nimport string\nimport string\n\n# Build the translation table once so it can be reused efficiently by every\n# call to ``strip_punctuation``.\n_PUNC_TABLE: dict[int, None] = str.maketrans('', '', string.punctuation)\n\ndef strip_punctuation(line: str) -> str:\n    \"\"\"Remove punctuation characters from a string.\n\n    The function deletes all characters listed in ``string.punctuation`` and\n    trims surrounding whitespace.\n\n    Args:\n        line (str): Text that may contain punctuation symbols.\n\n    Returns:\n        str: Text with punctuation removed and stripped of leading/trailing\n        whitespace.\n    \"\"\"\n    # Validate argument type (optional but defensive)\n    if not isinstance(line, str):\n        raise TypeError(\"strip_punctuation expects a str\")\n\n    # 1. Remove punctuation via translate\n    # 2. Strip leading/trailing whitespace\n    return line.translate(_PUNC_TABLE).strip()\n```"}
{"task_id": 467, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef regularization(\n    weights: list[float | int],\n    C: float = 0.01,\n    norm: str = \"l2\",\n) -> tuple[float, list[float]] | int:\n    \"\"\"Return the regularisation penalty and gradient for a list of weights.\n\n    Parameters\n    ----------\n    weights : list[float | int]\n        Vector of model weights.\n    C : float, default 0.01\n        Regularisation strength.\n    norm : str, {\"l1\", \"l2\"}, default \"l2\"\n        Type of regularisation to apply.\n\n    Returns\n    -------\n    tuple[float, list[float]]\n        (penalty, gradient) \u2013 each value rounded to 4 decimals.\n        If *norm* is unsupported, returns -1.\n    \"\"\"\n    # sanitise norm flag\n    norm = norm.lower()\n    w = np.asarray(weights, dtype=float)\n\n    if norm == \"l1\":\n        penalty = C * np.sum(np.abs(w))\n        gradient = C * np.sign(w)\n\n    elif norm == \"l2\":\n        penalty = 0.5 * C * np.sum(w ** 2)\n        gradient = C * w\n\n    else:                            # unsupported norm\n        return -1\n\n    # rounding to 4 decimal places\n    penalty = round(float(penalty), 4)\n    gradient = [round(float(g), 4) for g in gradient]\n\n    return penalty, gradient\n```\n\nExample usage\n```\n>>> regularization([1.2, -3, 0.0], C=0.1, norm=\"l1\")\n(0.42, [0.1, -0.1, 0.0])\n\n>>> regularization([1.2, -3, 0.0], C=0.1, norm=\"l2\")\n(0.522, [0.12, -0.3, 0.0])\n\n>>> regularization([1, 2, 3], norm=\"l0\")\n-1\n```"}
{"task_id": 468, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef logistic_regression_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iters: int = 10000) -> list[int]:\n    \"\"\"Train a binary logistic-regression model with gradient descent and\n    predict labels for the test set.\"\"\"\n    # ----- Helper -----------------------------------------------------------\n    def sigmoid(z: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # ----- 1. Prepare data --------------------------------------------------\n    m, n = X_train.shape                       # number of training samples / features\n\n    # Ensure y is column-vector of shape (m,1)\n    y = y_train.reshape(-1, 1)\n\n    # Add bias column (ones) to training and test design matrices\n    Xb_train = np.hstack((np.ones((m, 1)), X_train))          # (m, n+1)\n    Xb_test  = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # ----- 2. Initialise parameters to zeros -------------------------------\n    W = np.zeros((n + 1, 1))                                   # (n+1, 1)\n\n    # ----- 3. Gradient-descent optimisation -------------------------------\n    for _ in range(num_iters):\n        # Forward pass\n        h = sigmoid(Xb_train @ W)                              # (m,1) predictions\n\n        # Gradient of binary cross-entropy wrt W\n        dW = (Xb_train.T @ (h - y)) / m                        # (n+1,1)\n\n        # Parameter update\n        W -= learning_rate * dW\n\n    # ----- 4. Predict on test set ------------------------------------------\n    probs = sigmoid(Xb_test @ W).ravel()                       # (k,)\n    preds = (probs >= 0.5).astype(int).tolist()                # Python list[ int ]\n\n    return preds\n```"}
{"task_id": 469, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(**kwargs) -> callable:\n    \"\"\"Factory that builds a linear-kernel (dot-product) callable.\n\n    Any keyword arguments are accepted for API compatibility with other\n    kernel factories but are ignored here.\n\n    Returns\n    -------\n    callable\n        A function f(x1, x2) that:\n          * converts the two inputs to 1-D float `np.ndarray`s,\n          * returns \u20111 if their lengths differ,\n          * otherwise returns their dot product as a built-in ``float``.\n    \"\"\"\n    def f(x1, x2):\n        # Convert inputs to 1-D float arrays\n        v1 = np.asarray(x1, dtype=float).ravel()\n        v2 = np.asarray(x2, dtype=float).ravel()\n\n        # Length mismatch \u2192 sentinel value \u20111\n        if v1.shape[0] != v2.shape[0]:\n            return -1\n\n        # Dot product (linear kernel) as a Python float\n        return float(np.inner(v1, v2))\n\n    return f\n```"}
{"task_id": 470, "completion_id": 0, "completion": "```python\nimport os\n\n\ndef _strip_trailing_seps(path: str) -> str:\n    \"\"\"\n    Remove every trailing path-separator from *path* **except** when the path\n    itself is the root directory (``'/'`` on POSIX or ``'C:\\\\'`` on Windows).\n    \"\"\"\n    if path in (os.sep, os.path.abspath(os.sep)):      # Fast-path for POSIX\n        return path                                    # \u201c/\u201d must stay \u201c/\u201d\n\n    # On Windows we may also have an alternate separator (/ \u2194 \\)\n    seps = {os.sep}\n    if os.path.altsep:\n        seps.add(os.path.altsep)\n\n    # Strip the separators from the right edge\n    while len(path) > 1 and path[-1] in seps:\n        path = path[:-1]\n\n    return path\n\n\ndef get_scriptdir(file_path: str) -> str:\n    \"\"\"Return the absolute directory that contains *file_path*.\n\n    The function normalises *file_path* by resolving any symbolic links\n    and removing ``..`` / ``.`` path segments, then returns only the\n    directory component of this canonical path.\n\n    Args:\n        file_path: Path string pointing to a file.  May be absolute or\n                   relative and may contain symlinks or redundant path\n                   elements.\n\n    Returns:\n        The absolute directory path that contains *file_path*.  The\n        returned path never ends with a path separator (unless it is the\n        root directory).\n    \"\"\"\n    # 1. Resolve the path to its canonical, absolute form.\n    canonical_path = os.path.realpath(file_path)\n\n    # 2. Guarantee there is no trailing \u201c/\u201d (or \"\\\" on Windows) so that\n    #    os.path.dirname() treats the last path component as the file name.\n    canonical_path = _strip_trailing_seps(canonical_path)\n\n    # 3. Extract the directory portion.\n    directory = os.path.dirname(canonical_path)\n\n    # 4. Final normalisation: make sure no trailing separator (except root).\n    return _strip_trailing_seps(directory)\n```"}
{"task_id": 471, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One-step Expected-SARSA TD(0) update for a tabular Q function.\"\"\"\n    \n    n_actions = len(q_table[0])                         # |A|\n    \n    # -------- Expected value of next state's Q under \u03b5-soft policy ----------\n    if next_state is None:                              # terminal transition\n        expected_q_next = 0.0\n    else:\n        q_next = q_table[next_state]\n        \n        # Greedy action (first arg-max)\n        greedy_a = max(range(n_actions), key=lambda a: q_next[a])\n        \n        # Probabilities under \u03b5-soft policy\n        prob_non_greedy = epsilon / n_actions\n        probs = [prob_non_greedy] * n_actions\n        probs[greedy_a] = (1 - epsilon) + prob_non_greedy\n        \n        # Expectation \u03a3_a \u03c0(a|s') Q(s',a)\n        expected_q_next = sum(p * q for p, q in zip(probs, q_next))\n    \n    # ---------------------------- TD target ---------------------------------\n    target = reward + gamma * expected_q_next\n    \n    # --------------------------- Q-table update ------------------------------\n    q_table[state][action] += lr * (target - q_table[state][action])\n    \n    # --------------------------- Rounding step -------------------------------\n    rounded_q = [[round(val, 4) for val in row] for row in q_table]\n    \n    return rounded_q\n```"}
{"task_id": 472, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef softmax(x: list | list[list]):\n   Return the softmax probabilities and their element-wise gradient.\n\n    Parameters\n    ----------\n    x : list | list[list]\n        A one- or two-dimensional numeric list.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple (probabilities, gradient) where both elements keep the same\n        shape as *x* and are rounded to 4 decimal places.\n       \n    # ---- preparation -------------------------------------------------------\n    arr = np.asarray(x, dtype=float)           # work with NumPy internally\n\n    if arr.ndim == 0:                          # single scalar \u2192 treat as 1-D\n        arr = arr.reshape(1)\n    if arr.ndim not in (1, 2):\n        raise ValueErrorInput must be 1- or 2-dimensional    # ---- numerically-stable softmax ----------------------------------------\n    if arr.ndim == 1:                          # 1-D case\n        shifted = arr - np.max(arr)\n        exp_ = np.exp(shifted)\n        probs = exp_ / exp_.sum()\n    else:                                      # 2-D \u2192 row-wise\n        shifted = arr - np.max(arr, axis=1, keepdims=True)\n        exp_ = np.exp(shifted)\n        probs = exp_ / exp_.sum(axis=1, keepdims=True)\n\n    # ---- element-wise (diagonal) gradient ----------------------------------\n    grads = probs * (1.0 - probs)\n\n    # ---- rounding & conversion back to pure Python -------------------------\n    probs_list = np.round(probs, 4).tolist()\n    grads_list = np.round(grads, 4).tolist()\n\n    return probs_list, grads_list\n```"}
{"task_id": 474, "completion_id": 0, "completion": "```python\nimport math\nimport random\nimport numpy as np\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None\n                  ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n   Generate a 2-D toy data set consisting of axis-aligned rectangular\n    clusters.\n\n    Every class i is located in one of two horizontal rows:\n        row = i % 2                       (0 \u2192 lower, 1 \u2192 upper)\n        col = i // 2                      (column index)\n\n        x \u2208 [(col + 0.1) * 10 , (col + 0.9) * 10]\n        y \u2208 [(row*interval + 0.1) * 10 , (row*interval + 0.9) * 10]\n\n        label = (i - 0.5) * 2             (\u2026  -1, +1, 3, 5, \u2026)\n\n    All coordinates are rounded to four decimals.\n\n    Parameters\n    ----------\n    m         : int     \u2013 number of classes (\u22652)\n    n_train   : int     \u2013 samples per class for the training set\n    n_val     : int     \u2013 samples per class for the validation set\n    interval  : float   \u2013 vertical distance (inblocks of 10 between the two rows\n    seed      : int|None\u2013 optional seed for reproducibility\n\n    Returns\n    -------\n    X_train : (m*n_train, 2) ndarray\n    X_val   : (m*n_val,   2) ndarray\n    Y_train : (m*n_train,) ndarray\n    Y_val   : (m*n_val,)   ndarray\n       if m < 2:\n        raise ValueErrorm (number of classes) must be at least 2    if n_train <= 0 or n_val <= 0:\n        raise ValueErrorn_train and n_val must be positive    # ------------------------------------------------------------------ #\n    # 1.  Initialise RNGs (spec asks to call random.seed if seed given)   #\n    # ------------------------------------------------------------------ #\n    if seed is not None:\n        random.seed(seed)        # built-in RNG\n        np.random.seed(seed)     # NumPy RNG (so both are consistent)\n\n    # Containers for the whole data set\n    X_train_parts, Y_train_parts = [], []\n    X_val_parts,   Y_val_parts   = [], []\n\n    # ------------------------------------------------------------------ #\n    # 2.  Generate data class by class                                   #\n    # ------------------------------------------------------------------ #\n    for i in range(m):\n        col = i // 2\n        row = i % 2\n\n        x_low, x_high = (col + 0.1) * 10, (col + 0.9) * 10\n        y_low, y_high = ((row * interval) + 0.1) * 10, ((row * interval) + 0.9) * 10\n\n        label = int((i - 0.5) * 2)        #  \u2026  -1, +1, 3, 5, \u2026\n\n        # ----- training samples -----\n        xs_tr = np.random.uniform(x_low, x_high, size=n_train)\n        ys_tr = np.random.uniform(y_low, y_high, size=n_train)\n        X_train_parts.append(np.column_stack((xs_tr, ys_tr)))\n        Y_train_parts.append(np.full(n_train, label, dtype=int))\n\n        # ----- validation samples -----\n        xs_val = np.random.uniform(x_low, x_high, size=n_val)\n        ys_val = np.random.uniform(y_low, y_high, size=n_val)\n        X_val_parts.append(np.column_stack((xs_val, ys_val)))\n        Y_val_parts.append(np.full(n_val, label, dtype=int))\n\n    # ------------------------------------------------------------------ #\n    # 3.  Stack, round and return                                         #\n    # ------------------------------------------------------------------ #\n    X_train = np.round(np.vstack(X_train_parts), 4)\n    X_val   = np.round(np.vstack(X_val_parts),   4)\n    Y_train = np.concatenate(Y_train_parts)\n    Y_val   = np.concatenate(Y_val_parts)\n\n    return X_train, X_val, Y_train, Y_val\n```\n\nThe function now\n\n\u2022 works for any number of classes (`m \u2265 2`),  \n\u2022 accepts arbitrary per-class sample sizes,  \n\u2022 honours an optional random seed,  \n\u2022 delivers labels for both training *and* validation sets, and  \n\u2022 rounds every coordinate to four decimal places before returning the data."}
{"task_id": 475, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef adam_update(w, grad, m_prev, v_prev, t,\n                learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n   Perform a single Adam optimisation step    # --- ensure array-like objects (keeps scalars working too) ----------------\n    grad_arr = np.asarray(grad, dtype=float)\n\n    if m_prev is None:\n        m_prev_arr = np.zeros_like(grad_arr)\n    else:\n        m_prev_arr = np.asarray(m_prev, dtype=float)\n\n    if v_prev is None:\n        v_prev_arr = np.zeros_like(grad_arr)\n    else:\n        v_prev_arr = np.asarray(v_prev, dtype=float)\n\n    # --- update the two moving averages --------------------------------------\n    m_t = b1 * m_prev_arr + (1.0 - b1) * grad_arr\n    v_t = b2 * v_prev_arr + (1.0 - b2) * (grad_arr ** 2)\n\n    # --- bias corrections ----------------------------------------------------\n    m_hat = m_t / (1.0 - b1 ** t)\n    v_hat = v_t / (1.0 - b2 ** t)\n\n    # --- parameter update ----------------------------------------------------\n    w_arr = np.asarray(w, dtype=float)\n    w_new = w_arr - learning_rate * m_hat / (np.sqrt(v_hat) + eps)\n\n    # --- keep return types consistent with the inputs ------------------------\n    def _maybe_scalar(x, template):\n        # If the template was a Python scalar, return a scalar, else ndarray.\n        return x.item() if np.isscalar(template) else x\n\n    return (_maybe_scalar(w_new, w),\n            _maybe_scalar(m_t, m_prev if m_prev is not None else grad),\n            _maybe_scalar(v_t, v_prev if v_prev is not None else grad))\n```\n\nThe function follows exactly the Adam equations:  \n\u2022 updates first (m_t) and second (v_t) moments,  \n\u2022 corrects their bias (m_hat, v_hat),  \n\u2022 applies the parameter shift using the provided hyper-parameters.  \n\nIt treats missing `m_prev`/`v_prev` as zero arrays, works for scalars or arbitrary-shaped NumPy arrays, and returns the updated `(w, m, v)` in the same scalar/array form that the inputs had."}
{"task_id": 477, "completion_id": 0, "completion": "```python\nimport heapq\nfrom typing import List, Tuple\n\n\ndef astar(grid: List[List[int]], start: Tuple[int, int], dest: Tuple[int, int]) -> int:\n   Return the length of the shortest 4-directional path on a 0/1 grid\n    from *start* to *dest* using A* search.  If unreachable, return \u20111.\n       # Quick sanity checks ----------------------------------------------------\n    if not grid or not grid[0]:\n        return -1\n\n    rows, cols = len(grid), len(grid[0])\n    sr, sc = start\n    dr, dc = dest\n\n    # Start or goal out of bounds / inside an obstacle\n    if not (0 <= sr < rows and 0 <= sc < cols and\n            0 <= dr < rows and 0 <= dc < cols):\n        return -1\n    if grid[sr][sc] == 1 or grid[dr][dc] == 1:\n        return -1\n\n    # Trivial case: already at the goal\n    if start == dest:\n        return 0\n\n    # Heuristic: Manhattan distance -----------------------------------------\n    def h(r: int, c: int) -> int:\n        return abs(r - dr) + abs(c - dc)\n\n    # Priority queue elements: (f, g, r, c)\n    pq: List[Tuple[int, int, int, int]] = []\n    g_start = 0\n    heapq.heappush(pq, (g_start + h(sr, sc), g_start, sr, sc))\n\n    # Best cost discovered so far for each cell ------------------------------\n    best_g = {(sr, sc): 0}\n\n    # 4-directional moves\n    directions = ((1, 0), (-1, 0), (0, 1), (0, -1))\n\n    while pq:\n        f, g, r, c = heapq.heappop(pq)\n\n        # Reaching destination\n        if (r, c) == dest:\n            return g\n\n        # Skip if we have already found a better path to this cell\n        if g > best_g[(r, c)]:\n            continue\n\n        # Expand neighbours --------------------------------------------------\n        ng = g + 1  # cost to move to any neighbouring free cell\n        for drc, dcc in directions:\n            nr, nc = r + drc, c + dcc\n            if 0 <= nr < rows and 0 <= nc < cols and grid[nr][nc] == 0:\n                if ng < best_g.get((nr, nc), float('inf')):\n                    best_g[(nr, nc)] = ng\n                    heapq.heappush(pq, (ng + h(nr, nc), ng, nr, nc))\n\n    # Open list exhausted -> unreachable -------------------------------------\n    return -1\n```"}
{"task_id": 478, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_std_dev(X):\n   Return the population standard deviation for every feature in *X*.\n\n    Args:\n        X (list[list[int | float]] | np.ndarray): 2-D data set where rows are\n            samples and columns are features.\n\n    Returns:\n        list[float]: A list containing the standard deviation of each feature,\n            rounded to 4 decimal places.  If *X* is empty an empty list\n            is returned.\n       # Convert to a NumPy array of type float\n    arr = np.asarray(X, dtype=float)\n\n    # Handle empty input ([], np.array([]), or array with zero rows)\n    if arr.size == 0:\n        return []\n\n    # If a 1-D array is given, treat it as a single-feature data set\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n\n    n_samples = arr.shape[0]          # number of rows (samples)\n\n    # Compute the mean of each feature (column)\n    mean = arr.sum(axis=0) / n_samples\n\n    # Compute squared deviations from the mean\n    squared_diff = (arr - mean) ** 2\n\n    # Population variance for each feature: \u03c3\u00b2 = (1/n) * \u03a3 (x - \u03bc)\u00b2\n    variance = squared_diff.sum(axis=0) / n_samples\n\n    # Population standard deviation\n    std_dev = np.sqrt(variance)\n\n    # Round to 4 decimal places and return as a regular Python list\n    return np.round(std_dev, 4).tolist()\n```"}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n   Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit    \n    # --- seeding ------------------------------------------------------------\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # --- initialisation -----------------------------------------------------\n    n_arms = len(rewards[0]) if rewards else 0          # number of arms\n    value_estimates = np.full(n_arms, ev_prior, dtype=float)   # V_i\n    pull_counts     = np.zeros(n_arms, dtype=int)              # C_i\n    \n    # --- main loop ----------------------------------------------------------\n    for t_step in rewards:\n        # decide whether to explore or exploit\n        if np.random.rand() < epsilon:                # explore\n            arm = np.random.randint(n_arms)\n        else:                                         # exploit\n            arm = int(np.argmax(value_estimates))     # argmax breaks ties by lowest index\n        \n        # observe reward for the chosen arm at this time-step\n        r = t_step[arm]\n        \n        # update counter and value estimate (incremental mean)\n        pull_counts[arm] += 1\n        c = pull_counts[arm]\n        value_estimates[arm] += (r - value_estimates[arm]) / c\n    \n    # --- return rounded estimates ------------------------------------------\n    return [round(v, 4) for v in value_estimates]\n```"}
{"task_id": 480, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef mel_spectrogram(\n    x:np.ndarray    window_duration: float = 0.025,\n    stride_duration: float = 0.01,\n    mean_normalize: bool = True,\n    window: str =hamming    n_filters: int = 20,\n    center: bool = True,\n    alpha: float = 0.95,\n    fs: int = 44000,\n):\n   Return the Mel spectrogram of *x* together with the energy of every frame    x = np.asarray(x, dtype=np.float64).flatten()\n\n    # ------------------------------------------------------------- 1. pre\u2013emphasis\n    if alpha is not None and alpha != 0:\n        x = np.concatenate(([x[0]], x[1:] - alpha * x[:-1]))\n\n    # ------------------------------------------------------------- 2. framing\n    frame_len = int(round(window_duration * fs))           # N\n    hop_len = int(round(stride_duration * fs))\n\n    if center:\n        pad = frame_len // 2\n        x = np.pad(x, (pad, pad), modereflect    signal_len = len(x)\n    if signal_len < frame_len:                             # zero-pad if too short\n        x = np.pad(x, (0, frame_len - signal_len), modeconstant        signal_len = len(x)\n\n    n_frames = 1 + (signal_len - frame_len) // hop_len\n    idx = (\n        np.arange(n_frames)[:, None] * hop_len\n        + np.arange(frame_len)[None, :]\n    )\n    frames = x[idx]                                        # (G \u00d7 N)\n\n    # ------------------------------------------------------------- 3. windowing\n    n = np.arange(frame_len)\n    if window.lower() ==hann        win = 0.5 - 0.5 * np.cos(2.0 * np.pi * n / (frame_len - 1))\n    elif window.lower() ==blackman_harris        a0, a1, a2, a3 = 0.35875, 0.48829, 0.14128, 0.01168\n        w = 2.0 * np.pi * n / (frame_len - 1)\n        win = (\n            a0\n            - a1 * np.cos(w)\n            + a2 * np.cos(2 * w)\n            - a3 * np.cos(3 * w)\n        )\n    else:  # default: hamming\n        win = 0.54 - 0.46 * np.cos(2.0 * np.pi * n / (frame_len - 1))\n\n    frames *= win[None, :]\n\n    # ------------------------------------------------------------- 4. power spectrum\n    spec = np.fft.rfft(frames, n=frame_len, axis=1)\n    power = (np.abs(spec) ** 2) / frame_len                # (G \u00d7 (N/2+1))\n\n    # ------------------------------------------------------------- 5. Mel filter bank\n    def hz_to_mel(f):\n        return 2595.0 * np.log10(1.0 + f / 700.0)\n\n    def mel_to_hz(m):\n        return 700.0 * (10 ** (m / 2595.0) - 1.0)\n\n    mel_max = hz_to_mel(fs / 2.0)\n    mel_points = np.linspace(0.0, mel_max, n_filters + 2)\n    hz_points = mel_to_hz(mel_points)\n\n    bins = np.floor((frame_len) * hz_points / fs).astype(int)\n    n_fft_bins = power.shape[1]\n\n    fbank = np.zeros((n_filters, n_fft_bins))\n    for m in range(1, n_filters + 1):\n        f_left, f_center, f_right = bins[m - 1], bins[m], bins[m + 1]\n\n        if f_center == f_left:  # guard against identical bins\n            f_center += 1\n        if f_right == f_center:\n            f_right += 1\n\n        # rising slope\n        fbank[m - 1, f_left:f_center] = (\n            np.arange(f_left, f_center) - f_left\n        ) / (f_center - f_left)\n        # falling slope\n        fbank[m - 1, f_center:f_right] = (\n            f_right - np.arange(f_center, f_right)\n        ) / (f_right - f_center)\n\n    # ------------------------------------------------------------- 6. filtering\n    filter_energies = power @ fbank.T                      # (G \u00d7 n_filters)\n\n    # ------------------------------------------------------------- 7. mean normalization\n    if mean_normalize:\n        filter_energies -= filter_energies.mean(axis=0, keepdims=True)\n\n    # ------------------------------------------------------------- 8. numerical safety\n    eps = np.finfo(float).eps\n    filter_energies[filter_energies == 0] = eps\n\n    energy_per_frame = power.sum(axis=1)\n    energy_per_frame[energy_per_frame == 0] = eps\n\n    # ------------------------------------------------------------- format output\n    filter_energies = np.round(filter_energies, 4).tolist()\n    energy_per_frame = np.round(energy_per_frame, 4).tolist()\n\n    return filter_energies, energy_per_frame\n```"}
{"task_id": 481, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighbourhood to be considered reachable\n        (Euclidean distance).\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labelled \u22121 and\n        cluster indices start at 0 and increase sequentially in discovery\n        order.\n    \"\"\"\n    # ---------- basic sanity checks ----------\n    if data.size == 0:           # empty input\n        return []\n\n    if eps < 0:\n        raise ValueError(\"eps must be non-negative\")\n    if min_samples <= 0:\n        raise ValueError(\"min_samples must be positive\")\n    if data.ndim != 2:\n        raise ValueError(\"data must be a two-dimensional array\")\n\n    n_samples = data.shape[0]\n\n    # ---------- pre\u2013compute \u03b5-neighbourhoods ----------\n    # We store for every point the indices of its neighbours (including itself)\n    # whose Euclidean distance is \u2264 eps.\n    neighbourhoods: list[np.ndarray] = []\n    for i in range(n_samples):\n        # broadcasting to obtain all distances from point i\n        diff = data - data[i]\n        dist = np.linalg.norm(diff, axis=1)\n        neighbourhoods.append(np.where(dist <= eps)[0])\n\n    # ---------- bookkeeping arrays ----------\n    UNVISITED = 0\n    VISITED = 1\n    state = np.zeros(n_samples, dtype=np.uint8)     # UNVISITED / VISITED\n    labels = np.full(n_samples, -1, dtype=int)      # initialise as noise\n    cluster_id = 0\n\n    # ---------- main loop ----------\n    for point_idx in range(n_samples):\n        # already processed?\n        if state[point_idx] == VISITED:\n            continue\n\n        state[point_idx] = VISITED\n        neighbours = neighbourhoods[point_idx]\n\n        # not enough neighbours \u2192 noise (label already \u22121)\n        if neighbours.size < min_samples:\n            continue\n\n        # otherwise: start a new cluster\n        labels[point_idx] = cluster_id\n        seeds = list(neighbours)            # list so that we can iterate & grow\n        i = 0                               # index of current element in seeds\n        while i < len(seeds):\n            current = seeds[i]\n            if state[current] == UNVISITED:\n                state[current] = VISITED\n                current_neigh = neighbourhoods[current]\n                if current_neigh.size >= min_samples:\n                    # add *new* neighbours to the seed list\n                    for nb in current_neigh:\n                        if nb not in seeds:     # linear lookup, still O(n\u00b2) worst-case\n                            seeds.append(nb)\n\n            # If the point is not yet assigned to any cluster, assign it\n            if labels[current] == -1:\n                labels[current] = cluster_id\n            i += 1\n\n        cluster_id += 1                       # finished growing one cluster\n\n    return labels.tolist()\n```"}
{"task_id": 482, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n   Convert integer labels to one-hot encoded 2-D array.\n\n    Parameters\n    ----------\n    y : array_like\n        1-D sequence (list/tuple/ndarray) of non-negative integer labels.\n    num_classes : int or None, optional\n        Desired number of columns (= classes).  If ``None`` the value is\n        inferred as ``max(y) + 1``.  If supplied, every label in ``y`` must be\n        in the half-open interval ``[0, num_classes)`` or a ValueError is raised.\n\n    Returns\n    -------\n    np.ndarray\n        Matrix of shape ``(len(y), num_classes)`` containing only 0s and 1s\n        (dtype=int), the one-hot representation of the input labels.\n       # Convert input to a NumPy array and make sure it is 1-D\n    y = np.asarray(y, dtype=int)\n    if y.ndim != 1:\n        raise ValueError`y` must be a 1-D array of integer labels    # Handle empty input early (compatible with both behaviours)\n    if y.size == 0:\n        if num_classes is None:\n            raise ValueErrorCannot infer `num_classes` from an empty input.                            Provide `num_classes` explicitly        return np.zeros((0, int(num_classes)), dtype=int)\n\n    # Infer or validate the number of classes\n    if num_classes is None:\n        num_classes = int(y.max()) + 1\n    else:\n        num_classes = int(num_classes)\n        if num_classes <= 0:\n            raise ValueError`num_classes` must be a positive integer        if (y < 0).any() or (y >= num_classes).any():\n            raise ValueErrorAll labels must satisfy 0 <= label < num_classes    # Create the output array of zeros and set the appropriate elements to 1\n    result = np.zeros((y.size, num_classes), dtype=int)\n    result[np.arange(y.size), y] = 1\n    return result\n```\n\nExplanation of key points:\n\n1. The input is converted to a NumPy array with an integer dtype; a 1-D check enforces correct shape.\n2. If `num_classes` is omitted, it is inferred from the largest label (`max(y) + 1)` as specified.\n3. If `num_classes` is provided, each label is validated to be within `[0, num_classes)`.\n4. The output is created with `np.zeros` and filled using advanced indexing, ensuring the array contains only integers `0` and `1`."}
{"task_id": 483, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(labels, num_classes=None):\n    \"\"\"Convert integer class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    labels : sequence of int or 1-D np.ndarray\n        The class labels to transform.\n    num_classes : int, optional\n        Desired length of the one-hot vectors; if omitted the function will\n        use ``max(labels) + 1``.\n\n    Returns\n    -------\n    list[list[int]]\n        One-hot encoded representation of ``labels`` containing only\n        Python ints (0 or 1).\n    \"\"\"\n    # Convert the input to a 1-D NumPy array of integers\n    labels = np.asarray(labels, dtype=int).ravel()\n\n    # Empty input: return an empty list (respecting an explicitly supplied\n    # num_classes, if any)\n    if labels.size == 0:\n        return [] if num_classes is None else [[] for _ in range(0)]\n\n    # Infer number of classes if not provided\n    if num_classes is None:\n        num_classes = labels.max() + 1\n\n    # Build the one-hot representation\n    one_hot = [\n        [1 if class_idx == label else 0 for class_idx in range(num_classes)]\n        for label in labels\n    ]\n\n    return one_hot\n```"}
{"task_id": 484, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef lasso_regression_predict(\n    X_train: list[float | int],\n    y_train: list[float | int],\n    X_test: list[float | int],\n    degree: int,\n    reg_factor: float = 0.1,\n    n_iterations: int = 1000,\n) -> list[float]:\n   Fit a one\u2013dimensional Lasso-regression model (via coordinate descent)\n    and return the predictions on `X_test` (rounded to 4 decimals).    # -------------------- helper functions -------------------- #\n    def poly_design(x: np.ndarray, deg: int) -> np.ndarray:\n       Column\u2013stacked polynomial features [x**0, \u2026, x**deg].        return np.column_stack([x ** d for d in range(deg + 1)])\n\n    def soft_threshold(rho: float, lam: float) -> float:\n       Soft-thresholding operator S(rho, lam).        if rho > lam:\n            return rho - lam\n        if rho < -lam:\n            return rho + lam\n        return 0.0\n\n    # -------------------- data preparation -------------------- #\n    x_tr = np.asarray(X_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=float)\n    x_te = np.asarray(X_test, dtype=float)\n\n    \u03a6 = poly_design(x_tr, degree)          # design matrix for training\n    \u03a6_te = poly_design(x_te, degree)       # design matrix for test\n    n_samples, n_feats = \u03a6.shape\n\n    # -------------------- coordinate-descent -------------------- #\n    w = np.zeros(n_feats)                  # initial weights\n    z = (\u03a6 ** 2).sum(axis=0)               # \u2211_i \u03c6_ij\u00b2  \u2013   constant\n    r = y_tr.copy()                        # current residual  y - \u03a6w  (w=0 \u21d2 r=y)\n\n    lam = float(reg_factor)\n\n    for _ in range(n_iterations):\n        for j in range(n_feats):\n            # \u03c1_j = \u03c6_j\u1d40 r  +  z_j w_j      (because r = y - \u03a6w)\n            rho_j = \u03a6[:, j] @ r + z[j] * w[j]\n\n            # weight update\n            if j == 0:                     # bias (not regularised)\n                w_new = rho_j / z[j] if z[j] != 0 else 0.0\n            else:\n                w_new = soft_threshold(rho_j, lam) / z[j] if z[j] != 0 else 0.0\n\n            # residual update   r \u2190 r - \u03c6_j (w_new - w_old)\n            delta = w_new - w[j]\n            if delta != 0.0:               # skip work if nothing changed\n                r -= \u03a6[:, j] * delta\n                w[j] = w_new\n\n    # -------------------- prediction -------------------- #\n    y_pred = \u03a6_te @ w\n    return [round(float(val), 4) for val in y_pred]\n```"}
{"task_id": 485, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math                                   # only used for ``math.inf``\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n   Fit a univariate regression tree of depth *max_depth* and predict    # ------------------------------------------------------------------ #\n    # 0.  trivial empty-training-set case\n    # ------------------------------------------------------------------ #\n    if len(X_train) == 0:\n        return []\n    # convert once to numpy arrays \u2013 convenient and still allowed\n    X_train = np.asarray(X_train, dtype=float)\n    y_train = np.asarray(y_train, dtype=float)\n\n    # ------------------------------------------------------------------ #\n    # 1.  tree construction\n    # ------------------------------------------------------------------ #\n    def build(indices: np.ndarray, depth: int = 0):\n       Recursively build a CART regression tree.\n\n        Returns\n        -------\n        dict\n            Internal node: {'split': float, 'left': node, 'right': node}\n            Leaf         : {'split': None , 'pred': float}\n               # leaf-creation rule ------------------------------------------------\n        if depth == max_depth or indices.size < 2:\n            return {'split': None,               # leaf marker\n                    'pred':  float(y_train[indices].mean())}\n\n        # sort the current samples by their single feature\n        order      = np.argsort(X_train[indices])\n        idx_sorted = indices[order]\n        x_sorted   = X_train[idx_sorted]\n        y_sorted   = y_train[idx_sorted]\n\n        # prefix sums for efficient SSE calculation ------------------------\n        y_cumsum   = np.cumsum(y_sorted)\n        y2_cumsum  = np.cumsum(y_sorted ** 2)\n\n        best_sse      = math.inf\n        best_split_at = None          # position in *x_sorted*\n        n             = idx_sorted.size\n\n        # consider every possible split between different x-values ----------\n        for i in range(1, n):\n            if x_sorted[i-1] == x_sorted[i]:     # identical \u21d2 would give empty child\n                continue\n            n_left   = i\n            n_right  = n - i\n\n            sum_left  = y_cumsum[i-1]\n            sum2_left = y2_cumsum[i-1]\n\n            sum_right  = y_cumsum[-1]  - sum_left\n            sum2_right = y2_cumsum[-1] - sum2_left\n\n            sse_left  = sum2_left  - (sum_left  ** 2) / n_left\n            sse_right = sum2_right - (sum_right ** 2) / n_right\n\n            sse_total = sse_left + sse_right\n            if sse_total < best_sse:\n                best_sse      = sse_total\n                best_split_at = i\n\n        # if no admissible split exists (all x equal) \u203a leaf ---------------\n        if best_split_at is None:\n            return {'split': None, 'pred': float(y_sorted.mean())}\n\n        # determine the numeric split value  \u2013 midway between the two points\n        split_value = (x_sorted[best_split_at - 1] + x_sorted[best_split_at]) / 2.0\n\n        # recurse on the two partitions ------------------------------------\n        left_indices  = idx_sorted[:best_split_at]\n        right_indices = idx_sorted[best_split_at:]\n\n        return {'split': split_value,\n                'left' : build(left_indices , depth + 1),\n                'right': build(right_indices, depth + 1)}\n\n    root = build(np.arange(len(X_train)))\n\n    # ------------------------------------------------------------------ #\n    # 2.  prediction helper\n    # ------------------------------------------------------------------ #\n    def predict_one(x: float, node: dict) -> float:\n       Traverse the tree until a leaf is reached        while node['split'] is not None:          # internal node\n            node = node['left'] if x <= node['split'] else node['right']\n        return node['pred']\n\n    # ------------------------------------------------------------------ #\n    # 3.  vectorised predictions for X_test\n    # ------------------------------------------------------------------ #\n    return [predict_one(float(x), root) for x in X_test]\n```"}
{"task_id": 486, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n   Compute the mean-squared error (MSE) between two numeric sequences.\n\n    If the two inputs do not have the same non-zero length, the function\n    returns -1.  Otherwise it returns the MSE rounded to 4 decimal places.\n\n    Args:\n        y_true: Ground-truth values (list, tuple or NumPy array).\n        y_pred: Predicted values (list, tuple or NumPy array).\n\n    Returns:\n        float: MSE rounded to 4 decimals, or -1 on error/invalid input.\n       try:\n        # Convert to 1-D NumPy arrays of floats\n        true_arr = np.asarray(y_true, dtype=float).ravel()\n        pred_arr = np.asarray(y_pred, dtype=float).ravel()\n    except Exception:\n        # Conversion failed (e.g., non-numeric data)\n        return -1\n\n    # Validate lengths\n    if true_arr.size == 0 or true_arr.size != pred_arr.size:\n        return -1\n\n    # Compute MSE\n    mse = np.mean((true_arr - pred_arr) ** 2)\n\n    # Round to 4 decimal places and return as float\n    return float(round(mse, 4))\n```"}
{"task_id": 487, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_means(X: np.ndarray, k: int, max_iters: int = 300, tol: float = 1e-4) -> list[int]:\n   Cluster *X* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape = (n_samples, n_features)\n        The data to cluster.\n    k : int\n        Number of clusters (1 \u2264 k \u2264 n_samples).\n    max_iters : int, default=300\n        Maximum number of iterations.\n    tol : float, default=1e-4\n        Convergence tolerance. Iteration stops when the Euclidean\n        distance between old and new centroids is below *tol*.\n\n    Returns\n    -------\n    list[int]\n        Cluster index (0 \u2026 k-1) for each sample in the original order.\n       # ---- initialisation ----------------------------------------------------\n    centroids = X[:k].astype(float).copy()     # shape (k, n_features)\n\n    for _ in range(max_iters):\n        # ---- assignment step ----------------------------------------------\n        # distances: shape (n_samples, k)\n        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = distances.argmin(axis=1)      # shape (n_samples,)\n\n        # ---- update step ---------------------------------------------------\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):                   # centroid keeps its position if it lost all members\n                new_centroids[j] = X[mask].mean(axis=0)\n\n        # ---- convergence check --------------------------------------------\n        shift = np.linalg.norm(new_centroids - centroids)\n        centroids = new_centroids\n        if shift < tol:\n            break\n\n    return labels.tolist()\n```"}
{"task_id": 488, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_variance(X):\n    \"\"\"Return the population variance of every feature (column) in X.\n\n    Parameters\n    ----------\n    X : list[list[int | float]] | np.ndarray\n        A 2-D data set where each row is a sample and each column is a feature.\n\n    Returns\n    -------\n    list[float] | int\n        A list with the per-feature population variances rounded to 4 decimals,\n        or \u22121 when the input is empty, has <1 sample, or is not strictly 2-D.\n    \"\"\"\n    # ---------- Convert input to a NumPy array (if not already) ----------\n    arr = np.asarray(X, dtype=float)\n    \n    # ---------- Basic validity checks ----------\n    # Must be 2-D, have at least 1 row, and at least 1 column\n    if arr.ndim != 2 or arr.shape[0] < 1 or arr.shape[1] < 1:\n        return -1\n    \n    # NumPy will create a 1-D object array for ragged lists \u2013 catch that\n    if arr.dtype == object:\n        return -1\n    \n    # ---------- Population variance calculation ----------\n    # For population variance use ddof=0 (default) and divide by n\n    variances = arr.var(axis=0)           # NumPy already returns float64\n    \n    # ---------- Rounding & returning ----------\n    rounded = [round(v, 4) for v in variances.tolist()]\n    return rounded\n```"}
{"task_id": 489, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_nominal(one_hot):\n    \"\"\"Convert a batch of one-hot encoded vectors to nominal class labels.\n\n    Parameters\n    ----------\n    one_hot : list[list[int | float]] | numpy.ndarray\n        A 2-D structure where each row is expected to be a one-hot\n        encoded vector (all zeros except for a single 1).\n\n    Returns\n    -------\n    list[int] | int\n        A list of integer class labels when the input is valid;\n        otherwise the integer \u20111.\n    \"\"\"\n    # Convert the input to a NumPy array for uniform processing.\n    arr = np.asarray(one_hot)\n\n    # The input must be 2-D and non-empty.\n    if arr.ndim != 2 or arr.size == 0:\n        return -1\n\n    # All elements must be exactly 0 or 1.\n    if not np.logical_or(arr == 0, arr == 1).all():\n        return -1\n\n    # Every row must contain exactly one '1'.\n    if not np.all(arr.sum(axis=1) == 1):\n        return -1\n\n    # Retrieve the index of the '1' in each row.\n    return arr.argmax(axis=1).tolist()\n```"}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n   Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n\n    Raises:\n        ValueError: If *x* is not one-dimensional.\n        TypeError:  If *x* is not a list, tuple, or NumPy ndarray.\n       # -------------------- Normalise input to a NumPy 1-D array --------------------\n    if isinstance(x, (list, tuple)):\n        vec = np.asarray(x)\n    elif isinstance(x, np.ndarray):\n        vec = x\n    else:\n        raise TypeErrorInput must be a list, tuple, or NumPy ndarray    # Ensure the data are 1-D\n    if vec.ndim != 1:\n        raise ValueErrorInput must be one-dimensional    # -------------------- Handle empty input --------------------------------------\n    n = vec.size\n    if n == 0:\n        return []\n\n    # -------------------- Build diagonal matrix -----------------------------------\n    diag_matrix = np.diag(vec)        # Places vec on the principal diagonal\n\n    # -------------------- Return as list of lists ---------------------------------\n    return diag_matrix.tolist()\n```"}
{"task_id": 491, "completion_id": 0, "completion": "```python\nfrom collections import Counter, defaultdict\nimport itertools\n\n\nclass FPNode:\n    \"\"\"A node of an FP\u2013tree.\"\"\"\n\n    __slots__ = (\"item\", \"count\", \"parent\", \"children\", \"link\")\n\n    def __init__(self, item, parent):\n        self.item = item          # item label (None for root)\n        self.count = 0            # support (only meaningful if item is not None)\n        self.parent = parent      # pointer to parent node\n        self.children = {}        # item -> FPNode (child)\n        self.link = None          # next node that carries the same item\n\n\n# --------------------------------------------------------------------------- #\n#                           FP-tree construction                              #\n# --------------------------------------------------------------------------- #\ndef build_tree(transactions, min_sup, order):\n    \"\"\"\n    Build an FP-tree from 'transactions'.\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        Already filtered transactions (only frequent items) whose items are\n        ordered according to 'order'.\n    min_sup : int\n        The minimum support used while building conditional trees.\n    order : dict[item] -> int\n        Ranks that define the descending frequency order (smaller rank = earlier).\n    Returns\n    -------\n    root : FPNode\n    header_table : dict[item] -> (head, tail)\n        For every item the first and last node in its linked list.\n        If an item does not occur, it is absent from the dict.\n    \"\"\"\n    root = FPNode(None, None)\n    header_table = {}      # item -> [head, tail]\n\n    for transaction, trans_count in transactions:\n        # remove still infrequent items (needed in recursive calls)\n        items = [i for i in transaction if i in order]\n        if not items:\n            continue\n        # local descending frequency order\n        items.sort(key=lambda x: order[x])\n\n        # insert into tree\n        cur = root\n        for item in items:\n            if item in cur.children:\n                child = cur.children[item]\n                child.count += trans_count\n            else:\n                child = FPNode(item, cur)\n                child.count = trans_count\n                cur.children[item] = child\n\n                # link it into the header table\n                if item not in header_table:\n                    header_table[item] = [child, child]\n                else:\n                    header_table[item][1].link = child\n                    header_table[item][1] = child  # move tail\n            cur = child\n    return root, header_table\n\n\n# --------------------------------------------------------------------------- #\n#                              Helper utilities                               #\n# --------------------------------------------------------------------------- #\ndef single_path(node):\n    \"\"\"\n    Check whether the subtree rooted at 'node' contains only a single path.\n    Returns the list of nodes along that path (excluding the root) if it is a\n    path, otherwise None.\n    \"\"\"\n    path = []\n    while True:\n        if len(node.children) == 0:\n            return path\n        if len(node.children) > 1:\n            return None\n        child = next(iter(node.children.values()))\n        path.append(child)\n        node = child\n\n\ndef power_set(items):\n    \"\"\"All non-empty combinations of 'items'.\"\"\"\n    for r in range(1, len(items) + 1):\n        for comb in itertools.combinations(items, r):\n            yield comb\n\n\n# --------------------------------------------------------------------------- #\n#                          Recursive FP-Growth core                           #\n# --------------------------------------------------------------------------- #\ndef mine_tree(root, header_table, min_sup, prefix, results):\n    \"\"\"\n    Recursively mine the FP-tree.\n    Parameters\n    ----------\n    root : FPNode\n    header_table : dict[item] -> (head, tail)\n    prefix : list[item]\n        Items that are already fixed (the suffix pattern).\n    results : set[tuple]\n        Global result container \u2013 gets filled in-place.\n    \"\"\"\n    # items in increasing support order (Tie-break lexicographically)\n    sorted_items = sorted(header_table.keys(),\n                          key=lambda itm: (   # ascending support\n                              sum(n.count for n in iter_nodes(header_table[itm][0])),\n                              itm))\n\n    for item in sorted_items:\n        new_freq_set = prefix + [item]\n        results.add(tuple(sorted(new_freq_set)))\n\n        # Build conditional pattern base\n        conditional_transactions = []\n        node = header_table[item][0]\n        while node:\n            path = []\n            parent = node.parent\n            while parent and parent.item is not None:\n                path.append(parent.item)\n                parent = parent.parent\n            if path:\n                # store the path and count\n                conditional_transactions.append((path, node.count))\n            node = node.link\n\n        if not conditional_transactions:\n            continue\n\n        # first scan of conditional DB to obtain local frequent items\n        counter = Counter()\n        for path, cnt in conditional_transactions:\n            counter.update({i: cnt for i in path})\n\n        cond_freq_items = {i for i, c in counter.items() if c >= min_sup}\n        if not cond_freq_items:\n            continue\n\n        # ranking for the conditional tree (descending support, tie lexicographical)\n        rank = {i: r for r, (i, _) in\n                enumerate(sorted(counter.items(),\n                                 key=lambda x: (-x[1], x[0])))}\n\n        # construct conditional tree\n        filtered_transactions = [([i for i in path if i in cond_freq_items], cnt)\n                                 for path, cnt in conditional_transactions]\n        cond_root, cond_header = build_tree(filtered_transactions, min_sup, rank)\n\n        # Is the conditional tree a single path?\n        path_nodes = single_path(cond_root)\n        if path_nodes is not None:\n            items_on_path = [n.item for n in path_nodes]\n            for comb in power_set(items_on_path):\n                results.add(tuple(sorted(new_freq_set + list(comb))))\n        else:\n            # mine recursively\n            mine_tree(cond_root, cond_header, min_sup, new_freq_set, results)\n\n\ndef iter_nodes(start_node):\n    \"\"\"Iterate over the linked list of nodes with the same item.\"\"\"\n    node = start_node\n    while node:\n        yield node\n        node = node.link\n\n\n# --------------------------------------------------------------------------- #\n#                                 Front-end                                   #\n# --------------------------------------------------------------------------- #\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable\n        containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered\n        frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.\n        Every tuple itself is sorted lexicographically.\n    \"\"\"\n    if min_sup <= 0:\n        raise ValueError(\"min_sup must be > 0\")\n\n    # ----------------------------------------------------------------------- #\n    # First full scan \u2013 frequency of single items                             #\n    # ----------------------------------------------------------------------- #\n    transactions = [list(dict.fromkeys(t))          # remove duplicates in tx\n                    for t in transactions]          # (dict preserves order)\n    item_counter = Counter(itertools.chain.from_iterable(transactions))\n    freq_items = {i for i, c in item_counter.items() if c >= min_sup}\n    if not freq_items:\n        return []\n\n    # global ordering: descending frequency, then lexicographical\n    global_rank = {i: r for r, (i, _) in\n                   enumerate(sorted(((i, item_counter[i]) for i in freq_items),\n                                    key=lambda x: (-x[1], x[0])))}\n\n    # prepare transactions: filter + keep multiplicity 1\n    filtered_transactions = [([i for i in t if i in freq_items], 1)\n                             for t in transactions]\n\n    # ----------------------------------------------------------------------- #\n    # Build the initial FP-tree                                               #\n    # ----------------------------------------------------------------------- #\n    root, header_table = build_tree(filtered_transactions, min_sup, global_rank)\n\n    # ----------------------------------------------------------------------- #\n    # Mine the tree                                                           #\n    # ----------------------------------------------------------------------- #\n    results = set()\n    mine_tree(root, header_table, min_sup, [], results)\n\n    # ----------------------------------------------------------------------- #\n    # Sort result as required                                                 #\n    # ----------------------------------------------------------------------- #\n    return sorted(results, key=lambda x: (len(x), x))\n```\n\nThe routine takes the database and `min_sup`, constructs an FP-tree, mines it recursively (using the FP-Growth scheme with single-path optimisation), and finally returns all frequent item-sets sorted\n\n1. by their length and  \n2. lexicographically,\n\nwith every tuple itself being in lexicographic order."}
{"task_id": 492, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to the supplied degree.\n    \n    Each row of the returned matrix contains every monomial of the original\n    features whose total degree does not exceed *degree*.  Columns are ordered\n    first by rising total degree, then\u2014inside the same degree\u2014by the natural\n    lexicographic order produced by ``itertools.combinations_with_replacement``.\n    \n    The very first column therefore consists of 1\u2006s (the degree-0 term).\n    \n    If *degree* is negative the function returns -1.\n    \n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Input data of shape (n_samples, n_features).\n    degree : int\n        Maximum total degree of the generated polynomial terms.\n    \n    Returns\n    -------\n    list[list[float]]\n        The enlarged design matrix (Python list of lists) or -1 if\n        *degree* < 0.\n    \"\"\"\n    # ----- sanity check ----------------------------------------------------\n    if degree < 0:\n        return -1\n\n    # Convert input to a NumPy array (float) for easy arithmetic\n    X_arr = np.asarray(X, dtype=float)\n    if X_arr.ndim != 2:  # guard against malformed input\n        raise ValueError(\"X must be a 2-D array-like structure\")\n    n_samples, n_features = X_arr.shape\n\n    # List that will hold one 1-D array per output column\n    columns = [np.ones(n_samples, dtype=float)]          # degree-0 (constant) term\n\n    # Generate higher-degree terms\n    for k in range(1, degree + 1):\n        for comb in combinations_with_replacement(range(n_features), k):\n            # `comb` is a tuple of column indices, e.g. (0, 0, 2)\n            cols = list(comb)                             # make it a list for fancy indexing\n            term = np.prod(X_arr[:, cols], axis=1)        # product along the selected columns\n            columns.append(term)\n\n    # Stack every column side by side: shape -> (n_samples, n_output_features)\n    expanded = np.column_stack(columns)\n\n    return expanded.tolist()\n```"}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of the squared differences between each element\n    and the sample mean.  If *y* is empty the function returns -1.\n\n    Parameters\n    ----------\n    y : list[float | int] | numpy.ndarray\n        One-dimensional collection of numbers.\n\n    Returns\n    -------\n    float\n        Mean-squared error rounded to four decimal places, or -1 if *y* is\n        empty.\n    \"\"\"\n    # Convert to a NumPy array (no copy if already an ndarray)\n    arr = np.asarray(y, dtype=float)\n\n    # Handle empty input\n    if arr.size == 0:\n        return -1\n\n    # In case a non-1-D array sneaks in, flatten it\n    arr = arr.ravel()\n\n    mean = arr.mean()\n    mse_val = np.mean((arr - mean) ** 2)\n\n    # Round to four decimal places and return a Python float\n    return float(round(mse_val, 4))\n```"}
{"task_id": 494, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Union\n\n\ndef aggregate_predictions(\n    preds: Union[List[List[Union[int, float]]], np.ndarray],\n    classifier: bool = True,\n) -> List[Union[int, float]]:\n   Aggregate ensemble predictions.\n\n    Parameters\n    ----------\n    preds : list[list[int | float]] | np.ndarray\n        A 2-D structure where each row holds the predictions from one model\n        and each column corresponds to a data sample.\n    classifier : bool, default True\n        If True the task is classification and majority voting is used.\n        If False the task is regression and simple averaging is used.\n\n    Returns\n    -------\n    list[int | float]\n        Final aggregated predictions for every sample. For regression, values\n        are rounded to 4 decimal places.\n       # Convert to NumPy array for easier manipulation\n    preds = np.asarray(preds)\n\n    # Sanity-check: it must be 2-D (n_models, n_samples)\n    if preds.ndim != 2:\n        raise ValueError(\n            fpreds` must be 2-D (got shape {preds.shape!r}).        )\n\n    if classifier:\n        # ---- Majority vote (column-wise) ----\n        def _vote(col: np.ndarray) -> Union[int, float]:\n            labels, counts = np.unique(col, return_counts=True)\n            # labels are returned in ascending order, so on ties np.argmax\n            # naturally picks the smallest label.\n            return labels[np.argmax(counts)]\n\n        # Apply the voting function to every column (axis=0 is models)\n        agg = np.apply_along_axis(_vote, axis=0, arr=preds)\n        return agg.tolist()\n\n    else:\n        # ---- Regression: arithmetic mean ----\n        agg = preds.mean(axis=0)\n        agg = np.round(agg, 4)\n        return agg.tolist()\n```"}
{"task_id": 495, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef standardize(X: np.ndarray) -> list[list[float]]:\n   Standardize every column of a 2-D NumPy array.\n    \n    Each feature (column) is transformed to have mean 0 and variance 1.\n    Columns whose variance is zero become all zeros.  The input array\n    is *not* modified in-place.\n\n    Args\n    ----\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        Standardized data rounded to 4 decimal places, as a Python list.\n       if X.ndim != 2:\n        raise ValueErrorX must be a 2-D NumPy array    \n    # Compute column-wise mean and population standard deviation\n    means = X.mean(axis=0)\n    stds  = X.std(axis=0)          # ddof=0 by default\n    \n    # To avoid division by zero, temporarily replace 0 std with 1\n    stds_safe = stds.copy()\n    zero_std_mask = stds_safe == 0\n    stds_safe[zero_std_mask] = 1.0\n    \n    # Perform standardization on a *new* array\n    Z = (X - means) / stds_safe\n    \n    # Set columns that originally had zero std to all zeros\n    if zero_std_mask.any():\n        Z[:, zero_std_mask] = 0.0\n    \n    # Round to 4 decimal places and convert to Python list\n    return np.round(Z, 4).tolist()\n```"}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list, NumPy array, or scalar containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a 1-D NumPy array (scalars become 1-element arrays)\n    arr = np.atleast_1d(np.asarray(x, dtype=float)).ravel()\n\n    # Affine transformation\n    y = slope * arr + intercept\n\n    # First derivative (constant = slope)\n    grad = np.full_like(arr, slope, dtype=float)\n\n    # Second derivative (always 0 for an affine function)\n    grad2 = np.zeros_like(arr, dtype=float)\n\n    # Round to 4 decimal places and convert to Python lists\n    y_list     = np.round(y,     4).tolist()\n    grad_list  = np.round(grad,  4).tolist()\n    grad2_list = np.round(grad2, 4).tolist()\n\n    return y_list, grad_list, grad2_list\n```"}
{"task_id": 497, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef exponential_activation(x, order: int = 0):\n    \"\"\"Compute the exponential activation or one of its first two derivatives.\n\n    Parameters\n    ----------\n    x : list | numpy.ndarray\n        A (possibly nested) list or NumPy array containing real numbers.\n    order : int, optional\n        Indicates what to compute:\n        * 0 \u2013 activation itself (default)\n        * 1 \u2013 first derivative\n        * 2 \u2013 second derivative\n\n    Returns\n    -------\n    list | int\n        Python list mirroring the shape of *x* with every entry replaced by\n        the corresponding result rounded to 6 decimals, or -1 for invalid\n        *order*.\n    \"\"\"\n    # Only the first two derivatives are supported\n    if order not in (0, 1, 2):\n        return -1\n\n    # Convert the input to a NumPy array for vectorised computation\n    arr = np.asarray(x, dtype=float)\n\n    # e^x is the value for the activation and for both derivatives\n    result = np.exp(arr)\n\n    # Round each element to 6 decimal places\n    result = np.round(result, 6)\n\n    # Return the result with the same nesting structure as the input\n    return result.tolist()\n```"}
{"task_id": 498, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(x: np.ndarray | list | int | float,\n               alpha: float = 0.3,\n               order: int = 0) -> list:\n   Leaky-ReLU activation and its derivatives.\n\n    Parameters\n    ----------\n    x : np.ndarray | list | int | float\n        Input data (not modified in-place).\n    alpha : float, optional\n        Negative-slope coefficient (default 0.3).\n    order : int, optional\n        0 \u2192 value, 1 \u2192 first derivative, 2 \u2192 second derivative (default 0).\n\n    Returns\n    -------\n    list\n        Result with the same shape as *x* (a scalar is returned as a\n        one-element list).  If *order* is not 0, 1, or 2 an empty list\n        is returned.\n       if order not in (0, 1, 2):\n        return []\n\n    # Convert to NumPy array without altering the original input\n    arr = np.asarray(x)\n\n    if order == 0:            # Function value\n        res = np.where(arr > 0, arr, alpha * arr)\n    elif order == 1:          # First derivative\n        res = np.where(arr > 0, 1.0, alpha)\n    else:                     # Second derivative (identically zero)\n        res = np.zeros_like(arr, dtype=float)\n\n    # Ensure scalars are wrapped in a one-element list\n    if res.ndim == 0:\n        return [res.item()]\n    return res.tolist()\n```"}
{"task_id": 499, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array (n_samples \u00d7 n_features).\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        Projected data rounded to 4 decimal places, or \u22121 if *k* is invalid.\n    \"\"\"\n    # --------- sanity checks -------------------------------------------------\n    if data.ndim != 2:\n        raise ValueError(\"`data` must be a 2-D NumPy array.\")\n    n_samples, n_features = data.shape\n    if not (1 <= k <= n_features):\n        return -1\n\n    # --------- 1. standardise ------------------------------------------------\n    means = data.mean(axis=0)                    # per-feature mean\n    stds  = data.std(axis=0, ddof=0)             # population std (\u221avariance)\n    X = data - means                             # centre\n\n    non_zero_mask = stds > 0                    # features with variance > 0\n    X[:, non_zero_mask] /= stds[non_zero_mask]  # scale to unit variance\n    # features with zero variance remain zeros after centring (as required)\n\n    # --------- 2. covariance matrix (with Bessel\u2019s correction) --------------\n    if n_samples > 1:\n        cov = X.T @ X / (n_samples - 1)\n    else:                                       # n_samples == 1  \u2192  all zeros\n        cov = np.zeros((n_features, n_features), dtype=X.dtype)\n\n    # --------- 3. eigen-decomposition ---------------------------------------\n    eig_vals, eig_vecs = np.linalg.eigh(cov)     # symmetric matrix \u21d2 eigh\n    # --------- 4. sort eigenvalues/-vectors descending ----------------------\n    order = eig_vals.argsort()[::-1]\n    eig_vals = eig_vals[order]\n    eig_vecs = eig_vecs[:, order]\n\n    # --------- 5. fix sign of eigenvectors ----------------------------------\n    for j in range(eig_vecs.shape[1]):\n        v = eig_vecs[:, j]\n        idx_max = np.argmax(np.abs(v))\n        if v[idx_max] < 0:\n            eig_vecs[:, j] = -v\n\n    # --------- 6. project data ----------------------------------------------\n    components = eig_vecs[:, :k]           # n_features \u00d7 k\n    projected  = X @ components            # n_samples \u00d7 k\n\n    # --------- return --------------------------------------------------------\n    return np.round(projected, 4).tolist()\n```"}
{"task_id": 500, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _fit_best_stump(X: np.ndarray, r: np.ndarray):\n       Find the decision-stump h(x) that minimises the squared error\n    \u2211 (r_i \u2212 h(x_i))\u00b2.\n\n    Returns\n    -------\n    (feature_index, threshold, left_value, right_value)\n       n_samples, n_features = X.shape\n    best_err = np.inf\n    best_feat = 0\n    best_thr = 0.0\n    best_left_val = 0.0\n    best_right_val = 0.0\n\n    for j in range(n_features):\n        fvals = X[:, j]\n        order = np.argsort(fvals)\n        f_sorted = fvals[order]\n        r_sorted = r[order]\n\n        # cumulative sums for fast SSE computation\n        cumsum_r = np.cumsum(r_sorted)\n        cumsum_r2 = np.cumsum(r_sorted ** 2)\n\n        total_sum = cumsum_r[-1]\n        total_sum2 = cumsum_r2[-1]\n\n        # iterate over possible split points (between two distinct feature values)\n        for k in range(n_samples - 1):\n            if f_sorted[k] == f_sorted[k + 1]:\n                continue  # a split here would not change the groups\n\n            left_cnt = k + 1\n            right_cnt = n_samples - left_cnt\n\n            left_sum = cumsum_r[k]\n            right_sum = total_sum - left_sum\n\n            left_sum2 = cumsum_r2[k]\n            right_sum2 = total_sum2 - left_sum2\n\n            left_mean = left_sum / left_cnt\n            right_mean = right_sum / right_cnt\n\n            # SSE(left)   = \u03a3 r\u00b2 \u2212 n\u00b7mean\u00b2\n            left_sse = left_sum2 - left_sum ** 2 / left_cnt\n            right_sse = right_sum2 - right_sum ** 2 / right_cnt\n            err = left_sse + right_sse\n\n            if err < best_err:\n                best_err = err\n                best_feat = j\n                best_thr = (f_sorted[k] + f_sorted[k + 1]) / 2.0\n                best_left_val = left_mean\n                best_right_val = right_mean\n\n    # If no valid split was found (all samples have identical value),\n    # fall back to a stump that predicts a constant (=mean residual).\n    if np.isinf(best_err):\n        best_left_val = best_right_val = r.mean()\n        best_feat = 0\n        best_thr = X[:, 0].min()\n\n    return best_feat, best_thr, best_left_val, best_right_val\n\n\ndef gradient_boosting_classifier(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    n_estimators: int = 20,\n    learning_rate: float = 0.1,\n) -> list[int]:\n   Tiny gradient-boosting binary classifier using decision stumps    # --- prepare data --------------------------------------------------------\n    X_tr = np.asarray(X_train, dtype=float)\n    y_tr = np.asarray(y_train, dtype=float)\n    X_te = np.asarray(X_test, dtype=float)\n\n    n_samples = X_tr.shape[0]\n\n    # --- initial model F\u2080(x) = c --------------------------------------------\n    c0 = y_tr.mean()\n    F_tr = np.full(n_samples, c0)\n    stumps = []  # will hold (feature, threshold, left_val, right_val)\n\n    # --- boosting loop -------------------------------------------------------\n    for _ in range(n_estimators):\n        residuals = y_tr - F_tr                          # r = y \u2212 F_{m\u22121}\n        feat, thr, left_val, right_val = _fit_best_stump(X_tr, residuals)\n        stumps.append((feat, thr, left_val, right_val))\n\n        # update training predictions\n        h_pred = np.where(X_tr[:, feat] <= thr, left_val, right_val)\n        F_tr += learning_rate * h_pred                   # F_m = F_{m\u22121} + \u03b7h_m\n\n    # --- predict test set ----------------------------------------------------\n    F_te = np.full(len(X_te), c0)\n    for feat, thr, left_val, right_val in stumps:\n        h_pred = np.where(X_te[:, feat] <= thr, left_val, right_val)\n        F_te += learning_rate * h_pred\n\n    # turn regression score into class label\n    preds = (F_te >= 0.5).astype(int)\n    return preds.tolist()\n```"}
{"task_id": 501, "completion_id": 0, "completion": "```python\ndef constant_scheduler(initial_lr: float, step: int) -> float:\n    \"\"\"Constant learning-rate scheduler.\n\n    Parameters\n    ----------\n    initial_lr : float\n        The fixed learning rate that should be returned on every call.\n    step : int\n        Current training step (ignored).\n\n    Returns\n    -------\n    float\n        The same value as `initial_lr`.\n    \"\"\"\n    # Simply return the provided learning rate, regardless of `step`.\n    return float(initial_lr)\n```"}
{"task_id": 502, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hard_sigmoid(z: np.ndarray, order: int = 0) -> list[float]:\n    \"\"\"Compute Hard-Sigmoid activation or its derivatives (element-wise).\n\n    Parameters\n    ----------\n    z : np.ndarray\n        Input array.\n    order : int, optional\n        0 \u2013 activation (default)  \n        1 \u2013 first derivative  \n        2 \u2013 second derivative  \n\n    Returns\n    -------\n    list[float]\n        Result rounded to 4 decimals; empty list if *order* is invalid.\n    \"\"\"\n    # convert the input to a float ndarray to ease maths\n    z = np.asarray(z, dtype=float)\n\n    # ----- activation -------------------------------------------------------\n    if order == 0:\n        out = np.where(z < -2.5, 0.0,\n                       np.where(z > 2.5, 1.0, 0.2 * z + 0.5))\n\n    # ----- first derivative --------------------------------------------------\n    elif order == 1:\n        within = (-2.5 <= z) & (z <= 2.5)\n        out = np.where(within, 0.2, 0.0)\n\n    # ----- second derivative -------------------------------------------------\n    elif order == 2:\n        out = np.zeros_like(z, dtype=float)\n\n    # ----- unsupported order -------------------------------------------------\n    else:\n        return []\n\n    # round to 4 decimals and return as Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 503, "completion_id": 0, "completion": "```python\nfrom copy import deepcopy\n\ndef selective_update(hparams: dict | None, updates: dict | None) -> dict | int:\n    \"\"\"Return a deep-copied version of *hparams* where only keys that already\n    exist in *hparams* are updated with values from *updates*.\n\n    Parameters\n    ----------\n    hparams : dict | None\n        Original hyper-parameter dictionary.  If None, the function returns -1.\n    updates : dict | None\n        Dictionary containing candidate updates.  Keys absent from *hparams*\n        are ignored.  If None, no updating is attempted.\n\n    Returns\n    -------\n    dict | int\n        Deep-copied (and possibly updated) dictionary, or \u22121 when *hparams*\n        is None.\n    \"\"\"\n    # 1. Guard clause for missing hyper-parameters\n    if hparams is None:\n        return -1\n\n    # 2. Work on a deep copy to preserve functional behaviour\n    new_hparams = deepcopy(hparams)\n\n    # 3. Nothing to update?  Return the untouched copy.\n    if updates is None:\n        return new_hparams\n\n    # 4. Apply updates selectively\u2014only existing keys are overwritten\n    for key, value in updates.items():\n        if key in new_hparams:\n            new_hparams[key] = value\n\n    # 5. Return the updated copy\n    return new_hparams\n```\n\nUsage example\n--------------\n```python\nhparams  = {'lr': 0.1, 'momentum': 0.9}\nupdates  = {'lr': 0.01, 'weight_decay': 0.0001}\n\nprint(selective_update(hparams, updates))\n# Output: {'lr': 0.01, 'momentum': 0.9}\n\nprint(hparams)  # original remains unchanged\n# Output: {'lr': 0.1, 'momentum': 0.9}\n```"}
{"task_id": 505, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n   Perform one CD-k weight update for a binary RBM    \n    # ---------- helpers ----------\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n    \n    # ---------- early-exit for empty mini-batch ----------\n    m = X.shape[0]\n    if m == 0:\n        return []\n    \n    # ---------- positive phase ----------\n    h_prob = sigmoid(X @ W + hbias)          # h0 probabilities\n    pos_grad = X.T @ h_prob                  # (n_v, n_h)\n    \n    # ---------- Gibbs chain ----------\n    v_prob = X.copy()                        # will be overwritten below\n    for _ in range(k):\n        # visible \u2190 hidden\n        v_prob = sigmoid(h_prob @ W.T + vbias)\n        # hidden \u2190 visible\n        h_prob = sigmoid(v_prob @ W + hbias)\n    \n    # after k full steps\n    v_k_prob, h_k_prob = v_prob, h_prob\n    neg_grad = v_k_prob.T @ h_k_prob         # (n_v, n_h)\n    \n    # ---------- weight update ----------\n    W_new = W + learning_rate * (pos_grad - neg_grad) / m\n    \n    # ---------- rounding & conversion ----------\n    return np.round(W_new, 4).tolist()\n```"}
{"task_id": 508, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict, List\n\ndef params_with_name(params: Dict[str, Any], name: str) -> List[Any]:\n    \"\"\"Return all values whose keys contain a given substring.\n\n    The function preserves the insertion order of the input dictionary.\n\n    Args:\n        params: Dictionary mapping parameter names (strings) to parameter values.\n        name:   Substring to search for inside the dictionary keys (case-sensitive).\n\n    Returns:\n        A list of values whose corresponding keys contain *name*,\n        in the same order those keys appear in *params*.\n    \"\"\"\n    # Using a simple list comprehension over the dictionary\u2019s\n    # key-iteration order, which is guaranteed to be the insertion order\n    # as of Python 3.7+ (3.6 for CPython).\n    return [value for key, value in params.items() if name in key]\n```\n\nExample usage\n```python\nd = {\"weight1\": 0.1, \"bias1\": 0.0, \"weight2\": 0.3, \"Bias2\": 0.2}\nprint(params_with_name(d, \"weight\"))  # -> [0.1, 0.3]\nprint(params_with_name(d, \"Bias\"))    # -> [0.2]  (note the case sensitivity)\nprint(params_with_name(d, \"gamma\"))   # -> []\n```"}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\",\n              y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    Parameters\n    ----------\n    x, y : list[float] | np.ndarray\n        One-dimensional input vectors.  They must have identical length.\n\n    Returns\n    -------\n    float\n        Chebyshev distance rounded to four decimal places, or -1.0 if the\n        vectors differ in length.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays of float type\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n\n    # Length check\n    if x_arr.size != y_arr.size:\n        return -1.0\n\n    # Compute the maximum absolute component-wise difference\n    max_diff = np.max(np.abs(x_arr - y_arr)) if x_arr.size else 0.0\n\n    # Return as a built-in float rounded to 4 decimal places\n    return round(float(max_diff), 4)\n```"}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data, shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities, same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x), shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Mini-batch average of (binary cross-entropy + KL divergence)\n               rounded to 6 decimal places.\n    \"\"\"\n    # ---------- Reconstruction (binary cross-entropy) ----------\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n\n    # Element-wise BCE, then sum per sample\n    recon_per_sample = -(\n        y * np.log(y_pred_clipped) +\n        (1.0 - y) * np.log(1.0 - y_pred_clipped)\n    ).sum(axis=1)   # shape: (batch_size,)\n\n    # ---------- KL divergence ----------\n    # exp(log_var) == sigma^2\n    kl_per_sample = -0.5 * (\n        1 + t_log_var - np.square(t_mean) - np.exp(t_log_var)\n    ).sum(axis=1)   # shape: (batch_size,)\n\n    # ---------- Combine and average ----------\n    total_per_sample = recon_per_sample + kl_per_sample\n    avg_loss = total_per_sample.mean()\n\n    return round(float(avg_loss), 6)\n```"}
{"task_id": 511, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef random_bernoulli_mab(n_arms: int = 10) -> list[float]:\n    \"\"\"\n    Build a random Bernoulli multi-armed bandit environment.\n\n    Parameters\n    ----------\n    n_arms : int, default=10\n        Number of arms in the MAB.\n\n    Returns\n    -------\n    list[float]\n        List containing `n_arms` success probabilities rounded to 4 decimals.\n        If `n_arms` < 1 an empty list is returned.\n    \"\"\"\n    # Guard-case: no (or negative) arms requested\n    if n_arms < 1:\n        return []\n\n    rng = np.random.default_rng()                  # modern, thread-safe random generator\n    raw = rng.uniform(0.0, 1.0, n_arms)            # step 1: draw samples\n    probs = raw / raw.sum()                        # step 2: normalise to sum to 1\n\n    probs = np.round(probs, 4)                     # step 3: round to 4 dp\n\n    # Optional: adjust tiny rounding drift so the list re-sums to exactly 1.0\n    drift = 1.0 - probs.sum()\n    # The drift is at most \u00b1(0.5 \u00b7 10\u207b\u2074) \u00b7 n_arms, i.e. tiny, but we fix it anyway.\n    idx = int(np.argmax(probs))                    # pick the largest entry\n    probs[idx] = np.round(probs[idx] + drift, 4)   # keep result within [0,1]\n\n    return probs.tolist()\n```"}
{"task_id": 512, "completion_id": 0, "completion": "```python\ndef pooling_shape(\n    pool_shape: tuple[int, int],\n    image_shape: tuple[int, int, int, int],\n    stride: tuple[int, int],\n) -> tuple[int, int] | int:\n    \"\"\"Return the spatial output size (out_h, out_w) of a 2-D pooling layer.\n\n    The layer is *valid* only when                \u250c (h \u2212 k_h) is divisible by s_h\n        k_h \u2264 h, k_w \u2264 w        and also          \u2502 (w \u2212 k_w) is divisible by s_w\n                                                  \u2514 strides are non-zero\n    Otherwise the function returns -1.\n    \"\"\"\n    # unpack arguments --------------------------------------------------------\n    try:\n        k_h, k_w = pool_shape\n        _, _, h, w = image_shape\n        s_h, s_w = stride\n    except Exception:                       # wrong tuple lengths / types\n        return -1\n\n    # basic sanity checks ------------------------------------------------------\n    if min(k_h, k_w, h, w, s_h, s_w) <= 0:       # no non-positive values allowed\n        return -1\n    if k_h > h or k_w > w:                       # kernel must fit inside input\n        return -1\n\n    # compute differences ------------------------------------------------------\n    dh, dw = h - k_h, w - k_w\n\n    # divisibility check (no padding, exact fit) ------------------------------\n    if dh % s_h or dw % s_w:                     # not integer divisible\n        return -1\n\n    # final output size --------------------------------------------------------\n    out_h = dh // s_h + 1\n    out_w = dw // s_w + 1\n    return (out_h, out_w)\n```"}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix (n_samples, n_features).\n        w0: Global bias term.\n        w: Linear coefficients (length n_features).\n        V: Latent factor matrix (n_features, k).\n\n    Returns:\n        List with one prediction per sample, rounded to 4 decimals.\n    \"\"\"\n    # Convert everything to numpy arrays of type float\n    X = np.asarray(X, dtype=float)          # (n_samples, n_features)\n    w = np.asarray(w, dtype=float)          # (n_features,)\n    V = np.asarray(V, dtype=float)          # (n_features, k)\n\n    # ---- linear part ----\n    linear_part = X @ w                     # (n_samples,)\n\n    # ---- interaction part ----\n    if V.size == 0:                         # handle k = 0 (no latent factors)\n        interaction_part = 0.0\n    else:\n        XV        = X @ V                   # (n_samples, k)\n        term1     = XV ** 2                 # (n_samples, k)\n        term2     = (X ** 2) @ (V ** 2)     # (n_samples, k)\n        interaction_part = 0.5 * np.sum(term1 - term2, axis=1)  # (n_samples,)\n\n    # ---- total prediction ----\n    y_hat = w0 + linear_part + interaction_part\n\n    # Round to 4 decimals and convert to Python list\n    return np.round(y_hat, 4).tolist()\n```"}
{"task_id": 514, "completion_id": 0, "completion": "```python\ndef switch_phase(current_phase: str,\n                 set_train: bool | None = None,\n                 set_test: bool | None = None) -> str:\n    \"\"\"Update the training/testing phase.\"\"\"\n    # --------------------  validation  --------------------\n    # valid phase string\n    if current_phase not in (\"training\", \"testing\"):\n        return -1\n\n    # only one switch may be given\n    if set_train is not None and set_test is not None:\n        return -1\n\n    # switch values, if present, must be real booleans\n    if set_train is not None and not isinstance(set_train, bool):\n        return -1\n    if set_test is not None and not isinstance(set_test, bool):\n        return -1\n\n    # --------------------  phase logic  -------------------\n    # rule 3\n    if set_train is not None:\n        return \"training\" if set_train else \"testing\"\n\n    if set_test is not None:\n        return \"testing\" if set_test else \"training\"\n\n    # rule 4 \u2013 toggle\n    return \"testing\" if current_phase == \"training\" else \"training\"\n```"}
{"task_id": 515, "completion_id": 0, "completion": "```python\nfrom typing import Any, Dict\n\n\ndef flatten_parameters(params: Dict[str, Any]) -> Dict[str, Any]:\n   Return a flat copy of a (possibly) nested parameter dictionary.\n\n    Keys occurring on the path to a value are joined by a dot ('.').\n    The input dictionary is *not* modified.\n\n    Examples\n    --------\n    >>> flatten_parametersa 1,bc 2,de 3}}})\n    {'a': 1, 'b.c': 2, 'b.d.e': 3}\n       def _flatten(current: Dict[str, Any], prefix: str = -> Dict[str, Any]:\n        flat: Dict[str, Any] = {}\n        for key, value in current.items():\n            full_key = fprefix}.{key if prefix else key\n            if isinstance(value, dict):\n                # Recurse and merge the returned flat dictionary.\n                flat.update(_flatten(value, full_key))\n            else:\n                flat[full_key] = value\n        return flat\n\n    # Edge-case: empty input \u2192 empty output ( also handled by the helper ).\n    return _flatten(params)\n```"}
{"task_id": 516, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef euclidean_distance(a, b):\n    \"\"\"Compute the Euclidean distance between two numeric vectors.\n\n    The vectors may be provided as Python lists or NumPy arrays and may contain\n    integers or floats.  If the vectors have different numbers of total\n    elements, the function must return -1.\n\n    Args:\n        a (list[int | float] | np.ndarray): First vector.\n        b (list[int | float] | np.ndarray): Second vector.\n\n    Returns:\n        float: Euclidean distance rounded to 4 decimal places, or -1 if the\n               vectors are of unequal length.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays of floats\n    vec_a = np.asarray(a, dtype=float).ravel()\n    vec_b = np.asarray(b, dtype=float).ravel()\n\n    # Check for equal length\n    if vec_a.size != vec_b.size:\n        return -1\n\n    # Compute Euclidean (\u21132) distance\n    distance = np.linalg.norm(vec_a - vec_b)\n\n    # Round to 4 decimal places and return as Python float\n    return round(float(distance), 4)\n```\n\nThe function first flattens the inputs into one-dimensional arrays, validates that they contain the same number of elements, and then computes and returns the Euclidean distance rounded to four decimal places. If the vectors differ in length, it returns `-1` as required."}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute a numerically-stable, element-wise sigmoid activation.\n\n    Parameters\n    ----------\n    z : int, float, list, tuple or numpy.ndarray\n        Input data.\n\n    Returns\n    -------\n    float or list\n        If `z` is a scalar a float is returned, otherwise a (nested) Python\n        list whose items are rounded to four decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for vectorised computation\n    z_arr = np.asarray(z, dtype=np.float64)\n\n    # Allocate an output array of the same shape\n    out = np.empty_like(z_arr)\n\n    # Split the computation to avoid overflow/underflow\n    pos_mask = z_arr >= 0\n    neg_mask = ~pos_mask\n\n    # For non-negative values: \u03c3(x) = 1 / (1 + exp(-x))\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-z_arr[pos_mask]))\n\n    # For negative values:            exp(x) / (1 + exp(x))\n    # This form is numerically stable when x is very negative.\n    exp_x = np.exp(z_arr[neg_mask])\n    out[neg_mask] = exp_x / (1.0 + exp_x)\n\n    # Round to four decimal places\n    out = np.round(out, 4)\n\n    # Return according to the original input type\n    if np.isscalar(z):\n        return float(out)          # scalar \u2192 float\n    else:\n        return out.tolist()        # array-like \u2192 (nested) list\n```\n\nExample usage\n```\n>>> sigmoid([-1, 0, 1])\n[0.2689, 0.5, 0.7311]\n\n>>> sigmoid(1000)          # large positive\n1.0\n\n>>> sigmoid(-1000)         # large negative\n0.0\n```"}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\nfrom functools import wraps\n\ndef unhot(function: Callable) -> Callable:\n   Decorator that converts one-hot encoded label arrays to 1-D integer labels\n    before forwarding them to the wrapped metric function.\n\n    The wrapped metric must take exactly two positional arguments\n    (actual, predicted).  If either argument is a 2-D NumPy array whose second\n    dimension is larger than one, the array is interpreted as one-hot and\n    converted via argmax along axis 1.\n       @wraps(function)\n    def wrapper(actual: np.ndarray, predicted: np.ndarray, *args, **kwargs):\n        def _maybe_unhot(arr: np.ndarray) -> np.ndarray:\n            if isinstance(arr, np.ndarray) and arr.ndim == 2 and arr.shape[1] > 1:\n                return arr.argmax(axis=1)\n            return arr\n\n        actual_conv    = _maybe_unhot(actual)\n        predicted_conv = _maybe_unhot(predicted)\n\n        return function(actual_conv, predicted_conv, *args, **kwargs)\n\n    return wrapper\n```"}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef sigmoid_grad(z):\n   Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n       # ---------- scalar input -----------------------------------------------\n    if not isinstance(z, (list, np.ndarray)):\n        return float(np.round(float(z) * (1.0 - float(z)), 4))\n\n    # ---------- numpy array input ------------------------------------------\n    if isinstance(z, np.ndarray):\n        return np.round(z * (1.0 - z), 4).tolist()\n\n    # ---------- list / nested list input -----------------------------------\n    if len(z) == 0:                   # empty list stays empty\n        return []\n\n    def _rec(list_obj):\n       Recursively process nested lists        if isinstance(list_obj, list):\n            return [_rec(item) for item in list_obj]\n        else:  # scalar inside the list\n            val = float(list_obj)\n            return float(np.round(val * (1.0 - val), 4))\n\n    return _rec(z)\n```"}
{"task_id": 521, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef relu_grad(z):\n   Element-wise gradient of the ReLU activation.\n\n    Parameters\n    ----------\n    z : int, float, list, or np.ndarray\n        Input value(s).\n\n    Returns\n    -------\n    int or list\n        0/1 values with the same shape as `z` (scalar \u21d2 single int).\n       arr = np.asarray(z)                 # uniform NumPy view\n    if arr.ndim == 0:                   # scalar or 0-d array\n        return int(arr > 0)             # single 0 or 1\n    grad = (arr > 0).astype(int)        # element-wise comparison\n    return grad.tolist()                # Python list for non-scalars\n```"}
{"task_id": 522, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np   # (kept to match the given scaffold)\n\ndef standard_normal(weight_shape):\n   Generate a tensor filled with N(0, 1) samples.\n\n    The NumPy RNG is re-seeded to 42 every time for reproducibility.\n    All values are rounded to 4 decimal places and returned as a\n    (possibly nested) pure-Python list.\n\n    Parameters\n    ----------\n    weight_shape : int | tuple[int, ...] | list[int]\n        Desired tensor shape.\n\n    Returns\n    -------\n    list\n        A Python (nested) list with the sampled and rounded values.\n       # 1. Re-seed for deterministic output\n    np.random.seed(42)\n\n    # 2. Normalise the shape argument to a tuple\n    if isinstance(weight_shape, int):\n        shape = (weight_shape,)\n    elif isinstance(weight_shape, (tuple, list)):\n        shape = tuple(weight_shape)\n    else:\n        raise TypeError(\n           weight_shape` must be an int, tuple or list of positive integers        )\n\n    # 3. Draw samples from N(0, 1)\n    tensor = np.random.standard_normal(size=shape)\n\n    # 4. Round to 4 decimal places\n    tensor = np.round(tensor, 4)\n\n    # 5. Convert to regular Python lists\n    return tensor.tolist()\n```"}
{"task_id": 523, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef he_normal(weight_shape: tuple[int, ...]) -> list:\n    \"\"\"Initialise a weight tensor using the He normal distribution.\n\n    Args:\n        weight_shape: Shape of the desired weight tensor. For dense layers this\n            is usually (fan_in, fan_out). For 2-D convolutions it is\n            (kernel_h, kernel_w, in_channels, out_channels).\n\n    Returns:\n        Nested Python lists containing the initialised weights, rounded to four\n        decimal places.\n    \"\"\"\n    # 1. work out the fan-in\n    if len(weight_shape) == 4:                                   # convolution kernel\n        kernel_h, kernel_w, in_ch, _ = weight_shape\n        fan_in = kernel_h * kernel_w * in_ch\n    else:                                                        # treat as (fan_in, \u2026)\n        fan_in = weight_shape[0]\n\n    # 2. standard deviation of the He normal distribution\n    std = (2.0 / fan_in) ** 0.5\n\n    # 3. draw samples and round to 4 decimal places\n    weights = np.random.normal(loc=0.0, scale=std, size=weight_shape)\n    weights = np.round(weights, 4)\n\n    # 4. convert to (nested) Python list and return\n    return weights.tolist()\n```"}
{"task_id": 525, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(\n        X: np.ndarray,\n        y: np.ndarray,\n        lambda_: float = 0.0) -> tuple[list[float], float]:\n    \"\"\"Fit a Ridge (L2-regularised) linear regression model (closed form).\"\"\"\n    \n    # --- Sanity checks -------------------------------------------------------\n    if lambda_ < 0:\n        raise ValueError('lambda_ must be non-negative.')\n    if X.ndim != 2:\n        raise ValueError('X must be a 2-D array.')\n    \n    # --- Prepare the data ----------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).ravel()          # ensure shape (m,)\n    \n    m, n = X.shape\n    \n    # Column-wise means\n    x_mean = X.mean(axis=0)                         # shape (n,)\n    y_mean = y.mean()                               # scalar\n    \n    # Mean-centred data\n    Xc = X - x_mean\n    yc = y - y_mean\n    \n    # --- Closed-form solution -------------------------------------------------\n    # (X\u1d40X + \u03bbI)\u207b\u00b9 X\u1d40 y\n    A = Xc.T @ Xc + lambda_ * np.eye(n)\n    \n    # Use pseudo-inverse to be safe with singular matrices\n    w = np.linalg.pinv(A) @ (Xc.T @ yc)             # shape (n,)\n    \n    # Intercept that is not penalised\n    b = y_mean - x_mean @ w\n    \n    # --- Rounding and return --------------------------------------------------\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(float(b), 4)\n    \n    return w_rounded, b_rounded\n```"}
{"task_id": 527, "completion_id": 0, "completion": "```python\nimport os\nfrom typing import List, Union\nimport os   # allowed (duplicated on purpose, see task text)\n\ndef extract_archive(\n    file_path: str,\n    path: str = '.',                          # kept for API compatibility \u2013 ignored here\n    archive_format: Union[str, List[str], None] = 'auto'\n) -> bool:\n   Return True if *file_path* looks like an extractable archive.\n\n    The real extraction is NOT performed \u2013 only the file name is inspected.\n\n    See the task description for the complete specification.\n       # ------------------------------------------------------------------ #\n    # 1. Define the supported formats and their recognised extensions\n    # ------------------------------------------------------------------ #\n    _SUPPORTED_EXTS = {\n        'tar': ['.tar', '.tar.gz', '.tgz', '.tar.bz', '.tar.bz2', '.tbz'],\n        'zip': ['.zip'],\n    }\n\n    # ------------------------------------------------------------------ #\n    # 2. Normalise the *archive_format* argument into a list of formats\n    # ------------------------------------------------------------------ #\n    try:\n        if archive_format is None:\n            return False\n\n        # 'auto'  ->  both tar and zip\n        if archive_format == 'auto':\n            allowed_formats: List[str] = ['tar', 'zip']\n\n        # Single string such as 'tar', 'zip' (case-insensitive)\n        elif isinstance(archive_format, str):\n            allowed_formats = [archive_format]\n\n        # List / tuple of strings\n        elif isinstance(archive_format, (list, tuple)):\n            # Convert each item to string & lower\n            allowed_formats = list(archive_format)\n\n        # Any other type \u2013 treat as \u201cnothing allowed\u201d\n        else:\n            return False\n\n        # Convert to lower-case and keep only those we actually support\n        allowed_formats = [str(fmt).lower() for fmt in allowed_formats]\n        allowed_formats = [fmt for fmt in allowed_formats if fmt in _SUPPORTED_EXTS]\n\n        # Empty list after filtering  ->  no format allowed\n        if not allowed_formats:\n            return False\n\n    except Exception:      # Defensive \u2013 should never happen, but forbidden to raise\n        return False\n\n    # ------------------------------------------------------------------ #\n    # 3. Create the list of extensions we should accept\n    # ------------------------------------------------------------------ #\n    allowed_extensions: List[str] = []\n    for fmt in allowed_formats:\n        allowed_extensions.extend(_SUPPORTED_EXTS[fmt])\n\n    # ------------------------------------------------------------------ #\n    # 4. Check the file name (case-insensitive)\n    # ------------------------------------------------------------------ #\n    try:\n        filename = os.path.basename(file_path).lower()\n        for ext in allowed_extensions:\n            if filename.endswith(ext):\n                return True\n        return False\n    except Exception:\n        # If anything unforeseen happens (e.g., file_path is not a str)\n        return False\n```"}
{"task_id": 528, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    Each grid point (x\u2081 , x\u2082) is labelled\n        1  if  w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0\n       -1  otherwise.\n\n    The grid covers the axis-aligned bounding box of *X* with *grid_n*\n    equally spaced points per axis.\n\n    Returns\n    -------\n    list[list[int]]\n        A nested list of shape (grid_n \u00d7 grid_n) containing only 1 and \u22121.\n    \"\"\"\n    # --- Basic validation --------------------------------------------------\n    if grid_n < 1:\n        raise ValueError(\"grid_n must be a positive integer.\")\n    if len(W) != 2:\n        raise ValueError(\"W must contain exactly two elements.\")\n\n    # --- Prepare data ------------------------------------------------------\n    X_np = np.asarray(X, dtype=float)\n    if X_np.ndim != 2 or X_np.shape[1] != 2:\n        raise ValueError(\"X must be of shape (n_samples, 2).\")\n\n    w1, w2 = float(W[0]), float(W[1])\n\n    # Axis limits\n    x1_min, x2_min = X_np.min(axis=0)\n    x1_max, x2_max = X_np.max(axis=0)\n\n    # 1-D axes\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n\n    # Mesh-grid\n    X1_grid, X2_grid = np.meshgrid(x1_plot, x2_plot)  # both (grid_n, grid_n)\n\n    # Linear classifier scores\n    scores = w1 * X1_grid + w2 * X2_grid + b\n\n    # Apply sign rule: treat 0 as positive (label 1)\n    labels = np.where(scores >= 0, 1, -1)\n\n    # Convert to ordinary Python list-of-lists\n    return labels.tolist()\n```"}
{"task_id": 529, "completion_id": 0, "completion": "```python\nimport math\ndef generate_progress_bar(current: int, target: int | None, width: int = 30) -> str:\n    \"\"\"Return an ASCII progress bar string similar to Keras' *Progbar*.\"\"\"\n    # ---------- case 1 : total length unknown --------------------------------\n    if target is None:\n        # 7-character, right-aligned decimal number + \u201c/Unknown\u201d\n        return f\"{current:>7}/Unknown\"\n\n    # ---------- case 2 : total length known ----------------------------------\n    # number of digits needed for printing \u201ctarget\u201d\n    d = len(str(target))\n\n    # prefix \u201ccurrent/target [\u201d\n    out = f\"{current:>{d}}/{target} [\"\n\n    # how many bar\u2010characters are already filled?\n    prog_width = int(width * current / target) if target else 0\n    prog_width = min(prog_width, width)           # do not exceed bar width\n\n    bar = \"\"\n\n    if prog_width > 0:\n        # prog_width \u2013 1 full blocks\n        bar += \"=\" * (prog_width - 1)\n\n        # arrow or last \u201c=\u201d depending on completion\n        bar += \">\" if current < target else \"=\"\n    else:\n        # nothing filled yet \u2192 only arrow (when not finished)\n        if current < target:\n            bar += \">\"\n    # pad the remainder of the bar with dots\n    bar += \".\" * (width - len(bar))\n\n    # close bracket and return\n    out += bar + \"]\"\n    return out\n```"}
{"task_id": 530, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n   Calculates the mean Binary Cross-Entropy loss.\n\n    Parameters\n    ----------\n    y_true : np.ndarray\n        Ground-truth binary labels (0 or 1).\n    y_pred : np.ndarray\n        Predicted probabilities for the positive class. Must have the same shape\n        as *y_true*.\n\n    Returns\n    -------\n    float\n        Mean BCE loss rounded to 4 decimal places.\n       # Clip predictions to avoid log(0)\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    \n    # Compute per-sample BCE\n    loss = -(y_true * np.log(y_pred_clipped) +\n             (1 - y_true) * np.log(1 - y_pred_clipped))\n    \n    # Mean loss rounded to 4 decimals\n    return np.round(loss.mean(), 4)\n```"}
{"task_id": 531, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef generalized_cosine(window_len: int, coefs: list[float], symmetric: bool = False) -> list[float]:\n   Generate a generalized cosine window rounded to four decimals.\n\n    Args:\n        window_len (int): Desired length L of the final window (must be > 0).\n        coefs (list[float]): List of coefficients a_k (k = 0 \u2026 K).\n        symmetric (bool, optional): If True, build the symmetric window;\n            otherwise build the periodic version.  Defaults to False.\n\n    Returns:\n        list[float]: The requested window, values rounded to 4 decimals.\n       # ----------------- argument checking ------------------------------------\n    if window_len <= 0:\n        raise ValueErrorwindow_len must be a positive integer    if len(coefs) == 0:\n        raise ValueErrorcoefs list must not be empty    L = int(window_len)\n\n    # ----------------- phase grid \u03b8_n ---------------------------------------\n    if symmetric:\n        if L == 1:                                 # avoid division by zero\n            theta = np.array([0.0])\n        else:\n            theta = -np.pi + 2.0 * np.pi * np.arange(L) / (L - 1)\n    else:  # periodic\n        theta = -np.pi + 2.0 * np.pi * np.arange(L) / L\n\n    # ----------------- build the window -------------------------------------\n    w = np.zeros_like(theta)\n    for k, a_k in enumerate(coefs):\n        w += a_k * np.cos(k * theta)\n\n    # ----------------- rounding & return ------------------------------------\n    return np.round(w, 4).tolist()\n```"}
{"task_id": 532, "completion_id": 0, "completion": "```python\ndef count_fc_parameters(layers: list[int]) -> int:\n    \"\"\"Calculate total trainable parameters (weights + biases) in a fully-connected network.\n\n    Args:\n        layers: List with the neuron count of each layer in order\n                (input layer first, output layer last).\n\n    Returns:\n        Total number of trainable parameters (non-negative int). If the network\n        has fewer than two layers, returns 0.\n    \"\"\"\n    if len(layers) < 2:\n        return 0\n\n    total = 0\n    for n_in, n_out in zip(layers[:-1], layers[1:]):\n        total += n_in * n_out        # weights\n        total += n_out               # biases\n\n    return total\n```"}
{"task_id": 533, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gaussian_likelihood(x, mean, sigma):\n    \"\"\"Compute the probability density of a normal distribution.\n\n    Parameters\n    ----------\n    x : float | list[float] | np.ndarray\n        Point(s) where the density will be evaluated.\n    mean : float\n        The mean (\u00b5) of the distribution.\n    sigma : float\n        The standard deviation (\u03c3) of the distribution.\n\n    Returns\n    -------\n    float | list[float]\n        Density value(s) rounded to 4 decimal places. A single float is\n        returned for a scalar x, otherwise a list is returned.\n    \"\"\"\n    # Check whether x is a scalar (int/float) or an array-like container\n    is_scalar = np.isscalar(x)\n\n    # Convert input to NumPy array for vectorised computation\n    x_arr = np.asarray(x, dtype=float)\n\n    # Gaussian PDF formula\n    coef = 1.0 / (sigma * np.sqrt(2 * np.pi))\n    exponent = np.exp(-((x_arr - mean) ** 2) / (2 * sigma ** 2))\n    pdf_vals = coef * exponent\n\n    # Round to 4 decimal places\n    rounded = np.round(pdf_vals, 4)\n\n    # Return according to the original input type\n    if is_scalar:\n        # rounded is a 0-d array when x is scalar; convert to Python float\n        return float(rounded)\n    else:\n        return rounded.tolist()\n```"}
{"task_id": 534, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_(Z):\n    \"\"\"Compute the logistic sigmoid activation element-wise.\n\n    Args\n    ----\n    Z : int, float, list, or np.ndarray\n         A scalar, 1-D, or 2-D collection of real numbers.\n\n    Returns\n    -------\n    float or list\n        The sigmoid-transformed value(s) rounded to 4 decimals.\n        \u2022 Scalar input  \u2192  Python float  \n        \u2022 Non-scalar    \u2192  Python list with the same nested shape\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Helper: decide whether the argument should be treated as a scalar\n    # ------------------------------------------------------------------\n    is_scalar = np.isscalar(Z) or (isinstance(Z, np.ndarray) and Z.ndim == 0)\n\n    if is_scalar:                                   # ----- scalar path\n        z_val = float(Z)                            # cast once to float\n        sig   = 1.0 / (1.0 + np.exp(-z_val))        # logistic function\n        return round(sig, 4)\n\n    # --------------------------------------------------------------\n    # Non-scalar: convert to ndarray, vectorise, round, back to list\n    # --------------------------------------------------------------\n    arr   = np.asarray(Z, dtype=float)              # keeps original shape\n    sig   = 1.0 / (1.0 + np.exp(-arr))              # element-wise sigmoid\n    sig   = np.round(sig, 4)                        # round to 4 decimals\n    return sig.tolist()\n```"}
{"task_id": 536, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression(X_train, y_train, X_test, methodnormal alpha=0.01, num_iter=1000):\n   Linear regression with two training modes (Normal Equation or BGD).\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Feature matrix for training data of shape (m, n).\n    y_train : np.ndarray\n        Target vector for training data of length m.\n    X_test : np.ndarray\n        Feature matrix for which predictions are required, shape (k, n).\n    method : str, optional\n       normal (default) for the closed-form solution; any other string\n        triggers batch gradient descent.\n    alpha : float, optional\n        Learning rate for gradient descent (ignored for the normal equation).\n    num_iter : int, optional\n        Number of gradient descent iterations (ignored for the normal equation).\n\n    Returns\n    -------\n    list[float]\n        Predictions for each row in X_test, rounded to 4 decimal places.\n       # Convert inputs to numpy arrays (ensure float type for numerical stability)\n    X_train = np.asarray(X_train, dtype=float)\n    X_test  = np.asarray(X_test,  dtype=float)\n    y_train = np.asarray(y_train, dtype=float).reshape(-1, 1)  # column-vector\n\n    m, n = X_train.shape  # m samples, n features\n\n    # 1) Add bias (intercept) column of ones\n    X_b      = np.hstack((np.ones((m, 1)), X_train))\n    X_test_b = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # 2) Fit parameters \u03b8\n    if method ==normal        # \u03b8 = (X\u1d40X)\u207b\u00b9 X\u1d40 y   (closed-form)\n        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_train\n    else:\n        # Batch Gradient Descent\n        theta = np.zeros((n + 1, 1))\n        for _ in range(num_iter):\n            gradient = (X_b.T @ (X_b @ theta - y_train)) * (alpha / m)\n            theta    = theta - gradient\n\n    # 3) Predict on the test set\n    preds = X_test_b @ theta          # shape (k, 1)\n    preds = preds.flatten()           # convert to 1-D array\n\n    # 4) Round to 4 decimals and return as Python list\n    return np.round(preds, 4).tolist()\n```"}
{"task_id": 537, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray,\n                         X_test:  np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier (written from scratch).\"\"\"\n    \n    eps = 1e-9                         # small value to avoid zero\u2010division\n    classes = np.array([0, 1])         # we only have two classes by task\n   \n    # ---- 1.  Parameter estimation  ----------------------------------------\n    # Compute mean and (adjusted) standard deviation for every feature\n    # separately for each class.\n    means = {}\n    stds  = {}\n    priors = {}\n    \n    for c in classes:\n        X_c = X_train[y_train == c]                # samples that belong to class c\n        priors[c] = len(X_c) / len(X_train)        # P(y = c)\n        means[c]  = X_c.mean(axis=0)               # \u03bc_c (vector of shape n_features)\n        stds[c]   = X_c.std(axis=0) + eps          # \u03c3_c  (avoid zero)\n    \n    # ---- 2.  Prediction ----------------------------------------------------\n    log_priors = {c: np.log(priors[c] + eps) for c in classes}  # log P(c)\n    \n    # pre-compute the log-coefficients of the Gaussian for each class/feature\n    log_coeff = {}\n    for c in classes:\n        log_coeff[c] = -0.5 * np.log(2.0 * np.pi * stds[c]**2)\n    \n    preds = []\n    for x in X_test:                                # iterate over test samples\n        log_posterior = {}\n        for c in classes:\n            # Gaussian log-likelihood, summed over features\n            diff   = x - means[c]\n            ll     = log_coeff[c] - (diff**2) / (2.0 * stds[c]**2)\n            log_likelihood = ll.sum()              # independence assumption\n            log_posterior[c] = log_priors[c] + log_likelihood\n        \n        # choose class with higher (log) posterior probability\n        preds.append(int(max(log_posterior, key=log_posterior.get)))\n    \n    return preds\n```"}
{"task_id": 539, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Compute the binary cross-entropy (log-loss) cost.\n\n    Parameters\n    ----------\n    AL : np.ndarray\n        Predicted probabilities, shape (m,) or (1, m).\n    Y  : np.ndarray\n        Ground-truth labels (0 or 1), same shape as AL.\n\n    Returns\n    -------\n    float\n        Scalar cost value.\n    \"\"\"\n    # Ensure the two inputs have identical number of elements\n    if AL.size != Y.size:\n        raise ValueError(\"AL and Y must contain the same number of elements.\")\n\n    # Flatten to 1-D for convenience (keeps memory-view, no copy)\n    AL = AL.ravel()\n    Y  = Y.ravel()\n\n    # Numerical stability: clip predictions away from 0 and 1\n    eps = 1e-15\n    AL_clipped = np.clip(AL, eps, 1.0 - eps)\n\n    # Binary cross-entropy\n    m = Y.size\n    cost = -(1.0 / m) * np.sum(Y * np.log(AL_clipped) +\n                               (1 - Y) * np.log(1.0 - AL_clipped))\n\n    # Return as a plain Python float\n    return float(cost)\n```"}
{"task_id": 540, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef derivative_sigmoid(Z: np.ndarray) -> list:\n   Return the element-wise derivative of the sigmoid function.\n\n    Args:\n        Z (np.ndarray): input array of any shape.\n\n    Returns:\n        list: same shape as Z, each entry is \u03c3'(z) rounded to 4 decimals.\n       # ----- numerically-stable sigmoid ---------------------------------\n    with np.errstate(over='ignore'):  # silence overflow warnings\n        pos_mask = Z >= 0\n        neg_mask = ~pos_mask\n\n        sig = np.empty_like(Z, dtype=np.float64)\n\n        # For non-negative z: \u03c3(z) = 1 / (1 + e^(\u2212z))\n        sig[pos_mask] = 1.0 / (1.0 + np.exp(-Z[pos_mask]))\n\n        # For negative z: rewrite to avoid large exp(\u2212z)\n        exp_z = np.exp(Z[neg_mask])\n        sig[neg_mask] = exp_z / (1.0 + exp_z)\n\n    # ----- derivative \u03c3'(z) = \u03c3(z) * (1 \u2212 \u03c3(z)) -----------------------\n    deriv = sig * (1.0 - sig)\n\n    # ----- round to 4 decimal places and return as plain Python list ---\n    return np.round(deriv, 4).tolist()\n```"}
{"task_id": 541, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef return_input_grads(y, y_hat):\n   Compute \u2202BCE/\u2202z = y_hat \u2212 y, rounded to 4 decimals.\n    \n    Parameters\n    ----------\n    y : array-like\n        Ground-truth labels (0/1), any shape.\n    y_hat : array-like\n        Predicted probabilities (same shape as y).\n    \n    Returns\n    -------\n    list\n        Element-wise gradient rounded to 4 decimal places,\n        returned as a regular Python list (retains original shape).\n       # Convert inputs to NumPy arrays (ensure float dtype)\n    y_arr      = np.asarray(y,      dtype=float)\n    y_hat_arr  = np.asarray(y_hat,  dtype=float)\n\n    # Element\u2013wise gradient\n    grads = y_hat_arr - y_arr\n\n    # Round to 4 decimal places\n    grads = np.round(grads, 4)\n\n    # Ensure a Python list is returned (even for scalar input)\n    if grads.shape == ():          # 0-D (scalar) case\n        return [float(grads)]\n    return grads.tolist()\n```"}
{"task_id": 542, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\ndef rmsprop_update(\n    params: List[np.ndarray],\n    grads:  List[np.ndarray],\n    s:      Optional[List[np.ndarray]] = None,\n    alpha:  float = 0.01,\n    beta:   float = 0.9,\n    epsilon: float = 1e-8,\n) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n    \"\"\"Perform one RMSProp optimisation step.\n\n    Parameters\n    ----------\n    params  : list of parameter tensors to be updated\n    grads   : list of gradients (same shapes as `params`)\n    s       : list of running averages of squared gradients.  If None or empty,\n              it will be initialised with zeros of identical shapes.\n    alpha   : learning-rate\n    beta    : decay rate for the running average\n    epsilon : numerical stability term\n\n    Returns\n    -------\n    (new_params, new_s)\n        new_params : parameters after the update, **rounded to 6 decimals**\n        new_s      : updated running averages (not rounded)\n    \"\"\"\n    if len(params) != len(grads):\n        raise ValueError(\"`params` and `grads` must have the same length\")\n\n    # Initialise `s` if necessary\n    if not s:\n        s = [np.zeros_like(g, dtype=g.dtype) for g in grads]\n\n    if len(s) != len(params):\n        raise ValueError(\"`s` must be the same length as `params` and `grads`\")\n\n    new_params: List[np.ndarray] = []\n    new_s:      List[np.ndarray] = []\n\n    for p, g, s_prev in zip(params, grads, s):\n        # Update running average of squared gradients\n        s_curr = beta * s_prev + (1.0 - beta) * (g ** 2)\n\n        # Parameter update\n        p_updated = p - alpha * g / (np.sqrt(s_curr) + epsilon)\n\n        # Store results\n        new_params.append(np.round(p_updated, 6))\n        new_s.append(s_curr)\n\n    return new_params, new_s\n```"}
{"task_id": 543, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef cal_gini_index(data: list) -> float:\n    \"\"\"Calculate the Gini index of a list of class labels.\n\n    Args:\n        data: A list of hashable values representing class labels.\n\n    Returns:\n        The Gini index rounded to 4 decimal places. If the input list is empty,\n        return 0.0.\n    \"\"\"\n    # Requirement 1: empty input \u2192 0.0\n    if not data:\n        return 0.0\n\n    total = len(data)\n    counts = Counter(data)\n\n    # Compute \u2211 p_i\u00b2\n    sum_pi_sq = sum((cnt / total) ** 2 for cnt in counts.values())\n\n    gini = 1.0 - sum_pi_sq\n\n    # Requirement 2: round to 4 decimal places\n    return round(gini, 4)\n```"}
{"task_id": 544, "completion_id": 0, "completion": "```python\ndef split_tree(data: list, fea: int, value: float):\n   Split a data set into two ordered subsets according to a threshold.\n\n    Args:\n        data (list): List of samples. Each sample is a list or tuple of feature values.\n        fea (int): Index of the feature used for the split.\n        value (float): Threshold for the split.\n\n    Returns:\n        tuple[list, list]: Two lists (set1, set2) where set1 contains the samples\n        with feature value >= threshold and set2 contains the rest.\n       # Pre-allocate the two result containers\n    set1, set2 = [], []\n\n    # Iterate once over the data, preserving the original order\n    for sample in data:\n        # Guard against malformed samples that are shorter than expected\n        if fea >= len(sample):\n            raise IndexError(fFeature index {fea} is out of bounds for sample {sample        # Classic \u201cgreater-than-or-equal\u201d split\n        if sample[fea] >= value:\n            set1.append(sample)\n        else:\n            set2.append(sample)\n\n    return set1, set2\n```\n\nUsage example\n```python\nX = [\n    [5.1, 3.5, 1.4],\n    [7.0, 3.2, 4.7],\n    [6.3, 3.3, 6.0]\n]\n\nleft, right = split_tree(X, fea=0, value=6.0)\nprint(left)   # [[7.0, 3.2, 4.7], [6.3, 3.3, 6.0]]\nprint(right)  # [[5.1, 3.5, 1.4]]\n```\n\nThe original list `X` is left untouched, and the relative order of samples inside each subset is preserved."}
{"task_id": 545, "completion_id": 0, "completion": "```python\ndef tree_stats(root: \"node | None\") -> tuple[int, int]:\n    \"\"\"Return the number of leaf nodes and the maximum depth of the tree.\n\n    A **leaf** is defined as a node whose ``results`` attribute is not ``None``.\n\n    Parameters\n    ----------\n    root : node | None\n        Root of the decision tree.  If *None*, the tree is considered empty.\n\n    Returns\n    -------\n    tuple[int, int]\n        ``(leaf_count, max_depth)``\n        * *leaf_count* \u2013 total number of leaves in the tree.\n        * *max_depth* \u2013 length of the longest path from *root* to a leaf.\n    \"\"\"\n    if root is None:               # Empty tree\n        return 0, 0\n\n    def _visit(n: \"node | None\") -> tuple[int, int]:\n        \"\"\"\n        Recursively visit *n* and return (leaf_count, max_depth)\n        for the subtree rooted at *n*.\n        \"\"\"\n        if n is None:               # Reached a non-existent child\n            return 0, 0\n\n        # Leaf node\n        if n.results is not None:\n            return 1, 1\n\n        # Internal node \u2014 gather stats from children\n        left_leaves,  left_depth  = _visit(n.left)\n        right_leaves, right_depth = _visit(n.right)\n\n        leaf_count = left_leaves + right_leaves\n        max_depth  = 1 + max(left_depth, right_depth)\n        return leaf_count, max_depth\n\n    return _visit(root)\n```"}
{"task_id": 546, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Any, Dict, List\n\n\ndef _gini(labels: List[Any]) -> float:\n    \"\"\"Return the Gini impurity of a list of class labels.\"\"\"\n    total = len(labels)\n    if total == 0:\n        return 0.0\n    counts = Counter(labels)\n    return 1.0 - sum((cnt / total) ** 2 for cnt in counts.values())\n\n\ndef _majority(labels: List[Any]) -> Any:\n    \"\"\"\n    Return the majority class.\n    If several classes are tied, the one that occurs first in *labels* wins.\n    \"\"\"\n    counts = Counter(labels)\n    max_freq = max(counts.values())\n    tied = {cls for cls, c in counts.items() if c == max_freq}\n    for lbl in labels:           # preserves original order\n        if lbl in tied:\n            return lbl\n    return labels[0]             # fallback \u2013 should never be hit\n\n\ndef build_tree(data: List[List[Any]]) -> Dict[str, Any]:\n    \"\"\"Build a CART decision tree for classification using Gini impurity.\"\"\"\n    # Empty training set \u2192 empty tree\n    if not data:\n        return {}\n\n    n_features = len(data[0]) - 1\n    labels = [row[-1] for row in data]\n\n    # If no features are available or the node is pure \u2192 leaf\n    if n_features <= 0 or len(set(labels)) == 1:\n        return {\"label\": labels[0] if labels else None}\n\n    # Current node\u2019s impurity\n    current_gini = _gini(labels)\n\n    # Search the best split\n    best_gain = 0.0\n    best_feature = best_value = None\n    best_left = best_right = None\n\n    for feat_idx in range(n_features):\n        # All unique values the feature takes on this subset\n        try:\n            values = sorted({row[feat_idx] for row in data})\n        except TypeError:\n            # Mixed incomparable types: preserve first-occurrence order instead\n            seen, values = set(), []\n            for row in data:\n                v = row[feat_idx]\n                if v not in seen:\n                    seen.add(v)\n                    values.append(v)\n\n        for val in values:\n            left = [row for row in data if row[feat_idx] == val]\n            right = [row for row in data if row[feat_idx] != val]\n\n            if not left or not right:        # ignore empty splits\n                continue\n\n            # Weighted impurity after the split\n            g_left = _gini([r[-1] for r in left])\n            g_right = _gini([r[-1] for r in right])\n            weight_left = len(left) / len(data)\n            weighted_gini = weight_left * g_left + (1 - weight_left) * g_right\n            gain = current_gini - weighted_gini\n\n            # Update the best split: '>' ensures the first encountered\n            if gain > best_gain + 1e-12:     # small epsilon for float safety\n                best_gain = gain\n                best_feature, best_value = feat_idx, val\n                best_left, best_right = left, right\n\n    # No split reduced impurity \u2192 make a leaf\n    if best_gain <= 0.0 or best_feature is None:\n        return {\"label\": _majority(labels)}\n\n    # Otherwise create an internal node and recurse\n    return {\n        \"feature\": best_feature,\n        \"value\": best_value,\n        \"left\": build_tree(best_left),\n        \"right\": build_tree(best_right),\n    }\n```"}
{"task_id": 547, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef forward_step(A: list[list[float]],\n                 B: list[list[float]],\n                 S: list[float],\n                 obs: list[int],\n                 t: int) -> list[float] | int:\n    \"\"\"Compute the forward-probability vector \u03b1_t of an HMM.\n\n    Returns \u03b1_t rounded to 4 decimals, or \u22121 on any inconsistency.\n    \"\"\"\n    # --------------- basic sanity checks ---------------\n    # t must be within the observation sequence\n    if t < 0 or t >= len(obs):\n        return -1\n\n    # ---------- check shapes of A, B and S -------------\n    n = len(A)                       # number of states\n    if n == 0:\n        return -1\n\n    # A must be n\u00d7n\n    if any(len(row) != n for row in A):\n        return -1\n\n    # B: n\u00d7m   (all rows equal length)\n    if len(B) != n or len(B[0]) == 0:\n        return -1\n    m = len(B[0])\n    if any(len(row) != m for row in B):\n        return -1\n\n    # Initial state distribution\n    if len(S) != n:\n        return -1\n\n    # Every observation index must be a legal symbol\n    if any(o < 0 or o >= m for o in obs):\n        return -1\n\n    # --------------- convert to NumPy ------------------\n    A = np.asarray(A, dtype=float)       # (n, n)\n    B = np.asarray(B, dtype=float)       # (n, m)\n    alpha = np.asarray(S, dtype=float)   # (n,)\n\n    # --------------- \u03b1_0 --------------------------------\n    alpha *= B[:, obs[0]]\n\n    # --------------- \u03b1_1 \u2026 \u03b1_t ---------------------------\n    for step in range(1, t + 1):\n        alpha = alpha @ A        # shape (n,)\n        alpha *= B[:, obs[step]]\n\n    # --------------- rounding / result ------------------\n    return [round(float(x), 4) for x in alpha]\n```"}
{"task_id": 548, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_xi(A: list[list[float]],\n               B: list[list[float]],\n               pi: list[float],\n               obs: list[int],\n               t: int) -> list[list[float]]:\n    \"\"\"Return the \u03be (xi) matrix for one time step t in a discrete HMM.\n    \n    \u03be_t(i,j) = P(q_t = i , q_{t+1} = j | obs , \u03bb)\n    \n    All entries are rounded to 4 decimals.  If the requested index is\n    invalid (t < 0  or  t \u2265 len(obs) \u2212 1) the function returns \u22121.\n    \"\"\"\n    # -------------------------  sanity check  ----------------------------\n    T = len(obs)\n    if t < 0 or t >= T - 1:          # illegal time index\n        return -1\n    \n    # -------------------------  to numpy  --------------------------------\n    A  = np.asarray(A,  dtype=float)          # N \u00d7 N\n    B  = np.asarray(B,  dtype=float)          # N \u00d7 M\n    pi = np.asarray(pi, dtype=float)          # N\n    obs = np.asarray(obs, dtype=int)\n    \n    N = A.shape[0]                            # number of states\n    \n    # -------------------------  forward pass (\u03b1)  ------------------------\n    alpha = np.zeros((t + 1, N))\n    alpha[0] = pi * B[:, obs[0]]              # \u03b1_0\n    for k in range(1, t + 1):                 # up to time t\n        alpha[k] = (alpha[k-1] @ A) * B[:, obs[k]]\n    alpha_t = alpha[t]                        # \u03b1_t\n    \n    # -------------------------  backward pass (\u03b2)  -----------------------\n    beta = np.ones(N)                         # \u03b2_{T\u22121}\n    for k in range(T - 2, t, -1):             # down to t+1 (inclusive)\n        beta = A.dot(B[:, obs[k+1]] * beta)   # \u03b2_k  \u2190  A \u00b7 (B * \u03b2_{k+1})\n    beta_t1 = beta                            # \u03b2_{t+1}\n    \n    # -------------------------  \u03be_t(i,j)  --------------------------------\n    #   numerator  = \u03b1_t(i) \u00b7 A[i,j] \u00b7 B[j, obs[t+1]] \u00b7 \u03b2_{t+1}(j)\n    emis_next = B[:, obs[t+1]] * beta_t1      # length N,   j part\n    xi_num = (alpha_t[:, None] * A) * emis_next   # N \u00d7 N\n    denom = xi_num.sum()\n    xi = xi_num / denom                       # normalise\n    \n    # -------------------------  rounding / return  -----------------------\n    return np.round(xi, 4).tolist()\n```"}
{"task_id": 549, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cal_dis(x, y):\n    \"\"\"Compute the squared Euclidean distance between two vectors.\n\n    Parameters\n    ----------\n    x : list, tuple, or np.ndarray\n        First vector of numbers.\n    y : list, tuple, or np.ndarray\n        Second vector of numbers.\n\n    Returns\n    -------\n    float\n        Squared Euclidean distance rounded to 4 decimal places,\n        or -1 if the vectors are of different lengths.\n    \"\"\"\n    # Convert inputs to 1-D NumPy arrays of float type\n    x_arr = np.asarray(x, dtype=float).ravel()\n    y_arr = np.asarray(y, dtype=float).ravel()\n\n    # Length check\n    if x_arr.size != y_arr.size:\n        return -1\n\n    # Squared Euclidean distance\n    dist_sq = np.sum((x_arr - y_arr) ** 2, dtype=float)\n\n    # Round to four decimal places and return as Python float\n    return float(round(dist_sq, 4))\n```"}
{"task_id": 551, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef _gini(labels: np.ndarray) -> float:\n   Compute the Gini impurity of a 1-D array of class labels    n = len(labels)\n    if n == 0:            # by convention, an empty set is pure\n        return 0.0\n    counts = Counter(labels)\n    impurity = 1.0 - sum((cnt / n) ** 2 for cnt in counts.values())\n    return impurity\n\n\ndef best_gini_split(X: np.ndarray,\n                    y: np.ndarray,\n                    epsilon: float = 1e-3) -> tuple[int, object, float]:\n   Find the best equality (feature == value) split using Gini impurity    n_samples, n_features = X.shape\n    parent_gini = _gini(y)\n\n    best_feature = -1\n    best_value   = None\n    best_gini    = floatinf    for f in range(n_features):\n        # np.unique works for both numeric and object dtypes\n        for val in np.unique(X[:, f]):\n            mask_left = (X[:, f] == val)\n            left_size = int(mask_left.sum())\n\n            # skip invalid splits (all left or all right)\n            if left_size == 0 or left_size == n_samples:\n                continue\n\n            right_size = n_samples - left_size\n            y_left, y_right = y[mask_left], y[~mask_left]\n\n            g_left  = _gini(y_left)\n            g_right = _gini(y_right)\n\n            weighted_gini = (left_size  / n_samples) * g_left + \\\n                            (right_size / n_samples) * g_right\n\n            if weighted_gini < best_gini:\n                best_gini    = weighted_gini\n                best_feature = f\n                best_value   = val\n\n    # No split found or improvement too small\n    if best_feature == -1 or (parent_gini - best_gini) < epsilon:\n        return (-1, None, float(parent_gini))\n\n    return (best_feature, best_value, float(best_gini))\n```"}
{"task_id": 552, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n   Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation follows the scaled forward-backward algorithm to avoid\n    numerical under-/overflow.  The final values are rounded to four decimals.\n\n    Args:\n        hmm (dict): HMM parameters with keys\n                    'pi' (1-D array-like, length N),\n                    'A'  (2-D array-like, shape N\u00d7N),\n                    'B'  (2-D array-like, shape N\u00d7M).\n        observations (list[int]): Sequence of observation indices (length T).\n        t (int): Time step (0-based) for which \u03b3\u209c is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t\n                     (rounded to 4 decimals).  If t is outside the valid range\n                     an empty list is returned.\n       # ------------------------------------------------------------------\n    # 0. Small sanity checks\n    # ------------------------------------------------------------------\n    T = len(observations)\n    if t < 0 or t >= T:\n        return []\n\n    # Cast everything to numpy arrays of dtype float for convenience\n    pi = np.asarray(hmmpi dtype=float)\n    A  = np.asarray(hmmA  dtype=float)\n    B  = np.asarray(hmmB  dtype=float)\n\n    N = pi.size\n\n    # ------------------------------------------------------------------\n    # 1. Forward pass  (scaled)\n    # ------------------------------------------------------------------\n    alpha = np.zeros((T, N))\n    c     = np.zeros(T)               # scaling factors\n\n    # t = 0\n    alpha[0] = pi * B[:, observations[0]]\n    c[0]     = alpha[0].sum()\n    if c[0] == 0.0:                   # completely impossible observation\n        return [0.0]*N\n    alpha[0] /= c[0]\n\n    # t = 1 \u2026 T-1\n    for k in range(1, T):\n        alpha[k] = (alpha[k-1] @ A) * B[:, observations[k]]\n        c[k]     = alpha[k].sum()\n        if c[k] == 0.0:\n            return [0.0]*N\n        alpha[k] /= c[k]\n\n    # ------------------------------------------------------------------\n    # 2. Back-ward pass (scaled so that \u03b3 can be obtained directly)\n    # ------------------------------------------------------------------\n    beta = np.zeros((T, N))\n    beta[T-1] = 1.0 / c[T-1]          # \u03b2_{T-1}(i) = 1  (then scaled)\n\n    for k in range(T-2, -1, -1):\n        # vectorised version of \u03b2_k(i) = \u03a3_j A_{ij} B_{j,o_{k+1}} \u03b2_{k+1}(j)\n        beta[k] = (A * B[:, observations[k+1]]).dot(beta[k+1])\n        beta[k] /= c[k]\n\n    # ------------------------------------------------------------------\n    # 3. Combine to obtain \u03b3\u209c and normalise (minor numerical drift possible)\n    # ------------------------------------------------------------------\n    gamma_t = alpha[t] * beta[t]\n    gamma_t /= gamma_t.sum()\n\n    # ------------------------------------------------------------------\n    # 4. Round to four decimals and return as Python list\n    # ------------------------------------------------------------------\n    return [round(float(x), 4) for x in gamma_t]\n```"}
{"task_id": 553, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_regression(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    alpha: float = 0.01,\n    max_iter: int = 1000,\n) -> list[int]:\n   Train a binary Logistic Regression model with batch gradient descent and\n    predict class labels for a test set.\n       # ------------------------------------------------------------------\n    # Helper: numerically\u2013stable sigmoid\n    # ------------------------------------------------------------------\n    def _sigmoid(z: np.ndarray) -> np.ndarray:\n        # Clip to avoid overflow in exp(\u00b1z)\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # ------------------------------------------------------------------\n    # Ensure we are working with float arrays\n    # ------------------------------------------------------------------\n    X_train = np.asarray(X_train, dtype=np.float64)\n    y_train = np.asarray(y_train, dtype=np.float64).reshape(-1)\n    X_test  = np.asarray(X_test,  dtype=np.float64)\n\n    n_samples, n_features = X_train.shape\n\n    # ------------------------------------------------------------------\n    # Add bias column (a column of 1s) to both train and test matrices\n    # ------------------------------------------------------------------\n    X_train_bias = np.hstack((np.ones((n_samples, 1)), X_train))\n    X_test_bias  = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n\n    # ------------------------------------------------------------------\n    # Weight initialisation (including bias term)\n    # ------------------------------------------------------------------\n    w = np.zeros(n_features + 1, dtype=np.float64)\n\n    # ------------------------------------------------------------------\n    # Batch Gradient Descent\n    # ------------------------------------------------------------------\n    for _ in range(max_iter):\n        # Predicted probabilities\n        y_hat = _sigmoid(X_train_bias @ w)\n\n        # Gradient of (average) log-likelihood\n        gradient = X_train_bias.T @ (y_train - y_hat) / n_samples\n\n        # Update rule\n        w += alpha * gradient\n\n    # ------------------------------------------------------------------\n    # Prediction on test data\n    # ------------------------------------------------------------------\n    probs = _sigmoid(X_test_bias @ w)\n    preds = (probs >= 0.5).astype(int).tolist()  # Convert to plain Python ints\n\n    return preds\n```"}
{"task_id": 555, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import deque\n\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of shape (n_samples, n_features).\n    eps : float\n        Neighbourhood radius.\n    min_samples : int\n        Minimum points (including the point itself) that make a core point.\n\n    Returns\n    -------\n    list[int]\n        Cluster id for every sample (1, 2, \u2026), noise == \u22121.\n    \"\"\"\n    n_samples = data.shape[0]\n    if n_samples == 0:                       # empty input\n        return []\n\n    # ------------------------------------------------------------------\n    # 1. Pairwise distance matrix (Euclidean).\n    # ------------------------------------------------------------------\n    # Broadcasting trick: (n,1,d) - (1,n,d) -> (n,n,d)\n    diff = data[:, None, :] - data[None, :, :]\n    dist = np.linalg.norm(diff, axis=2)      # shape: (n_samples, n_samples)\n\n    # ------------------------------------------------------------------\n    # 2. Build \u03b5-neighbourhood list for every point.\n    # ------------------------------------------------------------------\n    neighbourhoods = [\n        np.where(dist[i] <= eps)[0]          # includes the point itself\n        for i in range(n_samples)\n    ]\n\n    # ------------------------------------------------------------------\n    # 3. Identify core points.\n    # ------------------------------------------------------------------\n    core_mask = np.array(\n        [len(neigh) >= min_samples for neigh in neighbourhoods], dtype=bool\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Cluster expansion (BFS over density-reachable core points).\n    #    0  -> unvisited / unlabeled\n    #   -1  -> noise\n    #   >0  -> cluster id\n    # ------------------------------------------------------------------\n    labels = np.zeros(n_samples, dtype=int)\n    current_cluster = 0\n\n    for point_idx in range(n_samples):\n        if not core_mask[point_idx] or labels[point_idx] != 0:\n            continue                         # skip non-core or already processed\n\n        current_cluster += 1                 # new cluster id\n        labels[point_idx] = current_cluster\n        queue = deque([point_idx])\n\n        # Breadth-first growth\n        while queue:\n            p = queue.popleft()\n            for neighbour in neighbourhoods[p]:\n                if labels[neighbour] == 0:   # not yet assigned to any cluster\n                    labels[neighbour] = current_cluster\n                    if core_mask[neighbour]:\n                        queue.append(neighbour)\n\n    # ------------------------------------------------------------------\n    # 5. Anything still unlabeled is noise (label \u22121).\n    # ------------------------------------------------------------------\n    labels[labels == 0] = -1\n    return labels.tolist()\n```"}
{"task_id": 559, "completion_id": 0, "completion": "```python\nfrom typing import List, Optional\n\n\nclass Node:\n   Binary-tree node used by the task.\n\n    Attributes\n    ----------\n    val\n        The value stored in the node. **Only leaf nodes have a value.**\n    right\n        Reference to the right child or ``None``.\n    left\n        Reference to the left child or ``None``.\n       def __init__(self, val: Optional[int] = None,\n                 right: 'Optional[Node]' = None,\n                 left: 'Optional[Node]' = None):\n        self.val = val\n        self.right = right\n        self.left = left\n\n\ndef leaf_values(root: Optional[Node]) -> List[int]:\n   Return all leaf values from left to right.\n\n    Parameters\n    ----------\n    root\n        Root node of the binary tree, or ``None`` for an empty tree.\n\n    Returns\n    -------\n    list[int]\n        Values stored in every leaf node, ordered from the\n        left-most leaf to the right-most leaf.\n       if root is None:                          # Empty tree\n        return []\n\n    leaves: List[int] = []\n\n    def dfs(node: Optional[Node]) -> None:\n        if node is None:\n            return\n        # A node is a leaf when it has no children.\n        if node.left is None and node.right is None:\n            # According to the problem statement, leaf nodes always\n            # carry a value, but we guard against the unexpected.\n            if node.val is not None:\n                leaves.append(node.val)\n            return\n        # Visit left child first to maintain left-to-right order.\n        dfs(node.left)\n        dfs(node.right)\n\n    dfs(root)\n    return leaves\n```"}
{"task_id": 560, "completion_id": 0, "completion": "```python\nimport math\nimport heapq\n\n\ndef k_nearest_neighbors(points: list[list[float]],\n                        query: list[float],\n                        k: int = 3) -> list[int]:\n   Return the indices of the *k* closest points to *query*.\n\n    A balanced k-d tree is built (plain ``dict`` nodes, no classes are used)\n    and probed with the standard back-tracking k-NN algorithm.\n\n    If *k* \u2265 number of points, the indices of **all** points are returned in the\n    required order.\n       # ------------------------------------------------------------------ helpers\n    def sq_dist(p, q):\n       Squared Euclidean distance        return sum((pi - qi) ** 2 for pi, qi in zip(p, q))\n\n    # ------------ k-d tree construction --------------------------------------\n    def build(idx_list, depth=0):\n       Recursively build a balanced k-d tree.\n\n        Each node is a dictionary with keys:\n        'idx'  : index of the point stored in this node\n        'axis' : splitting dimension\n        'left' : left sub-tree (None if absent)\n        'right': right sub-tree (None if absent)\n               if not idx_list:\n            return None\n        axis = depth % dims\n        idx_list.sort(key=lambda i: points[i][axis])          # sort w.r.t. axis\n        mid = len(idx_list) // 2                              # median\n        idx = idx_list[mid]\n        return {\n            'idx':   idx,\n            'axis':  axis,\n            'left':  build(idx_list[:mid], depth + 1),\n            'right': build(idx_list[mid + 1:], depth + 1),\n        }\n\n    # ------------ search the tree --------------------------------------------\n    def search(node):\n       Depth-first, back-tracking k-NN search        if node is None:\n            return\n\n        idx = node['idx']\n        axis = node['axis']\n\n        # update best-k heap ---------------------------------------------------\n        d2 = sq_dist(points[idx], query)\n        item = (-d2, -idx)                        # keep max-heap via negatives\n        if len(best) < k:\n            heapq.heappush(best, item)\n        else:\n            heapq.heappushpop(best, item)\n\n        # choose branch to explore first --------------------------------------\n        diff = query[axis] - points[idx][axis]\n        near, far = ('left', 'right') if diff < 0 else ('right', 'left')\n\n        search(node[near])\n\n        # decide whether the far branch could still hold a closer neighbour ---\n        worst_d2 = -best[0][0] if len(best) == k else math.inf\n        if diff * diff < worst_d2:\n            search(node[far])\n\n    # -------------------------------------------------------------------------\n    n = len(points)\n    if n == 0 or k <= 0:\n        return []\n\n    # If we need *all* points, a tree is over-kill ----------------------------\n    if k >= n:\n        dists = [(sq_dist(p, query), i) for i, p in enumerate(points)]\n        dists.sort()                                    # (dist, idx) ordering\n        return [i for _, i in dists]\n\n    # build & search the k-d tree ---------------------------------------------\n    dims = len(points[0])\n    root = build(list(range(n)))\n\n    best: list[tuple[int, int]] = []                    # (-dist\u00b2, -idx) heap\n    search(root)\n\n    # heap \u2192 sorted result list ------------------------------------------------\n    result = [(-d2, -idx) for d2, idx in best]          # back to positive vals\n    result.sort()                                       # (dist\u00b2, idx)\n    return [idx for _, idx in result]\n```"}
{"task_id": 561, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n\ndef _log_gaussian_pdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> np.ndarray:\n       Log\u2010density of a full multivariate normal\n    log N(x | mean , cov)  for a batch of samples x (N,D).\n       d = x.shape[1]\n    # Cholesky decomposition for stability\n    L = np.linalg.cholesky(cov)\n    # Solve L*y = (x-mean).T   \u2192  y = L^{-1}(x-mean)^T\n    y = np.linalg.solve(L, (x - mean).T)          # (D,N)\n    quad_form = np.sum(y ** 2, axis=0)            # (N,)\n    log_det = 2.0 * np.sum(np.log(np.diag(L)))    # log |\u03a3|\n    return -0.5 * (d * math.log(2.0 * math.pi) + log_det + quad_form)\n\n\ndef gmm_em(data: np.ndarray,\n           k: int,\n           max_iter: int = 200,\n           epsilon: float = 1e-4) -> list[int]:\n       Expectation\u2013Maximization for a Gaussian Mixture Model with full covariances.\n    Parameters\n    ----------\n    data : (N,D) ndarray\n        Input samples.\n    k : int\n        Number of Gaussian components.\n    max_iter : int, default 200\n        Maximum number of EM iterations.\n    epsilon : float, default 1e-4\n        Convergence threshold on the maximum absolute change of the mixture\n        weights \u03c0.\n    Returns\n    -------\n    list[int]\n        Final hard cluster assignment for every sample (length N),\n        labels are 0 \u2026 k\u22121 ordered by the first coordinate of the component means.\n       # ---------- special case -------------------------------------------------\n    n_samples, n_dims = data.shape\n    if k == 1:\n        return [0] * n_samples\n\n    rng = np.random.default_rng()        # independent random generator\n    reg_covar = 1e-6                     # diagonal regularisation\n\n    # ---------- initialisation ----------------------------------------------\n    means = data[rng.choice(n_samples, size=k, replace=False)].copy()        # (k,D)\n    covs = np.array([np.cov(data, rowvar=False) + reg_covar * np.eye(n_dims)\n                     for _ in range(k)])                                     # (k,D,D)\n    pis = np.full(k, 1.0 / k)                                                # (k,)\n\n    # ---------- EM -----------------------------------------------------------\n    for _ in range(max_iter):\n        pis_prev = pis.copy()\n\n        # E-step: responsibilities \u03b3_{nk}   shape (N,K)\n        log_resp = np.empty((n_samples, k))\n        for j in range(k):\n            log_resp[:, j] = math.log(pis[j]) + _log_gaussian_pdf(data, means[j], covs[j])\n        # log-sum-exp for normalisation\n        log_norm = log_resp.max(axis=1, keepdims=True)\n        log_resp = np.exp(log_resp - log_norm)\n        resp = log_resp / log_resp.sum(axis=1, keepdims=True)                # (N,K)\n\n        # M-step\n        Nk = resp.sum(axis=0)                                                # (K,)\n        pis = Nk / n_samples                                                 # (K,)\n        means = (resp.T @ data) / Nk[:, None]                                # (K,D)\n        for j in range(k):\n            diff = data - means[j]                                           # (N,D)\n            covs[j] = (resp[:, j][:, None] * diff).T @ diff / Nk[j]\n            covs[j].flat[:: n_dims + 1] += reg_covar                         # add to diagonal\n\n        # convergence check\n        if np.max(np.abs(pis - pis_prev)) < epsilon:\n            break\n\n    # ---------- hard assignment ---------------------------------------------\n    labels = np.argmax(resp, axis=1)                                         # (N,)\n\n    # ---------- deterministic relabelling -----------------------------------\n    order = np.argsort(means[:, 0])                                          # by first coordinate\n    relabel = np.empty_like(order)\n    relabel[order] = np.arange(k)                                            # mapping old\u2192new\n    final_labels = relabel[labels]\n\n    return final_labels.tolist()\n```"}
{"task_id": 562, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _kmeans(points: np.ndarray, k: int, max_iter: int = 300) -> np.ndarray:\n    \"\"\"\n    Very small, deterministic k-means:\n        \u2013 initial centroids = first k samples\n        \u2013 stops when labels do not change or max_iter reached\n        \u2013 if a cluster becomes empty, its centroid is left unchanged\n    Returns the final label array (shape (N,))\n    \"\"\"\n    n_samples, _ = points.shape\n    centroids = points[:k].copy()\n    labels = np.full(n_samples, -1, dtype=int)\n\n    for _ in range(max_iter):\n        # assignment step\n        # squared distances to every centroid\n        d2 = (\n            np.sum(points ** 2, axis=1, keepdims=True)\n            - 2 * points @ centroids.T\n            + np.sum(centroids ** 2, axis=1)\n        )\n        new_labels = np.argmin(d2, axis=1)\n\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # update step\n        for idx in range(k):\n            mask = labels == idx\n            if np.any(mask):\n                centroids[idx] = points[mask].mean(axis=0)\n\n    return labels\n\n\ndef spectral_clustering(\n    data: np.ndarray,\n    n_cluster: int,\n    gamma: float = 2.0,\n    method: str = \"unnormalized\",\n) -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\"\"\"\n    N, _ = data.shape\n    if n_cluster <= 1 or N == 0:\n        return [0] * N\n\n    # ------------------------------------------------------------------\n    # 1. Similarity matrix with Gaussian kernel\n    # ------------------------------------------------------------------\n    # pairwise squared Euclidean distances\n    sq_norm = np.sum(data**2, axis=1, keepdims=True)\n    dist2 = sq_norm + sq_norm.T - 2.0 * data @ data.T\n    # numerical stability: any tiny negative values \u2192 0\n    dist2 = np.clip(dist2, 0.0, None)\n\n    W = np.exp(-gamma * dist2)\n    np.fill_diagonal(W, 0.0)  # w_ii = 0\n\n    # ------------------------------------------------------------------\n    # 2. Laplacian\n    # ------------------------------------------------------------------\n    degree = np.sum(W, axis=1)\n    if method == \"normalized\":\n        # L_sym = I - D^{-1/2} W D^{-1/2}\n        with np.errstate(divide=\"ignore\"):\n            inv_sqrt_d = 1.0 / np.sqrt(degree)\n        inv_sqrt_d[np.isinf(inv_sqrt_d)] = 0.0\n        D_inv_sqrt = inv_sqrt_d[:, None]\n        L = np.eye(N) - (D_inv_sqrt * W) * D_inv_sqrt.T\n    else:  # unnormalised\n        L = np.diag(degree) - W\n\n    # ------------------------------------------------------------------\n    # 3. Eigen decomposition \u2013 take k smallest eigenvectors\n    # ------------------------------------------------------------------\n    # eigh returns eigenvalues in ascending order\n    eigvals, eigvecs = np.linalg.eigh(L)\n    embedding = eigvecs[:, :n_cluster]\n\n    if method == \"normalized\":\n        # row-normalise the embedding\n        row_norms = np.linalg.norm(embedding, axis=1, keepdims=True)\n        row_norms[row_norms == 0] = 1.0\n        embedding = embedding / row_norms\n\n    # ------------------------------------------------------------------\n    # 4. k-means in the spectral space\n    # ------------------------------------------------------------------\n    initial_labels = _kmeans(embedding, n_cluster)\n\n    # ------------------------------------------------------------------\n    # 5. Deterministic relabelling\n    # ------------------------------------------------------------------\n    smallest_per_cluster = []\n    for c in range(n_cluster):\n        idx = np.where(initial_labels == c)[0]\n        if idx.size == 0:\n            smallest = np.iinfo(int).max  # empty cluster \u2013 put at the end\n        else:\n            smallest = idx.min()\n        smallest_per_cluster.append((smallest, c))\n\n    # map old label \u2192 new label\n    smallest_per_cluster.sort()\n    new_label = {old: new for new, (_, old) in enumerate(smallest_per_cluster)}\n\n    final_labels = [new_label[l] for l in initial_labels]\n    return final_labels\n```"}
{"task_id": 563, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron in its dual formulation.\n\n    Parameters\n    ----------\n    X_data : ndarray of shape (N, d)\n        Training patterns.\n    y_data : ndarray of shape (N,)\n        Labels (+1 / -1).\n    eta : float, default=1.0\n        Learning-rate.\n    max_iter : int, default=1000\n        Maximal number of full passes through the data.\n\n    Returns\n    -------\n    (w, b) : tuple\n        w \u2013 list of floats (rounded to 4 decimals),  \n        b \u2013 float (rounded to 4 decimals)\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 1. initialise dual variables\n    # ------------------------------------------------------------------ #\n    N, d = X_data.shape\n    alpha = np.zeros(N, dtype=float)\n    b = 0.0\n\n    # ------------------------------------------------------------------ #\n    # 2. pre-compute Gram matrix G[i, j] = x_i \u00b7 x_j\n    # ------------------------------------------------------------------ #\n    G = X_data @ X_data.T        # shape (N, N)\n\n    # ------------------------------------------------------------------ #\n    # 3. dual perceptron learning loop\n    # ------------------------------------------------------------------ #\n    for _ in range(max_iter):\n        updated = False\n\n        for i in range(N):\n            # g = \u03a3_j \u03b1_j y_j G[j, i]\n            g = np.dot(alpha * y_data, G[:, i])\n\n            # misclassified ?\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n\n        if not updated:          # converged\n            break\n\n    # ------------------------------------------------------------------ #\n    # 4. recover primal weight vector  w = \u03a3_i \u03b1_i y_i x_i\n    # ------------------------------------------------------------------ #\n    w = (alpha * y_data) @ X_data     # shape (d,)\n\n    # ------------------------------------------------------------------ #\n    # 5. rounding & pythonic return types\n    # ------------------------------------------------------------------ #\n    w = np.round(w, 4).tolist()\n    b = float(np.round(b, 4))\n\n    return w, b\n```"}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    node = root\n\n    while True:\n        # 1.  Leaf check \u2013 the presence of attribute ``y`` marks a leaf.\n        if hasattr(node, \"y\"):\n            return node.y\n\n        # ------------------------------------------------------------------\n        #  Classification\n        # ------------------------------------------------------------------\n        if task_type == \"classification\":\n            feature_val = features[node.label]\n\n            # look for a child whose ``x`` matches the inspected value\n            chosen_child = None\n            for child in node.child:\n                if getattr(child, \"x\", None) == feature_val:\n                    chosen_child = child\n                    break\n\n            # default to the second child if no match was found\n            if chosen_child is None:\n                chosen_child = node.child[1]\n\n            node = chosen_child\n\n        # ------------------------------------------------------------------\n        #  Regression\n        # ------------------------------------------------------------------\n        else:  # task_type == \"regression\"\n            feature_val = features[node.label]\n\n            # decide based on split point ``s``\n            node = node.child[0] if feature_val <= node.s else node.child[1]\n```"}
{"task_id": 565, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _entropy(labels: np.ndarray) -> float:\n   Shannon entropy (base-2) of a 1-D array of integer class labels    if labels.size == 0:\n        return 0.0\n    _, counts = np.unique(labels, return_counts=True)\n    probs = counts / counts.sum()\n    return -np.sum(probs * np.log2(probs))\n\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n   Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) with discrete values\n        (non-negative integers starting from 0).\n    y : np.ndarray\n        1-D array of shape (n_samples,) with integer class labels\n        (non-negative integers starting from 0).\n\n    Returns\n    -------\n    tuple[int, float]\n        (best_feature_index, max_information_gain) where the gain is rounded\n        to 6 decimal places.\n       n_samples, n_features = X.shape\n    # Entropy of the whole data set\n    base_entropy = _entropy(y)\n\n    best_idx = 0\n    best_gain = -np.inf  # start lower than any possible gain (>=0)\n\n    for j in range(n_features):\n        col = X[:, j]\n        values, counts = np.unique(col, return_counts=True)\n\n        # Conditional entropy H(y | feature=j)\n        cond_entropy = 0.0\n        for v, cnt in zip(values, counts):\n            subset_labels = y[col == v]\n            cond_entropy += (cnt / n_samples) * _entropy(subset_labels)\n\n        info_gain = base_entropy - cond_entropy\n\n        # In case of ties choose the smallest index\n        if (info_gain > best_gain + 1e-12) or (\n            abs(info_gain - best_gain) <= 1e-12 and j < best_idx\n        ):\n            best_idx = j\n            best_gain = info_gain\n\n    return int(best_idx), round(float(best_gain), 6)\n```"}
{"task_id": 566, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\n# global deterministic seed ---------------------------------------------------\nnp.random.seed(42)\n\n\ndef _c(n: int) -> float:\n    \"\"\"Expected path length of an unsuccessful search in a BST.\"\"\"\n    if n <= 1:\n        return 0.0\n    return (\n        2.0 * math.log(n - 1) + 0.5772156649 - 2.0 * (n - 1) / n\n    )  # Euler-Mascheroni constant\n\n\nclass _Node:\n    \"\"\"Very small helper class representing a node in an isolation tree.\"\"\"\n\n    __slots__ = (\"left\", \"right\", \"feat\", \"thr\", \"size\", \"is_leaf\")\n\n    def __init__(\n        self,\n        *,\n        is_leaf: bool,\n        size: int,\n        feat: int | None = None,\n        thr: float | None = None,\n        left: \" _Node\" | None = None,\n        right: \" _Node\" | None = None,\n    ):\n        self.is_leaf = is_leaf\n        self.size = size\n        self.feat = feat\n        self.thr = thr\n        self.left = left\n        self.right = right\n\n\ndef _build_tree(\n    data: np.ndarray,\n    idx: np.ndarray,\n    height_limit: int,\n    current_depth: int,\n) -> _Node:\n    \"\"\"Recursively builds a random isolation tree.\"\"\"\n    n_samples = idx.size\n\n    # stopping conditions ------------------------------------------------------\n    if current_depth >= height_limit or n_samples <= 1:\n        return _Node(is_leaf=True, size=n_samples)\n\n    # choose random feature ----------------------------------------------------\n    feat = np.random.randint(0, data.shape[1])\n    col = data[idx, feat]\n    min_val, max_val = col.min(), col.max()\n\n    if min_val == max_val:  # no possible split, make a leaf\n        return _Node(is_leaf=True, size=n_samples)\n\n    # random threshold in [min, max) ------------------------------------------\n    thr = np.random.uniform(min_val, max_val)\n\n    # split samples ------------------------------------------------------------\n    left_mask = col < thr\n    right_mask = ~left_mask\n\n    if not left_mask.any() or not right_mask.any():  # degenerate split\n        return _Node(is_leaf=True, size=n_samples)\n\n    left_idx = idx[left_mask]\n    right_idx = idx[right_mask]\n\n    # recursive construction ---------------------------------------------------\n    left_node = _build_tree(data, left_idx, height_limit, current_depth + 1)\n    right_node = _build_tree(data, right_idx, height_limit, current_depth + 1)\n\n    return _Node(\n        is_leaf=False,\n        size=n_samples,\n        feat=feat,\n        thr=thr,\n        left=left_node,\n        right=right_node,\n    )\n\n\ndef _path_length(x: np.ndarray, node: _Node, current_depth: int = 0) -> float:\n    \"\"\"Path length of observation x in a given isolation tree.\"\"\"\n    if node.is_leaf:\n        return current_depth + _c(node.size)\n    if x[node.feat] < node.thr:\n        return _path_length(x, node.left, current_depth + 1)\n    return _path_length(x, node.right, current_depth + 1)\n\n\n# -----------------------------------------------------------------------------\n\n\ndef detect_outliers(\n    data: np.ndarray, n_trees: int, sample_size: int, epsilon: float\n) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\"\"\"\n    n_samples, n_features = data.shape\n\n    if epsilon == 0.0:\n        return []\n\n    # height limit depends on the subsample size --------------------------------\n    max_samples_for_tree = min(sample_size, n_samples)\n    height_limit = math.ceil(math.log2(max_samples_for_tree))\n\n    # grow the forest -----------------------------------------------------------\n    trees: list[_Node] = []\n    for _ in range(n_trees):\n        if sample_size >= n_samples:\n            subsample_idx = np.arange(n_samples)\n        else:\n            subsample_idx = np.random.choice(\n                n_samples, size=sample_size, replace=False\n            )\n        root = _build_tree(data, subsample_idx, height_limit, current_depth=0)\n        trees.append(root)\n\n    # compute path lengths for every observation -------------------------------\n    path_lengths = np.zeros(n_samples, dtype=float)\n    for tree in trees:\n        # accumulate path length of every sample for this tree\n        for i in range(n_samples):\n            path_lengths[i] += _path_length(data[i], tree)\n\n    mean_path_lengths = path_lengths / n_trees\n\n    # anomaly scores -----------------------------------------------------------\n    n = n_samples\n    phi = 2.0 * math.log(n - 1) - 2.0 * (n - 1) / n  # \u03c6\n    scores = np.power(2.0, -mean_path_lengths / phi)\n\n    # determine the number of outliers -----------------------------------------\n    k = int(math.ceil(epsilon * n_samples))\n    if k == 0:\n        return []\n\n    # indices of the k largest scores ------------------------------------------\n    top_k_idx = np.argsort(scores)[-k:][::-1]  # descending\n    return sorted(top_k_idx.tolist())\n```"}
{"task_id": 567, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef majority_vote(predictions: list[list[int | float]]) -> list[int]:\n   Ensemble majority voting.\n    \n    Parameters\n    ----------\n    predictions : list[list[int | float]]\n        A 2-D list/array where each row contains the predictions from one tree\n        and each column groups the votes for a single sample.\n    \n    Returns\n    -------\n    list[int]\n        The final class label for every sample after majority voting.  \n        In a tie, the smallest label is chosen.\n       # Convert to a 2-D NumPy array (this also works if `predictions` is already\n    # a NumPy array).  `copy=False` avoids an extra copy when possible.\n    votes = np.asarray(predictions, copy=False)\n\n    if votes.ndim != 2:\n        raise ValueError`predictions` must be a 2-D array-like object    \n    # Transpose to iterate over samples (columns) conveniently.\n    result: list[int] = []\n    for column in votes.T:                       # one sample at a time\n        labels, counts = np.unique(column, return_counts=True)\n        max_count = counts.max()                 # highest vote count\n\n        # All labels that reach the maximum count (handles ties).\n        tied_labels = labels[counts == max_count]\n\n        # Tie-break by taking the smallest label.\n        chosen = int(tied_labels.min())\n        result.append(chosen)\n\n    return result\n```"}
{"task_id": 568, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n       Maximum-Entropy classifier trained with Generalised Iterative Scaling (GIS).\n\n    Parameters\n    ----------\n    X_train : (N, d) ndarray  \u2013 categorical feature matrix\n    y_train : (N,)  ndarray   \u2013 integer class labels\n    X_test  : (M, d) ndarray  \u2013 data to classify\n    epsilon : float           \u2013 stopping threshold for the largest |\u0394w|\n    n_iter  : int             \u2013 maximum number of GIS iterations\n\n    Returns\n    -------\n    list[int]                  \u2013 predicted label for every row of X_test\n       # ------------------ helpers ------------------\n    def feature_key(col_idx: int, value, label: int):\n       Canonical key for the (feature-value, label) indicator        return (col_idx, int(value), int(label))\n\n    # ------------------ preparation --------------\n    N, n_features = X_train.shape\n    labels = [int(c) for c in sorted(set(y_train.astype(int).tolist()))]  # all seen labels\n    C = n_features  # every (x, y) activates exactly one feature per column\n\n    # Empirical expectations  \\tilde{E}[f_j]\n    emp_exp = defaultdict(float)  # key -> expectation\n    for x_row, y in zip(X_train, y_train):\n        y = int(y)\n        for j, v in enumerate(x_row):\n            emp_exp[feature_key(j, v, y)] += 1.0\n    for k in emp_exp:\n        emp_exp[k] /= N  # turn counts into expectations\n\n    # Weight vector  (all start at zero)\n    w = defaultdict(float)\n\n    # ------------------ GIS training -------------\n    tiny = 1e-12  # to avoid log(0)\n    for _ in range(n_iter):\n        # Model expectations  E_w[f_j]\n        mod_exp = defaultdict(float)\n\n        for x_row in X_train:\n            # Unnormalised log-scores for every label\n            log_scores = {}\n            for y in labels:\n                s = 0.0\n                for j, v in enumerate(x_row):\n                    s += w.get(feature_key(j, v, y), 0.0)\n                log_scores[y] = s\n\n            # Numerically stable soft-max\n            m = max(log_scores.values())\n            exp_scores = {y: math.exp(s - m) for y, s in log_scores.items()}\n            Z = sum(exp_scores.values())\n            probs = {y: exp_scores[y] / Z for y in labels}\n\n            # Accumulate model expectations\n            for y, p in probs.items():\n                for j, v in enumerate(x_row):\n                    mod_exp[feature_key(j, v, y)] += p / N\n\n        # GIS weight updates\n        max_delta = 0.0\n        for k, emp_val in emp_exp.items():\n            mod_val = mod_exp.get(k, tiny)\n            # \u0394w_j = (1/C) * log(empirical / model)\n            delta = (1.0 / C) * math.log(emp_val / max(mod_val, tiny))\n            w[k] += delta\n            max_delta = max(max_delta, abs(delta))\n\n        if max_delta < epsilon:\n            break\n\n    # ------------------ prediction ---------------\n    preds = []\n    for x_row in X_test:\n        best_label, best_score = None, -floatinf        for y in labels:\n            s = 0.0\n            for j, v in enumerate(x_row):\n                s += w.get(feature_key(j, v, y), 0.0)\n            if s > best_score:\n                best_score = s\n                best_label = y\n        preds.append(best_label)\n\n    return preds\n```"}
{"task_id": 569, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio is rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                 ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of ``(word, ratio)`` pairs and (2) a set with the\n        most frequent words as described above.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Basic checks\n    # ------------------------------------------------------------------\n    if not data:                              # no documents -> empty outputs\n        return [], set()\n\n    total_docs = len(data)\n\n    # ------------------------------------------------------------------\n    # 2. Count in how many documents each word occurs\n    # ------------------------------------------------------------------\n    doc_counter: Counter[str] = Counter()\n\n    for _label, words in data:\n        # 'words' can be any iterable -> turn into a set to avoid double counts\n        doc_counter.update(set(words))\n\n    # ------------------------------------------------------------------\n    # 3. Build list with (word, ratio) and sort\n    # ------------------------------------------------------------------\n    freq_list: List[Tuple[str, float]] = [\n        (word, round(count / total_docs, 4)) for word, count in doc_counter.items()\n    ]\n\n    # sort by ratio descending; a secondary key (`word`) gives deterministic order\n    freq_list.sort(key=lambda x: (-x[1], x[0]))\n\n    # ------------------------------------------------------------------\n    # 4. Build the set with the *cut_off* most common words\n    # ------------------------------------------------------------------\n    if cut_off is None:                       # keep every word\n        top_words: Set[str] = {word for word, _ in freq_list}\n    elif cut_off <= 0:                        # explicitly empty result\n        top_words = set()\n    else:                                     # take the first *cut_off* items\n        top_words = {word for word, _ in freq_list[:cut_off]}\n\n    return freq_list, top_words\n```\n\nExample usage\n-------------\n```python\ndocs = [\n    (\"doc1\", [\"apple\", \"banana\", \"cherry\", \"apple\"]),\n    (\"doc2\", [\"banana\", \"durian\"]),\n    (\"doc3\", [\"banana\", \"cherry\"]),\n]\n\nratios, top = statistic_key_word(docs, cut_off=2)\nprint(ratios)  # [('banana', 1.0), ('cherry', 0.6667), ('apple', 0.3333), ('durian', 0.3333)]\nprint(top)     # {'banana', 'cherry'}\n```\n"}
{"task_id": 570, "completion_id": 0, "completion": "```python\nfrom collections import defaultdict\n\ndef naive_bayes_predict(train_data: list[tuple[str, list[str]]],\n                        keywords: set[str],\n                        test_samples: list[list[str]]) -> list[str]:\n    \"\"\"Unsmmoothed Bernoulli Naive Bayes, keyword based, no class priors.\"\"\"\n    \n    # ------------------------------------------------------------------\n    # 1.  Training: document-level counts  \u21d2  P(w | c) = count / N_c\n    # ------------------------------------------------------------------\n    # N_c  \u2013 number of documents per class\n    class_doc_counts: dict[str, int] = defaultdict(int)\n    \n    # count_c[w] \u2013 in how many documents of class c the keyword w occurs\n    keyword_doc_counts: dict[str, dict[str, int]] = defaultdict(\n        lambda: defaultdict(int)\n    )\n\n    for label, tokens in train_data:\n        class_doc_counts[label] += 1\n        token_set = set(tokens)                 # Bernoulli \u2192 presence only\n        for w in keywords:\n            if w in token_set:\n                keyword_doc_counts[label][w] += 1\n\n    # Conditional probabilities P(w | c)\n    p_wc: dict[str, dict[str, float]] = defaultdict(dict)\n    for cls, N_c in class_doc_counts.items():\n        for w in keywords:\n            cnt = keyword_doc_counts[cls].get(w, 0)\n            p_wc[cls][w] = cnt / N_c            # No smoothing\n\n    # ------------------------------------------------------------------\n    # 2.  Prediction\n    # ------------------------------------------------------------------\n    predictions: list[str] = []\n    \n    # Make iteration over classes deterministic (sorted order helps ties)\n    classes = sorted(class_doc_counts.keys())\n\n    for tokens in test_samples:\n        T = set(tokens)\n        \n        best_cls   = None\n        best_score = -1.0                       # likelihoods are \u2265 0\n        \n        for cls in classes:\n            likelihood = 1.0\n            for w in keywords:\n                p = p_wc[cls][w]\n                likelihood *= p if w in T else (1.0 - p)\n            \n            if likelihood > best_score:\n                best_score = likelihood\n                best_cls   = cls\n        \n        predictions.append(best_cls)\n\n    return predictions\n```"}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n   Compute the weight vector and bias term of a linear SVM.\n\n    Args:\n        X: 2-D array of shape (n_samples, n_features)\n        y: 1-D array of shape (n_samples,) with labels in {-1, 1}\n        alpha: 1-D array of shape (n_samples,) with Lagrange multipliers\n\n    Returns:\n        (w_list, b) where every value is rounded to 4 decimals.\n       # Element-wise product \u03b1_i y_i (shape: n_samples,)\n    alpha_y = alpha * y\n\n    # Weight vector  w = X\u1d40(\u03b1 \u2299 y)  (shape: n_features,)\n    w = X.T @ alpha_y\n\n    # Bias  b = 1/n \u03a3_i ( y_i \u2212 w\u1d40 x_i )\n    b = np.mean(y - X @ w)\n\n    # Round to 4 decimal places\n    w_rounded = np.round(w, 4).tolist()        # Python list\n    b_rounded = round(float(b), 4)             # Python float\n\n    return w_rounded, b_rounded\n```"}
{"task_id": 572, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n       Compute Local Outlier Factor (LOF) for every sample in `data`.\n\n    Parameters\n    ----------\n    data : 2-D numpy array, shape (n_samples, n_features)\n    k    : int, 1 \u2264 k < n_samples\n    epsilon : float, threshold above which a sample is flagged as an outlier\n\n    Returns\n    -------\n    tuple (list_of_scores, list_of_outlier_indices)\n        list_of_scores          : list of LOF values rounded to 4 decimals\n        list_of_outlier_indices : sorted indices whose LOF > epsilon\n       data = np.asarray(data, dtype=float)\n    n = data.shape[0]\n    if not (1 <= k < n):\n        raise ValueErrork must satisfy 1 \u2264 k < number of samples    ###########################################################################\n    # 1. full pair-wise Euclidean distance matrix \u2011- shape (n,n)\n    ###########################################################################\n    # \u2016x\u2016\u00b2 = sum(x\u00b2);  (x\u2212y)\u00b2 = \u2016x\u2016\u00b2 + \u2016y\u2016\u00b2 \u2212 2 x\u00b7y\n    sq_norms = (data ** 2).sum(axis=1, keepdims=True)          # shape (n,1)\n    dist_sq  = sq_norms + sq_norms.T - 2 * data @ data.T       # Gram trick\n    dist_sq[dist_sq < 0] = 0                                   # numerical floor\n    D = np.sqrt(dist_sq)\n\n    ###########################################################################\n    # 2. k-distance & k nearest neighbours N_k(p)\n    ###########################################################################\n    # argsort once for every row (include self at pos 0)\n    idx_sorted = np.argsort(D, axis=1)\n    k_neigh_idx = idx_sorted[:, 1:k+1]              # skip self (col 0)\n    k_distances = D[np.arange(n), k_neigh_idx[:, -1]]   # distance to k-th nn\n\n    ###########################################################################\n    # 3. reachability distances: reach-dist_k(p,q) = max(k-dist(q), dist(p,q))\n    ###########################################################################\n    # We need k-dist(q) for every q.  Broadcast over neighbours:\n    k_dist_neighbors = k_distances[k_neigh_idx]         # shape (n,k)\n    dist_pq          = D[np.arange(n)[:, None], k_neigh_idx]\n    reach_dist       = np.maximum(k_dist_neighbors, dist_pq)\n\n    ###########################################################################\n    # 4. Local Reachability Density  LRD_k(p)\n    ###########################################################################\n    reach_sum = reach_dist.sum(axis=1)\n    # Guard against division by zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        lrd = k / reach_sum\n        lrd[~np.isfinite(lrd)] = 0.0                    # if sum==0 \u2192 density 0\n\n    ###########################################################################\n    # 5. Local Outlier Factor  LOF_k(p)\n    ###########################################################################\n    lrd_of_neigh = lrd[k_neigh_idx]                    # shape (n,k)\n    lof = (lrd_of_neigh.sum(axis=1) / k) / lrd\n    # If lrd==0 we previously set it to 0, avoid division by zero -> lof=0\n    lof[~np.isfinite(lof)] = 0.0\n\n    # ------------------------------------------------------------------------\n    # rounding & outlier indices\n    # ------------------------------------------------------------------------\n    lof_rounded = np.round(lof, 4)\n    outlier_idx = np.where(lof_rounded > epsilon)[0].tolist()\n\n    return lof_rounded.tolist(), sorted(outlier_idx)\n```\n\nExplanation of the most important steps\n\u2022 Distances are computed in one vectorised shot using the Gram-matrix trick, avoiding Python loops.  \n\u2022 For every point we keep exactly the first `k` nearest neighbours (`k_neigh_idx`) and its `k-distance`.  \n\u2022 The reachability distance matrix is built with simple `numpy.maximum` broadcasting.  \n\u2022 Local Reachability Density (LRD) and LOF are obtained through the definitions; divisions by zero are handled gracefully by converting non-finite results to 0.  \n\u2022 Finally every LOF score is rounded to the nearest 4th decimal place, and indices whose rounded score exceeds `epsilon` are returned, sorted in ascending order."}
{"task_id": 573, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef perceptron_train(X_train: np.ndarray,\n                     y_train: np.ndarray,\n                     eta: float = 0.1,\n                     n_iter: int = 1000) -> tuple[list[float], float, list[int]]:\n   Implement the Perceptron learning algorithm.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training patterns of shape (m, n).\n    y_train : np.ndarray\n        Target labels (+1 or -1) of length m.\n    eta : float, optional\n        Learning rate, by default 0.1.\n    n_iter : int, optional\n        Maximum number of passes over the data, by default 1000.\n\n    Returns\n    -------\n    tuple[list[float], float, list[int]]\n        A tuple containing the learned weight vector (as list), the bias term,\n        and a list with the number of errors in each epoch.\n       # Convert inputs to numpy arrays (no copy if already np.ndarray)\n    X = np.asarray(X_train, dtype=float)\n    y = np.asarray(y_train, dtype=float)\n\n    m, n = X.shape                     # m samples, n features\n    w = np.zeros(n, dtype=float)       # weight vector\n    b = 0.0                            # bias term\n    errors_history: list[int] = []     # error counts for each epoch\n\n    for _ in range(n_iter):\n        error_count = 0\n\n        # iterate through the training set once (in given order)\n        for xi, target in zip(X, y):\n            if target * (np.dot(w, xi) + b) <= 0.0:\n                # weight and bias update\n                w += eta * target * xi\n                b += eta * target\n                error_count += 1\n\n        errors_history.append(error_count)\n\n        # early stopping if no errors\n        if error_count == 0:\n            break\n\n    # Round weights and bias to the nearest 4th decimal place\n    w = np.round(w, 4)\n    b = float(np.round(b, 4))\n\n    return w.tolist(), b, errors_history\n```"}
{"task_id": 574, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # 1. Seed the RNGs (required: random, optional but convenient: NumPy)\n    # ------------------------------------------------------------------\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n\n    # ------------------------------------------------------------------\n    # 2. Allocate lists that will be concatenated at the end\n    # ------------------------------------------------------------------\n    X_train_parts, X_val_parts = [], []\n    y_train_parts, y_val_parts = [], []\n\n    # ------------------------------------------------------------------\n    # 3. Generate data class by class\n    # ------------------------------------------------------------------\n    for c in range(m):\n        base = 10 * (c // 2)        # left-bottom corner of the 8\u00d78 square\n        low, high = base + 1, base + 9\n\n        if n_train:\n            coords = np.random.uniform(low, high, size=(n_train, 2))\n            X_train_parts.append(coords)\n            y_train_parts.append(np.full(n_train, c, dtype=int))\n\n        if n_val:\n            coords = np.random.uniform(low, high, size=(n_val, 2))\n            X_val_parts.append(coords)\n            y_val_parts.append(np.full(n_val, c, dtype=int))\n\n    # ------------------------------------------------------------------\n    # 4. Stack per-class blocks together; fall back to empty arrays if any\n    #    of the requested set sizes is zero.\n    # ------------------------------------------------------------------\n    X_train = (np.vstack(X_train_parts)\n               if X_train_parts else np.empty((0, 2), dtype=float))\n    X_val   = (np.vstack(X_val_parts)\n               if X_val_parts else np.empty((0, 2), dtype=float))\n    y_train = (np.concatenate(y_train_parts)\n               if y_train_parts else np.empty((0,), dtype=int))\n    y_val   = (np.concatenate(y_val_parts)\n               if y_val_parts else np.empty((0,), dtype=int))\n\n    # ------------------------------------------------------------------\n    # 5. Round coordinates to 4 decimal places for compact, deterministic\n    #    output.\n    # ------------------------------------------------------------------\n    X_train = np.round(X_train, 4)\n    X_val   = np.round(X_val,   4)\n\n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 577, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport random\n\n\ndef generate_data(\n    m: int,\n    n_train: int,\n    n_val: int,\n    seed: int | None = None\n) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a tiny 2-D synthetic data set for classification.\n\n    The rectangles and labeling scheme follow the specification in the\n    task description.  All coordinates are rounded to four decimals.\n\n    Args\n    ----\n    m        : number of classes\n    n_train  : samples per class for the training set\n    n_val    : samples per class for the validation set\n    seed     : optional RNG seed for reproducibility\n\n    Returns\n    -------\n    X_train, X_val, y_train, y_val : four NumPy arrays\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    X_train, X_val = [], []\n    y_train, y_val = [], []\n\n    for i in range(m):\n        # class-specific rectangle limits\n        x_lo = ((i // 2) + 0.1) * 10\n        x_hi = ((i // 2) + 0.9) * 10\n        y_lo = (((i % 2) * 0.5) + 0.1) * 10\n        y_hi = (((i % 2) * 0.5) + 0.9) * 10\n\n        label = int((i - 0.5) * 2)\n\n        # training samples\n        for _ in range(n_train):\n            x = round(random.uniform(x_lo, x_hi), 4)\n            y = round(random.uniform(y_lo, y_hi), 4)\n            X_train.append([x, y])\n            y_train.append(label)\n\n        # validation samples\n        for _ in range(n_val):\n            x = round(random.uniform(x_lo, x_hi), 4)\n            y = round(random.uniform(y_lo, y_hi), 4)\n            X_val.append([x, y])\n            y_val.append(label)\n\n    # turn everything into NumPy arrays\n    X_train = np.asarray(X_train, dtype=float)\n    X_val   = np.asarray(X_val,   dtype=float)\n    y_train = np.asarray(y_train, dtype=int)\n    y_val   = np.asarray(y_val,   dtype=int)\n\n    return X_train, X_val, y_train, y_val\n```"}
{"task_id": 578, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple\n\ndef make_pairs(corpus: List) -> List[Tuple]:\n    \"\"\"Return every consecutive pair (bigram) found in *corpus*.\n\n    Args:\n        corpus: A list containing any kind of elements.\n\n    Returns:\n        A list of tuples where each tuple is a pair of consecutive\n        elements from *corpus*.\n    \"\"\"\n    # `zip` pairs each element with its successor; converting to `list`\n    # materialises the result as the required list of tuples.\n    return list(zip(corpus, corpus[1:]))\n```"}
{"task_id": 579, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Classical (Lloyd) K-Means with deterministic initialisation.\n\n    Args\n    ----\n    X : np.ndarray\n        Array of shape (m, d) with the input points.\n    k : int\n        Number of clusters, 1 \u2264 k \u2264 m.\n    max_iters : int, optional\n        Maximum number of Lloyd iterations (default 100).\n\n    Returns\n    -------\n    (labels, centres) : tuple\n        labels  \u2013 list of length m, the cluster index (0 \u2026 k-1) of every point.\n        centres \u2013 list of k lists, the final centres rounded to 4 decimals.\n    \"\"\"\n    # --- basic sanity checks -------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"`X` must be a 2-D array (m \u00d7 d).\")\n    m, d = X.shape\n    if not (1 <= k <= m):\n        raise ValueError(\"`k` must satisfy 1 \u2264 k \u2264 number of points.\")\n\n    # --- deterministic initialisation (first k points) ----------------------\n    centres = X[:k].copy()            # shape: (k, d)\n\n    labels = np.full(m, -1, dtype=int)    # \u201cprevious\u201d labels (none yet)\n\n    for _ in range(max_iters):\n        # -------------------- assignment step -------------------------------\n        # Squared Euclidean distance to every centre\n        #   distances[i, j] = || X[i] \u2013 centres[j] ||\u00b2\n        diff = X[:, None, :] - centres[None, :, :]        # (m, k, d)\n        distances = np.sum(diff ** 2, axis=2)             # (m, k)\n        new_labels = np.argmin(distances, axis=1)         # tie \u2192 smaller idx\n\n        # Convergence test\n        if np.array_equal(new_labels, labels):\n            break\n        labels = new_labels\n\n        # -------------------- update step -----------------------------------\n        for j in range(k):\n            idx = np.where(labels == j)[0]\n            if idx.size:                                   # cluster non-empty\n                centres[j] = X[idx].mean(axis=0)\n            # else: keep centre unchanged\n\n    # --- formatting of the results ------------------------------------------\n    centres_rounded = np.round(centres, 4).tolist()        # list[list[float]]\n    return labels.tolist(), centres_rounded\n```"}
{"task_id": 581, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n   Fits a linear model with Elastic-Net regularisation    \n    m, n = X.shape                          # number of samples and features\n    \n    # If nothing is to be learnt, return the initial parameters\n    if learning_rate == 0 or iterations == 0:\n        return ([0.0] * n, 0.0)\n    \n    w = np.zeros(n, dtype=float)            # weights w1 \u2026 wn\n    b = 0.0                                 # bias\n    \n    for _ in range(iterations):\n        y_hat = X @ w + b                   # predictions (m,)\n        r = y - y_hat                       # residuals  (m,)\n        \n        # ---- gradients ----\n        # sign(0) must be \u22121\n        sign_w = np.where(w == 0, -1.0, np.sign(w))\n        \n        grad_w = (-2.0 * (X.T @ r)                         # data part\n                  + l1_penalty * sign_w                    # L1 part\n                  + 2.0 * l2_penalty * w)                  # L2 part\n        \n        grad_b = -2.0 * r.sum()\n        \n        # ---- mean gradients ----\n        grad_w /= m\n        grad_b /= m\n        \n        # ---- simultaneous update ----\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n    \n    # ---- rounding ----\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(float(b), 4)\n    \n    return (w_rounded, b_rounded)\n```"}
{"task_id": 582, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef nearest_neighbor_graph(X):\n   Build the undirected k-nearest-neighbour adjacency matrix.\n\n    Parameters\n    ----------\n    X : Sequence[Sequence[float]]\n        2-D array-like of shape (n_samples, n_features).\n\n    Returns\n    -------\n    list[list[int]]\n        Adjacency matrix W (Python list of lists) where W[i][j] == 1\n        iff an undirected edge exists between samples i and j, else 0.\n       # Convert input to a NumPy array (float for distance computations)\n    X = np.asarray(X, dtype=float)\n    \n    # Handle degenerate cases -------------------------------------------------\n    if X.ndim == 1:                 # row vector -> single sample\n        X = X.reshape(1, -1)\n    n = X.shape[0]\n    if n == 0:                      # no samples\n        return []\n    if n == 1:                      # single sample\n        return [[0]]\n    \n    # 1. Determine k ----------------------------------------------------------\n    k = min(int(np.floor(np.sqrt(n))), 10)\n    if k == 0:                      # This only happens when n == 1 (already handled)\n        k = 1\n    \n    # 2. Pair-wise squared Euclidean distances (sqrt not needed for ranking) -\n    diff = X[:, None, :] - X[None, :, :]\n    dist_sq = np.einsum('ijk,ijk->ij', diff, diff)   # shape (n, n)\n    \n    # 3. Build directed k-NN adjacency matrix --------------------------------\n    directed = np.zeros((n, n), dtype=bool)\n    for i in range(n):\n        # Argsort returns indices sorted by distance (self-index i comes first)\n        order = np.argsort(dist_sq[i])\n        # Exclude the point itself, keep first k neighbours\n        neighbours = order[order != i][:k]\n        directed[i, neighbours] = True\n    \n    # 4. Symmetrise (undirected graph) ---------------------------------------\n    undirected = directed | directed.T\n    np.fill_diagonal(undirected, False)   # remove self-loops if any\n    \n    # 5. Return as a list of lists of ints ------------------------------------\n    return undirected.astype(int).tolist()\n```"}
{"task_id": 584, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef make_initial_state(state: list[list[int | float]]) -> list[list[list[int | float]]]:\n    \"\"\"Stack a single 2-D Atari frame four times to form the initial state.\n\n    Args:\n        state: 2-D list representing the grayscale frame (height \u00d7 width).\n\n    Returns:\n        3-D list with shape (height \u00d7 width \u00d7 4) where the frame is repeated\n        along the last axis.\n    \"\"\"\n    # Convert the nested list to a NumPy array\n    frame = np.asarray(state)\n\n    # Add a new last axis (height, width, 1) and replicate it 4 times\n    stacked = np.repeat(frame[..., None], repeats=4, axis=-1)\n\n    # Convert back to a regular Python nested list\n    return stacked.tolist()\n```"}
{"task_id": 585, "completion_id": 0, "completion": "```python\ndef cmp(a: object, b: object) -> int:\n    \"\"\"Compares two values and returns their ordering.\n\n    Mimics Python 2\u2019s built-in ``cmp``:\n\n        *  1  \u2192 a is greater than b\n        *  0  \u2192 a equals b\n        * \u20131 \u2192 a is less than b\n\n    Works for any pair of objects that implement the usual\n    comparison operators (<, >, ==).\n\n    Examples\n    --------\n    >>> cmp(3, 5)\n    -1\n    >>> cmp('spam', 'spam')\n    0\n    >>> cmp((2, 0), (1, 99))\n    1\n    \"\"\"\n    # The boolean results of the comparisons are instances of `bool`,\n    # which is a subclass of `int`;  True \u2192 1, False \u2192 0.\n    # Therefore, (a > b) - (a < b) collapses to:\n    #     1 - 0 = 1   if a > b\n    #     0 - 0 = 0   if a == b\n    #     0 - 1 = -1  if a < b\n    return (a > b) - (a < b)\n```\n\nHow it works\n-------------\n1. `a > b` evaluates to either `True` (1) or `False` (0).\n2. `a < b` does the same.\n3. Subtracting the two integers yields exactly **1**, **0**, or **-1**, fulfilling the required contract while staying concise and fully type-agnostic."}
{"task_id": 586, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nimport numpy as np\nimport math\n\ndef lowess_predict(x: np.ndarray, y: np.ndarray, x_query: float, window: int) -> float:\n    \"\"\"Single-point LOWESS prediction with linear local fitting.\"\"\"\n    \n    # --------------------- Validation ---------------------\n    if x.ndim != 1 or y.ndim != 1:\n        raise ValueError(\"x and y must be one-dimensional.\")\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have the same length.\")\n    if not (2 <= window <= len(x)):\n        raise ValueError(\"window must satisfy 2 \u2264 window \u2264 len(x).\")\n    \n    # ---------------- Normalisation helpers --------------\n    def _minmax_norm(arr, v):\n        \"\"\"Normalise array arr and single value v to [0,1].\"\"\"\n        a_min, a_max = arr.min(), arr.max()\n        span = a_max - a_min\n        if span == 0:         # constant array\n            return np.zeros_like(arr, dtype=float), 0.0\n        return (arr - a_min) / span, (v - a_min) / span\n    \n    # ------------------ 1. Normalise ---------------------\n    x_norm, xq_norm = _minmax_norm(x, x_query)\n    y_norm, _         = _minmax_norm(y, y[0])      # only need array; value unused\n    \n    # ------------- 2. Select nearest neighbours ----------\n    distances = np.abs(x_norm - xq_norm)\n    neighbour_idx = np.argsort(distances)[:window]\n    \n    x_w = x_norm[neighbour_idx]\n    y_w = y_norm[neighbour_idx]\n    d   = distances[neighbour_idx]\n    \n    # ---------------- 3. Tricubic weights ----------------\n    d_max = d.max()\n    if d_max == 0:                      # all neighbours on query x\n        w = np.ones_like(d)\n    else:\n        u = d / d_max                   # in [0,1]\n        w = (1 - u**3)**3               # tricubic\n    \n    # --------------- 4. Weighted line fit ----------------\n    w_sum = w.sum()\n    if w_sum == 0:                      # should not happen, but be safe\n        raise ValueError(\"All weights are zero.\")\n    \n    x_bar = np.dot(w, x_w) / w_sum\n    y_bar = np.dot(w, y_w) / w_sum\n    \n    denom = np.dot(w, (x_w - x_bar)**2)\n    if denom == 0:                      # slope undefined \u2192 use mean\n        b = 0.0\n        y_hat_norm = y_bar\n    else:\n        b = np.dot(w, (x_w - x_bar)*(y_w - y_bar)) / denom\n        a = y_bar - b * x_bar\n        y_hat_norm = a + b * xq_norm\n    \n    # --------------- 5. De-normalise result --------------\n    y_min, y_max = y.min(), y.max()\n    if y_max - y_min == 0:              # constant y\n        y_pred = y_min\n    else:\n        y_pred = y_hat_norm * (y_max - y_min) + y_min\n    \n    # --------------- 6. Round and return -----------------\n    return float(np.round(y_pred, 4))\n```"}
{"task_id": 587, "completion_id": 0, "completion": "```python\nimport random\nfrom typing import Any, List, Tuple\nimport random   # duplicate import is harmless and allowed by the task\n\ndef manage_replay_memory(operations: List[Tuple]) -> List[Any]:\n    \"\"\"Execute a series of replay-memory operations on an initially empty buffer.\n\n    The function collects the outputs of all commands that return a value\n    (\"pop\", \"sample\", and \"size\") and returns them in the order encountered.\n\n    Parameters\n    ----------\n    operations : List[Tuple]\n        Each tuple encodes an operation.  Its first element is one of the\n        strings \"append\", \"pop\", \"sample\", or \"size\"; remaining elements are\n        the arguments of that operation.\n\n    Returns\n    -------\n    List[Any]\n        Outputs from all operations that yield a result.\n    \"\"\"\n    # deterministic randomness for the whole function\n    random.seed(0)\n\n    buffer: List[Any] = []   # the replay memory itself\n    outputs: List[Any] = []  # collected results\n\n    for op in operations:\n        command = op[0]\n\n        if command == \"append\":\n            # ('append', transition)\n            transition = op[1]\n            buffer.append(transition)\n\n        elif command == \"pop\":\n            # ('pop',)\n            result = buffer.pop() if buffer else None\n            outputs.append(result)\n\n        elif command == \"sample\":\n            # ('sample', batch_size)\n            batch_size = op[1]\n            if batch_size > len(buffer):\n                outputs.append([])            # not enough data to sample\n            else:\n                sample = random.sample(buffer, batch_size)\n                outputs.append(sample)\n\n        elif command == \"size\":\n            # ('size',)\n            outputs.append(len(buffer))\n\n        else:\n            # For safety: ignore unknown commands or raise\n            raise ValueError(f\"Unknown command: {command}\")\n\n    return outputs\n```"}
{"task_id": 588, "completion_id": 0, "completion": "```python\nfrom typing import Optional\n\n\nclass Node:\n   A simple binary-tree node    def __init__(\n        self,\n        data=None,\n        split_variable=None,\n        split_variable_value=None,\n        left: Optional['Node'] = None,\n        right: Optional['Node'] = None,\n        depth: int = 0,\n        criterion_value=None\n    ):\n        self.data = data\n        self.split_variable = split_variable\n        self.split_variable_value = split_variable_value\n        self.left = left\n        self.right = right\n        self.depth = depth              # DO NOT trust or use this field\n        self.criterion_value = criterion_value\n\n\ndef tree_max_depth(root: Optional[Node]) -> int:\n   Return the maximum depth of the binary tree.\n\n    Depth of root      : 0\n    Depth of empty tree: -1\n       # Empty tree\n    if root is None:\n        return -1\n\n    # Depth-first recursion.\n    # For a `None` child we return \u22121 so that when we add 1\n    # the parent\u2019s depth is computed correctly (root becomes 0).\n    left_depth  = tree_max_depth(root.left)   # depth in left subtree\n    right_depth = tree_max_depth(root.right)  # depth in right subtree\n\n    return 1 + max(left_depth, right_depth)\n```"}
{"task_id": 590, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef minesweeper_step(ground: list[list[float | int]],\n                     flags : list[list[bool]]\n                     ) -> tuple[list[tuple[int, int]], list[tuple[int, int]]]:\n    \"\"\"Perform one logical deduction *pass* on a Minesweeper position.\n\n    Parameters\n    ----------\n    ground : 2-D list\n        Already revealed clues (0 \u2026 8); every still-hidden square is `np.nan`.\n    flags  : 2-D list\n        `True` exactly where the player has already put a flag, `False` elsewhere.\n\n    Returns\n    -------\n    clicks, new_flags : two *sorted* lists with unique `(row, col)` tuples\n        \u2022 clicks     \u2013 squares that can be safely revealed  \n        \u2022 new_flags  \u2013 squares that must be bombs according to the rules\n    \"\"\"\n    rows = len(ground)\n    cols = len(ground[0]) if rows else 0\n\n    # we collect coordinates in sets first to avoid duplicates\n    clicks:     set[tuple[int, int]] = set()\n    new_flags:  set[tuple[int, int]] = set()\n\n    # --- neighbourhood offsets ------------------------------------------------\n    neighbours = [(dr, dc)\n                  for dr in (-1, 0, 1)\n                  for dc in (-1, 0, 1)\n                  if not (dr == 0 and dc == 0)]\n    # --------------------------------------------------------------------------\n\n    for r in range(rows):\n        for c in range(cols):\n            value = ground[r][c]\n\n            # skip still hidden squares or (just in case) flagged ones\n            if np.isnan(value) or flags[r][c]:\n                continue\n\n            bombs      = 0                       # already flagged neighbours\n            unexplored = []                      # still-hidden & not flagged\n\n            # inspect the eight neighbours\n            for dr, dc in neighbours:\n                nr, nc = r + dr, c + dc\n                if 0 <= nr < rows and 0 <= nc < cols:\n                    if flags[nr][nc]:\n                        bombs += 1\n                    elif np.isnan(ground[nr][nc]):\n                        unexplored.append((nr, nc))\n\n            u_cnt = len(unexplored)              # number of *unexplored* squares\n            val   = int(value)                   # the clue itself (0 \u2026 8)\n\n            # ------------------------------------------------------------------\n            # apply the four simple rules described in the task\n            # ------------------------------------------------------------------\n            if val == 0:                         # rule 1\n                clicks.update(unexplored)\n\n            elif val == 8:                       # rule 2 (theoretical, but included)\n                new_flags.update(unexplored)\n\n            else:                                # rules 3 + 4\n                if bombs == val:                 # 3 \u2013 every other neighbour is safe\n                    clicks.update(unexplored)\n                elif u_cnt == val:               # 4 \u2013 every unexplored neighbour is a bomb\n                    new_flags.update(unexplored)\n\n    # convert to *sorted* lists\n    return (sorted(clicks), sorted(new_flags))\n```\nThe function\n\n1. scans every already-revealed (non-`np.nan`) square once,\n2. counts its flagged and still-hidden neighbours,\n3. applies the four deduction rules exactly as stated,\n4. gathers the resulting \u201ccertain\u201d clicks and new flags in sets, and finally\n5. returns the required sorted lists of unique coordinates."}
{"task_id": 591, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], \n                       x: list[float]) -> float | list[float]:\n   Forward pass through a fully-connected ReLU network with a linear output.\n    \n    Args\n    ----\n    weights : three-level nested list containing all network weights.\n              \u2022 weights[L] \u2026\u2026 list of neurons of layer L\n              \u2022 weights[L][i] \u2026 weights of neuron i\n              \u2022 weights[L][i][0] \u2026 bias weight of that neuron\n    x       : input feature vector (without the bias term)\n    \n    Returns\n    -------\n    Network prediction rounded to four decimals (float for a single output\n    neuron, otherwise list of floats).\n       \n    # Current layer activations (start with the input vector)\n    activations = np.asarray(x, dtype=float)\n    \n    n_layers = len(weights)\n    \n    for layer_idx, layer in enumerate(weights):\n        # Convert current layer weights to a NumPy array\n        w = np.asarray(layer, dtype=float)        # shape: (n_neurons, prev_size + 1)\n        \n        # Add bias term to the activations of previous layer\n        activations_with_bias = np.concatenate(([1.0], activations))\n        \n        # Linear combination (dot product)\n        z = w @ activations_with_bias            # shape: (n_neurons,)\n        \n        # Apply activation function\n        if layer_idx < n_layers - 1:             # hidden layer \u2192 ReLU\n            activations = np.maximum(0.0, z)\n        else:                                    # last layer \u2192 linear\n            activations = z\n    \n    # Round to 4 decimals\n    out = np.round(activations, 4)\n    \n    # Return Python scalars/lists, not NumPy types\n    if out.size == 1:\n        return float(out)                        # single output neuron\n    else:\n        return out.tolist()                      # multiple output neurons\n```"}
{"task_id": 592, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef analyze_cell(clicked: np.ndarray, flags: np.ndarray, row: int, col: int) -> tuple[list[tuple[int, int]], int]:\n   Return unopened neighbouring cells and flagged-mine count for a board position.\n\n    Args:\n        clicked: 2-D NumPy boolean array where True means the cell is already opened.\n        flags:   2-D NumPy boolean array \u2013 True means the cell is already marked as a mine.\n        row:     Row index of the cell to analyse.\n        col:     Column index of the cell to analyse.\n\n    Returns:\n        (unopened_neighbours, flagged_count)\n            unopened_neighbours \u2013 list with coordinates (row, col) of adjacent cells\n                                   that are still unopened.  Sorted ascending first\n                                   by row, then by column.\n            flagged_count       \u2013 how many adjacent cells are currently flagged.\n       # Board dimensions\n    n_rows, n_cols = clicked.shape\n\n    # All eight neighbour offsets\n    neighbour_offsets = (\n        (-1, -1), (-1, 0), (-1, 1),\n        ( 0, -1),          ( 0, 1),\n        ( 1, -1), ( 1, 0), ( 1, 1),\n    )\n\n    unopened: list[tuple[int, int]] = []\n    flagged_count = 0\n\n    for dr, dc in neighbour_offsets:\n        nr, nc = row + dr, col + dc\n\n        # Skip indices that fall outside the board\n        if nr < 0 or nc < 0 or nr >= n_rows or nc >= n_cols:\n            continue\n\n        # Count flags\n        if flags[nr, nc]:\n            flagged_count += 1\n\n        # Collect unopened neighbours\n        if not clicked[nr, nc]:\n            unopened.append((nr, nc))\n\n    # Return neighbours in ascending (row, col) order\n    unopened.sort()\n    return unopened, flagged_count\n```\n\nExplanation of the core steps\n1. Enumerate the eight relative neighbour positions.\n2. For each potential neighbour, verify that the indices stay inside the board.\n3. If the neighbour is flagged (`flags[nr, nc] is True`), increment `flagged_count`.\n4. If the neighbour is still unopened (`clicked[nr, nc] is False`), store its coordinates.\n5. Sort the collected coordinates lexicographically before returning.\n\nThe function automatically handles edge and corner cells (as well as the trivial 1\u00d71 board) by ignoring out-of-bounds indices."}
{"task_id": 593, "completion_id": 0, "completion": "```python\nfrom copy import deepcopy\n\n\ndef deduce_mines(constraints: list[list]) -> tuple[list[str], list[str]]:\n   Simplify Minesweeper constraints.\n\n    constraints \u2026 list of [variables, value] pairs, see task text for details\n    returns      \u2026 (safe, mines) \u2013 both alphabetically sorted lists\n       # -------------- helper: turn incoming data into useful structure ----------\n    # we only work with sets, duplicates inside a variable list are irrelevant\n    active = [(set(vars_), int(val)) for vars_, val in deepcopy(constraints)\n              if vars_]                                 # ignore empty clauses\n\n    safe, mines = set(), set()                          # knowledge base\n\n    # ----------------------------- main loop ---------------------------------\n    while True:\n        changed = False\n\n        # --- 1. solved constraints ------------------------------------------\n        for vars_, val in active:\n            if val == 0:                                # all safe\n                newly = vars_ - safe - mines\n                if newly:\n                    safe.update(newly)\n                    changed = True\n            elif val == len(vars_):                     # all mines\n                newly = vars_ - safe - mines\n                if newly:\n                    mines.update(newly)\n                    changed = True\n\n        # --- 2. propagation --------------------------------------------------\n        new_active = []\n        for vars_, val in active:\n            # remove already known variables\n            removed_safe  = vars_ & safe\n            removed_mines = vars_ & mines\n            if removed_safe or removed_mines:\n                changed = True\n\n            vars_ = vars_ - removed_safe - removed_mines\n            val  -= len(removed_mines)\n\n            if vars_:                     # keep only non-empty constraints\n                new_active.append((vars_, val))\n            else:                         # empty constraint is either solved (val==0)\n                pass                      # or inconsistent \u2013 ignored here\n        active = new_active\n\n        # --- 3. subset rule --------------------------------------------------\n        subset_changed = True\n        while subset_changed:\n            subset_changed = False\n            n = len(active)\n            for i in range(n):\n                Si, vi = active[i]\n                for j in range(n):\n                    if i == j:\n                        continue\n                    Sj, vj = active[j]\n\n                    if Si < Sj:                              # proper subset\n                        new_vars = Sj - Si\n                        new_val  = vj - vi\n                        if new_val < 0:                      # impossible, ignore\n                            continue\n                        if new_vars != Sj or new_val != vj:  # real change\n                            active[j] = (new_vars, new_val)\n                            subset_changed = changed = True\n\n            # after one sweep remove newly empty constraints\n            if subset_changed:\n                active = [(vars_, val) for vars_, val in active if vars_]\n\n        # --- 4. duplicate removal -------------------------------------------\n        unique = {}\n        for vars_, val in active:\n            key = (frozenset(vars_), val)\n            unique[key] = (vars_, val)       # keeps last, we don't care\n        if len(unique) != len(active):\n            changed = True\n        active = list(unique.values())\n\n        # --------------------------------------------------------------------\n        if not changed:                      # nothing new this round\n            break\n\n    return sorted(safe), sorted(mines)\n```"}
{"task_id": 594, "completion_id": 0, "completion": "```python\ndef maze_to_graph(maze: list[list[int]]) -> dict[tuple[int, int], list[tuple[int, int]]]:\n   Convert a binary maze into an adjacency-list graph.\n\n    A 0 in the maze denotes an open cell, while 1 denotes a wall.  \n    Two open cells are adjacent if they share an edge (up, down, left, right).\n\n    Args:\n        maze: Rectangular 2-D list containing only 0s and 1s.\n\n    Returns:\n        A dictionary whose keys are the coordinates (row, col) of every open\n        cell and whose values are *sorted* lists of orthogonally adjacent open\n        cells.  Walls do not appear in the dictionary.  If the maze contains no\n        open cells the function returns an empty dictionary.\n       # Handle completely empty mazes (e.g. [] or [[]])\n    if not maze or not maze[0]:\n        return {}\n\n    rows, cols = len(maze), len(maze[0])\n    graph: dict[tuple[int, int], list[tuple[int, int]]] = {}\n\n    # First pass: create an entry for every open cell.\n    for r in range(rows):\n        for c in range(cols):\n            if maze[r][c] == 0:\n                graph[(r, c)] = []        # initialise neighbour list\n\n    # Second pass: populate neighbour lists.\n    #               (Up, Down, Left, Right)\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    for (r, c) in graph:                 # iterate only over open cells\n        for dr, dc in directions:\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < rows and 0 <= nc < cols and maze[nr][nc] == 0:\n                graph[(r, c)].append((nr, nc))\n\n        # Ensure deterministic ordering of neighbours\n        graph[(r, c)].sort()\n\n    return graph\n```\n\nThe algorithm:\n\n1. Creates a dictionary entry for every walkable (0-valued) cell.  \n2. For each such cell, it checks its four orthogonal neighbours; if the neighbour\n   is within bounds and walkable, it is added to the cell\u2019s neighbour list.  \n3. Each neighbour list is sorted so the output is deterministic.  \n\nIf the maze has no open cells\u2014or the maze itself is empty\u2014an empty dictionary is returned."}
{"task_id": 595, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"\n    K-Means clustering (centroid computation only).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D data array of shape (n_samples, n_features).\n    k : int\n        Number of clusters (k <= n_samples is required).\n    max_iters : int, optional\n        Maximum number of iterations (default 100).\n\n    Returns\n    -------\n    list[list[float]]\n        Centroids sorted lexicographically and rounded to 4 decimals.\n    \"\"\"\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array.\")\n    n_samples, n_features = X.shape\n    if k < 1 or k > n_samples:\n        raise ValueError(\"k must be between 1 and the number of samples.\")\n\n    # 1. Deterministic initialisation with the first k samples\n    centroids = X[:k].astype(float).copy()  # shape (k, n_features)\n\n    for _ in range(max_iters):\n        # 2a. Assignment step \u2013 compute squared Euclidean distances\n        # distances: (n_samples, k)\n        diff = X[:, None, :] - centroids[None, :, :]\n        distances = np.sum(diff ** 2, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        # 2b. Update step \u2013 compute new centroids\n        new_centroids = centroids.copy()\n        for j in range(k):\n            mask = labels == j\n            if np.any(mask):                       # non-empty cluster\n                new_centroids[j] = X[mask].mean(axis=0)\n            # else: keep the old centroid unchanged (requirement 4)\n\n        # 3. Convergence test\n        shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n        centroids = new_centroids\n        if np.all(shifts < 1e-4):\n            break\n\n    # 5. Sort lexicographically and round to 4 decimals\n    centroids = np.round(centroids, 4)\n    sorted_centroids = sorted(centroids.tolist())\n\n    return sorted_centroids\n```"}
{"task_id": 596, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef convert_maze_to_graph(maze: np.ndarray) -> dict:\n   Convert a binary maze to an adjacency-list graph.\n\n    Each cell whose value is 1 becomes a vertex.  Two vertices are connected\n    iff their cells are orthogonally adjacent (up, down, left, or right).\n\n    The returned dictionary maps every free cell (row, col) to a *sorted*\n    list of all adjacent free neighbours.  If the maze contains no free\n    cells, an empty dictionary is returned.\n       # Make sure we really have a 2-D array\n    if maze.ndim != 2:\n        raise ValueErrormaze must be a 2-D NumPy array    rows, cols = maze.shape\n\n    # All coordinates whose cell contains a 1\n    free_rows, free_cols = np.where(maze == 1)\n    free_coords = list(zip(free_rows, free_cols))\n\n    # No free blocks \u21d2 empty graph\n    if not free_coords:\n        return {}\n\n    # Build adjacency information in sets first (avoids duplicates)\n    adj = {coord: set() for coord in free_coords}\n\n    # Relative moves: up, down, left, right\n    moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n    for r, c in free_coords:\n        for dr, dc in moves:\n            nr, nc = r + dr, c + dc\n            # Check bounds and whether the neighbour is free\n            if 0 <= nr < rows and 0 <= nc < cols and maze[nr, nc] == 1:\n                adj[(r, c)].add((nr, nc))\n\n    # Convert neighbour sets to *sorted* lists\n    for key in adj:\n        adj[key] = sorted(adj[key])\n\n    return adj\n```\n\nThe algorithm:\n\n1. Locate every cell whose value is `1`; each such coordinate becomes a vertex.\n2. For every vertex, inspect its four orthogonal neighbours.  \n   If a neighbour is within the maze and also free (`1`), record the connection.\n3. Store neighbours in a set to avoid duplicates, then transform each\n   neighbour set into a lexicographically sorted list.\n4. Return the dictionary.  If the maze has no free cells, return `{}`.\n\nThe resulting dictionary is an undirected, un-weighted adjacency list that\nconforms to all the requirements."}
{"task_id": 597, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport numpy as np      # (the second import does no harm, it fulfils the stub)\n\ndef best_split_mse(X: list | np.ndarray, y: list | np.ndarray) -> list:\n   Return the best (feature, threshold) split for a regression tree.\n\n    A split is evaluated with the weighted mean\u2013squared error (population\n    variance) of the two resulting groups.  Only thresholds that coincide\n    with an existing feature value are examined.\n\n    The result is a list [best_feature_index, best_threshold, best_mse],\n    where best_mse is rounded to 4 decimals.\n       # ---- preparation -----------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y,  dtype=float).ravel()\n    n_samples, n_features = X.shape\n\n    if y.shape[0] != n_samples:\n        raise ValueErrorX and y must contain the same number of samples    # overall length-independent constants for later reuse\n    FULL_N      = float(n_samples)\n    FULL_SUM    = y.sum()\n    FULL_SQ_SUM = np.dot(y, y)          # equivalent to (y ** 2).sum()\n\n    best_feat   = None\n    best_thr    = None\n    best_score  = np.inf                # minimal weighted MSE so far\n    EPS         = 1e-12                 # tolerance for equality when breaking ties\n\n    # ---- scan every feature ----------------------------------------------\n    for j in range(n_features):\n        col = X[:, j]\n\n        # If all values are identical, no valid split exists for this feature\n        if np.all(col == col[0]):\n            continue\n\n        # Order the samples by feature value \u2013 required for the\n        # left-to-right \u201csweep\u201d that evaluates every candidate threshold\n        order          = np.argsort(col, kind='mergesort')  # stable -> easier tie-handling\n        sorted_vals    = col[order]\n        sorted_targets = y[order]\n\n        # Initialise \u201cright\u201d side with the whole data set\n        right_cnt      = n_samples\n        right_sum      = FULL_SUM\n        right_sq_sum   = FULL_SQ_SUM\n\n        left_cnt       = 0\n        left_sum       = 0.0\n        left_sq_sum    = 0.0\n\n        idx = 0\n        while idx < n_samples:\n            value = sorted_vals[idx]\n\n            # ---- move *all* observations having this feature value\n            #      from right to left in one go -----------------------------\n            while idx < n_samples and sorted_vals[idx] == value:\n                t = sorted_targets[idx]\n                left_cnt   += 1\n                left_sum   += t\n                left_sq_sum+= t * t\n\n                right_cnt   -= 1\n                right_sum   -= t\n                right_sq_sum-= t * t\n                idx += 1\n\n            # after moving, current 'value' is a legitimate threshold\n            # only if both sides are non-empty\n            if left_cnt == 0 or right_cnt == 0:\n                continue\n\n            # population variance  Var = E[y\u00b2] \u2212 (E[y])\u00b2\n            l_mean = left_sum / left_cnt\n            r_mean = right_sum / right_cnt\n\n            l_var  = max(left_sq_sum  / left_cnt  - l_mean * l_mean, 0.0)\n            r_var  = max(right_sq_sum / right_cnt - r_mean * r_mean, 0.0)\n\n            mse = (left_cnt / FULL_N) * l_var + (right_cnt / FULL_N) * r_var\n\n            # ---- update optimum, honouring tie-breaking rules -------------\n            if (mse + EPS) < best_score:              # strictly smaller\n                best_score = mse\n                best_feat  = j\n                best_thr   = value\n            elif abs(mse - best_score) <= EPS:        # equal \u2013 apply tie rules\n                if j < best_feat or (j == best_feat and value < best_thr):\n                    best_feat = j\n                    best_thr  = value\n                    # best_score unchanged\n\n    # The task guarantees that at least one valid split exists\n    return [best_feat, best_thr, round(best_score, 4)]\n```"}
{"task_id": 598, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef standardize(data: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"Standardise a numeric data matrix feature-wise (z-score).\n\n    Args:\n        data: List of shape (n_samples, n_features) containing numbers.\n\n    Returns:\n        List[list[float]]: Standardised matrix, every entry rounded to 4 decimals.\n    \"\"\"\n    # Empty input \u2500 nothing to do\n    if not data:\n        return []\n\n    # Convert to 2-D float array\n    arr = np.asarray(data, dtype=float)\n    if arr.ndim == 1:                       # 1-D -> treat as single feature\n        arr = arr.reshape(-1, 1)\n\n    # Means and (population) standard deviations per feature (column)\n    means = arr.mean(axis=0)\n    stds  = arr.std(axis=0, ddof=0)\n\n    # Avoid division by zero: temporarily replace zeros by 1,\n    # afterwards overwrite those columns with zeros.\n    safe_stds = np.where(stds == 0, 1.0, stds)\n\n    z = (arr - means) / safe_stds\n\n    # Columns that were constant (std == 0) must be filled with zeros\n    if np.any(stds == 0):\n        z[:, stds == 0] = 0.0\n\n    # Replace any NaN/Inf produced during the computation with 0\n    z = np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Round to 4 decimals and return as regular Python list\n    return np.round(z, 4).tolist()\n```"}
