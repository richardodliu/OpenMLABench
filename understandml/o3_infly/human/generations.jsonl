{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Compute the dot-product of a matrix `a` (list of rows) with a vector `b`.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Matrix represented as a list of row lists.\n    b : list[int | float]\n        Vector represented as a flat list.\n\n    Returns\n    -------\n    list[int | float]\n        Resulting vector (list of dot-products), one entry per row in `a`.\n    -1\n        If the number of columns in `a` does not match `len(b)` or if `a` is empty.\n    \"\"\"\n    # Basic validation --------------------------------------------------------\n    if not a:                             # empty matrix\n        return -1\n    n_cols = len(b)\n    if n_cols == 0:                       # empty vector\n        return -1\n\n    # All rows must have the same length equal to len(b)\n    for row in a:\n        if len(row) != n_cols:\n            return -1\n\n    # Dot product for each row -----------------------------------------------\n    result: list[int | float] = [\n        sum(x * y for x, y in zip(row, b)) for row in a\n    ]\n    return result\n```\n\nExample\n-------\n```python\nA = [[1, 2, 3],\n     [4, 5, 6]]\n\nv = [7, 8, 9]\n\nprint(matrix_dot_vector(A, v))   # [ 1*7 + 2*8 + 3*9 = 50, 4*7 + 5*8 + 6*9 = 122 ]\n```\n\nOutput:\n```\n[50, 122]\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Return the transpose of a matrix.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        A rectangular (m \u00d7 n) matrix represented as a list of `m` rows,\n        where each row is a list of `n` numeric values.\n\n    Returns\n    -------\n    list[list[int | float]]\n        The transposed (n \u00d7 m) matrix.\n\n    Raises\n    ------\n    ValueError\n        If the input matrix is not rectangular (i.e., rows have differing lengths).\n    \"\"\"\n    # An empty matrix stays empty after transposing\n    if not a:\n        return []\n\n    # Verify rectangular shape\n    row_len = len(a[0])\n    for row in a:\n        if len(row) != row_len:\n            raise ValueError(\"All rows must have the same length to form a valid matrix.\")\n\n    # Transpose using a nested list-comprehension\n    return [[row[col] for row in a] for col in range(row_len)]\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a 2-D Python list `a` into the dimensions specified by `new_shape`.\n    If the total number of elements does not match, an empty list is returned.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Original matrix (nested list).\n    new_shape : tuple[int, int]\n        Desired shape as (rows, cols).\n\n    Returns\n    -------\n    list[list[int | float]]\n        Reshaped matrix as a nested list, or [] if reshape is impossible.\n    \"\"\"\n    # Basic validation of the requested shape\n    if (\n        not isinstance(new_shape, tuple)\n        or len(new_shape) != 2\n        or any(not isinstance(d, int) or d <= 0 for d in new_shape)\n    ):\n        return []\n\n    try:\n        arr = np.asarray(a)\n    except Exception:\n        # If the input cannot be converted to a NumPy array, return empty list\n        return []\n\n    # Ensure we know how many elements there are\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n\n    try:\n        reshaped = arr.reshape(new_shape)\n        return reshaped.tolist()\n    except Exception:\n        # Catch any NumPy reshape errors (e.g., if input isn't contiguous)\n        return []\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n\n    Parameters\n    ----------\n    matrix : list[list[float]]\n        A rectangular matrix (i.e., every row has the same length).\n    mode : str\n        Either \"row\" or \"column\" (case\u2013insensitive). \n        \"row\"    -> return the mean of each row\n        \"column\" -> return the mean of each column\n\n    Returns\n    -------\n    list[float]\n        A list containing the means, in the order of rows or columns.\n\n    Raises\n    ------\n    ValueError\n        * If matrix is empty, not rectangular, or contains empty rows.\n        * If mode is neither \"row\" nor \"column\".\n    \"\"\"\n    # ---------- Basic validations ----------\n    if not matrix:\n        raise ValueError(\"The matrix must contain at least one row.\")\n    if any(len(row) == 0 for row in matrix):\n        raise ValueError(\"Rows must not be empty.\")\n\n    row_lengths = {len(row) for row in matrix}\n    if len(row_lengths) != 1:\n        raise ValueError(\"All rows must have the same length.\")\n    n_rows = len(matrix)\n    n_cols = row_lengths.pop()\n\n    mode = mode.lower()\n    if mode not in {\"row\", \"column\"}:\n        raise ValueError(\"mode must be either 'row' or 'column'.\")\n\n    # ---------- Mean by row ----------\n    if mode == \"row\":\n        return [sum(row)/n_cols for row in matrix]\n\n    # ---------- Mean by column ----------\n    # Option 1: manual summation\n    column_sums = [0.0] * n_cols\n    for row in matrix:\n        for j, val in enumerate(row):\n            column_sums[j] += val\n    return [s / n_rows for s in column_sums]\n\n    # (Alternatively, you could compute columns with zip(*matrix)):\n    # return [sum(col)/n_rows for col in zip(*matrix)]\n```\n\nExample usage:\n\n```python\nA = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(calculate_matrix_mean(A, \"row\"))    # [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(A, \"column\")) # [4.0, 5.0, 6.0]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int | float]], \n                    scalar: int | float) -> list[list[int | float]]:\n       Multiply every element of `matrix` by `scalar` and return the resulting matrix.\n    \n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A 2-D list representing the matrix to be scaled.\n    scalar : int | float\n        The value by which to multiply each element of the matrix.\n        \n    Returns\n    -------\n    list[list[int | float]]\n        A new matrix whose elements are the products `element * scalar`.\n    \n    Notes\n    -----\n    \u2022 The original matrix is not modified; a new (deep-copied) matrix is returned.  \n    \u2022 The function is agnostic to row lengths, so \u201cragged\u201d matrices are handled\n      element-wise without raising an error.\n       # Validate the `matrix` argument is actually a list of lists\n    if not all(isinstance(row, list) for row in matrix):\n        raise TypeError`matrix` must be a list of lists    # Use a nested list-comprehension to build the scaled matrix\n    return [[value * scalar for value in row] for row in matrix]\n```\n\nExample usage\n```python\n>>> A = [[1, 2, 3],\n...      [4, 5, 6]]\n\n>>> scalar_multiply(A, 3)\n[[3, 6, 9], [12, 15, 18]]\n\n# Original matrix remains unchanged\n>>> A\n[[1, 2, 3], [4, 5, 6]]\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List\n\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2\u00d72 matrix.\n\n    Parameters\n    ----------\n    matrix : list[list[float | int]]\n        A 2\u00d72 matrix represented as a nested list:\n        [[a, b],\n         [c, d]]\n\n    Returns\n    -------\n    list[float]\n        A list containing the two eigenvalues, sorted from highest\n        to lowest.\n\n    Raises\n    ------\n    ValueError\n        If `matrix` is not 2\u00d72 or discriminant is negative (complex eigenvalues).\n    \"\"\"\n    # ---- Validation ----\n    if len(matrix) != 2 or any(len(row) != 2 for row in matrix):\n        raise ValueError(\"Input must be a 2\u00d72 matrix.\")\n\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # ---- Characteristic polynomial coefficients ----\n    # \u03bb\u00b2 - (a + d)\u03bb + (ad \u2212 bc) = 0\n    trace = a + d\n    determinant = a * d - b * c\n\n    # ---- Discriminant ----\n    discriminant = trace ** 2 - 4 * determinant\n    if discriminant < 0:\n        raise ValueError(\"The eigenvalues are complex; this function handles only real eigenvalues.\")\n\n    sqrt_discriminant = math.sqrt(discriminant)\n\n    # ---- Eigenvalues ----\n    \u03bb1 = (trace + sqrt_discriminant) / 2\n    \u03bb2 = (trace - sqrt_discriminant) / 2\n\n    # ---- Sort from highest to lowest and return ----\n    return sorted([\u03bb1, \u03bb2], reverse=True)\n```\n\nExample usage:\n```python\nmatrix = [[4, 2],\n          [1, 3]]\n\nprint(calculate_eigenvalues(matrix))  # -> [5.0, 2.0]\n```\n\nThe function first validates the input as a 2\u00d72 matrix, derives the characteristic polynomial coefficients, computes the discriminant, and, assuming it yields real roots, returns the eigenvalues in descending order."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int | float]],\n                     T: list[list[int | float]],\n                     S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Returns the matrix  T^{-1} \u00b7 A \u00b7 S  rounded to 4 decimal places.\n    \n    A   : n \u00d7 m  matrix\n    T   : n \u00d7 n  invertible matrix\n    S   : m \u00d7 m  invertible matrix\n    \n    On any failure (non-square, non-invertible, or incompatible sizes)\n    the function returns -1.\n    \"\"\"\n    try:\n        # --- Convert to numpy arrays ---------------------------------------\n        A_mat = np.array(A, dtype=float)\n        T_mat = np.array(T, dtype=float)\n        S_mat = np.array(S, dtype=float)\n\n        # --- Basic dimensional checks --------------------------------------\n        if A_mat.ndim != 2 or T_mat.ndim != 2 or S_mat.ndim != 2:\n            return -1\n\n        n, n2 = T_mat.shape\n        m, m2 = S_mat.shape\n        if n != n2 or m != m2:               # T or S not square\n            return -1\n        if A_mat.shape != (n, m):            # A not conformable\n            return -1\n\n        # --- Invertibility checks ------------------------------------------\n        # np.linalg.inv raises LinAlgError if the matrix is singular\n        T_inv = np.linalg.inv(T_mat)         # Will raise if singular / not square\n        _      = np.linalg.inv(S_mat)        # Only to verify invertibility\n\n        # --- Core computation ----------------------------------------------\n        result = T_inv @ A_mat @ S_mat\n\n        # --- Rounding and return -------------------------------------------\n        return np.round(result, 4).tolist()\n\n    except Exception:\n        # Covers LinAlgError, ValueError, etc.\n        return -1\n```"}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n       Calculate the inverse of a 2\u00d72 matrix.\n\n    Parameters\n    ----------\n    matrix : list[list[float]]\n        A 2\u00d72 matrix represented as a list of two lists, each containing two numerical (int/float) values.\n\n    Returns\n    -------\n    list[list[float]] | None\n        The inverse of the matrix as a new 2\u00d72 list-of-lists if it is invertible,\n        otherwise `None` (e.g., when the determinant is 0 or the input is not 2\u00d72).\n       # Basic shape validation\n    if (\n        not isinstance(matrix, list) or len(matrix) != 2\n        or any(not isinstance(row, list) or len(row) != 2 for row in matrix)\n    ):\n        return None\n\n    # Unpack the matrix elements\n    a, b = matrix[0]\n    c, d = matrix[1]\n\n    # Compute the determinant\n    det = a * d - b * c\n\n    # The matrix is not invertible if the determinant is zero (or extremely close)\n    if abs(det) < 1e-12:  # tolerance to handle floating-point noise\n        return None\n\n    # Compute the inverse using the closed-form formula\n    inv_det = 1.0 / det\n    return [\n        [ d * inv_det, -b * inv_det],\n        [-c * inv_det,  a * inv_det],\n    ]\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]],\n              b: list[list[int | float]]) -> list[list[int | float]] | int:\n       Multiply two matrices `a` and `b`.\n\n    Parameters\n    ----------\n    a : list[list[int | float]]\n        Left-hand matrix.\n    b : list[list[int | float]]\n        Right-hand matrix.\n\n    Returns\n    -------\n    list[list[int | float]]\n        The product matrix.\n    -1\n        Returned when the matrices have incompatible sizes or are not\n        proper rectangular matrices.\n       # ---------- Helper: validate \u201crectangular\u201d shape ----------\n    def is_rect(mat: list[list[int | float]]) -> bool:\n       All rows have the same length and matrix is non-empty        if not mat or not mat[0]:\n            return False\n        row_len = len(mat[0])\n        return all(len(row) == row_len for row in mat)\n\n    # ---------- Validation ----------\n    if not (is_rect(a) and is_rect(b)):\n        return -1\n\n    n_rows_a, n_cols_a = len(a), len(a[0])\n    n_rows_b, n_cols_b = len(b), len(b[0])\n\n    # Inner dimensions must match\n    if n_cols_a != n_rows_b:\n        return -1\n\n    # ---------- Core multiplication ----------\n    # Pre-allocate result matrix filled with zeros\n    result = [[0 for _ in range(n_cols_b)] for _ in range(n_rows_a)]\n\n    for i in range(n_rows_a):\n        for k in range(n_cols_a):          # iterate over shared dimension once\n            aik = a[i][k]                  # store to avoid repeated indexing\n            for j in range(n_cols_b):\n                result[i][j] += aik * b[k][j]\n\n    return result\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nfrom typing import List\n\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Return the *sample* covariance matrix of a sequence of feature-vectors.\n\n    Parameters\n    ----------\n    vectors : List[List[float]]\n        vectors[i][k] is the k-th observation of the i-th feature.\n        All inner lists must have identical length (\u2259 number of observations).\n\n    Returns\n    -------\n    List[List[float]]\n        A symmetric, square matrix where element (i, j) is the covariance\n        between feature i and feature j, normalised by (N \u2212 1).\n\n    Raises\n    ------\n    ValueError\n        * if `vectors` is empty,\n        * if the inner lists have different lengths,\n        * or if any inner list is empty.\n    \"\"\"\n    # ----------------- sanity checks -----------------\n    if not vectors:\n        raise ValueError(\"`vectors` must contain at least one feature.\")\n\n    n_obs = len(vectors[0])\n    if n_obs == 0:\n        raise ValueError(\"Each feature must contain at least one observation.\")\n    if any(len(v) != n_obs for v in vectors):\n        raise ValueError(\"All features must have the same number of observations.\")\n\n    n_features = len(vectors)\n\n    # ----------------- means -----------------\n    means = [sum(v) / n_obs for v in vectors]\n\n    # Divisor for *sample* covariance; fall back to 1 when N == 1\n    divisor = n_obs - 1 if n_obs > 1 else 1\n\n    # ----------------- covariance -----------------\n    cov_matrix: List[List[float]] = [[0.0] * n_features for _ in range(n_features)]\n\n    # Exploit symmetry: compute only upper triangle then mirror\n    for i in range(n_features):\n        for j in range(i, n_features):\n            cov_ij = 0.0\n            for k in range(n_obs):\n                cov_ij += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov_ij /= divisor\n            cov_matrix[i][j] = cov_ij\n            cov_matrix[j][i] = cov_ij    # symmetry\n\n    return cov_matrix\n\n\n# ---------------------------------------------------------------------\n#                         ----  TEST CASES ----\n# ---------------------------------------------------------------------\ndef _almost_equal(m1, m2, tol=1e-12):\n    \"\"\"Utility function for floating-point matrix equality.\"\"\"\n    return all(\n        abs(a - b) < tol\n        for row1, row2 in zip(m1, m2)\n        for a, b in zip(row1, row2)\n    )\n\n\ndef _run_tests() -> None:\n    # 1. Two identical features  ->  perfect positive correlation\n    data = [[1, 2, 3], [1, 2, 3]]\n    expected = [[1.0, 1.0],\n                [1.0, 1.0]]\n    assert _almost_equal(calculate_covariance_matrix(data), expected)\n\n    # 2. Perfect negative relationship\n    data = [[1, 2, 3], [3, 2, 1]]\n    expected = [[1.0, -1.0],\n                [-1.0, 1.0]]\n    assert _almost_equal(calculate_covariance_matrix(data), expected)\n\n    # 3. Three features, four observations  (values pre-computed with NumPy)\n    data = [[1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [10, 9, 8, 7]]\n    expected = [[ 1.6666666666666667,  1.6666666666666667, -1.6666666666666667],\n                [ 1.6666666666666667,  1.6666666666666667, -1.6666666666666667],\n                [-1.6666666666666667, -1.6666666666666667,  1.6666666666666667]]\n    assert _almost_equal(calculate_covariance_matrix(data), expected)\n\n    # 4. Only one observation per feature  ->  zero covariance by convention\n    data = [[10], [20]]\n    expected = [[0.0, 0.0],\n                [0.0, 0.0]]\n    assert calculate_covariance_matrix(data) == expected\n\n    print(\"All tests passed \u2714\")\n\n\nif __name__ == \"__main__\":\n    _run_tests()\n```"}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve the linear system Ax = b by the Jacobi iterative method.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Square coefficient matrix (m \u00d7 m).\n    b : np.ndarray\n        Right\u2013hand-side vector (length m or m \u00d7 1).\n    n : int\n        Number of Jacobi iterations to perform.\n\n    Returns\n    -------\n    list\n        Approximate solution vector rounded to four decimals,\n        returned as a regular Python list.\n    \"\"\"\n    # Ensure NumPy arrays of type float\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n\n    m, k = A.shape\n    if m != k:\n        raise ValueError(\"Matrix A must be square.\")\n    if b.size != m:\n        raise ValueError(\"Vector b must have the same length as A's dimension.\")\n\n    # Initial guess: x^(0) = 0\n    x = np.zeros_like(b)\n\n    # Diagonal elements and remainder matrix\n    D = np.diag(A)\n    if np.any(D == 0):\n        raise ZeroDivisionError(\"Jacobi method fails: zero found on diagonal of A.\")\n    R = A - np.diagflat(D)          # Everything except the diagonal\n\n    # Jacobi iterations\n    for _ in range(n):\n        x = (b - R @ x) / D         # Core Jacobi formula\n        x = np.round(x, 4)          # Round **each** intermediate vector\n\n    return x.reshape(-1).tolist()\n```\n\nThe routine:\n\n1. Converts all inputs to floating-point NumPy arrays and performs basic dimension checks.  \n2. Starts from the zero vector `x\u207d\u2070\u207e`.  \n3. Splits `A` into its diagonal part `D` and the remainder `R`.  \n4. Repeats the Jacobi update `n` times, rounding every intermediate solution to four decimal places.  \n5. Returns the final vector as a Python list via `.tolist()`, satisfying the output constraints."}
{"task_id": 12, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates (actually computes in one sweep) the singular values of a 2\u00d72\n    matrix with a single Jacobi rotation applied to A\u1d40A.  No call to\n    numpy.linalg.svd is used.\n\n    Parameters\n    ----------\n    A : np.ndarray\n        Real 2\u00d72 matrix.\n\n    Returns\n    -------\n    tuple\n        The two singular values \u03c3\u2081 \u2265 \u03c3\u2082, each rounded to 4 decimal places.\n    \"\"\"\n    # ------------------------------------------------------------------ #\n    # 0. Sanity check\n    # ------------------------------------------------------------------ #\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix has to be 2\u00d72.\")\n\n    # ------------------------------------------------------------------ #\n    # 1. Build the symmetric matrix S = A\u1d40A whose eigenvalues are \u03c3\u00b2\n    # ------------------------------------------------------------------ #\n    S = A.T @ A\n    a, b = S[0, 0], S[0, 1]      # S is symmetric \u21d2 S[1,0] = S[0,1] = b\n    d = S[1, 1]\n\n    # ------------------------------------------------------------------ #\n    # 2. One-step Jacobi diagonalisation of the 2\u00d72 symmetric matrix S\n    # ------------------------------------------------------------------ #\n    if abs(b) < 1e-12:           # Already (almost) diagonal\n        \u03bb1, \u03bb2 = a, d\n    else:\n        # Jacobi rotation parameters\n        tau = (d - a) / (2.0 * b)\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau * tau))\n        c = 1.0 / np.sqrt(1.0 + t * t)\n        s = c * t\n\n        # Rotated (diagonal) entries \u2013 the eigenvalues of S\n        \u03bb1 = c * c * a - 2.0 * s * c * b + s * s * d\n        \u03bb2 = s * s * a + 2.0 * s * c * b + c * c * d\n\n    # Numerical safety \u2013 tiny negative values \u2192 0\n    \u03bb1, \u03bb2 = max(\u03bb1, 0.0), max(\u03bb2, 0.0)\n\n    # ------------------------------------------------------------------ #\n    # 3. Singular values are the square\u2013roots of the eigenvalues\n    # ------------------------------------------------------------------ #\n    \u03c31, \u03c32 = np.sqrt(\u03bb1), np.sqrt(\u03bb2)\n\n    # Sort in the usual \u03c3\u2081 \u2265 \u03c3\u2082 order and round to 4 decimals\n    singular_values = sorted((\u03c31, \u03c32), reverse=True)\n    singular_values = tuple(round(float(\u03c3), 4) for \u03c3 in singular_values)\n\n    return singular_values\n```\n\nExample usage\n--------------\n```python\nA = np.array([[3., 1.],\n              [0., 2.]])\n\nprint(svd_2x2_singular_values(A))\n# e.g. \u2192 (3.1623, 1.8974)\n\n# Compare with NumPy (for peace of mind):\n# import numpy as np\n# print(np.round(np.linalg.svd(A, compute_uv=False), 4))\n```\n\nThe routine performs exactly one Jacobi rotation\u2014enough to fully\ndiagonalise a 2 \u00d7 2 symmetric matrix\u2014then takes the square roots of the\neigenvalues to obtain the singular values, which are returned rounded to\nthe nearest 4th decimal place."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n       Calculate the determinant of a 4\u00d74 matrix using Laplace's expansion.\n\n    Parameters\n    ----------\n    matrix : list[list[int | float]]\n        A 4\u00d74 matrix represented as a list of four lists, each containing four\n        numeric (int or float) entries.\n\n    Returns\n    -------\n    float\n        The determinant of the matrix.\n\n    Raises\n    ------\n    ValueError\n        If the input is not a 4\u00d74 matrix.\n       # ---------------------------------------------------------------------\n    # Basic validation \u2013 make sure we *really* have a 4\u00d74 matrix\n    # ---------------------------------------------------------------------\n    if (\n        len(matrix) != 4\n        or any(len(row) != 4 for row in matrix)\n    ):\n        raise ValueErrorInput must be a 4\u00d74 matrix (list of four lists, each of length four).    # ---------------------------------------------------------------------\n    # Helper: determinant of an arbitrary n\u00d7n matrix by Laplace expansion\n    # ---------------------------------------------------------------------\n    def _det(mat: list[list[int | float]]) -> float:\n        n = len(mat)\n\n        # Base cases\n        if n == 1:                           # 1\u00d71\n            return mat[0][0]\n        if n == 2:                           # 2\u00d72\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n\n        # Recursive Laplace expansion along the first row\n        det_val = 0.0\n        for col in range(n):\n            # Build the (n-1)\u00d7(n-1) minor that results from\n            # removing row 0 and column `col`\n            minor = [row[:col] + row[col + 1:] for row in mat[1:]]\n\n            # Cofactor sign: + - + - ...  (i+j parity, i=0 here)\n            sign = -1 if col % 2 else 1\n\n            # Accumulate cofactor * element\n            det_val += sign * mat[0][col] * _det(minor)\n\n        return det_val\n\n    # ---------------------------------------------------------------------\n    # Invoke recursion on the validated 4\u00d74 matrix\n    # ---------------------------------------------------------------------\n    return _det(matrix)\n\n\n# -------------------------------------------------------------------------\n# Example usage\n# -------------------------------------------------------------------------\nif __name__ ==main    A = [\n        [3, 2, -1, 4],\n        [2, 1,  5, 7],\n        [0, 3, -2, 6],\n        [1, 4,  2, 0]\n    ]\n    printdet(A) determinant_4x4(A))   # >>> det(A) = -240.0\n```\n\nHow it works\n1. The helper `_det` handles determinants of any size (1\u00d71, 2\u00d72, 3\u00d73, \u2026).  \n2. For an n > 2 matrix it:\n   \u2022 Removes the first row and each column in turn to create each minor.  \n   \u2022 Recursively computes each minor\u2019s determinant.  \n   \u2022 Applies the alternating sign (cofactor) pattern.  \n3. The outer function only validates that its input is 4\u00d74 and returns `_det`\u2019s result."}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Computes linear\u2013regression coefficients using the normal equation.\n    \n    Parameters\n    ----------\n    X : list[list[float]]\n        Feature matrix \u2013 each sub-list is a data sample.\n    y : list[float]\n        Target values.\n    \n    Returns\n    -------\n    list[float]\n        Coefficients (intercept first) rounded to 4 decimal places.\n    \"\"\"\n    # Convert to numpy arrays\n    X_np = np.asarray(X, dtype=float)\n    y_np = np.asarray(y, dtype=float).reshape(-1, 1)   # column vector\n    \n    # Add bias (intercept) term\n    ones = np.ones((X_np.shape[0], 1))\n    X_b = np.hstack((ones, X_np))\n    \n    # Normal equation: \u03b8 = (X\u1d40 X)\u207b\u00b9 X\u1d40 y\n    theta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y_np   # use pinv for numerical stability\n    theta = theta.flatten()\n    \n    # Round to 4 decimal places\n    return [round(float(v), 4) for v in theta]\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray,\n                                       y: np.ndarray,\n                                       alpha: float,\n                                       iterations: int) -> np.ndarray:\n       Perform ordinary\u2010least\u2010squares linear regression by batch gradient descent.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n). A column of ones for the intercept\n        term must already be included.\n    y : np.ndarray\n        Target values of shape (m,) or (m, 1).\n    alpha : float\n        Learning-rate (step size).\n    iterations : int\n        Number of gradient-descent iterations.\n\n    Returns\n    -------\n    list\n        Model coefficients (including the intercept as the first value),\n        rounded to four decimal places.\n       # Ensure y is a 1-D array of shape (m,)\n    y = y.reshape(-1)\n\n    m, n = X.shape\n    theta = np.zeros(n)                       # initialise coefficients\n\n    for _ in range(iterations):\n        predictions = X @ theta               # hypothesis h\u03b8(x)\n        errors = predictions - y              # residuals\n        gradient = (1 / m) * (X.T @ errors)   # cost gradient\n        theta -= alpha * gradient             # parameter update\n\n    # Round to 4 decimal places and return as a Python list\n    theta = np.round(theta, 4)\n\n    return theta.tolist()\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n       Perform feature scaling on the given 2-D NumPy array:\n        1. Standardization: (x - \u03bc) / \u03c3\n        2. Min-Max normalization: (x - min) / (max - min)\n\n    All results are rounded to 4 decimal places and returned as Python lists.\n       # Ensure floating point for safe division\n    data = np.asarray(data, dtype=float)\n\n    # ---------- Standardization ----------\n    means = data.mean(axis=0)\n    stds  = data.std(axis=0)          # population std (ddof=0)\n\n    # Prevent division by zero for constant columns\n    stds_safe = np.where(stds == 0, 1.0, stds)\n    standardized = (data - means) / stds_safe\n    standardized = np.where(stds == 0, 0.0, standardized)   # constant cols become 0\n\n    # ---------- Min-Max Normalization ----------\n    mins  = data.min(axis=0)\n    maxs  = data.max(axis=0)\n    ranges = maxs - mins\n\n    # Prevent division by zero for constant columns\n    ranges_safe = np.where(ranges == 0, 1.0, ranges)\n    minmax = (data - mins) / ranges_safe\n    minmax = np.where(ranges == 0, 0.0, minmax)             # constant cols become 0\n\n    # ---------- Rounding ----------\n    standardized = np.round(standardized, 4)\n    minmax       = np.round(minmax, 4)\n\n    # ---------- Return as Python lists ----------\n    return standardized.tolist(), minmax.tolist()\n```"}
{"task_id": 17, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Tuple\n\n\ndef k_means_clustering(\n    points: List[Tuple[float, ...]],\n    k: int,\n    initial_centroids: List[Tuple[float, ...]],\n    max_iterations: int,\n) -> List[Tuple[float, ...]]:\n    \"\"\"\n    k-Means clustering.\n\n    Parameters\n    ----------\n    points : list[tuple[float, ...]]\n        Data points to cluster (all points must have the same dimensionality).\n    k : int\n        Number of clusters.\n    initial_centroids : list[tuple[float, ...]]\n        Initial positions of the centroids (must contain exactly `k` elements).\n    max_iterations : int\n        Maximum number of iterations to run the algorithm.\n\n    Returns\n    -------\n    list[tuple[float, ...]]\n        Final centroids, with each coordinate rounded to 4 decimal places.\n    \"\"\"\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n    if k != len(initial_centroids):\n        raise ValueError(\"Number of initial_centroids must equal k.\")\n    if not points:\n        raise ValueError(\"points list cannot be empty.\")\n    if max_iterations <= 0:\n        raise ValueError(\"max_iterations must be a positive integer.\")\n\n    # Convert to NumPy arrays for vectorized computation\n    X = np.array(points, dtype=float)            # shape: (n_points, n_dims)\n    centroids = np.array(initial_centroids, dtype=float)  # shape: (k, n_dims)\n    n_points, n_dims = X.shape\n\n    # Main k-means loop\n    for _ in range(max_iterations):\n        # Assignment step: assign each point to the closest centroid\n        # Broadcasting: (n_points, 1, n_dims) - (1, k, n_dims) -> (n_points, k, n_dims)\n        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n        labels = np.argmin(distances, axis=1)  # shape: (n_points,)\n\n        # Update step: recompute centroids\n        new_centroids = centroids.copy()\n        for idx in range(k):\n            cluster_points = X[labels == idx]\n            if cluster_points.size:          # Only update if the cluster has points\n                new_centroids[idx] = cluster_points.mean(axis=0)\n\n        # Check for convergence\n        if np.allclose(centroids, new_centroids, atol=1e-8):\n            centroids = new_centroids\n            break\n\n        centroids = new_centroids\n\n    # Round each coordinate to 4 decimal places and return as list of tuples\n    return [tuple(np.round(c, 4)) for c in centroids]\n```"}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Split data into train/test indices for classic K-Fold cross-validation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Target array of shape (n_samples,) or compatible.\n    k : int, default=5\n        Number of folds.  Must satisfy 2 \u2264 k \u2264 n_samples.\n    shuffle : bool, default=True\n        If True, shuffle samples before splitting.\n    random_seed : int or None, default=None\n        Seed used when `shuffle` is True (for reproducibility).\n\n    Returns\n    -------\n    folds : list[tuple[np.ndarray, np.ndarray]]\n        List of length `k`; each element is a tuple\n        `(train_indices, test_indices)`.\n    \"\"\"\n    # ---------- Validation ----------\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n    n_samples = X.shape[0]\n\n    if not isinstance(k, int) or k < 2:\n        raise ValueError(\"k must be an integer greater than or equal to 2.\")\n    if k > n_samples:\n        raise ValueError(\"k cannot be greater than the number of samples.\")\n\n    # ---------- Index preparation ----------\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        rng.shuffle(indices)\n\n    # Compute fold sizes: distribute remainder among the first folds\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[: n_samples % k] += 1\n\n    # ---------- Build the splits ----------\n    folds = []\n    current = 0\n    for fold_size in fold_sizes:\n        start, stop = current, current + fold_size\n        test_idx = indices[start:stop]\n        train_idx = np.concatenate((indices[:start], indices[stop:]))  # everything else\n        folds.append((train_idx, test_idx))\n        current = stop\n\n    return folds\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n       Perform Principal Component Analysis (PCA) from scratch.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array with shape (n_samples, n_features).\n    k : int\n        Number of principal components to return (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        A list containing the first `k` principal components.  \n        Each component is a list of length `n_features` with every\n        entry rounded to 4 decimal places.\n       # -------- 1. Basic checks -------------------------------------------------\n    if data.ndim != 2:\n        raise ValueErrorInput `data` must be a 2-D NumPy array    n_samples, n_features = data.shape\n    if not (1 <= k <= n_features):\n        raise ValueError(fk` must be between 1 and {n_features} (got {k}).    # -------- 2. Standardize the data ----------------------------------------\n    X = data.astype(float)\n    means = X.mean(axis=0)\n    stds  = X.std(axis=0, ddof=0)          # population std. ddof=0\n    stds[stds == 0] = 1.0                  # avoid division by zero\n    Z = (X - means) / stds                 # standardized matrix\n\n    # -------- 3. Covariance matrix -------------------------------------------\n    # bias=True gives divisor n (population covariance)\n    cov_mat = np.cov(Z, rowvar=False, bias=True)   # shape (n_features, n_features)\n\n    # -------- 4. Eigen decomposition -----------------------------------------\n    # For symmetric matrices, eigh is more stable than eig\n    eig_vals, eig_vecs = np.linalg.eigh(cov_mat)\n\n    # -------- 5. Sort eigenvalues & eigenvectors -----------------------------\n    idx = np.argsort(eig_vals)[::-1]       # indices for descending order\n    eig_vecs = eig_vecs[:, idx]            # reorder eigenvectors\n\n    # -------- 6. Select the top-k components ---------------------------------\n    principal_components = eig_vecs[:, :k].T   # shape (k, n_features)\n\n    # -------- 7. Round to 4 decimal places -----------------------------------\n    principal_components = np.round(principal_components, 4).tolist()\n\n    return principal_components\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nfrom copy import deepcopy\n\n\ndef learn_decision_tree(\n    examples: list[dict], attributes: list[str], target_attr: str\n) -> dict:\n    \"\"\"\n    Learn a decision-tree classifier (ID3/recursive information-gain splitting).\n\n    Parameters\n    ----------\n    examples : list[dict]\n        Each element is a dictionary mapping attribute names to values.\n        The class label is stored under ``target_attr``.\n    attributes : list[str]\n        Candidate attributes that may be used for splitting.  ``target_attr``\n        should NOT be in this list (the function removes it if it is).\n    target_attr : str\n        Name of the key that contains the class label in every example.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree.  Internal nodes have the form\n\n            {attribute : {branch_value : subtree, \u2026}}\n\n        If the chosen attribute is numeric, only two branches are created\n        (binary split):\n\n            {attribute : {'<= threshold' : left_subtree,\n                          '> threshold' : right_subtree}}\n\n        Leaves are plain class labels (e.g. 'yes', 'no', 0, 1, \u2026).\n    \"\"\"\n\n    # ------------------------------------------------------------------ #\n    #                 Helpers: entropy / information gain                #\n    # ------------------------------------------------------------------ #\n\n    def entropy(sample: list[dict]) -> float:\n        \"\"\"Shannon entropy of the target attribute within a sample.\"\"\"\n        total = len(sample)\n        counts = Counter(row[target_attr] for row in sample)\n        return -sum((c / total) * math.log2(c / total) for c in counts.values())\n\n    def majority_class(sample: list[dict]):\n        \"\"\"Most common class label in a sample (ties broken arbitrarily).\"\"\"\n        return Counter(row[target_attr] for row in sample).most_common(1)[0][0]\n\n    def is_number(x) -> bool:  # tiny helper\n        return isinstance(x, (int, float))\n\n    # ----------  best binary split for a *numeric* attribute  ---------- #\n    def best_numeric_split(sample: list[dict], attr: str):\n        \"\"\"\n        For a numeric attribute return (gain, threshold).\n\n        The split is attr <= threshold   vs.   attr > threshold.\n        \"\"\"\n        values = sorted({row[attr] for row in sample})\n        if len(values) == 1:  # nothing to split\n            return 0.0, None\n\n        base_entropy = entropy(sample)\n        best_gain, best_th = 0.0, None\n\n        # Candidate thresholds: mid-points between consecutive distinct values\n        for i in range(len(values) - 1):\n            th = (values[i] + values[i + 1]) / 2\n\n            left = [row for row in sample if row[attr] <= th]\n            right = [row for row in sample if row[attr] > th]\n            if not left or not right:  # would create an empty branch\n                continue\n\n            rem = (len(left) / len(sample)) * entropy(left) + (\n                len(right) / len(sample)\n            ) * entropy(right)\n            gain = base_entropy - rem\n\n            if gain > best_gain:\n                best_gain, best_th = gain, th\n\n        return best_gain, best_th\n\n    # ----------  information gain for either numeric / categorical  ---- #\n    def info_gain(sample: list[dict], attr: str):\n        \"\"\"\n        Return (gain, threshold).  For categorical attributes, threshold is None.\n        \"\"\"\n        # Detect attribute type from the first non-None value\n        first_val = next(row[attr] for row in sample if row[attr] is not None)\n\n        if is_number(first_val):  # numeric \u2192 binary threshold split\n            return best_numeric_split(sample, attr)\n\n        # ------------- categorical, possibly multi-way split -------------\n        base_entropy = entropy(sample)\n        parts = {}\n        for row in sample:\n            parts.setdefault(row[attr], []).append(row)\n\n        remainder = sum((len(part) / len(sample)) * entropy(part) for part in parts.values())\n        gain = base_entropy - remainder\n        return gain, None\n\n    # ------------------------------------------------------------------ #\n    #                           Tree induction                           #\n    # ------------------------------------------------------------------ #\n\n    # Prevent accidental modification of caller\u2019s list\n    attributes = deepcopy([a for a in attributes if a != target_attr])\n\n    def build(sample: list[dict], attrs: list[str]):\n        # ----------  stopping conditions / leaves  ----------\n        if not sample:\n            return None  # no data \u2013 should not normally happen\n\n        # All examples have the same class \u2192 pure leaf\n        classes = [row[target_attr] for row in sample]\n        if len(set(classes)) == 1:\n            return classes[0]\n\n        # No attributes left to split on \u2192 majority vote leaf\n        if not attrs:\n            return majority_class(sample)\n\n        # ----------  choose attribute with max information gain ----------\n        scored = (info_gain(sample, a) + (a,) for a in attrs)\n        best_gain, threshold, best_attr = max(scored, key=lambda t: t[0])\n\n        # If no attribute offers any information gain, fall back to majority\n        if best_gain == 0:\n            return majority_class(sample)\n\n        # ----------  create internal node & recurse ----------\n        node = {best_attr: {}}\n\n        if threshold is None:  # categorical attribute \u2192 one branch per value\n            for val in {row[best_attr] for row in sample}:\n                subset = [row for row in sample if row[best_attr] == val]\n                node[best_attr][val] = build(\n                    subset, [a for a in attrs if a != best_attr]\n                )\n        else:  # numeric attribute \u2192 binary split\n            le_key = f\"<= {threshold}\"\n            gt_key = f\"> {threshold}\"\n            left_sub = [row for row in sample if row[best_attr] <= threshold]\n            right_sub = [row for row in sample if row[best_attr] > threshold]\n            node[best_attr][le_key] = build(left_sub, [a for a in attrs if a != best_attr])\n            node[best_attr][gt_key] = build(right_sub, [a for a in attrs if a != best_attr])\n\n        return node\n\n    # ------------------------------------------------------------------ #\n    #                          kick things off                           #\n    # ------------------------------------------------------------------ #\n    return build(examples, attributes)\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _kernel_matrix(data: np.ndarray, kernel: str, sigma: float = 1.0) -> np.ndarray:\n    \"\"\"\n    Build full kernel matrix.\n    \"\"\"\n    if kernel == 'linear':\n        return data @ data.T\n\n    if kernel == 'rbf':\n        # \u2016x\u2212z\u2016\u00b2 = \u2016x\u2016\u00b2+\u2016z\u2016\u00b2\u22122x\u00b7z\n        sq_norms = np.sum(data ** 2, axis=1).reshape(-1, 1)\n        dists = sq_norms - 2 * data @ data.T + sq_norms.T\n        return np.exp(-dists / (2.0 * sigma ** 2))\n\n    raise ValueError(\"Unknown kernel - use 'linear' or 'rbf'.\")\n\n\ndef pegasos_kernel_svm(\n        data: np.ndarray,\n        labels: np.ndarray,\n        kernel: str = 'linear',\n        lambda_val: float = 0.01,\n        iterations: int = 100,\n        sigma: float = 1.0):\n    \"\"\"\n    Deterministic (full-batch) Pegasos Kernel SVM.\n\n    Parameters\n    ----------\n    data : (n_samples, n_features) ndarray\n    labels : (n_samples,) ndarray, entries must be \u00b11 (0 is converted to \u22121)\n    kernel : 'linear' or 'rbf'\n    lambda_val : regularisation parameter \u03bb  (>0)\n    iterations : number of Pegasos iterations\n    sigma : \u03c3 used only if kernel == 'rbf'\n\n    Returns\n    -------\n    alphas_list, bias\n        \u2022 alphas_list \u2013 python list (rounded to 4 decimals)  \n        \u2022 bias \u2013 float (rounded to 4 decimals)\n    \"\"\"\n    # ---------- sanity checks -------------------------------------------------\n    if lambda_val <= 0:\n        raise ValueError(\"lambda_val must be positive.\")\n\n    labels = labels.astype(float).copy()\n    labels[labels == 0] = -1       # map 0 \u2192 \u22121 if necessary\n    if not np.all(np.isin(labels, [-1, 1])):\n        raise ValueError(\"labels must be \u00b11 (or 0/1).\")\n\n    n = data.shape[0]\n    # -------------------------------------------------------------------------\n    K = _kernel_matrix(data, kernel, sigma)\n\n    alpha = np.zeros(n, dtype=float)   # dual coefficients\n    bias = 0.0\n\n    # pre-compute constant 1/\u221a\u03bb   (projection radius in primal space)\n    proj_radius = 1.0 / np.sqrt(lambda_val)\n\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)           # learning-rate\n\n        # decision function on all samples\n        y_alpha = alpha * labels\n        f = y_alpha @ K + bias                 # (n,) vector\n        margins = labels * f\n\n        # hinge violation mask\n        violated = (margins < 1).astype(float)   # 1 if violation else 0\n\n        # --------- update alpha ------------------------------------------------\n        alpha *= (1.0 - eta * lambda_val)              # shrinkage\n        alpha += (eta / n) * violated                  # add gradient step\n\n        # --------- update bias -------------------------------------------------\n        bias += (eta / n) * np.sum(violated * labels)  # b has no regulariser\n\n        # --------- projection --------------------------------------------------\n        y_alpha = alpha * labels\n        w_norm_sq = y_alpha @ K @ y_alpha              # \u2016w\u2016\u00b2 in feature space\n        if w_norm_sq > 0 and w_norm_sq > proj_radius ** 2:\n            scaling = proj_radius / np.sqrt(w_norm_sq)\n            alpha *= scaling\n        # ----------------------------------------------------------------------\n\n    # ------------- rounding & output ------------------------------------------\n    alpha_rounded = np.round(alpha, 4).tolist()\n    bias_rounded = float(np.round(bias, 4))\n    return alpha_rounded, bias_rounded\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n       Compute the sigmoid activation for the given input.\n    \n    Parameters\n    ----------\n    z : float\n        The input value.\n    \n    Returns\n    -------\n    float\n        Sigmoid of z, rounded to four decimal places.\n       s = 1 / (1 + math.exp(-z))\n    return round(s, 4)\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a list of scores.\n\n    Args:\n        scores (list[float]): Input scores.\n\n    Returns:\n        list[float]: Softmax probabilities rounded to four decimal places.\n    \"\"\"\n    if not scores:                         # handle empty input\n        return []\n\n    max_score = max(scores)                # for numerical stability\n    exp_scores = [math.exp(s - max_score) for s in scores]\n    total = sum(exp_scores)\n\n    return [round(e / total, 4) for e in exp_scores]\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(\n        features: list[list[float]],\n        labels:   list[int],\n        weights:  list[float],\n        bias:     float\n    ) -> (list[float], float):\n    \"\"\"\n    Simulate a single sigmoid-activated neuron for binary classification.\n\n    Parameters\n    ----------\n    features : list of list of float\n        Each inner list is a feature vector for one sample.\n    labels   : list of int\n        True binary labels (0 or 1) for every sample.\n    weights  : list of float\n        One weight per feature (must match feature-vector length).\n    bias     : float\n        Bias term of the neuron.\n\n    Returns\n    -------\n    tuple\n        1. List of predicted probabilities (rounded to 4 decimals).\n        2. Mean-squared error w.r.t. the true labels (rounded to 4 decimals).\n    \"\"\"\n\n    # ------------------ basic validation ------------------\n    if not features:\n        raise ValueError(\"`features` must not be empty.\")\n    if len(features) != len(labels):\n        raise ValueError(\"`features` and `labels` must be of the same length.\")\n    if len(features[0]) != len(weights):\n        raise ValueError(\"Length of each feature vector must equal length of `weights`.\")\n\n    # ------------------ vectorised computation ------------------\n    X      = np.asarray(features, dtype=float)     # shape: (n_samples, n_features)\n    w      = np.asarray(weights,  dtype=float)     # shape: (n_features,)\n    y_true = np.asarray(labels,   dtype=float)     # shape: (n_samples,)\n\n    z           = X.dot(w) + bias                  # linear combination\n    predictions = 1.0 / (1.0 + np.exp(-z))         # sigmoid activation\n\n    # ------------------ rounding & output ------------------\n    prob_list = np.round(predictions, 4).reshape(-1).tolist()\n    mse       = round(float(np.mean((predictions - y_true) ** 2)), 4)\n\n    return prob_list, mse\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray,\n                 labels:   np.ndarray,\n                 initial_weights: np.ndarray,\n                 initial_bias:    float,\n                 learning_rate:   float,\n                 epochs:          int) -> (np.ndarray, float, list[float]):\n       Train a single sigmoid-activated neuron with batch gradient descent (MSE loss).\n\n    Parameters\n    ----------\n    features : 2-D array, shape = (n_samples, n_features)\n    labels   : 1-D array, shape = (n_samples,)\n               Binary targets (0 or 1)\n    initial_weights : 1-D array, shape = (n_features,)\n    initial_bias    : scalar\n    learning_rate   : scalar\n    epochs          : integer\n\n    Returns\n    -------\n    final_weights : numpy.ndarray   (rounded to 4 decimals)\n    final_bias    : float           (rounded to 4 decimals)\n    mse_history   : list[float]     (each rounded to 4 decimals)\n       def _sigmoid(z: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Work on copies so original inputs stay intact\n    w = initial_weights.astype(float).copy()\n    b = float(initial_bias)\n\n    n_samples = features.shape[0]\n    mse_history = []\n\n    for _ in range(epochs):\n        # Forward pass\n        z = features @ w + b                # (n_samples,)\n        preds = _sigmoid(z)                 # (n_samples,)\n\n        # MSE loss\n        mse = np.mean((preds - labels) ** 2)\n        mse_history.append(mse)\n\n        # Back-propagation\n        dloss_dpred = 2 * (preds - labels)              # (n_samples,)\n        dpred_dz    = preds * (1 - preds)               # (n_samples,)\n        dloss_dz    = dloss_dpred * dpred_dz            # (n_samples,)\n\n        grad_w = (features.T @ dloss_dz) / n_samples    # (n_features,)\n        grad_b = np.mean(dloss_dz)                      # scalar\n\n        # Parameter update\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n    # Rounding and type formatting\n    w = np.round(w, 4)\n    b = float(np.round(b, 4))\n    mse_history = np.round(np.array(mse_history), 4).tolist()\n\n    return w, b, mse_history\n```"}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = float(data)                 # make sure we store scalars as float\n        self.grad = 0.0                         # the accumulated gradient\n        self._backward = lambda: None           # function to propagate gradients to _children\n        self._prev = set(_children)             # the graph edges (parents \u2192 this node)\n        self._op = _op                          # the op that produced this node (for debug / viz)\n\n    # ------------------------------------------------------------------ helpers\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def _ensure_value(self, other):\n        return other if isinstance(other, Value) else Value(other)\n\n    # ------------------------------------------------------------------ math ops\n    def __add__(self, other):\n        other = self._ensure_value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            # d(out)/d(self) = 1\n            self.grad  += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    __radd__ = __add__  # addition is commutative\n\n    # -------------------------------------------------------------\n    def __mul__(self, other):\n        other = self._ensure_value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            # d(out)/d(self)   = other.data\n            # d(out)/d(other)  = self.data\n            self.grad  += other.data * out.grad\n            other.grad += self.data  * out.grad\n        out._backward = _backward\n        return out\n\n    __rmul__ = __mul__\n\n    # -------------------------------------------------------------\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0.0, (self,), 'ReLU')\n\n        def _backward():\n            # ReLU\u2032 = 1 if data>0 else 0\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    # ------------------------------------------------------------------ autograd\n    def backward(self):\n        \"\"\"\n        Back-propagate the gradients from this (output) node all the way down\n        to the leaves. This should be called on the scalar you want the gradient\n        of (typically the \"loss\").\n        \"\"\"\n        # 1) produce topological ordering of the graph\n        topo, visited = [], set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # 2) seed the gradient of the output\n        self.grad = 1.0\n\n        # 3) traverse in reverse topological order calling _backward\n        for node in reversed(topo):\n            node._backward()\n```\n\nExample usage:\n\n```python\n# f(x,y) = ReLU(x*y + y) ; compute \u2202f/\u2202x and \u2202f/\u2202y at x=2, y=-3\nx = Value(2.0)\ny = Value(-3.0)\nf = (x * y + y).relu()\nf.backward()\n\nprint(f)   # Value(data=0.0, grad=1.0)\nprint(x)   # \u2202f/\u2202x should be 0 because ReLU output is 0\nprint(y)   # \u2202f/\u2202y should be 0 for same reason\n```\n\nThe class now supports:\n\u2022 Forward computation with +, * and ReLU  \n\u2022 Automatic differentiation by calling `.backward()` on the result."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P that converts coordinates\n    from basis B to basis C in \u211d\u00b3.\n    \n        [x]_C = P \u00b7 [x]_B,     where  P = (S_C)\u207b\u00b9 \u00b7 S_B\n    \n    S_B : matrix whose columns are the vectors of B\n    S_C : matrix whose columns are the vectors of C\n    \n    Parameters\n    ----------\n    B : list[list[int | float]]\n        3\u00d73 list where each inner list is a vector of the basis B.\n        \n    C : list[list[int | float]]\n        3\u00d73 list where each inner list is a vector of the basis C.\n    \n    Returns\n    -------\n    list[list[float]]\n        3\u00d73 transformation matrix rounded to 4 decimal places,\n        converted back to a regular Python nested list.\n    \"\"\"\n    \n    # Build matrices with basis vectors as columns\n    S_B = np.column_stack(B)     # Shape (3, 3)\n    S_C = np.column_stack(C)     # Shape (3, 3)\n    \n    # Compute P = (S_C)^(-1) \u00b7 S_B\n    P = np.linalg.inv(S_C) @ S_B\n    \n    # Round to 4 decimal places\n    P = np.round(P, 4)\n    \n    # Convert to regular Python list and return\n    return P.tolist()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Singular Value Decomposition for a real 2\u00d72 matrix, constructed purely\n    from eigen-analysis (no direct call to np.linalg.svd).\n\n    Returns\n    -------\n    (U, S, V) : tuple of python lists\n        U \u2013 2\u00d72 orthogonal matrix (left singular vectors)\n        S \u2013 2\u00d72 diagonal matrix with non\u2013negative singular values\n        V \u2013 2\u00d72 orthogonal matrix (right singular vectors)\n\n    The relation satisfied is  A \u2248 U @ S @ V,  with every element\n    rounded to 4 decimal places and converted to plain python lists.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    if A.shape != (2, 2):\n        raise ValueError(\"Input must be a 2\u00d72 matrix.\")\n\n    # --- 1. Right singular vectors from eigendecomposition of A\u1d40A ----------\n    ATA = A.T @ A\n    eig_vals, eig_vecs = np.linalg.eigh(ATA)          # guaranteed real + ortho\n    order = eig_vals.argsort()[::-1]                  # largest first\n    eig_vals = eig_vals[order]\n    V = eig_vecs[:, order]\n\n    # --- 2. Singular values -------------------------------------------------\n    sigmas = np.sqrt(np.clip(eig_vals, 0.0, None))    # numerical safety\n    S = np.diag(sigmas)\n\n    # --- 3. Left singular vectors ------------------------------------------\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sigmas[i] > 1e-12:                         # non-zero \u03c3\u1d62\n            U[:, i] = (A @ V[:, i]) / sigmas[i]\n\n    # Rank\u2013deficient case (\u03c3\u2082 \u2248 0): fabricate a perpendicular vector\n    if sigmas[1] < 1e-12:\n        u0 = U[:, 0] / np.linalg.norm(U[:, 0])\n        U[:, 0] = u0\n        U[:, 1] = np.array([-u0[1], u0[0]])\n\n    # Final orthonormalisation (small numerical fix-ups)\n    U[:, 0] /= np.linalg.norm(U[:, 0])\n    U[:, 1] -= U[:, 0] * np.dot(U[:, 0], U[:, 1])\n    U[:, 1] /= np.linalg.norm(U[:, 1])\n\n    # --- 4. Round & return python lists ------------------------------------\n    return (np.round(U, 4).tolist(),\n            np.round(S, 4).tolist(),\n            np.round(V, 4).tolist())\n```\n\nExample\n-------\n```python\nA = np.array([[3, 1],\n              [1, 3]], dtype=float)\n\nU, S, V = svd_2x2(A)\n\nprint(\"U =\", U)\nprint(\"S =\", S)\nprint(\"V =\", V)\n```\n\nTypical output\n```\nU = [[0.7071, 0.7071],\n     [0.7071, -0.7071]]\n\nS = [[4.0, 0.0],\n     [0.0, 2.0]]\n\nV = [[0.7071, 0.7071],\n     [0.7071, -0.7071]]\n```\nand indeed `U @ S @ V` (within numerical rounding) reconstructs the original 2 \u00d7 2 matrix."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n       Randomly shuffles two NumPy arrays (X and y) in unison while\n    preserving the correspondence between their rows/samples.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature array of shape (n_samples, ...).\n    y : np.ndarray\n        Label/target array of shape (n_samples, ...).\n    seed : int or None, optional (default=None)\n        Seed for NumPy\u2019s random number generator to ensure reproducibility.\n\n    Returns\n    -------\n    tuple\n        A tuple containing two Python lists:\n        (shuffled_X_as_list, shuffled_y_as_list)\n       # Basic validation\n    if X.shape[0] != y.shape[0]:\n        raise ValueError`X` and `y` must have the same number of samples                        along the first axis    # Initialize a compatible random number generator\n    rng = np.random.default_rng(seed)\n\n    # Generate a permutation of indices and apply it\n    perm = rng.permutation(X.shape[0])\n    X_shuffled = X[perm]\n    y_shuffled = y[perm]\n\n    # Return Python lists (with original shapes preserved) using tolist()\n    return X_shuffled.tolist(), y_shuffled.tolist()\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yield successive batches from X (and y if supplied).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Array containing the features. First dimension equals the number\n        of samples.\n    y : np.ndarray or None, optional\n        Array containing the targets/labels. Must have the same number of\n        samples as X along the first axis. If None, only X is batched.\n    batch_size : int, default 64\n        Number of samples per batch.\n\n    Yields\n    ------\n    list\n        If `y` is None, yields a Python list representing a batch of X.\n    tuple(list, list)\n        If `y` is supplied, yields a tuple of two Python lists:\n        (X_batch, y_batch).\n\n    Notes\n    -----\n    \u2022 The function shuffles the data before creating batches so each epoch\n      presents the data in a different order.\n    \u2022 Each yielded batch (or pair of batches) is converted to a pure Python\n      list via NumPy's `tolist()` method, fulfilling the output constraint.\n    \"\"\"\n    # -- Sanity checks -----------------------------------------------------\n    if not isinstance(batch_size, int) or batch_size <= 0:\n        raise ValueError(\"`batch_size` must be a positive integer.\")\n    \n    # Convert inputs to np.ndarray if they aren\u2019t already\n    X = np.asarray(X)\n    if y is not None:\n        y = np.asarray(y)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"`X` and `y` must have the same number of samples.\")\n    \n    n_samples = X.shape[0]\n\n    # -- Shuffle indices so batches are sampled randomly -------------------\n    indices = np.random.permutation(n_samples)\n\n    # -- Yield batches -----------------------------------------------------\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_idx = indices[start:end]\n\n        X_batch = X[batch_idx].tolist()       # Convert to Python list\n\n        if y is None:\n            # Only features provided\n            yield X_batch\n        else:\n            y_batch = y[batch_idx].tolist()   # Convert to Python list\n            yield (X_batch, y_batch)\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Split a 2-D dataset into two parts according to whether the value in\n    column `feature_i` is greater than or equal to `threshold`.\n\n    Parameters\n    ----------\n    X : array-like (n_samples, n_features)\n        Input data.\n    feature_i : int\n        Index of the feature/column on which to split.\n    threshold : scalar\n        Threshold value for the split.\n\n    Returns\n    -------\n    tuple(list, list)\n        \u2022 First list  -> samples where X[:, feature_i] >= threshold  \n        \u2022 Second list -> samples where X[:, feature_i] <  threshold\n        Each sub-set is kept 2-D, then converted to a plain Python list\n        via NumPy\u2019s `tolist()` method.\n    \"\"\"\n    X = np.asarray(X)\n\n    # Basic sanity checks\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array-like object.\")\n    if not (0 <= feature_i < X.shape[1]):\n        raise IndexError(\"feature_i is out of bounds for input with \"\n                         f\"{X.shape[1]} feature(s).\")\n\n    # Boolean mask for the split\n    mask = X[:, feature_i] >= threshold\n\n    # Sub-sets (still NumPy arrays)\n    subset_true  = X[mask]\n    subset_false = X[~mask]\n\n    # Keep the 2-D shape explicitly (helps when subset has 0 or 1 row)\n    subset_true  = subset_true.reshape(subset_true.shape[0],  X.shape[1])\n    subset_false = subset_false.reshape(subset_false.shape[0], X.shape[1])\n\n    # Convert to regular Python lists\n    return subset_true.tolist(), subset_false.tolist()\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\n\ndef polynomial_features(X, degree):\n       Generate polynomial features of a 2-D input array up to a given degree.\n\n    Parameters\n    ----------\n    X : (n_samples, n_features) array-like\n        Input data.\n    degree : int\n        Highest polynomial degree to generate (must be \u2265 0).\n\n    Returns\n    -------\n    list\n        A (n_samples, n_output_features) nested Python list containing the\n        polynomial feature matrix.  The first column is the bias term (1\u2019s),\n        followed by increasing-degree monomials in lexicographic order,\n        exactly like scikit-learn\u2019s PolynomialFeatures(include_bias=True).\n       # -- basic checks --------------------------------------------------------\n    X = np.asarray(X)\n    if X.ndim != 2:\n        raise ValueErrorX must be a 2-D array    if degree < 0:\n        raise ValueErrordegree must be non-negative    n_samples, n_features = X.shape\n\n    # -- build list of index combinations ------------------------------------\n    # For each d = 0..degree, create all index tuples (i1, i2, \u2026, id)\n    # such that i1 \u2264 i2 \u2264 \u2026 \u2264 id to avoid duplicates.\n    combs = []\n    for d in range(degree + 1):                               # 0 gives the bias\n        combs.extend(combinations_with_replacement(range(n_features), d))\n\n    # -- allocate output ------------------------------------------------------\n    n_output_features = len(combs)\n    X_poly = np.empty((n_samples, n_output_features), dtype=X.dtype)\n\n    # -- compute each column --------------------------------------------------\n    for col_idx, comb in enumerate(combs):\n        if len(comb) == 0:                 # bias term: product of zero elements = 1\n            X_poly[:, col_idx] = 1\n        else:\n            # Start with the first feature column, then multiply in-place\n            feature_col = X[:, comb[0]]\n            for idx in comb[1:]:\n                feature_col = feature_col * X[:, idx]\n            X_poly[:, col_idx] = feature_col\n\n    # -- return as a pure Python nested list ---------------------------------\n    return X_poly.tolist()\n```"}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets from a dataset.\n\n    Parameters\n    ----------\n    X : np.ndarray (2-D)\n        Feature matrix with shape (n_samples, n_features).\n    y : np.ndarray (1-D)\n        Target/label vector with shape (n_samples,).\n    n_subsets : int\n        Number of random subsets to create.\n    replacements : bool, default=True\n        If True, sampling is done with replacement (bootstrapping);\n        if False, sampling is done without replacement.\n    seed : int, default=42\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    list\n        A list of length `n_subsets`. Each element is a tuple\n        (X_subset, y_subset) where both `X_subset` and `y_subset`\n        have been converted to pure Python lists via `.tolist()`.\n    \"\"\"\n    # ---------------------- basic validation ----------------------\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"X and y must both be numpy arrays.\")\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2-D array (matrix).\")\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1-D array (vector).\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples.\")\n    if not isinstance(n_subsets, int) or n_subsets <= 0:\n        raise ValueError(\"n_subsets must be a positive integer.\")\n\n    rng = np.random.default_rng(seed)\n    n_samples = X.shape[0]\n    subsets = []\n\n    # ---------------------- subset generation ---------------------\n    for _ in range(n_subsets):\n        # Choose indices with or without replacement\n        idx = rng.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[idx].tolist()  # Convert to Python list\n        y_subset = y[idx].tolist()  # Convert to Python list\n        subsets.append((X_subset, y_subset))\n\n    return subsets\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encode a 1-D array of integer class labels.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        1-D array of integer labels (e.g., [0, 2, 1, 3]).\n    n_col : int, optional\n        Desired number of columns (i.e., number of classes).  If None,\n        it is inferred as `max(x) + 1`.  If supplied value is smaller\n        than `max(x) + 1`, it is automatically enlarged.\n\n    Returns\n    -------\n    list\n        One-hot encoded representation of `x`, converted to a Python list.\n    \"\"\"\n    # Ensure `x` is a flattened NumPy array of integers\n    x = np.asarray(x).ravel().astype(int)\n\n    # Determine number of columns (classes)\n    max_class = x.max() if x.size else -1\n    if n_col is None or n_col <= max_class:\n        n_col = max_class + 1\n\n    # Build one-hot matrix\n    one_hot = np.zeros((x.size, n_col), dtype=int)\n    if x.size:                                 # handle empty input\n        one_hot[np.arange(x.size), x] = 1\n\n    return one_hot.tolist()\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n       Convert a 1-D NumPy array into a diagonal matrix\n    and return it as a regular Python (nested) list.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        A one-dimensional NumPy array.\n\n    Returns\n    -------\n    list\n        A 2-D list where the input elements form the main diagonal.\n       # Ensure the input is a NumPy array and flatten it to 1-D\n    x = np.asarray(x).ravel()\n    \n    # Build the diagonal matrix\n    diag_mat = np.diag(x)\n    \n    # Convert to a regular Python list\n    return diag_mat.tolist()\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Compute the accuracy (fraction of correct predictions).\n    \n    Parameters\n    ----------\n    y_true : 1-D array-like\n        Ground-truth (correct) labels.\n    y_pred : 1-D array-like\n        Predicted labels, must be the same length as `y_true`.\n        \n    Returns\n    -------\n    float\n        Accuracy score in the range [0, 1].\n    \n    Raises\n    ------\n    ValueError\n        If the input arrays differ in shape or are empty.\n    \"\"\"\n    \n    # Convert inputs to NumPy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Check that shapes match\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"`y_true` and `y_pred` must have the same shape.\")\n    \n    # Flatten in case the user supplies a column vector, etc.\n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n    \n    # Guard against empty input\n    if y_true.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    \n    # Count correct predictions and compute accuracy\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / y_true.size\n    \n    return float(accuracy)\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n       Calculate a correlation matrix.\n    \n    Parameters\n    ----------\n    X : 2-D numpy array, shape (n_samples, n_features_X)\n        Data whose columns are variables.\n    Y : 2-D numpy array, shape (n_samples, n_features_Y), optional (default=None)\n        If provided, correlations are computed between columns of X and columns of Y.\n        If None, correlations are computed among columns of X only.\n    \n    Returns\n    -------\n    list\n        2-D (nested) Python list containing the correlation matrix rounded\n        to 4 decimal places.\n       # --- Validation & preparation -------------------------------------------------\n    X = np.asarray(X, dtype=float)\n    \n    if X.ndim != 2:\n        raise ValueErrorX must be a 2-D array    \n    if Y is None:\n        Y = X\n    else:\n        Y = np.asarray(Y, dtype=float)\n        if Y.ndim != 2:\n            raise ValueErrorY must be a 2-D array        if X.shape[0] != Y.shape[0]:\n            raise ValueErrorX and Y must have the same number of rows (samples).    \n    n = X.shape[0]\n    if n < 2:\n        raise ValueErrorAt least two samples are required to compute correlation    \n    # --- Center the data ----------------------------------------------------------\n    Xc = X - X.mean(axis=0, keepdims=True)\n    Yc = Y - Y.mean(axis=0, keepdims=True)\n    \n    # --- Covariance matrix --------------------------------------------------------\n    cov = (Xc.T @ Yc) / (n - 1)        # shape: (n_features_X, n_features_Y)\n    \n    # --- Standard deviations ------------------------------------------------------\n    std_X = np.sqrt((Xc**2).sum(axis=0) / (n - 1))   # shape: (n_features_X,)\n    std_Y = np.sqrt((Yc**2).sum(axis=0) / (n - 1))   # shape: (n_features_Y,)\n    \n    # --- Correlation matrix -------------------------------------------------------\n    with np.errstate(divide='ignore', invalid='ignore'):\n        corr = cov / np.outer(std_X, std_Y)\n    \n    # Replace any divisions by zero with nan (optional; could also leave inf)\n    corr[~np.isfinite(corr)] = np.nan\n    \n    # --- Rounding & conversion to Python list -------------------------------------\n    corr_rounded = np.around(corr, 4)\n    return corr_rounded.tolist()\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n       Fit an AdaBoost classifier that uses decision-stumps (one-dimensional\n    thresholds) as weak learners.\n\n    Parameters\n    ----------\n    X      : 2-D numpy array of shape (n_samples, n_features)\n    y      : 1-D numpy array of shape (n_samples,)  \u2013 class labels.\n             May be in {0,1} or {-1,1}.                 \n    n_clf  : int \u2013 number of weak classifiers to build.\n\n    Returns\n    -------\n    clfs : list of dictionaries \u2013 every dictionary contains the parameters\n           of one stump:\n           { 'feature_index' : int,\n             'threshold'     : float (rounded to 4 d.p.),\n             'polarity'      : int   (1  or -1),\n             'alpha'         : float (rounded to 4 d.p.) }\n       X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float)\n\n    # Make sure the labels are in {-1, 1}\n    y = np.where(y <= 0, -1, 1)\n\n    n_samples, n_features = X.shape\n    # Initial sample weights (uniform)\n    w = np.full(n_samples, 1 / n_samples)\n\n    # List that will store every weak learner\n    clfs = []\n\n    # Small constant to avoid division by zero\n    EPS = 1e-10\n\n    for _ in range(n_clf):\n\n        # Variables for the best stump of this round\n        best_feature   = None\n        best_threshold = None\n        best_polarity  = None\n        min_error      = float('inf')\n        best_pred      = None\n\n        # Search for the best decision stump\n        for j in range(n_features):\n            X_j = X[:, j]\n            thresholds = np.unique(X_j)\n\n            for thr in thresholds:\n                for polarity in (1, -1):\n                    # Predict with current stump\n                    pred = np.ones(n_samples)\n                    if polarity == 1:\n                        pred[X_j <  thr] = -1\n                    else:  # polarity == -1\n                        pred[X_j >  thr] = -1\n\n                    # Weighted error\n                    err = np.sum(w[pred != y])\n\n                    # Pick the stump with the smallest error\n                    if err < min_error:\n                        min_error      = err\n                        best_feature   = j\n                        best_threshold = thr\n                        best_polarity  = polarity\n                        best_pred      = pred\n\n        # Protect against division by zero / perfect classifier\n        min_error = max(min_error, EPS)\n\n        # Compute learner weight\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n\n        # Update sample weights\n        w *= np.exp(-alpha * y * best_pred)\n        # Re-normalise so they sum to 1\n        w /= np.sum(w)\n\n        # Store the stump, rounding numerical values to 4 decimals\n        clfs.append({\n            'feature_index': int(best_feature),\n            'threshold'    : round(float(best_threshold), 4),\n            'polarity'     : int(best_polarity),\n            'alpha'        : round(float(alpha), 4)\n        })\n\n    return clfs\n```"}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the numerically\u2013stable log-softmax of a 1-D list/array.\n\n    Parameters\n    ----------\n    scores : list or 1-D array-like\n        Raw (unnormalized) scores.\n\n    Returns\n    -------\n    list\n        Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    # Convert to a NumPy float array\n    scores = np.asarray(scores, dtype=float)\n\n    # Numerical-stability trick: subtract the maximum score\n    shifted = scores - np.max(scores)\n\n    # Compute log-softmax\n    log_softmax_vals = shifted - np.log(np.exp(shifted).sum())\n\n    # Round to 4 decimal places and convert back to a Python list\n    return np.round(log_softmax_vals, 4).tolist()\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n\n\nclass Layer(object):\n\n    # ------------------------------------------------------------------ #\n    # ---------------  methods common to every layer  ------------------ #\n    # ------------------------------------------------------------------ #\n    def set_input_shape(self, shape):\n               Receives the shape of the previous layer so that the current one\n        can infer its own shapes (needed mainly for weight initialisation).\n               self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n               Total number of trainable parameters of the layer.\n               return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\n\nclass Dense(Layer):\n       Fully\u2013connected layer  ( a.k.a.  \u201cDense\u201d / \u201cLinear\u201d / \u201cAffine\u201d layer )\n       # ------------------------------------------------------------------ #\n    # ------------------------  constructor  --------------------------- #\n    # ------------------------------------------------------------------ #\n    def __init__(self, n_units, input_shape=None):\n               Parameters\n        ----------\n        n_units : int\n            Number of neurons of the layer.\n        input_shape : tuple or list, optional\n            Shape of the input **excluding** the batch dimension.\n            If passed we can already initialise the weights, otherwise\n            initialisation will be postponed until `set_input_shape` is called.\n               # -----  structural attributes  ----- #\n        self.n_units = n_units\n        self.input_shape = input_shape           # (n_features,)\n        self.layer_input = None                  # cache for backward pass\n\n        # -----  learnable parameters  ----- #\n        self.W = None                            # weights  (n_features , n_units)\n        self.w0 = None                           # biases   (n_units ,)\n\n        # -----  optimisation helpers  ----- #\n        self.W_opt = None                        # weight-optimizer   (will hold a COPY of the global optim.)\n        self.w0_opt = None                       # bias-optimizer\n        self.trainable = True                    # flag so that e.g. frozen layers skip the update step\n\n        # if the input shape is already known we can immediately initialise\n        if self.input_shape is not None:\n            self.initialize()\n\n    # ------------------------------------------------------------------ #\n    # ---------------------  mandatory interface  ---------------------- #\n    # ------------------------------------------------------------------ #\n    def set_input_shape(self, shape):\n               Receives the shape of the previous layer.\n        Triggers weight initialisation once the number\n        of incoming features is known.\n               super().set_input_shape(shape)\n        self.initialize()\n\n    def initialize(self):\n               Weight & bias initialisation.\n        Xavier/Glorot-uniform (u(-limit , limit)) where limit = 1/sqrt(n_in)\n        tends to work well for fully-connected layers.\n               n_in = self.input_shape[0]           # number of incoming features\n        limit = 1. / math.sqrt(n_in)\n\n        self.W = np.random.uniform(-limit, limit, (n_in, self.n_units))\n        self.w0 = np.zeros(self.n_units)\n\n    # -------------  API utility so that an external optimizer can be set\n    def set_optimizer(self, optimizer):\n               Each parameter gets its own COPY of the global optimizer\n        so that internal states (e.g. momentum, Adam moments, \u2026) are not shared\n        among different layers.\n               self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    # ------------------------------------------------------------------ #\n    # -------------------  bookkeeping / statistics  ------------------- #\n    # ------------------------------------------------------------------ #\n    def parameters(self):\n               Total number of trainable scalars (weights + biases).\n               if self.W is None:                        # not initialised yet\n            return 0\n        return self.W.size + self.w0.size\n\n    def output_shape(self):\n               Shape *per sample* (batch dimension not included) that this layer\n        will emit in its forward pass.\n               return (self.n_units,)\n\n    # ------------------------------------------------------------------ #\n    # ------------------------  forward pass  -------------------------- #\n    # ------------------------------------------------------------------ #\n    def forward_pass(self, X, training=True):\n               Y =  X \u00b7 W  +  w0\n               self.layer_input = X                      # cache for backward pass\n        y = X.dot(self.W) + self.w0               # affine transform\n\n        # rounding as requested by the exercise\n        y = np.round(y, 4)\n        return y\n\n    # ------------------------------------------------------------------ #\n    # ------------------------  backward pass  ------------------------- #\n    # ------------------------------------------------------------------ #\n    def backward_pass(self, accum_grad):\n               Parameters\n        ----------\n        accum_grad : ndarray  (batch_size , n_units)\n            Gradient that has accumulated from upper layers (\u2202L/\u2202Y).\n        Returns\n        -------\n        grad_input : ndarray  (batch_size , n_features)\n            Gradient with respect to this layer's input so that the chain\n            rule can propagate it further down.\n               # -----  gradients w.r.t. parameters  ----- #\n        grad_W = self.layer_input.T.dot(accum_grad)      # (n_features , n_units)\n        grad_w0 = np.sum(accum_grad, axis=0)             # (n_units ,)\n\n        # -----  update step if the layer is trainable  ----- #\n        if self.trainable:\n            if self.W_opt is not None:                   # user supplied optimizer\n                self.W = self.W_opt.update(self.W, grad_W)\n                self.w0 = self.w0_opt.update(self.w0, grad_w0)\n            else:                                        # fallback to vanilla SGD\n                lr = 1e-2\n                self.W -= lr * grad_W\n                self.w0 -= lr * grad_w0\n\n        # -----  gradient w.r.t. the input  ----- #\n        grad_input = accum_grad.dot(self.W.T)            # (batch_size , n_features)\n        return np.round(grad_input, 4)\n```"}
{"task_id": 41, "completion_id": 0, "completion": "Here is a compact, self-contained implementation of a 2-D convolution layer that honours padding, stride and four-decimal rounding requirements.\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray,\n                  kernel: np.ndarray,\n                  padding: int,\n                  stride: int):\n\n    # 1) pad the input (zero-padding on all sides)\n    if padding > 0:\n        inp = np.pad(input_matrix,\n                     pad_width=((padding, padding), (padding, padding)),\n                     mode='constant',\n                     constant_values=0)\n    else:\n        inp = input_matrix\n\n    in_h, in_w   = inp.shape\n    k_h,  k_w    = kernel.shape\n\n    # 2) compute output spatial size\n    out_h = (in_h - k_h) // stride + 1\n    out_w = (in_w - k_w) // stride + 1\n\n    # 3) perform convolution (cross-correlation, i.e. no kernel flip)\n    out = np.zeros((out_h, out_w), dtype=float)\n\n    for i in range(out_h):\n        for j in range(out_w):\n            v_start, v_end = i * stride, i * stride + k_h\n            h_start, h_end = j * stride, j * stride + k_w\n            window = inp[v_start:v_end, h_start:h_end]\n            out[i, j] = np.sum(window * kernel)\n\n    # 4) round to 4 decimal places and return as regular Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n       Rectified Linear Unit (ReLU) activation function.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n\n    Returns\n    -------\n    float\n        The input if it is greater than 0; otherwise 0.\n       return z if z > 0 else 0.0\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss (MSE + L2-penalty).\n\n    Parameters\n    ----------\n    X       : np.ndarray, shape (n_samples, n_features)\n              Feature/Design matrix.\n    w       : np.ndarray, shape (n_features,)\n              Coefficient vector.\n    y_true  : np.ndarray, shape (n_samples,)\n              Ground-truth target values.\n    alpha   : float\n              Regularisation strength (\u03bb).\n\n    Returns\n    -------\n    float\n        Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    # Ensure correct shapes / dtypes\n    X = np.asarray(X, dtype=float)\n    w = np.asarray(w, dtype=float).reshape(-1)\n    y_true = np.asarray(y_true, dtype=float).reshape(-1)\n\n    # Predicted values\n    y_pred = X @ w\n\n    # Mean Squared Error term\n    mse = np.mean((y_pred - y_true) ** 2)\n\n    # L2 (ridge) penalty term\n    l2_penalty = alpha * np.sum(w ** 2)\n\n    # Total Ridge loss\n    loss = mse + l2_penalty\n\n    # Round to the nearest 4th decimal\n    return np.round(loss, 4).item()\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float | int:\n       Compute the Leaky ReLU activation.\n\n    Parameters\n    ----------\n    z : float\n        Input value.\n    alpha : float, optional (default=0.01)\n        Slope for the negative part of the function.\n\n    Returns\n    -------\n    float | int\n        The result of applying Leaky ReLU to `z`.\n       return z if z >= 0 else alpha * z\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n       Compute the linear kernel (dot product) between two input vectors.\n\n    Parameters\n    ----------\n    x1 : array-like\n        First input vector.\n    x2 : array-like\n        Second input vector.\n\n    Returns\n    -------\n    float\n        Dot product of `x1` and `x2`.\n\n    Raises\n    ------\n    ValueError\n        If `x1` and `x2` do not have the same number of elements after flattening.\n       # Convert inputs to 1-D NumPy arrays\n    v1 = np.asarray(x1).ravel()\n    v2 = np.asarray(x2).ravel()\n\n    # Ensure both vectors have the same length\n    if v1.shape != v2.shape:\n        raise ValueErrorx1 and x2 must have the same number of elements    # Compute and return the dot product\n    return float(np.dot(v1, v2))\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision score for binary classification.\n    \n    Parameters\n    ----------\n    y_true : array-like (n_samples,)\n        Ground-truth (correct) binary labels. Must contain only 0s and 1s.\n    y_pred : array-like (n_samples,)\n        Predicted binary labels. Must contain only 0s and 1s.\n    \n    Returns\n    -------\n    float\n        Precision = TP / (TP + FP).  \n        If there are no predicted positives (TP + FP == 0) the function\n        returns 0.0 to avoid division-by-zero.\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # True positives: predicted 1 and actually 1\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    \n    # False positives: predicted 1 but actually 0\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    \n    denominator = tp + fp\n    if denominator == 0:\n        return 0.0  # No positive predictions \u2013 define precision as 0\n    \n    return tp / denominator\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations,\n                     batch_size=1, method='batch'):\n    \"\"\"\n    Gradient\u2013descent optimiser that supports:\n        \u2022 Batch Gradient Descent   (method='batch')\n        \u2022 Mini-Batch Gradient Descent (method='mini-batch')\n        \u2022 Stochastic Gradient Descent (method='stochastic' or 'sgd')\n    All updates use Mean-Squared-Error (MSE) loss.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n    y : array-like, shape (n_samples,)\n    weights : array-like, shape (n_features,) \u2013 initial coefficients\n    learning_rate : float \u2013 step size\n    n_iterations : int \u2013 number of passes over the whole data set\n    batch_size : int \u2013 size of each mini-batch (ignored for 'batch')\n    method : str \u2013  'batch', 'mini-batch', or 'stochastic' / 'sgd'\n\n    Returns\n    -------\n    list \u2013 optimised weights rounded to 4 dp\n    \"\"\"\n    # Ensure numpy arrays with correct shapes\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=float).reshape(-1)\n    w = np.asarray(weights, dtype=float).reshape(-1)  # working copy\n\n    n_samples = X.shape[0]\n\n    method = method.lower()\n    if method not in {'batch', 'mini-batch', 'minibatch',\n                      'stochastic', 'sgd'}:\n        raise ValueError(\"method must be 'batch', 'mini-batch' or 'stochastic'\")\n\n    for _ in range(n_iterations):\n\n        # ------- BATCH GRADIENT DESCENT -------------------------------------\n        if method == 'batch':\n            pred = X @ w\n            error = pred - y\n            grad = (2.0 / n_samples) * (X.T @ error)\n            w -= learning_rate * grad\n\n        # ------- STOCHASTIC GRADIENT DESCENT --------------------------------\n        elif method in {'stochastic', 'sgd'}:\n            # shuffle indices for each epoch\n            for i in np.random.permutation(n_samples):\n                xi = X[i]\n                yi = y[i]\n                error_i = xi @ w - yi\n                grad_i = 2.0 * xi * error_i          # gradient for one sample\n                w -= learning_rate * grad_i\n\n        # ------- MINI-BATCH GRADIENT DESCENT --------------------------------\n        else:   # 'mini-batch' / 'minibatch'\n            # Shuffle data at the beginning of each epoch\n            indices = np.random.permutation(n_samples)\n            X_shuf, y_shuf = X[indices], y[indices]\n\n            for start in range(0, n_samples, batch_size):\n                end = start + batch_size\n                xb = X_shuf[start:end]\n                yb = y_shuf[start:end]\n                pred_b = xb @ w\n                error_b = pred_b - yb\n                grad_b = (2.0 / xb.shape[0]) * (xb.T @ error_b)\n                w -= learning_rate * grad_b\n\n    # -------- Return weights as a Python list, rounded to 4th decimal --------\n    return np.round(w.reshape(-1), 4).tolist()\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix, tol=1e-12):\n    \"\"\"\n    Convert a matrix to Reduced Row-Echelon Form (RREF).\n\n    Parameters\n    ----------\n    matrix : (m,n) array_like\n        Input matrix (Python list of lists, tuple of tuples, NumPy ndarray \u2026).\n    tol : float, optional\n        Anything whose absolute value is <= tol is treated as 0.\n        (The default 1 \u00d7 10\u207b\u00b9\u00b2 works well for most floating-point data.)\n\n    Returns\n    -------\n    list\n        Matrix in reduced row-echelon form, returned as a (nested) Python list.\n    \"\"\"\n    # Make a working *float* copy so we never modify the caller\u2019s object\n    A = np.array(matrix, dtype=float).copy()\n    m, n = A.shape\n\n    pivot_row = 0                      # where the next pivot should be placed\n    for col in range(n):\n        if pivot_row >= m:             # ran out of rows\n            break\n\n        # ---------------------------------------------------------------\n        # 1.  Find a pivot in or below the current row in this column\n        # ---------------------------------------------------------------\n        # Index (in absolute coordinates) of the largest candidate\n        rows_below = np.arange(pivot_row, m)\n        pivot_candidates = np.abs(A[rows_below, col])\n        # Row that has the largest magnitude in current column\n        max_index = rows_below[pivot_candidates.argmax()]\n\n        # If the best we can find is \u201ctoo close\u201d to 0, no pivot here\n        if abs(A[max_index, col]) <= tol:\n            continue                   # move on to next column\n\n        # ---------------------------------------------------------------\n        # 2.  Swap so that the pivot row is exactly at `pivot_row`\n        # ---------------------------------------------------------------\n        if max_index != pivot_row:\n            A[[pivot_row, max_index]] = A[[max_index, pivot_row]]\n\n        # ---------------------------------------------------------------\n        # 3.  Scale the pivot row so that the pivot entry becomes 1\n        # ---------------------------------------------------------------\n        pivot_val = A[pivot_row, col]\n        A[pivot_row] = A[pivot_row] / pivot_val\n\n        # ---------------------------------------------------------------\n        # 4.  Use the pivot row to zero-out every other entry in this col\n        # ---------------------------------------------------------------\n        for r in range(m):\n            if r != pivot_row:\n                factor = A[r, col]\n                if abs(factor) > tol:\n                    A[r] -= factor * A[pivot_row]\n\n        # Done with this column; move to the next row for the next pivot\n        pivot_row += 1\n\n    # ---------------------------------------------------------------\n    # 5.  Anything extremely close to zero \u2192 set to exactly 0\n    #     (cosmetic, makes output cleaner)\n    # ---------------------------------------------------------------\n    A[np.abs(A) <= tol] = 0.0\n\n    return A.tolist()\n```\n\nExample usage\n-------------\n```python\n>>> mat = [[1, 2, 1, 4],\n...        [2, 3, 4, 7],\n...        [3, 5, 0, 6]]\n>>> rref(mat)\n[[1.0, 0.0, 0.0, -1.0000000000000002],\n [0.0, 1.0, 0.0, 2.0000000000000004],\n [0.0, 0.0, 1.0, 3.0000000000000004]]\n```\n\nThe function works for any rectangular matrix (including those with all-zero rows or zero pivots on the diagonal) and returns its reduced row-echelon form as a regular Python list of lists, as required."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(\n    f,                  # objective function (kept for API completeness)\n    grad,               # gradient of f\n    x0,                 # initial parameters\n    learning_rate=0.001,\n    beta1=0.9,\n    beta2=0.999,\n    epsilon=1e-8,\n    num_iterations=10   # default from the stub\n):\n       Adam (Adaptive Moment Estimation) optimizer.\n\n    Parameters\n    ----------\n    f : callable\n        Objective function to be minimized. (Not explicitly used by the routine,\n        but included to comply with the requested signature.)\n    grad : callable\n        Function that returns gradient of f evaluated at given parameters.\n    x0 : array-like\n        Initial parameter values.\n    learning_rate : float, optional\n        Step size (\u03b1 in the Adam paper).\n    beta1 : float, optional\n        Exponential decay rate for the first-moment estimates.\n    beta2 : float, optional\n        Exponential decay rate for the second-moment estimates.\n    epsilon : float, optional\n        Small term added to improve numerical stability.\n    num_iterations : int, optional\n        Number of optimization steps to perform.\n\n    Returns\n    -------\n    list\n        Optimized parameters, rounded to 4 decimal places and converted\n        to a standard Python list.\n       # Ensure parameters are NumPy array of floats\n    x = np.asarray(x0, dtype=np.float64)\n\n    # Initialize the first (m) and second (v) moment vectors\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n\n    for t in range(1, num_iterations + 1):\n        g = np.asarray(grad(x))       # Gradient at current parameters\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1.0 - beta1) * g\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1.0 - beta2) * (g ** 2)\n\n        # Compute bias-corrected moment estimates\n        m_hat = m / (1.0 - beta1 ** t)\n        v_hat = v / (1.0 - beta2 ** t)\n\n        # Parameter update\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Round to 4 decimal places and return as a standard Python list\n    return np.round(x, 4).tolist()\n```"}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(\n        X: np.array,\n        y: np.array,\n        alpha: float = 0.1,\n        learning_rate: float = 0.01,\n        max_iter: int = 1000,\n        tol: float = 1e-4\n    ) -> tuple:\n       Lasso (L1) regression solved with (sub-)gradient descent.\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_samples, n_features)\n        Input data.\n    y : ndarray, shape (n_samples,) or (n_samples, 1)\n        Target values.\n    alpha : float, default=0.1\n        L1\u2013regularisation strength.\n    learning_rate : float, default=0.01\n        Step size for gradient descent.\n    max_iter : int, default=1000\n        Maximum number of iterations.\n    tol : float, default=1e-4\n        If the L2-norm of the parameter update becomes smaller than `tol`\n        the optimisation terminates early.\n\n    Returns\n    -------\n    (w_list, b) : tuple\n        w_list : python list, the learnt weight vector rounded to 4 decimals\n        b       : float, the learnt bias term rounded to 4 decimals\n       # ----------  Preparation ----------\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64).ravel()            # ensure 1-D target\n    n_samples, n_features = X.shape\n\n    # Initialise parameters\n    w = np.zeros(n_features, dtype=np.float64)\n    b = 0.0\n\n    # ----------  Gradient-descent loop ----------\n    for _ in range(max_iter):\n        # Forward pass\n        y_pred = X @ w + b\n        error  = y_pred - y\n\n        # Gradients (sub-gradient for L1 term)\n        grad_w = (1.0 / n_samples) * (X.T @ error) + alpha * np.sign(w)\n        grad_b = (1.0 / n_samples) * np.sum(error)\n\n        # Parameter update\n        update_w = learning_rate * grad_w\n        update_b = learning_rate * grad_b\n        w -= update_w\n        b -= update_b\n\n        # Convergence check\n        if np.linalg.norm(update_w) < tol and abs(update_b) < tol:\n            break\n\n    # ----------  Output ----------\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = float(np.round(b, 4))\n    return (w_rounded, b_rounded)\n```"}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Optimal String Alignment (restricted Damerau-Levenshtein) distance.\n    \n    Parameters\n    ----------\n    source : str\n        The string that will be transformed.\n    target : str\n        The string that `source` is transformed into.\n    \n    Returns\n    -------\n    int\n        The minimum number of single-character edits\n        (insertions, deletions, substitutions, or adjacent transpositions)\n        required to change `source` into `target`.\n    \"\"\"\n    n, m = len(source), len(target)\n\n    # Trivial cases\n    if n == 0:\n        return m\n    if m == 0:\n        return n\n\n    # Distance matrix, (n+1) \u00d7 (m+1)\n    # d[i, j] = distance between source[:i] and target[:j]\n    d = np.zeros((n + 1, m + 1), dtype=int)\n\n    # Initialisation: transforming empty prefix to the other prefix\n    d[0, :] = np.arange(m + 1)\n    d[:, 0] = np.arange(n + 1)\n\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            # Cost of substituting the current pair of characters\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n\n            # Compute the three elementary edit operations\n            deletion      = d[i - 1, j]     + 1  # delete source[i-1]\n            insertion     = d[i,     j - 1] + 1  # insert target[j-1]\n            substitution  = d[i - 1, j - 1] + cost\n\n            d[i, j] = min(deletion, insertion, substitution)\n\n            # Check the transposition condition\n            if (\n                i > 1 and j > 1 and\n                source[i - 1] == target[j - 2] and\n                source[i - 2] == target[j - 1]\n            ):\n                # Transpose the adjacent characters\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + 1)\n\n    return int(d[n, m])\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate recall for binary classification.\n\n    Parameters\n    ----------\n    y_true : list or array-like\n        True labels (0 or 1).\n    y_pred : list or array-like\n        Predicted labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Recall rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorised operations\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # True Positives: predicted 1 and actually 1\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Actual positives (True Positives + False Negatives)\n    actual_positives = np.sum(y_true == 1)\n\n    # Avoid division by zero\n    if actual_positives == 0:\n        return 0.0\n\n    recall_value = tp / actual_positives\n    return round(recall_value, 3)\n```\n"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(Q, K, V):\n       Computes scaled-dot-product self-attention.\n\n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        Query, Key and Value matrices with identical shapes\n        (seq_len, depth). No batching is assumed.\n\n    Returns\n    -------\n    list\n        Attention output, rounded to 4 decimal places,\n        converted to a (nested) Python list.\n       \n    # Safety checks\n    if Q.shape != K.shape or K.shape != V.shape:\n        raise ValueErrorQ, K and V must have the same shape (seq_len, depth).    if Q.ndim != 2:\n        raise ValueErrorOnly 2-D tensors (seq_len, depth) are supported    # Scaled dot-product\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)          # (seq_len, seq_len)\n\n    # Stable row-wise softmax\n    scores_exp = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n\n    # Weighted sum of values\n    output = np.dot(attention_weights, V)           # (seq_len, depth)\n\n    # Round to 4 decimals\n    output = np.round(output, 4)\n\n    # Convert to plain Python list\n    return output.tolist()\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(\n    input_sequence: list[list[float]],\n    initial_hidden_state: list[float],\n    Wx: list[list[float]],\n    Wh: list[list[float]],\n    b: list[float]\n) -> list[float]:\n    \"\"\"\n    Runs a simple RNN over an input sequence and returns the final hidden state.\n\n    Parameters\n    ----------\n    input_sequence : list[list[float]]\n        Sequence of input vectors (length T, each of size input_dim).\n    initial_hidden_state : list[float]\n        Initial hidden state vector (size hidden_dim).\n    Wx : list[list[float]]\n        Weight matrix for input-to-hidden connections (shape hidden_dim \u00d7 input_dim).\n    Wh : list[list[float]]\n        Weight matrix for hidden-to-hidden connections (shape hidden_dim \u00d7 hidden_dim).\n    b : list[float]\n        Bias vector (size hidden_dim).\n\n    Returns\n    -------\n    list[float]\n        Final hidden state rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to NumPy arrays for vectorized math\n    h_t = np.asarray(initial_hidden_state, dtype=float)\n    Wx = np.asarray(Wx, dtype=float)\n    Wh = np.asarray(Wh, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    for x_t in input_sequence:\n        x_t = np.asarray(x_t, dtype=float)\n        h_t = np.tanh(Wx @ x_t + Wh @ h_t + b)  # RNN cell update\n\n    # Round to 4 decimal places and return as Python list\n    return np.round(h_t, 4).tolist()\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n       Apply a 2-D translation to a collection of points.\n\n    Parameters\n    ----------\n    points : list or ndarray, shape (n_points, 2)\n        Each element/pair represents an (x, y) coordinate.\n    tx : float or int\n        Translation distance along the x-axis.\n    ty : float or int\n        Translation distance along the y-axis.\n\n    Returns\n    -------\n    list\n        New list of translated points, obtained with NumPy and converted\n        back via `.tolist()`.\n       # Convert input to NumPy array and validate shape\n    pts = np.asarray(points, dtype=float)\n    if pts.size == 0:          # handle empty list input\n        return []\n    if pts.ndim != 2 or pts.shape[1] != 2:\n        raise ValueError`points` must be a 2-D array-like with shape (n_points, 2).    # 1. Build the 3\u00d73 translation matrix in homogeneous coordinates\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1 ]\n    ], dtype=float)\n\n    # 2. Convert the 2-D points to homogeneous coordinates (append a 1)\n    ones = np.ones((pts.shape[0], 1), dtype=float)\n    homogeneous_pts = np.hstack((pts, ones))          # shape -> (n_points, 3)\n\n    # 3. Apply the translation: P' = P \u00b7 T^T\n    translated_homogeneous = homogeneous_pts @ translation_matrix.T\n\n    # 4. Drop the homogeneous component and return as a Python list\n    translated_pts = translated_homogeneous[:, :2]     # keep x' and y'\n    return translated_pts.tolist()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n       Kullback\u2013Leibler divergence  D_KL( P || Q )  between two univariate normal\n    distributions\n    \n        P ~ N(mu_p , sigma_p^2)\n        Q ~ N(mu_q , sigma_q^2)\n\n    Parameters\n    ----------\n    mu_p, mu_q : float or array-like\n        Means of the two distributions.\n    sigma_p, sigma_q : float or array-like\n        Standard deviations ( > 0 ) of the two distributions.\n\n    Returns\n    -------\n    float or ndarray\n        KL divergence.  If the inputs are scalars a float is returned; otherwise\n        NumPy broadcasting rules apply and an array is returned.\n       # Convert to ndarray for possible broadcasting\n    mu_p   = np.asarray(mu_p,   dtype=float)\n    mu_q   = np.asarray(mu_q,   dtype=float)\n    sigma_p = np.asarray(sigma_p, dtype=float)\n    sigma_q = np.asarray(sigma_q, dtype=float)\n\n    if np.any(sigma_p <= 0) or np.any(sigma_q <= 0):\n        raise ValueErrorStandard deviations must be positive    # KL divergence formula for univariate normals\n    ratio = sigma_q / sigma_p\n    term1 = np.log(ratio)                       # log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    kl = term1 + term2 - 0.5\n\n    # If all inputs were scalars, return a Python float\n    return kl.item() if kl.ndim == 0 else kl\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n       Solve the linear system Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters\n    ----------\n    A : (m, m) array_like\n        Coefficient matrix.\n    b : (m,) array_like\n        Right-hand-side vector.\n    n : int\n        Number of iterations to perform.\n    x_ini : (m,) array_like, optional\n        Initial guess for the solution.  If None, a zero vector is used.\n\n    Returns\n    -------\n    list\n        Approximated solution vector after `n` iterations, rounded to 4 decimals.\n       # Convert inputs to numpy arrays of float type\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()\n\n    m = A.shape[0]\n    if A.shape[0] != A.shape[1]:\n        raise ValueErrorMatrix A must be square    if b.size != m:\n        raise ValueErrorDimension mismatch between A and b    # Initial guess\n    if x_ini is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.asarray(x_ini, dtype=float).flatten()\n        if x.size != m:\n            raise ValueErrorInitial guess has wrong length    # Main Gauss-Seidel iteration\n    for _ in range(int(n)):\n        x_old = x.copy()\n        for i in range(m):\n            # Using the latest (updated) values for j < i\n            sum1 = np.dot(A[i, :i], x[:i])\n\n            # Using values from the previous iteration for j > i\n            sum2 = np.dot(A[i, i + 1 :], x_old[i + 1 :])\n\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n\n    # Round to 4 decimal places\n    x = np.round(x, 4)\n\n    # Return as a regular Python list\n    return x.tolist()\n```"}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve a system of linear equations  Ax = b  using Gaussian\n    Elimination with partial pivoting.\n\n    Parameters\n    ----------\n    A :  (n, n) array-like  \u2013  coefficient matrix\n    b :  (n,)  or  (n,1) array-like  \u2013  right-hand-side vector\n\n    Returns\n    -------\n    list\n        Solution vector rounded to 4 decimal places.\n    \"\"\"\n    # --- Prepare data -------------------------------------------------------\n    A = np.array(A, dtype=float)          # work on floating-point copy\n    b = np.array(b, dtype=float).ravel()  # 1-D view of RHS\n    n = A.shape[0]\n\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    if b.size != n:\n        raise ValueError(\"Incompatible dimensions between A and b.\")\n\n    # --- Forward elimination (with partial pivoting) -----------------------\n    for i in range(n):\n        # 1. Pivot: find the row with the largest absolute value in column i\n        pivot_row = i + np.argmax(np.abs(A[i:, i]))\n        if np.isclose(A[pivot_row, i], 0):\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n\n        # 2. Swap rows in both A and b if necessary\n        if pivot_row != i:\n            A[[i, pivot_row]] = A[[pivot_row, i]]\n            b[i], b[pivot_row] = b[pivot_row], b[i]\n\n        # 3. Eliminate entries below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n            b[j]     -= factor * b[i]\n\n    # --- Backward substitution --------------------------------------------\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        s = np.dot(A[i, i + 1 :], x[i + 1 :])\n        x[i] = (b[i] - s) / A[i, i]\n\n    # --- Round to 4 decimal places & return as Python list -----------------\n    return np.round(x, 4).tolist()\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef _sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size  = input_size\n        self.hidden_size = hidden_size\n\n        # Xavier/Glorot style scaling for slightly better behaved random init\n        scale = 1.0 / np.sqrt(input_size + hidden_size)\n\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * scale\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * scale\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * scale\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * scale\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n               Parameters\n        ----------\n        x  : array-like, shape (seq_len, input_size)\n             Input sequence where x[t] is the input vector at time-step t\n        initial_hidden_state : array-like, shape (hidden_size,)\n        initial_cell_state   : array-like, shape (hidden_size,)\n\n        Returns\n        -------\n        hidden_sequence : python list of shape (seq_len, hidden_size)\n        final_hidden    : python list of shape (hidden_size,)\n        final_cell      : python list of shape (hidden_size,)\n        (all rounded to 4 decimal places)\n               x  = np.asarray(x, dtype=float)\n        h  = np.asarray(initial_hidden_state, dtype=float).reshape(self.hidden_size, 1)\n        c  = np.asarray(initial_cell_state,   dtype=float).reshape(self.hidden_size, 1)\n\n        seq_len = x.shape[0]\n        hidden_collector = []\n\n        for t in range(seq_len):\n            x_t = x[t].reshape(self.input_size, 1)\n\n            # Concatenate h_{t-1} and x_t\n            concat = np.vstack((h, x_t))\n\n            f_t = _sigmoid(self.Wf @ concat + self.bf)     # forget gate\n            i_t = _sigmoid(self.Wi @ concat + self.bi)     # input  gate\n            g_t = np.tanh( self.Wc @ concat + self.bc)     # candidate cell\n            o_t = _sigmoid(self.Wo @ concat + self.bo)     # output  gate\n\n            c = f_t * c + i_t * g_t                        # new cell state\n            h = o_t * np.tanh(c)                           # new hidden state\n\n            hidden_collector.append(h.copy())\n\n        # Stack hidden states to shape (seq_len, hidden_size)\n        hidden_sequence = np.hstack(hidden_collector).T\n\n        # Rounding and conversion to python lists as required\n        hidden_sequence = np.round(hidden_sequence, 4).tolist()\n        final_hidden    = np.round(h.reshape(-1), 4).tolist()\n        final_cell      = np.round(c.reshape(-1), 4).tolist()\n\n        return hidden_sequence, final_hidden, final_cell\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute a TF-IDF matrix for the words in *query* against the *corpus*.\n\n    Parameters\n    ----------\n    corpus : List[List[str]]\n        Each inner list is a tokenised document.\n    query  : List[str]\n        The terms for which the TF-IDF scores are required.\n\n    Returns\n    -------\n    List[List[float]]\n        Shape  = (len(corpus) , len(query)).\n        Values = TF-IDF scores rounded to 4 d.p.\n    \"\"\"\n\n    # --------------- 1.  Empty-corpus guard ----------------\n    if not corpus:                         #  [], None, etc.\n        return []                          #  or raise ValueError\n\n    N_docs = len(corpus)                   #  total number of documents\n\n    # --------------- 2.  Document-frequency (df) ---------\n    # df[t] = number of documents that contain term t\n    df = {t: 0 for t in query}\n    for doc in corpus:\n        seen = set(doc)\n        for t in df.keys():\n            if t in seen:\n                df[t] += 1\n\n    # --------------- 3.  Inverse document frequency (idf)-\n    # Smoothing (+1 to both numerator and denominator) avoids /0.\n    # idf[t] = ln( (N_docs + 1) / (df[t] + 1) ) + 1\n    idf = {t: np.log((N_docs + 1) / (df[t] + 1)) + 1 for t in query}\n\n    # --------------- 4.  TF-IDF per document -------------\n    rows = []                               # will become the 2-D result\n    for doc in corpus:\n        L = len(doc)                        # document length\n        # pre-compute raw term frequencies for the current document\n        tf_raw = {}\n        for w in doc:\n            tf_raw[w] = tf_raw.get(w, 0) + 1\n\n        row = []\n        for t in query:\n            # term-frequency: normalised by document length\n            tf = tf_raw.get(t, 0) / L if L else 0.0\n            row.append(tf * idf[t])\n        rows.append(row)\n\n    # --------------- 5.  Rounding / return ---------------\n    return np.round(np.array(rows), 4).tolist()\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array (or list-like) of true binary labels (0 / 1)\n    :param y_pred: Numpy array (or list-like) of predicted binary labels (0 / 1)\n    :param beta:   Non-negative float, the weight of recall relative to precision\n    :return:       F-Score rounded to three decimal places\n    \"\"\"\n    # Ensure numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    # Confusion\u2013matrix components\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    # Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n\n    # Handle the special case beta == 0 (returns precision)\n    if beta == 0:\n        return round(precision, 3)\n\n    beta_sq = beta ** 2\n\n    # If both precision and recall are zero the F-score is zero\n    if precision == 0 and recall == 0:\n        f = 0.0\n    else:\n        f = (1 + beta_sq) * precision * recall / (beta_sq * precision + recall)\n\n    return round(f, 3)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\n# ------------ 1.  Initialisation --------------------------------------------\ndef initialize_weights(input_size: int, hidden_size: int, output_size: int):\n    \"\"\"\n    Create and return the parameter matrices / vectors of a vanilla RNN:\n        h_t = tanh(W_xh x_t + W_hh h_{t-1} + b_h)\n        y_t =       W_hy h_t                 + b_y\n    Weights are small random numbers ( *=0.01 ), biases are 0.\n    \"\"\"\n    W_xh = 0.01 * np.random.randn(hidden_size, input_size)   # (H, I)\n    W_hh = 0.01 * np.random.randn(hidden_size, hidden_size)  # (H, H)\n    W_hy = 0.01 * np.random.randn(output_size, hidden_size)  # (O, H)\n\n    b_h  = np.zeros(hidden_size)     # (H,)\n    b_y  = np.zeros(output_size)     # (O,)\n    return W_xh, W_hh, W_hy, b_h, b_y\n\n\n# ------------ 2.  Forward pass ---------------------------------------------\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y,\n                hidden_size: int,\n                input_sequence):\n    \"\"\"\n    Iterate over the input_sequence and return:\n      outputs       \u2013 list with y_t for every time-step\n      last_inputs   \u2013 cached x_t  per step   (for BPTT)\n      last_hiddens  \u2013 cached h_t  per step   (for BPTT)\n    \"\"\"\n    h_prev = np.zeros(hidden_size)              # h_{-1}\n    outputs, last_inputs, last_hiddens = [], [], []\n\n    for x_t in input_sequence:\n        x_t = np.asarray(x_t)                   # ensure numpy array\n        last_inputs.append(x_t)\n\n        h_t = np.tanh(\n            np.dot(W_xh, x_t) +                # W_xh \u00b7 x_t\n            np.dot(W_hh, h_prev) +             # W_hh \u00b7 h_{t-1}\n            b_h                                # + bias\n        )\n        last_hiddens.append(h_t)\n\n        y_t = np.dot(W_hy, h_t) + b_y\n        outputs.append(y_t)\n\n        h_prev = h_t                           # move to next step\n\n    return outputs, last_inputs, last_hiddens\n\n\n# ------------ 3.  Back-propagation Through Time ----------------------------\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y,\n                 hidden_size: int,\n                 input_sequence,\n                 expected_output,\n                 outputs, last_inputs, last_hiddens,\n                 learning_rate: float):\n    \"\"\"\n    Classic BPTT for a vanilla RNN with \u00bd\u00b7MSE loss summed over all time-steps.\n    Gradients are accumulated over the whole sequence, then a SGD step is done.\n    \"\"\"\n    # Gradient accumulators (same shapes as params)\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h  = np.zeros_like(b_h)\n    db_y  = np.zeros_like(b_y)\n\n    dh_next = np.zeros(hidden_size)\n\n    T = len(input_sequence)\n    # Iterate backwards in time\n    for t in reversed(range(T)):\n        y_t   = outputs[t]\n        y_exp = np.asarray(expected_output[t])\n\n        # dL/dy  : derivative of \u00bd\u00b7(y-y*)\u00b2  is  (y-y*)\n        dy = (y_t - y_exp)\n\n        # Gradients for output layer\n        dW_hy += np.outer(dy, last_hiddens[t])   # (O,H)\n        db_y  += dy                              # (O,)\n\n        # Propagate into the hidden layer\n        dh = np.dot(W_hy.T, dy) + dh_next        # add upstream gradient\n\n        # tanh\u2019(z) = 1-tanh\u00b2(z).  last_hiddens[t] already equals tanh(z)\n        dh_raw = (1 - last_hiddens[t]**2) * dh   # (H,)\n\n        # Gradients for recurrent + input weights\n        dW_xh += np.outer(dh_raw, last_inputs[t])         # (H,I)\n        h_prev = last_hiddens[t-1] if t > 0 else np.zeros(hidden_size)\n        dW_hh += np.outer(dh_raw, h_prev)                 # (H,H)\n        db_h  += dh_raw                                   # (H,)\n\n        dh_next = np.dot(W_hh.T, dh_raw)                  # to previous step\n\n    # (Optional) clip to avoid exploding grads\n    for grad in (dW_xh, dW_hh, dW_hy, db_h, db_y):\n        np.clip(grad, -1, 1, out=grad)\n\n    # Parameter update (SGD)\n    W_xh -= learning_rate * dW_xh\n    W_hh -= learning_rate * dW_hh\n    W_hy -= learning_rate * dW_hy\n    b_h  -= learning_rate * db_h\n    b_y  -= learning_rate * db_y\n\n    return W_xh, W_hh, W_hy, b_h, b_y\n\n\n# ------------ 4.  Public entry-point ---------------------------------------\ndef entrypoint(input_sequence,\n               expected_output,\n               input_size,\n               hidden_size,\n               output_size,\n               learning_rate=0.01,\n               epochs=100,\n               random_seed=42):\n\n    np.random.seed(random_seed)  # For reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(\n        input_size, hidden_size, output_size)\n\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(\n            W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(\n            W_xh, W_hh, W_hy, b_h, b_y, hidden_size,\n            input_sequence, expected_output,\n            outputs, last_inputs, last_hiddens,\n            learning_rate)\n\n    return np.round(np.array(outputs), 5).tolist()\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array = None, tol: float = 1e-8):\n       Solve the system Ax = b using the Conjugate Gradient method.\n\n    Parameters\n    ----------\n    A : np.array\n        Symmetric positive\u2013definite matrix of shape (m, m).\n    b : np.array\n        Right\u2013hand-side vector of length m.\n    n : int\n        Maximum number of iterations.\n    x0 : np.array, optional\n        Initial guess (default is the zero vector).\n    tol : float\n        Convergence tolerance on the \u21132-norm of the residual.\n\n    Returns\n    -------\n    list\n        Solution vector rounded to 8 decimal places.\n       # Ensure inputs are NumPy arrays with float dtype\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    # Initial guess\n    if x0 is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.asarray(x0, dtype=float).copy()\n\n    # Initial residual and direction\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old) < tol:          # Already converged\n        return np.round(x, 8).tolist()\n\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n\n        x += alpha * p          # Update solution\n        r -= alpha * Ap         # Update residual\n\n        rs_new = np.dot(r, r)\n\n        # Convergence check\n        if np.sqrt(rs_new) < tol:\n            break\n\n        beta = rs_new / rs_old  # Direction scale factor\n        p = r + beta * p        # New search direction\n        rs_old = rs_new         # Prepare for next iteration\n\n    # Round the result to 8 decimal places and convert to list\n    return np.round(x, 8).tolist()\n```"}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Empty list \u21d2 no impurity (convention)\n    if not y:\n        return 0.0\n\n    # Convert to NumPy array for convenience\n    y_arr = np.asarray(y)\n\n    # Count occurrences of each unique class\n    _, counts = np.unique(y_arr, return_counts=True)\n\n    # Convert counts to probabilities\n    probs = counts / counts.sum()\n\n    # Gini formula: 1 - \u03a3 p_i\u00b2\n    impurity = 1.0 - np.sum(probs ** 2)\n\n    # Round to three decimals as requested\n    return round(float(impurity), 3)\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    CSR representation consists of three one-dimensional arrays:\n        1. values        \u2013 all non-zero elements row by row\n        2. col_indices   \u2013 column index of every stored value\n        3. row_ptr       \u2013 index in `values` where each row starts\n                           (length = n_rows + 1, row_ptr[0] == 0)\n\n    :param dense_matrix: 2-D list (list of lists) representing a dense matrix\n    :return: tuple (values, col_indices, row_ptr)\n    \"\"\"\n    # Guard against an empty matrix\n    if not dense_matrix:\n        return [], [], [0]\n\n    values = []          # Non\u2013zero values\n    col_indices = []     # Corresponding column indices\n    row_ptr = [0]        # Row pointer (starts with 0)\n\n    nnz = 0  # Cumulative count of non-zeros encountered so far\n    for row in dense_matrix:\n        for col_idx, element in enumerate(row):\n            if element != 0:\n                values.append(element)\n                col_indices.append(col_idx)\n                nnz += 1\n        row_ptr.append(nnz)\n\n    return values, col_indices, row_ptr\n```\n\nExample\n```\ndense = [[0, 0, 1],\n         [2, 0, 0],\n         [0, 3, 4]]\n\nvals, cols, rptr = compressed_row_sparse_matrix(dense)\nprint(vals)   # [1, 2, 3, 4]\nprint(cols)   # [2, 0, 1, 2]\nprint(rptr)   # [0, 1, 2, 4]\n```\n"}
{"task_id": 66, "completion_id": 0, "completion": "```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list/tuple of numbers)\n    :param L: The line vector defining the direction of projection (list/tuple of numbers)\n    :return: List representing the projection of v onto L, each component\n             rounded to three decimal places.\n    \"\"\"\n    # --- Basic validation ----------------------------------------------------\n    if not v or not L:\n        raise ValueError(\"Both v and L must be non-empty sequences.\")\n    if len(v) != len(L):\n        raise ValueError(\"v and L must have the same dimension.\")\n\n    # --- Compute scalar coefficient -----------------------------------------\n    # Projection formula: proj_L(v) = [(v\u00b7L)/(L\u00b7L)] * L\n    dot_vL   = sum(vi * li for vi, li in zip(v, L))\n    dot_LL   = sum(li * li for li in L)\n    if dot_LL == 0:\n        raise ValueError(\"Line vector L must not be the zero vector.\")\n\n    coeff = dot_vL / dot_LL\n\n    # --- Build the projection vector ----------------------------------------\n    proj = [round(coeff * li, 3) for li in L]\n\n    # Convert small rounded values like -0.0 to 0.0 (cosmetic)\n    proj = [0.0 if abs(x) < 1e-12 else x for x in proj]\n    return proj\n```\n\nExample usage:\n\n```python\n>>> v = [3, 4]\n>>> L = [1, 2]\n>>> orthogonal_projection(v, L)\n[2.2, 4.4]\n```\n\nThe function works for vectors of any dimension, performs basic input checks, and returns each component rounded to three decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n       Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: list[list[Any]]\n        The dense matrix to be converted. All rows must have the same length.\n    :returns: tuple[list, list[int], list[int]]\n        (values, row_indices, column_pointer)\n            values          \u2013 non-zero elements in column-major order\n            row_indices     \u2013 corresponding row indices for each value\n            column_pointer  \u2013 pointers such that values for column j are found in\n                              indices column_pointer[j] \u2026 column_pointer[j+1]-1\n       # Handle an empty (0\u00d70) matrix quickly\n    if not dense_matrix or not dense_matrix[0]:\n        return [], [], [0]\n\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n\n    values = []          # non-zero values\n    row_indices = []     # their row indices\n    col_ptr = [0]        # starting index of each column in 'values'\n\n    # Iterate column by column (CSC is column-major)\n    for col in range(n_cols):\n        for row in range(n_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        # After finishing the column, append the current length of `values`\n        col_ptr.append(len(values))\n\n    return values, row_indices, col_ptr\n\n\n# ---------------------------------------------------------------------\n# Quick self-test\nif __name__ ==main    mat = [\n        [0, 5, 0, 0],\n        [7, 0, 0, 0],\n        [0, 0, 3, 0],\n        [0, 2, 6, 0],\n    ]\n\n    v, r, c = compressed_col_sparse_matrix(mat)\n    printvalues        v)\n    printrow_indices   r)\n    printcolumn_pointer c)\n    # Expected:\n    # values         -> [7, 5, 2, 3, 6]\n    # row_indices    -> [1, 0, 3, 2, 3]\n    # column_pointer -> [0, 1, 3, 5, 5]\n```\n"}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n       Returns a list of column-vectors (kept as columns of one matrix) that form a basis\n    for the column space (image) of the input matrix A.\n\n    Parameters\n    ----------\n    A : array_like (m \u00d7 n)\n        Real-valued matrix.  It can be a Python list of lists or a NumPy array.\n\n    Returns\n    -------\n    list\n        A list-of-lists representation of an m \u00d7 r matrix whose r columns are the\n        independent columns of A (i.e. a basis of the column space), rounded to\n        8 decimal places.  If the column space is {0} the function returns [].\n       # Convert to a mutable float array\n    A = np.asarray(A, dtype=float)\n    m, n = A.shape\n    if m == 0 or n == 0:\n        return []\n\n    # Copy: we will perform elimination on B, while remembering pivot columns\n    B = A.copy()\n    tol        = 1e-12          # numerical tolerance\n    pivot_cols = []             # indices of independent (pivot) columns\n    row        = 0              # current pivot row\n\n    # Gaussian elimination to Row-Echelon Form, recording pivot columns\n    for col in range(n):\n        if row >= m:            # no more pivot rows possible\n            break\n\n        # Find the best pivot in this column (partial pivoting)\n        pivot_row = np.argmax(np.abs(B[row:, col])) + row\n        pivot_val = B[pivot_row, col]\n\n        # Skip tiny pivots\n        if abs(pivot_val) < tol:\n            continue\n\n        # Put pivot in the current row\n        if pivot_row != row:\n            B[[row, pivot_row], :] = B[[pivot_row, row], :]\n\n        # Record this pivot column\n        pivot_cols.append(col)\n\n        # Eliminate entries below the pivot\n        for r in range(row + 1, m):\n            factor = B[r, col] / B[row, col]\n            if abs(factor) > tol:\n                B[r, col:] -= factor * B[row, col:]\n\n        row += 1                                                    # move to next pivot row\n\n    # Extract the corresponding independent columns from the original matrix\n    if not pivot_cols:                                              # rank 0 \u21d2 zero column space\n        return []\n\n    basis = A[:, pivot_cols]                                        # m \u00d7 rank matrix\n    basis = np.round(basis, 8)                                      # round as requested\n    return basis.tolist()\n```"}
{"task_id": 69, "completion_id": 0, "completion": "Here\u2019s a compact, self-contained implementation that fulfils the requirements.\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the coefficient of determination (R-squared).\n\n    Parameters\n    ----------\n    y_true : array-like\n        Actual target values.\n    y_pred : array-like\n        Predicted target values.\n\n    Returns\n    -------\n    float\n        R-squared value rounded to three decimal places.\n    \"\"\"\n    # Convert to NumPy arrays (allows lists, tuples, Series, etc.)\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Basic validation\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n\n    # Residual sum of squares\n    ss_res = np.sum((y_true - y_pred) ** 2)\n\n    # Total sum of squares\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n\n    # If variance is zero (all y_true identical), R\u00b2 is not defined;\n    # here we return 0.0 by convention.\n    if ss_tot == 0:\n        return 0.0\n\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\n```\n\nThe function returns the R-squared value rounded to three decimal places, as requested."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n       Calculate the average brightness of a grayscale image.\n\n    Parameters\n    ----------\n    img : list[list[Number]]\n        2-D matrix where each inner list represents a row of pixel values\n        in the inclusive range [0, 255].\n\n    Returns\n    -------\n    float\n        Average brightness rounded to two decimal places, or\n        -1 for any invalid input/edge\u2010case.\n       # 1. Overall structure must be a non-empty list\n    if not isinstance(img, list) or len(img) == 0:\n        return -1\n\n    expected_len = None          # will hold the length of the first row\n    total = 0.0                  # running sum of pixel values\n    count = 0                    # total number of pixels\n\n    for row in img:\n        # Each row must itself be a list\n        if not isinstance(row, list):\n            return -1\n\n        # All rows must have the same length\n        if expected_len is None:\n            expected_len = len(row)\n            if expected_len == 0:           # empty first row \u2192 invalid\n                return -1\n        elif len(row) != expected_len:\n            return -1\n\n        # Process each pixel\n        for pixel in row:\n            # Pixel must be numeric and within 0-255 inclusive\n            if not isinstance(pixel, (int, float)) or pixel < 0 or pixel > 255:\n                return -1\n            total += pixel\n            count += 1\n\n    # No pixels at all (e.g. [[]]) \u2192 invalid\n    if count == 0:\n        return -1\n\n    average = total / count\n    return round(average, 2)\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n       Compute the Root Mean Square Error (RMSE) between two sequences.\n\n    Parameters\n    ----------\n    y_true : array-like\n        Ground-truth (correct) target values.\n    y_pred : array-like\n        Estimated target values.\n\n    Returns\n    -------\n    float\n        RMSE rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        If the inputs are empty, have mismatched shapes/sizes, or contain\n        non-numeric data that cannot be converted to floats.\n       # ---------- 1. Convert inputs to numpy arrays ----------\n    try:\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n    except (TypeError, ValueError):\n        raise ValueErrorInputs must be array-like and convertible to floats    # ---------- 2. Basic validations ----------\n    if y_true_arr.size == 0 or y_pred_arr.size == 0:\n        raise ValueErrorEmpty input: both y_true and y_pred must contain data    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueErrorShape mismatch: y_true and y_pred must have the same shape    # ---------- 3. Compute RMSE ----------\n    diff = y_true_arr - y_pred_arr\n    mse = np.mean(np.square(diff))\n    rmse_val = np.sqrt(mse)\n\n    # ---------- 4. Return rounded result ----------\n    return round(float(rmse_val), 3)\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n       Compute the Jaccard Index (a.k.a. Intersection over Union) for two binary\n    label vectors.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        Ground-truth binary labels (0 or 1).\n    y_pred : array-like of shape (n_samples,)\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Jaccard Index rounded to three decimal places.\n       # Convert inputs to NumPy boolean arrays\n    y_true = np.asarray(y_true).astype(bool)\n    y_pred = np.asarray(y_pred).astype(bool)\n\n    # Intersection and union\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union        = np.logical_or (y_true, y_pred).sum()\n\n    # Handle edge cases\n    if union == 0:        # both vectors are all zeros\n        return 1.000      # perfect overlap of two empty sets\n    if intersection == 0: # no common positives\n        return 0.000\n\n    # Standard Jaccard calculation\n    jaccard = intersection / union\n    return round(jaccard, 3)\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n       Compute the S\u00f8rensen\u2013Dice coefficient (a.k.a. Dice Score / F1-score)\n    for binary classification.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        Ground-truth binary labels (0 or 1).\n    y_pred : array-like of shape (n_samples,)\n        Predicted binary labels (0 or 1).\n\n    Returns\n    -------\n    float\n        Dice Score rounded to 3 decimal places.\n       # Convert to NumPy boolean arrays\n    y_true = np.asarray(y_true, dtype=bool)\n    y_pred = np.asarray(y_pred, dtype=bool)\n\n    if y_true.shape != y_pred.shape:\n        raise ValueErrory_true and y_pred must have the same shape    # Intersection and cardinalities\n    intersection = np.logical_and(y_true, y_pred).sum()\n    positives_true = y_true.sum()\n    positives_pred = y_pred.sum()\n\n    # Edge case: no positives in either array \u2192 perfect overlap of two empty sets\n    if positives_true + positives_pred == 0:\n        return 1.0\n\n    dice = 2 * intersection / (positives_true + positives_pred)\n    return round(float(dice), 3)\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport hashlib\n\n\ndef _stable_hash(value):\n    \"\"\"\n    Turn *any* python object into a 32-bit unsigned integer that is stable\n    across different interpreter sessions.\n    \"\"\"\n    h = hashlib.md5(str(value).encode()).hexdigest()\n    return int(h, 16) & 0xFFFFFFFF  # 32-bit\n\n\ndef _random_hv(dim, seed):\n    \"\"\"\n    Generate a reproducible bipolar (\u22121 / +1) hypervector of dimensionality\n    `dim` using `seed`.\n    \"\"\"\n    rs = np.random.RandomState(seed & 0xFFFFFFFF)          # make sure seed fits\n    # randint(0, 2)  -> {0,1}.  Map {0,1} \u2192 {-1, +1}\n    return rs.randint(0, 2, size=dim, dtype=np.int8) * 2 - 1\n\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite/bundled hypervector for `row`.\n    \n    Parameters\n    ----------\n    row : dict\n        {feature_name : feature_value}\n    dim : int\n        Dimensionality of each hypervector.\n    random_seeds : dict\n        {feature_name : int_seed}.  The same seed is re-used for every call\n        to guarantee reproducibility.\n    \n    Returns\n    -------\n    list\n        Bipolar (+1/-1) composite hypervector encoded as a Python list.\n    \"\"\"\n    # Running sum for bundling\n    bundle = np.zeros(dim, dtype=np.int32)\n\n    for feat, val in row.items():\n        # ------------------------------------------------------------------ #\n        # 1.  Hypervector for the *feature name*\n        # ------------------------------------------------------------------ #\n        if feat not in random_seeds:\n            # If no explicit seed was provided, create one deterministically\n            random_seeds[feat] = _stable_hash(feat)\n\n        feat_seed = random_seeds[feat]\n        feat_hv = _random_hv(dim, feat_seed)\n\n        # ------------------------------------------------------------------ #\n        # 2.  Hypervector for the *feature value*\n        #     Same feature seed combined with a stable hash of the value so\n        #     that   (feature, value)  pairs always receive the same vector,\n        #     but different values get different vectors.\n        # ------------------------------------------------------------------ #\n        val_seed = feat_seed ^ _stable_hash(val)            # xor keeps seed size\n        val_hv = _random_hv(dim, val_seed)\n\n        # ------------------------------------------------------------------ #\n        # 3.  Bind feature and value (element-wise multiplication)\n        # ------------------------------------------------------------------ #\n        bound_hv = feat_hv * val_hv\n\n        # ------------------------------------------------------------------ #\n        # 4.  Bundle (element-wise addition)\n        # ------------------------------------------------------------------ #\n        bundle += bound_hv.astype(np.int32)\n\n    # ---------------------------------------------------------------------- #\n    # 5.  Majority vote \u2011> final bipolar vector (ties go to +1)\n    # ---------------------------------------------------------------------- #\n    composite_hv = np.where(bundle >= 0, 1, -1).astype(np.int8)\n\n    return composite_hv.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n       Build the 2 \u00d7 2 confusion-matrix for a binary-classification task.\n\n    Parameters\n    ----------\n    data : list[list[Any, Any]]\n        Every inner list/tuple holds a ground-truth label (y_true)\n        followed by the predicted label (y_pred).\n\n    Returns\n    -------\n    list[list[int]]\n        [[TN, FP],\n         [FN, TP]]\n       # Empty input \u21d2 zero matrix\n    if not data:\n        return [[0, 0],\n                [0, 0]]\n\n    # Identify the two class labels *present in y_true*\n    labels = {row[0] for row in data}\n    if len(labels) != 2:\n        raise ValueErrorExpected exactly two distinct classes in y_true    neg_label, pos_label = sorted(labels)    # Conventional ordering: smaller first\n\n    # Count every (y_true, y_pred) pair\n    pair_counts = Counter(tuple(row) for row in data)\n\n    TN = pair_counts.get((neg_label, neg_label), 0)   # true negatives\n    FP = pair_counts.get((neg_label, pos_label), 0)   # false positives\n    FN = pair_counts.get((pos_label, neg_label), 0)   # false negatives\n    TP = pair_counts.get((pos_label, pos_label), 0)   # true positives\n\n    return [[TN, FP],\n            [FN, TP]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n       Compute the cosine similarity between two vectors.\n\n    Parameters\n    ----------\n    v1, v2 : numpy.ndarray\n        Input vectors of identical shape.\n\n    Returns\n    -------\n    float\n        Cosine similarity rounded to three decimal places.\n\n    Raises\n    ------\n    ValueError\n        If the vectors have different shapes, are empty, or have zero magnitude.\n       # Ensure input is ndarray\n    v1 = np.asarray(v1, dtype=float)\n    v2 = np.asarray(v2, dtype=float)\n\n    # 1. Shape check\n    if v1.shape != v2.shape:\n        raise ValueErrorInput vectors must have the same shape    # 2. Empty check\n    if v1.size == 0:\n        raise ValueErrorInput vectors must not be empty    # 3. Magnitude (norm) check\n    norm1 = np.linalg.norm(v1)\n    norm2 = np.linalg.norm(v2)\n    if norm1 == 0 or norm2 == 0:\n        raise ValueErrorInput vectors must not have zero magnitude    # 4. Cosine similarity\n    dot_product = np.dot(v1, v2)\n    cosine_sim = dot_product / (norm1 * norm2)\n\n    # 5. Round to three decimals and return plain float\n    return float(np.round(cosine_sim, 3))\n```\n\nExample usage\n```python\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nprint(cosine_similarity(a, b))  # 0.974\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[list, float, float, float, float]:\n       Computes confusion matrix, accuracy, F1-score, specificity and\n    negative predictive value for a binary classifier.\n    \n    Parameters\n    ----------\n    actual     : list[int]\n        Ground-truth labels  (1 = positive, 0 = negative)\n    predicted  : list[int]\n        Model-predicted labels (1 = positive, 0 = negative)\n    \n    Returns\n    -------\n    tuple\n        (\n            confusion_matrix : [[TN, FP], [FN, TP]],\n            accuracy               : float (rounded to 3 decimals),\n            f1_score               : float (rounded to 3 decimals),\n            specificity            : float (rounded to 3 decimals),\n            negative_predictive_value : float (rounded to 3 decimals)\n        )\n       # --- Basic validation ----------------------------------------------------\n    if len(actual) != len(predicted):\n        raise ValueError`actual` and `predicted` must have the same length    if not all(x in (0, 1) for x in actual + predicted):\n        raise ValueErrorAll elements in `actual` and `predicted` must be 0 or 1    # --- Count the four cells of the confusion matrix ------------------------\n    TP = TN = FP = FN = 0\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 1 and p == 0:\n            FN += 1\n\n    # Confusion matrix in scikit-learn layout: rows = actual, cols = predicted\n    confusion_matrix = [[TN, FP],\n                        [FN, TP]]\n\n    # --- Metrics -------------------------------------------------------------\n    total = TP + TN + FP + FN\n    accuracy = (TP + TN) / total if total else 0\n\n    precision = TP / (TP + FP) if (TP + FP) else 0\n    recall    = TP / (TP + FN) if (TP + FN) else 0\n    f1_score  = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n\n    specificity = TN / (TN + FP) if (TN + FP) else 0\n    negative_predictive_value = TN / (TN + FN) if (TN + FN) else 0\n\n    # --- Round to three decimal places ---------------------------------------\n    accuracy                  = round(accuracy, 3)\n    f1_score                  = round(f1_score, 3)\n    specificity               = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n\n    return (\n        confusion_matrix,\n        accuracy,\n        f1_score,\n        specificity,\n        negative_predictive_value\n    )\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate a collection of descriptive\u2013statistics measures for the supplied\n    data and return them in a dictionary.\n\n    Parameters\n    ----------\n    data : list, tuple or np.ndarray\n        One-dimensional iterable of numerical values.\n\n    Returns\n    -------\n    dict\n        {\n          'mean'               : float\n          'median'             : float\n          'mode'               : float | list[float]\n          'variance'           : float\n          'standard_deviation' : float\n          '25th_percentile'    : float\n          '50th_percentile'    : float\n          '75th_percentile'    : float\n          'interquartile_range': float\n        }\n    Notes\n    -----\n    \u2022  The mean, (population) variance, standard deviation, percentiles and IQR\n       are rounded to four decimal places.\n    \u2022  Statistics that are direct members of the original data set (median in\n       the odd-length case, and all mode values) are *not* rounded.  When more\n       than one mode exists, they are returned in a list.\n    \"\"\"\n\n    # ---------- sanity checks & preparation ---------------------------------\n    arr = np.asarray(data, dtype=float).ravel()          # 1-D NumPy array\n    if arr.size == 0:\n        raise ValueError(\"`data` must contain at least one numerical value.\")\n\n    # ---------- helper for uniform 4-decimal rounding -----------------------\n    def _round4(value):\n        \"\"\"Round scalars or 1-D iterables to 4 dp.\"\"\"\n        if isinstance(value, (list, tuple, np.ndarray)):\n            return [round(float(v), 4) for v in value]\n        return round(float(value), 4)\n\n    # ---------- central-tendency statistics ---------------------------------\n    mean_val   = np.mean(arr)\n    median_val = np.median(arr)\n\n    # Mode: allow for multi-modal data\n    cnt        = Counter(arr)\n    max_freq   = max(cnt.values())\n    mode_vals  = [val for val, freq in cnt.items() if freq == max_freq]\n    mode_val   = mode_vals[0] if len(mode_vals) == 1 else mode_vals\n\n    # ---------- dispersion ---------------------------------------------------\n    variance_val = np.var(arr, ddof=0)   # population variance\n    std_dev_val  = np.std(arr, ddof=0)   # population std-dev\n\n    # ---------- position (percentiles) & spread (IQR) ------------------------\n    p25 = np.percentile(arr, 25)\n    p50 = np.percentile(arr, 50)\n    p75 = np.percentile(arr, 75)\n    iqr = p75 - p25\n\n    # ---------- assemble results --------------------------------------------\n    return {\n        'mean'               : _round4(mean_val),\n        'median'             : median_val,           # left un-rounded\n        'mode'               : mode_val,             # left un-rounded\n        'variance'           : _round4(variance_val),\n        'standard_deviation' : _round4(std_dev_val),\n        '25th_percentile'    : _round4(p25),\n        '50th_percentile'    : _round4(p50),\n        '75th_percentile'    : _round4(p75),\n        'interquartile_range': _round4(iqr)\n    }\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n\n    :param n: int  \u2013 total number of trials (n \u2265 0)\n    :param k: int  \u2013 number of successes (0 \u2264 k \u2264 n)\n    :param p: float \u2013 probability of success on each trial (0 \u2264 p \u2264 1)\n    :return: float \u2013 probability of k successes in n trials, rounded to 5 decimal places\n    \"\"\"\n    # Basic validation\n    if not (0 <= p <= 1):\n        raise ValueError(\"p (probability of success) must be between 0 and 1.\")\n    if not (0 <= k <= n):\n        return 0.0  # Impossible event\n\n    # Compute the binomial coefficient (n choose k)\n    try:\n        # Python 3.8+: math.comb is available and efficient\n        combinations = math.comb(n, k)\n    except AttributeError:  # Fallback for older Python versions\n        combinations = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n    # Binomial probability\n    probability = combinations * (p ** k) * ((1 - p) ** (n - k))\n\n    # Return rounded result\n    return round(probability, 5)\n```\n"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\ndef normal_pdf(x, mean, std_dev):\n       Calculate the probability density function (PDF) of the normal distribution.\n\n    Parameters\n    ----------\n    x : float\n        The value at which the PDF is evaluated.\n    mean : float\n        The mean (\u03bc) of the distribution.\n    std_dev : float\n        The standard deviation (\u03c3) of the distribution. Must be positive.\n\n    Returns\n    -------\n    float\n        The PDF value rounded to 5 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If `std_dev` is not positive.\n       if std_dev <= 0:\n        raise ValueErrorStandard deviation must be positive    coefficient = 1.0 / (std_dev * math.sqrt(2.0 * math.pi))\n    exponent = math.exp(-((x - mean) ** 2) / (2.0 * std_dev ** 2))\n    pdf_value = coefficient * exponent\n\n    return round(pdf_value, 5)\n```\n\nExample usage:\n```python\nprint(normal_pdf(1.0, mean=0, std_dev=1))  # 0.24197\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n       Calculate the probability of observing exactly k events in a fixed interval\n    using the Poisson distribution:  P(X = k) = (\u03bb^k * e^(\u2212\u03bb)) / k!\n    \n    Parameters\n    ----------\n    k   : int\n        Number of events (must be a non-negative integer).\n    lam : float or int\n        Mean (expected) number of occurrences during the interval (\u03bb \u2265 0).\n\n    Returns\n    -------\n    float\n        Probability of observing exactly k events, rounded to 5 decimal places.\n\n    Raises\n    ------\n    ValueError\n        If k is negative or not an integer, or if \u03bb is negative.\n       # Input validation\n    if not (isinstance(k, int) and k >= 0):\n        raise ValueErrork must be a non-negative integer    if lam < 0:\n        raise ValueError\u03bb (lam) must be non-negative    # Poisson probability calculation\n    prob = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    \n    # Round to 5 decimal places\n    return round(prob, 5)\n```\n\nExample usage:\n\n```python\nprint(poisson_probability(3, 2.5))  # 0.21376\nprint(poisson_probability(0, 4))    # 0.01832\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Contrast is defined here as:\n        contrast = max_pixel_value - min_pixel_value\n\n    Args:\n        img (numpy.ndarray): 2-D array representing a grayscale image\n                             with pixel values in the range 0\u2013255.\n\n    Returns:\n        int or float: The contrast value of the image.\n\n    Raises:\n        ValueError: If `img` is not a 2-D NumPy array.\n    \"\"\"\n    # Basic validation -------------------------------------------------------\n    if not isinstance(img, np.ndarray):\n        raise ValueError(\"Input must be a NumPy array.\")\n    if img.ndim != 2:\n        raise ValueError(\"Input must be a 2-D array representing a grayscale image.\")\n\n    # Ensure we work in a numeric type that can represent the difference ------\n    img = img.astype(np.float32, copy=False)\n\n    # Compute contrast --------------------------------------------------------\n    max_val = img.max()\n    min_val = img.min()\n    contrast = max_val - min_val\n\n    return contrast\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float or int:\n            The dot product of vec1 and vec2.\n\n    Raises:\n        ValueError: If vec1 or vec2 is not a 1-D NumPy array or if their lengths differ.\n        TypeError:  If vec1 or vec2 is not a NumPy ndarray.\n    \"\"\"\n    # Validate that both inputs are NumPy ndarrays\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError(\"Both inputs must be NumPy arrays.\")\n\n    # Ensure they are 1-D vectors\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError(\"Both inputs must be 1-D NumPy arrays.\")\n\n    # Check that the lengths match\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError(\"Vectors must have the same length.\")\n\n    # Compute dot product\n    return float(np.dot(vec1, vec2))\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef phi_transform(data: list[float], degree: int):\n       Perform a Phi Transformation to map input features into a higher-dimensional\n    space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values (each value is treated as\n                            a separate data point with a single original feature).\n        degree (int): The highest degree of the polynomial expansion.  If this\n                      value is negative, an empty list is returned.\n\n    Returns:\n        list[list[float]]: Nested list where each inner list contains the\n                           polynomial features (from degree 0 to `degree`)\n                           of the corresponding data point, rounded to\n                           8 decimal places.\n       # Return empty list for invalid degree\n    if degree < 0:\n        return []\n\n    # Convert to NumPy array for efficient broadcasting\n    x = np.asarray(data, dtype=float)            # shape: (n_samples,)\n    powers = np.arange(degree + 1)              # shape: (degree+1,)\n\n    # Raise every data point to every power (broadcasting)\n    poly_matrix = np.power(x[:, None], powers)  # shape: (n_samples, degree+1)\n\n    # Round to 8 decimal places and convert to nested Python lists\n    poly_matrix = np.round(poly_matrix, 8)\n\n    return poly_matrix.tolist()\n```\n\nExample usage:\n```python\nprint(phi_transform([1.2, 2.5, -3.0], 3))\n# Output:\n# [[1.0, 1.2, 1.44, 1.728],\n#  [1.0, 2.5, 6.25, 15.625],\n#  [1.0, -3.0, 9.0, -27.0]]\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n       Compute the sinusoidal positional encodings used by the original Transformer.\n\n    Parameters\n    ----------\n    position : int\n        Maximum sequence length for which to create positional encodings.\n    d_model : int\n        Dimensionality of the model / embeddings.\n\n    Returns\n    -------\n    list or int\n        A (position, d_model) nested list containing the positional encodings\n        (dtype float16).  \n        Returns -1 for invalid inputs (position == 0 or d_model <= 0).\n       # Guard-clauses for invalid inputs\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create a matrix of shape (position, 1) with positions [0, 1, 2, ...]\n    pos = np.arange(position, dtype=np.float32)[:, np.newaxis]          # (position, 1)\n\n    # Column indices [0, 1, 2, ... , d_model-1]  -> shape (1, d_model)\n    i = np.arange(d_model, dtype=np.float32)[np.newaxis, :]            # (1, d_model)\n\n    # Compute the angle rates: 1 / (10000^(2i/d_model))\n    angle_rates = 1.0 / np.power(10000.0, (2 * (i // 2)) / np.float32(d_model))\n\n    # Apply the angle rates to every position\n    angles = pos * angle_rates                                          # (position, d_model)\n\n    # Initialise positional encoding matrix\n    pe = np.empty_like(angles)\n\n    # Even indices (2i):   sin\n    pe[:, 0::2] = np.sin(angles[:, 0::2])\n\n    # Odd  indices (2i+1): cos\n    pe[:, 1::2] = np.cos(angles[:, 1::2])\n\n    # Convert to float16 and return as nested Python lists\n    return pe.astype(np.float16).tolist()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n       Determine if the model is overfitting, underfitting, or a good fit based on\n    training and test accuracy.\n\n    Parameters\n    ----------\n    training_accuracy : float\n        Training accuracy of the model (expected range: 0.0 \u2013 1.0).\n    test_accuracy : float\n        Test/validation accuracy of the model (expected range: 0.0 \u2013 1.0).\n\n    Returns\n    -------\n    int\n        1  -> Overfitting   (training accuracy much higher than test accuracy)\n       -1  -> Underfitting  (both accuracies low)\n        0  -> Good fit      (neither over- nor underfitting)\n\n    Rules\n    -----\n    \u2022 Overfitting  : (training_accuracy - test_accuracy) > 0.20  \n    \u2022 Underfitting : training_accuracy < 0.70  and  test_accuracy < 0.70  \n    \u2022 Good fit     : otherwise\n       # --- optional sanity check on input range ---\n    if not (0.0 <= training_accuracy <= 1.0\n            and 0.0 <= test_accuracy <= 1.0):\n        raise ValueErrorAccuracies must be floats between 0.0 and 1.0 (inclusive).    # Check for overfitting first (takes priority)\n    if training_accuracy - test_accuracy > 0.20:\n        return 1          # Overfitting\n\n    # Check for underfitting\n    if training_accuracy < 0.70 and test_accuracy < 0.70:\n        return -1         # Underfitting\n\n    # Otherwise, model fits well\n    return 0               # Good fit\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t,\n                   learning_rate=0.001, beta1=0.9, beta2=0.999,\n                   epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer (with bias-correction).\n\n    Parameters\n    ----------\n    parameter : scalar / list / np.ndarray\n        Current value(s) of the parameter(s) being optimized.\n    grad : scalar / list / np.ndarray\n        Current gradient(s) of the loss w.r.t. the parameter(s).\n    m : scalar / list / np.ndarray\n        First-moment (mean) moving average from the previous step.\n    v : scalar / list / np.ndarray\n        Second-moment (uncentered variance) moving average from the previous step.\n    t : int\n        Current timestep (starts from 1).\n    learning_rate : float, optional\n        Step size (default 1 \u00d7 10\u207b\u00b3).\n    beta1 : float, optional\n        Decay rate for the first moment (default 0.9).\n    beta2 : float, optional\n        Decay rate for the second moment (default 0.999).\n    epsilon : float, optional\n        Small number to avoid division by zero (default 1 \u00d7 10\u207b\u2078).\n\n    Returns\n    -------\n    tuple\n        (updated_parameter, updated_m, updated_v) \u2013 each rounded to 5 decimals\n        and converted to Python lists (or scalars) via ``tolist()``.\n    \"\"\"\n    # Convert inputs to np.ndarray (works with scalars, lists, ndarrays)\n    parameter = np.asarray(parameter, dtype=np.float64)\n    grad       = np.asarray(grad,       dtype=np.float64)\n    m          = np.asarray(m,          dtype=np.float64)\n    v          = np.asarray(v,          dtype=np.float64)\n\n    # 1. Update biased first and second moment estimates\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n\n    # 2. Compute bias-corrected moments\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n\n    # 3. Parameter update\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # 4. Round to 5 decimals and convert to list/scalar\n    parameter_out = np.round(parameter, 5).tolist()\n    m_out         = np.round(m,         5).tolist()\n    v_out         = np.round(v,         5).tolist()\n\n    return parameter_out, m_out, v_out\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str =124M models_dir: str =models    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict =hello 1,world 2,UNK> 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict<UNK> for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            returnjoin([reversed_dict.get(tok_id,UNK> for tok_id in token_ids])\n\n    hparams = {\n       n_ctx 1024,   # context length\n       n_head 12     # number of attention heads\n    }\n\n    params = {\n        # (vocab_size, embed_size)\n       wte np.random.rand(3, 10),\n        # (max_position, embed_size)\n       wpe np.random.rand(1024, 10),\n       blocks [],    # not used \u2013 we build our own tiny block below\n       ln_f {\n           g np.ones(10),\n           b np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n       Very small \u2013 and very *inaccurate* \u2013 GPT-2-style text generator.\n    Only meant to showcase the mechanics (embeddings, positional encodings,\n    self-attention, FFN, layer-norm, autoregressive sampling).\n       np.random.seed(42)  # reproducibility \u2500 makes the run deterministic\n\n    # ------------------------------------------------------------------\n    # 1.  Load encoder, hyper-parameters and model parameters\n    # ------------------------------------------------------------------\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    wte = paramswte                # token embeddings\n    wpe = paramswpe                # positional embeddings\n\n    vocab_size, embed_size = wte.shape\n    n_ctx                         = hparamsn_ctx    n_head                        = hparamsn_head    n_head                        = max(1, min(n_head, embed_size))  # safety\n    head_dim                      = embed_size // n_head\n\n    # ------------------------------------------------------------------\n    # 2.  Create a single randomly-initialised transformer block\n    #     (weights are *not* trained \u2013 this is for demonstration only)\n    # ------------------------------------------------------------------\n    rnd = np.random.RandomState(0)\n\n    # attention weights\n    Wq = rnd.randn(embed_size, embed_size) / np.sqrt(embed_size)\n    Wk = rnd.randn(embed_size, embed_size) / np.sqrt(embed_size)\n    Wv = rnd.randn(embed_size, embed_size) / np.sqrt(embed_size)\n    Wo = rnd.randn(embed_size, embed_size) / np.sqrt(embed_size)\n\n    # feed-forward weights\n    ff_hidden = embed_size * 4\n    W1 = rnd.randn(embed_size, ff_hidden) / np.sqrt(embed_size)\n    b1 = np.zeros(ff_hidden)\n    W2 = rnd.randn(ff_hidden, embed_size) / np.sqrt(ff_hidden)\n    b2 = np.zeros(embed_size)\n\n    # --------------------------- helpers ------------------------------\n    def layer_norm(x, eps: float = 1e-5):\n       Applies LayerNorm over the last dimension of x        mean = x.mean(-1, keepdims=True)\n        var  = x.var(-1, keepdims=True)\n        return (x - mean) / np.sqrt(var + eps)\n\n    def softmax(x, axis=-1):\n        x = x - x.max(axis=axis, keepdims=True)\n        exp_x = np.exp(x)\n        return exp_x / exp_x.sum(axis=axis, keepdims=True)\n\n    # ------------------------------------------------------------------\n    # 3.  Define a minimal transformer block\n    # ------------------------------------------------------------------\n    def transformer_block(h):\n               h: (seq_len, embed_size)\n        returns: (seq_len, embed_size)  (post-block hidden states)\n               seq_len = h.shape[0]\n\n        # -------- self-attention ----------\n        ln1 = layer_norm(h)\n        q = ln1 @ Wq\n        k = ln1 @ Wk\n        v = ln1 @ Wv\n\n        # reshape to (n_head, seq_len, head_dim)\n        q = q.reshape(seq_len, n_head, head_dim).transpose(1, 0, 2)\n        k = k.reshape(seq_len, n_head, head_dim).transpose(1, 0, 2)\n        v = v.reshape(seq_len, n_head, head_dim).transpose(1, 0, 2)\n\n        attn_outputs = []\n        mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9  # causal mask\n        scale = 1.0 / np.sqrt(head_dim)\n\n        for h_idx in range(n_head):\n            qh = q[h_idx]                    # (seq_len, head_dim)\n            kh = k[h_idx].T                 # (head_dim, seq_len)\n            scores = qh @ kh * scale        # (seq_len, seq_len)\n            scores = scores + mask\n            probs  = softmax(scores, axis=-1)\n            ctx    = probs @ v[h_idx]        # (seq_len, head_dim)\n            attn_outputs.append(ctx)\n\n        # concatenate heads and project back\n        attn_out = np.concatenate(attn_outputs, axis=-1)  # (seq_len, embed_size)\n        attn_out = attn_out @ Wo\n        h = h + attn_out            # residual connection\n\n        # -------- feed-forward ----------\n        ln2 = layer_norm(h)\n        ff = ln2 @ W1 + b1\n        ff = np.maximum(0, ff)      # ReLU\n        ff = ff @ W2 + b2\n        h  = h + ff                 # residual\n\n        return h\n    # ------------------------------------------------------------------\n\n    # ------------------------------------------------------------------\n    # 4.  Encode the prompt\n    # ------------------------------------------------------------------\n    token_ids = encoder.encode(prompt)\n\n    # ------------------------------------------------------------------\n    # 5.  Autoregressive generation loop\n    # ------------------------------------------------------------------\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(token_ids)\n        if seq_len >= n_ctx:\n            break  # context window full\n\n        # embeddings + positions\n        tok_emb = wte[token_ids]                 # (seq_len, embed_size)\n        pos_emb = wpe[np.arange(seq_len)]        # (seq_len, embed_size)\n        h = tok_emb + pos_emb\n\n        # pass through our tiny transformer block\n        h = transformer_block(h)\n\n        # logits for the next token from the *last* position\n        last_hidden = h[-1]               # (embed_size,)\n        logits = last_hidden @ wte.T      # (vocab_size,)\n\n        # convert logits to probabilities and sample\n        probs = softmax(logits)\n        next_token = int(np.random.choice(np.arange(vocab_size), p=probs))\n\n        # append and continue\n        token_ids.append(next_token)\n\n    # ------------------------------------------------------------------\n    # 6.  Decode back to text and return\n    # ------------------------------------------------------------------\n    return encoder.decode(token_ids)\n\n\n# ===================== quick demo =====================\nif __name__ ==main    print(gen_texthello 10))\n```\n\nExplanation (concise)\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n1. We load a *dummy* byte-pair encoder plus a *tiny* random parameter set.\n2. The prompt is encoded into integer tokens.\n3. Each generation step constructs token + positional embeddings, runs them through  \n   one randomly initialised transformer block (LayerNorm \u2192 masked self-attention \u2192  \n   feed-forward), and then uses the last hidden vector to produce logits over the\n   vocabulary.\n4. A token is sampled (soft-max) and appended, continuing autoregressively.\n5. When the requested number of tokens is produced, we decode the full token list\n   back to text.\n\nNothing here is trained, so the output is essentially random \u2013 but every core\nbuilding-block of GPT-2\u2019s forward pass is present in simplified form."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n       A tiny, single-head, scalar self-attention:\n        \u2022 Similarity score s\u1d62\u2c7c  = (v\u1d62 \u00b7 v\u2c7c) / \u221ad          (d=dimension)\n        \u2022 Attention weights \u03b1\u1d62   = softmax(s\u1d62\u2022)\n        \u2022 Output o\u1d62              = \u03a3\u2c7c \u03b1\u1d62\u2c7c \u00b7 v\u2c7c\n    The resulting list is rounded to 4 decimal places.\n       def softmax(values):\n        # Numerically stable soft-max.\n        v = values - np.max(values)\n        exp_v = np.exp(v)\n        return exp_v / np.sum(exp_v)\n\n    values = np.asarray(crystal_values, dtype=float)\n    d_scale = np.sqrt(max(dimension, 1))          # avoid division by zero\n\n    # Similarity matrix (outer product because eachembedding is a scalar)\n    sim = np.outer(values, values) / d_scale      # shape (n, n)\n\n    # Compute attention for every crystal\n    outputs = []\n    for i in range(n):\n        weights = softmax(sim[i])                 # \u03b1\u1d62\u2c7c\n        enhanced = np.dot(weights, values)        # \u03a3\u2c7c \u03b1\u1d62\u2c7c \u00b7 v\u2c7c\n        outputs.append(round(enhanced, 4))\n\n    return outputs\n```"}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n       corpus : List[str]   \u2013 iterable with one string per document\n    query  : str         \u2013 query string\n    k1, b  : float       \u2013 BM25 hyper-parameters\n    returns: List[float] \u2013 BM25 score for each document (rounded to 3 d.p.)\n       # --- 1. Tokenise corpus & query (lower-case, whitespace split) -------------\n    tokenised_docs = [doc.lower().split() for doc in corpus]\n    query_terms    = query.lower().split()\n    \n    # --- 2. Pre-compute corpus statistics -------------------------------------\n    N             = len(tokenised_docs)                       # number of docs\n    doc_lengths   = np.array([len(d) for d in tokenised_docs])\n    avg_doc_len   = doc_lengths.mean() if N else 0.0\n    \n    # document frequency:  df[t] = |{ doc : t \u2208 doc }|\n    df = Counter()\n    for doc in tokenised_docs:\n        df.update(set(doc))\n    \n    # --- 3. Pre-compute IDF for every term appearing in any query -------------\n    idf = {}\n    for term in query_terms:\n        n_qi = df.get(term, 0)\n        # BM25 IDF with +1 to keep values positive even for frequent terms\n        idf[term] = np.log( (N - n_qi + 0.5) / (n_qi + 0.5) + 1 )\n    \n    # --- 4. Score each document -----------------------------------------------\n    scores = np.zeros(N, dtype=float)\n    \n    for idx, doc in enumerate(tokenised_docs):\n        if not doc:                 # empty document -> score stays 0\n            continue\n        term_freq = Counter(doc)    # raw term frequencies for this document\n        dl        = doc_lengths[idx]\n        denom_len = k1 * (1 - b + b * dl / avg_doc_len)  # pre-compute length part\n        \n        score = 0.0\n        for term in query_terms:\n            tf = term_freq.get(term, 0)\n            if tf == 0:\n                continue\n            numer = tf * (k1 + 1)\n            score += idf[term] * numer / (tf + denom_len)\n        scores[idx] = score\n    \n    # --- 5. Round to three decimals & convert to Python list ------------------\n    return [round(s, 3) for s in scores]\n```"}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n       Calculate the (macro-averaged) F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n       # Basic sanity check\n    if len(y_true) != len(y_pred):\n        raise ValueErrory_true and y_pred must have the same length    if not y_true:                       # empty input\n        return 0.0\n\n    # All unique labels that appear in either list\n    labels = set(y_true) | set(y_pred)\n\n    f1_sum = 0.0\n    for label in labels:\n        tp = fp = fn = 0\n        for yt, yp in zip(y_true, y_pred):\n            if yt == label and yp == label:\n                tp += 1                      # true positive\n            elif yt != label and yp == label:\n                fp += 1                      # false positive\n            elif yt == label and yp != label:\n                fn += 1                      # false negative\n\n        # Precision and recall with safe division\n        precision = tp / (tp + fp) if (tp + fp) else 0.0\n        recall    = tp / (tp + fn) if (tp + fn) else 0.0\n\n        # F1 for this label\n        if precision + recall:\n            f1 = 2 * precision * recall / (precision + recall)\n        else:\n            f1 = 0.0\n\n        f1_sum += f1\n\n    # Macro-average across all labels\n    macro_f1 = f1_sum / len(labels)\n    return round(macro_f1, 3)\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # ---------------------------\n    # 1) Remove the known fluctuation\n    # ---------------------------\n    detrended = []\n    for day, obs in enumerate(consumption_data, start=1):\n        fluct = 10 * math.sin(2 * PI * day / 10)\n        detrended.append(obs - fluct)\n\n    # ---------------------------\n    # 2) Simple linear regression  y = a + b\u00b7day\n    # ---------------------------\n    n        = len(detrended)\n    xs       = list(range(1, n + 1))\n    sum_x    = sum(xs)\n    sum_y    = sum(detrended)\n    sum_xx   = sum(x * x for x in xs)\n    sum_xy   = sum(x * y for x, y in zip(xs, detrended))\n\n    denom = n * sum_xx - sum_x ** 2\n    b = (n * sum_xy - sum_x * sum_y) / denom if denom else 0.0          # slope\n    a = (sum_y - b * sum_x) / n                                         # intercept\n\n    # ---------------------------\n    # 3) Predict base usage for day 15\n    # ---------------------------\n    day_15_base = a + b * 15\n\n    # ---------------------------\n    # 4) Add back the day-15 fluctuation\n    # ---------------------------\n    day_15_fluct = 10 * math.sin(2 * PI * 15 / 10)\n    forecast = day_15_base + day_15_fluct\n\n    # ---------------------------\n    # 5) Round, then add a 5 % safety margin\n    # ---------------------------\n    rounded      = round(forecast)          # nearest integer\n    with_margin  = math.ceil(rounded * 1.05)\n\n    # ---------------------------\n    # 6) Return the final integer\n    # ---------------------------\n    return int(with_margin)\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\ndef mae(y_true, y_pred):\n       Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n       \n    # Convert inputs to NumPy arrays (in case lists are passed)\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n\n    # Ensure both arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueErrory_true and y_pred must have the same shape    \n    # Compute the Mean Absolute Error\n    error = np.abs(y_true - y_pred)\n    mae_value = np.mean(error)\n\n    # Return the result rounded to 3 decimal places\n    return np.round(mae_value, 3)\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# ---------- helpers ----------\ndef _softmax(x, axis=-1):\n    \"\"\"\n    Numerically-stable softmax\n    \"\"\"\n    x = x - np.max(x, axis=axis, keepdims=True)\n    e = np.exp(x)\n    return e / np.sum(e, axis=axis, keepdims=True)\n\n\n# ---------- required functions ----------\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute query, key and value matrices.\n    X : (m, n)        \u2013 input sequence              (m = sequence length)\n    W_* : (n, n)      \u2013 learned weight matrices     (n = embedding dim.)\n    Returns\n        Q, K, V : each (m, n)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Single-head scaled dot-product attention\n    Q, K, V : (m, d)  \u2013 queries / keys / values for one head\n    Returns\n        (m, d)        \u2013 attended representations\n    \"\"\"\n    d_k = Q.shape[-1]               # dimension inside this head\n    scores = (Q @ K.T) / np.sqrt(d_k)   # (m, m)\n    weights = _softmax(scores, axis=-1) # softmax over the key dimension\n    return weights @ V                  # (m, d)\n\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Multi-head attention composed of n_heads independent self-attentions.\n    Q, K, V : (m, n)   \u2013 full-dimension query/key/value\n    n_heads : int      \u2013 number of parallel heads (n must be divisible by n_heads)\n    Returns\n        (m, n)         \u2013 concatenated per-head outputs\n    \"\"\"\n    m, n = Q.shape\n    assert n % n_heads == 0, \"`n_heads` must evenly divide the embedding dimension\"\n    d_k = n // n_heads\n\n    head_outputs = []\n    for h in range(n_heads):\n        s, e = h * d_k, (h + 1) * d_k       # slice boundaries\n        Q_h, K_h, V_h = Q[:, s:e], K[:, s:e], V[:, s:e]\n        head_outputs.append(self_attention(Q_h, K_h, V_h))\n\n    return np.concatenate(head_outputs, axis=-1)\n\n\n# ---------- provided entrypoint ----------\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n\n    # Generate input data\n    X = np.arange(m * n).reshape(m, n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n, n))\n    W_k = np.random.randint(0, 5, size=(n, n))\n    W_v = np.random.randint(0, 6, size=(n, n))\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport math\nfrom typing import List\n\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n        x (list[int]): A list of binary values (0 or 1).\n        y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n        float: The Phi coefficient rounded to 4 decimal places.\n\n    Raises:\n        ValueError: If the input lists are of different lengths\n                    or contain values other than 0 or 1.\n    \"\"\"\n    # -------- 1. Basic validation --------\n    if len(x) != len(y):\n        raise ValueError(\"Both input lists must have the same length.\")\n    if any(val not in (0, 1) for val in x+y):\n        raise ValueError(\"Input lists must contain only 0s and 1s.\")\n\n    # -------- 2. Build the 2\u00d72 contingency-table counts --------\n    #  a b\n    #  c d\n    a = b = c = d = 0\n    for xi, yi in zip(x, y):\n        if xi == 1 and yi == 1:\n            a += 1\n        elif xi == 1 and yi == 0:\n            b += 1\n        elif xi == 0 and yi == 1:\n            c += 1\n        else:  # xi == 0 and yi == 0\n            d += 1\n\n    # -------- 3. Compute the Phi coefficient --------\n    numerator   = a * d - b * c\n    denom_part1 = (a + b) * (c + d)\n    denom_part2 = (a + c) * (b + d)\n\n    denom = math.sqrt(denom_part1 * denom_part2)\n\n    # Handle degenerate cases where denominator is zero\n    if denom == 0:\n        return 0.0\n\n    phi = numerator / denom\n    return round(phi, 4)\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n       Implements the Hard Sigmoid activation function.\n\n    The hard-sigmoid is a piece-wise linear, clipped approximation of the\n    logistic sigmoid.  A common definition (used e.g. by Keras and Theano) is\n\n        hard_sigmoid(x) = clip(0.2 * x + 0.5, 0.0, 1.0)\n\n    where `clip(y, a, b)` constrains `y` to the interval [a, b].\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: The Hard Sigmoid of the input.\n       # Linear transform then clamp to [0, 1]\n    return max(0.0, min(1.0, 0.2 * x + 0.5))\n```\n"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places\n    \"\"\"\n    # Positive or zero input: linear identity\n    if x >= 0:\n        activated = x\n    # Negative input: scaled exponential\n    else:\n        activated = alpha * (math.exp(x) - 1)\n\n    # Return value rounded to 4 decimal places as required\n    return round(activated, 4)\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n       Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x (float): Input value.\n        alpha (float): Slope parameter for negative values (default: 0.25).\n\n    Returns:\n        float: PReLU activation value.\n       return x if x >= 0 else alpha * x\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute a numerically-stable Softplus: log(1 + e^x).\n\n    Args:\n        x (float): Input value.\n\n    Returns:\n        float: Softplus of `x`, rounded to 4 decimal places.\n    \"\"\"\n    # Handle NaN directly\n    if math.isnan(x):\n        return float('nan')\n\n    # \u00b1\u221e edge cases\n    if math.isinf(x):\n        return math.inf if x > 0 else 0.0\n\n    # Numerically stable formulation:\n    #   softplus(x) = max(0, x) + log(1 + exp(-|x|))\n    # This avoids overflow for large +x and underflow for large \u2013x.\n    result = max(0.0, x) + math.log1p(math.exp(-abs(x)))\n\n    # Round to 4 decimal places as required\n    return round(result, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n       Implements the Softsign activation function.\n\n    Softsign(x) = x / (1 + |x|)\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n       # Softsign formula: x / (1 + |x|)\n    softsign_value = x / (1.0 + abs(x))\n    \n    # Return the value rounded to 4 decimal places\n    return round(softsign_value, 4)\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n       Compute the GRPO objective function.\n\n    Args:\n        rhos            : Iterable of likelihood ratios \u03c1_i = \u03c0_\u03b8(o_i|q) / \u03c0_\u03b8_old(o_i|q)\n        A               : Iterable of advantage estimates \u00c2_i\n        pi_theta_old    : Iterable of old\u2013policy probabilities  \u03c0_\u03b8_old(o_i|q)\n        pi_theta_ref    : Iterable of reference\u2013policy probabilities  \u03c0_ref(o_i|q)\n        epsilon         : PPO\u2013style clipping coefficient\n        beta            : Coefficient for the KL\u2013divergence penalty term\n\n    Returns:\n        float: GRPO objective value (rounded to 6 d.p.).\n       \n    # --- Convert to numpy arrays ------------------------------------------------\n    rhos           = np.asarray(rhos, dtype=np.float64)\n    advantages     = np.asarray(A, dtype=np.float64)\n    pi_theta_old   = np.asarray(pi_theta_old, dtype=np.float64)\n    pi_theta_ref   = np.asarray(pi_theta_ref, dtype=np.float64)\n\n    # --- Basic validation -------------------------------------------------------\n    if not (rhos.shape == advantages.shape == pi_theta_old.shape == pi_theta_ref.shape):\n        raise ValueErrorAll inputs must have the same shape    # --- Compute PPO-style clipped surrogate -----------------------------------\n    clipped_rhos   = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate_full = rhos        * advantages\n    surrogate_clip = clipped_rhos * advantages\n    L_clip         = np.mean(np.minimum(surrogate_full, surrogate_clip))\n\n    # --- Reconstruct current policy prob. and compute KL(\u03c0_\u03b8 || \u03c0_ref) ---------\n    pi_theta       = rhos * pi_theta_old                        # element-wise\n    # numerical stability \u2014 avoid log(0)\n    eps            = 1e-8\n    kl_div_terms   = pi_theta * (np.log((pi_theta + eps) / (pi_theta_ref + eps)))\n    KL_mean        = np.mean(kl_div_terms)\n\n    # --- GRPO objective ---------------------------------------------------------\n    objective      = L_clip - beta * KL_mean\n\n    # --- Round to 6 decimal places as required ---------------------------------\n    return float(np.round(objective, 6))\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x):\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: A float, int, or NumPy array\n\n    Returns:\n        The Swish activation value(s) rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input to a NumPy array for unified processing\n    x_arr = np.asarray(x, dtype=float)\n\n    # Swish: x * sigmoid(x) where sigmoid(x) = 1 / (1 + e^{-x})\n    swish_val = x_arr / (1.0 + np.exp(-x_arr))\n\n    # Round to 4 decimal places\n    swish_val = np.round(swish_val, 4)\n\n    # If the original input was a scalar, return a scalar\n    if np.isscalar(x):\n        return float(swish_val)\n    return swish_val\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x):\n       Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value (float, int, list, tuple, np.ndarray)\n\n    Returns:\n        SELU activation value rounded to 4 decimal places.\n        \u2013 If x is a scalar, returns a float.\n        \u2013 If x is an array-like object, returns a NumPy array.\n       # SELU constants (from the original paper)\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    # Helper for scalar inputs (uses math.expm1 for better precision with small values)\n    def _selu_scalar(val: float) -> float:\n        if val > 0.0:\n            res = scale * val\n        else:\n            # expm1(x) == exp(x) - 1, numerically stable around 0\n            res = scale * (alpha * math.expm1(val))\n        # Round to 4 decimal places\n        return round(res, 4)\n\n    # If the input is a scalar (int, float, or numpy scalar), treat directly\n    if np.isscalar(x):\n        return _selu_scalar(float(x))\n\n    # Otherwise, convert to NumPy array and apply vectorized SELU\n    x_arr = np.asarray(x, dtype=float)\n\n    # Vectorized, numerically stable computation\n    positive = scale * x_arr\n    negative = scale * (alpha * np.expm1(x_arr))\n    result = np.where(x_arr > 0, positive, negative)\n\n    # Round element-wise to 4 decimals\n    return np.round(result, 4)\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n       Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X      : Input feature matrix of shape (N, D)\n        weights: Model weights of shape (D,) or (D, 1)\n        bias   : Scalar bias term\n\n    Returns:\n        Python list with binary predictions (0 or 1)\n       \n    # Ensure weight vector is 1-D for the dot product\n    w = np.ravel(weights)\n\n    # Linear combination\n    z = np.dot(X, w) + bias          # shape: (N,)\n\n    # Sigmoid activation for probabilities\n    probs = 1.0 / (1.0 + np.exp(-z)) # shape: (N,)\n\n    # Convert probabilities to binary labels using threshold 0.5\n    preds = (probs >= 0.5).astype(int)\n\n    # Return as a regular Python list\n    return preds.tolist()\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(\n        X: np.ndarray, \n        y: np.ndarray, \n        learning_rate: float, \n        iterations: int\n    ) -> tuple[list[list[float]], list[float]]:\n       Gradient\u2013descent training algorithm for Softmax regression, \n    optimizing parameters with Cross-Entropy loss.\n\n    Parameters\n    ----------\n    X : np.ndarray               -> shape (N , C) feature matrix\n    y : np.ndarray               -> shape (N,) class-label vector (any integers)\n    learning_rate : float        -> step size\n    iterations : int             -> number of gradient-descent updates\n\n    Returns\n    -------\n    B : list[list[float]]        -> C\u00d7M parameter matrix (rounded to 4 dp) \u2011- .tolist()\n    losses : list[float]         -> Cross-Entropy values over the `iterations`\n                                    (rounded to 4 dp)   \n       # ----------------------------- Preparation -----------------------------\n    N, C = X.shape\n    classes = np.unique(y)\n    M = len(classes)\n\n    # map arbitrary labels to 0..M-1\n    class_to_idx = {c: i for i, c in enumerate(classes)}\n    y_idx = np.vectorize(class_to_idx.get)(y)\n\n    # one-hot encoding of labels\n    Y = np.zeros((N, M))\n    Y[np.arange(N), y_idx] = 1.0\n\n    # parameter initialisation (zeros)\n    B = np.zeros((C, M))\n\n    losses = []\n\n    # --------------------------- Helper functions ---------------------------\n    def softmax(z: np.ndarray) -> np.ndarray:\n        # numerically stable softmax\n        z = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def cross_entropy(P: np.ndarray, Y: np.ndarray) -> float:\n        # mean cross-entropy\n        eps = 1e-15               # to avoid log(0)\n        return -np.mean(np.sum(Y * np.log(P + eps), axis=1))\n\n    # ------------------------ Gradient-Descent Loop -------------------------\n    for _ in range(iterations):\n        # forward\n        logits = X @ B                 # shape (N, M)\n        P      = softmax(logits)       # predicted probabilities\n\n        # loss\n        loss = cross_entropy(P, Y)\n        losses.append(float(np.round(loss, 4)))  # keep 4-dp rounded copy\n\n        # gradient (C, M)\n        grad = (X.T @ (P - Y)) / N\n\n        # update\n        B -= learning_rate * grad\n\n    # ----------------------------- Finishing -------------------------------\n    B_rounded = np.round(B, 4).tolist()\n\n    return B_rounded, losses\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef train_logreg(\n    X: np.ndarray,\n    y: np.ndarray,\n    learning_rate: float,\n    iterations: int,\n) -> tuple[list[float], list[float]]:\n       Gradient-descent training algorithm for logistic regression with\n    Binary-Cross-Entropy loss.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (m, n).  *The function automatically\n        augments a bias (intercept) column of 1\u2019s.*\n    y : np.ndarray\n        Binary targets of shape (m,) or (m, 1).\n    learning_rate : float\n        Step size (\u03b1) for gradient descent.\n    iterations : int\n        Number of gradient-descent steps.\n\n    Returns\n    -------\n    tuple[list[float], list[float]]\n        \u2022 Optimised coefficients (including the bias term) rounded to 4 d.p.  \n        \u2022 Per-iteration BCE losses (length = `iterations`) rounded to 4 d.p.\n       # ------------------  Helpers ------------------ #\n    def sigmoid(z: np.ndarray) -> np.ndarray:\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # ---------------  Pre-processing -------------- #\n    m = X.shape[0]\n    # ensure y is a flat vector of shape (m,)\n    y = y.reshape(-1)\n\n    # add bias column (ones) to X\n    X_b = np.c_[np.ones(m), X]          # shape \u2192 (m, n + 1)\n    n_params = X_b.shape[1]\n\n    # initialise \u03b8 (coefficients) to zeros\n    theta = np.zeros(n_params)\n\n    # container for losses\n    losses = np.empty(iterations)\n\n    eps = 1e-15  # for numerical stability inside log\n\n    # -------------  Gradient Descent Loop ---------- #\n    for i in range(iterations):\n        # forward pass\n        z = X_b @ theta\n        y_hat = sigmoid(z)\n\n        # Binary-Cross-Entropy loss\n        loss = -np.mean(y * np.log(y_hat + eps) + (1 - y) * np.log(1 - y_hat + eps))\n        losses[i] = loss\n\n        # gradient computation\n        gradient = (X_b.T @ (y_hat - y)) / m\n\n        # parameter update\n        theta -= learning_rate * gradient\n\n    # ---------  Rounding & Python-list casting ------- #\n    theta_rounded = np.round(theta, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n\n    return theta_rounded, losses_rounded\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef compute_qkv(X: np.ndarray,\n                W_q: np.ndarray,\n                W_k: np.ndarray,\n                W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef _softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n    \"\"\"\n    Numerically-stable softmax.\n    \"\"\"\n    x_max = np.max(x, axis=axis, keepdims=True)\n    e_x   = np.exp(x - x_max)\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n\ndef masked_attention(Q: np.ndarray,\n                     K: np.ndarray,\n                     V: np.ndarray,\n                     mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Parameters\n    ----------\n    Q, K, V : np.ndarray\n        Query, Key, and Value tensors.  Shape can be either:\n        (seq_len, d_k)            \u2013 no batch dimension, or\n        (batch, seq_len, d_k)     \u2013 with batch dimension.\n    mask : np.ndarray\n        Attention mask of shape (seq_len, seq_len) or\n        (batch, seq_len, seq_len).  Entries that are 0/False\n        are masked (i.e. positions that *cannot* be attended to).\n\n    Returns\n    -------\n    list\n        Attention output converted to Python list via .tolist().\n    \"\"\"\n    d_k = Q.shape[-1]\n\n    # 1. Scaled dot-product attention scores\n    if Q.ndim == 3:     # batched\n        # (batch, seq_len, seq_len)\n        scores = np.matmul(Q, np.transpose(K, (0, 2, 1))) / np.sqrt(d_k)\n    else:               # no batch dim\n        # (seq_len, seq_len)\n        scores = np.dot(Q, K.T) / np.sqrt(d_k)\n\n    # 2. Apply mask  \u2013 set disallowed positions to large negative value\n    #\n    # Ensure the mask is broadcastable to `scores`\n    mask_bool = mask.astype(bool)\n    if scores.ndim == 3 and mask_bool.ndim == 2:   # expand for batch\n        mask_bool = mask_bool[None, :, :]\n\n    scores = np.where(mask_bool, scores, -1e9)\n\n    # 3. Softmax \u2192 attention weights\n    attn_weights = _softmax(scores, axis=-1)\n\n    # 4. Weighted sum of values\n    if Q.ndim == 3:\n        # (batch, seq_len, d_v)\n        output = np.matmul(attn_weights, V)\n    else:\n        # (seq_len, d_v)\n        output = np.dot(attn_weights, V)\n\n    # 5. Return as Python list\n    return output.tolist()\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n       Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure used is Shannon entropy (base-2) of the color distribution.\n    \n    Properties satisfied\n    --------------------\n    \u2022 All apples same color  \u2192  disorder = 0.0000  \n    \u2022 More colour variety and/or a more even distribution of colours\n      \u2192 higher disorder value.\n    \n    The returned value is rounded to 4 decimal places.\n       n = len(apples)\n    if n == 0:\n        return 0.0                           # empty basket \u2192 no disorder\n    \n    counts = Counter(apples)\n    if len(counts) == 1:                     # only one colour present\n        return 0.0\n    \n    # Shannon entropy H = \u2212\u03a3 p_i log\u2082 p_i\n    entropy = 0.0\n    for c in counts.values():\n        p = c / n\n        entropy -= p * math.log2(p)\n    \n    return round(entropy, 4)\n\n\n# Quick self-check\nif __name__ ==main    examples = [\n        [0, 0, 0, 0],                       # \u2192 0.0\n        [1, 1, 0, 0],                       # \u2192 1.0\n        [0, 1, 2, 3],                       # \u2192 2.0\n        [0, 0, 1, 1, 2, 2, 3, 3],           # \u2192 2.0\n        [0, 0, 0, 0, 0, 1, 2, 3],           # \u2192 1.5488\n    ]\n    for lst in examples:\n        print(lst,\u2192 disorder(lst))\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, \n                        gamma: np.ndarray, \n                        beta: np.ndarray, \n                        epsilon: float = 1e-5):\n    \"\"\"\n    Layer-normalize a 3-D tensor (batch, sequence, feature).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor with shape (B, T, F).\n    gamma : np.ndarray\n        Scale parameters.  Expected shape (F,) or broadcast-compatible with X.\n    beta : np.ndarray\n        Shift parameters.  Expected shape (F,) or broadcast-compatible with X.\n    epsilon : float, optional\n        Small constant to avoid division by zero.  Default is 1e-5.\n\n    Returns\n    -------\n    list\n        Normalized, scaled and shifted tensor rounded to 5 decimals,\n        converted to a Python list.\n    \"\"\"\n    # Ensure X is float for numerical stability\n    X = X.astype(np.float32)\n    \n    # Compute mean and variance across the feature dimension (axis = -1)\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var  = np.var (X, axis=-1, keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape/broadcast gamma and beta so they match (B, T, F)\n    # If they are 1-D (F,), reshape to (1, 1, F) for broadcasting\n    if gamma.ndim == 1:\n        gamma = gamma.reshape(1, 1, -1)\n    if beta.ndim == 1:\n        beta  = beta.reshape (1, 1, -1)\n    \n    # Scale and shift\n    out = X_norm * gamma + beta\n    \n    # Round to 5 decimals and convert to list\n    return np.round(out, 5).tolist()\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np          # (only used if you want to extend the code later)\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n       Compute a simplified METEOR score for one reference / candidate pair.\n    \n    Parameters\n    ----------\n    reference : str\n        The gold-standard translation.\n    candidate : str\n        The machine translation output being evaluated.\n    alpha, beta, gamma : float\n        Standard METEOR parameters (defaults: 0.9, 3, 0.5).\n    \n    Returns\n    -------\n    float\n        METEOR score rounded to 3 decimal places.\n       \n    # 1. Tokenisation (case-insensitive, white-space split)\n    ref_tokens  = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n    \n    if not ref_tokens or not cand_tokens:\n        return 0.0\n    \n    # 2. Unigram matches (each word can be used at most once)\n    ref_count  = Counter(ref_tokens)\n    cand_count = Counter(cand_tokens)\n    \n    matches = sum(min(ref_count[w], cand_count[w]) for w in cand_count)\n    if matches == 0:\n        return 0.0\n    \n    # 3. Precision, Recall, F-mean\n    precision = matches / len(cand_tokens)\n    recall    = matches / len(ref_tokens)\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n    \n    # 4. Fragmentation penalty ---------------------------------------------\n    #    Build an alignment to obtain the sequence of reference indices\n    #    corresponding to the matched words in candidate order.\n    \n    # Map each reference token to a queue of its positions\n    positions_by_token = {}\n    for idx, tok in enumerate(ref_tokens):\n        positions_by_token.setdefault(tok, []).append(idx)\n    \n    # Produce the ordered list of reference positions for matched words\n    matched_positions = []\n    for tok in cand_tokens:\n        pos_list = positions_by_token.get(tok, [])\n        if pos_list:                       # still an unmatched occurrence\n            matched_positions.append(pos_list.pop(0))\n    \n    # Count chunks: contiguous, in-order subsequences inside matched_positions\n    chunks = 1\n    for i in range(1, len(matched_positions)):\n        if matched_positions[i] != matched_positions[i-1] + 1:\n            chunks += 1\n    \n    penalty = gamma * (chunks / matches) ** beta\n    \n    # 5. Final score\n    score = (1 - penalty) * f_mean\n    return round(score, 3)\n```\n\nExample\n-------\n```python\nr =the quick brown fox jumps over the lazy dogc =the quick fox jumps over lazy dogprint(meteor_score(r, c))   # -> 0.755\n```\n\nThe function follows METEOR\u2019s core steps\u2014exact-word matching, precision/recall harmonic mean, and a fragmentation penalty\u2014and returns the score rounded to three decimal places, satisfying the stated task constraints."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute Pointwise Mutual Information (PMI) between two events.\n\n    Parameters\n    ----------\n    joint_counts : int or float\n        Number of times both events co-occur.\n    total_counts_x : int or float\n        Total occurrences of event X.\n    total_counts_y : int or float\n        Total occurrences of event Y.\n    total_samples : int or float\n        Total number of samples/observations.\n\n    Returns\n    -------\n    float\n        PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Convert to float for safety\n    joint_counts   = float(joint_counts)\n    total_counts_x = float(total_counts_x)\n    total_counts_y = float(total_counts_y)\n    total_samples  = float(total_samples)\n\n    # Guard against zero counts to avoid division / log of zero\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0 or total_samples == 0:\n        return 0.0\n\n    # PMI = log2( P(x,y) / (P(x) * P(y)) )\n    #      = log2( (joint_counts/total_samples) / \n    #              ((total_counts_x/total_samples)*(total_counts_y/total_samples)) )\n    #      = log2( joint_counts * total_samples / (total_counts_x * total_counts_y) )\n    pmi = np.log2(joint_counts * total_samples / (total_counts_x * total_counts_y))\n    \n    return round(pmi, 3)\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n       Perform Min-Max normalization on a list of integers.\n    \n    Each value v is scaled to: (v \u2212 min) / (max \u2212 min)\n    Results are rounded to 4 decimal places and lie in [0, 1].\n    \n    Args:\n        x (list[int]): List of integer feature values.\n        \n    Returns:\n        list[float]: Normalized values rounded to 4 decimals.\n                     Returns an empty list for an empty input.\n                     If all elements are identical, every output is 0.0.\n       if not x:                       # Empty input\n        return []\n    \n    v_min, v_max = min(x), max(x)\n    \n    # Avoid division by zero when all values are identical\n    if v_max == v_min:\n        return [0.0 for _ in x]\n    \n    span = v_max - v_min\n    return [round((v - v_min) / span, 4) for v in x]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Simple 1-D residual block (\u00e0-la ResNet) implemented with NumPy.\n\n    Parameters\n    ----------\n    x  : np.ndarray\n        Input vector of shape (N,).\n    w1 : np.ndarray\n        First weight matrix of shape (M, N).\n    w2 : np.ndarray\n        Second weight matrix of shape (N, M).\n\n    Returns\n    -------\n    list\n        Output vector (shape = input shape) after residual connection,\n        ReLU-activated and rounded to 4 decimal places.\n    \"\"\"\n\n    # ---- inner helpers ----------------------------------------------------- #\n    def relu(t):\n        \"\"\"Element-wise ReLU.\"\"\"\n        return np.maximum(0, t)\n    # ----------------------------------------------------------------------- #\n\n    # Forward pass through the two linear layers + ReLUs\n    z1 = w1 @ x          # Linear 1\n    a1 = relu(z1)        # ReLU 1\n    z2 = w2 @ a1         # Linear 2\n    a2 = relu(z2)        # ReLU 2\n\n    # Add shortcut (residual) and final activation\n    out = relu(a2 + x)\n\n    # Round and convert to list per requirements\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n       Perform Global Average Pooling on a 3-D tensor.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array with shape (height, width, channels).\n\n    Returns\n    -------\n    np.ndarray\n        1-D array of length `channels` containing the average value of each\n        feature map.\n\n    Raises\n    ------\n    TypeError\n        If `x` is not a NumPy array.\n    ValueError\n        If `x` does not have exactly 3 dimensions.\n       # Basic validation\n    if not isinstance(x, np.ndarray):\n        raise TypeErrorInput must be a NumPy array    if x.ndim != 3:\n        raise ValueError(fExpected a 3-D tensor (H, W, C), got {x.ndim}-D    # Compute mean over height and width, keep channels\n    return x.mean(axis=(0, 1))\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, \n                        gamma: np.ndarray, \n                        beta: np.ndarray, \n                        epsilon: float = 1e-5):\n    \"\"\"\n    Batch\u2013Norm for a BCHW tensor.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input tensor of shape (N, C, H, W).\n    gamma : np.ndarray\n        Scale parameters of shape (C,).\n    beta : np.ndarray\n        Shift parameters of shape (C,).\n    epsilon : float, optional\n        Small constant for numerical stability.\n\n    Returns\n    -------\n    list\n        The batch-normalized tensor rounded to 4 decimals and\n        converted to a Python list.\n    \"\"\"\n    # Ensure float precision\n    X     = X.astype(np.float64)\n    gamma = gamma.astype(np.float64).reshape(1, -1, 1, 1)\n    beta  = beta.astype(np.float64).reshape(1, -1, 1, 1)\n\n    # Compute channel-wise mean and variance across N, H, W\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)           # shape: (1,C,1,1)\n    var  = X.var(axis=(0, 2, 3), keepdims=True)            # shape: (1,C,1,1)\n\n    # Normalize, then scale and shift\n    X_hat = (X - mean) / np.sqrt(var + epsilon)            # normalized\n    out   = gamma * X_hat + beta                           # scale + shift\n\n    # Round to 4 decimal places and convert to list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of a single polynomial term c * x^n at the point x,\n    using the power rule: d/dx [c * x^n] = c * n * x^(n - 1).\n\n    Parameters\n    ----------\n    c : float\n        Coefficient of the term.\n    x : float\n        Point at which to evaluate the derivative.\n    n : float\n        Exponent of x in the term.\n\n    Returns\n    -------\n    float\n        The derivative value rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:                       # derivative of a constant term\n        return 0.0\n    derivative_val = c * n * (x ** (n - 1))\n    return round(derivative_val, 4)\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n       Compute an orthonormal basis (via Gram\u2013Schmidt) for the sub-space spanned by a\n    collection of 2-D vectors.\n\n    Parameters\n    ----------\n    vectors : list[list[float]]\n        The input vectors.  Each inner list must have length 2.\n    tol : float, optional\n        Threshold below which a vector is considered to have zero norm\n        (i.e. to be linearly dependent on previously accepted vectors).\n\n    Returns\n    -------\n    list[list[float]]\n        Orthonormal basis vectors, rounded to 4 decimals and converted to\n        regular Python lists.\n       # hold the orthonormal vectors as np.ndarray\u2019s while we build them\n    ortho = []\n\n    for v in vectors:\n        # ensure we work with float-64 numpy array\n        v = np.asarray(v, dtype=float)\n\n        # subtract projections onto previously accepted basis vectors\n        for u in ortho:\n            v -= np.dot(v, u) * u\n\n        # check linear independence\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            ortho.append(v / norm)          # normalise and keep\n\n    # prepare the result: round and convert to normal Python lists\n    return [np.round(u, 4).tolist() for u in ortho]\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n       Compute the cross-product of two 3-D vectors.\n\n    Parameters\n    ----------\n    a, b : array-like\n        The input vectors. Each must be convertible to a 1-D array of length 3.\n\n    Returns\n    -------\n    list\n        The cross-product, rounded to 4 decimal places and converted to a list.\n       # Convert inputs to numpy arrays of floats\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n\n    # Basic validation\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueErrorBoth input vectors must be 1-D with exactly 3 elements    # Compute the cross product\n    cross = np.cross(a, b)\n\n    # Round to 4 decimal places and convert to list\n    cross_rounded = np.round(cross, 4)\n\n    return cross_rounded.tolist()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve Ax = b using Cramer's Rule.\n    \n    Parameters\n    ----------\n    A : array-like (n, n)\n        Coefficient matrix (must be square).\n    b : array-like (n,) or (n, 1)\n        Constant terms.\n        \n    Returns\n    -------\n    list\n        Solution vector rounded to 4 decimals,\n        or -1 if det(A) == 0 (no unique solution).\n    \"\"\"\n    # --- Convert inputs to numpy arrays -------------------------------------\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float).flatten()   # ensure shape (n,)\n    \n    # --- Basic validation ----------------------------------------------------\n    if A.ndim != 2 or A.shape[0] != A.shape[1] or b.size != A.shape[0]:\n        raise ValueError(\"A must be square and b must have compatible length.\")\n    \n    n = A.shape[0]\n    \n    # --- Determinant of A ----------------------------------------------------\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:          # treat near-zero as singular\n        return -1\n    \n    # --- Apply Cramer's Rule -------------------------------------------------\n    x = np.empty(n, dtype=float)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b                # replace i-th column with b\n        det_Ai = np.linalg.det(Ai)\n        x[i] = det_Ai / det_A\n    \n    # --- Round results and return as Python list ----------------------------\n    x = np.round(x, 4)\n    return x.tolist()\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n       Compute the Bhattacharyya distance between two discrete distributions.\n\n    Parameters\n    ----------\n    p, q : list[float]\n        Lists containing the (non-negative) weights of two discrete\n        probability distributions.  They need not be normalised beforehand.\n\n    Returns\n    -------\n    float\n        Bhattacharyya distance rounded to 4 decimal places.\n        If the lists have unequal length or are empty, 0.0 is returned.\n       # Basic sanity checks\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n\n    # Convert to NumPy arrays for vectorised operations\n    p_arr = np.asarray(p, dtype=float)\n    q_arr = np.asarray(q, dtype=float)\n\n    # Invalid inputs (negative probabilities) \u21d2 return 0.0\n    if np.any(p_arr < 0) or np.any(q_arr < 0):\n        return 0.0\n\n    # Normalise so they sum to 1 (only if sums are positive)\n    p_sum = p_arr.sum()\n    q_sum = q_arr.sum()\n    if p_sum == 0 or q_sum == 0:\n        return 0.0\n\n    p_arr /= p_sum\n    q_arr /= q_sum\n\n    # Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p_arr * q_arr))\n\n    # Guard against log(0)\n    if bc <= 0:\n        return float('inf')\n\n    # Bhattacharyya distance\n    distance = -np.log(bc)\n\n    return round(float(distance), 4)\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n       Compute the element-wise sum of two vectors (Python lists).\n\n    Parameters\n    ----------\n    a : list[int | float]\n        First vector.\n    b : list[int | float]\n        Second vector.\n\n    Returns\n    -------\n    list[int | float]\n        Element-wise sum of a and b if they have the same length.\n    int\n        -1 if the vectors have incompatible dimensions (different lengths).\n\n    Examples\n    --------\n    >>> vector_sum([1, 2, 3], [4, 5, 6])\n    [5, 7, 9]\n    >>> vector_sum([1.5, 2.5], [3.5, 4.5])\n    [5.0, 7.0]\n    >>> vector_sum([1, 2], [1, 2, 3])\n    -1\n       # Check for compatible dimensions\n    if len(a) != len(b):\n        return -1\n\n    # Compute element-wise sum\n    return [x + y for x, y in zip(a, b)]\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n       REINFORCE policy\u2013gradient estimator.\n\n    Parameters\n    ----------\n    theta     : np.ndarray (num_states , num_actions)\n                Current policy parameters.\n    episodes  : list of episodes.\n                Each episode is a list of (state , action , reward) tuples.\n\n    Returns\n    -------\n    list      : Gradient having the same shape as `theta`, rounded to\n                4 decimal places and converted to a regular Python list.\n       # --- helpers ----------------------------------------------------------\n    def softmax_row(row: np.ndarray) -> np.ndarray:\n        row = row - np.max(row)          # numerical stability\n        e = np.exp(row)\n        return e / e.sum()\n\n    # --- main loop --------------------------------------------------------\n    grad = np.zeros_like(theta, dtype=float)\n    total_timesteps = 0\n\n    for episode in episodes:\n        if not episode:                     # skip empty episodes\n            continue\n\n        # ---------- compute returns G_t ----------\n        # G_t = R_t + R_{t+1} + \u2026    (undiscounted)\n        rewards = [step[2] for step in episode]\n        returns = np.cumsum(rewards[::-1])[::-1]   # reverse cumulative sum\n\n        # ---------- accumulate gradients ----------\n        for (state, action, _), G_t in zip(episode, returns):\n            pi_s = softmax_row(theta[state])       # \u03c0(a | s)\n\n            # \u2207_\u03b8 log \u03c0(a|s)  :  one_hot(a) - \u03c0(s)\n            grad_row = -pi_s\n            grad_row[action] += 1.0\n\n            grad[state] += G_t * grad_row\n\n        total_timesteps += len(episode)\n\n    # Avoid division by zero in degenerate case\n    if total_timesteps > 0:\n        grad /= total_timesteps\n\n    # rounding & returning as regular python list\n    grad = np.round(grad, 4)\n    return grad.tolist()\n```"}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n       Estimate the compute (in FLOPs) of a single forward pass through\n    1) a standard dense (fully-connected) layer\n    2) a sparsely\u2013gated Mixture-of-Experts (MoE) layer\n\n    Parameters\n    ----------\n    n_experts : int      # total number of experts in the MoE layer\n    k_active  : int      # experts actually evaluated per example (sparsity)\n    d_in      : int      # input dimension\n    d_out     : int      # output dimension of each expert / dense layer\n\n    Returns\n    -------\n    dict with keys\n        dense_flops      : FLOPs for a dense layer (rounded, 1-dp)\n        moe_flops        : FLOPs for an MoE layer  (rounded, 1-dp)\n        savings_percent  : Percentage reduction   (rounded, 1-dp)\n       # -------- 1. Dense layer ----------------------------------------------\n    # Every output unit: d_in multiplications + (d_in-1) additions  \u2248 2\u00b7d_in\n    # Across d_out units \u21d2 2\u00b7d_in\u00b7d_out FLOPs\n    flops_dense = 2.0 * d_in * d_out\n\n    # -------- 2. MoE layer -------------------------------------------------\n    # a) Gating network  : linear transform (d_in \u2192 n_experts)  + softmax\n    flops_gate_linear = 2.0 * d_in * n_experts\n    flops_gate_softmx = 5.0 * n_experts            # exp, add, div \u2013 rough count\n\n    # b) Experts that are actually executed\n    flops_experts     = 2.0 * d_in * d_out * k_active\n\n    flops_moe = flops_gate_linear + flops_gate_softmx + flops_experts\n\n    # -------- 3. Savings ---------------------------------------------------\n    savings = (1.0 - flops_moe / flops_dense) * 100.0\n\n    # -------- 4. Round to nearest 0.1 -------------------------------------\n    result = {\n        'dense_flops'     : round(flops_dense,     1),\n        'moe_flops'       : round(flops_moe,       1),\n        'savings_percent' : round(savings,         1)\n    }\n    return result\n```\n\nExample\n```\n>>> compute_efficiency(n_experts=64, k_active=4, d_in=4096, d_out=4096)\n{'dense_flops': 33554432.0,\n 'moe_flops':   13443072.0,\n 'savings_percent': 60.0}\n```\nHence, running only four out of sixty-four experts cuts the compute by roughly 60 %."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n\ndef _softplus(x: np.ndarray) -> np.ndarray:\n       Numerically-stable softplus:  log(1 + e^x)\n       return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)\n\n\ndef _rowwise_softmax(logits: np.ndarray) -> np.ndarray:\n       Standard softmax on the last axis of a 2-D array.\n       logits_max = np.max(logits, axis=1, keepdims=True)          # for numerical stability\n    exp_logits = np.exp(logits - logits_max)\n    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n       Noisy-Top-K gating used in Mixture-of-Experts (MoE).\n\n    Parameters\n    ----------\n    X         : (batch_size, input_dim)              \u2013 input minibatch\n    W_g       : (input_dim, n_experts)               \u2013 clean-logit weights\n    W_noise   : (input_dim, n_experts)               \u2013 noise-stddev weights\n    N         : (batch_size, n_experts)              \u2013 i.i.d. Gaussian noise already sampled\n    k         : int                                  \u2013 how many experts to keep (per sample)\n\n    Returns\n    -------\n    Python list produced from the final gating-probability matrix,\n    rounded to 4 decimal places.\n       batch_size, n_experts = X.shape[0], W_g.shape[1]\n\n    # 1. Clean logits\n    clean_logits = X @ W_g                                # (B, E)\n\n    # 2. Noise standard deviations (positive via softplus)\n    sigma = _softplus(X @ W_noise) + 1e-8                 # (B, E)\n\n    # 3. Add the noise\n    noisy_logits = clean_logits + sigma * N               # (B, E)\n\n    # 4. Keep only the Top-k experts for every example\n    if k >= n_experts:                                    # degenerate \u201call experts\u201d case\n        probs = _rowwise_softmax(noisy_logits)\n    else:\n        # indices of the k largest noisy logits (unordered)\n        topk_idx = np.argpartition(noisy_logits, -k, axis=1)[:, -k:]   # (B, k)\n        probs = np.zeros_like(noisy_logits)\n\n        # Compute softmax only over those k logits for every sample\n        for i in range(batch_size):\n            idx = topk_idx[i]\n            logits_i = noisy_logits[i, idx]\n            probs_i = np.exp(logits_i - np.max(logits_i))\n            probs_i = probs_i / np.sum(probs_i)\n            probs[i, idx] = probs_i\n\n    # 5. Round to 4 decimal places and convert to python list\n    probs = np.round(probs, 4)\n\n    return probs.tolist()\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef moe(x: np.ndarray, \n        We: np.ndarray,   # shape: (n_experts, d_in, d_out)\n        Wg: np.ndarray,   # shape: (d_in, n_experts)\n        n_experts: int, \n        top_k: int):\n    \"\"\"\n    Sparse Mixture-of-Experts layer (softmax gating + top-k routing).\n\n    Parameters\n    ----------\n    x : ndarray              shape (tokens, d_in)\n    We: ndarray              shape (n_experts, d_in, d_out)\n    Wg: ndarray              shape (d_in, n_experts)\n    n_experts : int          number of experts  (sanity-check only)\n    top_k : int              number of experts to route each token to\n\n    Returns\n    -------\n    list                     Python list of the MoE outputs rounded to 4-decimals\n    \"\"\"\n    # ------------- safety / shape checks -----------------\n    if n_experts != We.shape[0]:\n        raise ValueError(\"n_experts does not match first dim. of We\")\n    if top_k > n_experts:\n        raise ValueError(\"top_k cannot exceed n_experts\")\n    if Wg.shape[1] != n_experts:\n        raise ValueError(\"second dim. of Wg must equal n_experts\")\n    if x.shape[-1] != Wg.shape[0]:\n        raise ValueError(\"last dim. of x must equal first dim. of Wg\")\n\n    tokens, d_in = x.shape\n    d_out = We.shape[2]\n\n    # ------------------ 1. gating ------------------------\n    logits = x @ Wg                                # (tokens, n_experts)\n    logits -= logits.max(axis=1, keepdims=True)    # for numerical stability\n    probs  = np.exp(logits)\n    probs /= probs.sum(axis=1, keepdims=True)      # softmax over experts\n\n    # ------------------ 2. top-k routing ----------------\n    top_idx  = np.argsort(-probs, axis=1)[:, :top_k]                 # (tokens, k)\n    rows     = np.arange(tokens)[:, None]\n    top_prob = probs[rows, top_idx]                                  # (tokens, k)\n    top_prob /= top_prob.sum(axis=1, keepdims=True)                  # renormalise\n\n    # ------------------ 3. expert transforms ------------\n    output = np.zeros((tokens, d_out))\n    for t in range(tokens):\n        for j in range(top_k):\n            e_id   = top_idx[t, j]               # chosen expert index\n            weight = We[e_id]                    # (d_in , d_out)\n            out_e  = x[t] @ weight               # (d_out,)\n            output[t] += top_prob[t, j] * out_e  # weighted aggregation\n\n    # ------------------ 4. rounding / return ------------\n    output = np.round(output, 4)\n    return output.tolist()\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\nnp.random.seed(42)\n\ndef group_normalization(X: np.ndarray, \n                        gamma: np.ndarray, \n                        beta : np.ndarray, \n                        num_groups: int, \n                        epsilon : float = 1e-5):\n    \"\"\"\n    Group Normalization for a 4-D tensor (B, C, H, W).\n\n    Parameters\n    ----------\n    X          : np.ndarray, shape (B, C, H, W)\n    gamma      : np.ndarray, per-channel scale, shape (C,) or broadcastable to (1,C,1,1)\n    beta       : np.ndarray, per-channel shift, shape (C,) or broadcastable to (1,C,1,1)\n    num_groups : int        , number of channel groups (C must be divisible by num_groups)\n    epsilon    : float      , numerical stability term\n\n    Returns\n    -------\n    list : normalized tensor rounded to 4 decimal places, converted via .tolist()\n    \"\"\"\n    \n    B, C, H, W = X.shape\n    if C % num_groups != 0:\n        raise ValueError(\"Number of channels must be divisible by num_groups.\")\n        \n    group_size = C // num_groups\n    \n    # 1. Reshape so that channels are split into `num_groups`\n    X_group = X.reshape(B, num_groups, group_size, H, W)     # (B, G, gC, H, W)\n\n    # 2. Compute mean and variance for each group\n    mean = X_group.mean(axis=(2, 3, 4), keepdims=True)       # (B, G, 1, 1, 1)\n    var  = X_group.var (axis=(2, 3, 4), keepdims=True)       # (B, G, 1, 1, 1)\n\n    # 3. Normalize\n    X_norm_group = (X_group - mean) / np.sqrt(var + epsilon) # (B, G, gC, H, W)\n\n    # 4. Restore original shape\n    X_norm = X_norm_group.reshape(B, C, H, W)                # (B, C, H, W)\n\n    # 5. Apply learned scale (gamma) and shift (beta)\n    #    Ensure gamma and beta are broadcastable\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta  =  beta.reshape(1, C, 1, 1)\n    out   = X_norm * gamma + beta\n\n    # 6. Round to 4 decimals and convert to Python list\n    return np.round(out, 4).tolist()\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\n# land\u2019s height\ndef _f(x: float) -> float:         \n    return x**4 - 3*x**3 + 2\n\n# its gradient\ndef _grad_f(x: float) -> float:    \n    return 4*x**3 - 9*x**2\n\n\ndef find_treasure(start_x: float,\n                  learning_rate: float = 0.1,\n                  tolerance: float = 1e-6,\n                  max_iters: int = 10_000) -> float:\n       Gradient\u2013descent search for the global minimum of\n        f(x) = x\u2074 \u2013 3x\u00b3 + 2\n\n    Parameters\n    ----------\n    start_x : float\n        Initial position on the \u201cmap\u201d.\n    learning_rate : float, default 0.1\n        Step size for every iteration.\n    tolerance : float, default 1e-6\n        Stop when |f\u2032(x)| becomes smaller than this.\n    max_iters : int, default 10 000\n        Hard limit on iterations in case convergence is slow.\n\n    Returns\n    -------\n    float\n        x-coordinate of the treasure, rounded to 4 decimals.\n       x = float(start_x)\n\n    for _ in range(max_iters):\n        g = _grad_f(x)\n\n        # have we dug deep enough?\n        if abs(g) < tolerance:\n            break\n\n        # basic gradient\u2013descent step\n        x_new = x - learning_rate * g\n\n        # if we take a numerically wild step, back off and continue\n        if np.isnan(x_new) or np.isinf(x_new):\n            learning_rate *= 0.1\n            continue\n\n        x = x_new\n\n    # final answer rounded to the 4th decimal place\n    return np.round(x, 4)\n```\n\nTypical usage (any random start point):\n\n```python\n>>> np.random.seed(1)\n>>> start = np.random.uniform(-10, 10)\n>>> start\n-1.6595591990327396\n>>> find_treasure(start)\n2.25\n```\n\nNo matter where you begin, the algorithm homes in on the global minimum at x \u2248 2.25, faithfully returning the position to 4-decimal precision."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic-Tanh (DyT)\n\n    Parameters\n    ----------\n    x      : np.ndarray \u2013 input tensor.\n    alpha  : float      \u2013 output\u2013range (amplitude) scaling factor.\n    gamma  : float      \u2013 slope/temperature term applied inside tanh.\n    beta   : float      \u2013 additive bias applied after the non-linearity.\n\n    Returns\n    -------\n    list[float] \u2013 flattened python list with every value rounded to 4 decimals.\n    \"\"\"\n    # Ensure `x` is a NumPy array with float dtype\n    x = np.asarray(x, dtype=np.float32)\n\n    # Core Dynamic-Tanh transformation\n    y = beta + alpha * np.tanh(gamma * x)\n\n    # Round to 4 decimals, flatten, and convert to a python list\n    return np.round(y, 4).reshape(-1).tolist()\n```"}
