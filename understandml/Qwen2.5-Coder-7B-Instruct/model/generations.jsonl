{"task_id": 0, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"\n    Implement Ridge (L2-regularised) Linear Regression using its closed-form solution.\n    \n    Parameters:\n    - data: A numpy ndarray where the last column is the target y and all preceding columns are the features X.\n    - alpha: A float representing the regularisation strength. Default is 0.0.\n    \n    Returns:\n    - A list of floats representing the weight vector rounded to 4 decimal places.\n    \"\"\"\n    # Extract features and target\n    X = data[:, :-1]\n    y = data[:, -1]\n    \n    # Add bias term to features\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Compute the weight vector using the closed-form solution\n    try:\n        # Check if the matrix is invertible\n        if np.linalg.det(X_b.T @ X_b + alpha * np.eye(X_b.shape[1])) == 0:\n            return -1\n        w = np.linalg.inv(X_b.T @ X_b + alpha * np.eye(X_b.shape[1])) @ X_b.T @ y\n    except np.linalg.LinAlgError:\n        return -1\n    \n    # Round the coefficients to 4 decimal places and return as a list\n    return [round(coef, 4) for coef in w]\n\n# Example usage:\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nalpha = 0.1\nresult = ridge_regression(data, alpha)\nprint(result)\n```\n\nThis code defines a function `ridge_regression` that takes a numpy array `data` and a regularization strength `alpha` as input. It computes the weight vector using the closed-form solution for Ridge Regression. If the matrix is not invertible, it returns `-1`. The result is rounded to 4 decimal places and returned as a list."}
{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    gram_matrix = np.dot(X_train, X_train.T)\n    \n    for _ in range(n_iter):\n        updated = False\n        for i in range(n_samples):\n            activation = np.sum(alpha * y_train * gram_matrix[:, i]) + b\n            if y_train[i] * (activation + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_train[i]\n                updated = True\n        if not updated:\n            break\n    \n    w = np.round(np.sum(alpha * y_train * X_train, axis=0), 4)\n    b = round(b, 4)\n    \n    return list(w), b\n```\n\nThis implementation of the dual-form perceptron learning algorithm follows the steps outlined in the task description. It initializes the dual parameters `alpha` and the bias `b`, computes the Gram matrix, and iteratively updates the parameters based on the margin condition. After training, it computes the primal weight vector `w` from the final `alpha` values and returns the weight vector and bias, both rounded to four decimal places."}
{"task_id": 2, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    def identity_link(mu):\n        return mu\n\n    def identity_inv_link(eta):\n        return eta\n\n    def identity_derivative(mu):\n        return np.ones_like(mu)\n\n    def log_link(mu):\n        return np.log(mu)\n\n    def log_inv_link(eta):\n        return np.exp(eta)\n\n    def log_derivative(mu):\n        return 1 / mu\n\n    def logit_link(mu):\n        return np.log(mu / (1 - mu))\n\n    def logit_inv_link(eta):\n        return 1 / (1 + np.exp(-eta))\n\n    def logit_derivative(mu):\n        return 1 / (mu * (1 - mu))\n\n    return {\n        \"identity\": {\n            \"link\": identity_link,\n            \"inv_link\": identity_inv_link,\n            \"derivative\": identity_derivative\n        },\n        \"log\": {\n            \"link\": log_link,\n            \"inv_link\": log_inv_link,\n            \"derivative\": log_derivative\n        },\n        \"logit\": {\n            \"link\": logit_link,\n            \"inv_link\": logit_inv_link,\n            \"derivative\": logit_derivative\n        }\n    }\n\n# Example usage:\nlinks = glm_links()\nmu = np.array([0.2, 0.8])\neta = links[\"logit\"][\"link\"](mu)        # \u2192 [-1.3863, 1.3863]\nmu2 = links[\"logit\"][\"inv_link\"](eta)    # \u2192 [0.2, 0.8]\n```\n\nThis implementation provides the required link functions for the identity, log, and logit families in a dictionary format. Each function is designed to work with both scalar and numpy array inputs, performing element-wise operations as required."}
{"task_id": 3, "completion_id": 0, "completion": "To solve the problem of computing the depth of the leaf reached by every sample in a random isolation tree, we need to follow the steps outlined in the task description. Here's a step-by-step implementation of the `isolation_tree_path_length` function:\n\n1. **Initialize the Random Seed**: Ensure that the random seed is set to 0 to make the results reproducible.\n2. **Define the Tree Node Class**: Create a class to represent each node in the isolation tree. Each node will store the indices of the samples that reach it and whether it is a leaf node.\n3. **Build the Isolation Tree**: Recursively build the isolation tree according to the rules provided.\n4. **Traverse the Tree**: Traverse the tree to compute the depth of the leaf for each sample.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\nclass TreeNode:\n    def __init__(self, indices):\n        self.indices = indices\n        self.left = None\n        self.right = None\n        self.is_leaf = False\n\ndef build_isolation_tree(data, indices):\n    if len(indices) < 3:\n        node = TreeNode(indices)\n        node.is_leaf = True\n        return node\n    \n    np.random.seed(0)  # Ensure reproducibility\n    feature_index = np.random.randint(data.shape[1])\n    down = np.min(data[indices, feature_index])\n    up = np.max(data[indices, feature_index])\n    v = np.random.uniform(down, up)\n    \n    left_indices = indices[data[indices, feature_index] <= v]\n    right_indices = indices[data[indices, feature_index] > v]\n    \n    node = TreeNode(indices)\n    node.left = build_isolation_tree(data, left_indices)\n    node.right = build_isolation_tree(data, right_indices)\n    \n    return node\n\ndef traverse_tree(node, depth):\n    if node.is_leaf:\n        return depth\n    else:\n        return max(traverse_tree(node.left, depth + 1), traverse_tree(node.right, depth + 1))\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    n_samples = data.shape[0]\n    indices = np.arange(n_samples)\n    root = build_isolation_tree(data, indices)\n    \n    depths = [traverse_tree(root, 0) for _ in range(n_samples)]\n    return depths\n\n# Example usage:\ndata = np.arange(5).reshape(-1, 1)\nprint(isolation_tree_path_length(data))  # Output: [2, 2, 2, 1, 1]\n```\n\n### Explanation:\n1. **TreeNode Class**: This class represents a node in the isolation tree. It stores the indices of the samples that reach it and whether it is a leaf node.\n2. **build_isolation_tree Function**: This function recursively builds the isolation tree. If the number of samples is less than 3, it creates a leaf node. Otherwise, it selects a random feature, splits the samples based on a random value, and recursively builds the left and right subtrees.\n3. **traverse_tree Function**: This function traverses the tree to compute the depth of the leaf for each sample.\n4. **isolation_tree_path_length Function**: This function initializes the process by building the isolation tree and then traversing it to compute the depths of the leaves for all samples.\n\nThis implementation ensures that the tree is built reproducibly and that the depths of the leaves are correctly computed for each sample."}
{"task_id": 7, "completion_id": 0, "completion": "To solve the problem of frequent itemset mining, we can use the Apriori algorithm. The Apriori algorithm is a classic algorithm for mining frequent itemsets in a database. It works by iteratively generating candidate itemsets and then pruning those that do not meet the minimum support threshold.\n\nHere's the implementation of the `find_frequent_itemsets` function using the Apriori algorithm:\n\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Calculate the minimum support count\n    minsup_count = math.ceil(minsup * len(transactions))\n\n    # Initialize the frequent itemsets dictionary\n    frequent_itemsets = {}\n\n    # Step 1: Find all frequent 1-itemsets\n    item_counts = Counter(item for transaction in transactions for item in set(transaction))\n    frequent_1_itemsets = {item: count for item, count in item_counts.items() if count >= minsup_count}\n    frequent_itemsets.update(frequent_1_itemsets)\n\n    # Step 2: Generate candidate itemsets and find frequent itemsets iteratively\n    k = 2\n    while frequent_itemsets:\n        # Generate candidate k-itemsets\n        candidate_itemsets = set()\n        for itemset1 in frequent_itemsets:\n            for itemset2 in frequent_itemsets:\n                if len(itemset1) == len(itemset2) == k - 1 and itemset1[:-1] == itemset2[:-1]:\n                    candidate_itemsets.add(tuple(sorted(itemset1 + (itemset2[-1],))))\n\n        # Count the support of each candidate k-itemset\n        candidate_counts = Counter()\n        for transaction in transactions:\n            for candidate in candidate_itemsets:\n                if set(candidate).issubset(set(transaction)):\n                    candidate_counts[candidate] += 1\n\n        # Filter out the candidate itemsets that do not meet the minimum support\n        frequent_itemsets = {itemset: count for itemset, count in candidate_counts.items() if count >= minsup_count}\n        frequent_itemsets.update(frequent_itemsets)\n\n        k += 1\n\n    return frequent_itemsets\n\n# Example usage\ntransactions = [\n    ['bread', 'milk'],\n    ['bread', 'diaper', 'beer', 'egg'],\n    ['milk', 'diaper', 'beer', 'coke'],\n    ['bread', 'milk', 'diaper', 'beer'],\n    ['bread', 'milk', 'diaper', 'coke']\n]\nminsup = 0.6\n\nresult = find_frequent_itemsets(transactions, minsup)\nprint(result)\n```\n\n### Explanation:\n1. **Initialization**:\n   - Calculate the minimum support count using `math.ceil(minsup * len(transactions))`.\n   - Initialize an empty dictionary `frequent_itemsets` to store the frequent itemsets.\n\n2. **Finding Frequent 1-Itemsets**:\n   - Use a `Counter` to count the occurrences of each item in all transactions.\n   - Filter out items that do not meet the minimum support threshold and add them to `frequent_itemsets`.\n\n3. **Generating and Filtering Candidate Itemsets**:\n   - Iterate to generate candidate itemsets of increasing size `k`.\n   - For each pair of frequent itemsets of size `k-1`, generate candidate itemsets of size `k` by combining them.\n   - Count the support of each candidate itemset by checking if it is a subset of any transaction.\n   - Filter out candidate itemsets that do not meet the minimum support threshold and add them to `frequent_itemsets`.\n\n4. **Return the Result**:\n   - Return the dictionary of frequent itemsets.\n\nThis implementation ensures that the algorithm works for any 0 < `minsup` \u2264 1 and does not use any third-party libraries."}
{"task_id": 11, "completion_id": 0, "completion": "Certainly! Below is the implementation of the K-Means clustering algorithm from scratch in Python, adhering to the given requirements:\n\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Step 1: Initialize cluster centers\n    centers = data[:k].copy()\n    \n    for iteration in range(max_iter):\n        # Step 2: Assign samples to the nearest center\n        labels = np.argmin(np.linalg.norm(data[:, np.newaxis] - centers, axis=2), axis=1)\n        \n        # Step 3: Recompute cluster centers\n        new_centers = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Step 4: Handle empty clusters by re-initializing their centers\n        for i in range(k):\n            if np.isnan(new_centers[i]).any():\n                new_centers[i] = data[np.random.randint(n_samples)]\n        \n        # Step 5: Check for convergence\n        if np.all(np.abs(new_centers - centers) < epsilon):\n            break\n        \n        centers = new_centers\n    \n    # Round the centers to 4 decimal places\n    centers = np.round(centers, 4)\n    \n    return centers.tolist(), labels.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [1, 4], [1, 0],\n#                  [4, 2], [4, 4], [4, 0]])\n# k = 2\n# centers, labels = kmeans(data, k)\n# print(\"Centers:\", centers)\n# print(\"Labels:\", labels)\n```\n\n### Explanation:\n1. **Initialization**: The first `k` samples are chosen as the initial cluster centers.\n2. **Assignment**: Each sample is assigned to the nearest center using the squared Euclidean distance.\n3. **Recomputation**: The new center of each cluster is computed as the arithmetic mean of all samples assigned to it.\n4. **Handling Empty Clusters**: If a cluster becomes empty, its center is re-initialized with a random sample from the dataset.\n5. **Convergence Check**: The algorithm stops if the maximum change of any center between two consecutive iterations is smaller than `epsilon` or if the maximum number of iterations (`max_iter`) is reached.\n6. **Rounding**: The final centers are rounded to 4 decimal places before returning.\n\nThis implementation ensures that the K-Means algorithm is implemented from scratch without relying on any external machine-learning libraries."}
{"task_id": 13, "completion_id": 0, "completion": "To implement the item-based k-NN collaborative filtering recommender, we need to follow the steps outlined in the description. Here's a complete implementation of the function `knn_recommend`:\n\n```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    n_user, n_item = data.shape\n    \n    # Step 1: Build the item-item similarity matrix\n    S = np.zeros((n_item, n_item))\n    for i in range(n_item):\n        for j in range(i + 1, n_item):\n            # Collect users that rated both items\n            rated_both = np.where((data[:, i] > 0) & (data[:, j] > 0))[0]\n            if len(rated_both) == 0:\n                continue\n            \n            # Form the rating vectors\n            v1 = data[rated_both, i]\n            v2 = data[rated_both, j]\n            \n            # Compute similarity based on the criterion\n            if criterion == 'cosine':\n                std1 = np.std(v1)\n                std2 = np.std(v2)\n                if std1 > 1e-3 and std2 > 1e-3:\n                    v1 = v1 - np.mean(v1)\n                    v2 = v2 - np.mean(v2)\n                similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n            elif criterion == 'pearson':\n                similarity = np.corrcoef(v1, v2)[0, 1]\n            else:\n                raise ValueError(\"Invalid criterion. Choose 'cosine' or 'pearson'.\")\n            \n            S[i, j] = similarity\n            S[j, i] = similarity\n    \n    # Step 2: Collect the indices of the items the active user has already rated\n    rated_items = np.where(data[user_ind, :] > 0)[0]\n    \n    # Step 3: Compute the predicted attractiveness score for every yet unrated item\n    scores = defaultdict(float)\n    for t in range(n_item):\n        if data[user_ind, t] > 0:\n            continue\n        \n        numerator = 0\n        denominator = 0\n        for i in rated_items:\n            numerator += data[user_ind, i] * S[t, i]\n            denominator += abs(S[t, i])\n        \n        if denominator > 0:\n            scores[t] = numerator / denominator\n    \n    # Step 4: Return the indices of the k most promising items\n    sorted_items = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n    return [item[0] for item in sorted_items[:k]]\n\n# Example usage:\ndata = np.array([\n    [5, 3, 0, 1],\n    [4, 0, 0, 1],\n    [1, 1, 0, 5],\n    [1, 0, 0, 4],\n    [0, 1, 5, 4],\n])\n\nuser_ind = 0\nk = 2\ncriterion = 'cosine'\n\nrecommendations = knn_recommend(data, user_ind, k, criterion)\nprint(recommendations)\n```\n\n### Explanation:\n1. **Item-Item Similarity Matrix Construction**:\n   - We iterate over all pairs of items `(i, j)` and collect users who have rated both items.\n   - For each pair, we compute the cosine similarity if the criterion is 'cosine' or the Pearson correlation if the criterion is 'pearson'.\n   - We store the similarity in the matrix `S`.\n\n2. **Collect Rated Items**:\n   - We identify the items that the active user has already rated.\n\n3. **Predict Attractiveness Scores**:\n   - For each unrated item `t`, we compute the predicted attractiveness score using the formula provided in the description.\n   - We store the scores in a dictionary `scores`.\n\n4. **Return Top k Recommendations**:\n   - We sort the items based on the predicted scores and return the top `k` items.\n\nThis implementation ensures that the function follows the specified algorithm and returns the correct recommendations."}
{"task_id": 19, "completion_id": 0, "completion": "To solve the problem of finding the best split for a gradient-boosting tree, we need to evaluate the gain for every possible split and choose the one that maximizes the gain. Here's a step-by-step approach to implement the `best_split` function:\n\n1. **Iterate over each feature**: For each feature, consider every unique value as a potential threshold.\n2. **Split the data**: For each threshold, split the data into left and right children.\n3. **Calculate the gain**: Compute the gain for each split using the provided formula.\n4. **Track the best split**: Keep track of the split that yields the maximum positive gain.\n\nHere's the implementation of the `best_split` function:\n\n```python\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n\n    for feature_index in range(n_features):\n        unique_values = np.unique(X[:, feature_index])\n        for threshold in unique_values:\n            # Split the data\n            left_indices = X[:, feature_index] <= threshold\n            right_indices = X[:, feature_index] > threshold\n            \n            # Ensure at least two samples on each side\n            if np.sum(left_indices) < 2 or np.sum(right_indices) < 2:\n                continue\n            \n            # Calculate the gain\n            left_g = g[left_indices]\n            left_h = h[left_indices]\n            right_g = g[right_indices]\n            right_h = h[right_indices]\n            \n            left_loss = -0.5 * (left_g.sum() ** 2) / (left_h.sum() + lam) + gamma\n            right_loss = -0.5 * (right_g.sum() ** 2) / (right_h.sum() + lam) + gamma\n            current_loss = -0.5 * (g.sum() ** 2) / (h.sum() + lam) + gamma\n            \n            gain = current_loss - left_loss - right_loss\n            \n            # Update the best split if the gain is better\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature_index\n                best_threshold = threshold\n\n    if best_gain > 0:\n        return best_feature, best_threshold\n    else:\n        return None\n```\n\n### Explanation:\n1. **Iterate over features**: We loop through each feature in the dataset.\n2. **Unique values**: For each feature, we find all unique values to consider as potential thresholds.\n3. **Split data**: We split the data into left and right children based on the threshold.\n4. **Check sample count**: We ensure that each split leaves at least two samples on each side.\n5. **Calculate gain**: We compute the gain for each split using the provided formula.\n6. **Track best split**: We keep track of the split that yields the maximum positive gain.\n\nThis function will return the best split that maximizes the gain, or `None` if no valid split yields a positive gain."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    # Convert input to NumPy array for uniform processing\n    x_array = np.array(x)\n    \n    # Compute the sigmoid function\n    sigmoid_x = 1 / (1 + np.exp(-x_array))\n    \n    # Compute the gradient of the sigmoid function\n    gradient_x = sigmoid_x * (1 - sigmoid_x)\n    \n    # Round the results to 4 decimal places\n    sigmoid_x_rounded = np.round(sigmoid_x, 4)\n    gradient_x_rounded = np.round(gradient_x, 4)\n    \n    # Convert the results back to the original input type\n    if isinstance(x, (int, float)):\n        return (float(sigmoid_x_rounded), float(gradient_x_rounded))\n    else:\n        return (sigmoid_x_rounded.tolist(), gradient_x_rounded.tolist())\n```\n\nThis function `sigmoid_activation` takes an input `x` which can be a scalar, a Python list, or a NumPy array. It computes the sigmoid of `x` and its gradient. The results are rounded to 4 decimal places. If the input is a scalar, the function returns a tuple of two floats. If the input is an array-like, the function returns a tuple of two lists. The conversion between NumPy arrays and Python lists is handled appropriately."}
{"task_id": 21, "completion_id": 0, "completion": "To implement the AdaBoost algorithm with one-dimensional decision stumps, we need to follow the steps outlined in the task description. Below is the complete implementation of the `adaboost_1d_predict` function:\n\n```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\n\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Initialize sample weights\n    n = len(x_train)\n    weights = np.ones(n) / n\n    \n    # Initialize the list to store the stumps and their coefficients\n    stumps = []\n    coefficients = []\n    \n    # Sort the training data to find the thresholds\n    sorted_indices = np.argsort(x_train)\n    x_train_sorted = x_train[sorted_indices]\n    y_train_sorted = y_train[sorted_indices]\n    \n    # Enumerate all possible stumps\n    for i in range(n - 1):\n        theta = (x_train_sorted[i] + x_train_sorted[i + 1]) / 2\n        stump = (0, theta)  # Predict +1 if x <= theta, -1 otherwise\n        \n        # Compute the weighted error for this stump\n        predictions = np.where(x_train_sorted <= theta, 1, -1)\n        errors = np.where(predictions != y_train_sorted, weights, 0)\n        weighted_error = np.sum(errors)\n        \n        # Compute the coefficient alpha\n        if weighted_error == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * math.log((1 - weighted_error) / weighted_error)\n        \n        # Store the stump and its coefficient\n        stumps.append(stump)\n        coefficients.append(alpha)\n        \n        # Update the sample weights\n        predictions = np.where(x_train <= theta, 1, -1)\n        errors = np.where(predictions != y_train, weights, 0)\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)\n        \n        # Check if the training error is within the allowed bound\n        ensemble_predictions = np.sign(np.sum(alpha * np.where(x_train <= theta, 1, -1) for alpha, theta in zip(coefficients, stumps)))\n        training_error = np.mean(ensemble_predictions != y_train)\n        if training_error <= epsilon:\n            break\n    \n    # Predict the labels for the test set\n    test_predictions = np.sign(np.sum(alpha * np.where(x_test <= theta, 1, -1) for alpha, theta in zip(coefficients, stumps)))\n    \n    return test_predictions.tolist()\n\n# Example usage:\nx_train = [1.0, 2.0, 3.0, 4.0, 5.0]\ny_train = [1, -1, 1, -1, 1]\nx_test = [2.5, 3.5, 4.5]\nepsilon = 0.0\npredictions = adaboost_1d_predict(x_train, y_train, x_test, epsilon)\nprint(predictions)\n```\n\n### Explanation:\n1. **Initialization**: We start by converting the input lists to numpy arrays for easier manipulation. We initialize the sample weights to be uniform.\n2. **Sorting**: We sort the training data to find the possible thresholds for the decision stumps.\n3. **Stump Enumeration**: For each pair of consecutive points, we create a decision stump and compute its weighted error.\n4. **Coefficient Calculation**: We calculate the coefficient `\u03b1` for each stump using the formula provided.\n5. **Weight Update**: We update the sample weights based on the error of the current stump.\n6. **Training Error Check**: We check if the training error of the current ensemble is within the allowed bound `epsilon`. If it is, we stop the training process.\n7. **Prediction**: Finally, we use the trained ensemble to predict the labels for the test set.\n\nThis implementation ensures that the AdaBoost algorithm with one-dimensional decision stumps is correctly implemented and returns the predicted labels for the test set."}
{"task_id": 25, "completion_id": 0, "completion": "To implement the `gaussian_svm_predict` function, we need to compute the decision function \\( g(z) \\) for each test sample \\( z \\) in \\( X_test \\) and then determine the predicted class as \\( \\text{sign}(g(z)) \\). Here's how you can do it:\n\n1. Compute the Euclidean distance between each test sample and each training sample.\n2. Apply the Gaussian kernel function to these distances.\n3. Compute the decision function using the given formula.\n4. Determine the predicted class for each test sample.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nimport math\n\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # Compute the decision function for each test sample\n    m, d = X_test.shape\n    n, _ = X_train.shape\n    \n    # Initialize the decision function values\n    decision_function = np.zeros(m)\n    \n    # Compute the decision function for each test sample\n    for i in range(m):\n        for j in range(n):\n            distance = np.linalg.norm(X_test[i] - X_train[j])\n            kernel_value = np.exp(-gamma * distance**2)\n            decision_function[i] += alpha[j] * y_train[j] * kernel_value\n    \n    # Add the bias term\n    decision_function += b\n    \n    # Determine the predicted class for each test sample\n    predicted_labels = np.sign(decision_function).astype(int).tolist()\n    \n    return predicted_labels\n```\n\n### Explanation:\n1. **Distance Calculation**: We compute the Euclidean distance between each test sample \\( z \\) and each training sample \\( x_i \\) using `np.linalg.norm`.\n2. **Kernel Function**: We apply the Gaussian kernel function \\( \\exp(-\\gamma \\cdot ||x_i - z||^2) \\).\n3. **Decision Function**: We compute the decision function \\( g(z) \\) by summing up the contributions from all training samples.\n4. **Prediction**: We determine the predicted class as \\( \\text{sign}(g(z)) \\) and convert the result to a list of integers.\n\nThis implementation is fully vectorized and should work efficiently for arbitrary numbers of training and test samples."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    m, n = len(X), len(X[0])\n    if k < 1 or k > min(m, n):\n        return -1\n\n    # Convert the list of lists to a numpy array\n    X_np = np.array(X)\n\n    # Perform SVD\n    U, s, Vt = np.linalg.svd(X_np, full_matrices=False)\n\n    # Truncate SVD\n    S_k = np.zeros((m, n))\n    np.fill_diagonal(S_k[:k, :k], s[:k])\n    X_hat_np = U @ S_k @ Vt\n\n    # Convert the reconstructed numpy array back to a list of lists\n    X_hat = X_hat_np.round(4).tolist()\n\n    # Calculate the mean squared error\n    mse = np.mean((X_np - X_hat_np) ** 2)\n    mse = round(mse, 4)\n\n    return X_hat, mse\n```\n\nThis function first checks if the value of `k` is valid. If it is not, it returns `-1`. If `k` is valid, it converts the input list of lists to a numpy array, performs the SVD, truncates it to keep only the first `k` singular values and vectors, reconstructs the matrix, and then rounds the reconstructed values and the MSE to four decimal places before returning them as a tuple."}
{"task_id": 29, "completion_id": 0, "completion": "To implement the one-dimensional gradient boosting regressor with decision stumps, we need to follow the steps outlined in the task description. Here's a step-by-step implementation of the `predict_boosting_tree` function:\n\n1. **Sort the training samples by the single feature \\( x \\).**\n2. **Determine all possible split points (mid-points between consecutive \\( x \\)-values).**\n3. **Iteratively add the stump that minimizes the squared residuals until the total residual sum of squares becomes smaller than the tolerance \\( \\epsilon \\).**\n4. **Return the prediction for \\( x_{\\text{query}} \\) obtained by summing the constants contributed by every learned stump.**\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    # Sort the training data by x\n    sorted_indices = np.argsort(x_train)\n    x_train = x_train[sorted_indices]\n    y_train = y_train[sorted_indices]\n    \n    # Initialize the ensemble and residuals\n    ensemble = []\n    residuals = y_train.copy()\n    \n    # Determine all possible split points\n    split_points = (x_train[:-1] + x_train[1:]) / 2\n    \n    # Iterate until the residual sum of squares is smaller than epsilon\n    while True:\n        min_rss = float('inf')\n        best_stump = None\n        \n        for s in split_points:\n            # Calculate the mean of residuals on both sides of the split\n            c1 = np.mean(residuals[x_train <= s])\n            c2 = np.mean(residuals[x_train > s])\n            \n            # Calculate the RSS if we add this stump\n            left_rss = np.sum((residuals[x_train <= s] - c1) ** 2)\n            right_rss = np.sum((residuals[x_train > s] - c2) ** 2)\n            current_rss = left_rss + right_rss\n            \n            # Update the best stump if this one is better\n            if current_rss < min_rss:\n                min_rss = current_rss\n                best_stump = (s, c1, c2)\n        \n        # If the best RSS is larger than epsilon, break the loop\n        if min_rss >= epsilon:\n            break\n        \n        # Add the best stump to the ensemble\n        ensemble.append(best_stump)\n        \n        # Update the residuals\n        for i, x in enumerate(x_train):\n            if x <= best_stump[0]:\n                residuals[i] -= best_stump[1]\n            else:\n                residuals[i] -= best_stump[2]\n    \n    # Predict the value for x_query\n    prediction = 0.0\n    for stump in ensemble:\n        if x_query <= stump[0]:\n            prediction += stump[1]\n        else:\n            prediction += stump[2]\n    \n    # Return the prediction rounded to 4 decimal places\n    return round(prediction, 4)\n\n# Example usage:\nx_train = [1, 2, 3, 4, 5]\ny_train = [2, 4, 6, 8, 10]\nx_query = 3.5\nepsilon = 1e-2\nprediction = predict_boosting_tree(x_train, y_train, x_query, epsilon)\nprint(prediction)  # Output should be close to 7.0\n```\n\nThis implementation follows the steps outlined in the task description and should work correctly for the given problem. The function sorts the training data, determines possible split points, iteratively adds the best stump to the ensemble, and updates the residuals until the stopping criterion is met. Finally, it predicts the value for the query point by summing the contributions of all learned stumps."}
{"task_id": 34, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `random_binary_tensor` function according to the provided specifications:\n\n```python\nimport numpy as np\n\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None) -> np.ndarray:\n    \"\"\"\n    Generates a random binary tensor with the specified shape and sparsity.\n    \n    Parameters:\n    - shape: A tuple of integers representing the shape of the tensor.\n    - sparsity: A float in the range [0, 1] representing the probability of generating a 1.0.\n    - seed: An optional integer to seed the random number generator for reproducibility.\n    \n    Returns:\n    - A numpy.ndarray of shape `shape` filled with 0.0s and 1.0s.\n      If `sparsity` is outside the legal range [0, 1], returns -1.\n    \"\"\"\n    if not (0 <= sparsity <= 1):\n        return -1\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random array of the specified shape\n    random_array = np.random.rand(*shape)\n    \n    # Convert the random array to binary based on the sparsity\n    binary_array = (random_array >= 1 - sparsity).astype(float)\n    \n    return binary_array\n\n# Example usage:\nshape = (2, 3)\nsparsity = 0.3\nseed = 42\nresult = random_binary_tensor(shape, sparsity, seed)\nprint(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the `sparsity` value is within the valid range \\([0, 1]\\). If not, it returns `-1`.\n2. **Random Seed**: If a `seed` is provided, it sets the seed for the random number generator using `np.random.seed(seed)` to ensure reproducibility.\n3. **Random Array Generation**: It generates a random array of the specified `shape` using `np.random.rand(*shape)`.\n4. **Binary Conversion**: It converts the random array to a binary array where each element is `1.0` if it is greater than or equal to `1 - sparsity`, otherwise `0.0`. This is achieved using the condition `(random_array >= 1 - sparsity).astype(float)`.\n5. **Return**: The function returns the resulting binary array.\n\nThis implementation ensures that the function meets all the specified requirements and constraints."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the median split for KD-Tree construction, we can use `numpy.argpartition` to efficiently find the median without sorting the entire column. Here's how you can do it:\n\n1. Use `numpy.argpartition` to find the index of the median value in column `d`.\n2. Use this index to partition the data into left and right subsets.\n3. Ensure the lists of indices are sorted.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    # Find the index of the median value in column d\n    median_index = np.argpartition(data[:, d], data.shape[0] // 2)[data.shape[0] // 2]\n    \n    # Get the median value\n    median_value = data[median_index, d]\n    \n    # Partition the data into left and right subsets\n    left_indices = np.where(data[:, d] < median_value)[0]\n    right_indices = np.where(data[:, d] > median_value)[0]\n    \n    # Ensure the pivot is the index of the median value\n    pivot = median_index\n    \n    return pivot, sorted(left_indices), sorted(right_indices)\n\n# Example usage:\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\nd = 0\npivot, left, right = get_split(data, d)\nprint(f\"Pivot: {pivot}, Left: {left}, Right: {right}\")\n```\n\n### Explanation:\n1. **Finding the Median Index**:\n   - `np.argpartition(data[:, d], data.shape[0] // 2)` returns the indices that would sort the column `d` up to the middle element.\n   - `median_index = np.argpartition(data[:, d], data.shape[0] // 2)[data.shape[0] // 2]` gives us the index of the median value in column `d`.\n\n2. **Partitioning the Data**:\n   - `left_indices = np.where(data[:, d] < median_value)[0]` finds the indices of rows where the value in column `d` is less than the median.\n   - `right_indices = np.where(data[:, d] > median_value)[0]` finds the indices of rows where the value in column `d` is greater than the median.\n\n3. **Sorting the Indices**:\n   - The `sorted` function ensures that the lists of indices are sorted increasingly.\n\nThis implementation ensures that the split is done in \\(O(n)\\) time complexity using `numpy.argpartition`, and the output is deterministic."}
{"task_id": 40, "completion_id": 0, "completion": "Here is a possible implementation of the `to_frames` function:\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # Check pre-conditions\n    assert x.ndim == 1, \"Input signal must be one-dimensional\"\n    assert stride >= 1, \"Stride must be at least 1\"\n    assert len(x) >= frame_width, \"Input signal must be at least as long as the frame width\"\n\n    # Calculate the number of frames\n    n_frames = (len(x) - frame_width) // stride + 1\n\n    # Calculate the strides for the new array\n    strides = (x.itemsize, stride * x.itemsize)\n\n    # Create the new array using as_strided\n    result = as_strided(x, shape=(n_frames, frame_width), strides=strides, writeable=writeable)\n\n    return result\n```\nThis implementation first checks the pre-conditions using `assert` statements. It then calculates the number of frames and the strides for the new array using the `as_strided` function. Finally, it creates the new array and returns it."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    # Convert input to a NumPy array for easier manipulation\n    X = np.array(X)\n    \n    # Check if the matrix is two-dimensional\n    if X.ndim != 2:\n        return False\n    \n    # Check if all elements are between 0 and 1 inclusive\n    if not np.all((X >= 0) & (X <= 1)):\n        return False\n    \n    # Check if each row sums to 1 within a tolerance of 1e-8\n    if not np.allclose(X.sum(axis=1), 1, atol=1e-8):\n        return False\n    \n    return True\n```\n\nThis function first converts the input matrix to a NumPy array for easier manipulation. It then checks if the matrix is two-dimensional and if all elements are within the valid probability range [0, 1]. Finally, it checks if each row sums to 1 within a specified numerical tolerance. If all these conditions are met, the function returns `True`, indicating that the matrix is row-stochastic. Otherwise, it returns `False`."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)\n    s = np.zeros(d)\n    \n    for i in range(max_iter):\n        if n < batch_size:\n            batch_indices = np.arange(n)\n        else:\n            batch_indices = np.random.choice(n, batch_size, replace=False)\n        \n        X_batch = X[batch_indices]\n        y_batch = y[batch_indices]\n        \n        g = (1 / n) * np.dot(X_batch.T, np.dot(X_batch, w) - y_batch)\n        \n        if np.linalg.norm(g) < epsilon:\n            break\n        \n        s = rho * s + (1 - rho) * g ** 2\n        w = w - eta * g / (np.sqrt(s + eps_station))\n    \n    return [round(w_i, 4) for w_i in w.tolist()]\n```\n\nThis implementation of the RMSprop algorithm for linear regression follows the given specifications. It initializes the weight vector `w` to zero and the running average of squared gradients `s` to zero. The algorithm iteratively updates the weight vector using the RMSprop rule until the norm of the gradient becomes smaller than `epsilon` or the maximum number of iterations is reached. The function returns the final weight vector rounded to four decimal places and converted to a Python list."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(x, deriv: bool = False):\n    \"\"\"Implement the SoftPlus activation function and its derivative.\"\"\"\n    if isinstance(x, (int, float)):\n        x = np.array([x])\n    \n    if deriv:\n        # Derivative of SoftPlus is the sigmoid function\n        result = 1 / (1 + np.exp(-x))\n    else:\n        # SoftPlus function\n        result = np.log1p(np.exp(x))\n    \n    # Round to the nearest 4th decimal\n    result = np.round(result, 4)\n    \n    # Convert back to list if the input was a scalar\n    if len(result) == 1:\n        return result[0]\n    else:\n        return result.tolist()\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `x` is a scalar (int or float) and converts it to a NumPy array if necessary.\n2. **Derivative Calculation**: If `deriv` is `True`, the function computes the sigmoid of `x` using `1 / (1 + np.exp(-x))`.\n3. **SoftPlus Calculation**: If `deriv` is `False`, the function computes the SoftPlus using `np.log1p(np.exp(x))` which is numerically stable.\n4. **Rounding**: The result is rounded to the nearest 4th decimal using `np.round(result, 4)`.\n5. **Output Formatting**: If the input was a scalar, the function returns a single float. Otherwise, it returns a list of rounded values.\n\nThis implementation ensures that the function handles both scalar and array inputs and provides the correct SoftPlus or its derivative with the specified rounding."}
{"task_id": 58, "completion_id": 0, "completion": "To implement the unsmoothed Maximum-Likelihood Estimation (MLE) N-gram language model, we need to follow these steps:\n\n1. **Count N-grams and their prefixes**: We will use `collections.Counter` to count the occurrences of each N-gram and its prefix in the corpus.\n2. **Calculate probabilities**: Using the counts, we will calculate the probability of each N-gram.\n3. **Compute the log-probability of the sequence**: We will sum the log-probabilities of each length-N sliding window in the sequence.\n\nHere is the complete implementation of the function:\n\n```python\nimport numpy as np\nfrom collections import Counter\nfrom collections import defaultdict\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n    \"\"\"\n    if N < 1:\n        raise ValueError(\"N must be at least 1\")\n    \n    # Count N-grams and their prefixes\n    ngram_counts = defaultdict(int)\n    prefix_counts = defaultdict(int)\n    \n    for i in range(len(corpus) - N + 1):\n        ngram = tuple(corpus[i:i+N])\n        prefix = tuple(corpus[i:i+N-1])\n        ngram_counts[ngram] += 1\n        prefix_counts[prefix] += 1\n    \n    # Calculate the log-probability of the sequence\n    total_log_prob = 0.0\n    \n    for i in range(len(sequence) - N + 1):\n        ngram = tuple(sequence[i:i+N])\n        prefix = tuple(sequence[i:i+N-1])\n        \n        if ngram_counts[ngram] == 0 or prefix_counts[prefix] == 0:\n            return float('-inf')\n        \n        log_prob = np.log(ngram_counts[ngram] / prefix_counts[prefix])\n        total_log_prob += log_prob\n    \n    return round(total_log_prob, 4)\n\n# Example usage:\ncorpus = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\nsequence = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\nN = 2\nprint(unsmoothed_ngram_log_prob(corpus, sequence, N))  # Output should be the log-probability of the sequence\n```\n\n### Explanation:\n1. **Counting N-grams and Prefixes**:\n   - We use `defaultdict(int)` to count the occurrences of each N-gram and its prefix.\n   - For each N-gram in the corpus, we update the counts in `ngram_counts` and `prefix_counts`.\n\n2. **Calculating Log-Probabilities**:\n   - We iterate over each length-N sliding window in the sequence.\n   - For each N-gram, we check if its count and the count of its prefix are non-zero.\n   - If either count is zero, we return `float('-inf')` immediately.\n   - Otherwise, we calculate the log-probability using the formula provided and add it to `total_log_prob`.\n\n3. **Rounding**:\n   - Finally, we return the total log-probability rounded to 4 decimal places.\n\nThis implementation ensures that the function correctly handles the unsmoothed MLE N-gram log-probability calculation and returns the appropriate values for the given constraints."}
{"task_id": 62, "completion_id": 0, "completion": "To implement the univariate Lasso regression with polynomial features using coordinate descent, we need to follow the steps outlined in the task description. Below is the complete implementation of the `lasso_regression` function:\n\n```python\nimport numpy as np\n\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Build the design matrix with polynomial features\n    X_poly = np.vander(X, degree + 1, increasing=True)\n    \n    # Initialize weights\n    w = np.zeros(degree + 1)\n    \n    # Coordinate descent\n    for _ in range(n_iterations):\n        # Update bias term\n        w[0] = np.mean(y - X_poly[:, 1:].dot(w[1:]))\n        \n        # Update other coefficients\n        for j in range(1, degree + 1):\n            x_j = X_poly[:, j]\n            rho = x_j.T.dot(y - X_poly.dot(w) + w[j] * x_j)\n            w[j] = soft_threshold(rho, reg_factor) / x_j.T.dot(x_j)\n        \n        # Check for early stopping\n        if np.all(np.abs(w - w_old) < tol):\n            break\n        \n        # Update old weights\n        w_old = w.copy()\n    \n    # Round the weights to 4 decimal places\n    w = np.round(w, 4).tolist()\n    \n    return w\n\ndef soft_threshold(x, lambda_):\n    \"\"\"Soft thresholding function.\"\"\"\n    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n\n# Example usage:\nX = [1, 2, 3, 4, 5]\ny = [1, 2, 3, 4, 5]\ndegree = 2\nreg_factor = 0.1\nn_iterations = 1000\ntol = 1e-6\n\ncoefficients = lasso_regression(X, y, degree, reg_factor, n_iterations, tol)\nprint(coefficients)\n```\n\n### Explanation:\n1. **Input Conversion**: The input lists `X` and `y` are converted to numpy arrays for easier manipulation.\n2. **Design Matrix**: The design matrix `X_poly` is created using `np.vander` to include polynomial features up to the specified degree.\n3. **Initialization**: The weights `w` are initialized to zero.\n4. **Coordinate Descent**:\n   - The bias term `w[0]` is updated in every iteration.\n   - For each coefficient `w[j]` (where `j >= 1`), the update is performed using the soft thresholding function.\n   - The algorithm checks for early stopping if the largest absolute change in any coefficient is below the specified tolerance `tol`.\n5. **Soft Thresholding Function**: The `soft_threshold` function is defined to handle the L1 regularization.\n6. **Output**: The final weights are rounded to 4 decimal places and returned as a list.\n\nThis implementation ensures that the Lasso regression model is trained using coordinate descent and supports polynomial feature expansion."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"\n    Calculate the backward probability vector \u03b2_t for a given time index t in a Hidden Markov Model (HMM).\n\n    Parameters:\n    A (list[list[float]]): Transition-probability matrix of shape (N, N).\n    B (list[list[float]]): Emission-probability matrix of shape (N, M).\n    obs (list[int]): List of observation indices (length T).\n    t (int): Integer time index (0 \u2264 t < T).\n\n    Returns:\n    list[float]: A list containing the N backward probabilities rounded to 4 decimal places.\n    \"\"\"\n    N = len(A)\n    T = len(obs)\n    beta = [0.0] * N\n    \n    # Base case: \u03b2_{T-1}(i) = 1 for all i\n    if t == T - 1:\n        return [round(1.0, 4) for _ in range(N)]\n    \n    # Recursive case: \u03b2_t(i) = \u03a3_j a_{ij} * b_j(o_{t+1}) * \u03b2_{t+1}(j)\n    for i in range(N):\n        beta[i] = sum(A[i][j] * B[j][obs[t + 1]] * backward_beta(A, B, obs, t + 1)[j] for j in range(N))\n    \n    return [round(prob, 4) for prob in beta]\n```\n\nThis function calculates the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\) in a Hidden Markov Model (HMM). It uses the provided transition probability matrix \\(A\\), emission probability matrix \\(B\\), and a list of observation indices \\(obs\\). The function handles the base case where \\(t = T - 1\\) by setting \\(\\beta_{T-1}(i) = 1\\) for all states \\(i\\). For other time indices, it recursively calculates \\(\\beta_t(i)\\) using the formula provided in the task description. The results are rounded to 4 decimal places and returned as a list."}
{"task_id": 65, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or not pi or not obs:\n        return 0.0\n\n    N = len(pi)  # Number of states\n    T = len(obs)  # Length of the observation sequence\n\n    # Initialize the backward variables\n    beta = np.zeros((N, T))\n    beta[:, T-1] = 1.0\n\n    # Fill in the backward variables using the recursive formula\n    for t in range(T-2, -1, -1):\n        for i in range(N):\n            beta[i, t] = sum(A[i][j] * B[j][obs[t+1]] * beta[j, t+1] for j in range(N))\n\n    # Calculate the total probability of the observation sequence\n    total_prob = sum(pi[i] * B[i][obs[0]] * beta[i, 0] for i in range(N))\n\n    return round(total_prob, 6)\n```\n\nThis function implements the backward algorithm for a Hidden Markov Model. It first checks if any of the input lists are empty and returns 0.0 if so. It then initializes the backward variables and fills them using the recursive formula provided. Finally, it calculates the total probability of the observation sequence and returns it rounded to six decimal places."}
{"task_id": 69, "completion_id": 0, "completion": "To implement the forward algorithm for Hidden Markov Models (HMMs), we need to follow these steps:\n\n1. Validate the input dimensions.\n2. Check that every observation index is in the valid range [0, m\u22121].\n3. Return -1 when the input is invalid.\n4. Otherwise, implement the forward algorithm and return the result rounded to 4 decimal places.\n\nHere's the implementation of the `forward_algorithm` function:\n\n```python\nimport numpy as np\n\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    # Validate input dimensions\n    n = len(S)\n    if not (all(len(row) == n for row in A) and all(len(row) == n for row in B)):\n        return -1\n    \n    m = len(B[0])\n    if not all(0 <= obs < m for obs in observations):\n        return -1\n    \n    if not observations:\n        return -1\n    \n    # Initialize the forward variables\n    T = len(observations)\n    alpha = np.zeros((n, T))\n    \n    # Initialize the first time step\n    for i in range(n):\n        alpha[i, 0] = S[i] * B[i][observations[0]]\n    \n    # Forward pass\n    for t in range(1, T):\n        for i in range(n):\n            alpha[i, t] = sum(alpha[j, t-1] * A[j][i] for j in range(n)) * B[i][observations[t]]\n    \n    # Compute the total probability of the observation sequence\n    total_probability = sum(alpha[i, T-1] for i in range(n))\n    \n    # Return the result rounded to 4 decimal places\n    return round(total_probability, 4)\n\n# Example usage:\nS = [0.6, 0.4]\nA = [[0.7, 0.3], [0.4, 0.6]]\nB = [[0.1, 0.4, 0.5], [0.7, 0.2, 0.1]]\nobservations = [0, 2, 1]\nprint(forward_algorithm(S, A, B, observations))  # Output: 0.0158\n```\n\n### Explanation:\n1. **Input Validation**:\n   - Check if the dimensions of `A` and `B` are `n x n`.\n   - Ensure all observation indices are within the valid range `[0, m-1]`.\n   - Return `-1` if any of these conditions are not met.\n\n2. **Initialization**:\n   - Initialize the `alpha` array to store the forward probabilities.\n   - Set the initial probabilities for the first time step based on the initial state probabilities `S` and the emission probabilities `B`.\n\n3. **Forward Pass**:\n   - Iterate through each time step and each state to compute the forward probabilities using the transition and emission probabilities.\n\n4. **Total Probability**:\n   - Sum the forward probabilities for the last time step to get the total probability of the observation sequence.\n\n5. **Return Result**:\n   - Round the total probability to 4 decimal places and return it.\n\nThis implementation ensures that the forward algorithm is correctly applied to compute the likelihood of an observation sequence given an HMM."}
{"task_id": 70, "completion_id": 0, "completion": "To implement the FP-Growth algorithm, we need to follow these steps:\n\n1. **Count the support of every single item and discard infrequent ones.**\n2. **Build one FP-tree (a prefix tree in which every node stores *item name* and *support count*).**\n3. **Recursively mine conditional FP-trees to obtain larger item-sets.**\n\nHere is the complete implementation of the FP-Growth algorithm in Python:\n\n```python\nfrom collections import Counter, defaultdict\nfrom typing import List\n\ndef fp_growth(transactions: List[List[str]], min_support: int) -> List[List[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    if not transactions:\n        return []\n\n    # Step 1: Count the support of every single item and discard infrequent ones\n    item_counts = Counter(item for transaction in transactions for item in transaction)\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n\n    if not frequent_items:\n        return []\n\n    # Step 2: Build the FP-tree\n    def build_fp_tree(transactions, frequent_items):\n        header_table = defaultdict(list)\n        fp_tree = defaultdict(list)\n        root = fp_tree['']\n\n        for transaction in transactions:\n            transaction = [item for item in transaction if item in frequent_items]\n            transaction.sort(key=frequent_items.get, reverse=True)\n            for item in transaction:\n                header_table[item].append(root)\n                if item not in fp_tree[root]:\n                    fp_tree[root].append(item)\n                else:\n                    for node in fp_tree[root]:\n                        if node == item:\n                            node[1] += 1\n                            break\n\n        return fp_tree, header_table\n\n    fp_tree, header_table = build_fp_tree(transactions, frequent_items)\n\n    # Step 3: Recursively mine conditional FP-trees\n    def mine_fp_tree(fp_tree, header_table, prefix, min_support):\n        frequent_itemsets = []\n        for item in sorted(header_table.keys(), key=frequent_items.get, reverse=True):\n            prefix_path = []\n            node = header_table[item][0]\n            while node != root:\n                prefix_path.append(node)\n                node = node[0]\n            prefix_path.reverse()\n            prefix_path = [item] + prefix_path\n\n            frequent_itemsets.append(prefix_path)\n            conditional_fp_tree, conditional_header_table = build_fp_tree([transaction for transaction in transactions if item in transaction], frequent_items)\n            frequent_itemsets.extend(mine_fp_tree(conditional_fp_tree, conditional_header_table, prefix_path, min_support))\n\n        return frequent_itemsets\n\n    root = fp_tree['']\n    frequent_itemsets = mine_fp_tree(fp_tree, header_table, [], min_support)\n\n    # Sort the result\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n\n    return frequent_itemsets\n\n# Example usage:\ntransactions = [\n    ['milk', 'bread', 'butter'],\n    ['bread', 'butter', 'jam'],\n    ['milk', 'bread', 'jam'],\n    ['milk', 'butter', 'jam'],\n    ['bread', 'butter', 'jam']\n]\nmin_support = 2\nprint(fp_growth(transactions, min_support))\n```\n\n### Explanation:\n1. **Counting Item Support**: We first count the support of each item using `Counter` and filter out items that do not meet the minimum support threshold.\n2. **Building the FP-Tree**: We build the FP-tree by iterating through each transaction, sorting items by their support counts, and adding them to the tree.\n3. **Mining Conditional FP-Trees**: We recursively mine the conditional FP-trees to find larger item-sets. For each frequent item, we build a conditional FP-tree and mine it to find all frequent item-sets that include that item.\n4. **Sorting the Result**: Finally, we sort the frequent item-sets by their length and lexicographical order.\n\nThis implementation ensures that the result is deterministic and meets the specified constraints."}
{"task_id": 75, "completion_id": 0, "completion": "Here is a Python implementation of the k-Nearest Neighbour (k-NN) classifier:\n\n```python\nimport numpy as np\n\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    # Check if k is valid\n    if not isinstance(k, int) or k <= 0 or k > len(X_train):\n        return -1\n    \n    # Convert lists to numpy arrays for efficient computation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Initialize the list to store predictions\n    predictions = []\n    \n    # Iterate over each test sample\n    for test_sample in X_test:\n        # Compute the Euclidean distances to all training samples\n        distances = np.linalg.norm(X_train - test_sample, axis=1)\n        \n        # Get the indices of the k smallest distances\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get the corresponding class labels\n        nearest_labels = y_train[nearest_indices]\n        \n        # Perform majority voting\n        unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n        predicted_label = unique_labels[np.argmax(counts)]\n        \n        # If there is a tie, return the smallest class label\n        if np.max(counts) > 1:\n            predicted_label = np.min(unique_labels[unique_labels == predicted_label])\n        \n        predictions.append(predicted_label)\n    \n    return predictions\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `k` is a positive integer and within the valid range (1 to the number of training samples). If not, it returns `-1`.\n2. **Data Conversion**: The input lists are converted to NumPy arrays for efficient computation.\n3. **Prediction Loop**: For each test sample, the function computes the Euclidean distances to all training samples using `np.linalg.norm`.\n4. **Nearest Neighbors**: The indices of the `k` smallest distances are found using `np.argsort`.\n5. **Majority Voting**: The class labels of the nearest neighbors are counted using `np.unique` and `return_counts=True`. The class with the highest count is selected. If there is a tie, the smallest class label is chosen.\n6. **Return Predictions**: The predicted labels for all test samples are collected in a list and returned.\n\nThis implementation ensures that the k-NN classifier works as specified, handling edge cases and performing efficiently with NumPy for large datasets."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for efficient computation\n    y = np.array(y)\n    y_pred = np.array(y_pred)\n    \n    # Ensure numerical stability by adding a small epsilon to y_pred\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Calculate the cross-entropy loss\n    loss = -np.sum(y * np.log(y_pred))\n    \n    # Round the loss to 4 decimal places and return\n    return round(loss, 4)\n```\n\nThis function first converts the input lists to NumPy arrays for efficient computation. It then ensures numerical stability by clipping the predicted probabilities between a small epsilon and one minus epsilon. The cross-entropy loss is calculated by summing the negative log of the predicted probabilities for the true classes. Finally, the loss is rounded to four decimal places and returned."}
{"task_id": 77, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `L_model_forward` function that performs forward propagation for an L-layer neural network with ReLU activation in all hidden layers and Sigmoid activation in the output layer.\n\n```python\nimport numpy as np\n\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2  # Number of layers in the network\n\n    for l in range(1, L):\n        # Linear step\n        W = parameters[f'W{l}']\n        b = parameters[f'b{l}']\n        Z = np.dot(W, A) + b\n        \n        # Non-linear step (ReLU)\n        A = np.maximum(0, Z)\n        \n        # Store cache\n        cache = (A, W, b, Z)\n        caches.append(cache)\n\n    # Output layer\n    W = parameters[f'W{L}']\n    b = parameters[f'b{L}']\n    Z = np.dot(W, A) + b\n    \n    # Non-linear step (Sigmoid)\n    AL = 1 / (1 + np.exp(-Z))\n    \n    # Store cache\n    cache = (AL, W, b, Z)\n    caches.append(cache)\n\n    # Round AL to 4 decimal places and convert to a plain Python list\n    AL = AL.round(4).tolist()\n\n    return AL, caches\n\n# Example usage:\n# parameters = {\n#     'W1': np.array([[1, 2], [3, 4]]),\n#     'b1': np.array([[0], [0]]),\n#     'W2': np.array([[1, 2]]),\n#     'b2': np.array([[0]])\n# }\n# X = np.array([[1, 2]])\n# AL, caches = L_model_forward(X, parameters)\n# print(AL)\n```\n\n### Explanation:\n1. **Initialization**: The function initializes an empty list `caches` to store the cache for each layer. The input `X` is assigned to `A` as the initial activation.\n2. **Loop through layers**: The function iterates through each layer from 1 to L-1:\n   - **Linear step**: Computes the linear combination `Z` using the current layer's weights `W` and biases `b`.\n   - **Non-linear step (ReLU)**: Applies the ReLU activation function to `Z`.\n   - **Store cache**: Stores the current activation `A`, weights `W`, biases `b`, and linear combination `Z` in a tuple and appends it to the `caches` list.\n3. **Output layer**: For the output layer (layer L):\n   - **Linear step**: Computes the linear combination `Z` using the current layer's weights `W` and biases `b`.\n   - **Non-linear step (Sigmoid)**: Applies the Sigmoid activation function to `Z`.\n   - **Store cache**: Stores the current activation `AL`, weights `W`, biases `b`, and linear combination `Z` in a tuple and appends it to the `caches` list.\n4. **Return**: Rounds the final activation `AL` to 4 decimal places and converts it to a plain Python list before returning it along with the `caches`.\n\nThis implementation ensures that the forward propagation is correctly performed for an L-layer neural network with the specified activation functions and parameter structure."}
{"task_id": 81, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `compute_cost` function that computes the binary cross-entropy cost between the predicted probabilities `A2` and the ground-truth labels `Y`. The function clips the values in `A2` to avoid taking the logarithm of zero.\n\n```python\nimport numpy as np\n\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    # Ensure A2 is a 1D array for simplicity\n    A2 = A2.flatten()\n    Y = Y.flatten()\n    \n    # Clip A2 to avoid log(0)\n    epsilon = 1e-15\n    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)\n    \n    # Compute the cross-entropy cost\n    m = A2.shape[0]\n    cost = -1 / m * np.sum(Y * np.log(A2_clipped) + (1 - Y) * np.log(1 - A2_clipped))\n    \n    # Round the cost to 6 decimal places\n    return round(cost, 6)\n\n# Example usage:\nA2 = np.array([[0.9, 0.1, 0.8]])\nY = np.array([[1, 0, 1]])\nprint(compute_cost(A2, Y))  # Output should be the computed cost rounded to 6 decimal places\n```\n\n### Explanation:\n1. **Flattening the Arrays**: The function first flattens the input arrays `A2` and `Y` to ensure they are 1D for simplicity.\n2. **Clipping A2**: The values in `A2` are clipped to the interval `[epsilon, 1 - epsilon]` to avoid taking the logarithm of zero.\n3. **Computing the Cost**: The cross-entropy cost is computed using the formula provided. The sum of the terms `y\u1d62\u00b7ln(a\u1d62) + (1\u2013y\u1d62)\u00b7ln(1\u2013a\u1d62)` is divided by the number of samples `m`, and the result is negated.\n4. **Rounding**: The final cost is rounded to 6 decimal places before being returned.\n\nThis function should work correctly for any input arrays `A2` and `Y` of the same shape."}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    Q = np.array(Q)\n    n = len(Q)\n    max_q = np.max(Q)\n    greedy_indices = np.where(Q == max_q)[0]\n    greedy_index = greedy_indices[0]  # Pick the first occurrence in case of ties\n    base = epsilon / n\n    probabilities = np.full(n, base)\n    probabilities[greedy_index] = 1 - epsilon + base\n    return np.round(probabilities, 4)\n\n# Example usage:\nQ = [1.2, 0.3, 1.2, -0.1]\nepsilon = 0.1\nprint(epsilon_soft(Q, epsilon))  # Output: [0.925, 0.025, 0.025, 0.025]\n```\n\nThis function first converts the input Q-values to a NumPy array for easier manipulation. It then calculates the number of actions, the maximum Q-value, and the indices of the greedy actions. The base probability for each action is calculated as `epsilon / n`. A NumPy array of base probabilities is created, and the probability of the greedy action is adjusted to `1 - epsilon + base`. Finally, the probabilities are rounded to 4 decimal places and returned."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this task, we need to implement a function that aggregates predictions from multiple decision trees using majority voting. The function will handle both numeric and string labels and break ties by choosing the smallest label. Here's how you can implement this:\n\n```python\nfrom collections import Counter\nfrom typing import List, Union\n\ndef aggregate_random_forest_votes(predictions: List[List[Union[int, float, str]]]) -> List[Union[int, float, str]]:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Initialize a list to store the final predictions\n    final_predictions = []\n    \n    # Iterate over each sample (column in the 2D list)\n    for sample_predictions in zip(*predictions):\n        # Count the occurrences of each prediction\n        vote_counts = Counter(sample_predictions)\n        \n        # Find the prediction with the highest count\n        max_count = max(vote_counts.values())\n        top_predictions = [label for label, count in vote_counts.items() if count == max_count]\n        \n        # Break ties by choosing the smallest label\n        final_prediction = min(top_predictions)\n        \n        # Append the final prediction to the list\n        final_predictions.append(final_prediction)\n    \n    return final_predictions\n\n# Example usage:\npredictions = [\n    [1, 2, 2, 1],\n    [2, 1, 1, 2],\n    [1, 2, 1, 2]\n]\n\nprint(aggregate_random_forest_votes(predictions))  # Output: [1, 2, 1, 2]\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing an empty list `final_predictions` to store the aggregated predictions for each sample.\n2. **Iterate Over Samples**: We use `zip(*predictions)` to iterate over each sample across all trees. This effectively transposes the 2D list, allowing us to process each sample column by column.\n3. **Count Votes**: For each sample, we use `Counter` to count the occurrences of each prediction.\n4. **Determine Majority**: We find the maximum count and collect all predictions that have this count.\n5. **Break Ties**: We break ties by selecting the smallest label using the `min` function.\n6. **Store Result**: We append the final prediction for the current sample to `final_predictions`.\n\nThis function handles both numeric and string labels and ensures that ties are broken correctly, providing the desired majority vote aggregation."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    # Convert input to NumPy array if it's not already\n    z_array = np.array(z)\n    \n    # Compute softplus using np.logaddexp\n    softplus_values = np.logaddexp(0.0, z_array)\n    \n    # Round to 4 decimal places\n    softplus_values_rounded = np.round(softplus_values, 4)\n    \n    # Convert to Python list if the input was a list or array\n    if isinstance(z, (list, np.ndarray)):\n        return softplus_values_rounded.tolist()\n    else:\n        # Return a single float if the input was a scalar\n        return float(softplus_values_rounded)\n\n# Example usage:\nz = np.array([-1000, 0, 3])\nprint(softplus(z))  # Output: [0.0, 0.6931, 3.0486]\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `z` is a list or NumPy array and converts it to a NumPy array if necessary.\n2. **Softplus Calculation**: It uses `np.logaddexp(0.0, z_array)` to compute the softplus values in a numerically stable manner.\n3. **Rounding**: The results are rounded to 4 decimal places using `np.round`.\n4. **Output Conversion**: If the input was a list or array, the result is converted back to a Python list using `tolist()`. If the input was a scalar, a single `float` is returned.\n\nThis implementation ensures that the function handles both scalar and array inputs correctly and returns the results in the desired format."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Check if the policy has any estimates\n    if not policy.get('ev_estimates') or not policy['ev_estimates']:\n        return np.nan\n    \n    # Extract the true expected values and the policy's estimates\n    arm_evs = bandit.get('arm_evs')\n    ev_estimates = policy['ev_estimates']\n    \n    # Ensure the estimates are in the same order as the true expected values\n    sorted_indices = sorted(ev_estimates.keys())\n    true_values = [arm_evs[i] for i in sorted_indices]\n    estimated_values = [ev_estimates[i] for i in sorted_indices]\n    \n    # Calculate the squared errors\n    squared_errors = [(true - est) ** 2 for true, est in zip(true_values, estimated_values)]\n    \n    # Compute the mean-squared error\n    mse_value = np.mean(squared_errors)\n    \n    # Round the result to 4 decimal places\n    return round(mse_value, 4)\n```\n\nThis function first checks if the policy has any estimates. If not, it returns `numpy.nan`. It then extracts the true expected values and the policy's estimates, ensuring they are in the same order. It calculates the squared errors, computes the mean-squared error, and returns the result rounded to 4 decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter, defaultdict\n\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # Split the corpus and sequence into words\n    words = [sentence.split() for sentence in corpus]\n    sequence_words = sequence.split()\n    \n    # Build N-gram frequency tables\n    ngram_counts = defaultdict(Counter)\n    for sentence in words:\n        for i in range(len(sentence) - N + 1):\n            ngram = tuple(sentence[i:i+N])\n            ngram_counts[ngram[:N-1]][ngram[-1]] += 1\n    \n    # Add \"<UNK>\" token to the vocabulary\n    vocabulary = set(word for sentence in words for word in sentence)\n    vocabulary.add(\"<UNK>\")\n    \n    # Estimate the probability of every contiguous N-gram in the target sentence\n    log_prob = 0.0\n    for i in range(len(sequence_words) - N + 1):\n        context = tuple(sequence_words[i:i+N-1])\n        word = sequence_words[i+N-1]\n        if context in ngram_counts:\n            count_context = sum(ngram_counts[context].values())\n            count_context_word = ngram_counts[context][word]\n            log_prob += math.log((count_context_word + K) / (count_context + K * len(vocabulary)), math.e)\n        else:\n            log_prob += math.log(K / (K * len(vocabulary)), math.e)\n    \n    # Return the natural logarithm of the sentence probability rounded to 4 decimals\n    return round(log_prob, 4)\n```\nThis function first splits the corpus and sequence into words, then builds N-gram frequency tables. It adds an explicit \"<UNK>\" token to the vocabulary to handle unseen words. For each contiguous N-gram in the target sentence, it estimates the probability using additive smoothing and adds the log-probability to the total. Finally, it returns the natural logarithm of the sentence probability rounded to 4 decimal places."}
{"task_id": 108, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    # Initialize factor matrices with a fixed random seed for deterministic grading\n    np.random.seed(0)\n    W = np.random.rand(X.shape[0], K)\n    H = np.random.rand(K, X.shape[1])\n\n    best_loss = np.inf\n    best_W, best_H = W, H\n\n    for iteration in range(max_iter):\n        # Update W while keeping H fixed\n        W = np.linalg.solve((H @ H.T) + alpha * np.eye(K), H @ X.T).T\n\n        # Update H while keeping W fixed\n        H = np.linalg.solve((W.T @ W) + alpha * np.eye(K), W.T @ X)\n\n        # Calculate the current loss\n        current_loss = np.linalg.norm(X - W @ H, 'fro')**2 + alpha * (np.linalg.norm(W, 'fro')**2 + np.linalg.norm(H, 'fro')**2)\n\n        # Check if the loss has converged\n        if current_loss < best_loss - tol:\n            best_loss = current_loss\n            best_W, best_H = W, H\n        else:\n            break\n\n    # Reconstruct the matrix using the best factor matrices found\n    X_hat = best_W @ best_H\n    X_hat_rounded = X_hat.round(4)\n\n    # Convert the result to a list of lists\n    return X_hat_rounded.tolist()\n```\n\nThis implementation of the regularized Alternating Least Squares (ALS) algorithm factorizes a given matrix `X` into two low-rank factor matrices `W` and `H`. The algorithm iteratively updates `W` and `H` while keeping the other fixed, aiming to minimize the regularized Frobenius reconstruction loss. The process stops when the loss drops below a specified tolerance `tol` or when the maximum number of iterations `max_iter` is reached. The reconstructed matrix is then rounded to 4 decimal places and returned as a list of lists."}
{"task_id": 109, "completion_id": 0, "completion": "Here is a Python implementation of the K-Means++ clustering algorithm:\n\n```python\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    if random_state is not None:\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n    m, n = X.shape\n\n    # Step 1: Initialize centroids using K-Means++\n    centroids = [X[np.random.choice(m)]]\n    for _ in range(1, K):\n        distances = np.array([min([np.linalg.norm(x - c) for c in centroids]) for x in X])\n        probabilities = distances ** 2 / distances.sum()\n        cumulative_probabilities = np.cumsum(probabilities)\n        r = random.random()\n        for j, p in enumerate(cumulative_probabilities):\n            if r < p:\n                centroids.append(X[j])\n                break\n\n    # Step 2: Lloyd's iterations\n    for _ in range(max_iter):\n        # Assign samples to the nearest centroid\n        labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1)\n\n        # Update centroids\n        new_centroids = np.array([X[labels == k].mean(axis=0) if len(X[labels == k]) > 0 else centroids[k] for k in range(K)])\n\n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n\n        centroids = new_centroids\n\n    # Step 3: Round centroids to 4 decimal places and sort\n    centroids = [list(np.round(c, 4)) for c in centroids]\n    centroids.sort(key=lambda x: (x[0], x))\n\n    return centroids\n```\n\n### Explanation:\n1. **Initialization with K-Means++**:\n   - The first centroid is chosen randomly from the data points.\n   - Subsequent centroids are chosen based on the squared distance from the existing centroids, ensuring that the new centroid is far from the existing ones.\n\n2. **Lloyd's Iterations**:\n   - Each sample is assigned to the nearest centroid.\n   - The centroids are updated to be the mean of the samples assigned to them.\n   - The process repeats until the centroids do not change or the maximum number of iterations is reached.\n\n3. **Output**:\n   - The centroids are rounded to four decimal places and sorted by their first coordinate.\n\nThis implementation ensures reproducibility with the `random_state` parameter and adheres to the constraints provided."}
{"task_id": 111, "completion_id": 0, "completion": "Certainly! Below is the implementation of the three helper functions `string_to_int`, `int_to_string`, and `run_example` as per the given requirements.\n\n```python\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"Encodes every character of the input string using a provided vocabulary.\n    The encoded list has a fixed length of TIME_STEPS. If the input is shorter,\n    append the padding token 0 on the right; if it is longer, truncate the sequence.\n    Any character that does not exist in the vocabulary is also encoded as the padding token 0.\"\"\"\n    \n    encoded = [vocabulary.get(char, 0) for char in text]\n    if len(encoded) < time_steps:\n        encoded += [0] * (time_steps - len(encoded))\n    else:\n        encoded = encoded[:time_steps]\n    \n    return encoded\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"Converts a list/array of integer ids back to text using an inverse vocabulary.\n    Padding tokens (0) must be ignored during decoding.\"\"\"\n    \n    decoded = [inverse_vocab[idx] for idx in indices if idx != 0]\n    return ''.join(decoded)\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"Encodes the raw text with string_to_int, calls model.predict on the encoded batch,\n    applies argmax over the last axis to obtain one predicted id per time step, and finally\n    decodes the ids with int_to_string.\"\"\"\n    \n    encoded_text = string_to_int(text, TIME_STEPS, input_vocabulary)\n    encoded_text = np.array(encoded_text).reshape(1, TIME_STEPS)\n    \n    predictions = model.predict(encoded_text)\n    predicted_ids = np.argmax(predictions, axis=-1).flatten()\n    \n    decoded_text = int_to_string(predicted_ids, inv_output_vocabulary)\n    return decoded_text\n```\n\n### Explanation:\n\n1. **`string_to_int` Function:**\n   - This function takes a raw text string, a fixed length `TIME_STEPS`, and a vocabulary dictionary.\n   - It encodes each character in the text using the provided vocabulary. If a character is not in the vocabulary, it is encoded as `0`.\n   - If the encoded list is shorter than `TIME_STEPS`, it appends `0`s to the right. If it is longer, it truncates the sequence to `TIME_STEPS`.\n\n2. **`int_to_string` Function:**\n   - This function takes a list of integer IDs and an inverse vocabulary dictionary.\n   - It decodes the IDs back to characters using the inverse vocabulary, ignoring any padding tokens (`0`).\n\n3. **`run_example` Function:**\n   - This function encodes the input text using `string_to_int`.\n   - It reshapes the encoded text to a batch of size 1 and passes it to the model's `predict` method.\n   - It applies `argmax` over the last axis to get the predicted IDs for each time step.\n   - It decodes the predicted IDs using `int_to_string` and returns the resulting string.\n\nThis implementation ensures that the sequence is encoded and decoded correctly, and the padding tokens are handled appropriately."}
{"task_id": 113, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `run_examples` function as described:\n\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples=None):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    if examples is None:\n        try:\n            examples = EXAMPLES\n        except NameError:\n            raise ValueError(\"No examples provided and EXAMPLES is not defined in the environment.\")\n    \n    predictions = []\n    \n    for example in examples:\n        # Call the helper function to get the predicted characters\n        predicted_chars = run_example(model, input_vocabulary, inv_output_vocabulary, example)\n        \n        # Concatenate the predicted characters into a single string\n        predicted_string = ''.join(predicted_chars)\n        \n        # Print the input and output\n        print(f\"input:  {example}\")\n        print(f\"output: {predicted_string}\")\n        \n        # Collect the predicted string in the list\n        predictions.append(predicted_string)\n    \n    return predictions\n```\n\n### Explanation:\n1. **Function Signature**: The function `run_examples` takes four parameters: `model`, `input_vocabulary`, `inv_output_vocabulary`, and `examples`. The `examples` parameter is optional and defaults to `None`.\n\n2. **Handling `examples`**:\n   - If `examples` is `None`, the function attempts to use a global constant `EXAMPLES`. If `EXAMPLES` is not defined, it raises a `ValueError`.\n\n3. **Processing Each Example**:\n   - For each example in the `examples` iterable, the function calls the helper function `run_example` to get the predicted characters.\n   - These predicted characters are then concatenated into a single string.\n   - The input and output strings are printed in the specified format.\n   - The predicted string is appended to the `predictions` list.\n\n4. **Return Value**: The function returns the list of predicted strings in the same order as the input examples.\n\n### Usage:\nTo use this function, you need to define the `run_example` function and ensure that the `EXAMPLES` constant is defined in your environment if you want to use the default examples.\n\n```python\n# Example usage\ndef run_example(model, input_vocabulary, inv_output_vocabulary, example):\n    # Dummy implementation of run_example\n    return list(example)  # This is just a placeholder\n\nEXAMPLES = [\"hello\", \"world\"]\n\n# Define a simple model function\ndef simple_model(input_string):\n    return input_string.upper()\n\n# Run the examples\npredictions = run_examples(simple_model, {}, {}, EXAMPLES)\nprint(predictions)  # Output: ['HELLO', 'WORLD']\n```\n\nIn this example, the `run_example` function is a dummy implementation that simply returns the input string as a list of characters. The `simple_model` function is a dummy model that converts the input string to uppercase. The `EXAMPLES` constant is defined with two example strings. The `run_examples` function is then called with these parameters, and it prints and returns the predictions."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # Ensure y is a 1D array for element-wise operations\n    y = y.flatten()\n    \n    # Compute the linear combination of features and weights\n    z = np.dot(X, w)\n    \n    # Compute the sigmoid function\n    p = 1 / (1 + np.exp(-z))\n    \n    # Avoid numerical issues by clamping p values\n    p = np.clip(p, 1e-20, 1 - 1e-20)\n    \n    # Compute the binary cross-entropy loss\n    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    # Compute the gradient of the loss with respect to the weights\n    gradient = (1 / X.shape[0]) * np.dot(X.T, (p - y))\n    \n    # Round the loss and gradient to 4 decimal places\n    loss = round(loss, 4)\n    gradient = gradient.round(4).tolist()\n    \n    return loss, gradient\n```\n\nThis function first ensures that the target vector `y` is flattened to a 1D array for element-wise operations. It then computes the linear combination of features and weights, followed by the sigmoid function to get the predicted probabilities. To avoid numerical issues, it clamps the predicted probabilities `p` between a small constant `\u03b5` and `1 - \u03b5`. The binary cross-entropy loss is computed, and the gradient with respect to the weights is calculated. Finally, the loss and gradient are rounded to 4 decimal places and returned as specified."}
{"task_id": 118, "completion_id": 0, "completion": "To implement the AdaBoost algorithm with decision stumps, we need to follow these steps:\n\n1. Initialize the weights of the training examples.\n2. For each iteration, train a decision stump that minimizes the weighted classification error.\n3. Calculate the weight of the stump and update the weights of the training examples.\n4. Aggregate the votes of all stumps to make predictions on the test set.\n\nHere's the complete implementation of the `adaboost_predict` function:\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n\n    m, n = X_train.shape\n    weights = np.ones(m) / m\n    stump_weights = []\n\n    for _ in range(n_clf):\n        # Initialize the best stump parameters\n        best_stump = None\n        best_error = float('inf')\n        best_threshold = 0\n        best_direction = 1\n\n        # Try splitting on each feature and threshold\n        for feature_idx in range(n):\n            for threshold in np.unique(X_train[:, feature_idx]):\n                # Try both directions\n                for direction in [-1, 1]:\n                    # Create the stump\n                    stump = lambda x, feature_idx=feature_idx, threshold=threshold, direction=direction: \\\n                        np.where(direction * x[:, feature_idx] > direction * threshold, 1, -1)\n                    \n                    # Calculate the error\n                    predictions = stump(X_train, feature_idx, threshold, direction)\n                    error = np.sum(weights * (predictions != y_train))\n                    \n                    # Update the best stump if this one is better\n                    if error < best_error:\n                        best_stump = stump\n                        best_error = error\n                        best_threshold = threshold\n                        best_direction = direction\n\n        # Calculate the weight of the best stump\n        alpha = 0.5 * np.log((1 - best_error) / best_error)\n        stump_weights.append(alpha)\n\n        # Update the weights of the training examples\n        predictions = best_stump(X_train, best_threshold, best_direction)\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)\n\n    # Make predictions on the test set\n    predictions = np.zeros(X_test.shape[0])\n    for alpha, stump in zip(stump_weights, stump_weights):\n        predictions += alpha * stump(X_test, best_threshold, best_direction)\n\n    # Aggregate the votes\n    predictions = np.sign(predictions)\n    predictions = predictions.tolist()\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# y_train = np.array([1, -1, -1, 1])\n# X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# n_clf = 2\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: We initialize the weights of the training examples to be equal and set up a list to store the weights of each stump.\n2. **Stump Training**: For each iteration, we try to find the best decision stump by splitting on each feature and threshold, and choosing the direction that minimizes the weighted classification error.\n3. **Stump Weight Calculation**: We calculate the weight of the best stump using the formula $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)$.\n4. **Weight Update**: We update the weights of the training examples based on the predictions of the current stump.\n5. **Prediction**: For each test sample, we aggregate the votes of all stumps and output the sign of the weighted sum as the predicted label.\n\nThis implementation ensures that the AdaBoost algorithm is trained with decision stumps and makes predictions on the test set as required."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    if n_components <= 0:\n        return []\n\n    # Compute the mean of each class\n    unique_labels = np.unique(y)\n    class_means = np.array([X[y == label].mean(axis=0) for label in unique_labels])\n\n    # Compute the global mean\n    global_mean = X.mean(axis=0)\n\n    # Compute the within-class scatter matrix S_W\n    S_W = np.zeros((X.shape[1], X.shape[1]))\n    for label in unique_labels:\n        class_samples = X[y == label]\n        S_W += np.cov(class_samples, rowvar=False)\n\n    # Compute the between-class scatter matrix S_B\n    S_B = np.zeros((X.shape[1], X.shape[1]))\n    for label in unique_labels:\n        class_size = class_means.shape[0]\n        S_B += class_size * np.outer(class_means[label] - global_mean, class_means[label] - global_mean)\n\n    # Compute the matrix A = pinv(S_W)\u00b7S_B\n    A = np.linalg.pinv(S_W).dot(S_B)\n\n    # Perform eigen-decomposition of A\n    eigenvalues, eigenvectors = np.linalg.eigh(A)\n\n    # Sort the eigen-pairs in descending order of the eigen-values\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    eigenvalues = eigenvalues[sorted_indices]\n\n    # (Deterministic sign) For every chosen eigenvector flip its sign if the first non-zero element is negative\n    for i in range(eigenvectors.shape[1]):\n        if np.any(eigenvectors[:, i] != 0):\n            if eigenvectors[0, i] < 0:\n                eigenvectors[:, i] *= -1\n\n    # Project X on the first n_components eigenvectors\n    projected_data = X.dot(eigenvectors[:, :n_components])\n\n    # Round every element to four decimal places\n    projected_data = np.round(projected_data, 4)\n\n    # Return the projected data as a list of lists\n    return projected_data.tolist()\n```\n\nThis code implements the multi-class Linear Discriminant Analysis (LDA) as described in the task. It computes the within-class scatter matrix \\( S_W \\) and the between-class scatter matrix \\( S_B \\), then solves the generalized eigen-value problem \\( S_W^{-1}S_B w = \\lambda w \\) to find the eigenvectors. It ensures the eigenvectors have a deterministic sign and projects the data onto the first `n_components` eigenvectors, rounding the results to four decimal places."}
{"task_id": 140, "completion_id": 0, "completion": "To solve the \"Escape from Fire Maze\" problem, we need to simulate the spread of fire and the movement of the agent simultaneously. Here's a step-by-step approach to implement the solution:\n\n1. **Initialize the Grid and Queues**: We'll use a queue to manage the fire spread and another queue to manage the agent's movements. We'll also use a set to keep track of visited cells to avoid redundant calculations.\n\n2. **Fire Spread Simulation**: We'll simulate the fire spreading by iterating through the grid and updating the fire positions based on the current fire positions.\n\n3. **Agent Movement Simulation**: We'll simulate the agent's movement by iterating through the grid and checking if the agent can move to a new position without entering a burning cell.\n\n4. **Check for Valid Moves**: For each possible move (up, down, left, right), we'll check if the move is within bounds, not a wall, and not on fire.\n\n5. **Return the Result**: If the agent reaches the goal, we return the number of minutes taken. If the agent cannot reach the goal, we return -1.\n\nHere's the complete implementation:\n\n```python\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    n = len(grid)\n    if n == 1 and grid[0][0] == '.':\n        return 0\n    \n    # Directions for moving up, down, left, right\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    \n    # Initialize fire positions and agent position\n    fire_positions = set()\n    agent_position = (0, 0)\n    \n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_positions.add((i, j))\n            elif grid[i][j] == '#':\n                fire_positions.add((i, j))\n    \n    # Initialize the queue for fire spread and agent movement\n    fire_queue = deque(fire_positions)\n    agent_queue = deque([agent_position])\n    visited = set(fire_positions)\n    \n    minutes = 0\n    \n    while agent_queue:\n        fire_count = len(fire_queue)\n        agent_count = len(agent_queue)\n        \n        # Spread the fire\n        for _ in range(fire_count):\n            x, y = fire_queue.popleft()\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < n and 0 <= ny < n and (nx, ny) not in visited:\n                    visited.add((nx, ny))\n                    fire_queue.append((nx, ny))\n        \n        # Move the agent\n        for _ in range(agent_count):\n            x, y = agent_queue.popleft()\n            if (x, y) == (n-1, n-1):\n                return minutes + 1\n            for dx, dy in directions:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < n and 0 <= ny < n and (nx, ny) not in visited:\n                    visited.add((nx, ny))\n                    agent_queue.append((nx, ny))\n        \n        minutes += 1\n    \n    return -1\n\n# Example usage:\ngrid = [\n    \"F..\",\n    \".#.\",\n    \"...\",\n]\nprint(escape_fire_maze(grid))  # Output: 4\n```\n\n### Explanation:\n1. **Initialization**: We initialize the fire positions and the agent's starting position. We also initialize the queues for fire spread and agent movement.\n2. **Fire Spread**: We simulate the fire spread by iterating through the fire positions and updating the fire positions based on the current fire positions.\n3. **Agent Movement**: We simulate the agent's movement by iterating through the agent's positions and checking if the agent can move to a new position without entering a burning cell.\n4. **Check for Valid Moves**: For each possible move, we check if the move is within bounds, not a wall, and not on fire.\n5. **Return the Result**: If the agent reaches the goal, we return the number of minutes taken. If the agent cannot reach the goal, we return -1.\n\nThis approach ensures that we correctly simulate the spread of fire and the movement of the agent, and it efficiently finds the minimum number of minutes required for the agent to reach the goal while staying safe."}
{"task_id": 141, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Nearest Neighbors (k-NN) classifier in Python using NumPy. This function will handle the three specified distance metrics and perform the necessary steps to predict the labels for the test set.\n\n```python\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement your code here.\"\"\"\n    \n    # Initialize an array to store the predicted labels\n    y_pred = np.zeros(len(X_test), dtype=y_train.dtype)\n    \n    # Define the distance functions\n    def euclidean_distance(x1, x2):\n        return np.sqrt(np.sum((x1 - x2) ** 2))\n    \n    def manhattan_distance(x1, x2):\n        return np.sum(np.abs(x1 - x2))\n    \n    def cosine_distance(x1, x2):\n        return 1 - np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n    \n    # Map the metric string to the corresponding distance function\n    distance_func = {\n        'euclidean': euclidean_distance,\n        'manhattan': manhattan_distance,\n        'cosine': cosine_distance\n    }.get(metric, euclidean_distance)\n    \n    # Iterate over each test sample\n    for i, x_test in enumerate(X_test):\n        # Compute distances to all training samples\n        distances = np.array([distance_func(x_test, x_train) for x_train in X_train])\n        \n        # Get the indices of the k nearest neighbors\n        nearest_indices = np.argsort(distances)[:k]\n        \n        # Get the labels of the k nearest neighbors\n        nearest_labels = y_train[nearest_indices]\n        \n        # Predict the label by majority vote\n        unique_labels, counts = np.unique(nearest_labels, return_counts=True)\n        if len(unique_labels) == 1:\n            y_pred[i] = unique_labels[0]\n        else:\n            y_pred[i] = unique_labels[np.argmax(counts)]\n            if np.sum(counts) > 1 and counts[np.argmax(counts)] == counts[np.argmax(counts) - 1]:\n                y_pred[i] = np.min(unique_labels[counts == counts[np.argmax(counts)]])\n    \n    return y_pred\n\n# Example usage:\n# X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y_train = np.array([0, 0, 1, 1])\n# X_test = np.array([[2, 3], [6, 7]])\n# k = 2\n# y_pred = knn_predict(X_train, y_train, X_test, k, metric='euclidean')\n# print(y_pred)\n```\n\n### Explanation:\n1. **Distance Functions**: The function defines three distance functions (`euclidean_distance`, `manhattan_distance`, and `cosine_distance`).\n2. **Metric Mapping**: The `distance_func` dictionary maps the metric string to the corresponding distance function. If an unknown metric is provided, it defaults to Euclidean distance.\n3. **Prediction Loop**: For each test sample, the function computes the distances to all training samples using the selected distance function.\n4. **Nearest Neighbors**: It finds the indices of the k nearest neighbors based on the computed distances.\n5. **Majority Vote**: The function predicts the label by majority vote among the k nearest neighbors. In case of a tie, it returns the smallest label value.\n6. **Return**: The function returns a 1-D NumPy array of the predicted labels.\n\nThis implementation should work efficiently for the given task and handle the specified distance metrics."}
{"task_id": 146, "completion_id": 0, "completion": "Certainly! Below is the implementation of the k-Nearest Neighbors (k-NN) classifier from scratch in Python using NumPy. This implementation includes the three specified distance metrics: Euclidean, Manhattan, and Cosine.\n\n```python\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    # Initialize the list to store predicted labels\n    predictions = []\n\n    # Iterate over each test sample\n    for x_test in X_test:\n        # Initialize a list to store distances and corresponding labels\n        distances = []\n\n        # Compute distances to all training samples\n        for x_train, label in zip(X, y):\n            if metric == 'euclidean':\n                dist = np.sqrt(np.sum((x_test - x_train) ** 2))\n            elif metric == 'manhattan':\n                dist = np.sum(np.abs(x_test - x_train))\n            elif metric == 'cosine':\n                dot_product = np.dot(x_test, x_train)\n                norm_x_test = np.linalg.norm(x_test)\n                norm_x_train = np.linalg.norm(x_train)\n                if norm_x_test == 0 or norm_x_train == 0:\n                    dist = 1  # Avoid division by zero\n                else:\n                    dist = 1 - dot_product / (norm_x_test * norm_x_train)\n            else:\n                raise ValueError(\"Invalid metric. Choose 'euclidean', 'manhattan', or 'cosine'.\")\n\n            distances.append((dist, label))\n\n        # Sort distances and get the k nearest neighbors\n        distances.sort()\n        k_nearest = distances[:k]\n\n        # Perform majority vote\n        label_counts = {}\n        for _, label in k_nearest:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        # Find the label with the highest count\n        predicted_label = max(label_counts, key=label_counts.get)\n\n        # Append the predicted label to the list\n        predictions.append(predicted_label)\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y_train = np.array([0, 0, 1, 1])\n# X_test = np.array([[2.5, 3.5]])\n# k = 3\n# metric = 'euclidean'\n# predictions = knn_predict(X_train, y_train, X_test, k, metric)\n# print(predictions)  # Output: [0]\n```\n\n### Explanation:\n1. **Initialization**: The function initializes an empty list `predictions` to store the predicted labels for each test sample.\n2. **Distance Calculation**: For each test sample, the function iterates over all training samples to compute the distance using the specified metric.\n3. **Sorting and Selecting Neighbors**: The distances and corresponding labels are stored in a list, which is then sorted. The k nearest neighbors are selected.\n4. **Majority Vote**: A dictionary `label_counts` is used to count the occurrences of each label among the k nearest neighbors. The label with the highest count is selected as the predicted label.\n5. **Prediction**: The predicted label for each test sample is appended to the `predictions` list.\n6. **Return**: The function returns the list of predicted labels.\n\nThis implementation ensures that the k-NN classifier is implemented from scratch without using any third-party machine-learning libraries."}
{"task_id": 155, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `actor_forward` function that performs the forward pass of a two-hidden-layer actor network using NumPy. The function takes a one-dimensional state vector and a dictionary of weights, and returns the resulting action vector as a Python list rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Convert state to a NumPy array if it's a list\n    state = np.array(state)\n    \n    # First linear transformation and ReLU activation\n    z1 = np.dot(state, weights['W1']) + weights['b1']\n    a1 = np.maximum(0, z1)\n    \n    # Second linear transformation and ReLU activation\n    z2 = np.dot(a1, weights['W2']) + weights['b2']\n    a2 = np.maximum(0, z2)\n    \n    # Third linear transformation and tanh activation\n    z3 = np.dot(a2, weights['W3']) + weights['b3']\n    action = np.tanh(z3)\n    \n    # Round the action vector to 4 decimal places\n    action_rounded = np.round(action, 4).tolist()\n    \n    return action_rounded\n\n# Example usage:\n# weights = {\n#     'W1': np.array([[0.1, 0.2], [0.3, 0.4]]),\n#     'b1': np.array([0.5, 0.6]),\n#     'W2': np.array([[0.7, 0.8], [0.9, 1.0]]),\n#     'b2': np.array([1.1, 1.2]),\n#     'W3': np.array([[1.3, 1.4], [1.5, 1.6]]),\n#     'b3': np.array([1.7, 1.8])\n# }\n# state = [0.1, 0.2]\n# action = actor_forward(state, weights)\n# print(action)\n```\n\n### Explanation:\n1. **State Conversion**: The input state is converted to a NumPy array if it is provided as a list.\n2. **First Linear Transformation and ReLU Activation**: The input state is transformed using the first weight matrix `W1` and bias vector `b1`, followed by the ReLU activation function.\n3. **Second Linear Transformation and ReLU Activation**: The output of the first layer is transformed using the second weight matrix `W2` and bias vector `b2`, followed by the ReLU activation function.\n4. **Third Linear Transformation and tanh Activation**: The output of the second layer is transformed using the third weight matrix `W3` and bias vector `b3`, followed by the tanh activation function.\n5. **Rounding**: The resulting action vector is rounded to four decimal places and returned as a Python list.\n\nThis function assumes that the input dimensions and weight shapes are compatible. If they are not, NumPy will raise an error during the matrix operations."}
{"task_id": 160, "completion_id": 0, "completion": "Here is the implementation of the Lasso regression via coordinate descent in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    m, n = X.shape\n    \n    # Add intercept column if required\n    if fit_intercept:\n        X = np.hstack((np.ones((m, 1)), X))\n        n += 1\n    \n    # Initialize weights and intercept\n    w = np.zeros(n)\n    if fit_intercept:\n        bias = np.mean(y)\n        y -= bias\n    else:\n        bias = 0\n    \n    # Coordinate descent\n    for _ in range(max_iters):\n        for j in range(n):\n            if j == 0 and fit_intercept:\n                continue  # Skip intercept update\n            \n            # Temporarily set the current weight to 0\n            w_temp = w.copy()\n            w_temp[j] = 0\n            \n            # Compute the partial residual\n            r_j = y - X @ w_temp\n            \n            # Compute the dot product of x_j and r_j\n            x_j = X[:, j]\n            dot_product = np.dot(x_j, r_j)\n            \n            # Update weight using soft-thresholding\n            w[j] = soft_threshold(dot_product, lambda_param * m) / np.sum(x_j**2)\n    \n    # If intercept was fitted, add it back\n    if fit_intercept:\n        bias += np.mean(y - X @ w)\n    \n    # Round the results to 4 decimal places\n    w = np.round(w, 4)\n    bias = np.round(bias, 4)\n    \n    return w.tolist(), bias\n\ndef soft_threshold(a, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(a) * np.maximum(np.abs(a) - tau, 0)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# lambda_param = 0.1\n# max_iters = 100\n# fit_intercept = True\n# weights, bias = fit_lasso(X, y, lambda_param, max_iters, fit_intercept)\n# print(weights, bias)\n```\n\n### Explanation:\n1. **Initialization**:\n   - If `fit_intercept` is `True`, an all-ones column is added to `X` to account for the intercept.\n   - The weights are initialized to zero, and if an intercept is fitted, the initial bias is set to the mean of `y`.\n\n2. **Coordinate Descent**:\n   - For each iteration, the algorithm iterates over each weight (excluding the intercept).\n   - The current weight is temporarily set to zero, and the partial residual `r_j` is computed.\n   - The weight is updated using the soft-thresholding operator.\n\n3. **Soft-Thresholding**:\n   - The soft-thresholding operator `S(a, \u03c4)` is defined to shrink the weight towards zero by a factor of `\u03c4`.\n\n4. **Final Adjustment**:\n   - If an intercept was fitted, it is adjusted to account for the mean residual.\n\n5. **Output**:\n   - The final weights and bias are rounded to 4 decimal places and returned as a tuple.\n\nThis implementation ensures that the Lasso regression model is learned using the coordinate descent method, adhering to the constraints and requirements provided."}
{"task_id": 165, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    # Convert inputs to NumPy arrays if they are not already\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Check if the lengths of the arrays are equal\n    if x.shape != y.shape:\n        return -1\n    \n    # Calculate the Hamming distance\n    distance = np.sum(x != y) / len(x)\n    \n    # Return the distance rounded to 4 decimal places\n    return round(distance, 4)\n```\n\nThis function first ensures that both inputs are NumPy arrays for efficient computation. It then checks if the arrays have the same length. If they do not, it returns -1 immediately. If they are of the same length, it calculates the Hamming distance by counting the number of positions at which the elements differ and dividing by the total number of elements. Finally, it returns the distance rounded to 4 decimal places."}
{"task_id": 169, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `generate_window` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    if N <= 0:\n        raise ValueError(\"N must be a positive integer.\")\n    \n    if window == \"hamming\":\n        w = [0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1)) for n in range(N)]\n    elif window == \"hann\":\n        w = [0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1)) for n in range(N)]\n    elif window == \"blackman_harris\":\n        a0 = 0.35875\n        a1 = 0.48829\n        a2 = 0.14128\n        a3 = 0.01168\n        w = [a0 - a1 * np.cos(2 * np.pi * n / (N - 1)) + a2 * np.cos(4 * np.pi * n / (N - 1)) - a3 * np.cos(6 * np.pi * n / (N - 1)) for n in range(N)]\n    elif window == \"generalized_cosine\":\n        if coefficients is None:\n            raise ValueError(\"coefficients must be provided for the generalized_cosine window.\")\n        w = [sum(coefficients[k] * np.cos(2 * np.pi * k * n / (N - 1)) for k in range(len(coefficients))) for n in range(N)]\n    else:\n        raise ValueError(\"Unknown window type.\")\n    \n    return [round(coeff, 4) for coeff in w]\n\n# Example usage:\nprint(generate_window(\"hamming\", 5))\nprint(generate_window(\"hann\", 5))\nprint(generate_window(\"blackman_harris\", 5))\nprint(generate_window(\"generalized_cosine\", 5, [0.35875, 0.48829, 0.14128, 0.01168]))\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `N` is a positive integer. If not, it raises a `ValueError`.\n2. **Window Types**:\n   - **Hamming Window**: Uses the formula \\( w[n] = 0.54 - 0.46 \\cos(2\\pi n / (N-1)) \\).\n   - **Hann Window**: Uses the formula \\( w[n] = 0.5 - 0.5 \\cos(2\\pi n / (N-1)) \\).\n   - **Blackman-Harris Window**: Uses the formula \\( w[n] = a_0 - a_1 \\cos(2\\pi n / (N-1)) + a_2 \\cos(4\\pi n / (N-1)) - a_3 \\cos(6\\pi n / (N-1)) \\) with predefined coefficients.\n   - **Generalized Cosine Window**: Uses the formula \\( w[n] = \\sum_{k=0}^{M} a_k \\cos(2\\pi k n / (N-1)) \\) where `coefficients` is a list of coefficients \\( a_k \\).\n3. **Rounding**: Each coefficient is rounded to 4 decimal places before returning the list.\n\nThis function should work correctly for the specified window types and constraints."}
{"task_id": 171, "completion_id": 0, "completion": "Here is a Python implementation of the binary Logistic Regression classifier using mini-batch Gradient Descent:\n```python\nimport numpy as np\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    # Convert input lists to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Add intercept term (bias) to the data\n    X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n    \n    # Map labels to {0, 1}\n    label_map = {y_train[0]: 0, y_train[1]: 1}\n    y_train_mapped = np.array([label_map[label] for label in y_train])\n    \n    # Initialize weight vector with zeros\n    weights = np.zeros(X_train.shape[1])\n    \n    # Handle the special case where all training labels are identical\n    if np.all(y_train_mapped == y_train_mapped[0]):\n        return [y_train[0]] * len(X_test)\n    \n    # Mini-batch Gradient Descent\n    for epoch in range(epochs):\n        # Shuffle the training data\n        indices = np.random.permutation(len(X_train))\n        X_train_shuffled = X_train[indices]\n        y_train_shuffled = y_train_mapped[indices]\n        \n        # Iterate over mini-batches\n        for i in range(0, len(X_train_shuffled), batch_size):\n            X_batch = X_train_shuffled[i:i+batch_size]\n            y_batch = y_train_shuffled[i:i+batch_size]\n            \n            # Compute predictions\n            predictions = 1 / (1 + np.exp(-np.dot(X_batch, weights)))\n            \n            # Compute gradients\n            gradients = np.dot(X_batch.T, predictions - y_batch) / batch_size\n            \n            # Update weights\n            weights -= learning_rate * gradients\n    \n    # Predict labels for the test set\n    predictions = 1 / (1 + np.exp(-np.dot(X_test, weights)))\n    predicted_labels = [y_train[0] if pred < 0.5 else y_train[1] for pred in predictions]\n    \n    return predicted_labels\n```\nThis implementation follows the requirements specified in the task description. It includes the necessary steps for training the weight vector using mini-batch Gradient Descent and making predictions for the test set. The special case where all training labels are identical is handled by simply predicting that unique label for every test sample."}
{"task_id": 176, "completion_id": 0, "completion": "Certainly! Below is the implementation of the AdaBoost algorithm with decision stumps from scratch in Python. This code follows the specifications provided in the task description.\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    # Convert input lists to NumPy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Initialize sample weights\n    weights = np.ones(len(y_train)) / len(y_train)\n    \n    # Initialize list to store the stumps and their weights\n    stumps = []\n    stump_weights = []\n    \n    for t in range(n_estimators):\n        # Initialize best stump parameters\n        best_feature_index = None\n        best_threshold = None\n        best_polarity = None\n        best_error = 1.0\n        \n        # Iterate over each feature\n        for feature_index in range(X_train.shape[1]):\n            # Sort the feature values and corresponding labels\n            sorted_indices = np.argsort(X_train[:, feature_index])\n            sorted_X = X_train[sorted_indices, feature_index]\n            sorted_y = y_train[sorted_indices]\n            \n            # Iterate over each possible threshold\n            for i in range(1, len(sorted_X)):\n                threshold = (sorted_X[i] + sorted_X[i - 1]) / 2\n                polarity = 1\n                error = np.sum(weights[sorted_y != polarity * (sorted_X < threshold)])\n                \n                if error > 0.5:\n                    polarity = -1\n                    error = np.sum(weights[sorted_y != polarity * (sorted_X >= threshold)])\n                \n                if error < best_error:\n                    best_feature_index = feature_index\n                    best_threshold = threshold\n                    best_polarity = polarity\n                    best_error = error\n        \n        # If a perfect stump is found, stop early\n        if best_error == 0:\n            break\n        \n        # Calculate the weight of the current stump\n        alpha = 0.5 * np.log((1 - best_error) / (best_error + 1e-10))\n        \n        # Update the sample weights\n        stump_weights.append(alpha)\n        stump = (best_feature_index, best_threshold, best_polarity)\n        stumps.append(stump)\n        \n        for i in range(len(y_train)):\n            if y_train[i] == best_polarity * (X_train[i, best_feature_index] < best_threshold):\n                weights[i] *= np.exp(-alpha)\n            else:\n                weights[i] *= np.exp(alpha)\n        \n        # Normalize the weights\n        weights /= np.sum(weights)\n    \n    # Make predictions on the test set\n    predictions = []\n    for x_test in X_test:\n        stump_vote = 0\n        for alpha, stump in zip(stump_weights, stumps):\n            feature_index, threshold, polarity = stump\n            if polarity * (x_test[feature_index] < threshold) == 1:\n                stump_vote += alpha\n            else:\n                stump_vote -= alpha\n        \n        predictions.append(1 if stump_vote > 0 else 0)\n    \n    return predictions\n\n# Example usage:\n# X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n# y_train = [0, 0, 1, 1]\n# X_test = [[2.5, 3.5], [3.5, 4.5]]\n# n_estimators = 10\n# predictions = adaboost_predict(X_train, y_train, X_test, n_estimators)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The sample weights are initialized to be equal for all samples.\n2. **Stump Selection**: For each iteration, the code iterates over each feature and each possible threshold to find the decision stump with the smallest weighted classification error.\n3. **Stump Update**: If a perfect stump is found (weighted error = 0), the training stops early. Otherwise, the stump's weight is calculated, and the sample weights are updated.\n4. **Prediction**: After training, the code makes predictions on the test set by aggregating the weighted votes of the stumps.\n\nThis implementation ensures that the AdaBoost algorithm is correctly implemented with decision stumps as weak learners."}
{"task_id": 178, "completion_id": 0, "completion": "Here is a Python implementation of the Particle Swarm Optimization (PSO) algorithm to minimize the Sphere function:\n\n```python\nimport numpy as np\n\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    # Validate input arguments\n    if n_dims <= 0 or num_particles <= 0 or num_iterations <= 0:\n        return -1\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n    \n    # Initialize particles\n    positions = rng.uniform(-1, 1, (num_particles, n_dims))\n    velocities = np.zeros((num_particles, n_dims))\n    personal_best_positions = positions.copy()\n    personal_best_values = np.sum(positions**2, axis=1)\n    global_best_index = np.argmin(personal_best_values)\n    global_best_position = personal_best_positions[global_best_index]\n    global_best_value = personal_best_values[global_best_index]\n    \n    # PSO parameters\n    w = 0.5\n    c1 = 1.5\n    c2 = 1.5\n    \n    # PSO optimization loop\n    for _ in range(num_iterations):\n        for i in range(num_particles):\n            # Update velocity\n            velocities[i] = w * velocities[i] + c1 * rng.random() * (personal_best_positions[i] - positions[i]) + c2 * rng.random() * (global_best_position - positions[i])\n            \n            # Update position\n            new_position = positions[i] + velocities[i]\n            new_position = np.clip(new_position, -1, 1)\n            \n            # Evaluate new position\n            new_value = np.sum(new_position**2)\n            \n            # Update personal best\n            if new_value < personal_best_values[i]:\n                personal_best_positions[i] = new_position\n                personal_best_values[i] = new_value\n            \n            # Update global best\n            if new_value < global_best_value:\n                global_best_position = new_position\n                global_best_value = new_value\n    \n    # Return the best value rounded to four decimals\n    return round(global_best_value, 4)\n\n# Example usage:\n# result = particle_swarm_optimisation(n_dims=2, num_particles=10, num_iterations=100, seed=1)\n# print(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the input arguments are positive integers. If not, it returns `-1`.\n2. **Initialization**: \n   - Random positions and velocities are initialized within the specified bounds.\n   - Each particle's personal best position and value are initialized.\n   - The global best position and value are initialized based on the initial positions.\n3. **PSO Parameters**: The inertia weight `w`, cognitive weight `c1`, and social weight `c2` are set.\n4. **Optimization Loop**: \n   - For each iteration, each particle's velocity and position are updated based on the PSO equations.\n   - The new position is clipped to ensure it remains within the bounds.\n   - The new position's value is evaluated.\n   - If the new value is better than the personal best, the personal best is updated.\n   - If the new value is better than the global best, the global best is updated.\n5. **Return Value**: The best value found is returned rounded to four decimal places.\n\nThis implementation ensures that the PSO algorithm is fully deterministic with respect to the seed."}
{"task_id": 180, "completion_id": 0, "completion": "Here is a pure-function version of the k-Nearest Neighbours (k-NN) algorithm that can work both as a classifier (majority vote) and as a regressor (average). The function receives a training feature matrix, the corresponding target vector, a test feature matrix, the integer k and a string that specifies the task type (\"classification\" or \"regression\").\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((np.array(x1) - np.array(x2)) ** 2))\n\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    predictions = []\n    \n    for x_test in X_test:\n        distances = [(euclidean_distance(x_test, x_train_i), y_train_i) for x_train_i, y_train_i in zip(X_train, y_train)]\n        distances.sort()\n        \n        if k == 0 or k > len(distances):\n            k = len(distances)\n        \n        if task == \"classification\":\n            k_nearest_labels = [label for _, label in distances[:k]]\n            most_common = Counter(k_nearest_labels).most_common()\n            prediction = most_common[0][0]\n            if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n                prediction = min(prediction, most_common[1][0])\n        elif task == \"regression\":\n            k_nearest_values = [value for _, value in distances[:k]]\n            prediction = round(np.mean(k_nearest_values), 4)\n        else:\n            raise ValueError(\"Invalid task type. Must be 'classification' or 'regression'.\")\n        \n        predictions.append(prediction)\n    \n    return predictions\n```\n\n### Explanation:\n1. **Euclidean Distance Calculation**: The `euclidean_distance` function calculates the Euclidean distance between two feature vectors.\n2. **K-NN Prediction**:\n   - For each test sample, compute the Euclidean distance to all training samples.\n   - Sort the distances and select the k nearest neighbors.\n   - For classification, use the most frequent label among the k neighbors. In case of a tie, return the smallest label.\n   - For regression, compute the arithmetic mean of the k nearest neighbors' target values and round it to 4 decimal places.\n3. **Output**: The function returns a list of predictions for each test sample.\n\nThis implementation ensures that the function is pure and follows the specified rules and details."}
{"task_id": 184, "completion_id": 0, "completion": "To implement a CART decision tree classifier from scratch, we need to follow several steps. We'll start by defining the necessary functions to calculate Gini impurity, split the data, and build the decision tree. Finally, we'll use the trained tree to make predictions on the test data.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Calculate the Gini impurity of a set of labels.\"\"\"\n    _, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return 1 - np.sum(probabilities ** 2)\n\ndef split_data(X, y, feature, threshold):\n    \"\"\"Split the data based on a feature and threshold.\"\"\"\n    left_mask = X[:, feature] <= threshold\n    right_mask = X[:, feature] > threshold\n    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n\ndef best_split(X, y):\n    \"\"\"Find the best feature and threshold to split the data.\"\"\"\n    best_feature = None\n    best_threshold = None\n    best_gain = 0\n    parent_gini = gini_impurity(y)\n    \n    for feature in range(X.shape[1]):\n        sorted_indices = np.argsort(X[:, feature])\n        thresholds = (X[sorted_indices, feature][:-1] + X[sorted_indices, feature][1:]) / 2\n        \n        for threshold in thresholds:\n            left_X, left_y, right_X, right_y = split_data(X, y, feature, threshold)\n            if len(left_y) == 0 or len(right_y) == 0:\n                continue\n            left_gini = gini_impurity(left_y)\n            right_gini = gini_impurity(right_y)\n            gain = parent_gini - (len(left_y) / len(y)) * left_gini - (len(right_y) / len(y)) * right_gini\n            \n            if gain > best_gain:\n                best_gain = gain\n                best_feature = feature\n                best_threshold = threshold\n    \n    return best_feature, best_threshold, best_gain\n\ndef build_tree(X, y, max_depth, current_depth=0):\n    \"\"\"Build the decision tree recursively.\"\"\"\n    if len(np.unique(y)) == 1 or (max_depth is not None and current_depth >= max_depth):\n        return {'type': 'leaf', 'label': np.argmax(np.bincount(y))}\n    \n    feature, threshold, gain = best_split(X, y)\n    if gain == 0:\n        return {'type': 'leaf', 'label': np.argmax(np.bincount(y))}\n    \n    left_X, left_y, right_X, right_y = split_data(X, y, feature, threshold)\n    \n    left_tree = build_tree(left_X, left_y, max_depth, current_depth + 1)\n    right_tree = build_tree(right_X, right_y, max_depth, current_depth + 1)\n    \n    return {\n        'type': 'node',\n        'feature': feature,\n        'threshold': threshold,\n        'left': left_tree,\n        'right': right_tree\n    }\n\ndef predict(tree, x):\n    \"\"\"Predict the label for a single sample using the decision tree.\"\"\"\n    if tree['type'] == 'leaf':\n        return tree['label']\n    else:\n        if x[tree['feature']] <= tree['threshold']:\n            return predict(tree['left'], x)\n        else:\n            return predict(tree['right'], x)\n\ndef decision_tree_predict(X_train, y_train, X_test, max_depth=None):\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\"\"\"\n    tree = build_tree(X_train, y_train, max_depth)\n    predictions = [predict(tree, x) for x in X_test]\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y_train = np.array([0, 0, 1, 1])\n# X_test = np.array([[2, 3], [6, 7]])\n# predictions = decision_tree_predict(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n1. **Gini Impurity**: This function calculates the Gini impurity of a set of labels.\n2. **Split Data**: This function splits the data based on a given feature and threshold.\n3. **Best Split**: This function finds the best feature and threshold to split the data based on the Gini gain.\n4. **Build Tree**: This function builds the decision tree recursively using the best split.\n5. **Predict**: This function predicts the label for a single sample using the decision tree.\n6. **Decision Tree Predict**: This is the main function that builds the decision tree on the training data and predicts labels for the test data.\n\nThis implementation ensures that the decision tree is built deterministically based on the given rules and constraints."}
{"task_id": 190, "completion_id": 0, "completion": "To solve the problem of finding the best Gini split, we need to follow these steps:\n\n1. **Calculate the Gini impurity of the entire dataset**.\n2. **Iterate over each feature and each unique value in that feature** to form potential splits.\n3. **Compute the Gini impurity for each split** and keep track of the best split found.\n4. **Handle edge cases** where no valid split exists.\n\nHere's the implementation of the `best_gini_split` function:\n\n```python\nimport numpy as np\n\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Calculate the Gini impurity of the entire dataset\n    def gini_impurity(labels):\n        _, counts = np.unique(labels, return_counts=True)\n        probabilities = counts / len(labels)\n        return 1 - np.sum(probabilities ** 2)\n    \n    G_whole_dataset = gini_impurity(y)\n    \n    # If all labels are the same, return the Gini impurity of the whole dataset\n    if len(np.unique(y)) == 1:\n        return (-1, None, round(G_whole_dataset, 4))\n    \n    best_feature_index = -1\n    best_threshold_value = None\n    best_gini = G_whole_dataset\n    \n    for feature_index in range(X.shape[1]):\n        unique_values = np.unique(X[:, feature_index])\n        for threshold in unique_values:\n            left_indices = X[:, feature_index] <= threshold\n            right_indices = X[:, feature_index] > threshold\n            \n            if np.any(left_indices) and np.any(right_indices):\n                left_gini = gini_impurity(y[left_indices])\n                right_gini = gini_impurity(y[right_indices])\n                n_left = np.sum(left_indices)\n                n_right = np.sum(right_indices)\n                n_total = len(y)\n                G_split = (n_left / n_total) * left_gini + (n_right / n_total) * right_gini\n                \n                if G_split < best_gini or (G_split == best_gini and (\n                    feature_index < best_feature_index or \n                    (feature_index == best_feature_index and threshold < best_threshold_value)\n                )):\n                    best_feature_index = feature_index\n                    best_threshold_value = threshold\n                    best_gini = G_split\n    \n    return (best_feature_index, best_threshold_value, round(best_gini, 4))\n\n# Example usage:\nX = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]\ny = [0, 0, 1, 1, 1]\nprint(best_gini_split(X, y))  # Output should be (0, 3.5, 0.0)\n```\n\n### Explanation:\n1. **Gini Impurity Calculation**: The `gini_impurity` function calculates the Gini impurity of a given set of labels.\n2. **Initial Check**: If all labels are the same, the function returns the Gini impurity of the whole dataset.\n3. **Iterate Over Features and Thresholds**: For each feature and each unique value in that feature, the function forms a split and calculates the Gini impurity of the resulting left and right nodes.\n4. **Update Best Split**: The function keeps track of the best split found based on the Gini impurity and tie-breaking rules.\n5. **Return Result**: The function returns the best feature index, threshold value, and Gini impurity rounded to 4 decimal places.\n\nThis implementation ensures that the best Gini split is found efficiently and correctly handles edge cases."}
{"task_id": 191, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    def euclidean(x, y):\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        if len(x) != len(y) or len(x) == 0:\n            return -1\n        return round(np.sqrt(np.sum((x - y) ** 2)), 4)\n\n    def manhattan(x, y):\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        if len(x) != len(y) or len(x) == 0:\n            return -1\n        return round(np.sum(np.abs(x - y)), 4)\n\n    def chebyshev(x, y):\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        if len(x) != len(y) or len(x) == 0:\n            return -1\n        return round(np.max(np.abs(x - y)), 4)\n\n    def cosine(x, y):\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        if len(x) != len(y) or len(x) == 0:\n            return -1\n        if np.linalg.norm(x) == 0 or np.linalg.norm(y) == 0:\n            return -1\n        return round(1 - np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)), 4)\n\n    if name == 'euclidean':\n        return euclidean\n    elif name == 'manhattan':\n        return manhattan\n    elif name == 'chebyshev':\n        return chebyshev\n    elif name == 'cosine':\n        return cosine\n    else:\n        return lambda x, y: -1\n```\n\nThis code defines a factory function `metric` that returns a callable distance function based on the provided metric name. Each distance function includes validation to ensure the inputs are valid numeric vectors of the same length and non-empty. The cosine distance function also checks for zero-norm vectors. If the metric name is not supported or the validation fails, the callable returns -1."}
{"task_id": 197, "completion_id": 0, "completion": "To solve this task, we need to handle different types of action spaces in a reinforcement learning environment. The function `action_stats` will take an environment object `env`, and two Boolean indicators `md_action` and `cont_action` to determine the type of action space. Based on these indicators, the function will return the necessary statistics about the action space.\n\nHere's the implementation of the `action_stats` function:\n\n```python\nimport numpy as np\nfrom itertools import product\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    n_actions_per_dim = []\n    action_ids = None\n    action_dim = 0\n\n    if cont_action:\n        # Continuous action space\n        if md_action:\n            action_dim = env.action_space.shape[0]\n            n_actions_per_dim = [np.inf] * action_dim\n        else:\n            action_dim = 1\n            n_actions_per_dim = [np.inf]\n    else:\n        # Discrete action space\n        if md_action:\n            action_dim = len(env.action_space.spaces)\n            n_actions_per_dim = [space.n for space in env.action_space.spaces]\n            action_ids = list(product(*[range(space.n) for space in env.action_space.spaces]))\n        else:\n            action_dim = 1\n            n_actions_per_dim = [env.action_space.n]\n            action_ids = list(range(env.action_space.n))\n\n    return n_actions_per_dim, action_ids, action_dim\n\n# Example usage:\n# Assuming env is an OpenAI Gym environment\n# n_actions_per_dim, action_ids, action_dim = action_stats(env, md_action, cont_action)\n```\n\n### Explanation:\n1. **Initialization**:\n   - `n_actions_per_dim` is initialized as an empty list to store the number of actions per dimension.\n   - `action_ids` is initialized as `None` to store the list of all discrete actions.\n   - `action_dim` is initialized as 0 to store the total number of action dimensions.\n\n2. **Continuous Action Space**:\n   - If `cont_action` is `True`, the function checks if the action space is multi-dimensional (`md_action`).\n   - If `md_action` is `True`, the number of dimensions is determined by `env.action_space.shape[0]`, and all dimensions are set to `np.inf`.\n   - If `md_action` is `False`, the action space is 1-dimensional, and the dimension is set to `np.inf`.\n\n3. **Discrete Action Space**:\n   - If `cont_action` is `False`, the function checks if the action space is multi-dimensional (`md_action`).\n   - If `md_action` is `True`, the number of dimensions is determined by the length of `env.action_space.spaces`, and the number of actions per dimension is determined by `space.n` for each sub-space.\n   - The `action_ids` are generated using the cartesian product of the ranges for each dimension.\n   - If `md_action` is `False`, the action space is 1-dimensional, and the number of actions per dimension is determined by `env.action_space.n`.\n   - The `action_ids` are generated using the range of `env.action_space.n`.\n\n4. **Return**:\n   - The function returns a tuple containing `n_actions_per_dim`, `action_ids`, and `action_dim`.\n\nThis implementation ensures that the function correctly handles both continuous and discrete action spaces, and it can handle both single- and multi-dimensional cases."}
{"task_id": 198, "completion_id": 0, "completion": "To implement the \u03b2-maximisation step in Latent Dirichlet Allocation (LDA), we need to follow these steps:\n\n1. Initialize a matrix `beta` of shape `(V, T)` with zeros.\n2. Iterate over each document and its tokens.\n3. For each token, update the corresponding entry in the `beta` matrix based on the variational parameter `phi`.\n4. Normalize each column of the `beta` matrix so that the sum of each column is 1.\n5. Round the entries of the `beta` matrix to 4 decimal places.\n\nHere is the complete implementation of the `update_beta` function:\n\n```python\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    T = len(phi[0][0])  # Number of topics\n    beta = np.zeros((V, T))\n\n    # Count the occurrences of each word in each topic\n    for d, doc_phi in enumerate(phi):\n        for n, word in enumerate(corpus[d]):\n            for t in range(T):\n                beta[word, t] += doc_phi[n, t]\n\n    # Normalize each column of beta\n    for t in range(T):\n        if np.sum(beta[:, t]) > 0:\n            beta[:, t] /= np.sum(beta[:, t])\n\n    # Round the entries to 4 decimal places\n    beta = np.round(beta, 4).tolist()\n\n    return beta\n```\n\n### Explanation:\n\n1. **Initialization**: We initialize a `beta` matrix of shape `(V, T)` with zeros.\n2. **Counting Occurrences**: We iterate over each document and its tokens. For each token, we update the corresponding entry in the `beta` matrix based on the variational parameter `phi`.\n3. **Normalization**: After counting the occurrences, we normalize each column of the `beta` matrix so that the sum of each column is 1.\n4. **Rounding**: Finally, we round the entries of the `beta` matrix to 4 decimal places and convert it to a list of lists before returning it.\n\nThis implementation ensures that the `beta` matrix is updated correctly and meets the specified constraints."}
{"task_id": 202, "completion_id": 0, "completion": "To implement the Partitioning Around Medoids (PAM) clustering algorithm, we need to follow the steps outlined in the problem description. Here's a step-by-step implementation of the `pam_clustering` function:\n\n1. **Initialize Medoids**: Start with the first `k` samples of the data matrix `X` as the initial medoids.\n2. **Iterative Improvement**: Repeatedly try all possible swaps between a medoid and a non-medoid, accepting the swap that strictly reduces the total cost the most. Stop when no swap can further decrease the cost.\n3. **Assign Labels**: After convergence, assign each sample to the nearest medoid.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    n, d = X.shape\n    \n    # Step 1: Initialize medoids as the first k samples\n    medoids = np.arange(k)\n    \n    # Step 2: Calculate the total cost of the initial clustering\n    def calculate_cost(medoids, X):\n        cost = 0\n        for i in range(n):\n            medoid_index = np.argmin(np.linalg.norm(X[i] - X[medoids], axis=1))\n            cost += np.linalg.norm(X[i] - X[medoids[medoid_index]])\n        return cost\n    \n    initial_cost = calculate_cost(medoids, X)\n    \n    # Step 3: Iteratively improve the medoids\n    while True:\n        best_swap = None\n        best_cost = initial_cost\n        \n        for medoid_index in range(k):\n            for non_medoid_index in range(k, n):\n                # Try swapping medoid_index with non_medoid_index\n                new_medoids = medoids.copy()\n                new_medoids[medoid_index] = non_medoid_index\n                \n                # Calculate the new cost\n                new_cost = calculate_cost(new_medoids, X)\n                \n                # Check if the swap reduces the cost\n                if new_cost < best_cost:\n                    best_cost = new_cost\n                    best_swap = (medoid_index, non_medoid_index)\n        \n        if best_swap is None:\n            break\n        \n        # Perform the best swap\n        medoids[best_swap[0]] = best_swap[1]\n        initial_cost = best_cost\n    \n    # Step 4: Assign labels to each sample\n    labels = []\n    for i in range(n):\n        medoid_index = np.argmin(np.linalg.norm(X[i] - X[medoids], axis=1))\n        labels.append(medoid_index)\n    \n    return labels\n\n# Example usage\nX = np.array([[0,0], [1,0], [0,1], [5,5], [6,5], [5,6]])\nk = 2\nprint(pam_clustering(X, k))  # Output: [0, 0, 0, 1, 1, 1]\n```\n\n### Explanation:\n1. **Initialization**: The first `k` samples are selected as the initial medoids.\n2. **Cost Calculation**: The total cost is calculated as the sum of the Euclidean distances from each sample to its assigned medoid.\n3. **Iterative Improvement**: The algorithm iteratively tries to swap each medoid with each non-medoid and selects the swap that reduces the total cost the most. If no swap reduces the cost, the algorithm stops.\n4. **Label Assignment**: Each sample is assigned to the nearest medoid, and the labels are returned as a list.\n\nThis implementation ensures that the algorithm is deterministic and can be tested for correctness."}
{"task_id": 216, "completion_id": 0, "completion": "Here is a Python implementation of the Elastic Net regression from scratch using batch gradient descent:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    x_test = np.array(x_test)\n    \n    # Check if the shapes of x_train and x_test are compatible\n    if x_train.shape[1] != x_test.shape[1]:\n        return -1\n    \n    # Check if n_iterations is non-positive\n    if n_iterations < 1:\n        return -1\n    \n    # Add bias term to the feature matrix\n    x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n    x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n    \n    # Generate polynomial features if degree > 1\n    if degree > 1:\n        x_train = np.hstack((x_train, np.vander(x_train[:, 1], degree + 1)[:, 1:]))\n        x_test = np.hstack((x_test, np.vander(x_test[:, 1], degree + 1)[:, 1:]))\n    \n    # Standardize the non-bias features\n    x_train[:, 1:] = (x_train[:, 1:] - np.mean(x_train[:, 1:], axis=0)) / np.std(x_train[:, 1:], axis=0)\n    x_test[:, 1:] = (x_test[:, 1:] - np.mean(x_test[:, 1:], axis=0)) / np.std(x_test[:, 1:], axis=0)\n    \n    # Initialize weight vector\n    weights = np.zeros(x_train.shape[1])\n    \n    # Elastic Net loss function\n    def loss_function(weights, x, y):\n        predictions = np.dot(x, weights)\n        mse = np.mean((predictions - y) ** 2)\n        l1_penalty = l1_ratio * np.sum(np.abs(weights[1:]))\n        l2_penalty = (1 - l1_ratio) * 0.5 * np.sum(weights[1:] ** 2)\n        return mse + reg_factor * (l1_penalty + l2_penalty)\n    \n    # Elastic Net gradient function\n    def gradient_function(weights, x, y):\n        predictions = np.dot(x, weights)\n        mse_gradient = 2 * np.dot(x.T, (predictions - y)) / x.shape[0]\n        l1_gradient = l1_ratio * np.sign(weights[1:])\n        l2_gradient = (1 - l1_ratio) * weights[1:]\n        return mse_gradient + reg_factor * (l1_gradient + l2_gradient)\n    \n    # Batch gradient descent\n    for _ in range(n_iterations):\n        gradient = gradient_function(weights, x_train, y_train)\n        weights -= learning_rate * gradient\n    \n    # Predictions for x_test\n    predictions = np.dot(x_test, weights)\n    predictions = np.round(predictions, 4).tolist()\n    \n    return predictions\n```\n\nThis function implements Elastic Net regression from scratch using batch gradient descent. It handles polynomial feature expansion, standardization of features, and the Elastic Net loss function with both L1 and L2 regularization. The function returns predictions for the test set rounded to four decimal places. If the input shapes are incompatible or if `n_iterations` is non-positive, it returns `-1`."}
{"task_id": 217, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized computation\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Compute sigmoid transformation\n    sigmoid = 1 / (1 + np.exp(-predicted))\n    \n    # Compute gradient\n    gradient = actual * sigmoid * (1 - actual)\n    \n    # Compute Hessian\n    hessian = sigmoid * (1 - sigmoid)\n    \n    # Round results to 6 decimal places\n    gradient = np.round(gradient, 6)\n    hessian = np.round(hessian, 6)\n    sigmoid = np.round(sigmoid, 6)\n    \n    # Convert numpy arrays back to lists\n    gradient_list = gradient.tolist()\n    hessian_list = hessian.tolist()\n    probability_list = sigmoid.tolist()\n    \n    return gradient_list, hessian_list, probability_list\n```\n\nThis function first converts the input lists or tuples to NumPy arrays to enable vectorized computation. It then calculates the sigmoid transformation, gradient, and Hessian using the provided formulas. The results are rounded to six decimal places and finally converted back to Python lists before being returned."}
{"task_id": 218, "completion_id": 0, "completion": "To solve this problem, we need to implement the `blackjack_outcome` function that evaluates the outcome of a finished round of Blackjack based on the given rules. Here's a step-by-step approach to achieve this:\n\n1. **Calculate the Hand Value**: We need a helper function to calculate the value of a hand, considering the rules for usable aces.\n2. **Check for Bust**: Determine if a hand is a bust (score > 21).\n3. **Check for Natural Blackjack**: Determine if a hand is a natural blackjack (two cards, one Ace and one 10-value card).\n4. **Evaluate the Outcome**: Compare the player's and dealer's hands to determine the reward.\n\nHere's the complete implementation:\n\n```python\ndef calculate_hand_value(hand: list[int]) -> int:\n    \"\"\"Calculate the value of a hand, considering the rules for usable aces.\"\"\"\n    total = sum(hand)\n    aces = hand.count(1)\n    \n    # If there are aces and the total is less than or equal to 11, use one ace as 11\n    if aces > 0 and total + 10 <= 21:\n        total += 10\n    \n    return total\n\ndef is_bust(hand: list[int]) -> bool:\n    \"\"\"Check if a hand is a bust (score > 21).\"\"\"\n    return calculate_hand_value(hand) > 21\n\ndef is_natural_blackjack(hand: list[int]) -> bool:\n    \"\"\"Check if a hand is a natural blackjack (two cards, one Ace and one 10-value card).\"\"\"\n    return len(hand) == 2 and 1 in hand and sum(hand) == 11\n\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\"\"\"\n    player_value = calculate_hand_value(player)\n    dealer_value = calculate_hand_value(dealer)\n    \n    if is_bust(player):\n        return -1.0  # Player busts, loses immediately\n    \n    if is_bust(dealer):\n        return 1.0  # Dealer busts, player wins\n    \n    if player_value == dealer_value:\n        return 0.0  # Draw\n    \n    if player_value > dealer_value:\n        if is_natural_blackjack(player):\n            return 1.5  # Player wins with natural blackjack\n        return 1.0  # Player wins\n    \n    return -1.0  # Dealer wins\n\n# Example usage:\nplayer_hand = [10, 1]  # Player's hand: 10 and Ace\ndealer_hand = [9, 2]  # Dealer's hand: 9 and 2\n\nprint(blackjack_outcome(player_hand, dealer_hand))  # Output: 1.0\n```\n\n### Explanation:\n1. **calculate_hand_value**: This function calculates the total value of a hand, considering the rules for usable aces. It sums the values of the cards and checks if there are any aces that can be counted as 11 to keep the total score as low as possible without busting.\n2. **is_bust**: This function checks if a hand is a bust by calling `calculate_hand_value` and comparing the result to 21.\n3. **is_natural_blackjack**: This function checks if a hand is a natural blackjack by ensuring it has exactly two cards, one of which is an Ace and the sum of the cards is 11.\n4. **blackjack_outcome**: This function evaluates the outcome of the game by comparing the player's and dealer's hands. It handles all the rules, including natural blackjacks and busts, and returns the appropriate reward.\n\nThis implementation ensures that the function correctly evaluates the outcome of a Blackjack round according to the given rules."}
{"task_id": 221, "completion_id": 0, "completion": "Here is a Python implementation of the Mean Squared Logarithmic Error (MSLE) metric:\n```python\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"Helper function that returns element-wise squared log errors.\"\"\"\n    # Convert inputs to NumPy arrays\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    \n    # Check if inputs have the same shape\n    if actual.shape != predicted.shape:\n        return -1\n    \n    # Check if all values are non-negative\n    if np.any(actual < 0) or np.any(predicted < 0):\n        return -1\n    \n    # Calculate element-wise squared log errors\n    return (np.log(1 + actual) - np.log(1 + predicted)) ** 2\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns -1.\n    \"\"\"\n    # Calculate element-wise squared log errors\n    squared_errors = squared_log_error(actual, predicted)\n    \n    # Check if the calculation was successful\n    if squared_errors == -1:\n        return -1\n    \n    # Calculate the mean of the squared errors\n    msle = np.mean(squared_errors)\n    \n    # Round the result to 4 decimal places\n    return round(msle, 4)\n```\nThis implementation includes the helper function `squared_log_error` that calculates the element-wise squared logarithmic errors, and the main function `mean_squared_log_error` that calculates the mean of those squared errors and rounds the result to 4 decimal places. The function also includes input validation to ensure that the inputs have the same shape, contain only non-negative values, and are not empty. If any of these checks fail, the function returns -1."}
{"task_id": 222, "completion_id": 0, "completion": "Certainly! Below is the implementation of the sigmoid activation function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    if isinstance(z, (int, float)):\n        # Single scalar input\n        if z >= 0:\n            result = 1 / (1 + np.exp(-z))\n        else:\n            result = np.exp(z) / (1 + np.exp(z))\n        return round(result, 4)\n    elif isinstance(z, list):\n        # List input\n        z_array = np.array(z)\n        if np.any(z_array >= 0):\n            result_array = 1 / (1 + np.exp(-z_array))\n        else:\n            result_array = np.exp(z_array) / (1 + np.exp(z_array))\n        return result_array.tolist()\n    elif isinstance(z, np.ndarray):\n        # NumPy array input\n        if np.any(z >= 0):\n            result_array = 1 / (1 + np.exp(-z))\n        else:\n            result_array = np.exp(z) / (1 + np.exp(z))\n        return result_array.tolist()\n    else:\n        raise ValueError(\"Input must be a scalar, list, or NumPy array\")\n\n# Example usage:\nprint(sigmoid(1000))  # Single scalar\nprint(sigmoid([1, -1, 1000]))  # List\nprint(sigmoid(np.array([1, -1, 1000])))  # NumPy array\n```\n\n### Explanation:\n1. **Single Scalar Input**: If the input is a single scalar, the function checks if `z` is non-negative. If it is, it computes the sigmoid using the standard formula. If `z` is negative, it uses the numerically stable alternative `exp(z) / (1 + exp(z))`. The result is then rounded to 4 decimal places.\n2. **List Input**: If the input is a list, it converts the list to a NumPy array and applies the same logic as for the scalar case. The result is then converted back to a Python list and returned.\n3. **NumPy Array Input**: If the input is a NumPy array, it applies the same logic as for the list case. The result is returned as a Python list.\n4. **Error Handling**: If the input is not a scalar, list, or NumPy array, the function raises a `ValueError`.\n\nThis implementation ensures that the sigmoid function is numerically stable for very large positive or negative inputs and works element-wise for any 1-D or 2-D array-like input."}
{"task_id": 224, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    # Helper function to apply Leaky ReLU to a single value\n    def apply_leaky_relu(value):\n        if value >= 0:\n            return value\n        else:\n            return a * value\n\n    # Convert input to a numpy array if it's not already\n    if not isinstance(z, np.ndarray):\n        z = np.array(z)\n\n    # Apply Leaky ReLU to each element in the array\n    activated_array = np.vectorize(apply_leaky_relu)(z)\n\n    # Convert the result back to a list with the same structure as the input\n    if isinstance(z, list):\n        return activated_array.tolist()\n    else:\n        return activated_array.tolist()\n```\n\nThis function `leaky_relu` takes an input `z` which can be a scalar, a list, or a `numpy.ndarray`, and applies the Leaky ReLU activation function to each element. The optional parameter `a` controls the slope for negative inputs, defaulting to 0.01. The function returns the activated values as a Python list with the same nested structure as the input."}
{"task_id": 226, "completion_id": 0, "completion": "Certainly! Below is the implementation of the AdaBoost algorithm with decision stumps from scratch in Python:\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    ensemble_predictions = np.zeros(n_samples)\n    \n    for _ in range(n_clf):\n        # Initialize best stump parameters\n        best_stump = None\n        best_error = float('inf')\n        best_threshold = None\n        best_polarity = None\n        \n        # Iterate over each feature\n        for feature_index in range(n_features):\n            thresholds = np.unique(X_train[:, feature_index])\n            for threshold in thresholds:\n                # Create two stumps for each polarity\n                for polarity in [-1, 1]:\n                    # Predict labels using the current stump\n                    predictions = np.where(X_train[:, feature_index] < threshold, polarity, -polarity)\n                    # Calculate the error\n                    error = np.sum(sample_weights * (predictions != y_train))\n                    # Update the best stump if the current one has a lower error\n                    if error < best_error:\n                        best_error = error\n                        best_stump = (feature_index, threshold, polarity)\n        \n        # Calculate the alpha value for the best stump\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + 1e-10))\n        \n        # Update the sample weights\n        predictions = np.where(X_train[:, best_stump[0]] < best_stump[1], best_stump[2], -best_stump[2])\n        sample_weights *= np.exp(-alpha * y_train * predictions)\n        sample_weights /= np.sum(sample_weights)\n        \n        # Update the ensemble predictions\n        ensemble_predictions += alpha * predictions\n    \n    # Predict the labels for X_test\n    test_predictions = np.sign(ensemble_predictions)\n    return test_predictions.tolist()\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y_train = np.array([-1, -1, 1, 1])\n# X_test = np.array([[2.5, 3.5]])\n# n_clf = 5\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**:\n   - `sample_weights` are initialized to be equal for all samples.\n   - `ensemble_predictions` are initialized to zero.\n\n2. **Training Loop**:\n   - For each boosting round, the algorithm iterates over each feature and threshold to find the best decision stump.\n   - The best stump is chosen based on the weighted classification error.\n   - The alpha value for the best stump is calculated using the formula provided.\n   - The sample weights are updated to give higher weights to misclassified samples.\n   - The ensemble predictions are updated by adding the weighted predictions of the current stump.\n\n3. **Prediction**:\n   - The final predictions for `X_test` are obtained by taking the sign of the weighted sum of stump decisions.\n\nThis implementation ensures that the AdaBoost ensemble is trained using decision stumps and predicts labels for new data based on the ensemble's predictions."}
{"task_id": 241, "completion_id": 0, "completion": "To implement the orthogonal weight initializer as described, we need to follow the steps outlined in the problem statement. Here's the complete implementation of the `orthogonal` function:\n\n```python\nimport numpy as np\n\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    if len(shape) < 2:\n        return -1\n    \n    # Step 2: Create a 2-D matrix A \u2208 \u211d^{d0\u00d7(d1\u22efdn)} filled with i.i.d. samples from the standard normal distribution.\n    d0 = shape[0]\n    d1 = np.prod(shape[1:])\n    A = np.random.randn(d0, d1)\n    \n    # Step 3: Compute the singular value decomposition (SVD) of A\n    U, _, Vt = np.linalg.svd(A, full_matrices=False)\n    \n    # Step 4: Choose the SVD factor that has the same size as A\n    if U.shape == A.shape:\n        Q = U\n    else:\n        Q = Vt\n    \n    # Step 5: Reshape Q back to the requested tensor shape and multiply it by scale\n    result = Q.reshape(shape) * scale\n    \n    # Check orthogonality condition\n    flat = result.reshape(shape[0], -1)\n    if shape[0] <= flat.shape[1]:\n        orthogonality_condition = np.allclose(flat @ flat.T, scale**2 * np.eye(shape[0]), atol=1e-6)\n    else:\n        orthogonality_condition = np.allclose(flat.T @ flat, scale**2 * np.eye(shape[1]), atol=1e-6)\n    \n    if not orthogonality_condition:\n        raise ValueError(\"The resulting tensor does not satisfy the orthogonality condition.\")\n    \n    return result.tolist()\n\n# Example usage\nnp.random.seed(0)\nshape = (2, 2)\nscale = 0.5\nresult = orthogonal(shape, scale)\nprint(result)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the length of the shape is less than 2. If so, it returns `-1`.\n2. **Matrix Initialization**: It initializes a 2D matrix `A` with i.i.d. samples from the standard normal distribution.\n3. **SVD Computation**: It computes the singular value decomposition (SVD) of `A`.\n4. **Selection of SVD Factor**: It selects the appropriate SVD factor `Q` based on the shape of `A`.\n5. **Reshaping and Scaling**: It reshapes `Q` back to the desired shape and multiplies it by the scale factor.\n6. **Orthogonality Check**: It checks if the resulting tensor satisfies the orthogonality condition. If not, it raises a `ValueError`.\n\nThis implementation ensures that the resulting tensor is orthogonally initialized and satisfies the specified orthogonality condition."}
{"task_id": 243, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    # Define the weights and biases\n    W1 = np.ones((3, 4))\n    b1 = np.zeros(4)\n    W2 = np.ones((4, 4))\n    b2 = np.zeros(4)\n    W3 = np.ones((4, 4))\n    b3 = np.zeros(4)\n    Wa = np.ones((4, 3))\n    ba = np.zeros(3)\n    Wc = np.ones((4, 1))\n    bc = np.zeros(1)\n\n    # Perform the forward pass\n    z1 = np.maximum(0, np.dot(state, W1) + b1)\n    z2 = np.maximum(0, np.dot(z1, W2) + b2)\n    z3 = np.maximum(0, np.dot(z2, W3) + b3)\n    logits = np.dot(z3, Wa) + ba\n    action_probs = np.exp(logits) / np.sum(np.exp(logits))\n    state_value = np.dot(z3, Wc) + bc\n\n    # Round the results to 4 decimal places\n    action_probs = [round(prob, 4) for prob in action_probs]\n    state_value = round(state_value[0], 4)\n\n    return action_probs, state_value\n\n# Example usage:\nstate = [1.0, 2.0, 3.0]\naction_probs, state_value = actor_critic_forward(state)\nprint(\"Action Probabilities:\", action_probs)\nprint(\"State Value:\", state_value)\n```\n\nThis function performs the forward pass of the described Actor\u2013Critic network using the given weights and biases. It calculates the action probabilities and the state value for the input state and returns them rounded to four decimal places."}
{"task_id": 249, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # Convert state to a numpy array\n    state = np.array(state)\n    \n    # Shared layers\n    z1 = np.dot(state, params['W1']) + params['b1']\n    a1 = np.maximum(0, z1)  # ReLU activation\n    \n    z2 = np.dot(a1, params['W2']) + params['b2']\n    a2 = np.maximum(0, z2)  # ReLU activation\n    \n    z3 = np.dot(a2, params['W3']) + params['b3']\n    a3 = np.maximum(0, z3)  # ReLU activation\n    \n    # Actor head\n    z_a = np.dot(a3, params['Wa']) + params['ba']\n    probabilities = np.exp(z_a) / np.sum(np.exp(z_a))  # Soft-max activation\n    \n    # Critic head\n    z_c = np.dot(a3, params['Wc']) + params['bc']\n    value = z_c[0]  # Scalar output\n    \n    # Round results to 4 decimal places\n    probabilities = [round(p, 4) for p in probabilities]\n    value = round(value, 4)\n    \n    return probabilities, value\n```\n\nThis function performs the forward pass through the Actor-Critic network as described. It first processes the input state through three shared layers with ReLU activations. Then, it uses the output of these layers to compute the action probabilities through the actor head and the state value through the critic head. Finally, it rounds the results to four decimal places and returns them."}
{"task_id": 253, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    w = np.array(w)\n    penalty = alpha * (l1_ratio * np.sum(np.abs(w)) + (1 - l1_ratio) * 0.5 * np.sum(w ** 2))\n    if gradient:\n        gradient = alpha * (l1_ratio * np.sign(w) + (1 - l1_ratio) * w)\n        return [round(g, 4) for g in gradient]\n    else:\n        return round(penalty, 4)\n\n# Example usage:\nw = [1.0, -2.0, 3.0]\nalpha = 0.5\nl1_ratio = 0.75\n\n# Compute the Elastic-Net penalty\npenalty = elastic_net_regularization(w, alpha, l1_ratio)\nprint(\"Penalty:\", penalty)\n\n# Compute the Elastic-Net gradient\ngradient = elastic_net_regularization(w, alpha, l1_ratio, gradient=True)\nprint(\"Gradient:\", gradient)\n```\n\nThis function `elastic_net_regularization` computes the Elastic-Net penalty and its gradient based on the provided weight vector, regularization strength, and L1 ratio. The function handles both cases where the gradient is needed and where only the penalty value is required, rounding the results to 4 decimal places as specified."}
{"task_id": 256, "completion_id": 0, "completion": "To implement the numerical gradient checker for a vanilla RNN parameter, we need to follow these steps:\n\n1. Check if the parameter name is \"Ba\" or \"Bx\" and redirect it to the lower-case variant (\"ba\" or \"bx\").\n2. Check if the parameter name is \"X\" or \"y\" and return `None` immediately.\n3. Iterate over every element of the chosen parameter.\n4. Perturb the parameter by `+\u03b5` and `-\u03b5`.\n5. Run the forward loop `n_t` times for each perturbation, collect the predictions, and evaluate the loss.\n6. Approximate the partial derivative using the centered finite-difference formula.\n7. Store the numerical gradient in `grads` at the same index but finally return `grads.T` (transpose of the accumulated array).\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\nfrom copy import deepcopy\n\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    # Handle special cases\n    if param_name in [\"Ba\", \"Bx\"]:\n        param_name = param_name.lower()\n    if param_name in [\"X\", \"y\"]:\n        return None\n\n    # Get the parameter and its shape\n    param = model.parameters[param_name]\n    param_shape = param.shape\n    param_size = np.prod(param_shape)\n    grads = np.zeros(param_size, dtype=np.float64)\n\n    # Create a copy of the model to avoid modifying the original model\n    model_copy = deepcopy(model)\n\n    # Iterate over every element of the parameter\n    for i in range(param_size):\n        # Reshape the parameter to a 1D array for easy indexing\n        param_flat = param.flatten()\n        param_flat[i] += epsilon\n        model_copy.parameters[param_name] = param_flat.reshape(param_shape)\n\n        # Forward pass with perturbed parameter\n        predictions_plus = [model_copy.forward(X[:, :, t]) for t in range(n_t)]\n        loss_plus = loss_func(predictions_plus)\n\n        # Reset the model and perturb the parameter by -epsilon\n        model_copy.flush_gradients()\n        param_flat[i] -= 2 * epsilon\n        model_copy.parameters[param_name] = param_flat.reshape(param_shape)\n\n        # Forward pass with perturbed parameter\n        predictions_minus = [model_copy.forward(X[:, :, t]) for t in range(n_t)]\n        loss_minus = loss_func(predictions_minus)\n\n        # Reset the model and restore the original parameter\n        model_copy.flush_gradients()\n        param_flat[i] += epsilon\n        model_copy.parameters[param_name] = param.reshape(param_shape)\n\n        # Approximate the partial derivative\n        grad = (loss_plus - loss_minus) / (2 * epsilon)\n        grads[i] = grad\n\n    # Reshape the gradients to the original parameter shape and transpose\n    grads = grads.reshape(param_shape).T\n    return grads\n```\n\nThis implementation follows the steps outlined in the task description and ensures that the numerical gradient is computed correctly using the centered finite-difference formula. The function handles special cases and returns the transposed numerical gradient as required."}
{"task_id": 257, "completion_id": 0, "completion": "To implement the AdaBoost algorithm with decision stumps, we need to follow these steps:\n\n1. Initialize the weights of the training examples.\n2. For each iteration, train a decision stump that minimizes the weighted classification error.\n3. Calculate the weight of the stump and update the weights of the training examples.\n4. Aggregate the votes of all stumps to make predictions on the test set.\n\nHere's the complete implementation of the `adaboost_predict` function:\n\n```python\nimport numpy as np\n\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n\n    m, n = X_train.shape\n    weights = np.ones(m) / m\n    stump_weights = []\n\n    for _ in range(n_clf):\n        # Initialize the best stump parameters\n        best_stump = None\n        best_error = float('inf')\n        best_threshold = 0\n        best_direction = 1\n\n        # Try splitting on each feature and threshold\n        for feature_idx in range(n):\n            for threshold in np.unique(X_train[:, feature_idx]):\n                # Try both directions\n                for direction in [-1, 1]:\n                    # Create the stump\n                    stump = lambda x, feature_idx=feature_idx, threshold=threshold, direction=direction: \\\n                        np.where(direction * x[:, feature_idx] > direction * threshold, 1, -1)\n                    \n                    # Calculate the error\n                    predictions = stump(X_train, feature_idx, threshold, direction)\n                    error = np.sum(weights * (predictions != y_train))\n                    \n                    # Update the best stump if this one is better\n                    if error < best_error:\n                        best_stump = stump\n                        best_error = error\n                        best_threshold = threshold\n                        best_direction = direction\n\n        # Calculate the weight of the best stump\n        alpha = 0.5 * np.log((1 - best_error) / best_error)\n        stump_weights.append(alpha)\n\n        # Update the weights of the training examples\n        predictions = best_stump(X_train, best_threshold, best_direction)\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)\n\n    # Make predictions on the test set\n    predictions = np.zeros(X_test.shape[0])\n    for alpha, stump in zip(stump_weights, stump_weights):\n        predictions += alpha * stump(X_test, best_threshold, best_direction)\n\n    # Aggregate the votes\n    predictions = np.sign(predictions)\n    predictions = predictions.tolist()\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# y_train = np.array([1, -1, -1, 1])\n# X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# n_clf = 2\n# predictions = adaboost_predict(X_train, y_train, X_test, n_clf)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: We initialize the weights of the training examples to be equal and set up a list to store the weights of each stump.\n2. **Stump Training**: For each iteration, we try to find the best decision stump by splitting on each feature and threshold, and choosing the direction that minimizes the weighted classification error.\n3. **Stump Weight Calculation**: We calculate the weight of the best stump using the formula $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)$.\n4. **Weight Update**: We update the weights of the training examples based on the predictions of the current stump.\n5. **Prediction**: For each test sample, we aggregate the votes of all stumps and output the sign of the weighted sum as the predicted label.\n\nThis implementation ensures that the AdaBoost algorithm is trained with decision stumps and makes predictions on the test set as required."}
{"task_id": 261, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Glorot Xavier normal weight-initialisation function in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    if len(shape) < 2:\n        raise ValueError(\"Shape must have at least two dimensions for well-defined fan_in and fan_out.\")\n    \n    # Compute fan_in and fan_out\n    if len(shape) == 2:\n        fan_in, fan_out = shape\n    else:\n        fan_in = np.prod(shape[1:])\n        fan_out = np.prod(shape[:-1])\n    \n    # Calculate the standard deviation\n    s = np.sqrt(2 / (fan_in + fan_out))\n    \n    # Generate the array with Glorot Xavier normal initialisation\n    return np.random.normal(loc=0.0, scale=s, size=shape)\n\n# Example usage:\nshape = (3, 4, 5)  # Example shape for a convolutional kernel\ninitialised_array = glorot_normal(shape)\nprint(initialised_array)\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if the shape has at least two dimensions. If not, it raises a `ValueError`.\n2. **Fan In and Fan Out Calculation**:\n   - For a 2D shape `(fan_in, fan_out)`, the values are directly taken from the shape.\n   - For shapes with more than two dimensions, the receptive-field size is calculated as the product of all dimensions after the first two. Then, `fan_in` and `fan_out` are computed accordingly.\n3. **Standard Deviation Calculation**: The standard deviation `s` is calculated using the formula `sqrt(2 / (fan_in + fan_out))`.\n4. **Weight Initialization**: The function uses `np.random.normal` to generate a NumPy array with the specified shape, mean 0, and the calculated standard deviation `s`.\n\nThis implementation ensures that the returned array has the exact requested shape and dtype `float`, with elements drawn from a normal distribution with the specified mean and variance."}
{"task_id": 266, "completion_id": 0, "completion": "To build an adjacency list for an undirected graph, we need to follow these steps:\n\n1. Initialize an empty list `G` with the same length as `V`, where each element is an empty list.\n2. Iterate through the edge list `E` and add each vertex to the adjacency list of the other vertex.\n3. Ensure that each vertex appears only once in the adjacency list of another vertex.\n4. Sort the adjacency lists according to the order of appearance in `V`.\n\nHere's the implementation of the `build_adj_list` function:\n\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # Initialize the adjacency list with empty lists\n    G = [[] for _ in V]\n    \n    # Create a dictionary to map vertices to their indices in V\n    vertex_index = {v: i for i, v in enumerate(V)}\n    \n    # Add edges to the adjacency list\n    for u, v in E:\n        i, j = vertex_index[u], vertex_index[v]\n        if v not in G[i]:\n            G[i].append(v)\n        if u not in G[j]:\n            G[j].append(u)\n    \n    # Sort each adjacency list according to the order of appearance in V\n    for i in range(len(G)):\n        G[i].sort(key=lambda x: vertex_index[x])\n    \n    return G\n\n# Example usage:\nV = [1, 2, 3, 4]\nE = [(1, 2), (2, 3), (3, 4), (4, 1), (2, 4)]\nprint(build_adj_list(V, E))\n# Output: [[2, 4], [1, 3, 4], [2, 4], [1, 3, 2]]\n```\n\n### Explanation:\n1. **Initialization**: We initialize an empty list `G` with the same length as `V`, where each element is an empty list.\n2. **Mapping Vertices to Indices**: We create a dictionary `vertex_index` to map each vertex to its index in `V`. This helps in quickly finding the index of any vertex.\n3. **Adding Edges**: We iterate through the edge list `E` and add each vertex to the adjacency list of the other vertex. We ensure that each vertex appears only once in the adjacency list of another vertex.\n4. **Sorting**: We sort each adjacency list according to the order of appearance in `V` using the `sort` method with a custom key.\n\nThis implementation ensures that the adjacency list is correctly built, with no duplicate neighbors and sorted according to the order of appearance in `V`."}
{"task_id": 267, "completion_id": 0, "completion": "To solve the problem of finding the optimal weighted decision stump, we need to iterate over each feature and each unique value of that feature to determine the best threshold and polarity that minimizes the weighted classification error. Here's a step-by-step approach to achieve this:\n\n1. **Iterate over each feature**: For each feature, find all unique values.\n2. **For each unique value, consider it as a threshold**: Calculate the weighted error for both possible polarities (1 and -1).\n3. **Track the best stump**: Keep track of the stump with the lowest weighted error.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\n\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    n_samples, n_features = X.shape\n    best_stump = {\n        \"feature_index\": -1,\n        \"threshold\": np.inf,\n        \"polarity\": 1,\n        \"weighted_error\": np.inf\n    }\n\n    for feature_index in range(n_features):\n        unique_values = np.unique(X[:, feature_index])\n        for threshold in unique_values:\n            # Calculate the error for polarity 1\n            y_pred_1 = np.where(X[:, feature_index] < threshold, 1, -1)\n            error_1 = np.sum(sample_weights * (y_pred_1 != y))\n            \n            # Calculate the error for polarity -1\n            y_pred_neg1 = np.where(X[:, feature_index] < threshold, -1, 1)\n            error_neg1 = np.sum(sample_weights * (y_pred_neg1 != y))\n            \n            # Determine the best polarity for this threshold\n            if error_1 < error_neg1:\n                polarity = 1\n                error = error_1\n            else:\n                polarity = -1\n                error = error_neg1\n            \n            # Update the best stump if this one is better\n            if error < best_stump[\"weighted_error\"]:\n                best_stump[\"feature_index\"] = feature_index\n                best_stump[\"threshold\"] = round(threshold, 4)\n                best_stump[\"polarity\"] = polarity\n                best_stump[\"weighted_error\"] = round(error, 4)\n\n    return best_stump\n\n# Example usage:\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([-1, -1, 1, 1])\nsample_weights = np.array([0.25, 0.25, 0.25, 0.25])\n\nresult = train_decision_stump(X, y, sample_weights)\nprint(result)\n```\n\n### Explanation:\n1. **Initialization**: We initialize `best_stump` with a dictionary containing the worst possible values.\n2. **Feature Iteration**: We loop through each feature.\n3. **Threshold Iteration**: For each feature, we loop through each unique value and consider it as a threshold.\n4. **Error Calculation**: We calculate the weighted error for both polarities (1 and -1).\n5. **Best Polarity Selection**: We choose the polarity that gives the lowest error.\n6. **Update Best Stump**: If the current stump's error is lower than the best error found so far, we update `best_stump`.\n\nThis approach ensures that we find the optimal weighted decision stump by considering all possible thresholds and polarities for each feature."}
{"task_id": 273, "completion_id": 0, "completion": "To solve this problem, we need to implement the `best_split` function that calculates the information gain for each possible threshold and returns the threshold that maximizes this gain. If there are multiple thresholds with the same maximum gain, we return the smallest one. If no useful split exists (i.e., all feature values are identical or no positive information gain is possible), we return `(None, 0.0)`.\n\nHere's the implementation of the `best_split` function:\n\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels):\n    \"\"\"Calculate the Shannon entropy of a list of labels.\"\"\"\n    label_counts = Counter(labels)\n    total = len(labels)\n    entropy = 0.0\n    for count in label_counts.values():\n        probability = count / total\n        entropy -= probability * math.log2(probability)\n    return entropy\n\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    feature = np.array(feature)\n    target = np.array(target)\n    \n    # Sort the feature values and corresponding target labels\n    sorted_indices = np.argsort(feature)\n    sorted_feature = feature[sorted_indices]\n    sorted_target = target[sorted_indices]\n    \n    # Calculate the entropy of the parent node\n    parent_entropy = entropy(sorted_target)\n    \n    # Initialize variables to store the best split\n    best_threshold = None\n    best_gain = 0.0\n    \n    # Iterate over the sorted feature values to find the best split\n    for i in range(1, len(sorted_feature)):\n        # Calculate the threshold as the midpoint between two consecutive values\n        threshold = (sorted_feature[i] + sorted_feature[i - 1]) / 2.0\n        \n        # Split the data into left and right subsets\n        left_indices = sorted_indices[:i]\n        right_indices = sorted_indices[i:]\n        left_target = sorted_target[left_indices]\n        right_target = sorted_target[right_indices]\n        \n        # Calculate the weights of the left and right subsets\n        left_weight = len(left_target) / len(sorted_target)\n        right_weight = len(right_target) / len(sorted_target)\n        \n        # Calculate the entropy of the left and right subsets\n        left_entropy = entropy(left_target)\n        right_entropy = entropy(right_target)\n        \n        # Calculate the information gain\n        gain = parent_entropy - (left_weight * left_entropy + right_weight * right_entropy)\n        \n        # Update the best split if the current gain is higher or equal and the threshold is smaller\n        if gain > best_gain or (gain == best_gain and threshold < best_threshold):\n            best_threshold = threshold\n            best_gain = gain\n    \n    # Round the results to 4 decimal places\n    best_threshold = round(best_threshold, 4) if best_threshold is not None else None\n    best_gain = round(best_gain, 4)\n    \n    return (best_threshold, best_gain)\n\n# Example usage:\nfeature = [1.0, 2.0, 3.0, 4.0, 5.0]\ntarget = [0, 0, 1, 1, 1]\nprint(best_split(feature, target))  # Output should be (3.0, 0.9709)\n```\n\n### Explanation:\n1. **Entropy Calculation**: The `entropy` function calculates the Shannon entropy of a list of labels.\n2. **Sorting**: The feature values and corresponding target labels are sorted to facilitate finding candidate thresholds.\n3. **Parent Entropy**: The entropy of the entire dataset is calculated.\n4. **Iterate Over Thresholds**: For each pair of consecutive sorted values, a threshold is calculated as the midpoint.\n5. **Split and Calculate Gain**: The dataset is split into left and right subsets based on the threshold, and the information gain is calculated.\n6. **Update Best Split**: The best split is updated if the current gain is higher or equal and the threshold is smaller.\n7. **Rounding**: The results are rounded to 4 decimal places before being returned.\n\nThis implementation ensures that the function correctly identifies the optimal threshold that maximizes information gain, adhering to the constraints and requirements provided."}
{"task_id": 286, "completion_id": 0, "completion": "Here is the implementation of the `get_initializer` function that meets the requirements specified in the task:\n\n```python\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    return [[0.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    return [[1.0 for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    random.seed(seed)\n    return [[random.uniform(low, high) for _ in range(shape[1])] for _ in range(shape[0])]\n\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    if name in globals() and callable(globals()[name]):\n        return globals()[name]\n    else:\n        raise ValueError(\"Invalid initialization function.\")\n\n# Example usage:\nname = \"ones_init\"\nshape = (2, 2)\ninit_fn = get_initializer(name)      # returns the function ones_init\noutput = init_fn(shape)             # [[1.0, 1.0], [1.0, 1.0]]\nprint(output)  # Output: [[1.0, 1.0], [1.0, 1.0]]\n```\n\n### Explanation:\n1. **Function Definitions**:\n   - `zeros_init`: Returns a matrix filled with zeros.\n   - `ones_init`: Returns a matrix filled with ones.\n   - `random_uniform_init`: Returns a matrix with elements drawn uniformly from the interval `[low, high]`.\n\n2. **get_initializer Function**:\n   - The function checks if the provided `name` exists in the global namespace and if it is callable.\n   - If both conditions are met, it returns the callable initializer.\n   - If not, it raises a `ValueError` with the message \"Invalid initialization function.\"\n\n### Example Usage:\n- The example demonstrates how to use the `get_initializer` function to retrieve the `ones_init` function and then call it with a shape of `(2, 2)`, resulting in a matrix of ones.\n\nThis implementation ensures that the `get_initializer` function correctly retrieves and returns the appropriate initializer function based on the provided name, and raises an error if the name does not correspond to a valid initializer."}
{"task_id": 287, "completion_id": 0, "completion": "To implement the 2-D average pooling forward and backward passes, we need to follow these steps:\n\n1. **Forward Pass**:\n   - Slide a fixed-size window over the input feature map.\n   - Compute the average of the elements within each window.\n   - Store the result in the pooled output.\n\n2. **Backward Pass**:\n   - Propagate the gradient back to the input.\n   - Since the gradient is equally distributed to every element that took part in each average, we need to divide the gradient by the number of elements in each window.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    N, C, H, W = X.shape\n    p_h, p_w = pool_shape\n    s_h, s_w = stride\n    \n    # Compute the output dimensions\n    out_h = (H - p_h) // s_h + 1\n    out_w = (W - p_w) // s_w + 1\n    \n    # Initialize the pooled output and gradient input\n    pooled_output = np.zeros((N, C, out_h, out_w), dtype=X.dtype)\n    grad_input = np.zeros_like(X, dtype=X.dtype)\n    \n    # Forward pass\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    start_h = i * s_h\n                    start_w = j * s_w\n                    end_h = start_h + p_h\n                    end_w = start_w + p_w\n                    pooled_output[n, c, i, j] = np.mean(X[n, c, start_h:end_h, start_w:end_w])\n    \n    # Backward pass\n    for n in range(N):\n        for c in range(C):\n            for i in range(out_h):\n                for j in range(out_w):\n                    start_h = i * s_h\n                    start_w = j * s_w\n                    end_h = start_h + p_h\n                    end_w = start_w + p_w\n                    pool_size = p_h * p_w\n                    grad_input[n, c, start_h:end_h, start_w:end_w] += accum_grad[n, c, i, j] / pool_size\n    \n    # Round to the nearest 4-th decimal and convert to lists\n    pooled_output = np.round(pooled_output, 4).tolist()\n    grad_input = np.round(grad_input, 4).tolist()\n    \n    return pooled_output, grad_input\n```\n\n### Explanation:\n\n1. **Forward Pass**:\n   - We iterate over each element in the output tensor.\n   - For each element, we compute the corresponding window in the input tensor.\n   - We calculate the mean of the elements in the window and store it in the output tensor.\n\n2. **Backward Pass**:\n   - We iterate over each element in the output tensor.\n   - For each element, we compute the corresponding window in the input tensor.\n   - We distribute the gradient equally to each element in the window by dividing the gradient by the number of elements in the window.\n\n3. **Rounding and Conversion**:\n   - We round the results to the nearest 4-th decimal.\n   - We convert the NumPy arrays to Python lists using `tolist()`.\n\nThis implementation ensures that the forward and backward passes of the 2-D average pooling layer are correctly computed and the results are rounded and converted as required."}
{"task_id": 290, "completion_id": 0, "completion": "To compare two decision trees recursively, we need to ensure that both trees have the same structure, the same feature indices, the same thresholds, and the same leaf values. We can achieve this by defining a recursive function that checks each node and leaf in both trees.\n\nHere's the implementation of the `compare_trees` function:\n\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # Check if both nodes are leaves\n    if isinstance(tree_a, Leaf) and isinstance(tree_b, Leaf):\n        return np.allclose(tree_a.value, tree_b.value)\n    \n    # Check if both nodes are internal nodes\n    if isinstance(tree_a, Node) and isinstance(tree_b, Node):\n        # Check if feature indices and thresholds are the same\n        if tree_a.feature != tree_b.feature or not np.isclose(tree_a.threshold, tree_b.threshold, atol=1e-8):\n            return False\n        \n        # Recursively compare the left and right subtrees\n        return compare_trees(tree_a.left, tree_b.left) and compare_trees(tree_a.right, tree_b.right)\n    \n    # If one is a leaf and the other is a node, they are not equivalent\n    return False\n\n# Example usage:\n# tree_a = Node(feature=0, threshold=0.5, left=Leaf(value=1), right=Leaf(value=2))\n# tree_b = Node(feature=0, threshold=0.5, left=Leaf(value=1), right=Leaf(value=2))\n# print(compare_trees(tree_a, tree_b))  # Output: True\n```\n\n### Explanation:\n1. **Leaf Comparison**: If both nodes are leaves, we use `np.allclose` to check if their values are equal within the specified tolerance.\n2. **Node Comparison**: If both nodes are internal nodes, we first check if their feature indices and thresholds are the same within the tolerance. If they are, we recursively compare their left and right subtrees.\n3. **Mismatch Handling**: If one node is a leaf and the other is a node, they are not equivalent, so we return `False`.\n\nThis recursive approach ensures that we check every node and leaf in both trees to determine if they are equivalent."}
{"task_id": 292, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `single_point_crossover` function according to the provided specifications:\n\n```python\nimport numpy as np\n\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    # Convert input lists to NumPy arrays for easier manipulation\n    parent1_np = np.array(parent1)\n    parent2_np = np.array(parent2)\n    \n    # Validate that the two parent matrices have the same shape\n    if parent1_np.shape != parent2_np.shape:\n        return -1\n    \n    # Get the number of columns in the matrices\n    num_columns = parent1_np.shape[1]\n    \n    # Ensure the cutoff is within the valid range\n    if cutoff < 0 or cutoff > num_columns:\n        return -1\n    \n    # Perform the single-point column crossover\n    child1_np = np.copy(parent1_np)\n    child2_np = np.copy(parent2_np)\n    \n    # Swap columns from the cutoff point to the end\n    child1_np[:, cutoff:] = parent2_np[:, cutoff:]\n    child2_np[:, cutoff:] = parent1_np[:, cutoff:]\n    \n    # Convert the children back to ordinary Python lists\n    child1 = child1_np.tolist()\n    child2 = child2_np.tolist()\n    \n    return child1, child2\n\n# Example usage:\nparent1 = [[1, 2, 3], [4, 5, 6]]\nparent2 = [[7, 8, 9], [10, 11, 12]]\ncutoff = 1\n\nchildren = single_point_crossover(parent1, parent2, cutoff)\nprint(children)  # Output: ([[1, 8, 9], [4, 11, 12]], [[7, 2, 3], [10, 5, 6]])\n```\n\n### Explanation:\n1. **Input Validation**: The function first converts the input lists to NumPy arrays for easier manipulation. It then checks if the shapes of the two parent matrices are the same. If not, it returns `-1`.\n2. **Cutoff Validation**: The function ensures that the `cutoff` is within the valid range (0 to the number of columns).\n3. **Crossover Operation**: The function creates copies of the parent matrices and swaps the columns from the `cutoff` point to the end between the two copies.\n4. **Conversion to Lists**: Finally, the function converts the resulting child matrices back to ordinary Python lists and returns them as a tuple.\n\nThis implementation ensures that the crossover operation is performed correctly and efficiently."}
{"task_id": 294, "completion_id": 0, "completion": "To convert the custom graph dictionary `G` into a plain adjacency dictionary `adj`, we need to follow these steps:\n\n1. Initialize an empty dictionary `adj` to store the adjacency list.\n2. Iterate through the vertices and their corresponding edges.\n3. For each edge, add it to the adjacency list of the source vertex.\n4. If the graph is undirected, also add the reverse edge to the adjacency list of the target vertex.\n5. Ensure that the neighbour lists are sorted alphabetically by the neighbour label.\n6. Handle vertices without incident edges by adding them to the adjacency dictionary with an empty list.\n\nHere's the implementation of the `to_networkx` function:\n\n```python\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    adj = {}\n    \n    # Iterate through the vertices and their corresponding edges\n    for source_label, edges in zip(G['_V2I'].keys(), G['_G']):\n        # Add the source vertex to the adjacency dictionary if it doesn't exist\n        if source_label not in adj:\n            adj[source_label] = []\n        \n        # Add the edges to the adjacency list of the source vertex\n        for target_label, weight in edges:\n            adj[source_label].append((target_label, weight))\n        \n        # If the graph is undirected, add the reverse edge\n        if not G['is_directed']:\n            if target_label not in adj:\n                adj[target_label] = []\n            adj[target_label].append((source_label, weight))\n    \n    # Sort the neighbour lists alphabetically by the neighbour label\n    for vertex in adj:\n        adj[vertex].sort(key=lambda x: x[0])\n    \n    return adj\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing an empty dictionary `adj` to store the adjacency list.\n2. **Iterating through vertices and edges**: We use `zip(G['_V2I'].keys(), G['_G'])` to iterate through the vertices and their corresponding edges.\n3. **Adding edges to the adjacency list**: For each edge `(target_label, weight)`, we add it to the adjacency list of the source vertex `source_label`.\n4. **Handling undirected graphs**: If the graph is undirected, we also add the reverse edge `(source_label, weight)` to the adjacency list of the target vertex `target_label`.\n5. **Sorting neighbour lists**: After processing all edges, we sort each neighbour list alphabetically by the neighbour label using `sort(key=lambda x: x[0])`.\n6. **Returning the adjacency dictionary**: Finally, we return the adjacency dictionary `adj`.\n\nThis implementation ensures that the adjacency dictionary is correctly formatted and sorted as required."}
{"task_id": 296, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Convert the input to a NumPy array if it's not already\n    x = np.array(x)\n    # Check if all elements are either 0 or 1\n    return np.all((x == 0) | (x == 1))\n```\n\nThis solution uses NumPy's `all` function combined with a boolean mask to check if all elements in the array are either `0` or `1`. The expression `(x == 0) | (x == 1)` creates a boolean array where each element is `True` if the corresponding element in `x` is `0` or `1`, and `False` otherwise. The `np.all` function then checks if all elements in this boolean array are `True`, which would mean that all elements in the original array are binary."}
{"task_id": 298, "completion_id": 0, "completion": "To implement the `ngram_log_prob` function, we need to follow these steps:\n\n1. Split each sentence in the corpus into tokens.\n2. Pad the query sentence with special tokens if `N > 1`.\n3. Count the occurrences of N-grams and their (N-1)-gram prefixes in the corpus.\n4. Compute the MLE conditional probabilities for each N-gram in the padded query sentence.\n5. Sum the natural logarithms of these probabilities.\n6. Return the result rounded to 4 decimal places or `float('-inf')` if any N-gram is unseen.\n\nHere's the complete implementation:\n\n```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    # Split each sentence in the corpus into tokens\n    tokens = [sentence.split() for sentence in corpus]\n    \n    # Pad the query sentence with special tokens if N > 1\n    if N > 1:\n        query = ['<bol>'] * (N - 1) + query.split() + ['<eol>']\n    else:\n        query = query.split()\n    \n    # Count the occurrences of N-grams and their (N-1)-gram prefixes\n    ngram_counts = defaultdict(int)\n    prefix_counts = defaultdict(int)\n    \n    for sentence in tokens:\n        if N > 1:\n            sentence = ['<bol>'] * (N - 1) + sentence + ['<eol>']\n        \n        for i in range(len(sentence) - N + 1):\n            prefix = tuple(sentence[i:i + N - 1])\n            ngram = tuple(sentence[i:i + N])\n            prefix_counts[prefix] += 1\n            ngram_counts[ngram] += 1\n    \n    # Compute the MLE conditional probabilities and sum the log-probabilities\n    log_prob = 0.0\n    for i in range(len(query) - N + 1):\n        prefix = tuple(query[i:i + N - 1])\n        ngram = tuple(query[i:i + N])\n        \n        if prefix_counts[prefix] == 0:\n            return float('-inf')\n        \n        prob = ngram_counts[ngram] / prefix_counts[prefix]\n        log_prob += math.log(prob)\n    \n    return round(log_prob, 4)\n\n\n# Example usage:\ncorpus = [\"a b a\", \"b a b\"]\nquery = \"a b\"\nN = 1\nprint(ngram_log_prob(corpus, query, N))  # Output: -1.3863\n```\n\n### Explanation:\n1. **Tokenization**: Each sentence in the corpus is split into tokens.\n2. **Padding**: If `N > 1`, the query sentence is padded with `<bol>` tokens at the beginning and `<eol>` tokens at the end.\n3. **Counting**: We count the occurrences of N-grams and their (N-1)-gram prefixes in the corpus.\n4. **Probability Calculation**: For each N-gram in the padded query sentence, we compute the MLE probability using the counts. If any prefix count is zero, we return `float('-inf')`.\n5. **Log-Probability Sum**: We sum the natural logarithms of the probabilities and round the result to 4 decimal places.\n\nThis implementation ensures that the function correctly handles the edge cases and computes the log-probability as required."}
{"task_id": 302, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Spectral Clustering algorithm from scratch using only NumPy:\n\n```python\nimport numpy as np\n\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n    n, d = X.shape\n    \n    # Step 1: Build the weighted adjacency matrix W\n    W = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                W[i, j] = 1 / (1 + np.linalg.norm(X[i] - X[j]))\n    \n    # Step 2: Compute the (unnormalised) graph Laplacian L\n    D = np.diag(np.sum(W, axis=1))\n    L = D - W\n    \n    # Step 3: Obtain the first k eigenvectors of L\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    E = eigenvectors[:, :k]\n    \n    # Step 4: Run k-means on the rows of E\n    centroids = E[:k]\n    labels = np.zeros(n, dtype=int)\n    max_iterations = 100\n    for _ in range(max_iterations):\n        new_labels = np.zeros(n, dtype=int)\n        for i in range(n):\n            distances = np.linalg.norm(E[i] - centroids, axis=1)\n            new_labels[i] = np.argmin(distances)\n        \n        if np.array_equal(labels, new_labels):\n            break\n        \n        labels = new_labels\n        \n        # Re-initialize empty clusters\n        for i in range(k):\n            if np.sum(labels == i) == 0:\n                empty_indices = np.where(labels == -1)[0]\n                if len(empty_indices) > 0:\n                    centroids[i] = E[empty_indices[0]]\n    \n    # Step 5: Map the labels to the original data points\n    final_labels = np.zeros(n, dtype=int)\n    for i in range(k):\n        cluster_indices = np.where(labels == i)[0]\n        cluster_points = X[cluster_indices]\n        centroid = np.mean(cluster_points, axis=0)\n        for j in cluster_indices:\n            final_labels[j] = np.argmin(np.linalg.norm(X[j] - centroids, axis=1))\n    \n    return final_labels.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n# k = 2\n# labels = spectral_clustering(X, k)\n# print(labels)\n```\n\n### Explanation:\n1. **Weighted Adjacency Matrix (W)**: The weight between two points is calculated using the formula \\( w_{ij} = \\frac{1}{1 + \\|x_i - x_j\\|_2} \\). The diagonal of \\( W \\) is set to 0.\n2. **Graph Laplacian (L)**: The Laplacian matrix \\( L \\) is computed as \\( L = D - W \\), where \\( D \\) is the diagonal degree matrix.\n3. **Eigenvectors**: The first \\( k \\) eigenvectors of \\( L \\) are obtained using `np.linalg.eigh`, which is efficient for symmetric matrices.\n4. **K-means Clustering**: K-means is run on the rows of \\( E \\). The centroids are initialized with the first \\( k \\) rows of \\( E \\). The algorithm iterates until the assignments stop changing or a maximum of 100 iterations is reached. If a cluster becomes empty, it is re-initialized with a random row of \\( E \\).\n5. **Final Labels**: The final labels are determined by mapping the labels obtained from k-means to the original data points.\n\nThis implementation ensures that all intermediate steps are manually implemented using NumPy, as required."}
{"task_id": 303, "completion_id": 0, "completion": "```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    # Determine the previous and current elements\n    prev_ix = max(ix - 1, 0)\n    prev_label, prev_mine = params[prev_ix]\n    curr_label, curr_mine = params[ix]\n\n    # Format the output string\n    output = f\"------------------------- DEBUG -------------------------\\n\"\n    output += f\"Mine (prev) [{prev_label}]:\\n{prev_mine}\\n\"\n    output += f\"Theirs (prev) [{prev_label}]:\\n{golds[prev_label]}\\n\"\n    output += f\"Mine [{curr_label}]:\\n{curr_mine}\\n\"\n    output += f\"Theirs [{curr_label}]:\\n{golds[curr_label]}{warn_str}\\n\"\n    output += f\"----------------------- END DEBUG -----------------------\\n\"\n\n    return output\n```\n\nThis function `err_fmt` takes the provided parameters and formats them into a detailed debugging string as specified. It handles the edge case where `ix` is 0 by ensuring the previous element is the first element itself. The function then constructs the string with the exact layout and returns it."}
{"task_id": 304, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    if fit_intercept:\n        X = np.c_[np.ones(X.shape[0]), X]\n    \n    if V is None:\n        V = np.eye(X.shape[1])\n    elif isinstance(V, (int, float)):\n        V = V * np.eye(X.shape[1])\n    elif isinstance(V, (list, tuple)):\n        V = np.diag(V)\n    \n    V_inv = np.linalg.inv(V)\n    X_transpose = X.T\n    X_transpose_X = X_transpose @ X\n    sigma_b_inv = V_inv + X_transpose_X\n    sigma_b = np.linalg.inv(sigma_b_inv)\n    mu_b = sigma_b @ (V_inv @ mu + X_transpose @ y)\n    \n    return [round(coeff, 4) for coeff in mu_b]\n```\n\nThis function first checks if an intercept should be added to the design matrix `X`. It then processes the prior scale `V` to ensure it is in the correct format (identity matrix, scalar multiple of the identity, or diagonal matrix). The posterior mean of the coefficients `mu_b` is computed using the closed-form formula provided in the task description. Finally, the function returns the posterior mean coefficients rounded to 4 decimal places."}
{"task_id": 308, "completion_id": 0, "completion": "To implement the `mel_filterbank` function, we need to follow the steps outlined in the task description. Here's a step-by-step breakdown of the implementation:\n\n1. **Convert limits expressed in Hertz to the Mel scale and generate `n_filters+2` equally-spaced values on the Mel axis.**\n2. **Convert those Mel values back to Hertz \u2013 these are the (n_filters+2) corner frequencies of the triangular filters.**\n3. **For every DFT bin `k` (whose centre frequency is `k*fs/N`) and every Mel filter `i` compute the left-hand and right-hand slopes of the triangle and keep the positive minimum of both \u2013 this is the weight for filter `i` and bin `k`.**\n4. **If `normalize` is true scale every filter by \\( w_i \\leftarrow \\frac{2}{f_{i+2} - f_{i}} w_i \\) so that its area in Mel space equals 1.**\n5. **Return the complete filterbank as a plain Python list whose shape is `(n_filters, N//2+1)`.**\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    if max_freq is None:\n        max_freq = fs / 2\n\n    # Convert min_freq and max_freq to Mel scale\n    min_mel = 2595 * np.log10(1 + min_freq / 700)\n    max_mel = 2595 * np.log10(1 + max_freq / 700)\n\n    # Generate n_filters + 2 equally spaced Mel values\n    mel_points = np.linspace(min_mel, max_mel, n_filters + 2)\n\n    # Convert Mel values back to Hertz\n    freq_points = 700 * (10 ** (mel_points / 2595))\n\n    # Initialize the filterbank matrix\n    filterbank = np.zeros((n_filters, N // 2 + 1))\n\n    # Compute the filterbank weights\n    for i in range(n_filters):\n        # Frequency range for the current filter\n        f_left = freq_points[i]\n        f_center = freq_points[i + 1]\n        f_right = freq_points[i + 2]\n\n        # Compute the bin indices for the current filter\n        k_left = int(np.floor(f_left * N / fs))\n        k_center = int(np.floor(f_center * N / fs))\n        k_right = int(np.floor(f_right * N / fs))\n\n        # Compute the weights for the current filter\n        for k in range(k_left, k_right + 1):\n            if k < k_center:\n                # Left-hand slope\n                weight = (k - k_left) / (k_center - k_left)\n            else:\n                # Right-hand slope\n                weight = (k_right - k) / (k_right - k_center)\n\n            # Ensure the weight is positive\n            weight = max(weight, 0)\n\n            # Assign the weight to the filterbank matrix\n            filterbank[i, k] = weight\n\n    # Normalize the filterbank if required\n    if normalize:\n        for i in range(n_filters):\n            # Compute the area of the filter in Mel space\n            mel_width = mel_points[i + 2] - mel_points[i]\n            filterbank[i, :] *= 2 / mel_width\n\n    # Round the filterbank to 4 decimal places\n    filterbank = np.round(filterbank, 4).tolist()\n\n    return filterbank\n\n# Example usage:\nN = 1024\nn_filters = 20\nfs = 44100\nmin_freq = 0\nmax_freq = None\nnormalize = True\n\nfilterbank = mel_filterbank(N, n_filters, fs, min_freq, max_freq, normalize)\nprint(filterbank)\n```\n\nThis implementation follows the steps outlined in the task description and ensures that the filterbank matrix is correctly constructed and normalized if required. The output is a plain Python list with the specified shape and rounded to 4 decimal places."}
{"task_id": 312, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    elif window_len == 1:\n        return [1.0]\n    \n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    \n    if symmetric:\n        N = window_len\n    else:\n        N = window_len - 1\n    \n    n = np.arange(N)\n    window = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    \n    if not symmetric:\n        window = np.append(window, window[0])  # Append the first element to make it periodic\n    \n    return np.round(window, 4).tolist()\n```\n\nThis function generates a Blackman\u2013Harris window based on the given parameters. It handles both symmetric and periodic forms and ensures that the output is rounded to four decimal places. Special cases for window lengths less than or equal to 0 and exactly 1 are also handled appropriately."}
{"task_id": 313, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    # Convert gamma to a NumPy array for easier manipulation\n    gamma = np.array(gamma)\n    \n    # Extract the row corresponding to the d-th sample\n    gamma_d = gamma[d, :]\n    \n    # Compute the sum of the d-th row\n    sum_gamma_d = np.sum(gamma_d)\n    \n    # Compute the digamma function for gamma[d,t]\n    def digamma(x):\n        if x < 6:\n            return digamma(x + 1) - 1 / x\n        else:\n            return math.log(x) - 1 / (2 * x) - 1 / (12 * x**2) + 1 / (120 * x**4) - 1 / (252 * x**6)\n    \n    psi_gamma_dt = digamma(gamma_d[t])\n    psi_sum_gamma_d = digamma(sum_gamma_d)\n    \n    # Compute the expected value\n    expected_value = psi_gamma_dt - psi_sum_gamma_d\n    \n    # Round the result to 4 decimal places\n    return round(expected_value, 4)\n```\n\nThis function first converts the input `gamma` to a NumPy array for easier manipulation. It then extracts the row corresponding to the `d`-th sample and computes the sum of this row. The `digamma` function is implemented using the provided recursion and asymptotic expansion. Finally, the expected value is computed and rounded to four decimal places before being returned."}
{"task_id": 317, "completion_id": 0, "completion": "```python\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # Get the current item\n    curr_prediction, curr_label = params[ix]\n    curr_gold = golds[curr_label]\n    \n    # Get the previous item if it exists\n    prev_prediction, prev_label = params[max(ix - 1, 0)]\n    prev_gold = golds[prev_label]\n    \n    # Format the output string\n    output = f\"------------------------- DEBUG -------------------------\\n\"\n    output += f\"Mine (prev) [{prev_label}]:\\n{prev_prediction}\\n\\n\"\n    output += f\"Theirs (prev) [{prev_label}]:\\n{prev_gold}\\n\\n\"\n    output += f\"Mine [{curr_label}]:\\n{curr_prediction}\\n\\n\"\n    output += f\"Theirs [{curr_label}]:\\n{curr_gold}{warn_str}\\n\"\n    output += f\"----------------------- END DEBUG -----------------------\\n\"\n    \n    return output\n```\n\nThis function `err_fmt` takes the provided inputs and constructs the formatted debug-error report as specified. It handles the case where `ix` is 0 by ensuring that the previous item is still accessible without causing an index error. The function then formats the report with the required structure, including the header, previous and current items, and the footer with the optional warning string."}
{"task_id": 318, "completion_id": 0, "completion": "To implement the AdaBoost.M1 algorithm from scratch using decision stumps as weak learners, we need to follow these steps:\n\n1. Initialize the weights for each training sample.\n2. For each boosting round:\n   - Train a decision stump on the weighted data.\n   - Calculate the weighted classification error of the stump.\n   - Compute the weight of the stump.\n   - Update the weights of the training samples.\n3. Combine the predictions of all stumps to form the final classifier.\n\nHere's the complete implementation of the `adaboost_predict` function:\n\n```python\nimport math\nimport numpy as np\n\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    \n    # Initialize weights\n    weights = np.ones(n_samples) / n_samples\n    \n    # Initialize the list to store the stumps and their weights\n    stumps = []\n    stump_weights = []\n    \n    for _ in range(n_estimators):\n        # Find the best decision stump\n        best_stump = None\n        best_error = float('inf')\n        best_j, best_t, best_p = None, None, None\n        \n        for j in range(n_features):\n            for polarity in [-1, 1]:\n                # Try different thresholds for the current feature and polarity\n                for t in np.unique(X_train[:, j]):\n                    # Calculate the predictions for the current stump\n                    predictions = polarity * np.sign(X_train[:, j] - t)\n                    # Calculate the weighted error\n                    error = np.sum(weights * (predictions != y_train))\n                    # Update the best stump if the current one has a lower error\n                    if error < best_error:\n                        best_error = error\n                        best_stump = (j, t, polarity)\n        \n        # Calculate the weight of the best stump\n        alpha = 0.5 * math.log((1 - best_error) / best_error)\n        \n        # Update the weights\n        predictions = adaboost_stump_predict(X_train, best_stump)\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)\n        \n        # Store the best stump and its weight\n        stumps.append(best_stump)\n        stump_weights.append(alpha)\n    \n    # Predict the labels for the test set\n    predictions = np.zeros(X_test.shape[0])\n    for stump, weight in zip(stumps, stump_weights):\n        predictions += weight * adaboost_stump_predict(X_test, stump)\n    \n    # Apply the sign function to get the final predictions\n    final_predictions = np.sign(predictions)\n    \n    # Convert the final predictions to a list of integers\n    return final_predictions.tolist()\n\ndef adaboost_stump_predict(X, stump):\n    \"\"\"Predict the labels using a single decision stump.\"\"\"\n    j, t, polarity = stump\n    predictions = polarity * np.sign(X[:, j] - t)\n    return predictions\n```\n\n### Explanation:\n1. **Initialization**:\n   - We initialize the weights for each training sample to be equal, i.e., `1/n_samples`.\n   - We create lists to store the best stumps and their corresponding weights.\n\n2. **Training Loop**:\n   - For each boosting round, we find the best decision stump by trying all possible features, thresholds, and polarities.\n   - We calculate the weighted classification error for each stump and keep track of the best one.\n   - We compute the weight of the best stump using the formula `alpha = 0.5 * log((1 - epsilon) / epsilon)`.\n   - We update the weights of the training samples based on the predictions of the best stump.\n   - We store the best stump and its weight.\n\n3. **Prediction**:\n   - For each test sample, we compute the weighted sum of the predictions of all stumps.\n   - We apply the sign function to the weighted sum to get the final prediction.\n   - We convert the final predictions to a list of integers and return them.\n\nThis implementation ensures that the AdaBoost.M1 algorithm is trained using decision stumps and that the predictions for the test set are made using the final boosted classifier."}
{"task_id": 329, "completion_id": 0, "completion": "Here is the implementation of the `dft_bins` function based on the provided requirements:\n\n```python\nimport numpy as np\n\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # Check if N or fs is non-positive\n    if N <= 0 or fs <= 0:\n        return []\n    \n    # Calculate the frequency step\n    df = fs / N\n    \n    # Generate the frequency bins\n    if positive_only:\n        # Return only the non-negative frequencies\n        frequencies = np.arange(N // 2 + 1) * df\n    else:\n        # Return all N bins\n        frequencies = np.concatenate((np.arange(N // 2 + 1) * df, \n                                      np.arange(-N // 2, 0) * df))\n    \n    # Round to 4 decimal places and convert to list\n    return frequencies.round(4).tolist()\n\n# Example usage:\nprint(dft_bins(8, 44000, positive_only=True))  # Output: [0.0, 5500.0, 11000.0, 16500.0, 22000.0]\nprint(dft_bins(8, 44000, positive_only=False)) # Output: [0.0, 5500.0, 11000.0, 16500.0, 22000.0, -22000.0, -16500.0, -11000.0]\nprint(dft_bins(-1, 44000, positive_only=True)) # Output: []\nprint(dft_bins(8, -44000, positive_only=True)) # Output: []\n```\n\n### Explanation:\n1. **Input Validation**: The function first checks if `N` or `fs` is non-positive. If either is non-positive, it returns an empty list.\n2. **Frequency Step Calculation**: The frequency step `df` is calculated as `fs / N`.\n3. **Frequency Bins Generation**:\n   - If `positive_only` is `True`, it generates the non-negative frequency bins using `np.arange(N // 2 + 1) * df`.\n   - If `positive_only` is `False`, it generates all `N` bins by concatenating the non-negative frequencies with the negative frequencies.\n4. **Rounding and Conversion**: The frequencies are rounded to 4 decimal places and converted to a list before being returned.\n\nThis implementation ensures that the function behaves as specified and handles edge cases appropriately."}
{"task_id": 331, "completion_id": 0, "completion": "To compute the out-of-bag mean squared error (OOB-MSE) for a Random Forest regressor, we need to follow these steps:\n\n1. Iterate through each sample and check if it has at least one out-of-bag prediction.\n2. For each sample with at least one OOB prediction, average its OOB predictions.\n3. Compute the squared error between this average and the true value.\n4. Calculate the mean of these squared errors for all samples that have at least one OOB prediction.\n5. If no sample has an OOB prediction, return -1.\n\nHere's the implementation of the `oob_mse` function:\n\n```python\nimport numpy as np\n\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    y_true = np.array(y_true)\n    predictions = np.array(predictions)\n    oob_mask = np.array(oob_mask)\n    \n    n_samples, n_estimators = predictions.shape\n    \n    # Initialize the sum of squared errors and the count of samples with OOB predictions\n    sum_squared_errors = 0\n    count_with_oob = 0\n    \n    for i in range(n_samples):\n        oob_predictions = predictions[i, oob_mask[i]]\n        if len(oob_predictions) > 0:\n            avg_oob_prediction = np.mean(oob_predictions)\n            squared_error = (avg_oob_prediction - y_true[i]) ** 2\n            sum_squared_errors += squared_error\n            count_with_oob += 1\n    \n    if count_with_oob == 0:\n        return -1\n    \n    oob_mse = sum_squared_errors / count_with_oob\n    return round(oob_mse, 4)\n\n# Example usage:\ny_true = [3, -0.5, 2, 7]\npredictions = [[2.5, 3.1, 2.8], [-0.4, -0.6, -0.5], [2.1, 2.2, 2.0], [6.8, 7.2, 7.0]]\noob_mask = [[1, 0, 1], [1, 1, 0], [0, 1, 1], [1, 0, 0]]\n\nprint(oob_mse(y_true, predictions, oob_mask))  # Output should be the OOB-MSE rounded to 4 decimal places\n```\n\n### Explanation:\n1. **Input Conversion**: Convert the input lists to NumPy arrays for easier manipulation.\n2. **Initialization**: Initialize variables to keep track of the sum of squared errors and the count of samples with at least one OOB prediction.\n3. **Loop Through Samples**: For each sample, extract its OOB predictions using the `oob_mask`.\n4. **Check for OOB Predictions**: If the sample has at least one OOB prediction, compute the average of these predictions and the squared error with the true value.\n5. **Update Sum and Count**: Add the squared error to the sum and increment the count.\n6. **Final Calculation**: If no sample has an OOB prediction, return -1. Otherwise, compute the OOB-MSE by dividing the sum of squared errors by the count and round it to 4 decimal places.\n\nThis function should correctly compute the out-of-bag mean squared error for a Random Forest regressor based on the provided inputs."}
{"task_id": 332, "completion_id": 0, "completion": "To construct an FP-Tree without using classes, we can follow these steps:\n\n1. Compute the support for each item.\n2. Filter out items with support less than the minimum support threshold.\n3. Create a global ordering of the remaining items based on decreasing support and then alphabetically.\n4. Build the FP-Tree by iterating through each transaction, reordering items according to the global ordering, and updating the tree.\n\nHere's the implementation of the `build_fp_tree` function:\n\n```python\nfrom collections import Counter, defaultdict\n\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    # Step 1: Compute the support for each item\n    item_counts = Counter(item for transaction in transactions for item in transaction)\n    \n    # Step 2: Filter out items with support less than min_support\n    frequent_items = {item: count for item, count in item_counts.items() if count >= min_support}\n    \n    # Step 3: Create a global ordering of the remaining items\n    global_order = sorted(frequent_items, key=lambda x: (-item_counts[x], x))\n    \n    # Step 4: Build the FP-Tree\n    root = {\"support\": 0, \"children\": {}}\n    \n    for transaction in transactions:\n        # Filter and reorder the transaction based on the global ordering\n        filtered_transaction = [item for item in transaction if item in frequent_items]\n        filtered_transaction.sort(key=lambda x: global_order.index(x))\n        \n        current_node = root\n        for item in filtered_transaction:\n            if item not in current_node[\"children\"]:\n                current_node[\"children\"][item] = {\"support\": 0, \"children\": {}}\n            current_node = current_node[\"children\"][item]\n            current_node[\"support\"] += 1\n    \n    return root\n\n# Example usage:\ntransactions = [\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\", \"c\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\", \"c\", \"d\"],\n    [\"a\", \"b\", \"c\"],\n    [\"a\", \"b\"],\n    [\"a\","}
{"task_id": 336, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # Convert input to NumPy arrays for easier manipulation\n    X = np.array(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.array(Y)\n    \n    # Check if the number of columns in X and Y are the same\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    # Compute the kernel matrix\n    K = np.dot(X, Y.T) + c0\n    \n    # Round the kernel matrix to 4 decimal places\n    K_rounded = np.round(K, 4)\n    \n    # Convert the result back to a list of lists\n    return K_rounded.tolist()\n```\n\nThis function first converts the input lists to NumPy arrays for easier manipulation. If `Y` is `None`, it sets `Y` to `X`. It then checks if the number of columns in `X` and `Y` are the same. If they are not, it returns `-1`. If they are the same, it computes the kernel matrix using the dot product and adds the constant `c0`. The result is then rounded to 4 decimal places and converted back to a list of lists before being returned."}
{"task_id": 340, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Adam optimisation step in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n\n    # Update the first moment estimate\n    m_t = beta1 * m + (1 - beta1) * grad\n    \n    # Update the second moment estimate\n    v_t = beta2 * v + (1 - beta2) * grad**2\n    \n    # Compute bias-corrected first moment estimate\n    m_hat_t = m_t / (1 - beta1**t)\n    \n    # Compute bias-corrected second moment estimate\n    v_hat_t = v_t / (1 - beta2**t)\n    \n    # Update the parameter\n    param_prime = param - lr * m_hat_t / (np.sqrt(v_hat_t) + eps)\n    \n    # Return the updated parameter and new moment estimates as lists\n    return [round(x, 8) for x in param_prime.flatten().tolist()], \\\n           [round(x, 8) for x in m_t.flatten().tolist()], \\\n           [round(x, 8) for x in v_t.flatten().tolist()]\n\n# Example usage:\nparam = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nt = 1\nm = np.zeros_like(param)\nv = np.zeros_like(param)\n\nupdated_param, new_m, new_v = adam_update(param, grad, t, m, v)\nprint(\"Updated Parameter:\", updated_param)\nprint(\"New Moment (m_t):\", new_m)\nprint(\"New Variance (v_t):\", new_v)\n```\n\n### Explanation:\n1. **First Moment Update (`m_t`)**: This is the running average of the gradients, exponentially decaying the previous estimates and adding the current gradient.\n2. **Second Moment Update (`v_t`)**: This is the running average of the squared gradients, exponentially decaying the previous estimates and adding the squared current gradient.\n3. **Bias Correction**: The bias correction is applied to the first and second moment estimates to account for the initial bias in the estimates.\n4. **Parameter Update (`param_prime`)**: The parameter is updated using the bias-corrected first and second moment estimates, the learning rate, and a small constant (`eps`) to avoid division by zero.\n5. **Return Values**: The updated parameter and the new moment estimates are returned as lists, each rounded to 8 decimal places.\n\nThis implementation ensures that the function works with multi-dimensional parameters and is fully vectorised."}
{"task_id": 343, "completion_id": 0, "completion": "To implement the Cross-Entropy Method (CEM) parameter update, we need to follow these steps:\n\n1. **Sort the samples by their rewards**: This allows us to easily identify the top `retain_prcnt` fraction of samples.\n2. **Select the top `retain_prcnt` fraction of samples**: This is done by taking the bottom `int(retain_prcnt * N)` samples from the sorted list.\n3. **Compute the mean and variance of the selected samples**: This will give us the updated parameters for the next iteration of the CEM algorithm.\n\nHere is the implementation of the `cross_entropy_update` function:\n\n```python\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # Convert rewards to a NumPy array for easier manipulation\n    rewards = np.array(rewards)\n    \n    # Sort the indices of theta_samples based on the corresponding rewards\n    sorted_indices = np.argsort(rewards)\n    \n    # Select the top retain_prcnt fraction of samples\n    num_to_retain = int(retain_prcnt * len(theta_samples))\n    elite_indices = sorted_indices[-num_to_retain:]\n    elite_samples = theta_samples[elite_indices]\n    \n    # Compute the mean and variance of the elite samples\n    mean = np.mean(elite_samples, axis=0).round(4).tolist()\n    variance = np.var(elite_samples, axis=0).round(4).tolist()\n    \n    return mean, variance\n```\n\n### Explanation:\n1. **Sorting**: We use `np.argsort` to get the indices that would sort the `rewards` array. This allows us to easily select the top `retain_prcnt` fraction of samples.\n2. **Selecting Elite Samples**: We take the bottom `int(retain_prcnt * N)` indices from the sorted list to get the elite samples.\n3. **Computing Mean and Variance**: We use `np.mean` and `np.var` to compute the mean and variance of the elite samples along the first axis (i.e., across the samples). We then round the results to four decimal places and convert them to lists.\n\nThis function should work correctly for any valid input as specified in the task description."}
{"task_id": 353, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\nimport numpy as np\n\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    if not labels:\n        return 0.0\n\n    # Count the frequency of each label\n    label_counts = Counter(labels)\n    total_labels = len(labels)\n    \n    # Calculate the probability of each label\n    probabilities = [count / total_labels for count in label_counts.values()]\n    \n    # Calculate the Shannon entropy\n    entropy_value = -sum(p * math.log2(p) for p in probabilities if p > 0)\n    \n    # Round the entropy to 5 decimal places\n    return round(entropy_value, 5)\n```\n\nThis function first checks if the input list is empty and returns 0.0 if it is. It then counts the frequency of each label using `Counter` from the `collections` module. The probabilities of each label are calculated by dividing the count of each label by the total number of labels. The Shannon entropy is computed using the formula provided, and the result is rounded to 5 decimal places before being returned."}
{"task_id": 354, "completion_id": 0, "completion": "To implement the 2-D convolution using the `im2col` strategy, we need to follow these steps:\n\n1. **Padding**: Apply the specified padding to the input images.\n2. **Im2Col**: Convert the padded images into a column matrix.\n3. **GEMM (General Matrix Multiplication)**: Perform matrix multiplication between the column matrix and the kernel matrix.\n4. **Reshape**: Reshape the result back to the desired output shape.\n\nHere's the complete implementation of the `conv2D` function:\n\n```python\nimport numpy as np\n\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    n_ex, in_rows, in_cols, in_ch = X.shape\n    kernel_rows, kernel_cols, _, out_ch = W.shape\n    \n    # Handle dilation\n    if dilation > 0:\n        X = np.pad(X, ((0, 0), (dilation, dilation), (dilation, dilation), (0, 0)), mode='constant')\n        in_rows += 2 * dilation\n        in_cols += 2 * dilation\n    \n    # Handle padding\n    if isinstance(pad, int):\n        pad = (pad, pad, pad, pad)\n    elif isinstance(pad, tuple):\n        if len(pad) == 2:\n            pad = (pad[0], pad[0], pad[1], pad[1])\n        elif len(pad) == 4:\n            pass\n        elif pad == 'same':\n            pad = ((kernel_rows - 1) // 2, kernel_rows // 2, (kernel_cols - 1) // 2, kernel_cols // 2)\n        else:\n            raise ValueError(\"Invalid padding specification\")\n    else:\n        raise ValueError(\"Invalid padding specification\")\n    \n    pr1, pr2, pc1, pc2 = pad\n    X = np.pad(X, ((0, 0), (pr1, pr2), (pc1, pc2), (0, 0)), mode='constant')\n    \n    # Compute output dimensions\n    out_rows = (in_rows - kernel_rows + 2 * pr1 + 2 * pr2) // stride + 1\n    out_cols = (in_cols - kernel_cols + 2 * pc1 + 2 * pc2) // stride + 1\n    \n    # Im2Col\n    X_col = np.zeros((n_ex * out_rows * out_cols, kernel_rows * kernel_cols * in_ch))\n    for n in range(n_ex):\n        for i in range(out_rows):\n            for j in range(out_cols):\n                row = i * stride\n                col = j * stride\n                X_col[n * out_rows * out_cols + i * out_cols + j] = X[n, row:row+kernel_rows, col:col+kernel_cols, :].reshape(-1)\n    \n    # GEMM\n    W_col = W.reshape(kernel_rows * kernel_cols * in_ch, out_ch)\n    Y_col = np.dot(X_col, W_col)\n    \n    # Reshape to output shape\n    Y = Y_col.reshape(n_ex, out_rows, out_cols, out_ch)\n    \n    # Convert to Python list\n    return Y.tolist()\n\n# Example usage:\nX = np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]])\nW = np.array([[[[1, 0], [0, 1]]], [[[0, 1], [1, 0]]]])\nstride = 1\npad = 'same'\ndilation = 0\n\nresult = conv2D(X, W, stride, pad, dilation)\nprint(result)\n```\n\nThis code defines the `conv2D` function, which performs 2-D convolution using the `im2col` strategy. It handles padding, dilation, and computes the output dimensions accordingly. The result is then reshaped and converted to a Python list."}
{"task_id": 355, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    centered_data = data - mean\n\n    # Step 2: Compute the covariance matrix\n    if solver == \"svd\":\n        # Use SVD to compute the principal directions\n        U, S, Vt = np.linalg.svd(centered_data, full_matrices=False)\n        directions = Vt.T\n    elif solver == \"eigen\":\n        # Use eigen-decomposition to compute the principal directions\n        cov_matrix = np.cov(centered_data, rowvar=False, ddof=1)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        directions = eigenvectors.T\n    else:\n        raise ValueError(\"Invalid solver. Choose 'svd' or 'eigen'.\")\n\n    # Step 3: Sort directions by variance explained and keep the first n_components\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    directions = directions[sorted_indices]\n    eigenvalues = eigenvalues[sorted_indices]\n    directions = directions[:n_components]\n    eigenvalues = eigenvalues[:n_components]\n\n    # Step 4: Make the sign of every kept direction deterministic\n    for i in range(n_components):\n        if directions[i, 0] < 0:\n            directions[i, :] *= -1\n\n    # Step 5: Project the centred data onto the retained directions\n    scores = np.dot(centered_data, directions.T)\n    scores = np.round(scores, 4).tolist()\n\n    # Step 6: Calculate the explained variance ratio\n    total_variance = np.sum(eigenvalues)\n    explained_variance_ratio = np.round(eigenvalues / total_variance, 4).tolist()\n\n    return scores, explained_variance_ratio\n```\n\nThis function performs PCA on the given data using either SVD or eigen-decomposition, standardizes the data, and ensures the sign of the principal components is deterministic. It then projects the data onto the selected principal components and returns the scores and the explained variance ratio."}
{"task_id": 356, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `leaf_predict` function that extracts the correct prediction from a `Leaf` instance based on whether the tree is a classification tree or a regression tree.\n\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    if classifier:\n        # For classification, return the index of the maximum probability\n        return np.argmax(leaf.value)\n    else:\n        # For regression, return the scalar value\n        return leaf.value\n```\n\n### Explanation:\n1. **Classification Tree**:\n   - When `classifier` is `True`, the function uses `np.argmax(leaf.value)` to find the index of the maximum probability in the `leaf.value` array. This index represents the predicted class.\n\n2. **Regression Tree**:\n   - When `classifier` is `False`, the function simply returns the scalar value stored in `leaf.value`.\n\n### Usage Example:\n```python\nimport numpy as np\n\n# Example for classification\nclassification_leaf = Leaf([0.1, 0.3, 0.6])\nprint(leaf_predict(classification_leaf, classifier=True))  # Output: 2\n\n# Example for regression\nregression_leaf = Leaf(42.0)\nprint(leaf_predict(regression_leaf, classifier=False))  # Output: 42.0\n```\n\nThis implementation ensures that the function correctly handles both classification and regression trees by returning the appropriate prediction based on the `classifier` flag."}
{"task_id": 357, "completion_id": 0, "completion": "Here is a Python implementation of the K-Means clustering algorithm from scratch:\n```python\nimport numpy as np\n\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    # Initialize centroids with the first K samples\n    centroids = data[:K].copy()\n    \n    for _ in range(max_iters):\n        # Assign each sample to the closest centroid\n        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Recompute centroids as the mean of the samples assigned to them\n        new_centroids = np.array([data[labels == k].mean(axis=0) for k in range(K)])\n        \n        # Check if centroids have changed\n        if np.allclose(centroids, new_centroids, atol=1e-6):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids lexicographically and round to four decimals\n    sorted_centroids = np.sort(centroids, axis=0).round(4).tolist()\n    \n    return sorted_centroids\n```\nThis implementation follows the steps outlined in the task description. It initializes the centroids with the first K samples, then iteratively assigns samples to the closest centroid and recomputes the centroids as the mean of the assigned samples. The process stops early if the centroids do not change significantly, and the final centroids are sorted lexicographically and rounded to four decimals before being returned."}
{"task_id": 362, "completion_id": 0, "completion": "Certainly! Below is the Python function that accomplishes the task as described:\n\n```python\nimport numpy as np\n\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    # Convert the input data to a float NumPy array\n    matrix = np.array(data, dtype=float)\n    \n    # Compute the sum of every row\n    row_sums = np.sum(matrix, axis=1)\n    \n    # Check if any row sum is 0\n    if np.any(row_sums == 0):\n        return -1\n    \n    # Normalize each row by dividing by the row sum\n    normalized_matrix = matrix / row_sums[:, np.newaxis]\n    \n    # Round every entry to 4 decimal places\n    rounded_matrix = np.round(normalized_matrix, 4)\n    \n    # Convert the result back to a Python list of lists\n    return rounded_matrix.tolist()\n\n# Example usage:\ndata = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nresult = row_stochastic_matrix(data)\nprint(result)\n```\n\n### Explanation:\n1. **Conversion to NumPy Array**: The input data is converted to a float NumPy array to ensure numerical operations are performed correctly.\n2. **Row Sums Calculation**: The sum of each row is calculated using `np.sum(matrix, axis=1)`.\n3. **Check for Zero Row Sums**: The function checks if any row sum is zero using `np.any(row_sums == 0)`. If any row sum is zero, the function returns `-1`.\n4. **Normalization**: Each row is normalized by dividing it by its respective row sum. This is done using `matrix / row_sums[:, np.newaxis]` to ensure the division is performed element-wise.\n5. **Rounding**: The normalized matrix is rounded to 4 decimal places using `np.round(normalized_matrix, 4)`.\n6. **Conversion to Python List**: The final normalized and rounded matrix is converted back to a Python list of lists using `tolist()`.\n\nThis function ensures that the input data is correctly transformed into a row-stochastic matrix, adhering to the specified constraints."}
{"task_id": 363, "completion_id": 0, "completion": "To solve this task, we need to implement the `is_continuous` function that checks whether the action and observation spaces of a given environment are continuous. The function should consider the special cases where the action or observation space is a `Tuple` or `Dict`, and in those cases, it should only consider the sub-spaces to determine continuity.\n\nHere's the implementation of the `is_continuous` function:\n\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    def is_continuous_space(space: Space) -> bool:\n        if isinstance(space, Box):\n            return True\n        elif isinstance(space, Tuple):\n            return all(is_continuous_space(sub_space) for sub_space in space.spaces)\n        elif isinstance(space, Dict):\n            return all(is_continuous_space(sub_space) for sub_space in space.spaces.values())\n        else:\n            return False\n\n    cont_action = is_continuous_space(env.action_space) if not tuple_action else all(is_continuous_space(sub_space) for sub_space in env.action_space.spaces)\n    cont_obs = is_continuous_space(env.observation_space) if not tuple_obs else all(is_continuous_space(sub_space) for sub_space in env.observation_space.spaces)\n\n    return (cont_action, cont_obs)\n```\n\n### Explanation:\n1. **Helper Function `is_continuous_space`**:\n   - This function checks if a given space is continuous.\n   - If the space is a `Box`, it returns `True`.\n   - If the space is a `Tuple`, it recursively checks all sub-spaces.\n   - If the space is a `Dict`, it recursively checks all sub-spaces.\n   - For any other type of space, it returns `False`.\n\n2. **Main Function `is_continuous`**:\n   - It uses the helper function to check the continuity of the action and observation spaces.\n   - If `tuple_action` is `False`, it checks the continuity of the action space directly.\n   - If `tuple_action` is `True`, it checks the continuity of each sub-space in the action space.\n   - Similarly, it checks the continuity of the observation space based on the value of `tuple_obs`.\n\nThis implementation ensures that the function correctly determines the continuity of the action and observation spaces according to the given rules."}
{"task_id": 369, "completion_id": 0, "completion": "To implement the `gradient_boosting_predict` function, we need to follow the steps outlined in the problem description. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    # Initialize the running prediction with zeros\n    running_pred = np.zeros_like(updates[0])\n    \n    # Aggregate the updates\n    for u in updates:\n        running_pred -= learning_rate * u\n    \n    # For regression, return the running prediction rounded to 4 decimals\n    if regression:\n        return np.round(running_pred, 4)\n    \n    # For classification, apply softmax and then argmax\n    softmax_pred = np.exp(running_pred) / np.exp(running_pred).sum(axis=1, keepdims=True)\n    predicted_classes = np.argmax(softmax_pred, axis=1)\n    \n    return predicted_classes\n\n# Example usage:\n# For regression\nupdates_regression = [np.array([1.0, 2.0, 3.0]), np.array([0.5, 0.5, 0.5])]\nlearning_rate_regression = 0.1\nregression = True\nprint(gradient_boosting_predict(updates_regression, learning_rate_regression, regression))\n\n# For classification\nupdates_classification = [np.array([[0.2, 0.1, 0.7], [0.6, 0.2, 0.2], [0.1, 0.8, 0.1]]),\n                          np.array([[0.1, 0.9, 0.0], [0.2, 0.3, 0.5], [0.8, 0.1, 0.1]])]\nlearning_rate_classification = 0.1\nregression = False\nprint(gradient_boosting_predict(updates_classification, learning_rate_classification, regression))\n```\n\n### Explanation:\n1. **Initialization**: We start by initializing the `running_pred` array with zeros, which has the same shape as the individual updates.\n2. **Aggregation**: We iterate over each update and subtract it from the `running_pred` after scaling by the `learning_rate`.\n3. **Regression**: If `regression` is `True`, we return the `running_pred` rounded to 4 decimal places.\n4. **Classification**: If `regression` is `False`, we apply the softmax function to the `running_pred` to convert it into probabilities. Then, we use `argmax` to get the predicted class labels.\n\nThis implementation ensures that the function is fully vectorized and relies only on NumPy operations."}
{"task_id": 371, "completion_id": 0, "completion": "To implement the 1-D cross-correlation with the specified features, we need to handle the following aspects:\n\n1. **Batch Support**: The input and kernel arrays should support batches.\n2. **Arbitrary Number of Channels**: Both input and kernel arrays should support multiple input and output channels.\n3. **Strides**: The convolution should slide with a specified stride.\n4. **Padding**: The function should support padding with a single integer, a 2-tuple, or the string \"same\".\n5. **Dilation**: The kernel should have a specified number of zeros inserted between its elements.\n\nHere's the implementation of the `conv1D` function:\n\n```python\nimport numpy as np\n\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    # Extract shapes\n    n_examples, signal_length, in_channels = X.shape\n    kernel_width, _, out_channels = W.shape\n    \n    # Handle dilation\n    effective_kernel = kernel_width + (kernel_width - 1) * dilation\n    \n    # Handle padding\n    if isinstance(pad, int):\n        p_left = p_right = pad\n    elif isinstance(pad, tuple) and len(pad) == 2:\n        p_left, p_right = pad\n    elif pad == \"same\":\n        out_len = np.ceil(signal_length / stride).astype(int)\n        total_pad = max(0, (out_len - 1) * stride + effective_kernel - signal_length)\n        p_left = total_pad // 2\n        p_right = total_pad - p_left\n    else:\n        raise ValueError(\"Invalid padding value\")\n    \n    # Pad the input signals\n    X_padded = np.pad(X, ((0, 0), (p_left, p_right), (0, 0)), mode='constant')\n    \n    # Initialize the output volume\n    output_length = (signal_length + p_left + p_right - effective_kernel) // stride + 1\n    output = np.zeros((n_examples, output_length, out_channels))\n    \n    # Perform the cross-correlation\n    for example in range(n_examples):\n        for out_ch in range(out_channels):\n            for in_ch in range(in_channels):\n                for i in range(output_length):\n                    start = i * stride\n                    end = start + effective_kernel\n                    output[example, i, out_ch] += np.sum(X_padded[example, start:end, in_ch] * W[:, in_ch, out_ch])\n    \n    return output.tolist()\n\n# Example usage:\nX = np.array([[[1, 2, 3, 4, 5]], [[6, 7, 8, 9, 10]]])\nW = np.array([[[1, 1]], [[2, 2]]])\nstride = 1\npad = \"same\"\ndilation = 0\n\nresult = conv1D(X, W, stride, pad, dilation)\nprint(result)\n```\n\n### Explanation:\n1. **Padding**: The input signals are padded according to the specified padding method.\n2. **Dilation**: The effective kernel width is calculated by considering the dilation.\n3. **Output Length**: The length of the output is determined based on the input length, stride, and effective kernel width.\n4. **Cross-Correlation**: The cross-correlation is performed by iterating over each example, output channel, and input channel, and summing the products of the corresponding elements.\n\nThis implementation should handle all the specified requirements and return the result as a nested Python list."}
{"task_id": 373, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    if not y:\n        return 0.0\n    \n    # Count the frequency of each class\n    class_counts = Counter(y)\n    total_samples = len(y)\n    \n    # Calculate the Gini impurity\n    gini_impurity = 1.0 - sum((count / total_samples) ** 2 for count in class_counts.values())\n    \n    return round(gini_impurity, 4)\n```\n\nThis function first checks if the input list `y` is empty and returns `0.0` if it is. It then uses the `Counter` class from the `collections` module to count the frequency of each class in the input list. The total number of samples is calculated, and the Gini impurity is computed using the formula provided. Finally, the result is rounded to 4 decimal places and returned."}
{"task_id": 374, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Your task is to implement this function following the specification\n    provided in the problem statement.\n    \"\"\"\n    # Check if the input data are consistent\n    if X.shape[0] != y.shape[0]:\n        return -1\n\n    # Initialize parameters\n    m, n = X.shape\n    theta = np.zeros(n)\n    bias = 0\n\n    # Gradient descent\n    for _ in range(n_iterations):\n        # Predictions\n        y_pred = np.dot(X, theta) + bias\n\n        # Compute gradients\n        dw = (2 / m) * np.dot(X.T, (y_pred - y))\n        db = (2 / m) * np.sum(y_pred - y)\n\n        # Update parameters\n        theta -= learning_rate * dw\n        bias -= learning_rate * db\n\n    # Return parameters rounded to 4 decimal places\n    return [round(param, 4) for param in theta], round(bias, 4)\n```\n\nThis function initializes the parameters `theta` and `bias` and then iteratively updates them using the batch gradient descent algorithm. The predictions are made using the current parameters, and the gradients are computed with respect to these predictions. The parameters are then updated in the direction that minimizes the mean squared error. The function returns the learned parameters rounded to 4 decimal places. If the input data are inconsistent, the function returns -1."}
{"task_id": 376, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Convert the list of lists to a NumPy array\n    X_np = np.array(X)\n    \n    # Check if the matrix is square\n    if X_np.shape[0] != X_np.shape[1]:\n        return False\n    \n    # Check if the matrix is equal to its transpose\n    return np.allclose(X_np, X_np.T, atol=1e-8)\n```\n\nThis function first converts the input list of lists into a NumPy array. It then checks if the matrix is square by comparing the number of rows and columns. If the matrix is not square, it returns `False`. If the matrix is square, it uses `np.allclose` to compare the matrix with its transpose, allowing for a tolerance of 1 \u00d7 10\u207b\u2078 for floating-point comparisons. If the matrix is equal to its transpose within the specified tolerance, the function returns `True`; otherwise, it returns `False`."}
{"task_id": 377, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"\n    Calculate the negative gradient for logistic loss in binary classification.\n    \n    Parameters:\n    y (list): Binary class labels (0 or 1).\n    f (list): Current prediction scores (floats).\n    \n    Returns:\n    list: List of negative gradients rounded to four decimal places.\n    \"\"\"\n    # Check if y contains only 0 or 1\n    if not all(label in [0, 1] for label in y):\n        return [-1]\n    \n    # Convert labels to the set {-1, 1}\n    y_prime = [2 * label - 1 for label in y]\n    \n    # Calculate the negative gradient\n    g = [y_prime_i / (1 + np.exp(y_prime_i * f_i)) for y_prime_i, f_i in zip(y_prime, f)]\n    \n    # Round to four decimal places\n    g = [round(grad, 4) for grad in g]\n    \n    return g\n```\n\nThis function first checks if the labels in `y` are either 0 or 1. If not, it returns `-1`. It then converts the labels to the set {-1, 1} using the rule `y' = 2y - 1`. After that, it calculates the negative gradient for each sample using the formula `g\u1d62 = y\u2032\u1d62 / (1 + exp(y\u2032\u1d62 f\u1d62))` and rounds each gradient to four decimal places before returning the list of gradients."}
{"task_id": 380, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    # Constants from the original paper\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n\n    # Convert input to NumPy array for element-wise operations\n    x = np.array(x)\n\n    # Define the SELU function and its derivatives\n    def elu(x, alpha):\n        return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n\n    def selu_function(x, alpha, scale):\n        return scale * elu(x, alpha)\n\n    def selu_derivative(x, alpha, scale):\n        return scale * np.where(x > 0, 1, alpha * np.exp(x))\n\n    def selu_second_derivative(x, alpha, scale):\n        return scale * np.where(x > 0, 0, alpha * np.exp(x))\n\n    # Compute the result based on the order\n    if order == 0:\n        result = selu_function(x, alpha, scale)\n    elif order == 1:\n        result = selu_derivative(x, alpha, scale)\n    elif order == 2:\n        result = selu_second_derivative(x, alpha, scale)\n    else:\n        raise ValueError(\"Invalid order. Must be 0, 1, or 2.\")\n\n    # Round to 4 decimal places and convert to list\n    return result.round(4).tolist()\n```\n\nThis implementation defines the SELU function and its first and second derivatives. It uses NumPy for element-wise operations and array handling. The function checks the `order` parameter to determine which computation to perform and returns the result as a list, rounded to 4 decimal places."}
{"task_id": 387, "completion_id": 0, "completion": "Here is a Python implementation of the Gradient Boosting Regressor with One-Dimensional Stumps:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n_samples = len(x)\n    \n    # Initialize predictions with the mean of the targets\n    y_pred = np.full(n_samples, np.mean(y))\n    \n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y - y_pred\n        \n        # Sort the samples by their x value\n        sorted_indices = np.argsort(x)\n        x_sorted = x[sorted_indices]\n        residuals_sorted = residuals[sorted_indices]\n        \n        # Initialize variables to track the best split\n        best_split = None\n        best_error = float('inf')\n        \n        # Evaluate every possible split that lies halfway between two different consecutive x values\n        for i in range(1, n_samples):\n            split_point = (x_sorted[i] + x_sorted[i - 1]) / 2\n            left_residuals = residuals_sorted[:i]\n            right_residuals = residuals_sorted[i:]\n            \n            # Calculate the mean residuals on the left and right side of the split\n            mean_left = np.mean(left_residuals)\n            mean_right = np.mean(right_residuals)\n            \n            # Calculate the sum of squared errors for the current split\n            error = np.sum((left_residuals - mean_left) ** 2) + np.sum((right_residuals - mean_right) ** 2)\n            \n            # Update the best split if the current one has a smaller error\n            if error < best_error:\n                best_error = error\n                best_split = split_point\n        \n        # If all feature values are identical, use the global mean residual for every point\n        if best_split is None:\n            best_split = np.mean(residuals)\n        \n        # Update the running prediction\n        y_pred += learning_rate * np.where(x <= best_split, best_split, best_split)\n    \n    # Round the predictions to 4 decimal places\n    y_pred = np.round(y_pred, 4)\n    \n    return y_pred.tolist()\n\n# Example usage:\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\nn_estimators = 10\nlearning_rate = 0.1\npredictions = gradient_boosting_1d(x, y, n_estimators, learning_rate)\nprint(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The predictions are initialized to the mean of the target values.\n2. **Residuals Calculation**: For each boosting round, the residuals are calculated as the difference between the target values and the current predictions.\n3. **Sorting**: The samples are sorted by their x values to facilitate the evaluation of possible splits.\n4. **Split Evaluation**: For each possible split (halfway between two consecutive x values), the sum of squared residuals is calculated. The split that minimizes this error is chosen.\n5. **Update Prediction**: The running prediction is updated by adding the learning rate times the stump prediction.\n6. **Final Prediction**: The final predictions are rounded to 4 decimal places and returned as a list.\n\nThis implementation ensures that the Gradient Boosting Regressor with One-Dimensional Stumps is trained according to the specified procedure."}
{"task_id": 394, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    epsilon = 1e-8\n    if Eg is None:\n        Eg = np.zeros_like(grad)\n    \n    Eg = rho * Eg + (1 - rho) * grad ** 2\n    w_next = w - (learning_rate * grad) / np.sqrt(Eg + epsilon)\n    \n    w_next = np.round(w_next, 4).tolist()\n    Eg = np.round(Eg, 4).tolist()\n    \n    return w_next, Eg\n```\n\nThis function implements the RMSprop update step as described. It initializes the running average `Eg` with zeros if it is `None`, updates it according to the RMSprop formula, and then updates the parameter vector `w`. Both the updated parameter vector and the new running average are rounded to four decimal places and converted to regular Python lists before being returned."}
{"task_id": 398, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    # Convert input to NumPy array for efficient computation\n    x_np = np.array(x)\n    \n    if derivative:\n        # Compute the gradient of ELU\n        result = np.where(x_np >= 0, 1, elu(x_np, alpha) + alpha)\n    else:\n        # Compute the ELU activation\n        result = np.where(x_np >= 0, x_np, alpha * (np.exp(x_np) - 1))\n    \n    # Round the result to 4 decimal places and convert back to list\n    return [round(val, 4) for val in result.tolist()]\n```\n\nThis function first converts the input list `x` to a NumPy array for efficient computation. It then checks the `derivative` flag to determine whether to compute the ELU activation or its gradient. The `np.where` function is used to apply the ELU formula or its derivative based on the condition `x >= 0`. Finally, the result is rounded to 4 decimal places and returned as a Python list."}
{"task_id": 411, "completion_id": 0, "completion": "To implement the `env_stats` function, we need to analyze the input lists of actions and observations to extract the required statistics. Here's a step-by-step breakdown of how we can achieve this:\n\n1. **Determine if the data are multi-dimensional or not**:\n   - Check if any element in the actions or observations list is an iterable (list, tuple, numpy array).\n\n2. **Determine if the values are discrete or continuous**:\n   - Check if any value in the flattened collection of actions or observations is a non-integer float.\n\n3. **Count the number of unique values in each dimension**:\n   - Flatten the actions and observations lists and count the unique values in each dimension.\n\n4. **Determine the dimensionality of the action and observation spaces**:\n   - Determine the number of dimensions for each action and observation.\n\n5. **Collect the unique values in each dimension**:\n   - Collect the sorted list of unique values for each dimension.\n\nHere's the implementation of the `env_stats` function:\n\n```python\nimport numpy as np\n\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    # Helper function to check if an element is iterable\n    def is_iterable(x):\n        return isinstance(x, (list, tuple, np.ndarray))\n    \n    # Helper function to check if an element is a non-integer float\n    def is_non_integer_float(x):\n        return isinstance(x, float) and not x.is_integer()\n    \n    # Helper function to flatten a list and get unique values\n    def get_unique_values(lst):\n        flattened = [item for sublist in lst for item in sublist]\n        unique_values = sorted(set(flattened))\n        return unique_values\n    \n    # Helper function to count unique values in each dimension\n    def count_unique_values(lst):\n        if not lst:\n            return []\n        if is_iterable(lst[0]):\n            return [len(set(sublist)) for sublist in zip(*lst)]\n        else:\n            return [len(set(lst))]\n    \n    # Determine if the data are multi-dimensional or not\n    tuple_actions = any(is_iterable(action) for action in actions)\n    tuple_observations = any(is_iterable(observation) for observation in observations)\n    \n    # Determine if the values are discrete or continuous\n    continuous_actions = any(is_non_integer_float(action) for action in actions)\n    continuous_observations = any(is_non_integer_float(observation) for observation in observations)\n    \n    # Count the number of unique values in each dimension\n    n_actions_per_dim = count_unique_values(actions)\n    n_obs_per_dim = count_unique_values(observations)\n    \n    # Determine the dimensionality of the action and observation spaces\n    action_dim = len(n_actions_per_dim)\n    obs_dim = len(n_obs_per_dim)\n    \n    # Collect the unique values in each dimension\n    action_ids = [get_unique_values([action[i] for action in actions]) for i in range(action_dim)]\n    obs_ids = [get_unique_values([observation[i] for observation in observations]) for i in range(obs_dim)]\n    \n    # Return the dictionary with the computed statistics\n    return {\n        'tuple_actions': tuple_actions,\n        'tuple_observations': tuple_observations,\n        'multidim_actions': any(len(dim) > 1 for dim in n_actions_per_dim),\n        'multidim_observations': any(len(dim) > 1 for dim in n_obs_per_dim),\n        'continuous_actions': continuous_actions,\n        'continuous_observations': continuous_observations,\n        'n_actions_per_dim': n_actions_per_dim,\n        'action_dim': action_dim,\n        'n_obs_per_dim': n_obs_per_dim,\n        'obs_dim': obs_dim,\n        'action_ids': action_ids,\n        'obs_ids': obs_ids\n    }\n\n# Example usage\nactions = [(0, 1), (1, 0), (1, 1)]\nobservations = [10.0, 11.5, 12.0]\nprint(env_stats(actions, observations))\n```\n\nThis implementation should correctly compute the required statistics for the given input lists of actions and observations."}
{"task_id": 413, "completion_id": 0, "completion": "To implement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch, we need to follow the steps outlined in the task. Below is the complete implementation of the `gbdt_regression` function:\n\n```python\nimport numpy as np\n\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    # Initialize the predictions with the mean of the training targets\n    y_pred = np.full_like(y_train, np.mean(y_train))\n    \n    for _ in range(n_estimators):\n        # Compute the residuals\n        residuals = y_train - y_pred\n        \n        # Fit a CART regression tree to the residuals\n        tree = DecisionTreeRegressor(max_depth=max_depth)\n        tree.fit(X_train, residuals)\n        \n        # Update the predictions\n        y_pred += learning_rate * tree.predict(X_train)\n    \n    # Predict on the test set\n    y_pred_test = y_pred\n    \n    # Round the predictions to 4 decimal places and convert to a list\n    y_pred_test = [round(pred, 4) for pred in y_pred_test]\n    \n    return y_pred_test\n\nclass DecisionTreeRegressor:\n    def __init__(self, max_depth=None):\n        self.max_depth = max_depth\n        self.tree = None\n    \n    def fit(self, X, y):\n        self.tree = self._build_tree(X, y, depth=0)\n    \n    def predict(self, X):\n        return np.array([self._predict_single(x, self.tree) for x in X])\n    \n    def _build_tree(self, X, y, depth):\n        if depth == self.max_depth or len(np.unique(y)) == 1 or len(X) == 0:\n            return {'value': np.mean(y)}\n        \n        best_split = self._find_best_split(X, y)\n        if best_split is None:\n            return {'value': np.mean(y)}\n        \n        left_mask = X[:, best_split['feature']] <= best_split['threshold']\n        right_mask = X[:, best_split['feature']] > best_split['threshold']\n        \n        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return {\n            'feature': best_split['feature'],\n            'threshold': best_split['threshold'],\n            'left': left_tree,\n            'right': right_tree\n        }\n    \n    def _find_best_split(self, X, y):\n        best_gain = -np.inf\n        best_split = None\n        \n        for feature in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                left_mask = X[:, feature] <= threshold\n                right_mask = X[:, feature] > threshold\n                \n                if len(left_mask) == 0 or len(right_mask) == 0:\n                    continue\n                \n                gain = self._gain(y, y[left_mask], y[right_mask])\n                if gain > best_gain:\n                    best_gain = gain\n                    best_split = {\n                        'feature': feature,\n                        'threshold': threshold\n                    }\n        \n        return best_split\n    \n    def _gain(self, parent, left, right):\n        n = len(parent)\n        n_left = len(left)\n        n_right = len(right)\n        gini_parent = self._gini(parent)\n        gini_left = self._gini(left)\n        gini_right = self._gini(right)\n        \n        return gini_parent - (n_left / n) * gini_left - (n_right / n) * gini_right\n    \n    def _gini(self, y):\n        if len(y) == 0:\n            return 0\n        counts = np.bincount(y)\n        total = np.sum(counts)\n        return 1 - np.sum((counts / total) ** 2)\n    \n    def _predict_single(self, x, node):\n        if 'value' in node:\n            return node['value']\n        if x[node['feature']] <= node['threshold']:\n            return self._predict_single(x, node['left'])\n        else:\n            return self._predict_single(x, node['right'])\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y_train = np.array([1, 2, 3, 4])\n# X_test = np.array([[2.5, 3.5], [3.5, 4.5]])\n# predictions = gbdt_regression(X_train, y_train, X_test)\n# print(predictions)\n```\n\n### Explanation:\n1. **Initialization**: The initial prediction is set to the mean of the training targets.\n2. **Residuals Calculation**: For each iteration, the residuals are calculated as the difference between the actual targets and the current predictions.\n3. **Tree Fitting**: A CART regression tree is fitted to the residuals. The tree is limited by `max_depth`.\n4. **Prediction Update**: The predictions are updated by adding the tree's output multiplied by the learning rate.\n5. **Final Prediction**: After all iterations, the final predictions are rounded to 4 decimal places and returned as a list.\n\nThis implementation uses a simple binary split strategy and mean squared error as the split criterion. The `DecisionTreeRegressor` class is a basic implementation of a decision tree regressor, which can be used to fit the residuals and make predictions."}
{"task_id": 416, "completion_id": 0, "completion": "Here is a Python function that implements the multivariate Gaussian PDF:\n\n```python\nimport numpy as np\nimport math\n\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"\n    Compute the multivariate Gaussian PDF for a given set of data points.\n\n    Parameters:\n    X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the data points.\n    mean (np.ndarray): A 1D array of length n_features representing the mean vector.\n    cov (np.ndarray): A 2D array of shape (n_features, n_features) representing the covariance matrix.\n\n    Returns:\n    list: A list of PDF values for each data point in X, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure X is a 2D array\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    \n    # Compute the number of samples and features\n    n_samples, n_features = X.shape\n    \n    # Compute the determinant of the covariance matrix\n    det_cov = np.linalg.det(cov)\n    \n    # Compute the inverse of the covariance matrix\n    inv_cov = np.linalg.inv(cov)\n    \n    # Compute the PDF values for each data point\n    pdf_values = []\n    for x in X:\n        diff = x - mean\n        exponent = -0.5 * np.dot(np.dot(diff.T, inv_cov), diff)\n        pdf = (1 / (math.sqrt((2 * math.pi) ** n_features * det_cov))) * math.exp(exponent)\n        pdf_values.append(round(pdf, 4))\n    \n    return pdf_values\n```\n\n### Explanation:\n1. **Input Validation**: The function first ensures that `X` is a 2D array. If `X` is a 1D array, it is reshaped to a 2D array with a single column.\n2. **Dimensionality**: The number of samples (`n_samples`) and features (`n_features`) is determined.\n3. **Covariance Matrix Properties**: The determinant and inverse of the covariance matrix are computed.\n4. **PDF Calculation**: For each data point `x` in `X`, the PDF is calculated using the formula provided. The result is rounded to four decimal places and added to the list `pdf_values`.\n5. **Return**: The list of PDF values is returned.\n\nThis function should work correctly for any valid input as specified in the task description."}
{"task_id": 419, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    if fit_intercept:\n        X = np.hstack([np.ones((X.shape[0], 1)), X])\n        X_new = np.hstack([np.ones((X_new.shape[0], 1)), X_new])\n    \n    if V is None:\n        V = np.eye(X.shape[1])\n    elif isinstance(V, (int, float)):\n        V = V * np.eye(X.shape[1])\n    elif isinstance(V, np.ndarray):\n        if V.shape != (X.shape[1], X.shape[1]):\n            raise ValueError(\"V must be a square matrix with the same number of rows and columns as X has features.\")\n    \n    if isinstance(mu, (int, float)):\n        mu = np.full(X.shape[1], mu)\n    \n    Sigma_p = np.linalg.inv(np.linalg.inv(V) + X.T @ X)\n    mu_p = Sigma_p @ (np.linalg.inv(V) @ mu + X.T @ y)\n    \n    y_pred = X_new @ mu_p\n    y_pred = np.round(y_pred, 4).tolist()\n    \n    return y_pred\n```\n\nThis function implements Bayesian linear regression with a conjugate Gaussian prior and known observation variance. It handles optional intercept fitting, computes the posterior distribution, and returns the MAP predictive means for new data points. The predictions are rounded to 4 decimal places and returned as a list."}
{"task_id": 423, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Stub for the RMSLE metric.  Complete the body of the function.\"\"\"\n    # Check if both lists have the same length\n    if len(actual) != len(predicted):\n        return -1\n    \n    # Check if all elements in both lists are non-negative\n    if any(x < 0 for x in actual) or any(x < 0 for x in predicted):\n        return -1\n    \n    # Calculate the RMSLE\n    n = len(actual)\n    rmsle = np.sqrt((1/n) * np.sum((np.log1p(predicted) - np.log1p(actual))**2))\n    \n    # Return the result rounded to four decimal places\n    return round(rmsle, 4)\n```\nThis function first checks if the input lists have the same length and contain only non-negative numbers. If any of these conditions are not met, it returns -1. If the inputs are valid, it calculates the RMSLE using the provided formula and returns the result rounded to four decimal places."}
{"task_id": 428, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    elif window_len == 1:\n        return [1.0]\n    \n    if symmetric:\n        n = np.arange(window_len)\n        window = 0.5 * (1 - np.cos(2 * np.pi * n / (window_len - 1)))\n    else:\n        n = np.arange(window_len + 1)\n        window = 0.5 * (1 - np.cos(2 * np.pi * n / (window_len - 1)))\n        window = window[:-1]  # Drop the last element to make it periodic\n    \n    return np.round(window, 4).tolist()\n```\n\nThis function generates a Hann window based on the specified length and symmetry. It handles special cases where the window length is less than or equal to 1. For a symmetric window, it calculates the Hann coefficients directly. For a periodic window, it calculates one extra coefficient and then discards the last one to ensure continuity. The result is rounded to 4 decimal places and returned as a list."}
{"task_id": 433, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    if len(shape) == 4:\n        # For 4-D convolutional kernels\n        in_channels = shape[1]\n        kernel_height = shape[2]\n        kernel_width = shape[3]\n        receptive_field_size = kernel_height * kernel_width\n        fan_in = in_channels * receptive_field_size\n        fan_out = shape[0] * receptive_field_size\n    else:\n        # For other tensors\n        fan_in = shape[0]\n        fan_out = shape[1]\n    \n    return (fan_in, fan_out)\n```\n\nThis function `glorot_fan` takes a shape tuple or list and computes the `fan_in` and `fan_out` based on the rules provided. It handles both 4-D convolutional kernels and other tensors appropriately."}
{"task_id": 435, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    X = np.array(X, dtype=float)\n    y = np.array(y, dtype=float)\n    \n    # Check for special cases\n    if n_estimators <= 0 or learning_rate == 0:\n        return [round(np.mean(y), 4)] * len(y)\n    \n    # Initialize the current prediction with the mean of y\n    y_pred = np.full_like(y, np.mean(y), dtype=float)\n    \n    # Perform the gradient boosting iterations\n    for _ in range(n_estimators):\n        # Compute the residuals\n        residuals = y - y_pred\n        \n        # Fit an OLS linear model to predict the residuals\n        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add intercept term\n        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(residuals)\n        \n        # Obtain the weak-learner prediction\n        h_pred = X_b.dot(theta_best)\n        \n        # Update the overall prediction\n        y_pred += learning_rate * h_pred\n    \n    # Round the predictions to 4 decimal places and convert to a list\n    return [round(pred, 4) for pred in y_pred]\n```\n\nThis code implements a very small-scale version of the Gradient Boosting Regressor as described in the task. It uses ordinary least-squares (OLS) linear regression as the weak learner and the squared error as the loss function. The function handles the special cases where `n_estimators` is less than or equal to 0 or `learning_rate` is 0 by returning a vector filled with the target mean. The predictions are rounded to 4 decimal places and returned as a regular Python list."}
{"task_id": 437, "completion_id": 0, "completion": "Here is a Python function that implements the logistic sigmoid activation function and its derivatives:\n\n```python\nimport numpy as np\n\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        x = np.array([x])\n    \n    if order == 0:\n        result = 1 / (1 + np.exp(-x))\n    elif order == 1:\n        sigma = 1 / (1 + np.exp(-x))\n        result = sigma * (1 - sigma)\n    elif order == 2:\n        sigma = 1 / (1 + np.exp(-x))\n        result = sigma * (1 - sigma) * (1 - 2 * sigma)\n    else:\n        return -1\n    \n    result = np.round(result, 4)\n    \n    if order == 0 and result.size == 1:\n        return result[0]\n    else:\n        return result.tolist()\n\n# Example usage:\nscalar_result = sigmoid(1.0, order=0)\nlist_result = sigmoid([1.0, 2.0, 3.0], order=1)\nndarray_result = sigmoid(np.array([1.0, 2.0, 3.0]), order=2)\n\nprint(scalar_result)  # Output: 0.7311\nprint(list_result)    # Output: [0.1964, 0.8808, 0.9526]\nprint(ndarray_result) # Output: [[0.1964, 0.8808, 0.9526]]\n```\n\n### Explanation:\n1. **Input Handling**: The function first checks if the input `x` is a scalar (int or float) and converts it to a NumPy array if necessary.\n2. **Order Handling**: Depending on the `order` parameter, the function computes the appropriate value:\n   - **Order 0**: Computes the logistic sigmoid function \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\).\n   - **Order 1**: Computes the first derivative \\(\\sigma(x)(1-\\sigma(x))\\).\n   - **Order 2**: Computes the second derivative \\(\\sigma(x)(1-\\sigma(x))(1-2\\sigma(x))\\).\n   - **Other Orders**: Returns `-1` if the order is not 0, 1, or 2.\n3. **Rounding**: The result is rounded to four decimal places using `np.round`.\n4. **Output Formatting**: If the input is a scalar and the order is 0, the function returns a single float. Otherwise, it returns a list using `tolist()` to preserve the shape of the input.\n\nThis function should work correctly for scalar, list, and NumPy array inputs, and it handles the specified orders of the sigmoid function and its derivatives."}
{"task_id": 438, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `to_categorical` function according to the provided specifications:\n\n```python\nimport numpy as np\n\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # Convert input to NumPy array for easier manipulation\n    y = np.array(y)\n    \n    # Determine the number of classes if not provided\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n    \n    # Check if the provided num_classes is sufficient\n    if num_classes < np.max(y) + 1:\n        return -1\n    \n    # Create the one-hot encoded matrix\n    one_hot_matrix = np.eye(num_classes)[y]\n    \n    # Convert the result to a list of lists\n    one_hot_list = one_hot_matrix.tolist()\n    \n    return one_hot_list\n\n# Test cases\nprint(to_categorical([1, 2, 3, 0]))  # [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\nprint(to_categorical([1, 2, 3, 0], num_classes=4))  # [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\nprint(to_categorical([1, 2, 3, 0], num_classes=3))  # -1\nprint(to_categorical([1, 2, 3, 0], num_classes=5))  # [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0]]\nprint(to_categorical([1, 2, 3, 0], num_classes=None))  # [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n```\n\n### Explanation:\n1. **Input Conversion**: The input `y` is converted to a NumPy array for easier manipulation.\n2. **Determine `num_classes`**: If `num_classes` is not provided, it is determined as `max(y) + 1`.\n3. **Validation**: If the provided `num_classes` is smaller than `max(y) + 1`, the function returns `-1`.\n4. **One-Hot Encoding**: The `np.eye` function is used to create a one-hot encoded matrix.\n5. **Conversion to List**: The resulting one-hot encoded matrix is converted to a list of lists using `tolist()`.\n\nThis implementation ensures that the function behaves as specified and handles edge cases appropriately."}
{"task_id": 439, "completion_id": 0, "completion": "To implement the Apriori algorithm for frequent itemset mining, we need to follow the steps outlined in the problem description. Here's a step-by-step implementation of the Apriori algorithm in Python:\n\n1. **Generate candidate itemsets**: Start with all single-item candidates and keep only those that are frequent.\n2. **Join and prune**: Repeatedly generate size-`k` candidates by self-joining the frequent itemsets of size `k-1` and prune any candidate that contains an infrequent subset.\n3. **Check support**: Calculate the empirical support for each candidate and keep only those that meet the minimum support threshold.\n4. **Repeat**: Continue the process until no new frequent itemsets are found.\n\nHere's the complete implementation:\n\n```python\nimport itertools\n\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # Helper function to calculate the support of an itemset\n    def calculate_support(itemset):\n        return sum(all(item in transaction for item in itemset) for transaction in transactions) / len(transactions)\n    \n    # Initialize the list of frequent itemsets\n    frequent_itemsets = []\n    candidate_itemsets = set(frozenset([item]) for item in set(item for transaction in transactions for item in transaction))\n    \n    while candidate_itemsets:\n        # Filter out candidates that are not frequent\n        frequent_itemsets = [itemset for itemset in candidate_itemsets if calculate_support(itemset) >= min_sup]\n        \n        # If no frequent itemsets are found, break the loop\n        if not frequent_itemsets:\n            break\n        \n        # Sort frequent itemsets by length and lexicographically\n        frequent_itemsets.sort(key=lambda x: (len(x), x))\n        \n        # Add the current frequent itemsets to the result\n        frequent_itemsets.extend(frequent_itemsets)\n        \n        # Generate candidate itemsets of size k+1\n        candidate_itemsets = set()\n        for i in range(len(frequent_itemsets)):\n            for j in range(i + 1, len(frequent_itemsets)):\n                candidate = frequent_itemsets[i].union(frequent_itemsets[j])\n                if len(candidate) == len(frequent_itemsets[i]) + 1:\n                    candidate_itemsets.add(candidate)\n    \n    # Sort the final frequent itemsets by length and lexicographically\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return frequent_itemsets\n\n# Example usage:\ntransactions = [\n    [1, 2, 3],\n    [2, 3, 4],\n    [1, 3, 4],\n    [1, 2, 4],\n    [1, 2, 3, 4]\n]\nmin_sup = 0.5\nprint(apriori_frequent_itemsets(transactions, min_sup))\n```\n\n### Explanation:\n1. **calculate_support**: This helper function calculates the empirical support of a given itemset.\n2. **Initial candidate itemsets**: We start with all single-item candidates.\n3. **Main loop**: We repeatedly generate candidate itemsets and filter out those that are not frequent.\n4. **Sorting**: We sort the frequent itemsets by length and lexicographically.\n5. **Candidate generation**: We generate candidate itemsets by joining frequent itemsets of size `k-1` and pruning any candidate that contains an infrequent subset.\n\nThis implementation ensures that the frequent itemsets are returned in the required order and meet the minimum support threshold."}
{"task_id": 440, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    # Convert the nested list to a numpy array for efficient computation\n    predictions_array = np.array(predictions)\n    \n    # Average the probabilities across estimators\n    averaged_probabilities = np.mean(predictions_array, axis=0)\n    \n    # Round the averaged probabilities to 4 decimal places\n    averaged_probabilities_rounded = np.round(averaged_probabilities, 4)\n    \n    # Convert the rounded numpy array back to a list of lists\n    averaged_probabilities_list = averaged_probabilities_rounded.tolist()\n    \n    # Find the predicted class index for each sample\n    predicted_classes = np.argmax(averaged_probabilities, axis=1).tolist()\n    \n    return averaged_probabilities_list, predicted_classes\n```\n\nThis function first converts the nested list of predictions into a NumPy array for efficient computation. It then averages the probabilities across the estimators for each sample. The averaged probabilities are rounded to four decimal places, and the function converts the result back into a list of lists. Finally, it uses `np.argmax` to find the class index with the highest probability for each sample, breaking ties by returning the smallest index. The function returns both the averaged probability matrix and the predicted class labels."}
{"task_id": 444, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Validate inputs\n    if Y is None:\n        Y = X\n    \n    X = np.array(X)\n    Y = np.array(Y)\n    \n    if X.shape[1] != Y.shape[1]:\n        return -1\n    \n    if sigma is None:\n        sigma = np.sqrt(X.shape[1] / 2)\n    elif isinstance(sigma, (int, float)):\n        sigma = [sigma] * X.shape[1]\n    elif isinstance(sigma, list):\n        if len(sigma) != X.shape[1]:\n            return -1\n    else:\n        return -1\n    \n    sigma = np.array(sigma)\n    if np.any(sigma <= 0):\n        return -1\n    \n    # Compute the kernel matrix\n    diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n    scaled_diff = diff / sigma\n    squared_diff = np.sum(scaled_diff ** 2, axis=2)\n    kernel_matrix = np.exp(-0.5 * squared_diff)\n    \n    # Round to 4 decimal places and return as a nested list\n    return kernel_matrix.round(4).tolist()\n```\n\nThis function first validates the inputs, including checking the feature dimensions and the validity of the `sigma` parameter. It then computes the RBF kernel matrix using the provided formula and returns the result rounded to four decimal places as a nested list. If any invalid input is detected, the function returns `-1`."}
{"task_id": 446, "completion_id": 0, "completion": "To implement the `decision_stump` function, we need to follow these steps:\n\n1. Calculate the initial entropy of the dataset.\n2. For each feature, find all possible thresholds (midpoints between successive distinct sorted values).\n3. For each threshold, split the dataset into left and right subsets.\n4. Calculate the entropy for each subset.\n5. Compute the information gain for each split.\n6. Track the split with the highest information gain, breaking ties according to the specified rules.\n7. If no split improves the information gain, return the majority label.\n\nHere's the implementation of the `decision_stump` function:\n\n```python\nimport numpy as np\n\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    \n    n_samples, n_features = X.shape\n    \n    # Calculate the initial entropy of the dataset\n    def entropy(labels):\n        _, counts = np.unique(labels, return_counts=True)\n        probabilities = counts / len(labels)\n        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n    \n    initial_entropy = entropy(y)\n    \n    best_feature_index = -1\n    best_threshold = None\n    best_gain = -np.inf\n    left_majority_label = None\n    right_majority_label = None\n    \n    for feature_index in range(n_features):\n        feature_values = X[:, feature_index]\n        sorted_indices = np.argsort(feature_values)\n        sorted_values = feature_values[sorted_indices]\n        sorted_labels = y[sorted_indices]\n        \n        for i in range(1, len(sorted_values)):\n            threshold = (sorted_values[i] + sorted_values[i - 1]) / 2.0\n            left_labels = sorted_labels[sorted_values <= threshold]\n            right_labels = sorted_labels[sorted_values > threshold]\n            \n            p_left = len(left_labels) / len(sorted_labels)\n            p_right = len(right_labels) / len(sorted_labels)\n            \n            gain = initial_entropy - p_left * entropy(left_labels) - p_right * entropy(right_labels)\n            \n            if gain > best_gain:\n                best_gain = gain\n                best_feature_index = feature_index\n                best_threshold = round(threshold, 4)\n                left_majority_label = np.argmax(np.bincount(left_labels))\n                right_majority_label = np.argmax(np.bincount(right_labels))\n            elif gain == best_gain:\n                if feature_index < best_feature_index:\n                    best_feature_index = feature_index\n                    best_threshold = round(threshold, 4)\n                    left_majority_label = np.argmax(np.bincount(left_labels))\n                    right_majority_label = np.argmax(np.bincount(right_labels))\n                elif feature_index == best_feature_index and threshold < best_threshold:\n                    best_threshold = round(threshold, 4)\n                    left_majority_label = np.argmax(np.bincount(left_labels))\n                    right_majority_label = np.argmax(np.bincount(right_labels))\n    \n    if best_gain == -np.inf:\n        majority_label = np.argmax(np.bincount(y))\n        return (-1, None, majority_label, majority_label)\n    \n    return (best_feature_index, best_threshold, left_majority_label, right_majority_label)\n\n# Example usage:\nX = [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]\ny = [0, 0, 1, 1, 1]\nprint(decision_stump(X, y))\n```\n\nThis implementation follows the steps outlined above and ensures that the decision stump with the highest information gain is found, breaking ties according to the specified rules. If no split improves the information gain, it returns the majority label."}
{"task_id": 452, "completion_id": 0, "completion": "Here is a Python function that implements the described behavior:\n\n```python\nimport numpy as np\nfrom typing import List\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # Convert X to a NumPy array if it's not already\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    \n    # Split the data based on the feature and threshold\n    if np.issubdtype(X[:, feature_i].dtype, np.number):\n        # Numeric threshold\n        subset1 = X[X[:, feature_i] >= threshold].tolist()\n        subset2 = X[X[:, feature_i] < threshold].tolist()\n    else:\n        # Categorical threshold\n        subset1 = X[X[:, feature_i] == threshold].tolist()\n        subset2 = X[X[:, feature_i] != threshold].tolist()\n    \n    return [subset1, subset2]\n\n# Example usage:\nX = np.array([[1, 5], [3, 2], [4, 6], [2, 1]])\nfeature_i = 0\nthreshold = 3\nprint(divide_on_feature(X, feature_i, threshold))  # \u279c  [[ [3, 2], [4, 6] ], [ [1, 5], [2, 1] ]]\n\nX = np.array([[1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"C\"]])\nfeature_i = 1\nthreshold = \"A\"\nprint(divide_on_feature(X, feature_i, threshold))  # \u279c  [[ [1, \"A\"], [3, \"A\"] ], [ [2, \"B\"], [4, \"C\"] ]]\n```\n\n### Explanation:\n1. **Type Conversion**: The function first checks if the input `X` is a NumPy array. If not, it converts it to a NumPy array.\n2. **Feature and Threshold Check**: It checks the data type of the feature column to determine whether the threshold is numeric or categorical.\n3. **Splitting Logic**:\n   - For numeric thresholds, it splits the data into two subsets based on whether the feature value is greater than or equal to the threshold.\n   - For categorical thresholds, it splits the data into two subsets based on whether the feature value is exactly equal to the threshold.\n4. **Conversion to List**: Both subsets are converted to regular Python lists using `tolist()` and returned as a list of two lists.\n\nThis function ensures that the original order of samples is preserved and handles both numeric and categorical data types appropriately."}
{"task_id": 453, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    # Check for invalid inputs\n    if not (0 <= gamma <= 1):\n        return -1\n    if len(Q) != len(Q_next) or len(Q) != len(actions) or len(Q) != len(rewards) or len(Q) != len(dones):\n        return -1\n    \n    # Initialize the target Q-values matrix\n    targets = Q.copy()\n    \n    # Update the target Q-values for each sample\n    for i in range(len(Q)):\n        if dones[i]:\n            targets[i, actions[i]] = rewards[i]\n        else:\n            targets[i, actions[i]] = rewards[i] + gamma * np.max(Q_next[i])\n    \n    # Round the targets to 4 decimal places\n    targets = np.round(targets, 4)\n    \n    # Convert the NumPy array back to a nested Python list\n    return targets.tolist()\n```\n\nThis function first checks if the inputs are valid, including the discount factor `gamma` and the lengths of the input arrays. If any of these checks fail, it returns `-1`. If the inputs are valid, it creates a copy of the `Q` matrix and updates the entries corresponding to the actions taken in each state based on the given formula. Finally, it rounds the updated Q-values to four decimal places and returns them as a nested Python list."}
{"task_id": 458, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `to_categorical` function that meets the specified requirements:\n\n```python\nimport numpy as np\n\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    # Convert labels to a numpy array for easier manipulation\n    labels = np.array(labels)\n    \n    # Check for negative labels\n    if np.any(labels < 0):\n        return -1\n    \n    # Infer num_classes if not provided\n    if num_classes is None:\n        num_classes = max(labels) + 1\n    \n    # Check if num_classes is too small\n    if num_classes < max(labels) + 1:\n        return -1\n    \n    # Initialize the one-hot encoded matrix\n    one_hot_matrix = np.zeros((len(labels), num_classes), dtype=int)\n    \n    # Set the appropriate indices to 1\n    one_hot_matrix[np.arange(len(labels)), labels] = 1\n    \n    # Convert the numpy array back to a list of lists\n    return one_hot_matrix.tolist()\n\n# Example usage:\nlabels = [0, 1, 2, 3]\nnum_classes = 4\none_hot_encoded = to_categorical(labels, num_classes)\nprint(one_hot_encoded)  # Output: [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n```\n\n### Explanation:\n1. **Input Validation**:\n   - The function first converts the input `labels` to a NumPy array for easier manipulation.\n   - It checks if any label is negative. If so, it returns `-1`.\n   - If `num_classes` is not provided, it infers the number of classes as `max(labels) + 1`.\n   - It then checks if the provided `num_classes` is smaller than `max(labels) + 1`. If so, it returns `-1`.\n\n2. **One-Hot Encoding**:\n   - A zero matrix of shape `(len(labels), num_classes)` is initialized.\n   - The appropriate indices are set to `1` using NumPy's advanced indexing.\n\n3. **Conversion to List of Lists**:\n   - The resulting NumPy array is converted back to a list of lists of integers before being returned.\n\nThis implementation ensures that the function meets all the specified requirements and constraints."}
{"task_id": 461, "completion_id": 0, "completion": "Here is a Python function that implements the gradient boosting algorithm with decision stumps for one-dimensional regression data:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    x = np.array(x)\n    y = np.array(y)\n    \n    # Initialize predictions to the mean of the targets\n    y_pred = np.full_like(y, np.mean(y))\n    \n    # Convert to lists for easier manipulation\n    x_list = x.tolist()\n    y_list = y.tolist()\n    \n    for _ in range(n_estimators):\n        # Compute residuals\n        residuals = y - y_pred\n        \n        # Find unique x values and ignore the greatest one\n        unique_x = sorted(set(x))\n        if len(unique_x) < 2:\n            break\n        \n        min_sse = float('inf')\n        best_split = None\n        \n        for i in range(len(unique_x) - 1):\n            t = unique_x[i]\n            left_indices = [j for j, xi in enumerate(x_list) if xi <= t]\n            right_indices = [j for j, xi in enumerate(x_list) if xi > t]\n            \n            if not left_indices or not right_indices:\n                continue\n            \n            left_residuals = residuals[left_indices]\n            right_residuals = residuals[right_indices]\n            \n            v_L = np.mean(left_residuals)\n            v_R = np.mean(right_residuals)\n            \n            sse = np.sum((left_residuals - v_L) ** 2) + np.sum((right_residuals - v_R) ** 2)\n            \n            if sse < min_sse:\n                min_sse = sse\n                best_split = (t, v_L, v_R)\n        \n        if best_split is None:\n            # If no valid split exists, set the stump prediction to the mean residual of the whole data\n            v_L = np.mean(residuals)\n            v_R = v_L\n            best_split = (unique_x[0], v_L, v_R)\n        \n        t, v_L, v_R = best_split\n        \n        # Update predictions\n        y_pred += learning_rate * np.array([v_L if xi <= t else v_R for xi in x])\n    \n    # Round predictions to 4 decimal places\n    return np.round(y_pred, 4).tolist()\n```\n\n### Explanation:\n1. **Initialization**: The predictions are initialized to the mean of the target values.\n2. **Residual Calculation**: For each iteration, the residuals are calculated as the difference between the actual target values and the current predictions.\n3. **Splitting**: The algorithm iterates over possible split points (unique x values except the greatest one) and computes the sum of squared errors (SSE) for each split.\n4. **Best Split Selection**: The split with the smallest SSE is selected. If no valid split exists, the algorithm falls back to using the mean residual of the whole data.\n5. **Prediction Update**: The predictions are updated using the selected split and the learning rate.\n6. **Final Output**: The final predictions are rounded to four decimal places and returned as a list.\n\nThis function should work for the given task and constraints."}
{"task_id": 471, "completion_id": 0, "completion": "To implement the one-step Expected SARSA TD(0) update for a tabular setting, we need to follow these steps:\n\n1. Calculate the target value using the Expected SARSA formula.\n2. Update the Q-table entry for the given state and action using the target value.\n3. Return the updated Q-table with all entries rounded to four decimal places.\n\nHere's the implementation of the `expected_sarsa_update` function:\n\n```python\nimport numpy as np\n\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    # Get the number of actions\n    num_actions = len(q_table[0])\n    \n    # Calculate the expected value using the \u03b5-soft policy\n    if next_state is not None:\n        # Find the greedy action\n        greedy_action = np.argmax(q_table[next_state])\n        \n        # Calculate the expected value\n        expected_value = 0\n        for a in range(num_actions):\n            if a == greedy_action:\n                expected_value += (1 - epsilon + epsilon / num_actions) * q_table[next_state][a]\n            else:\n                expected_value += epsilon / num_actions * q_table[next_state][a]\n    else:\n        expected_value = 0\n    \n    # Calculate the target value\n    target = reward + gamma * expected_value\n    \n    # Update the Q-table entry\n    q_table[state][action] += lr * (target - q_table[state][action])\n    \n    # Round all entries in the Q-table to four decimal places\n    q_table = [[round(q, 4) for q in row] for row in q_table]\n    \n    return q_table\n```\n\n### Explanation:\n1. **Calculate the Expected Value**:\n   - If `next_state` is not `None`, we calculate the expected value using the \u03b5-soft policy.\n   - We find the greedy action using `np.argmax(q_table[next_state])`.\n   - We then compute the expected value by summing up the contributions from each action, weighted by the \u03b5-soft policy probabilities.\n\n2. **Calculate the Target Value**:\n   - The target value is calculated using the formula: `target = reward + gamma * expected_value`.\n\n3. **Update the Q-table**:\n   - We update the Q-table entry for the given state and action using the target value and the learning rate `lr`.\n\n4. **Round the Q-table**:\n   - Finally, we round all entries in the Q-table to four decimal places and return the updated Q-table.\n\nThis implementation ensures that the Q-table is updated correctly according to the Expected SARSA algorithm and that all entries are rounded to four decimal places."}
{"task_id": 474, "completion_id": 0, "completion": "Here is the implementation of the `generate_data` function that addresses the issues mentioned in the task:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n\n    X_train = []\n    Y_train = []\n    X_val = []\n    Y_val = []\n\n    for i in range(m):\n        label = (i - 0.5) * 2\n        for _ in range(n_train):\n            x = np.random.uniform((i // 2 + 0.1) * 10, (i // 2 + 0.9) * 10)\n            y = np.random.uniform(((i % 2) * interval + 0.1) * 10, ((i % 2) * interval + 0.9) * 10)\n            X_train.append([x, y])\n            Y_train.append(label)\n\n        for _ in range(n_val):\n            x = np.random.uniform((i // 2 + 0.1) * 10, (i // 2 + 0.9) * 10)\n            y = np.random.uniform(((i % 2) * interval + 0.1) * 10, ((i % 2) * interval + 0.9) * 10)\n            X_val.append([x, y])\n            Y_val.append(label)\n\n    X_train = np.array(X_train).round(4)\n    Y_train = np.array(Y_train).round(4)\n    X_val = np.array(X_val).round(4)\n    Y_val = np.array(Y_val).round(4)\n\n    return X_train, X_val, Y_train, Y_val\n```\n\n### Explanation:\n1. **Random Seed Handling**: The function checks if a seed is provided and sets it using `random.seed` and `np.random.seed` to ensure reproducibility.\n2. **Data Generation**:\n   - The function iterates over each class index `i`.\n   - For each class, it calculates the label based on the formula `(i - 0.5) * 2`.\n   - It then generates `n_train` and `n_val` samples for the training and validation sets, respectively.\n   - Each sample is generated using `np.random.uniform` within the specified ranges for `x` and `y`.\n3. **Rounding**: All coordinates are rounded to four decimal places using the `round` method.\n4. **Return Values**: The function returns four NumPy arrays: `X_train`, `X_val`, `Y_train`, and `Y_val`.\n\nThis implementation ensures that the data set is reusable and that all coordinates are rounded to four decimal places, as required."}
{"task_id": 475, "completion_id": 0, "completion": "Here is the implementation of the Adam optimizer update step in Python:\n\n```python\nimport numpy as np\n\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Initialize m_prev and v_prev if they are None\n    if m_prev is None:\n        m_prev = np.zeros_like(grad)\n    if v_prev is None:\n        v_prev = np.zeros_like(grad)\n    \n    # Compute the bias-corrected first and second moments\n    m_t = b1 * m_prev + (1 - b1) * grad\n    v_t = b2 * v_prev + (1 - b2) * grad**2\n    \n    # Compute the bias correction terms\n    m_hat_t = m_t / (1 - b1**t)\n    v_hat_t = v_t / (1 - b2**t)\n    \n    # Update the parameters\n    w_new = w - learning_rate * m_hat_t / (np.sqrt(v_hat_t) + eps)\n    \n    return w_new, m_t, v_t\n\n# Example call\nw_new, m_new, v_new = adam_update(1.0, 0.1, 0.0, 0.0, 1)\nprint(round(w_new, 9))  # \u2248 0.999000001\n```\n\n### Explanation:\n1. **Initialization**: If `m_prev` or `v_prev` is `None`, they are initialized to zero arrays of the same shape as `grad`.\n2. **First Moment Update**: The first moment `m_t` is updated using the formula `m_t = \u03b2\u2081 \u00b7 m_{t\u22121} + (1\u2212\u03b2\u2081) \u00b7 grad`.\n3. **Second Moment Update**: The second moment `v_t` is updated using the formula `v_t = \u03b2\u2082 \u00b7 v_{t\u22121} + (1\u2212\u03b2\u2082) \u00b7 grad\u00b2`.\n4. **Bias Correction**: The bias-corrected first moment `m_hat_t` and second moment `v_hat_t` are computed using the formulas `m\u0302_t = m_t / (1\u2212\u03b2\u2081\u1d57)` and `v\u0302_t = v_t / (1\u2212\u03b2\u2082\u1d57)`.\n5. **Parameter Update**: The parameters `w` are updated using the formula `w_new = w \u2212 \u03b1 \u00b7 m\u0302_t / (\u221av\u0302_t + \u03b5)`.\n\nThis implementation ensures that the function works for both scalars and arbitrary-shaped NumPy arrays, and it returns the updated parameters along with the new moment estimates."}
{"task_id": 479, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import List, Optional\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    N = len(rewards[0])\n    estimates = [ev_prior] * N\n    counts = [0] * N\n\n    for t in range(len(rewards)):\n        if np.random.rand() < epsilon:\n            # Explore: choose a random arm\n            arm = np.random.randint(N)\n        else:\n            # Exploit: choose the arm with the highest estimated value\n            arm = np.argmax(estimates)\n\n        # Pull the chosen arm and receive the reward\n        reward = rewards[t][arm]\n\n        # Update the estimate for the chosen arm\n        counts[arm] += 1\n        estimates[arm] += (reward - estimates[arm]) / counts[arm]\n\n    # Return the final estimates rounded to 4 decimal places\n    return [round(v, 4) for v in estimates]\n```\n\nThis implementation follows the \u03b5-greedy algorithm as described. It initializes the estimates for each arm with `ev_prior` and counts the number of times each arm has been pulled. For each time-step, it either explores by choosing a random arm or exploits by choosing the arm with the highest estimated value. After pulling an arm, it updates the estimate using the incremental sample mean formula. Finally, it returns the estimates rounded to 4 decimal places."}
{"task_id": 481, "completion_id": 0, "completion": "To implement the DBSCAN algorithm from scratch, we need to follow these steps:\n\n1. **Initialize**: Create a list to store the cluster labels for each point.\n2. **Mark Points as Unvisited**: Create a boolean array to keep track of which points have been visited.\n3. **Iterate Over Each Point**: For each point, if it hasn't been visited, determine if it is a core point, border point, or noise point.\n4. **Expand Clusters**: If a point is a core point, start a new cluster and expand it to include all reachable points.\n5. **Assign Labels**: Assign cluster labels to border points and noise points accordingly.\n\nHere's the complete implementation of the DBSCAN algorithm:\n\n```python\nimport numpy as np\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    if data.shape[0] == 0:\n        return []\n\n    n_samples, n_features = data.shape\n    labels = [-1] * n_samples\n    visited = [False] * n_samples\n    cluster_id = 0\n\n    def get_neighbors(point_index):\n        distances = np.linalg.norm(data - data[point_index], axis=1)\n        return np.where(distances <= eps)[0]\n\n    def expand_cluster(point_index, neighbors):\n        nonlocal cluster_id\n        labels[point_index] = cluster_id\n        i = 0\n        while i < len(neighbors):\n            neighbor_index = neighbors[i]\n            if not visited[neighbor_index]:\n                visited[neighbor_index] = True\n                neighbor_neighbors = get_neighbors(neighbor_index)\n                if len(neighbor_neighbors) >= min_samples:\n                    neighbors = np.concatenate((neighbors, neighbor_neighbors))\n            if labels[neighbor_index] == -1:\n                labels[neighbor_index] = cluster_id\n            i += 1\n        cluster_id += 1\n\n    for point_index in range(n_samples):\n        if not visited[point_index]:\n            visited[point_index] = True\n            neighbors = get_neighbors(point_index)\n            if len(neighbors) >= min_samples:\n                expand_cluster(point_index, neighbors)\n\n    return labels\n\n# Example usage:\ndata = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\neps = 3\nmin_samples = 2\nlabels = dbscan(data, eps, min_samples)\nprint(labels)\n```\n\n### Explanation:\n1. **Initialization**:\n   - `labels` is initialized to `-1` for all points, indicating they are noise points.\n   - `visited` is initialized to `False` for all points, indicating they have not been visited yet.\n   - `cluster_id` is initialized to `0` to keep track of the current cluster index.\n\n2. **Helper Functions**:\n   - `get_neighbors(point_index)`: Returns the indices of all points within `eps` distance of the given point.\n   - `expand_cluster(point_index, neighbors)`: Expands the cluster starting from the given point, marking all reachable points with the current cluster label.\n\n3. **Main Loop**:\n   - Iterate over each point. If the point has not been visited, check if it is a core point.\n   - If it is a core point, call `expand_cluster` to mark all points in the cluster with the current cluster label and increment the cluster index.\n\nThis implementation ensures that the DBSCAN algorithm is correctly applied to the given data, following the rules for core points, border points, and noise points."}
{"task_id": 482, "completion_id": 0, "completion": "Here is a possible implementation of the `to_categorical` function:\n```python\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    y = np.array(y)\n    if num_classes is None:\n        num_classes = np.max(y) + 1\n    else:\n        if np.any(y < 0) or np.any(y >= num_classes):\n            raise ValueError(\"All labels must be non-negative and less than num_classes\")\n    one_hot = np.zeros((len(y), num_classes), dtype=int)\n    one_hot[np.arange(len(y)), y] = 1\n    return one_hot\n```\nThis implementation first converts the input `y` to a NumPy array. If `num_classes` is not provided, it is inferred as `max(y) + 1`. If `num_classes` is provided, it is checked for validity. Then, a 2-D array of zeros is created with shape `(len(y), num_classes)`. The one-hot encoding is achieved by setting the appropriate elements to 1."}
{"task_id": 485, "completion_id": 0, "completion": "Certainly! Below is a Python implementation of a univariate regression tree using the CART algorithm. The function `regression_tree_predict` builds a binary tree and uses it to predict the target values for the test set.\n\n```python\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    if not X_train or not y_train:\n        return []\n\n    class Node:\n        def __init__(self, feature_value=None, left=None, right=None, depth=0, mean=None):\n            self.feature_value = feature_value\n            self.left = left\n            self.right = right\n            self.depth = depth\n            self.mean = mean\n\n    def build_tree(X, y, depth):\n        if len(X) < 2 or depth == max_depth:\n            return Node(mean=np.mean(y))\n        \n        best_split_value = None\n        best_sse = float('inf')\n        \n        for i in range(len(X)):\n            split_value = X[i]\n            left_indices = [j for j in range(len(X)) if X[j] <= split_value]\n            right_indices = [j for j in range(len(X)) if X[j] > split_value]\n            \n            left_sse = sum((y[j] - np.mean(y[left_indices])) ** 2 for j in left_indices)\n            right_sse = sum((y[j] - np.mean(y[right_indices])) ** 2 for j in right_indices)\n            sse = left_sse + right_sse\n            \n            if sse < best_sse:\n                best_sse = sse\n                best_split_value = split_value\n        \n        if best_split_value is None:\n            return Node(mean=np.mean(y))\n        \n        left_indices = [j for j in range(len(X)) if X[j] <= best_split_value]\n        right_indices = [j for j in range(len(X)) if X[j] > best_split_value]\n        \n        left_node = build_tree([X[j] for j in left_indices], [y[j] for j in left_indices], depth + 1)\n        right_node = build_tree([X[j] for j in right_indices], [y[j] for j in right_indices], depth + 1)\n        \n        return Node(feature_value=best_split_value, left=left_node, right=right_node, depth=depth)\n    \n    def predict(node, x):\n        if node.feature_value is None:\n            return node.mean\n        if x <= node.feature_value:\n            return predict(node.left, x)\n        else:\n            return predict(node.right, x)\n    \n    root = build_tree(X_train, y_train, 0)\n    \n    predictions = [predict(root, x) for x in X_test]\n    \n    return predictions\n\n# Example usage:\nX_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny_train = [2, 4, 5, 4, 5, 6, 7, 8, 9, 10]\nX_test = [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.5]\nmax_depth = 3\n\npredictions = regression_tree_predict(X_train, y_train, X_test, max_depth)\nprint(predictions)\n```\n\n### Explanation:\n1. **Node Class**: Represents a node in the tree with attributes for the split value, left and right children, depth, and mean of the target values in the node.\n2. **build_tree Function**: Recursively builds the tree by finding the optimal split point that minimizes the sum of squared errors (SSE). It stops recursing when the depth reaches `max_depth` or when a node contains fewer than two samples.\n3. **predict Function**: Traverses the tree to predict the target value for a given test point.\n4. **regression_tree_predict Function**: Initializes the tree building and prediction process.\n\nThis implementation ensures that the tree is built and used to make predictions efficiently."}
{"task_id": 490, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    if not x:\n        return []\n    \n    # Convert input to NumPy array for easy manipulation\n    x_array = np.array(x)\n    \n    # Create a diagonal matrix using NumPy's diag function\n    diagonal_matrix = np.diag(x_array)\n    \n    # Convert the resulting NumPy array back to a list of lists\n    return diagonal_matrix.tolist()\n```\n\nThis function first checks if the input vector `x` is empty and returns an empty list if it is. Otherwise, it converts the input to a NumPy array for easy manipulation. It then uses NumPy's `diag` function to create a diagonal matrix with the elements of the input vector on the principal diagonal and zeros elsewhere. Finally, it converts the resulting NumPy array back to a list of lists and returns it."}
{"task_id": 491, "completion_id": 0, "completion": "To implement the FP-Growth algorithm, we need to follow the two major stages: FP-tree construction and recursive mining. Below is the complete implementation of the `fp_growth` function:\n\n```python\nfrom collections import Counter, defaultdict\nimport itertools\n\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # Step 1: Build the FP-tree and header table\n    item_counts = Counter(item for transaction in transactions for item in transaction)\n    frequent_items = {item for item, count in item_counts.items() if count >= min_sup}\n    sorted_frequent_items = sorted(frequent_items)\n    \n    # Filter transactions to only include frequent items\n    filtered_transactions = [[item for item in transaction if item in frequent_items] for transaction in transactions]\n    \n    # Build the FP-tree\n    fp_tree = defaultdict(list)\n    header_table = defaultdict(list)\n    \n    for transaction in filtered_transactions:\n        transaction.sort(key=lambda x: sorted_frequent_items.index(x))\n        for item in transaction:\n            fp_tree[item].append(transaction)\n            header_table[item].append(transaction)\n    \n    # Step 2: Recursive mining\n    def mine_fp_tree(prefix, fp_tree, header_table):\n        frequent_itemsets = []\n        for item in sorted(header_table.keys()):\n            prefix_path = []\n            current_node = header_table[item][-1]\n            while current_node:\n                prefix_path.append(current_node)\n                current_node = current_node[-1]\n            prefix_path.reverse()\n            \n            # Generate frequent itemsets from the prefix path\n            for i in range(len(prefix_path)):\n                prefix_itemset = tuple(sorted(prefix + [prefix_path[i][0]]))\n                frequent_itemsets.append(prefix_itemset)\n            \n            # Recursively mine the conditional FP-tree\n            conditional_fp_tree = defaultdict(list)\n            conditional_header_table = defaultdict(list)\n            for transaction in prefix_path:\n                conditional_transaction = [item for item in transaction if item > prefix_path[i][0]]\n                if conditional_transaction:\n                    conditional_transaction.sort(key=lambda x: sorted_frequent_items.index(x))\n                    for item in conditional_transaction:\n                        conditional_fp_tree[item].append(conditional_transaction)\n                        conditional_header_table[item].append(conditional_transaction)\n            \n            frequent_itemsets.extend(mine_fp_tree(prefix + [prefix_path[i][0]], conditional_fp_tree, conditional_header_table))\n        \n        return frequent_itemsets\n    \n    # Start mining from the empty prefix\n    all_frequent_itemsets = mine_fp_tree([], fp_tree, header_table)\n    \n    # Sort the frequent itemsets by length and lexicographically\n    all_frequent_itemsets.sort(key=lambda x: (len(x), x))\n    \n    return all_frequent_itemsets\n\n# Example usage\ntransactions = [\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"D\", \"E\"],\n    [\"A\", \"B\", \"C\", \"E\"],\n    [\"A\", \"B\", \"C\", \"D\", \"E\"],\n    [\"B\", \"C\", \"D\"]\n]\nmin_sup = 3\nprint(fp_growth(transactions, min_sup))\n```\n\n### Explanation:\n1. **FP-tree Construction**:\n   - We first count the frequency of each item and filter out items that do not meet the minimum support.\n   - We then filter the transactions to only include these frequent items.\n   - We build the FP-tree by inserting each transaction into the tree, maintaining the order of items based on their frequency.\n   - We also build a header table to keep track of the nodes that contain each item.\n\n2. **Recursive Mining**:\n   - We define a recursive function `mine_fp_tree` that takes a prefix, the current FP-tree, and the header table.\n   - For each item in the header table, we generate frequent itemsets by appending the item to the prefix and recursively mining the conditional FP-tree.\n   - If a conditional tree consists of a single path, we enumerate all non-empty combinations of the items on that path and add them to the result.\n\n3. **Sorting**:\n   - Finally, we sort the frequent itemsets by their length and lexicographically before returning them.\n\nThis implementation ensures that all frequent itemsets are mined and returned in the required format."}
{"task_id": 492, "completion_id": 0, "completion": "To solve this task, we need to generate all possible polynomial features up to a given degree for each sample in the input matrix \\( X \\). The features should be ordered first by their total degree and then lexicographically by the indices of the features.\n\nHere's a step-by-step approach to implement the function:\n\n1. **Check if the degree is negative**: If the degree is negative, return `-1`.\n2. **Initialize the result matrix**: Create an empty list to store the result.\n3. **Generate polynomial features**:\n   - For each sample in \\( X \\), generate all possible combinations of features up to the given degree.\n   - For each combination, compute the product of the features raised to the appropriate powers.\n   - Append the resulting feature vector to the result list.\n4. **Return the result**: Convert the result list to a list of lists and return it.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    if degree < 0:\n        return -1\n    \n    n_samples, n_features = len(X), len(X[0])\n    result = []\n    \n    for sample in X:\n        feature_vector = [1]  # Start with the degree-0 term (1)\n        for d in range(1, degree + 1):\n            for combo in combinations_with_replacement(range(n_features), d):\n                product = 1\n                for index in combo:\n                    product *= sample[index]\n                feature_vector.append(product)\n        result.append(feature_vector)\n    \n    return result\n\n# Example usage:\nX = [[1, 2], [3, 4]]\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\n### Explanation:\n1. **Check for negative degree**: If `degree` is negative, the function returns `-1`.\n2. **Initialize result**: An empty list `result` is initialized to store the feature vectors.\n3. **Iterate over samples**: For each sample in `X`, a feature vector starting with `[1]` (the degree-0 term) is created.\n4. **Generate combinations**: For each degree `d` from 1 to `degree`, generate all combinations of feature indices using `combinations_with_replacement`.\n5. **Compute products**: For each combination, compute the product of the features raised to the appropriate powers and append the result to the feature vector.\n6. **Store the feature vector**: Append the complete feature vector to the result list.\n7. **Return the result**: Convert the result list to a list of lists and return it.\n\nThis implementation ensures that the polynomial features are generated correctly and ordered as specified."}
{"task_id": 493, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    y = np.array(y)\n    \n    # Check if the array is empty\n    if y.size == 0:\n        return -1\n    \n    # Calculate the mean of the array\n    mean_y = np.mean(y)\n    \n    # Calculate the mean-squared error\n    mse_value = np.mean((y - mean_y) ** 2)\n    \n    # Return the result rounded to 4 decimal places\n    return round(mse_value, 4)\n```\n\nThis function first checks if the input array is empty and returns -1 if it is. If not, it calculates the mean of the array and then computes the mean-squared error by averaging the squared differences from the mean. The result is then rounded to four decimal places and returned."}
{"task_id": 496, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to NumPy array for element-wise operations\n    x = np.array(x)\n    \n    # Apply affine transformation\n    y = slope * x + intercept\n    \n    # First derivative is constant and equal to the slope\n    grad = np.full_like(x, slope)\n    \n    # Second derivative is zero for all elements\n    grad2 = np.zeros_like(x)\n    \n    # Round results to 4 decimal places\n    y = np.round(y, 4)\n    grad = np.round(grad, 4)\n    grad2 = np.round(grad2, 4)\n    \n    # Convert back to list and return\n    return y.tolist(), grad.tolist(), grad2.tolist()\n```\n\nThis function `apply_affine` takes a 1-D list or NumPy array `x`, and two optional scalars `slope` and `intercept`. It computes the affine transformation `y = slope * x + intercept`, the first derivative `grad = slope`, and the second derivative `grad2 = 0`. All results are rounded to 4 decimal places and returned as lists. If the input `x` is a scalar, it is treated as a length-one vector."}
{"task_id": 499, "completion_id": 0, "completion": "Here is a Python implementation of the PCA dimensionality reduction routine from scratch:\n```python\nimport numpy as np\n\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    n_samples, n_features = data.shape\n\n    # Step 1: Standardise each feature (zero mean, unit population variance)\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0, ddof=1)  # Bessel's correction\n    std[std == 0] = 1  # Avoid division by zero\n    data_standardised = (data - mean) / std\n\n    # Step 2: Compute the sample covariance matrix\n    covariance_matrix = np.cov(data_standardised, rowvar=False, ddof=1)\n\n    # Step 3: Perform an eigen-decomposition of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n\n    # Step 4: Sort eigenvalues in descending order and arrange the corresponding eigenvectors accordingly\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Fix the sign of every eigenvector so that its entry with the largest absolute value is positive\n    for i in range(eigenvectors.shape[1]):\n        max_abs_index = np.argmax(np.abs(eigenvectors[:, i]))\n        eigenvectors[:, i] *= np.sign(eigenvectors[max_abs_index, i])\n\n    # Step 6: Project the standardised data on the first k eigenvectors\n    if k < 1 or k > n_features:\n        return -1\n    projected_data = np.dot(data_standardised, eigenvectors[:, :k])\n\n    # Round to 4 decimal places and convert to a regular Python list\n    return projected_data.round(4).tolist()\n```\nThis implementation follows the steps outlined in the task description and returns the projected data rounded to four decimal places as a regular Python list. If `k` is outside the valid range, it returns `-1`."}
{"task_id": 500, "completion_id": 0, "completion": "To implement the Gradient Boosting Binary Classifier using decision stumps, we need to follow these steps:\n\n1. Initialize the model with the average of the training labels.\n2. For each iteration (up to `n_estimators`):\n   - Compute the residuals.\n   - Fit a decision stump to the residuals.\n   - Update the model by adding the contribution of the decision stump scaled by the learning rate.\n3. Predict the class labels for the test set by thresholding the final score at 0.5.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    \n    # Initialize the model with the average of the training labels\n    c = np.mean(y_train)\n    F = np.full_like(y_train, c)\n    \n    # Initialize predictions\n    predictions = np.zeros_like(y_train)\n    \n    for m in range(n_estimators):\n        # Compute residuals\n        residuals = y_train - F\n        \n        # Fit a decision stump to the residuals\n        best_stump = None\n        best_threshold = None\n        best_error = float('inf')\n        \n        for feature_index in range(X_train.shape[1]):\n            thresholds = np.unique(X_train[:, feature_index])\n            for threshold in thresholds:\n                # Predictions for the left and right sides of the threshold\n                left_pred = np.where(X_train[:, feature_index] <= threshold, 1, 0)\n                right_pred = np.where(X_train[:, feature_index] > threshold, 1, 0)\n                \n                # Calculate the error for both predictions\n                error_left = np.sum((residuals * left_pred) ** 2)\n                error_right = np.sum((residuals * right_pred) ** 2)\n                \n                # Choose the prediction with the lower error\n                if error_left < error_right:\n                    error = error_left\n                    pred = left_pred\n                else:\n                    error = error_right\n                    pred = right_pred\n                \n                # Update the best stump if this one has lower error\n                if error < best_error:\n                    best_error = error\n                    best_threshold = threshold\n                    best_stump = pred\n        \n        # Update the model\n        F += learning_rate * best_stump\n        predictions += learning_rate * best_stump\n    \n    # Predict the class labels for the test set\n    test_predictions = np.where(F >= 0.5, 1, 0)\n    \n    return test_predictions.tolist()\n\n# Example usage:\nX_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_train = [0, 0, 1, 1]\nX_test = [[2.5, 3.5], [3.5, 4.5]]\npredictions = gradient_boosting_classifier(X_train, y_train, X_test)\nprint(predictions)  # Output: [0, 1]\n```\n\n### Explanation:\n1. **Initialization**: We start by calculating the average of the training labels and initializing the model with this average.\n2. **Residual Calculation**: For each iteration, we compute the residuals as the difference between the actual labels and the current model predictions.\n3. **Decision Stump Fitting**: We fit a decision stump to the residuals by trying all possible thresholds for each feature. The stump that minimizes the squared error is chosen.\n4. **Model Update**: We update the model by adding the contribution of the decision stump scaled by the learning rate.\n5. **Prediction**: Finally, we predict the class labels for the test set by thresholding the final score at 0.5.\n\nThis implementation ensures that the model is built using decision stumps and that the predictions are made by thresholding the final score at 0.5."}
{"task_id": 505, "completion_id": 0, "completion": "To implement the Contrastive Divergence (CD-k) update for a binary Restricted Boltzmann Machine (RBM), we need to follow these steps:\n\n1. Compute the positive phase hidden probabilities.\n2. Run `k` full Gibbs sampling steps.\n3. Compute the positive and negative gradients.\n4. Update the weight matrix.\n5. Return the updated weight matrix rounded to 4 decimal places and converted to a plain Python list.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    if X.shape[0] == 0:\n        return []\n\n    n_visible, n_hidden = W.shape\n\n    # Compute positive phase hidden probabilities\n    h0_prob = 1 / (1 + np.exp(-(np.dot(X, W) + hbias)))\n\n    # Initialize positive gradient\n    pos_grad = np.dot(X.T, h0_prob)\n\n    # Gibbs sampling\n    for _ in range(k):\n        # Sample visible units from hidden units\n        v_k_prob = 1 / (1 + np.exp(-(np.dot(h0_prob, W.T) + vbias)))\n        v_k = (v_k_prob > np.random.rand(v_k_prob.shape[0], v_k_prob.shape[1])).astype(int)\n\n        # Sample hidden units from visible units\n        h_k_prob = 1 / (1 + np.exp(-(np.dot(v_k, W) + hbias)))\n\n    # Compute negative gradient\n    neg_grad = np.dot(v_k.T, h_k_prob)\n\n    # Update weight matrix\n    W_new = W + learning_rate * (pos_grad - neg_grad) / X.shape[0]\n\n    # Round to 4 decimal places and convert to list of lists\n    W_new_rounded = np.round(W_new, 4).tolist()\n\n    return W_new_rounded\n```\n\n### Explanation:\n\n1. **Positive Phase Hidden Probabilities**:\n   - Compute the hidden probabilities given the visible units using the sigmoid function.\n\n2. **Gibbs Sampling**:\n   - Perform `k` full Gibbs sampling steps:\n     - Sample visible units from the hidden units using the sigmoid function.\n     - Sample hidden units from the visible units using the sigmoid function.\n\n3. **Gradients**:\n   - Compute the positive gradient using the visible units and the hidden probabilities.\n   - Compute the negative gradient using the sampled visible units and the hidden probabilities.\n\n4. **Weight Update**:\n   - Update the weight matrix using the learning rate and the difference between the positive and negative gradients.\n\n5. **Output**:\n   - Round the updated weight matrix to 4 decimal places and convert it to a plain Python list of lists.\n\nThis implementation ensures that the weight matrix is updated according to the Contrastive Divergence (CD-k) algorithm for a binary Restricted Boltzmann Machine."}
{"task_id": 509, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    # Convert inputs to numpy arrays if they are not already\n    x = np.array(x)\n    y = np.array(y)\n    \n    # Check if the vectors have the same length\n    if x.shape != y.shape:\n        return -1\n    \n    # Compute the Chebyshev distance\n    distance = np.max(np.abs(x - y))\n    \n    # Return the distance rounded to four decimal places\n    return round(distance, 4)\n```\n\nThis function first ensures that both input vectors are converted to NumPy arrays for efficient computation. It then checks if the vectors have the same length. If they do not, the function returns -1. If they do, it computes the Chebyshev distance by finding the maximum absolute difference between corresponding elements of the vectors and returns this value rounded to four decimal places."}
{"task_id": 510, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    epsilon = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    \n    # Reconstruction loss\n    reconstruction_loss = -np.sum(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped), axis=1)\n    \n    # KL Divergence\n    kl_divergence = -0.5 * np.sum(1 + t_log_var - t_mean**2 - np.exp(t_log_var), axis=1)\n    \n    # Average loss\n    average_loss = np.mean(reconstruction_loss + kl_divergence)\n    \n    return round(average_loss, 6)\n```\n\nThis function calculates the VAE loss for Bernoulli visible units by computing both the reconstruction loss and the KL divergence, then averaging them. The reconstruction loss is computed using the binary cross-entropy, and the KL divergence is computed using the analytical formula provided. The function ensures numerical stability by clipping the predicted probabilities and rounds the final average loss to six decimal places."}
{"task_id": 513, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    # Convert input lists to numpy arrays for efficient computation\n    X = np.array(X)\n    w = np.array(w)\n    V = np.array(V)\n    \n    # Calculate the linear part of the FM model\n    linear_part = np.dot(X, w)\n    \n    # Calculate the interaction part of the FM model\n    interaction_part = 0.5 * np.sum((np.dot(X, V) ** 2 - np.dot(X ** 2, V ** 2)), axis=1)\n    \n    # Combine the linear and interaction parts and add the bias term\n    predictions = w0 + linear_part + interaction_part\n    \n    # Round the predictions to 4 decimal places\n    predictions = np.round(predictions, 4)\n    \n    # Convert the predictions to a list and return\n    return predictions.tolist()\n```\n\nThis function first converts the input lists to numpy arrays for efficient numerical computations. It then calculates the linear part of the FM model by taking the dot product of the feature matrix `X` with the weight vector `w`. The interaction part is calculated by taking the dot product of `X` with `V`, squaring the result, and subtracting the square of the dot product of `X` squared with `V` squared. The bias term `w0` is added to both parts, and the final predictions are rounded to 4 decimal places before being returned as a list."}
{"task_id": 517, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # Convert input to numpy array for element-wise operations\n    z_array = np.array(z)\n    \n    # Compute the sigmoid using the formula\n    # Use np.clip to avoid overflow for very large positive or negative values\n    sigmoid_values = 1 / (1 + np.exp(-np.clip(z_array, -700, 700)))\n    \n    # Round the results to four decimal places\n    sigmoid_values_rounded = np.round(sigmoid_values, 4)\n    \n    # Check if the input was a scalar and return the appropriate type\n    if isinstance(z, (int, float)):\n        return float(sigmoid_values_rounded)\n    else:\n        return sigmoid_values_rounded.tolist()\n\n# Example usage:\nprint(sigmoid([-1, 0, 1]))  # Output: [0.2689, 0.5, 0.7311]\n```\n\n### Explanation:\n1. **Input Handling**: The function first converts the input `z` to a NumPy array to facilitate element-wise operations.\n2. **Numerical Stability**: The `np.clip` function is used to limit the values of `z` between -700 and 700. This prevents overflow issues when computing the exponential function.\n3. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( g(z) = \\frac{1}{1 + e^{-z}} \\).\n4. **Rounding**: The results are rounded to four decimal places using `np.round`.\n5. **Output Type**: The function checks if the input was a scalar and returns a float if true, otherwise it returns a list using `tolist()`.\n\nThis implementation ensures that the function works correctly for scalars, lists, tuples, and NumPy arrays, and it handles large values of `z` numerically stably."}
{"task_id": 518, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> any:\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = np.argmax(actual, axis=1)\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = np.argmax(predicted, axis=1)\n        return function(actual, predicted)\n    \n    return wrapper\n```\n\nThis solution defines the `unhot` decorator, which checks if the `actual` and `predicted` arrays are one-hot encoded by verifying if they are two-dimensional and have more than one column. If they are, it converts them to their corresponding integer label representations using `np.argmax`. The decorator then calls the original function with the processed arrays and returns its result unchanged."}
{"task_id": 520, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to numpy array for uniform processing\n    z_array = np.array(z)\n    \n    # Compute the gradient\n    gradient = z_array * (1 - z_array)\n    \n    # Round the results to 4 decimal places\n    rounded_gradient = np.round(gradient, 4)\n    \n    # Check if the input was a scalar and return the appropriate type\n    if isinstance(z, (int, float)):\n        return rounded_gradient.item()\n    else:\n        return rounded_gradient.tolist()\n```\n\nThis function first converts the input to a NumPy array to handle the computation efficiently. It then computes the gradient using the formula \\( g'(z) = z \\cdot (1 - z) \\). The results are rounded to 4 decimal places using `np.round`. Finally, the function checks if the input was a scalar and returns the result as a float if it was, or as a list otherwise. This ensures that the output structure matches the input structure."}
{"task_id": 528, "completion_id": 0, "completion": "To solve this task, we need to follow these steps:\n\n1. Extract the minimum and maximum values of the features in the dataset `X` to determine the range for the grid.\n2. Create two 1-D arrays `x1_plot` and `x2_plot` that span the range of the features in `X` and have `grid_n` points each.\n3. Create a meshgrid from these two arrays to get all possible combinations of points.\n4. Classify each point in the meshgrid using the given linear classifier rule.\n5. Store the labels in a 2-D list and return it.\n\nHere is the implementation of the function:\n\n```python\nimport numpy as np\n\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # Extract the minimum and maximum values of the features in X\n    x1_min, x1_max = min(point[0] for point in X), max(point[0] for point in X)\n    x2_min, x2_max = min(point[1] for point in X), max(point[1] for point in X)\n    \n    # Create 1-D arrays for the grid points\n    x1_plot = np.linspace(x1_min, x1_max, grid_n)\n    x2_plot = np.linspace(x2_min, x2_max, grid_n)\n    \n    # Create a meshgrid\n    x1_grid, x2_grid = np.meshgrid(x1_plot, x2_plot)\n    \n    # Flatten the meshgrid to apply the classifier rule\n    x1_flat = x1_grid.flatten()\n    x2_flat = x2_grid.flatten()\n    \n    # Apply the classifier rule\n    labels = np.sign(W[0] * x1_flat + W[1] * x2_flat + b)\n    \n    # Reshape the labels back to the grid shape\n    labels_grid = labels.reshape(grid_n, grid_n).tolist()\n    \n    return labels_grid\n\n# Example usage:\nX = [[-1, -1], [-1, 1], [1, -1], [1, 1]]\nW = [1, 1]\nb = 0\ngrid_n = 100\nresult = decision_boundary_grid(X, W, b, grid_n)\nprint(result)\n```\n\n### Explanation:\n1. **Extracting Min and Max Values**: We find the minimum and maximum values of the features in `X` to determine the range for the grid.\n2. **Creating Grid Points**: We use `np.linspace` to create `grid_n` points between the minimum and maximum values for each feature.\n3. **Creating Meshgrid**: We use `np.meshgrid` to create a grid of points.\n4. **Flattening and Classifying**: We flatten the meshgrid to apply the classifier rule to each point. The classifier rule is applied using `np.sign`.\n5. **Reshaping**: We reshape the labels back to the grid shape and convert it to a Python list of lists.\n\nThis function will generate a decision boundary grid for a given linear classifier and dataset."}
{"task_id": 537, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Gaussian Naive Bayes classifier from scratch in Python:\n\n```python\nimport numpy as np\n\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    n_samples, n_features = X_train.shape\n    classes = np.unique(y_train)\n    epsilon = 1e-9\n\n    # Initialize parameters\n    means = {}\n    stds = {}\n    priors = {}\n\n    # Training phase: estimate parameters for each class\n    for c in classes:\n        X_c = X_train[y_train == c]\n        means[c] = np.mean(X_c, axis=0)\n        stds[c] = np.std(X_c, axis=0) + epsilon\n        priors[c] = np.mean(y_train == c)\n\n    # Prediction phase: compute posterior probabilities and predict labels\n    predictions = []\n    for x in X_test:\n        posteriors = {}\n        for c in classes:\n            likelihood = np.prod(1 / np.sqrt(2 * np.pi * stds[c]) * np.exp(-(x - means[c]) ** 2 / (2 * stds[c] ** 2)))\n            posterior = likelihood * priors[c]\n            posteriors[c] = posterior\n        predicted_class = max(posteriors, key=posteriors.get)\n        predictions.append(predicted_class)\n\n    return predictions\n\n# Example usage:\n# X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n# y_train = np.array([0, 0, 1, 1, 1])\n# X_test = np.array([[2.5, 3.5]])\n# predictions = gaussian_naive_bayes(X_train, y_train, X_test)\n# print(predictions)  # Output: [1]\n```\n\n### Explanation:\n1. **Training Phase**:\n   - We calculate the mean and standard deviation for each feature of both classes.\n   - We also compute the class prior as the relative frequency of each class in the training set.\n   - To avoid division by zero, we add a small constant `epsilon` to every standard deviation.\n\n2. **Prediction Phase**:\n   - For each test sample, we compute the likelihood of the sample belonging to each class using the Gaussian likelihood formula.\n   - We then compute the posterior probability for each class by multiplying the likelihood by the class prior.\n   - Finally, we predict the class with the highest posterior probability.\n\nThis implementation should work correctly for the given task and constraints."}
{"task_id": 539, "completion_id": 0, "completion": "Certainly! Below is the implementation of the binary cross-entropy cost function in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"\n    Compute the binary cross-entropy cost.\n\n    Parameters:\n    AL (np.ndarray): The predicted probabilities of the positive class, shape (1, m) or (m,)\n    Y (np.ndarray): The true binary labels, shape (1, m) or (m,)\n\n    Returns:\n    float: The computed binary cross-entropy cost\n    \"\"\"\n    # Ensure the arrays are 1D\n    AL = AL.flatten()\n    Y = Y.flatten()\n    \n    # Number of training examples\n    m = AL.shape[0]\n    \n    # Clip the probabilities to avoid numerical overflow\n    AL = np.clip(AL, 1e-15, 1 - 1e-15)\n    \n    # Compute the cost\n    cost = -1 / m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n    \n    # Return the cost rounded to 4 decimal places\n    return round(cost, 4)\n\n# Example usage:\nAL = np.array([[0.9, 0.1, 0.8]])\nY = np.array([[1, 0, 1]])\nprint(compute_cost(AL, Y))  # Output should be close to 0.4159\n```\n\n### Explanation:\n1. **Flattening the Arrays**: The input arrays `AL` and `Y` are flattened to ensure they are 1D, which simplifies the computation.\n2. **Number of Training Examples**: The number of training examples `m` is determined.\n3. **Clipping Probabilities**: The probabilities `AL` are clipped to the range `[1e-15, 1 - 1e-15]` to avoid numerical overflow when taking the logarithm.\n4. **Cost Calculation**: The binary cross-entropy cost is computed using the formula provided.\n5. **Rounding**: The final cost is rounded to 4 decimal places before being returned.\n\nThis function should work correctly for the given task and handle the edge cases as specified."}
{"task_id": 552, "completion_id": 0, "completion": "To solve this problem, we need to implement the forward-backward algorithm to compute the posterior probabilities of being in each state at a given time step \\( t \\). Here's a step-by-step breakdown of the implementation:\n\n1. **Forward Probabilities (\u03b1):** Compute the probability of the partial observation sequence up to time \\( t \\) and being in state \\( i \\) at time \\( t \\).\n2. **Backward Probabilities (\u03b2):** Compute the probability of the remaining observation sequence from time \\( t+1 \\) given that we are in state \\( i \\) at time \\( t \\).\n3. **Posterior Probabilities (\u03b3):** Combine the forward and backward probabilities to get the posterior probability of being in state \\( i \\) at time \\( t \\).\n4. **Normalization:** Normalize the posterior probabilities to ensure they sum to 1.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    # Extract HMM parameters\n    pi = hmm['pi']\n    A = hmm['A']\n    B = hmm['B']\n    num_states = len(pi)\n    num_observations = len(observations)\n    \n    # Check if t is within the valid range\n    if t < 0 or t >= num_observations:\n        return []\n    \n    # Initialize forward and backward probabilities\n    alpha = np.zeros(num_states)\n    beta = np.zeros(num_states)\n    \n    # Forward probabilities\n    for i in range(num_states):\n        alpha[i] = pi[i] * B[i, observations[t]]\n    \n    # Normalize forward probabilities\n    alpha /= np.sum(alpha)\n    \n    # Backward probabilities\n    for i in range(num_states):\n        beta[i] = 1.0\n    \n    # Normalize backward probabilities\n    beta /= np.sum(beta)\n    \n    # Compute gamma probabilities\n    gamma = np.zeros(num_states)\n    for i in range(num_states):\n        gamma[i] = alpha[i] * beta[i]\n    \n    # Normalize gamma probabilities\n    gamma /= np.sum(gamma)\n    \n    # Round to four decimal places\n    gamma = [round(g, 4) for g in gamma]\n    \n    return gamma\n\n# Example usage:\nhmm = {\n    'pi': np.array([0.6, 0.4]),\n    'A': np.array([[0.7, 0.3],\n                   [0.4, 0.6]]),\n    'B': np.array([[0.1, 0.4, 0.5],\n                   [0.7, 0.2, 0.1]])\n}\n\nobservations = [0, 1, 2]\nt = 1\n\nprint(gamma_probabilities(hmm, observations, t))\n```\n\n### Explanation:\n1. **Initialization:** We initialize the forward and backward probabilities to zero arrays.\n2. **Forward Probabilities:** We compute the forward probabilities using the initial distribution \\( \\pi \\) and the emission probabilities \\( B \\).\n3. **Backward Probabilities:** We initialize the backward probabilities to 1 and normalize them.\n4. **Gamma Probabilities:** We compute the gamma probabilities by multiplying the forward and backward probabilities and normalizing them.\n5. **Rounding:** We round the gamma probabilities to four decimal places and return them as a list.\n\nThis implementation ensures that the posterior probabilities are computed correctly and rounded to the required precision."}
{"task_id": 555, "completion_id": 0, "completion": "To implement the DBSCAN algorithm from scratch, we need to follow the steps outlined in the task description. Below is the complete implementation of the `dbscan` function:\n\n```python\nimport numpy as np\nfrom collections import deque\n\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples, n_features = data.shape\n    labels = np.full(n_samples, -1, dtype=int)\n    cluster_id = 0\n\n    def get_neighbors(point_index):\n        neighbors = []\n        for i in range(n_samples):\n            if np.linalg.norm(data[point_index] - data[i]) <= eps:\n                neighbors.append(i)\n        return neighbors\n\n    for i in range(n_samples):\n        if labels[i] != -1:\n            continue\n\n        neighbors = get_neighbors(i)\n        if len(neighbors) < min_samples:\n            labels[i] = -1\n        else:\n            cluster_id += 1\n            labels[i] = cluster_id\n            queue = deque(neighbors)\n\n            while queue:\n                current_point = queue.popleft()\n                current_neighbors = get_neighbors(current_point)\n                if len(current_neighbors) >= min_samples:\n                    for neighbor in current_neighbors:\n                        if labels[neighbor] == -1:\n                            labels[neighbor] = cluster_id\n                            queue.append(neighbor)\n                        elif labels[neighbor] == 0:\n                            labels[neighbor] = cluster_id\n\n    return labels.tolist()\n\n# Example usage:\ndata = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\neps = 3\nmin_samples = 2\nlabels = dbscan(data, eps, min_samples)\nprint(labels)\n```\n\n### Explanation:\n1. **Initialization**:\n   - `labels` is initialized to `-1` for all points, indicating that they are initially unassigned.\n   - `cluster_id` is initialized to `0` to keep track of the current cluster number.\n\n2. **Helper Function**:\n   - `get_neighbors(point_index)`: This function computes the indices of all points within the `eps` radius of the given point.\n\n3. **Main Loop**:\n   - Iterate over each point in the dataset.\n   - If the point is already labeled, skip it.\n   - Get the neighbors of the current point.\n   - If the number of neighbors is less than `min_samples`, label the point as noise (`-1`).\n   - Otherwise, label the point as a new cluster and perform a breadth-first search (BFS) to expand the cluster:\n     - Add the point to the queue.\n     - While the queue is not empty, pop a point from the queue and add all its unvisited neighbors to the cluster.\n     - If a neighbor is a core point, add it to the queue to continue expanding the cluster.\n\n4. **Return**:\n   - The function returns the list of cluster labels for each sample.\n\nThis implementation ensures that the DBSCAN algorithm correctly identifies clusters and noise points based on the given parameters."}
{"task_id": 561, "completion_id": 0, "completion": "To implement the Gaussian Mixture Model (GMM) via the Expectation-Maximization (EM) algorithm, we need to follow these steps:\n\n1. **Initialization**: Randomly initialize the parameters of the K Gaussian components (mixture weights, means, and full covariance matrices).\n2. **Expectation (E) Step**: Compute the responsibilities (posterior probabilities) of each sample belonging to each Gaussian component.\n3. **Maximization (M) Step**: Update the parameters (mixture weights, means, and covariance matrices) based on the responsibilities.\n4. **Convergence Check**: Check if the change in mixture weights is smaller than the prescribed tolerance or if the maximum number of iterations is reached.\n5. **Assignment**: Assign each sample to the component with the largest responsibility.\n6. **Sorting and Relabelling**: Sort the components by the first coordinate of their mean in ascending order and re-label the cluster indices accordingly.\n\nHere is the complete implementation of the GMM via EM algorithm:\n\n```python\nimport numpy as np\nimport math\n\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    N, D = data.shape\n    \n    # Initialize parameters\n    pi = np.ones(k) / k  # Mixture weights\n    mu = data[np.random.choice(N, k, replace=False)]  # Means\n    sigma = np.array([np.eye(D) for _ in range(k)])  # Covariance matrices\n    \n    for iteration in range(max_iter):\n        # E-step: Compute responsibilities\n        responsibilities = np.zeros((N, k))\n        for i in range(k):\n            det_sigma = np.linalg.det(sigma[i])\n            inv_sigma = np.linalg.inv(sigma[i])\n            diff = data - mu[i]\n            exponent = -0.5 * np.sum(diff @ inv_sigma * diff, axis=1)\n            responsibilities[:, i] = pi[i] * (1 / (np.sqrt(2 * math.pi * det_sigma))) * np.exp(exponent)\n        \n        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n        \n        # M-step: Update parameters\n        Nk = responsibilities.sum(axis=0)\n        pi = Nk / N\n        \n        mu = np.dot(responsibilities.T, data) / Nk[:, np.newaxis]\n        \n        sigma = np.zeros_like(sigma)\n        for i in range(k):\n            diff = data - mu[i]\n            sigma[i] = np.dot(responsibilities[:, i] * diff.T, diff) / Nk[i]\n        \n        # Check for convergence\n        if np.allclose(pi, pi_old, atol=epsilon):\n            break\n        pi_old = pi.copy()\n    \n    # Assignment: Assign each sample to the component with the largest responsibility\n    labels = responsibilities.argmax(axis=1)\n    \n    # Sorting and relabelling\n    sorted_indices = np.argsort(mu[:, 0])\n    labels = np.argsort(sorted_indices)[labels]\n    \n    return labels.tolist()\n```\n\n### Explanation:\n1. **Initialization**:\n   - `pi`: Mixture weights are initialized uniformly.\n   - `mu`: Means are initialized randomly from the data.\n   - `sigma`: Covariance matrices are initialized as identity matrices.\n\n2. **E-step**:\n   - Compute the responsibilities for each sample belonging to each Gaussian component using the formula for the Gaussian distribution.\n\n3. **M-step**:\n   - Update the mixture weights, means, and covariance matrices based on the responsibilities.\n\n4. **Convergence Check**:\n   - Check if the change in mixture weights is smaller than the prescribed tolerance (`epsilon`).\n\n5. **Assignment**:\n   - Assign each sample to the component with the largest responsibility.\n\n6. **Sorting and Relabelling**:\n   - Sort the components by the first coordinate of their mean in ascending order and re-label the cluster indices accordingly.\n\nThis implementation ensures that the output is deterministic for identical inputs by sorting the components and re-labeling the cluster indices."}
{"task_id": 562, "completion_id": 0, "completion": "Here is a Python implementation of the Spectral Clustering algorithm:\n\n```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.sparse.linalg import eigs\nfrom sklearn.cluster import KMeans\n\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    if n_cluster == 1:\n        return [0] * len(data)\n    \n    # Step 1: Build a fully-connected similarity graph using the Gaussian kernel\n    pairwise_distances = squareform(pdist(data, 'euclidean'))\n    similarities = np.exp(-gamma * pairwise_distances ** 2)\n    \n    # Step 2: Construct the un-normalised Laplacian L = D - W\n    W = csr_matrix(similarities)\n    D = diags(W.sum(axis=1))\n    \n    if method == 'normalized':\n        # Step 2 (normalized): Construct the symmetric normalized Laplacian L_sym = D^(-1/2) L D^(-1/2)\n        D_inv_sqrt = diags(1 / np.sqrt(D.diagonal()))\n        L_sym = D_inv_sqrt @ D @ D_inv_sqrt\n        eigenvalues, eigenvectors = eigs(L_sym, k=n_cluster, which='SM')\n    else:\n        # Step 2 (unnormalized): Construct the un-normalised Laplacian L = D - W\n        L = D - W\n        eigenvalues, eigenvectors = eigs(L, k=n_cluster, which='SM')\n    \n    # Step 3: Compute the eigenvectors that correspond to the n_cluster smallest eigen-values\n    eigenvectors = eigenvectors.real\n    \n    # Step 4: Run k-means in the eigenvector space to obtain final cluster labels\n    kmeans = KMeans(n_clusters=n_cluster, init=eigenvectors[:n_cluster].T, n_init=1)\n    labels = kmeans.fit_predict(eigenvectors)\n    \n    # Step 5: Relabel clusters so that the cluster containing the smallest original index gets label 0, the next one 1, etc.\n    original_indices = np.argsort(np.argsort(data[:, 0]))\n    relabel_map = {original_indices[i]: i for i in range(n_cluster)}\n    labels = [relabel_map[label] for label in labels]\n    \n    return labels\n```\n\n### Explanation:\n1. **Similarity Graph Construction**: The pairwise distances between points are computed using `pdist` and `squareform`. The Gaussian kernel is then applied to these distances to get the similarity matrix `W`.\n2. **Laplacian Construction**: Depending on the `method`, either the unnormalized or normalized Laplacian is constructed.\n3. **Eigenvalue Decomposition**: The eigenvalues and eigenvectors of the Laplacian are computed using `eigs`.\n4. **K-means Clustering**: K-means is run in the eigenvector space using the first `n_cluster` eigenvectors as initial centroids.\n5. **Relabeling**: The cluster labels are relabeled to ensure that the cluster containing the smallest original index gets label 0, the next one 1, etc.\n\nThis implementation ensures that the Spectral Clustering algorithm is performed without using any third-party machine-learning libraries, adhering to the constraints provided."}
{"task_id": 563, "completion_id": 0, "completion": "Here is the implementation of the Dual-form Perceptron learning algorithm in Python:\n\n```python\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    N, d = X_data.shape\n    alpha = np.zeros(N)\n    b = 0.0\n    \n    # Build the Gram matrix G\n    G = np.dot(X_data, X_data.T)\n    \n    for _ in range(max_iter):\n        updated = False\n        for i in range(N):\n            g = np.sum(alpha * y_data * G[:, i])\n            if y_data[i] * (g + b) <= 0:\n                alpha[i] += eta\n                b += eta * y_data[i]\n                updated = True\n        \n        if not updated:\n            break\n    \n    # Recover the weight vector\n    w = np.sum(alpha * y_data * X_data, axis=0)\n    \n    # Round the weight vector and bias term\n    w = [round(w_i, 4) for w_i in w]\n    b = round(b, 4)\n    \n    return w, b\n\n# Example usage:\n# X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y_data = np.array([1, 1, -1, -1])\n# w, b = perceptron_dual_train(X_data, y_data)\n# print(w, b)\n```\n\n### Explanation:\n1. **Initialization**: We initialize `alpha` to a zero vector of length `N` and `b` to 0.0.\n2. **Gram Matrix**: We compute the Gram matrix `G` where `G[i, j] = X_data[i] \u00b7 X_data[j]`.\n3. **Training Loop**: We iterate up to `max_iter` times. For each sample `i`, we compute `g = \u03a3\u2c7c \u03b1\u2c7c y\u2c7c G[j, i]`. If `y\u1d62 (g + b) \u2264 0`, we update `alpha[i]` and `b` and set `updated` to `True`.\n4. **Convergence Check**: If no updates are made in a pass, we break out of the loop.\n5. **Weight Vector Recovery**: After the loop, we recover the weight vector `w` as `\u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62`.\n6. **Rounding**: We round each component of `w` and `b` to 4 decimal places and return them as a tuple.\n\nThis implementation follows the dual-form Perceptron learning algorithm as described in the task."}
{"task_id": 564, "completion_id": 0, "completion": "```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n    current_node = root\n    while current_node.child:\n        if task_type == 'classification':\n            feature_value = features[current_node.label]\n            for child in current_node.child:\n                if child.x == feature_value:\n                    current_node = child\n                    break\n            else:\n                current_node = current_node.child[1]\n        elif task_type == 'regression':\n            feature_value = features[current_node.label]\n            if feature_value <= current_node.s:\n                current_node = current_node.child[0]\n            else:\n                current_node = current_node.child[1]\n    return current_node.y\n```\n\nThis function `predict_sample` traverses the decision tree based on the given `task_type` and the feature values of the sample. It follows the rules specified for classification and regression trees to determine the path to the leaf node and returns the value stored in that leaf."}
{"task_id": 565, "completion_id": 0, "completion": "To implement the `choose_best_feature` function, we need to calculate the information gain for each feature and then select the feature with the highest information gain. If there are multiple features with the same highest information gain, we will choose the one with the smallest column index.\n\nHere's the implementation of the function:\n\n```python\nimport numpy as np\nimport math\n\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    def entropy(p):\n        \"\"\"Calculate the entropy of a probability distribution.\"\"\"\n        return -np.sum(p * np.log2(p + 1e-10))\n\n    def conditional_entropy(y, A):\n        \"\"\"Calculate the conditional entropy of y given A.\"\"\"\n        unique_A = np.unique(A)\n        total_entropy = 0.0\n        for a in unique_A:\n            mask = A == a\n            p_y_given_a = np.mean(y[mask])\n            total_entropy += p_y_given_a * entropy([p_y_given_a, 1 - p_y_given_a])\n        return total_entropy\n\n    def information_gain(y, A):\n        \"\"\"Calculate the information gain of feature A.\"\"\"\n        p_y = np.mean(y)\n        return entropy([p_y, 1 - p_y]) - conditional_entropy(y, A)\n\n    n_samples, n_features = X.shape\n    max_gain = -np.inf\n    best_feature = -1\n\n    for feature in range(n_features):\n        gain = information_gain(y, X[:, feature])\n        if gain > max_gain:\n            max_gain = gain\n            best_feature = feature\n\n    return best_feature, round(max_gain, 6)\n\n# Example usage:\nX = np.array([[0, 1, 2],\n              [1, 0, 1],\n              [2, 1, 0],\n              [0, 0, 2],\n              [1, 1, 1]])\ny = np.array([0, 1, 0, 1, 0])\n\nbest_feature, max_gain = choose_best_feature(X, y)\nprint(f\"Best feature: {best_feature}, Max information gain: {max_gain}\")\n```\n\n### Explanation:\n1. **Entropy Calculation**: The `entropy` function calculates the entropy of a probability distribution. We use `np.log2` to compute the base-2 logarithm and add a small constant (`1e-10`) to avoid log(0) errors.\n\n2. **Conditional Entropy Calculation**: The `conditional_entropy` function calculates the conditional entropy of the label `y` given a feature `A`. It iterates over each unique value of `A`, calculates the probability of `y` given `A`, and sums up the weighted entropies.\n\n3. **Information Gain Calculation**: The `information_gain` function calculates the information gain of a feature `A` by subtracting the conditional entropy from the entropy of the label distribution.\n\n4. **Feature Selection**: The `choose_best_feature` function iterates over each feature, calculates its information gain, and keeps track of the feature with the highest gain. If two features have the same gain, the one with the smallest index is chosen.\n\n5. **Output**: The function returns a tuple containing the index of the best feature and the maximum information gain rounded to 6 decimal places.\n\nThis implementation ensures that the function correctly calculates the information gain for each feature and selects the best feature based on the given criteria."}
{"task_id": 566, "completion_id": 0, "completion": "To implement the Mini Isolation Forest algorithm for outlier detection, we need to follow the steps outlined in the task description. Below is the complete implementation of the `detect_outliers` function:\n\n```python\nimport numpy as np\nimport math\n\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    if epsilon == 0:\n        return []\n\n    np.random.seed(42)\n\n    def build_tree(subsample):\n        if len(subsample) <= 1:\n            return None\n\n        if len(subsample) == 2:\n            return (subsample[0], subsample[1])\n\n        depth = math.ceil(math.log2(len(subsample)))\n        feature_index = np.random.randint(0, subsample.shape[1])\n        feature_values = subsample[:, feature_index]\n        threshold = np.random.uniform(np.min(feature_values), np.max(feature_values))\n\n        left_indices = np.where(subsample[:, feature_index] < threshold)[0]\n        right_indices = np.where(subsample[:, feature_index] >= threshold)[0]\n\n        left_subsample = subsample[left_indices]\n        right_subsample = subsample[right_indices]\n\n        return (feature_index, threshold, build_tree(left_subsample), build_tree(right_subsample))\n\n    def path_length(node, observation):\n        if node is None:\n            return 0\n        if node[1] is None:\n            return 1 + c(len(node[0]))\n        if observation[node[0]] < node[1]:\n            return 1 + path_length(node[2], observation)\n        else:\n            return 1 + path_length(node[3], observation)\n\n    def c(n):\n        if n == 1:\n            return 0\n        return 2 * math.log(n - 1) + 0.5772156649 - 2 * (n - 1) / n\n\n    trees = [build_tree(np.random.choice(data, sample_size, replace=False)) for _ in range(n_trees)]\n    path_lengths = np.zeros(data.shape[0])\n\n    for tree in trees:\n        for i, observation in enumerate(data):\n            path_lengths[i] += path_length(tree, observation)\n\n    average_path_lengths = path_lengths / n_trees\n    phi = 2 * math.log(data.shape[0] - 1) - 2 * (data.shape[0] - 1) / data.shape[0]\n    scores = 2 ** (-average_path_lengths / phi)\n\n    threshold = np.percentile(scores, 100 * (1 - epsilon))\n    outliers = np.where(scores >= threshold)[0]\n\n    return sorted(outliers)\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [100, 100], [3, 4], [4, 5]])\nn_trees = 10\nsample_size = 3\nepsilon = 0.2\noutliers = detect_outliers(data, n_trees, sample_size, epsilon)\nprint(outliers)\n```\n\n### Explanation:\n1. **Initialization**: The function initializes the random seed for reproducibility and checks if `epsilon` is zero, in which case it returns an empty list.\n2. **Tree Building**: The `build_tree` function recursively builds an isolation tree. It stops when the depth reaches the height limit or when a node contains at most one sample.\n3. **Path Length Calculation**: The `path_length` function calculates the path length of an observation in a given tree.\n4. **C Function**: The `c` function calculates the approximation of the expected path length for unsuccessful searches in a binary search tree.\n5. **Score Calculation**: The function builds multiple trees, calculates the average path length for each observation, and converts it to an anomaly score using the formula provided.\n6. **Outlier Detection**: The function determines the threshold based on the `epsilon` fraction and identifies the outliers.\n\nThis implementation ensures that the function is reproducible and meets the requirements specified in the task description."}
{"task_id": 568, "completion_id": 0, "completion": "To implement the Maximum Entropy (MaxEnt) classifier using the Generalized Iterative Scaling (GIS) algorithm, we need to follow these steps:\n\n1. **Build the empirical distributions**: Calculate the empirical distribution of the labels and the empirical distribution of the feature-value-label tuples.\n2. **Iteratively update the weight vector**: Use the GIS algorithm to iteratively update the weight vector until the largest absolute update is smaller than `epsilon` or the number of iterations reaches `n_iter`.\n3. **Predict the labels**: For each sample in the test set, compute the conditional probabilities and predict the label with the highest probability.\n\nHere is the complete implementation of the `maxent_predict` function:\n\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    # Step 1: Build the empirical distributions\n    num_samples, num_features = X_train.shape\n    num_labels = len(np.unique(y_train))\n    \n    # Empirical distribution of labels\n    P_y = np.zeros(num_labels)\n    for label in y_train:\n        P_y[label] += 1\n    P_y /= num_samples\n    \n    # Empirical distribution of feature-value-label tuples\n    P_fy = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n    for i in range(num_samples):\n        label = y_train[i]\n        for feature_index in range(num_features):\n            feature_value = X_train[i, feature_index]\n            P_fy[feature_index][feature_value][label] += 1\n    \n    # Step 2: Initialize the weight vector\n    weights = np.zeros(num_features * num_labels)\n    \n    # Step 3: Iteratively update the weight vector using GIS\n    for iteration in range(n_iter):\n        max_update = 0\n        for feature_index in range(num_features):\n            for feature_value in np.unique(X_train[:, feature_index]):\n                for label in range(num_labels):\n                    # Calculate the current probability\n                    current_prob = 0\n                    for j in range(num_features):\n                        if j == feature_index:\n                            current_prob += weights[j * num_labels + label]\n                        else:\n                            current_prob += weights[j * num_labels + label] * X_train[i, j]\n                    current_prob = math.exp(current_prob)\n                    current_prob /= sum(math.exp(weights[j * num_labels + label] * X_train[i, j]) for j in range(num_features))\n                    \n                    # Calculate the empirical probability\n                    empirical_prob = P_fy[feature_index][feature_value][label] / (num_samples * P_y[label])\n                    \n                    # Calculate the update\n                    update = empirical_prob * (1 - current_prob)\n                    weights[feature_index * num_labels + label] += update\n                    \n                    # Update the maximum update\n                    max_update = max(max_update, abs(update))\n        \n        # Check if the maximum update is smaller than epsilon\n        if max_update < epsilon:\n            break\n    \n    # Step 4: Predict the labels for the test set\n    predictions = []\n    for i in range(X_test.shape[0]):\n        scores = np.zeros(num_labels)\n        for feature_index in range(num_features):\n            for label in range(num_labels):\n                scores[label] += weights[feature_index * num_labels + label] * X_test[i, feature_index]\n        predictions.append(np.argmax(scores))\n    \n    return predictions\n```\n\n### Explanation:\n1. **Empirical Distributions**:\n   - `P_y`: Empirical distribution of labels.\n   - `P_fy`: Empirical distribution of feature-value-label tuples.\n\n2. **Weight Initialization**:\n   - Initialize the weight vector `weights` to zero.\n\n3. **GIS Iteration**:\n   - For each feature, feature value, and label, calculate the current probability using the current weight vector.\n   - Calculate the empirical probability from the training data.\n   - Compute the update for the weight and apply it.\n   - Track the maximum update to check for convergence.\n\n4. **Prediction**:\n   - For each test sample, compute the scores for each label using the learned weights.\n   - Predict the label with the highest score.\n\nThis implementation ensures that the MaxEnt classifier is trained using the GIS algorithm and can be used to predict labels for new samples."}
{"task_id": 569, "completion_id": 0, "completion": "To solve this task, we need to follow these steps:\n\n1. Count the occurrences of each word across all documents.\n2. Calculate the document-frequency ratio for each word.\n3. Sort the words by their document-frequency ratio in descending order.\n4. Create a set of the top `k` most frequent words if `cut_off` is provided.\n5. Return the sorted list of word-frequency pairs and the set of top `k` words.\n\nHere's the implementation of the `statistic_key_word` function:\n\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    # Step 1: Count the occurrences of each word\n    word_counts = Counter(word for _, words in data for word in words)\n    \n    # Step 2: Calculate the document-frequency ratio for each word\n    total_documents = len(data)\n    document_frequency = {word: count / total_documents for word, count in word_counts.items()}\n    \n    # Step 3: Sort the words by their document-frequency ratio in descending order\n    sorted_word_frequency = sorted(document_frequency.items(), key=lambda x: x[1], reverse=True)\n    \n    # Step 4: Create a set of the top k most frequent words if cut_off is provided\n    if cut_off is not None:\n        top_k_words = {word for word, _ in sorted_word_frequency[:cut_off]}\n    else:\n        top_k_words = set(word for word, _ in sorted_word_frequency)\n    \n    # Step 5: Return the sorted list of word-frequency pairs and the set of top k words\n    return sorted_word_frequency, top_k_words\n\n\n# Example usage:\ndata = [\n    (\"doc1\", [\"apple\", \"banana\", \"apple\"]),\n    (\"doc2\", [\"banana\", \"cherry\"]),\n    (\"doc3\", [\"apple\", \"banana\", \"cherry\", \"cherry\"])\n]\n\nresult, top_k = statistic_key_word(data, cut_off=2)\nprint(\"Word-Frequency Ratio:\", result)\nprint(\"Top K Words:\", top_k)\n```\n\n### Explanation:\n1. **Counting Words**: We use `Counter` to count the occurrences of each word across all documents.\n2. **Document-Frequency Ratio**: We calculate the ratio by dividing the count of each word by the total number of documents.\n3. **Sorting**: We sort the words by their document-frequency ratio in descending order.\n4. **Top K Words**: If `cut_off` is provided, we create a set of the top `k` most frequent words. If `cut_off` is `None`, we include all words.\n5. **Return**: We return a tuple containing the sorted list of word-frequency pairs and the set of top `k` words.\n\nThis implementation ensures that the function meets the requirements and constraints specified in the task."}
{"task_id": 571, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # Compute the weight vector w\n    w = np.sum(alpha * y[:, np.newaxis] * X, axis=0)\n    w_list = w.round(4).tolist()\n    \n    # Compute the bias term b\n    b = np.mean(y - np.dot(X, w))\n    b = round(b, 4)\n    \n    return w_list, b\n```\n\nThis function first computes the weight vector `w` by summing up the products of the Lagrange multipliers, labels, and corresponding samples. It then calculates the bias term `b` by averaging the differences between the labels and the dot product of the samples and the weight vector. Both `w` and `b` are rounded to four decimal places and returned as a tuple."}
{"task_id": 572, "completion_id": 0, "completion": "Certainly! Below is the implementation of the Local Outlier Factor (LOF) algorithm from scratch in Python. The function `local_outlier_factor` takes in the data, the number of neighbors `k`, and an optional parameter `epsilon` to determine the outliers.\n\n```python\nimport numpy as np\n\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Your task is to implement this function!\"\"\"\n    n_samples = data.shape[0]\n    \n    # Step 1: Compute the full pair-wise Euclidean distance matrix\n    distances = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(i + 1, n_samples):\n            distances[i, j] = np.linalg.norm(data[i] - data[j])\n            distances[j, i] = distances[i, j]\n    \n    # Step 2: For every sample p obtain its k-distance and the index list of those k nearest neighbours N_k(p)\n    k_distances = np.partition(distances, k, axis=1)[:, k]\n    k_nearest_indices = np.argsort(distances, axis=1)[:, :k]\n    \n    # Step 3: Define the reachability distance between two points\n    def reachability_distance(p, q, k_distances):\n        return max(k_distances[q], np.linalg.norm(p - q))\n    \n    # Step 4: The Local Reachability Density (LRD) of p\n    def local_reachability_density(p, k_nearest_indices, k_distances):\n        return k / np.sum([reachability_distance(data[p], data[q], k_distances) for q in k_nearest_indices[p]])\n    \n    # Step 5: Finally the Local Outlier Factor\n    lof_scores = []\n    for p in range(n_samples):\n        lrd_p = local_reachability_density(p, k_nearest_indices, k_distances)\n        lof_p = np.sum([local_reachability_density(q, k_nearest_indices, k_distances) for q in k_nearest_indices[p]]) / k / lrd_p\n        lof_scores.append(np.round(lof_p, 4))\n    \n    # Find indices of outliers\n    outlier_indices = [i for i, score in enumerate(lof_scores) if score > epsilon]\n    \n    return lof_scores, sorted(outlier_indices)\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [10, 11], [11, 12], [12, 13]])\nk = 2\nepsilon = 1.5\nlof_scores, outlier_indices = local_outlier_factor(data, k, epsilon)\nprint(\"LOF Scores:\", lof_scores)\nprint(\"Outlier Indices:\", outlier_indices)\n```\n\n### Explanation:\n1. **Distance Matrix Calculation**: We compute the full pair-wise Euclidean distance matrix using nested loops.\n2. **k-Nearest Neighbors**: We use `np.partition` to find the k-th smallest distance for each point, and `np.argsort` to get the indices of the k-nearest neighbors.\n3. **Reachability Distance**: The reachability distance is calculated as the maximum of the k-distance of the neighbor and the Euclidean distance between the two points.\n4. **Local Reachability Density**: The LRD is calculated by averaging the reachability distances of the k-nearest neighbors.\n5. **Local Outlier Factor**: The LOF is calculated by averaging the LRDs of the k-nearest neighbors and dividing by the LRD of the point itself.\n6. **Outlier Detection**: We identify outliers by checking if the LOF score is greater than `epsilon`.\n\nThis implementation ensures that the LOF scores are rounded to four decimal places and that the outlier indices are returned in ascending order."}
{"task_id": 574, "completion_id": 0, "completion": "Certainly! Below is the implementation of the `generate_data` function that meets the specified requirements:\n\n```python\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n        np.random.seed(seed)\n    \n    X_train = []\n    y_train = []\n    X_val = []\n    y_val = []\n    \n    for c in range(m):\n        base = 10 * (c // 2)\n        for _ in range(n_train):\n            x = round(random.uniform(base + 1, base + 9), 4)\n            y = round(random.uniform(base + 1, base + 9), 4)\n            X_train.append([x, y])\n            y_train.append(c)\n        for _ in range(n_val):\n            x = round(random.uniform(base + 1, base + 9), 4)\n            y = round(random.uniform(base + 1, base + 9), 4)\n            X_val.append([x, y])\n            y_val.append(c)\n    \n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_val = np.array(X_val)\n    y_val = np.array(y_val)\n    \n    return X_train, X_val, y_train, y_val\n\n# Example usage:\n# X_train, X_val, y_train, y_val = generate_data(m=3, n_train=10, n_val=5, seed=42)\n# print(X_train)\n# print(y_train)\n# print(X_val)\n# print(y_val)\n```\n\n### Explanation:\n1. **Random Seed Handling**: If a seed is provided, both `random` and `np.random` are seeded to ensure reproducibility.\n2. **Data Generation**:\n   - For each class `c` from `0` to `m-1`, calculate the base value as `10 * (c // 2)`.\n   - Generate `n_train` training points and `n_val` validation points within the specified square.\n   - Each point's coordinates are sampled from a uniform distribution and rounded to 4 decimal places.\n3. **Data Storage**: Points and labels are stored in lists and then converted to NumPy arrays before returning.\n\nThis function ensures that the generated data is consistent and reproducible if the same seed is used."}
{"task_id": 579, "completion_id": 0, "completion": "Certainly! Below is the implementation of the K-Means clustering algorithm from scratch in Python using NumPy:\n\n```python\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    m, d = X.shape\n    \n    # Step 1: Initialization\n    centres = X[:k].tolist()\n    \n    for iteration in range(max_iters):\n        # Step 2: Assignment step\n        labels = []\n        for point in X:\n            distances = np.linalg.norm(point - np.array(centres), axis=1)\n            min_distance_index = np.argmin(distances)\n            labels.append(min_distance_index)\n        \n        # Step 3: Update step\n        new_centres = []\n        for i in range(k):\n            assigned_points = X[labels == i]\n            if len(assigned_points) > 0:\n                new_centre = np.mean(assigned_points, axis=0).tolist()\n            else:\n                new_centre = centres[i]\n            new_centres.append(new_centre)\n        \n        # Check for convergence\n        if new_centres == centres:\n            break\n        \n        centres = new_centres\n    \n    # Round the centres to 4 decimal places\n    centres = [list(map(lambda x: round(x, 4), centre)) for centre in centres]\n    \n    return labels, centres\n\n# Example usage:\n# X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n# k = 2\n# labels, centres = kmeans(X, k)\n# print(\"Labels:\", labels)\n# print(\"Centres:\", centres)\n```\n\n### Explanation:\n1. **Initialization**: The first `k` points in the dataset `X` are used as the initial cluster centers.\n2. **Assignment Step**: For each point in `X`, the Euclidean distance to each center is computed. The point is assigned to the nearest center. In case of a tie, the center with the smaller index is chosen.\n3. **Update Step**: The new center for each cluster is computed as the arithmetic mean of all points assigned to that cluster. If a cluster loses all its points, the center remains unchanged.\n4. **Convergence Check**: The algorithm stops if the cluster centers do not change between iterations or if the maximum number of iterations (`max_iters`) is reached.\n5. **Rounding**: The final cluster centers are rounded to 4 decimal places.\n\nThis implementation ensures that the K-Means algorithm is deterministic and can be tested easily."}
{"task_id": 581, "completion_id": 0, "completion": "To implement the Elastic-Net regression from scratch using batch gradient descent, we need to follow the steps outlined in the problem description. Below is the complete implementation of the `elastic_net_regression` function:\n\n```python\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    # Initialize weights and bias\n    m, n = X.shape\n    weights = np.zeros(n)\n    bias = 0.0\n\n    # Check if learning rate or iterations are zero\n    if learning_rate == 0 or iterations == 0:\n        return [round(w, 4) for w in weights], round(bias, 4)\n\n    # Perform batch gradient descent\n    for _ in range(iterations):\n        # Compute predictions\n        predictions = X @ weights + bias\n        \n        # Compute residuals\n        residuals = y - predictions\n        \n        # Compute gradients\n        gradient_weights = -2 * (X.T @ residuals) + l1_penalty * np.sign(weights) + 2 * l2_penalty * weights\n        gradient_bias = -2 * np.sum(residuals)\n        \n        # Update parameters\n        weights -= learning_rate * gradient_weights / m\n        bias -= learning_rate * gradient_bias / m\n\n    # Return rounded weights and bias\n    return [round(w, 4) for w in weights], round(bias, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# learning_rate = 0.01\n# iterations = 1000\n# l1_penalty = 0.1\n# l2_penalty = 0.1\n# weights, bias = elastic_net_regression(X, y, learning_rate, iterations, l1_penalty, l2_penalty)\n# print(weights, bias)\n```\n\n### Explanation:\n1. **Initialization**: We initialize the weights to zero and the bias to zero.\n2. **Edge Case Handling**: If the learning rate or the number of iterations is zero, we return the initial parameters.\n3. **Gradient Descent Loop**:\n   - For each iteration, we compute the predictions using the current weights and bias.\n   - We then compute the residuals, which are the differences between the actual target values and the predictions.\n   - We calculate the gradients for the weights and the bias using the formulas provided in the problem description.\n   - We update the weights and bias using the gradients and the learning rate.\n4. **Rounding**: Finally, we round the weights and bias to four decimal places and return them as a tuple.\n\nThis implementation ensures that the Elastic-Net regression model is trained using batch gradient descent and the learned parameters are returned rounded to four decimal places."}
{"task_id": 591, "completion_id": 0, "completion": "To solve this problem, we need to implement a forward pass through a fully-connected feed-forward neural network using the given weight matrices and input vector. The network uses ReLU activation functions in all hidden layers and a linear activation function in the output layer. Here's how we can achieve this:\n\n1. **Add the bias input**: For each input vector, we need to add a bias term (which is typically 1) to the input vector.\n2. **Forward pass through each layer**: For each layer, we perform a dot product between the current input vector and the weight matrix of that layer. Then, we apply the ReLU activation function to the result of the dot product, except for the output layer where we use a linear activation (identity function).\n3. **Update the input vector**: The output of the current layer becomes the input to the next layer.\n4. **Return the final output**: After processing all layers, we return the output of the last layer, rounded to four decimal places.\n\nHere's the implementation of the `neural_net_predict` function:\n\n```python\nimport numpy as np\n\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Add bias term to the input vector\n    x = np.array([1] + x)\n    \n    # Iterate through each layer\n    for layer_weights in weights:\n        # Perform dot product and apply ReLU activation (except for the last layer)\n        x = np.maximum(0, np.dot(x, np.array(layer_weights).T))\n    \n    # Return the final output rounded to four decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage:\nweights = [\n    [[0.1, 0.2], [0.3, 0.4]],  # Layer 1 weights\n    [[0.5, 0.6], [0.7, 0.8]]   # Layer 2 weights\n]\nx = [0.5, 0.3]\nprint(neural_net_predict(weights, x))  # Output: [0.5, 0.6]\n```\n\n### Explanation:\n1. **Bias Term Addition**: We add a bias term (1) to the input vector `x` using `np.array([1] + x)`.\n2. **Forward Pass**: We iterate through each layer's weight matrix. For each layer, we perform a dot product between the current input vector `x` and the weight matrix of that layer. We then apply the ReLU activation function using `np.maximum(0, ...)`. This step is repeated for all layers except the last one.\n3. **Output Layer**: For the last layer, we use a linear activation (identity function) by simply taking the dot product without applying ReLU.\n4. **Rounding**: Finally, we round the output to four decimal places using `np.round(x, 4)` and convert it to a list before returning.\n\nThis implementation ensures that the forward pass through the neural network is performed correctly, and the output is rounded to the required precision."}
{"task_id": 595, "completion_id": 0, "completion": "Certainly! Below is the implementation of the K-Means clustering algorithm from scratch, focusing only on computing the centroids:\n\n```python\nimport numpy as np\n\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Your task: implement here. See docstring in the description.\"\"\"\n    # Initialize centroids with the first k samples\n    centroids = X[:k].copy()\n    \n    for _ in range(max_iters):\n        # Assign each sample to the nearest centroid\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        \n        # Update each centroid to the arithmetic mean of the samples assigned to it\n        new_centroids = np.array([X[labels == i].mean(axis=0) if len(X[labels == i]) > 0 else centroids[i] for i in range(k)])\n        \n        # Check if centroids have moved less than 1e-4\n        if np.all(np.linalg.norm(new_centroids - centroids) < 1e-4):\n            break\n        \n        centroids = new_centroids\n    \n    # Sort centroids in ascending lexicographical order and round to 4 decimal places\n    sorted_centroids = np.round(centroids[np.lexsort(centroids.T)], 4).tolist()\n    \n    return sorted_centroids\n\n# Example usage:\nX = np.array([[1, 2], [1, 4], [1, 0],\n              [4, 2], [4, 4], [4, 0]])\nk = 2\ncentroids = kmeans_centroids(X, k)\nprint(centroids)\n```\n\n### Explanation:\n1. **Initialization**: The centroids are initialized with the first `k` samples from the input array `X`.\n2. **Assignment**: For each sample, the nearest centroid is determined using the squared Euclidean distance.\n3. **Update**: Each centroid is updated to the arithmetic mean of the samples assigned to it. If a cluster becomes empty, its centroid remains unchanged.\n4. **Convergence Check**: The algorithm stops early if all centroids move less than `1e-4` in a full iteration.\n5. **Sorting and Rounding**: The centroids are sorted in ascending lexicographical order and rounded to 4 decimal places before being returned.\n\nThis implementation ensures that the centroids are computed correctly according to the specified requirements."}
