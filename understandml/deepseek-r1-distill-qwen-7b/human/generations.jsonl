{"task_id": 2, "completion_id": 0, "completion": "To solve this problem, we need to compute the transpose of a given matrix. The transpose of a matrix is obtained by converting its rows into columns and its columns into rows. This means that the element at position [i][j] in the original matrix will move to position [j][i] in the transposed matrix.\n\n### Approach\n1. **Understand the Transpose Operation**: The transpose of a matrix is formed by swapping its rows with columns. For example, the element at position (i, j) in the original matrix will be at position (j, i) in the transposed matrix.\n2. **Iterate Through Columns**: To construct the transposed matrix, iterate through each column index of the original matrix. For each column index, create a new row in the transposed matrix by collecting elements from each row of the original matrix at that column index.\n3. **Handle Edge Cases**: Specifically handle the case where the input matrix is empty. If the input matrix is empty, return an empty list.\n\n### Solution Code\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    if not a:\n        return []\n    return [[row[j] for row in a] for j in range(len(a[0]))]\n```\n\n### Explanation\n1. **Check for Empty Matrix**: The function first checks if the input matrix `a` is empty. If it is, the function returns an empty list immediately.\n2. **Construct Transposed Matrix**: Using a list comprehension, the function constructs each row of the transposed matrix by iterating over each column index `j`. For each `j`, it creates a new row by collecting the `j`-th element from each row of the original matrix.\n3. **Return the Result**: The constructed transposed matrix is returned as the result.\n\nThis approach efficiently constructs the transposed matrix by leveraging list comprehensions, ensuring clarity and conciseness in the implementation."}
{"task_id": 3, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that reshapes a given matrix into a specified shape. If the reshaping is not possible, the function should return an empty list. The solution involves using numpy for efficient array manipulation.\n\n### Approach\n1. **Check Feasibility**: First, we need to determine if the given matrix can be reshaped into the new specified shape. This is done by comparing the total number of elements in the original matrix with the total number of elements required by the new shape. If these totals do not match, reshaping is not possible, and we return an empty list.\n2. **Convert to numpy Array**: Convert the given matrix into a numpy array for efficient reshaping operations.\n3. **Reshape Using numpy**: Use numpy's reshape method to reshape the array into the desired shape.\n4. **Convert Back to List**: Convert the reshaped numpy array back into a Python list of lists using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Check if total elements match\n    total_elements = len(a) * len(a[0])\n    new_total = new_shape[0] * new_shape[1]\n    if total_elements != new_total:\n        return []\n    \n    # Convert to numpy array and reshape\n    arr = np.array(a)\n    reshaped = arr.reshape(new_shape)\n    \n    # Convert back to list of lists\n    return reshaped.tolist()\n```\n\n### Explanation\n1. **Feasibility Check**: The function first calculates the total number of elements in the original matrix and the new shape. If these totals do not match, it returns an empty list immediately.\n2. **Conversion to numpy Array**: The given matrix is converted into a numpy array to leverage numpy's efficient reshaping capabilities.\n3. **Reshaping**: Using numpy's `reshape` method, the array is reshaped into the new specified dimensions.\n4. **Conversion Back**: The reshaped numpy array is converted back into a Python list of lists using `tolist()`, ensuring the output format matches the required structure.\n\nThis approach ensures that the function efficiently handles the reshaping process and returns the correct result or an empty list if reshaping is not possible."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the mean of elements in a matrix either by row or by column, based on a given mode. The function will take a matrix (a list of lists of floats) and a mode ('row' or 'column') as inputs and return a list of means accordingly.\n\n### Approach\n1. **Understand the Problem**: The task is to compute the mean for each row or each column of a given matrix. If the mode is 'row', the function should return a list of means where each element is the mean of the corresponding row. If the mode is 'column', it should return a list of means where each element is the mean of the corresponding column.\n\n2. **Row Mean Calculation**: For each row in the matrix, compute the mean by summing all elements in the row and dividing by the number of elements in that row.\n\n3. **Column Mean Calculation**: For each column, sum all the elements across all rows and then divide by the number of rows to get the mean for that column. This requires iterating over each column index and summing the corresponding elements from all rows.\n\n4. **Handle Different Modes**: Use conditional statements to determine whether to compute row means or column means.\n\n### Solution Code\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        if num_rows == 0:\n            return []\n        num_cols = len(matrix[0])\n        result = []\n        for col in range(num_cols):\n            total = 0\n            for row in matrix:\n                total += row[col]\n            mean = total / num_rows\n            result.append(mean)\n        return result\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n```\n\n### Explanation\n- **Row Mode**: When"}
{"task_id": 5, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that multiplies each element of a given matrix by a specified scalar and returns the resulting matrix. This operation is known as scalar multiplication.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Initialize an empty list**: This will store the resulting matrix after scalar multiplication.\n2. **Iterate through each row of the input matrix**: For each row, we will create a new row where each element is the product of the corresponding element in the original row and the scalar.\n3. **Create a new row**: Using a list comprehension, we multiply each element of the current row by the scalar and collect these values into a new row.\n4. **Append the new row to the resulting matrix**: After processing each row, we add the newly created row to our resulting matrix.\n5. **Return the resulting matrix**: Once all rows have been processed, we return the new matrix.\n\nThis approach ensures that each element in the matrix is multiplied by the scalar exactly once, resulting in an efficient and clear solution.\n\n### Solution Code\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    new_matrix = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        new_matrix.append(new_row)\n    return new_matrix\n```\n\n### Explanation\n- **Initialization**: We start by creating an empty list `new_matrix` to store the resulting matrix.\n- **Iteration**: For each row in the input matrix, we use a list comprehension to create a new row where each element is the product of the corresponding element in the original row and the scalar.\n- **Appending Rows**: Each newly created row is appended to `new_matrix`.\n- **Return**: Finally, the resulting matrix is returned after all rows have been processed.\n\nThis solution efficiently handles the scalar multiplication by leveraging list comprehensions for concise and readable code, ensuring that each element is processed exactly once. The function works correctly for any valid input matrix and scalar, including edge cases such as an empty matrix or a scalar of zero."}
{"task_id": 8, "completion_id": 0, "completion": "To solve this problem, we need to calculate the inverse of a 2x2 matrix. The inverse of a matrix exists only if its determinant is non-zero. If the determinant is zero, the matrix is not invertible, and we should return `None` in such cases.\n\n### Approach\n1. **Calculate the Determinant**: For a 2x2 matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), the determinant is calculated as \\(ad - bc\\). If the determinant is zero, the matrix is singular and does not have an inverse.\n2. **Check for Invertibility**: If the determinant is zero, return `None`.\n3. **Compute the Inverse**: If the determinant is non-zero, compute the inverse using the formula:\n   \\[\n   \\frac{1}{\\text{determinant}} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n   \\]\n   Each element of the matrix is divided by the determinant to form the inverse matrix.\n\n### Solution Code\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1.0 / det\n    return [\n        [d * inv_det, -b * inv_det],\n        [-c * inv_det, a * inv_det]\n    ]\n```\n\n### Explanation\n1. **Extract Elements**: The elements of the matrix are extracted into variables `a`, `b`, `c`, and `d`.\n2. **Calculate Determinant**: The determinant is computed using the formula \\(ad - bc\\).\n3. **Check Determinant**: If the determinant is zero, the function returns `None` as the matrix is not invertible.\n4. **Inverse Calculation**: If the determinant is non-zero, each element of the matrix is scaled by the inverse of the determinant to form the inverse matrix.\n\nThis approach ensures that we correctly handle both invertible and non-invertible matrices, providing the correct inverse when possible."}
{"task_id": 9, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that multiplies two matrices. If the matrices cannot be multiplied due to incompatible dimensions, the function should return -1.\n\n### Approach\n1. **Check Matrix Dimensions**: The primary condition for matrix multiplication is that the number of columns in the first matrix (A) must equal the number of rows in the second matrix (B). If this condition is not met, the matrices cannot be multiplied, and we return -1.\n2. **Initialize Result Matrix**: If the matrices can be multiplied, we initialize a result matrix with dimensions equal to the number of rows of the first matrix (A) and the number of columns of the second matrix (B). All elements of this result matrix are initially set to zero.\n3. **Compute Dot Products**: For each element in the result matrix, compute the dot product of the corresponding row from the first matrix (A) and the column from the second matrix (B). This involves iterating through each element of the row and column, multiplying corresponding elements, and summing"}
{"task_id": 15, "completion_id": 0, "completion": "To solve this problem, we need to implement a linear regression model using gradient descent. This approach allows us to iteratively adjust the coefficients of our model to minimize the cost function, which measures the difference between our predictions and the actual target values.\n\n### Approach\n1. **Initialization**: Start by initializing the coefficient vector (theta) to zeros. This vector has the same number of elements as the number of features in our dataset, including an intercept term.\n2. **Gradient Descent Iterations**: For a specified number of iterations, compute the predictions using the current coefficients. Then, calculate the gradient of the cost function with respect to"}
{"task_id": 16, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs feature scaling on a given dataset using two common techniques: standardization and min-max normalization. The function will take a 2D NumPy array as input and return two 2D lists: one scaled by standardization and the other by min-max normalization. Each result will be rounded to four decimal places and returned as a list of lists.\n\n### Approach\n1. **Standardization**: This technique scales the data such that each feature has a mean of 0 and a standard deviation of 1. The formula used is:\n   \\[\n   \\text{Standardized Value} = \\frac{(\\text{Value} - \\text{Mean})}{\\text{Standard Deviation}}\n   \\]\n   - Compute the mean and standard deviation for each feature.\n   - Subtract the mean from each value and divide by the standard deviation.\n\n2. **Min-Max Normalization**: This technique scales the data to a range between 0 and 1. The formula used is:\n   \\[\n   \\text{Normalized Value} = \\frac{\\text{Value} - \\text{Min Value}}{\\text{Max Value} - \\text{Min Value}}\n   \\]\n   - Compute the minimum and maximum values for each feature.\n   - Subtract the minimum value from each value and divide by the difference between the maximum and minimum values.\n\n3. **Rounding and Conversion**: After scaling the data, round each value to four decimal places. Convert the resulting NumPy arrays to lists of lists using NumPy's `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Compute mean and standard deviation for each feature\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    \n    # Standardize the data\n    standardized = (data - mean) / std\n    \n    # Compute min and max for each feature\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    \n    # Normalize the data\n    normalized = (data - min_vals) / (max_vals - min_vals)\n    \n    # Round to four decimal places\n    standardized_rounded = np.round(standardized, 4)\n    normalized_rounded = np.round(normalized, 4)\n    \n    # Convert to lists of lists\n    standardized_list = standardized_rounded.tolist()\n    normalized_list = normalized_rounded.tolist()\n    \n    return standardized_list, normalized_list\n```\n\n### Explanation\n- **Standardization**: The function first calculates the mean and standard deviation for each feature (column) using NumPy's `mean` and `std` functions. It then applies the standardization formula to each value in the dataset.\n- **Min-Max Normalization**: The function computes the minimum and maximum values for each feature using NumPy's `min` and `max` functions. It then applies the min-max normalization formula to each value.\n- **Rounding and Conversion**: After scaling, the values are rounded to four decimal places using NumPy's `round` function. The resulting NumPy arrays are converted to lists of lists using the `tolist()` method to meet the problem's requirements.\n\nThis approach ensures that the dataset is scaled appropriately using both standardization and min-max normalization, providing two transformed versions of the dataset that can be used for further analysis or modeling."}
{"task_id": 22, "completion_id": 0, "completion": "To solve this problem, we need to compute the output of the sigmoid activation function given an input value \\( z \\). The sigmoid function is a common activation function used in machine learning, especially in neural networks, and it maps any real-valued number to a value between 0 and 1. The function is defined as \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\).\n\n### Approach\n1. **Understand the Sigmoid Function**: The sigmoid function takes a real-valued input \\( z \\) and outputs a value between 0 and 1. This makes it useful for converting a number into a probability.\n2. **Mathematical Formula**: The formula for the sigmoid function is \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\). Here, \\( e \\) is the base of the natural logarithm, approximately equal to 2.71828.\n3. **Compute the Function**: Using Python's `math.exp` function, we can compute \\( e^{-z} \\) efficiently. The result is then used to compute the sigmoid value.\n4. **Rounding**: The result is rounded to four decimal places to meet the problem's requirement.\n\n### Solution Code\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    output = 1.0 / (1.0 + math.exp(-z))\n    return round(output, 4)\n```\n\n### Explanation\n- **Import Math Module**: The `math` module is imported to use the `exp` function, which calculates the exponential of a number.\n- **Function Definition**: The function `sigmoid` takes a single parameter `z`, which is a float.\n- **Compute Sigmoid Value**: The value of the sigmoid function is computed using the formula \\( \\frac{1}{1 + e^{-z}} \\).\n- **Round the Result**: The computed value is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision.\n\nThis approach efficiently computes the sigmoid function and ensures the result is precise to four decimal places, making it suitable for applications where such precision is required."}
{"task_id": 23, "completion_id": 0, "completion": "To solve this problem, we need to implement the softmax activation function, which transforms a list of scores into a list of probabilities that sum up to 1. This function is commonly used in machine learning and neural networks to convert scores into probabilities.\n\n### Approach\nThe approach to implement the softmax function involves the following steps:\n\n1. **Compute Exponentials**: For each score in the input list, compute the exponential of the score after subtracting the maximum score from each score. This step helps in avoiding numerical instability by ensuring that the largest exponent is 0, which simplifies calculations and prevents very large numbers.\n\n2. **Sum of Exponentials**: Calculate the sum of all the exponentials computed in the previous step. This sum will be used as the denominator for each probability calculation.\n\n3. **Compute Probabilities**: For each score, divide its exponential value by the sum of all exponentials to get the probability. This ensures that the probabilities sum up to 1.\n\n4. **Round Values**: Each probability value is rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    if"}
{"task_id": 27, "completion_id": 0, "completion": "To solve this problem, we need to compute the transformation matrix from one basis to another in three-dimensional space (R\u00b3). The transformation matrix will allow us to convert coordinates from one basis to another.\n\n### Approach\n1. **Understand the Problem**: We are given two bases, B and C, each consisting of three vectors in R\u00b3. We need to find the transformation matrix P such that any vector expressed in basis B can be converted to its equivalent form in basis C using P.\n\n2. **Matrix Representation**: Each basis can be represented as a matrix where each column is a vector from the basis. Let\u2019s denote the matrix of vectors from basis B as [B] and similarly [C] for basis C.\n\n3. **Transformation Matrix**: The transformation matrix P from basis B to C can be found using the formula:\n   \\[\n   P = [B]^{-1} \\times [C]\n   \\]\n   Here, [B]^{-1} is the inverse of the matrix formed by vectors in basis B, and [C] is the matrix formed by vectors in basis C.\n\n4. **Implementation Steps**:\n   - Convert the input lists of vectors into numpy arrays to form matrices [B] and [C].\n   - Compute the inverse of matrix [B].\n   - Multiply the inverse of [B] with matrix [C] to get the transformation matrix P.\n   - Round the resulting matrix to four decimal places for precision.\n   - Convert the numpy array result into a list of lists for the final output.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the lists of vectors into numpy matrices\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    \n    # Compute the inverse of B_matrix\n    inv_B = np.linalg.inv(B_matrix)\n    \n    # Calculate the transformation matrix P\n    P = inv_B @ C_matrix\n    \n    # Round the results to 4 decimal places\n    P_rounded = np.round(P, 4)\n    \n    # Convert the numpy array to a list of lists\n    return P_rounded.tolist()\n```\n\n### Explanation\n- **Conversion to Matrices**: The input lists of basis vectors are converted into numpy matrices for easy manipulation.\n- **Matrix Inversion**: The inverse of matrix [B] is computed to facilitate the transformation from basis B to C.\n- **Matrix Multiplication**: The product of the inverse of [B] and matrix [C] gives the transformation matrix P.\n- **Rounding and Conversion**: The resulting matrix is rounded to four decimal places and converted to a list of lists to match the required output format.\n\nThis approach efficiently computes the transformation matrix using linear algebra operations, ensuring accuracy and clarity in the solution."}
{"task_id": 29, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that randomly shuffles two numpy arrays, X and y, while maintaining their corresponding order. The function should also allow for an optional seed parameter to ensure reproducibility of the shuffle.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check the Seed**: If a seed is provided, set the random seed using numpy's `random.seed()` method. This ensures that the shuffle is reproducible when the same seed is used again.\n2. **Generate Indices**: Create an array of indices using `numpy.arange()` to represent the positions of the elements in the arrays.\n3. **Shuffle Indices**: Use `numpy.random.permutation()` to generate a shuffled version of the indices array. This shuffled array will determine the new order of elements in both X and y.\n4. **Reorder Arrays**: Use the shuffled indices to reorder both X and y arrays. This ensures that the corresponding elements in X and y remain paired after shuffling.\n5. **Convert to Lists**: Convert the shuffled numpy arrays back to Python lists using the `tolist()` method as required.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n```\n\n### Explanation\n- **Random Seed Handling**: The function checks if a seed is provided. If so, it sets the random seed to ensure that the shuffle is reproducible.\n- **Index Generation**: The number of samples is determined using `X.shape[0]`, which gives the length of the first dimension of the numpy array X. This is used to create an array of indices.\n- **Shuffling Indices**: The `numpy.random.permutation()` function is used to generate a shuffled version of the indices array. This shuffled array is then used to reorder both X and y.\n- **Reordering Arrays**: The arrays X and y are reordered using the shuffled indices, ensuring that both arrays are shuffled in the same way.\n- **Conversion to Lists**: The shuffled numpy arrays are converted to Python lists using the `tolist()` method, as required by the problem statement.\n\nThis approach ensures that both arrays are shuffled correctly while maintaining their correspondence, and it allows for reproducibility when a seed is provided."}
{"task_id": 30, "completion_id": 0, "completion": "To solve this problem, we need to implement a batch iterator function in Python that works with numpy arrays. The function should yield batches of data, either as pairs of (X, y) if y is provided or just X if y is not provided. Each batch should be converted into a list using numpy's tolist() method.\n\n### Approach\n1. **Problem Analysis**: The task is to create a function that iterates over a numpy array X and an optional numpy array y, yielding batches of data. The batch size is specified, and each batch should be converted into a list.\n2. **Handling Different Cases**: The function should handle cases where y is None (only X is used) and where y is provided (both X and y are used in each batch).\n3. **Batching Logic**: The function calculates the number of samples and the number of batches. It then iterates over the data in steps of the specified batch size, slicing the data into batches.\n4. **Conversion to List**: Each batch is converted into a list using numpy's tolist() method. If y is provided, each batch is a tuple of two lists; otherwise, it is a single list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0] if y is None else y.shape[0]\n    for i in range(0, n_samples, batch_size):\n        end = i + batch_size\n        if y is None:\n            batch = X[i:end]\n        else:\n            batch = (X[i:end], y[i:end])\n        if y is not None:\n            yield (batch[0].tolist(), batch[1].tolist())\n        else:\n            yield batch.tolist()\n```\n\n### Explanation\n1. **Initialization**: The function starts by determining the number of samples. If y is provided, the number of samples is taken from y; otherwise, it is taken from X.\n2. **Loop Through Batches**: The function then loops through the data in steps of the batch size. For each step, it calculates the end index of the current batch.\n3. **Slicing Data**: Depending on whether y is provided, the function slices X (and y if applicable) to get the current batch.\n4. **Conversion and Yielding**: Each batch is converted into a list using tolist(). If y is provided, the batch is yielded as a tuple of two lists; otherwise, it is yielded as a single list.\n\nThis approach ensures that the function efficiently handles both cases (with and without y) and correctly yields the data in batches as specified."}
{"task_id": 31, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that divides a dataset into two subsets based on whether the values of a specified feature meet or exceed a given threshold. The function should return these subsets as lists after converting them into the required format.\n\n### Approach\n1. **Understand the Input Structure**: The dataset is provided as a numpy array where each row represents a sample and each column represents a feature.\n2. **Feature Selection**: Identify the specified feature using its index and the given threshold.\n3. **Mask Creation**: Create a boolean mask where each element is `True` if the corresponding feature value meets or exceeds the threshold, and `False` otherwise.\n4. **Subset Creation**: Use the boolean mask to create two subsets: one containing samples that meet the condition and another containing those that do not.\n5. **Conversion to List**: Convert the resulting numpy arrays into lists using `tolist()` to meet the output requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    mask = (X[:, feature_i] >= threshold)\n    subset_1 = X[mask]\n    subset_0 = X[~mask]\n    return [subset_1.tolist(), subset_0.tolist()]\n```\n\n### Explanation\n1. **Mask Creation**: The boolean mask `mask` is created using `X[:, feature_i] >= threshold`. This checks each value in the specified feature column against the threshold.\n2. **Subset Creation**: Using the mask, `subset_1` contains all samples where the feature meets or exceeds the threshold, and `subset_0` contains the remaining samples.\n3. **Conversion to List**: The numpy arrays `subset_1` and `subset_0` are converted to lists using `tolist()`, ensuring the output format is as required.\n\nThis approach efficiently splits the dataset using vectorized operations provided by numpy, ensuring the solution is both concise and performant."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that converts a 1D numpy array into a 2D diagonal matrix and then returns this matrix as a Python list. A diagonal matrix is a square matrix where all elements are zero except for those on the main diagonal.\n\n### Approach\n1. **Understand the Problem**: Convert a 1D numpy array into a 2D diagonal matrix. The diagonal matrix should have the elements of the 1D array on its main diagonal, and all other elements should be zero.\n2. **Diagonal Matrix Construction**: Use numpy's `diag` function, which takes a 1D array and places its elements on the main diagonal of a square matrix.\n3. **Conversion to List**: Convert the resulting 2D numpy array into a Python list using the `tolist()` method.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    mat = np.diag(x)\n    return mat.tolist()\n```\n\n### Explanation\n1. **Import numpy**: The numpy library is imported to utilize its functions for matrix operations.\n2. **Function Definition**: The function `make_diagonal` takes a 1D numpy array `x` as input.\n3. **Create Diagonal Matrix**: The `np.diag(x)` function constructs a square matrix where the elements of `x` are placed along the main diagonal.\n4. **Convert to List**: The resulting 2D numpy array is converted to a Python list using the `tolist()` method, ensuring the output is a nested list structure.\n\nThis approach efficiently constructs the diagonal matrix and converts it into the required format, handling all edge cases such as empty arrays or arrays of any length."}
{"task_id": 36, "completion_id": 0, "completion": "To solve this problem, we need to calculate the accuracy score of a model's predictions. The accuracy score is a measure of how often the model's predictions match the true labels. \n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Comparison of Predictions and True Labels**: We compare each element in the true labels (`y_true`) array with the corresponding element in the predicted labels (`y_pred`) array. This comparison results in a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` match, and `False` otherwise.\n2. **Counting Correct Predictions**: Using the boolean array from the comparison, we count the number of correct predictions. In numpy, this is efficiently done using the `np.sum` function, which sums up the boolean values (where `True` is treated as 1 and `False` as 0).\n3. **Calculating Accuracy**: The accuracy is calculated by dividing the number of correct predictions by the total number of predictions. This can be done efficiently using the `np.mean` function, which computes the mean of the boolean array, effectively giving the proportion of correct predictions.\n4. **Rounding the Result**: The accuracy is then rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    return round(np.mean(y_true == y_pred), 4)\n```\n\n### Explanation\n- **Comparison and Counting**: The expression `y_true == y_pred` creates a boolean array where each element is `True` if the corresponding elements in `y_true` and `y_pred` are equal, and `False` otherwise. Using `np.mean` on this boolean array gives the proportion of correct predictions.\n- **Efficiency**: Using `np.mean` is efficient as it leverages numpy's optimized C backend for array operations, making the computation faster even for large arrays.\n- **Rounding**: The `round` function ensures that the result is presented with four decimal places, as required.\n\nThis approach ensures that the accuracy score is computed efficiently and correctly, handling all edge cases where the arrays are of the same length."}
{"task_id": 39, "completion_id": 0, "completion": "To solve this problem, we need to implement the log-softmax function, which is a numerically stable version of the softmax function. The log-softmax function is commonly used in machine learning for tasks such as classification, especially when dealing with large numbers to avoid numerical overflow issues.\n\n### Approach\nThe log-softmax function can be computed efficiently using the natural logarithm and exponentiation properties. Here\u2019s the step-by-step approach:\n\n1. **Compute Exponentials**: Compute the exponential of each element in the input array. This step transforms the input scores into positive values, which are then summed up.\n2. **Log Summation**: Compute the logarithm of the sum of these exponentials"}
{"task_id": 42, "completion_id": 0, "completion": "To solve this problem, we need to implement the Rectified Linear Unit (ReLU) activation function. The ReLU function is a piecewise function that returns the input value if it is greater than zero, otherwise, it returns zero. This function is widely used in artificial neural networks due to its simplicity and effectiveness.\n\n### Approach\nThe ReLU function can be implemented using a simple conditional check. The steps are as follows:\n1. Check if the input value `z` is greater than zero.\n2. If `z` is greater than zero, return `z`.\n3. Otherwise, return zero.\n\nThis approach ensures that any positive input is passed through unchanged, while any non-positive input is set to zero, which is the essence of the ReLU function.\n\n### Solution Code\n\n```python\ndef relu(z: float) -> float:\n    return z if z > 0 else 0\n```\n\n### Explanation\nThe provided solution is a concise implementation of the ReLU function. The function takes a single float `z` as input and returns a float. The function uses a ternary conditional operator to check if `z` is greater than zero. If true, it returns `z`; otherwise, it returns zero. This implementation is efficient and handles all edge cases, including when `z` is zero or a negative number. The use of a single line with a ternary operator makes the code clean and easy to understand."}
{"task_id": 43, "completion_id": 0, "completion": "To solve this problem, we need to implement the Ridge Regression loss function. Ridge Regression is a type of linear regression that uses a regularization term to prevent overfitting. The loss function combines the Mean Squared Error (MSE) with a regularization term to balance the trade-off between fitting the training data and keeping the model coefficients small.\n\n### Approach\nThe approach involves the following steps:\n1. **Compute Predicted Values**: Use the feature matrix \\(X\\) and the coefficients \\(w\\) to compute the predicted values \\(y_{\\text{pred}}\\) using matrix multiplication.\n2. **Calculate Mean Squared Error (MSE)**: Compute the MSE, which is the average of the squared differences between the predicted values and the true values.\n3. **Compute Regularization Term**: Calculate the regularization term, which is the sum of the squares of the coefficients multiplied by the regularization parameter \\(\\alpha\\).\n4. **Sum MSE and Regularization Term**: Add the MSE and the regularization term to get the total loss.\n5. **Round the Result**: Round the total loss to four decimal places to meet the problem's requirement.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    mse = np.mean((y_pred - y_true) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    total_loss = mse + reg_term\n    return round(total_loss, 4)\n```\n\n### Explanation\n1. **Compute Predicted Values**: The predicted values are calculated using `y_pred = X.dot(w)`, where `X` is the feature matrix and `w` is the coefficient vector.\n2. **Calculate MSE**: The MSE is computed as the mean of the squared differences between the predicted and true values using `np.mean((y_pred - y_true) ** 2)`.\n3. **Compute Regularization Term**: The regularization term is calculated as `alpha * np.sum(w ** 2)`, where `alpha` is the regularization parameter and `w` are the coefficients.\n4. **Sum MSE and Regularization Term**: The total loss is the sum of the MSE and the regularization term.\n5. **Round the Result**: The total loss is rounded to four decimal places using `round(total_loss, 4)` to ensure the result meets the specified precision.\n\nThis approach efficiently combines the core components of Ridge Regression to compute the loss function, ensuring both accuracy and adherence to the problem constraints."}
{"task_id": 44, "completion_id": 0, "completion": "To solve this problem, we need to implement the Leaky Rectified Linear Unit (Leaky ReLU) activation function. This function is a variation of the Rectified Linear Unit (ReLU) function, which helps in addressing the \"dying ReLU\" problem by allowing a small negative activation for negative inputs.\n\n### Approach\nThe Leaky ReLU function is defined as follows:\n- For any input value `z`, if `z` is non-negative (i.e., `z >= 0`), the function returns `z` as is.\n- If `z` is negative, the function returns `alpha * z`, where `alpha` is a small positive value (default is 0.01). This small positive slope allows the function to output a small negative value when `z` is negative, preventing it from dying.\n\nThe function can be implemented using a conditional expression for conciseness and readability. This approach checks if the input `z` is non-negative and returns `z` if true; otherwise, it returns `alpha * z`.\n\n### Solution Code\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    return z if z >= 0 else alpha * z\n```\n\n### Explanation\n- The function `leaky_relu` takes two parameters: `z` (a float) and `alpha` (a float with a default value of 0.01).\n- It returns `z` if `z` is non-negative.\n- If `z` is negative, it returns `alpha * z`, which is a small negative value scaled by `alpha`.\n- This implementation efficiently handles both cases using a single line of code, ensuring clarity and performance."}
{"task_id": 45, "completion_id": 0, "completion": "To solve this problem, we need to compute the linear kernel between two input vectors, which is defined as their dot product. The dot product of two vectors is a fundamental operation in linear algebra and is widely used in various machine learning algorithms.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understand the Linear Kernel**: The linear kernel between two vectors is simply their dot product. The dot product of two vectors is the sum of the products of their corresponding elements.\n2. **Use Numpy for Computation**: The numpy library in Python provides a convenient `dot` function to compute the dot product of two arrays. This function efficiently handles the computation, especially for large vectors.\n3. **Function Implementation**: Implement a function that takes two vectors as input, computes their dot product using numpy's `dot` function, and returns the result.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\n```\n\n### Explanation\n- **Importing Numpy**: The numpy library is imported at the beginning of the code to leverage its efficient numerical operations.\n- **Function Definition**: The function `kernel_function` takes two parameters, `x1` and `x2`, which are the input vectors.\n- **Computing Dot Product**: Inside the function, `np.dot(x1, x2)` computes the dot product of the two vectors. This operation is vectorized and optimized for performance, making it suitable for large datasets.\n- **Return Result**: The result of the dot product is returned as the output of the function.\n\nThis approach ensures that the computation is efficient and concise, leveraging the capabilities of the numpy library for handling vector operations."}
{"task_id": 46, "completion_id": 0, "completion": "To solve this problem, we need to implement a function to calculate the precision metric given two numpy arrays: `y_true` and `y_pred`. Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP). \n\n### Approach\n1. **Understanding Precision**: Precision is a metric used to evaluate the performance of a classification model. It measures the accuracy of positive predictions. The formula for precision is:\n   \\[\n   \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}\n   \\]\n   Here, True Positives (TP) are the cases where both the true label and the predicted label are 1. False Positives (FP) are the cases where the true label is 0 but the predicted label is 1.\n\n2. **Calculate TP and FP**: \n   - True Positives (TP) can be calculated as the sum of the element-wise product of `y_true` and `y_pred`.\n   - False Positives (FP) can be calculated as the sum of the element-wise product of `y_pred` and the complement of `y_true` (i.e., `1 - y_true`).\n\n3. **Handle Edge Cases**: If both TP and FP are zero, the precision is undefined. In such cases, we return 0 to avoid division by zero.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Calculate True Positives and False Positives\n    tp = np.sum(y_true * y_pred)\n    fp = np.sum(y_pred * (1 - y_true))\n    \n    # Avoid division by zero\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)\n```\n\n### Explanation\n- **True Positives (TP)**: This is computed by taking the element-wise product of `y_true` and `y_pred` and summing the results. This gives the count of cases where both the true label and the prediction are 1.\n- **False Positives (FP)**: This is computed by taking the element-wise product of `y_pred` and the complement of `y_true` (i.e., `1 - y_true`). Summing these results gives the count of cases where the true label is 0 but the prediction is 1.\n- **Precision Calculation**: The precision is then calculated as the ratio of TP to the sum of TP and FP. If both TP and FP are zero, precision is set to 0 to avoid division by zero.\n\nThis approach efficiently computes the precision metric using numpy operations, ensuring that the solution is both concise and performant."}
{"task_id": 66, "completion_id": 0, "completion": "To solve this problem, we need to compute the orthogonal projection of a vector **v** onto another vector **L**. The orthogonal projection is the vector on **L** that is closest to **v**. This can be achieved using vector algebra, specifically the dot product.\n\n### Approach\nThe approach involves the following steps:\n\n1. **Compute the Dot Product of v and L**: This gives a scalar value that represents the projection factor.\n2. **Compute the Dot Product of L with Itself**: This gives the squared magnitude of vector **L**, which is used as the denominator in the projection formula.\n3. **Calculate the Scalar Projection Factor**: This is done by dividing the dot product of **v** and **L** by the dot product of **L** with itself.\n4. **Compute the Projection Vector**: Multiply the scalar projection factor by each component of vector **L** to get the projection vector.\n5. **Round the Result**: Each component of the projection vector is rounded to three decimal places to meet the problem requirements.\n\n### Solution Code\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L, rounded to three decimal places.\n    \"\"\"\n    # Compute the dot product of v and L\n    v_dot_L = sum(x * y for x, y in zip(v, L))\n    # Compute the dot product of L with itself\n    l_dot_l = sum(x * x for x in L)\n    # Compute the scalar projection factor\n    scalar = v_dot_L / l_dot_l\n    # Compute the projection vector\n    projection = [scalar * x for x in L]\n    # Round each component to three decimal places\n    rounded_projection = [round(comp, 3) for comp in projection]\n    return rounded_projection\n```\n\n### Explanation\n1. **Dot Product Calculation**: The dot product of two vectors is computed by summing the products of their corresponding components. This is used to determine the projection factor and the squared magnitude of the direction vector **L**.\n2. **Scalar Projection Factor**: This factor is computed by dividing the dot product of **v** and **L** by the dot product of **L** with itself. This factor"}
{"task_id": 72, "completion_id": 0, "completion": "To solve this problem, we need to implement a function to calculate the Jaccard Index for binary classification. The Jaccard Index measures the similarity between two sets and is defined as the size of the intersection divided by the size of the union of the sets. This index ranges from 0 to 1, where 0 indicates no overlap and 1 indicates perfect overlap.\n\n### Approach\n1. **Convert Inputs to NumPy Arrays**: Convert the input lists `y_true` and `y_pred` into NumPy arrays to leverage efficient array operations.\n2. **Compute Intersection**: Use element-wise logical AND"}
{"task_id": 73, "completion_id": 0, "completion": "To solve this problem, we need to compute the Dice Score (also known as the S\u00f8rensen-Dice coefficient) between two binary arrays, `y_true` and `y_pred`. The Dice Score measures the similarity between two sets and is particularly useful in tasks like image segmentation and binary classification.\n\n### Approach\nThe Dice Score is defined as:\n\\[\nDice Score = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}}\n\\]\n\nThe steps to compute the Dice Score are as follows:\n1. **Compute the Intersection**: Calculate the number of positions where both `y_true` and `y_pred` have a 1. This is done using element-wise multiplication or a bitwise AND operation.\n2. **Sum of True Positives**: Calculate the total number of 1s in `y_true`.\n3. **Sum of Predicted Positives**: Calculate the total number of 1s in `y_pred`.\n4. **Handle Edge Cases**: If the sum of true positives and predicted positives is zero, return 0.0 to avoid division by zero.\n5. **Compute Dice Score**: Use the formula to compute the score and round it to three decimal places.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    intersection = np.sum(y_true & y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    denominator = sum_true + sum_pred\n    if denominator == 0:\n        return 0.0\n    else:\n        dice = (2 * intersection) / denominator\n        return round(dice, 3)\n```\n\n### Explanation\n1. **Intersection Calculation**: Using `np.sum(y_true & y_pred)`, we find the number of elements where both arrays have a 1. This gives the intersection count.\n2. **Sum of Elements**: `np.sum(y_true)` and `np.sum(y_pred)` give the total number of 1s in the true and predicted arrays, respectively.\n3. **Denominator Check**: If the sum of true and predicted elements is zero, it means there are no elements to compare, so the score is 0.0.\n4. **Dice Score Calculation**: The formula is applied to compute the score, which is then rounded to three decimal places for the final result.\n\nThis approach efficiently handles all edge cases and ensures the Dice Score is computed accurately and efficiently using numpy operations."}
{"task_id": 75, "completion_id": 0, "completion": "To solve this problem, we need to generate a confusion matrix for a binary classification problem. A confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positives, false positives, true negatives, and false negatives.\n\n### Approach\n1. **Problem Analysis**: The input is a list of lists where each inner list contains two elements: the true label (`y_true"}
{"task_id": 76, "completion_id": 0, "completion": "To solve this problem, we need to calculate the cosine similarity between two vectors. Cosine similarity measures the cosine of the angle between two vectors, which indicates their directional similarity. This measure is widely used in various applications such as information retrieval, text analysis, and recommendation systems.\n\n### Approach\n1. **Dot Product Calculation**: The dot product of two vectors is computed using the formula:\n   \\[\n   \\text{dot\\_product} = v1 \\cdot v2 = \\sum_{i=1}^{n} v1_i \\times v2_i\n   \\]\n   In Python, this can be efficiently calculated using `np.dot(v1, v2)` from the numpy library.\n\n2. **Magnitude Calculation**: The magnitude (or Euclidean norm) of a vector is computed using the formula:\n   \\[\n   \\text{magnitude} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\n   \\]\n   This can be calculated using `np.linalg.norm(vector)` from the numpy library.\n\n3. **Cosine Similarity Calculation**: The cosine similarity is then determined by dividing the dot product by the product of the magnitudes of the two vectors:\n   \\[\n   \\text{cosine\\_similarity} = \\frac{\\text{dot\\_product}}{|\\text{v1}| \\times |\\text{v2}|}\n   \\]\n\n4. **Rounding**: The result is rounded to three decimal places to meet the problem's requirements.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    magnitude1 = np.linalg.norm(v1)\n    magnitude2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude1 * magnitude2)\n    return round(similarity, 3)\n```\n\n### Explanation\n- **Dot Product**: The dot product is computed to understand how similar the vectors are in direction.\n- **Magnitudes**: Each vector's magnitude is calculated to normalize the dot product, ensuring that the similarity measure is not biased by the vectors' lengths.\n- **Cosine Similarity**: By normalizing the dot product with the product of the vectors' magnitudes, we get a value between -1 and 1, where 1 indicates identical directions, 0 indicates orthogonality, and -1 indicates opposite directions.\n- **Rounding**: The result is rounded to three decimal places for precision and readability.\n\nThis approach efficiently computes the cosine similarity using vector operations, ensuring accuracy and performance even for larger vectors."}
{"task_id": 79, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with a probability p of success. This can be efficiently done using the binomial distribution formula.\n\n### Approach\nThe binomial distribution formula is given by:\n\\[ P = C(n, k) \\times p^k \\times (1-p)^{n-k} \\]\nWhere:\n- \\( C(n, k) \\) is the binomial coefficient, which represents the number of ways to choose k successes out of n trials.\n- \\( p^k \\) is the probability of achieving k successes.\n- \\( (1-p)^{n-k} \\) is the probability of achieving (n-k) failures.\n\nThe steps to compute this probability are:\n1. Calculate the binomial coefficient \\( C(n, k) \\) using the formula \\( C(n, k) = \\frac{n!}{k!(n-k)!} \\).\n2. Compute \\( p^k \\) and \\( (1-p)^{n-k} \\).\n3. Multiply these three values together to get the probability.\n4. Round the result to five decimal places for the final output.\n\n### Solution Code\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    if k < 0 or k > n:\n        return 0.0\n    combination = math.comb(n, k)\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    return round(probability, 5)\n```\n\n### Explanation\n- The function starts by importing the `math` module to use the `math.comb` function for calculating the binomial coefficient.\n- It then checks if k is within the valid range (0 to n). If not, it returns 0.0 as the probability is zero.\n- The binomial coefficient is calculated using `math.comb(n, k)`.\n- The probability of achieving k successes is calculated as \\( p^k \\).\n- The probability of achieving (n-k) failures is calculated as \\( (1-p)^{n-k} \\).\n- These values are multiplied together along with the binomial coefficient to get the final probability.\n- The result is rounded to five decimal places and returned.\n\nThis approach ensures that the function efficiently and accurately computes the required probability using the binomial distribution formula."}
{"task_id": 80, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the probability density function (PDF) of the normal distribution for a given value, mean, and standard deviation. The function should return the PDF value rounded to 5 decimal places.\n\n### Approach\nThe normal distribution's PDF is given by the formula:\n\\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\]\nwhere:\n- \\( \\mu \\) is the mean,\n- \\( \\sigma \\) is the standard deviation,\n- \\( x \\) is the value at which the PDF is evaluated.\n\nThe steps to compute the PDF are as follows:\n1. Calculate the exponent part: \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\).\n2. Compute the exponential part using the `math.exp()` function.\n3. Compute the normalization factor: \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\).\n4. Multiply the normalization factor by the exponential part to get the PDF value.\n5. Round the result to 5 decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    pdf = (1 / (std_dev * math.sqrt(2 * math.pi))) * math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    return round(pdf, 5)\n```\n\n### Explanation\n1. **Exponent Calculation**: The exponent in the formula is computed as \\( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\). This part adjusts the shape of the normal distribution curve based on how far `x` is from the mean `\u03bc` and the spread determined by the standard deviation `\u03c3`.\n2. **Exponential Function**: Using `math.exp()`, we compute the exponential part of the formula, which gives the height of the curve at the point `x`.\n3. **Normalization Factor**: The normalization factor \\( \\frac{1}{\\sigma \\sqrt{2\\pi}} \\) ensures that the total area under the curve of the normal distribution equals 1, making it a valid probability density function.\n4. **Result Calculation**: The final PDF value is obtained by multiplying the normalization factor with the exponential part.\n5. **Rounding**: The result is rounded to 5 decimal places to meet the problem's requirement for precision.\n\nThis approach efficiently computes the PDF using the mathematical formula, ensuring accuracy and correctness for any valid input values."}
{"task_id": 81, "completion_id": 0, "completion": "To solve this problem, we need to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution formula. The Poisson distribution is used to model the number of events occurring within a given time period, given the average rate of occurrence.\n\n### Approach\nThe Poisson probability formula is given by:\n\\[ P(k; \\lambda) = \\frac{(\\lambda^k \\cdot e^{-\\lambda})}{k!} \\]\nwhere:\n- \\( \\lambda \\) (lam) is the average rate (mean) of occurrences.\n- \\( k \\) is the number of events we are interested in.\n\nThe steps to compute this probability are:\n1. Calculate \\( \\lambda \\) raised to the power of \\( k \\).\n2. Compute \\( e \\) raised to the power of \\( -\\lambda \\).\n3. Compute the factorial of \\( k \\).\n4. Divide the product of the first two results by the factorial result.\n5. Round the final result to 5 decimal places.\n\n### Solution Code\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    numerator = (lam ** k) * math.exp(-lam)\n    denominator = math.factorial(k)\n    probability = numerator / denominator\n    return round(probability, 5)\n```\n\n### Explanation\nThe function `poisson_probability` takes two parameters, `k` (number of events) and `lam` (mean rate of occurrences). It calculates the probability using the Poisson distribution formula by computing the necessary components step-by-step:\n1. The numerator is computed as \\( \\lambda^k \\cdot e^{-\\lambda} \\).\n2. The denominator is the factorial of \\( k \\).\n3. The probability is the division of the numerator by the denominator.\n4. The result is rounded to 5 decimal places to ensure precision and meet the problem requirements.\n\nThis approach efficiently computes the desired probability using fundamental mathematical operations and ensures accuracy through rounding."}
{"task_id": 82, "completion_id": 0, "completion": "To solve this problem, we need to calculate the contrast of a grayscale image. The contrast is defined as the difference between the maximum and minimum pixel values in the image. This is a straightforward task given that the image is represented as a 2D numpy array with pixel values ranging from 0 to 255.\n\n### Approach\n1. **Problem Analysis**: The task is to compute the contrast of a grayscale image. The contrast is determined by the difference between the highest and lowest pixel intensities in the image.\n2. **Intuition**: For a given 2D array representing an image, the maximum pixel value can be found using the `max()` function, and the minimum pixel value can be found using the `min()` function. The contrast is simply the difference between these two values.\n3. **Algorithm Selection**: Using numpy's built-in `max()` and `min()` functions will efficiently compute the required values. These functions are optimized for performance and are appropriate for handling numpy arrays.\n4. **Complexity Analysis**: The time complexity for finding the maximum and minimum values in a 2D array using numpy's functions is O(n*m), where n and m are the dimensions of the array. This is efficient for typical image sizes.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_val = img.max()\n    min_val = img.min()\n    return max_val - min_val\n```\n\n### Explanation\n- **Function Definition**: The function `calculate_contrast` takes a 2D numpy array `img` as input.\n- **Compute Maximum and Minimum Values**: Using `img.max()`, we find the highest pixel value in the image, and `img.min()` finds the lowest pixel value.\n- **Calculate Contrast**: The contrast is computed as the difference between the maximum and minimum values, which is returned as the result.\n\nThis approach efficiently computes the contrast using built-in numpy functions, ensuring that the solution is both simple and performant."}
{"task_id": 83, "completion_id": 0, "completion": "To solve this problem, we need to write a Python function that calculates the dot product of two vectors using NumPy. The dot product of two vectors is a scalar value obtained by summing the products of their corresponding elements. This is a common operation in linear algebra and has various applications in fields like physics, engineering, and machine learning.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understand the Dot Product**: The dot product of two vectors is computed by multiplying corresponding elements and then summing those products. For vectors `vec1` and `vec2`, the dot product is given by the sum of `vec1[i] * vec2[i]` for all valid indices `i`.\n2. **Use NumPy's Built-in Function**: NumPy provides a convenient function `np.dot()` that efficiently computes the dot product of two arrays. This function handles the underlying operations, making the implementation straightforward and efficient.\n3. **Function Implementation**: The function `calculate_dot_product` takes two 1D NumPy arrays as input and returns their dot product using `np.dot()`.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    return np.dot(vec1, vec2)\n```\n\n### Explanation\n- **Import NumPy**: The code starts by importing the NumPy library, which is necessary for handling arrays and performing vector operations efficiently.\n- **Define the Function**: The function `calculate_dot_product` is defined to take two arguments, `vec1` and `vec2`, which are 1D NumPy arrays.\n- **Compute Dot Product**: Inside the function, `np.dot(vec1, vec2)` is used to compute the dot product. This function efficiently handles the element-wise multiplication and summation, even for large arrays.\n- **Return the Result**: The result of the dot product computation is returned as a single number.\n\nThis approach ensures that the function is both efficient and easy to understand, leveraging NumPy's optimized operations for vector calculations."}
{"task_id": 86, "completion_id": 0, "completion": "To solve this problem, we need to determine whether a machine learning model is overfitting, underfitting, or performing well based on its training and test accuracy values. The function will return specific values to indicate the model's performance quality.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n\n1. **Check for Overfitting**: Determine if the model is overfitting by checking if the training accuracy is significantly higher than the test accuracy. Specifically, if the difference between training accuracy and test accuracy is greater than 0.2, the model is considered overfitted.\n2. **Check for Underfitting**: If the model is not overfitted, then check if both the training and test accuracies are below 0.7. If both are below this threshold, the model is considered underfitted.\n3. **Good Fit**: If neither of the above conditions is met, the model is considered to be performing well.\n\n### Solution Code\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if (training_accuracy - test_accuracy) > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n```\n\n### Explanation\n- **Overfitting Check**: The function first checks if the difference between training accuracy and test accuracy is greater than 0.2. If true, it returns 1, indicating overfitting.\n- **Underfitting Check**: If the model is not overfitted, the function then checks if both training and test accuracies are below 0.7. If true, it returns -1, indicating underfitting.\n- **Good Fit Check**: If neither overfitting nor underfitting conditions are met, the function returns 0, indicating that the model is performing well.\n\nThis approach ensures that the model's performance is evaluated efficiently and accurately based on the given criteria."}
{"task_id": 93, "completion_id": 0, "completion": "To solve this problem, we need to implement a function to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. The MAE is a metric used to measure the average magnitude of errors in a set of predictions without considering their direction.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Compute Absolute Differences**: Calculate the absolute differences between each corresponding element of the true values and the predicted values.\n2. **Calculate Mean of Absolute Differences**: Compute the mean of these absolute differences to get the MAE.\n3. **Round the Result**: Round the computed MAE to three decimal places for the final result.\n\nThe steps are implemented using numpy arrays for efficient computation. The mean of the absolute differences is calculated using numpy's built-in functions, ensuring that the solution is both concise and efficient.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    differences = np.abs(y_true - y_pred)\n    ma = np.mean(differences)\n    return round(ma, 3)\n```\n\n### Explanation\n- **Compute Absolute Differences**: The function first computes the absolute differences between each element of `y_true` and `y_pred` using `np.abs(y_true - y_pred)`. This gives an array of absolute errors.\n- **Calculate Mean of Absolute Differences**: The mean of these absolute errors is computed using `np.mean()`, which gives the average absolute error, i.e., the MAE.\n- **Round the Result**: The computed MAE is then rounded to three decimal places using `round(ma, 3)` to meet the requirement of returning the result rounded to three decimal places.\n\nThis approach ensures that the function is efficient and leverages numpy's optimized functions for array operations, making it suitable for handling large datasets."}
{"task_id": 96, "completion_id": 0, "completion": "To solve this problem, we need to implement the Hard Sigmoid activation function, which is a computationally efficient approximation of the standard sigmoid function. This function is commonly used in machine learning and neural networks due to its computational efficiency.\n\n### Approach\nThe Hard Sigmoid function is a piecewise linear function that approximates the sigmoid function. It is defined as follows:\n- For input values less than -2.5, the output is 0.\n- For input values greater than 2.5, the output is 1.\n- For values between -2.5 and 2.5, the output is a linear transformation of the input. Specifically, it is calculated as (x + 2.5) / 5, which simplifies to 0.2x + 0.5.\n\nThis approach ensures that the function is both efficient and easy to implement. The function can be expressed concisely using the `max` and `min` functions to handle the piecewise nature of the function.\n\n### Solution Code\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, (x + 2.5) / 5.0))\n```\n\n### Explanation\n- The function `hard_sigmoid` takes a single floating-point input `x`.\n- It calculates the transformed value using the formula `(x + 2.5) / 5.0`, which linearly maps the input range from -2.5 to 2.5 to the output range from 0 to 1.\n- The `max` function ensures that the output does not drop below 0, and the `min` function ensures it does not exceed 1.\n- This implementation efficiently computes the Hard Sigmoid without the need for conditional checks, making it both concise and computationally efficient."}
{"task_id": 97, "completion_id": 0, "completion": "To solve this problem, we need to implement the ELU (Exponential Linear Unit) activation function, which helps mitigate the limitations of ReLU by allowing negative outputs for negative inputs. The ELU function is defined as follows: for a given input value `x`, if `x` is positive, the function returns `x`. If `x` is negative, the function returns `alpha * (exp(x) - 1)`, where `alpha` is a parameter that controls the slope for negative inputs. The result should be rounded to four decimal places.\n\n### Approach\nThe approach to implement the ELU function involves the following steps:\n\n1. **Import the Math Module**: This is necessary to use the `exp` function, which computes the exponential of a given number.\n2. **Define the Function**: The function `elu` takes two parameters: `x` (the input value) and `alpha` (the parameter for negative values, defaulting to 1.0).\n3. **Check Input Value**: If `x` is non-negative, the function returns `x` rounded to four decimal places. If `x` is negative, it computes the ELU value using the formula `alpha * (exp(x) - 1)` and then rounds the result to four decimal places.\n4. **Return the Result**: The computed value is returned after rounding to ensure precision.\n\n### Solution Code\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places.\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```\n\n### Explanation\n- **Importing the Math Module**: The `math` module is imported to use the `exp` function, which is essential for computing the exponential part of the ELU function.\n- **Function Definition**: The function `elu` is defined with two parameters: `x` and `alpha`, where `alpha` has a default value of 1.0.\n- **Condition Check**: The function checks if `x` is non-negative. If true, it returns `x` rounded to four decimal places. If `x` is negative, it computes the ELU value using the formula `alpha * (math.exp(x) - 1)` and then rounds the result to four decimal places.\n- **Rounding**: The result is rounded to four decimal places using Python's built-in `round` function to ensure the output meets the required precision.\n\nThis approach ensures that the ELU function is implemented efficiently and correctly, handling both positive and negative input values appropriately."}
{"task_id": 98, "completion_id": 0, "completion": "To solve this problem, we need to implement the PReLU (Parametric Rectified Linear Unit) activation function. PReLU is a variant of the ReLU activation function that introduces a learnable parameter for negative inputs, which can be optimized during training. This makes PReLU more flexible than the standard ReLU function.\n\n### Approach\nThe PReLU function is defined as follows: for a given input value `x`, if `x` is positive, the function returns `x`. If `x` is negative or zero, the function returns `alpha * x`, where `alpha` is a learnable parameter. The default value for `alpha` is 0.25, but it can be adjusted during training.\n\nThe approach to implement this function involves the following steps:\n1. Check if the input value `x` is non-negative (i.e., `x >= 0`). If true, return `x`.\n2. If `x` is negative, return the product of `alpha` and `x`.\n\nThis approach ensures that the function behaves like a ReLU for positive inputs and introduces a learned parameter for negative inputs, allowing the model to adjust the slope of the negative part during training.\n\n### Solution Code\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\n```\n\n### Explanation\n- The function `prelu` takes two parameters: `x` (the input value) and `alpha` (the slope parameter for negative inputs, defaulting to 0.25).\n- The function checks if `x` is non-negative. If `x` is non-negative, it returns `x`, which is the standard ReLU behavior.\n- If `x` is negative, the function returns `alpha * x`, allowing the slope of the negative part to be adjusted during training. This flexibility can lead to better performance in certain neural network architectures compared to the standard ReLU function."}
{"task_id": 100, "completion_id": 0, "completion": "To solve this problem, we need to implement the Softsign activation function, which is a smooth activation function used in neural networks. The function ensures that the output is bounded between -1 and 1, making it useful for training neural networks.\n\n### Approach\nThe Softsign function is defined as \\( f(x) = \\frac{x}{1 + |x|} \\). This function takes any real-valued input and maps it to a value between -1 and 1. Here's the step-by-step approach to implement this function:\n\n1. **Compute the Absolute Value**: Calculate the absolute value of the input \\( x \\) using `abs(x)`.\n2. **Calculate the Denominator**: Add 1 to the absolute value obtained in the previous step.\n3. **Compute the Result**: Divide the input \\( x \\) by the denominator calculated in the previous step.\n4. **Round the Result**: Round the result to four decimal places to ensure the output meets the required precision.\n\nThis approach ensures that the function is smooth and differentiable, which is beneficial for training neural networks using gradient-based optimization methods.\n\n### Solution Code\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to four decimal places.\n    \"\"\"\n    denominator = 1 + abs(x)\n    result = x / denominator\n    return round(result, 4)\n```\n\n### Explanation\n- **Softsign Function**: The function \\( f(x) = \\frac{x}{1 + |x|} \\) is implemented to compute the Softsign value for a given input \\( x \\).\n- **Absolute Value**: The absolute value of \\( x \\) is computed to handle both positive and negative inputs uniformly.\n- **Denominator Calculation**: Adding 1 to the absolute value ensures that the denominator is always positive and greater than or equal to 1, preventing division by zero.\n- **Division and Rounding**: The input \\( x \\) is divided by the computed denominator, and the result is rounded to four decimal places to meet the precision requirement.\n\nThis implementation efficiently computes the Softsign activation function, ensuring the output is within the range [-1, 1], making it suitable for use in neural network applications."}
{"task_id": 102, "completion_id": 0, "completion": "To solve this problem, we need to implement the Swish activation function, which is a self-gated activation function used in deep learning. The Swish function is defined as the product of the input value and the sigmoid of the input value. This function is known for its superior performance compared to ReLU in certain deep neural network architectures.\n\n### Approach\nThe Swish activation function can be broken down into the following steps:\n1. **Compute the Sigmoid Function**: The sigmoid function is defined as \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This function maps any real-valued number to a value between 0 and 1.\n2. **Multiply the Input by the Sigmoid Value**: The Swish function is then computed as \\( \\text{Swish}(x) = x \\times \\sigma(x) \\).\n3. **Round the Result**: The result is rounded to the nearest 4th decimal place to ensure precision.\n\n### Solution Code\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    result = x * sigmoid_x\n    return round(result, 4)\n```\n\n### Explanation\n1. **Sigmoid Calculation**: The sigmoid function is computed using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This is implemented using `math.exp(-x)` to compute \\( e^{-x} \\) and then applying the formula.\n2. **Swish Calculation**: The input value `x` is multiplied by the computed sigmoid value to get the Swish value.\n3. **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the output meets the required precision.\n\nThis approach ensures that the Swish function is computed efficiently and accurately, adhering to the constraints and requirements of the problem."}
{"task_id": 104, "completion_id": 0, "completion": "To solve this problem, we need to implement a binary classification prediction using Logistic Regression. The goal is to compute class probabilities using the sigmoid function and return binary predictions based on a threshold of 0.5.\n\n### Approach\n1. **Compute Linear Combination**: First, we compute the linear combination of the input features (X) with the model weights (weights) and add the model bias (bias). This is done using the matrix multiplication of X and weights, followed by adding the bias term.\n2. **Apply Sigmoid Function**: The result of the linear combination is then passed through the sigmoid function. The sigmoid function maps any real-valued number to a value between 0 and 1, which represents the probability that the input belongs to the positive class (class 1).\n3. **Convert Probabilities to Binary Predictions**: Using a threshold of 0.5, we convert the probabilities into binary predictions. If the probability is greater than or equal to 0.5, we predict the class as 1; otherwise, we predict it as 0.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a list.\n    \"\"\"\n    z = X.dot(weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int).tolist()\n    return predictions\n```\n\n### Explanation\n1. **Linear Combination Calculation**: The input matrix X (shape N x D) is multiplied by the weights vector (shape D), resulting in a vector of size N. Adding the bias term (a scalar) to this vector gives us the linear combination (z).\n2. **Sigmoid Function Application**: The sigmoid function is applied element-wise to the linear combination z. This transforms the output into probabilities between 0 and 1.\n3. **Binary Prediction Conversion**: Each probability value is compared against the threshold of 0.5. If the probability is 0.5 or higher, the prediction is 1; otherwise, it is 0. The resulting binary predictions are converted to a list using the `tolist()` method for consistency with the required output format.\n\nThis approach efficiently computes the binary predictions using the principles of logistic regression, ensuring that the output is in the correct format and meets the problem constraints."}
{"task_id": 109, "completion_id": 0, "completion": "To solve this problem, we need to implement a function for layer normalization of a 3D tensor. Layer normalization involves normalizing the input tensor across the feature dimension for each sequence, then applying scaling and shifting parameters. \n\n### Approach\n1. **Compute Mean and Variance**: For each element in the input tensor, compute the mean and variance across the feature dimension (the third dimension). This is done using numpy functions `mean` and `var` with the `keepdims` parameter set to `True` to maintain the tensor shape.\n2. **Normalize the Tensor**: Subtract the computed mean from each element and divide by the square root of the computed variance plus a small epsilon (1e-5) to avoid division by zero.\n3. **Scale and Shift**: Multiply the normalized tensor by the scaling parameter `gamma` and add the shifting parameter `beta`. This step adjusts the normalized values to the desired scale and shift.\n4. **Rounding and Conversion**: Round the resulting tensor to 5 decimal places and convert it to a list using `tolist()` for the final output.\n\n### Solution Code\n```"}
{"task_id": 111, "completion_id": 0, "completion": "To solve this problem, we need to compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\n### Approach\nThe approach to compute PMI involves the following steps:\n\n1. **Understand the Formula**: PMI is calculated using the formula:\n   \\[\n   \\text{PMI} = \\log\\left(\\frac{P(x, y)}{P(x) \\times P(y)}\\right)\n   \\]\n   where \\(P(x, y)\\) is the joint probability of events \\(x\\) and \\(y\\), \\(P(x)\\) is the probability of event \\(x\\), and \\(P(y)\\) is the probability of event \\(y\\).\n\n2. **Calculate Probabilities**:\n   - The joint probability \\(P(x, y)\\) is the joint occurrence count divided by the total number of samples.\n   - The individual probabilities \\(P(x)\\) and \\(P(y)\\) are the respective counts divided by the total number of samples.\n\n3. **Compute the Ratio**: Using the probabilities, compute the ratio of the joint probability to the product of the individual probabilities.\n\n4. **Apply Logarithm**: Take the natural logarithm of the computed ratio to get the PMI.\n\n5. **Round the Result**: Finally, round the result to three decimal places for the output.\n\n### Solution Code\n\n```python\nimport math\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    ratio = (joint_counts * total_samples) / (total_counts_x * total_counts_y)\n    pmi = math.log(ratio)\n    return round(pmi, 3)\n```\n\n### Explanation\n- **Ratio Calculation**: The ratio is calculated as \\(\\frac{\\text{joint\\_counts} \\times \\text{total\\_samples}}{\\text{total\\_counts\\_x} \\times \\text{total\\_counts\\_y}}\\). This step normalizes the counts relative to the total number of samples.\n- **Logarithm**: The natural logarithm is used to compute the PMI, which scales the ratio to a more interpretable measure.\n- **Rounding**: The result is rounded to three decimal places to meet the problem's requirements.\n\nThis approach ensures that we accurately compute the PMI"}
{"task_id": 113, "completion_id": 0, "completion": "To solve this problem, we need to implement a simple residual block using NumPy. The residual block processes a 1D input array through two weight layers, applies ReLU activations, adds the original input via a shortcut connection, and then applies a final ReLU activation. The result is rounded to 4 decimal places and converted to a list.\n\n### Approach\nThe approach involves the following steps:\n1. **First Weight Layer**: Multiply the input array by the first weight matrix and apply the ReLU activation.\n2. **Second Weight Layer**: Multiply the output of the first ReLU activation by the second weight matrix.\n3. **Shortcut Connection**: Add the original input to the output of the second weight layer.\n4. **Final ReLU Activation**: Apply the ReLU activation to the result of the shortcut connection.\n5. **Rounding and Conversion**: Round the final output to 4 decimal places and convert it to a list.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    a = x.dot(w1)\n    a = np.maximum(0, a)\n    b = a.dot(w2)\n    c = b + x\n    output = np.maximum(0, c)\n    return output.round(4).tolist()\n```\n\n### Explanation\n1. **First Weight Layer**: The input array `x` is multiplied by the weight matrix `w1` using matrix multiplication. The result is then passed through a ReLU activation, which sets all negative values to zero.\n2. **Second Weight Layer**: The output from the first ReLU activation is multiplied by the weight matrix `w2` again using matrix multiplication.\n3. **Shortcut Connection**: The original input array `x` is added to the output of the second weight layer. This step is crucial in the residual block as it allows the network to learn residuals, which helps in training deeper networks by addressing the vanishing gradient problem.\n4. **Final ReLU Activation**: The sum from the shortcut connection is passed through a ReLU activation to introduce non-linearity and ensure all negative values are zero.\n5. **Rounding and Conversion**: The final output is rounded to 4 decimal places to meet the precision requirement and then converted to a list using `tolist()` for the required output format.\n\nThis approach ensures that the residual block is implemented efficiently using NumPy operations, which are optimized for performance with arrays and matrices."}
{"task_id": 114, "completion_id": 0, "completion": "To solve this problem, we need to implement a function that performs Global Average Pooling on a 3D NumPy array. This involves computing the average of all values in each feature map across all spatial dimensions.\n\n### Approach\nThe approach to solve this problem involves the following steps:\n1. **Understand the Input Structure**: The input is a 3D NumPy array with shape (height, width, channels). Each element in the last dimension (channels) represents a feature map.\n2. **Compute Averages**: For each feature map (each channel), compute the average of all the pixel values across the entire height and width. This can be efficiently done using the `mean` function from NumPy.\n3. **Axis Specification**: Use the `mean` function with `axis=(0,1)` to average across the height and width dimensions, leaving us with an array of averages for each channel. This results in a 1D array of shape (channels,).\n\n### Solution Code\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    return x.mean(axis=(0, 1))\n```\n\n### Explanation\n- **Global Average Pooling Concept**: This operation reduces the spatial dimensions (height and width) of the feature maps to a single value for each channel. This is done by computing the average of all elements in each feature map.\n- **Efficient Computation**: Using NumPy's `mean` function with `axis=(0,1)` allows us to efficiently compute the average across all spatial dimensions for each channel in a vectorized manner, ensuring the solution is both concise and performant.\n\nThis approach ensures that each feature map is reduced to a single scalar value representing its average, which is crucial for tasks like feature extraction and dimensionality reduction in convolutional neural networks."}
{"task_id": 116, "completion_id": 0, "completion": "To solve this problem, we need to compute the derivative of a polynomial term of the form \\( c \\times x^n \\) at a given point \\( x \\). The function should return the value of the derivative, rounded to four decimal places.\n\n### Approach\nThe problem can be approached using the power rule from calculus, which states that the derivative of \\( c \\times x^n \\) is \\( c \\times n \\times x^{(n-1)} \\). Here's the step-by-step approach:\n\n1. **Apply the Power Rule**: The derivative of \\( c \\times x^n \\) with respect to \\( x \\) is calculated using the formula \\( c \\times n \\times x^{(n-1)} \\).\n2. **Handle Special Cases**: If \\( n \\) is zero, the term \\( x^n \\) becomes 1, and its derivative is zero. If \\( n \\) is 1, the term simplifies to \\( c \\times x \\), whose derivative is \\( c \\).\n3. **Round the Result**: The result is rounded to four decimal places to meet the problem's requirements.\n\n### Solution Code\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n```\n\n### Explanation\n- **Power Rule Application**: The function first calculates the derivative using the formula \\( c \\times n \\times x^{(n-1)} \\). This handles all cases of \\( n \\) correctly, including when \\( n \\) is zero or one.\n- **Rounding**: The result is rounded to four decimal places using Python's `round` function to ensure the output meets the specified precision requirement.\n\nThis approach efficiently computes the derivative and handles all edge cases, ensuring accurate results for any valid input values."}
{"task_id": 121, "completion_id": 0, "completion": "To solve this problem, we need to compute the element-wise sum of two vectors. The vectors can only be summed if they have the same length. If they do not, we return -1. Otherwise, we return a new vector where each element is the sum of the corresponding elements from the input vectors.\n\n### Approach\n1. **Check Lengths**: First, we check if the lengths of the two input vectors are the same. If they are not, we return -1 immediately.\n2. **Element-wise Sum**: If the lengths are the same, we proceed to compute the sum of each corresponding element from the two vectors. This is done using a list comprehension that iterates over each index and adds the elements from both vectors.\n3. **Return Result**: The resulting list of sums is returned as the output.\n\n### Solution Code\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]\n```\n\n### Explanation\n- **Check Lengths**: The function first checks if the lengths of the input vectors `a` and `b` are equal. If they are not, it returns -1.\n- **Element-wise Sum**: Using a list comprehension, the function iterates over each index from 0 to the length of the vectors. For each index, it adds the corresponding elements from `a` and `b` and collects these sums into a new list.\n- **Return Result**: The resulting list of sums is returned. If the vectors were of different lengths, the function returns -1 as specified.\n\nThis approach ensures that we efficiently compute the element-wise sum when possible and handle cases where the vectors are incompatible by returning -1."}
{"task_id": 128, "completion_id": 0, "completion": "To solve this problem, we need to implement the Dynamic Tanh (DyT) function, which is a normalization-free transformation inspired by the Tanh function. This function is designed to replace layer normalization in Transformer architectures while preserving the squashing behavior and enabling stable training.\n\n### Approach\nThe DyT function involves three main parameters: alpha, gamma, and beta. The function transforms each element of the input array by first scaling the input by alpha, then applying the Tanh function, scaling the result by gamma, and finally shifting it by beta. The transformation can be broken down into the following steps:\n\n1. **Scale by Alpha**: Multiply each element of the input array by alpha.\n2. **Apply Tanh**: Compute the hyperbolic tangent (Tanh) of each scaled element.\n3. **Scale by Gamma**: Multiply each Tanh result by gamma.\n4. **Shift by Beta**: Add beta to each scaled result.\n5. **Round Results**: Round each transformed result to four decimal places.\n6. **Return as List**: Convert the resulting array to a list and return it.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    transformed = gamma * np.tanh(alpha * x) + beta\n    rounded = np.round(transformed, 4)\n    return rounded.tolist()\n```\n\n### Explanation\n1. **Scaling by Alpha**: Each element of the input array `x` is scaled by the factor `alpha`. This step adjusts the slope of the activation function.\n2. **Applying Tanh**: The scaled values are then passed through the Tanh function, which squashes the values into the range (-1, 1).\n3. **Scaling by Gamma**: The output of the Tanh function is scaled by `gamma`, allowing for dynamic adjustment of the output range.\n4. **Shifting by Beta**: The scaled output is shifted by `beta`, providing an additional degree of freedom to adjust the output range.\n5. **Rounding**: Each element of the resulting array is rounded to four decimal places to ensure precision.\n6. **Conversion to List**: The final result is converted from a NumPy array to a Python list using the `tolist()` method, as required.\n\nThis approach ensures that the DyT function is both normalization-free and effective in preserving the squashing behavior necessary for stable training in"}
