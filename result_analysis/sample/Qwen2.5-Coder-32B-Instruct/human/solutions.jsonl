{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = [0] * len(a)\n    for i in range(len(a)):\n        for j in range(len(b)):\n            result[i] += a[i][j] * b[j]\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [1, 0, -1]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 1, "completion_id": 1, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((element * b[i] for (i, element) in enumerate(row)))\n        result.append(dot_product)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [1, 0, -1]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 1, "completion_id": 2, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b:\n        return -1\n    num_columns = len(a[0])\n    vector_length = len(b)\n    if num_columns != vector_length:\n        return -1\n    result_vector = []\n    for row in a:\n        dot_product = sum((row[i] * b[i] for i in range(vector_length)))\n        result_vector.append(dot_product)\n    return result_vector"}
{"task_id": 1, "completion_id": 3, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [1, 0, -1]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 1, "completion_id": 4, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 5, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6]]\nvector = [7, 8, 9]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 1, "completion_id": 6, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((row[i] * b[i] for i in range(len(b))))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 7, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 8, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 9, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6]]\nvector = [7, 8, 9]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 1, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 2, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 3, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 4, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 5, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 6, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 7, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 2, "completion_id": 8, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [list(row) for row in zip(*a)]\n    return transposed"}
{"task_id": 2, "completion_id": 9, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 1, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != np.prod(new_shape):\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 2, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 3, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    np_matrix = np.array(a)\n    if np_matrix.size != np.prod(new_shape):\n        return []\n    reshaped_np_matrix = np_matrix.reshape(new_shape)\n    reshaped_list = reshaped_np_matrix.tolist()\n    return reshaped_list"}
{"task_id": 3, "completion_id": 4, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        array = np.array(a)\n        reshaped_array = array.reshape(new_shape)\n        return reshaped_array.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 5, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        array = np.array(a)\n        reshaped_array = array.reshape(new_shape)\n        reshaped_list = reshaped_array.tolist()\n        return reshaped_list\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 6, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 7, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    reshaped_list = reshaped_array.tolist()\n    return reshaped_list"}
{"task_id": 3, "completion_id": 8, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 9, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != np.prod(new_shape):\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        if num_rows == 0:\n            return []\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(num_rows))) / num_rows for col in range(num_columns)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 1, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix or not matrix[0]:\n        raise ValueError('Matrix must not be empty and must have at least one row and one column.')\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        num_rows = len(matrix)\n        for row in matrix:\n            if len(row) != num_columns:\n                raise ValueError('All rows must have the same number of columns.')\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        return [column_sum / num_rows for column_sum in column_sums]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 2, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        if not matrix:\n            return []\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 4, "completion_id": 3, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        if not matrix or not matrix[0]:\n            return []\n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        num_rows = len(matrix)\n        for row in matrix:\n            if len(row) != num_columns:\n                raise ValueError('All rows must have the same number of columns')\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        return [column_sum / num_rows for column_sum in column_sums]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 4, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        for row in matrix:\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        return [column_sum / len(matrix) for column_sum in column_sums]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 5, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        num_rows = len(matrix)\n        for row in matrix:\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        return [column_sum / num_rows for column_sum in column_sums]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 6, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 7, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    else:\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 8, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        if not matrix:\n            return []\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 9, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if not matrix or not matrix[0]:\n        raise ValueError('Matrix must not be empty and must contain at least one row and one column')\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        num_rows = len(matrix)\n        for row in matrix:\n            if len(row) != num_columns:\n                raise ValueError('All rows in the matrix must have the same number of columns')\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        return [column_sums[i] / num_rows for i in range(num_columns)]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[scalar * element for element in row] for row in matrix]\n    return result"}
{"task_id": 5, "completion_id": 1, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the scalar and returns the resulting matrix.\n\n    :param matrix: A list of lists where each inner list represents a row of the matrix.\n    :param scalar: An integer or float to multiply each element of the matrix.\n    :return: A new matrix with each element multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 2, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result_matrix = []\n    for row in matrix:\n        result_row = [element * scalar for element in row]\n        result_matrix.append(result_row)\n    return result_matrix"}
{"task_id": 5, "completion_id": 3, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list of integers or floats representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 5, "completion_id": 4, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[i])):\n            result[i][j] = matrix[i][j] * scalar\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nscalar = 3\nresult = scalar_multiply(matrix, scalar)"}
{"task_id": 5, "completion_id": 5, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[0] * len(matrix[0]) for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 5, "completion_id": 6, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list of integers or floats representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[scalar * element for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 7, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float by which the matrix will be multiplied.\n    :return: A 2D list of integers or floats representing the scaled matrix.\n    \"\"\"\n    result = [[0] * len(matrix[0]) for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 5, "completion_id": 8, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply with the matrix.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        result_row = [element * scalar for element in row]\n        result.append(result_row)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nscalar = 3\nresult = scalar_multiply(matrix, scalar)"}
{"task_id": 5, "completion_id": 9, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A list of lists where each inner list represents a row of the matrix.\n    :param scalar: An integer or float by which the matrix is to be multiplied.\n    :return: A new matrix resulting from the scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import cmath\n    eigenvalue1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalues = sorted([eigenvalue1.real if eigenvalue1.imag == 0 else eigenvalue1, eigenvalue2.real if eigenvalue2.imag == 0 else eigenvalue2], reverse=True)\n    return eigenvalues\nmatrix = [[2, -1], [1, 2]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 6, "completion_id": 1, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigenvalue1 = (trace + discriminant ** 0.5) / 2\n    eigenvalue2 = (trace - discriminant ** 0.5) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 6, "completion_id": 2, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('The input matrix must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import math\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 0], [0, 3]]"}
{"task_id": 6, "completion_id": 3, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import cmath\n    eigenvalue1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue1 = eigenvalue1.real if eigenvalue1.imag == 0 else eigenvalue1\n    eigenvalue2 = eigenvalue2.real if eigenvalue2.imag == 0 else eigenvalue2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 6, "completion_id": 4, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import cmath\n    eigenvalue1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalues = sorted([complex(eigenvalue1).real if eigenvalue1.imag == 0 else eigenvalue1, complex(eigenvalue2).real if eigenvalue2.imag == 0 else eigenvalue2], reverse=True)\n    return eigenvalues\nmatrix = [[2, 1], [1, 2]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 6, "completion_id": 5, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import math\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[4, 2], [1, 3]]"}
{"task_id": 6, "completion_id": 6, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    lambda_1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    lambda_2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return [max(lambda_1, lambda_2), min(lambda_1, lambda_2)]\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 6, "completion_id": 7, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('The input matrix must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    import cmath\n    eigenvalue1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue1 = float(eigenvalue1.real) if eigenvalue1.imag == 0 else eigenvalue1\n    eigenvalue2 = float(eigenvalue2.real) if eigenvalue2.imag == 0 else eigenvalue2\n    eigenvalues = sorted([eigenvalue1, eigenvalue2], reverse=True)\n    return eigenvalues\nmatrix = [[4, 2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 6, "completion_id": 8, "solution": "import cmath\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    eigenvalue1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue1 = eigenvalue1.real if eigenvalue1.imag == 0 else eigenvalue1\n    eigenvalue2 = eigenvalue2.real if eigenvalue2.imag == 0 else eigenvalue2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 6, "completion_id": 9, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigenvalue1 = (trace + discriminant ** 0.5) / 2\n    eigenvalue2 = (trace - discriminant ** 0.5) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    if det_T == 0 or det_S == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    S_inv = np.linalg.inv(S_np)\n    try:\n        result = T_inv @ A_np @ S_np\n    except ValueError:\n        return -1\n    result_rounded = np.round(result, 4)\n    result_list = result_rounded.tolist()\n    return result_list\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 1, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    try:\n        T_inv = np.linalg.inv(T)\n        S_inv = np.linalg.inv(S)\n    except np.linalg.LinAlgError:\n        return -1\n    try:\n        result = T_inv @ A @ S\n    except ValueError:\n        return -1\n    result_rounded = np.round(result, 4).tolist()\n    return result_rounded"}
{"task_id": 7, "completion_id": 2, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    try:\n        result = T_inv @ A @ S\n    except ValueError:\n        return -1\n    result = np.round(result, 4)\n    return result.tolist()\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 3, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    S_inv = np.linalg.inv(S_np)\n    transformed_matrix = T_inv @ A_np @ S_np\n    transformed_matrix_rounded = np.round(transformed_matrix, 4)\n    result = transformed_matrix_rounded.tolist()\n    return result\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 4, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n        return -1\n    T_inv_np = np.linalg.inv(T_np)\n    S_inv_np = np.linalg.inv(S_np)\n    transformed_matrix_np = np.dot(T_inv_np, np.dot(A_np, S_np))\n    transformed_matrix_np = np.round(transformed_matrix_np, 4)\n    transformed_matrix = transformed_matrix_np.tolist()\n    return transformed_matrix"}
{"task_id": 7, "completion_id": 5, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1] or A_np.shape[1] != T_np.shape[0] or (A_np.shape[0] != S_np.shape[0]):\n        return -1\n    if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    S_inv = np.linalg.inv(S_np)\n    transformed_matrix = T_inv @ A_np @ S_np\n    transformed_matrix_rounded = np.round(transformed_matrix, 4)\n    result = transformed_matrix_rounded.tolist()\n    return result\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 6, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    result = T_inv @ A @ S\n    result_rounded = np.round(result, 4)\n    result_list = result_rounded.tolist()\n    return result_list\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 7, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    if det_T == 0 or det_S == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    S_inv = np.linalg.inv(S_np)\n    transformed_matrix = np.dot(np.dot(T_inv, A_np), S_np)\n    transformed_matrix_rounded = np.round(transformed_matrix, 4)\n    return transformed_matrix_rounded.tolist()"}
{"task_id": 7, "completion_id": 8, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    if det_T == 0 or det_S == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    result = np.dot(np.dot(T_inv, A), S)\n    result = np.round(result, 4)\n    return result.tolist()"}
{"task_id": 7, "completion_id": 9, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    result = T_inv @ A @ S\n    result_rounded = np.round(result, 4)\n    result_list = result_rounded.tolist()\n    return result_list"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1 / det\n    inverse_matrix = [[inv_det * d, inv_det * -b], [inv_det * -c, inv_det * a]]\n    return inverse_matrix\nmatrix = [[1, 2], [3, 4]]"}
{"task_id": 8, "completion_id": 1, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse\nmatrix = [[1, 2], [3, 4]]"}
{"task_id": 8, "completion_id": 2, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_det = 1 / det\n    inverse = [[inv_det * d, -inv_det * b], [-inv_det * c, inv_det * a]]\n    return inverse\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 8, "completion_id": 3, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse_det = 1 / det\n    inverse_matrix = [[d * inverse_det, -b * inverse_det], [-c * inverse_det, a * inverse_det]]\n    return inverse_matrix\nmatrix = [[4, 7], [2, 6]]"}
{"task_id": 8, "completion_id": 4, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse = [[d / det, -b / det], [-c / det, a / det]]\n    return inverse"}
{"task_id": 8, "completion_id": 5, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse"}
{"task_id": 8, "completion_id": 6, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse = [[d / det, -b / det], [-c / det, a / det]]\n    return inverse\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 8, "completion_id": 7, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse\nmatrix = [[1, 2], [3, 4]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 8, "completion_id": 8, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse"}
{"task_id": 8, "completion_id": 9, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse = [[d / det, -b / det], [-c / det, a / det]]\n    return inverse\nmatrix = [[1, 2], [3, 4]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 1, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 2, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 3, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 4, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 5, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 6, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 7, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0] * cols_b for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 8, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 9, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    mean_vector = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - mean_vector\n    num_observations = data.shape[1]\n    covariance_matrix = centered_data @ centered_data.T / (num_observations - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 1, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1)\n    centered_data = data - means[:, np.newaxis]\n    n = data.shape[1]\n    covariance_matrix = 1 / (n - 1) * np.dot(centered_data, centered_data.T)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 2, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    mean = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - mean\n    n = data.shape[1]\n    covariance_matrix = 1 / n * np.dot(centered_data, centered_data.T)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 3, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - means\n    n = data.shape[1]\n    covariance_matrix = 1 / (n - 1) * np.dot(centered_data, centered_data.T)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 4, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    mean_vector = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - mean_vector\n    num_observations = data.shape[1]\n    covariance_matrix = centered_data @ centered_data.T / (num_observations - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 5, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - means\n    n = data.shape[1]\n    covariance_matrix = 1 / (n - 1) * np.dot(centered_data, centered_data.T)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 6, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - means\n    num_observations = data.shape[1]\n    covariance_matrix = centered_data @ centered_data.T / (num_observations - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 7, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1, keepdims=True)\n    centered_data = data - means\n    n = data.shape[1]\n    covariance_matrix = 1 / (n - 1) * np.dot(centered_data, centered_data.T)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 8, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1)\n    centered_data = data - means[:, np.newaxis]\n    covariance_matrix = np.dot(centered_data, centered_data.T) / (centered_data.shape[1] - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 9, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors).T\n    means = np.mean(data, axis=0)\n    centered_data = data - means\n    n = data.shape[0]\n    covariance_matrix = 1 / (n - 1) * np.dot(centered_data.T, centered_data)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    num_eqs = len(b)\n    x = np.zeros(num_eqs)\n    x_new = np.zeros(num_eqs)\n    for _ in range(n):\n        for i in range(num_eqs):\n            sigma = 0.0\n            for j in range(num_eqs):\n                if j != i:\n                    sigma += A[i, j] * x[j]\n            x_new[i] = (b[i] - sigma) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\nn = 10"}
{"task_id": 11, "completion_id": 1, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_old = np.copy(x)\n        for i in range(N):\n            s = sum((A[i, j] * x_old[j] for j in range(N) if j != i))\n            x[i] = round((b[i] - s) / A[i, i], 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 2, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    D = np.diag(A)\n    L_plus_U = A - np.diagflat(D)\n    x = np.zeros_like(b, dtype=float)\n    for _ in range(n):\n        x_new = (b - np.dot(L_plus_U, x)) / D\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 3, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 4, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = A.shape[0]\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()"}
{"task_id": 11, "completion_id": 5, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    num_vars = A.shape[0]\n    x = np.zeros(num_vars)\n    for _ in range(n):\n        x_new = np.zeros(num_vars)\n        for i in range(num_vars):\n            s = sum((A[i][j] * x[j] for j in range(num_vars) if j != i))\n            x_new[i] = (b[i] - s) / A[i][i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 6, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 10"}
{"task_id": 11, "completion_id": 7, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x_new = np.round(x_new, decimals=4)\n        x = x_new\n    return x.tolist()"}
{"task_id": 11, "completion_id": 8, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:\n        raise ValueError('A must be a square matrix and the dimensions of A and b must be compatible.')\n    num_equations = A.shape[0]\n    x = np.zeros(num_equations)\n    for _ in range(n):\n        x_new = np.zeros(num_equations)\n        for i in range(num_equations):\n            sum_part = sum((A[i, j] * x[j] for j in range(num_equations) if j != i))\n            x_new[i] = (b[i] - sum_part) / A[i, i]\n        x = x_new.round(4)\n    return x.tolist()\nA = np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 4]])\nb = np.array([1, 2, 3])\nn = 10"}
{"task_id": 11, "completion_id": 9, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    num_vars = len(b)\n    x = np.zeros(num_vars)\n    for _ in range(n):\n        x_new = np.zeros(num_vars)\n        for i in range(num_vars):\n            s = sum((A[i, j] * x[j] for j in range(num_vars) if j != i))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[10, -1, 2, 0], [-1, 11, -1, 3], [2, -1, 10, -1], [0, 3, -1, 8]], dtype=float)\nb = np.array([6, 25, -11, 15], dtype=float)\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    u = np.array([1.0, 0.0])\n    v = np.array([1.0, 0.0])\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    while iter_count < max_iter:\n        v_new = A.T @ u\n        v_new /= np.linalg.norm(v_new)\n        u_new = A @ v_new\n        u_new /= np.linalg.norm(u_new)\n        if np.linalg.norm(u_new - u) < tol and np.linalg.norm(v_new - v) < tol:\n            break\n        u = u_new\n        v = v_new\n        iter_count += 1\n    sigma1 = np.linalg.norm(A @ v_new)\n    sigma2 = np.linalg.norm(A @ v_new - sigma1 * u_new)\n    return (round(sigma1, 4), round(sigma2, 4))\nA = np.array([[1.0, 2.0], [3.0, 4.0]])"}
{"task_id": 12, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    sigma1 = np.linalg.norm(A)\n    sigma2 = 0.0\n\n    def jacobian_update(sigma1, sigma2):\n        B = A.T @ A\n        (b11, b12) = (B[0, 0], B[0, 1])\n        (b21, b22) = (B[1, 0], B[1, 1])\n        grad_sigma1 = np.array([2 * b11 * (sigma1 ** 2 - sigma2 ** 2) + 2 * b12 * sigma1 * sigma2, 2 * b21 * (sigma1 ** 2 - sigma2 ** 2) + 2 * b22 * sigma1 * sigma2])\n        grad_sigma2 = np.array([2 * b11 * (sigma2 ** 2 - sigma1 ** 2) - 2 * b12 * sigma1 * sigma2, 2 * b21 * (sigma2 ** 2 - sigma1 ** 2) - 2 * b22 * sigma1 * sigma2])\n        return (grad_sigma1, grad_sigma2)\n    tolerance = 1e-10\n    max_iterations = 1000\n    for _ in range(max_iterations):\n        (grad_sigma1, grad_sigma2) = jacobian_update(sigma1, sigma2)\n        new_sigma1 = sigma1 - tolerance * grad_sigma1[0]\n        new_sigma2 = sigma2 - tolerance * grad_sigma2[0]\n        if np.abs(new_sigma1 - sigma1) < tolerance and np.abs(new_sigma2 - sigma2) < tolerance:\n            break\n        (sigma1, sigma2) = (new_sigma1, new_sigma2)\n    singular_values = sorted([sigma1, sigma2], reverse=True)\n    singular_values = tuple((round(val, 4) for val in singular_values))\n    return singular_values\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 12, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    U = np.eye(2)\n    V = np.eye(2)\n    max_iterations = 1000\n    tolerance = 1e-10\n    for _ in range(max_iterations):\n        B = A.T @ A\n        off_B = B[0, 1] + B[1, 0]\n        diag_diff_B = B[0, 0] - B[1, 1]\n        theta_B = 0.5 * np.arctan2(off_B, diag_diff_B)\n        J_B = np.array([[np.cos(theta_B), -np.sin(theta_B)], [np.sin(theta_B), np.cos(theta_B)]])\n        V = V @ J_B\n        A = J_B.T @ A\n        C = A @ A.T\n        off_C = C[0, 1] + C[1, 0]\n        diag_diff_C = C[0, 0] - C[1, 1]\n        theta_C = 0.5 * np.arctan2(off_C, diag_diff_C)\n        J_C = np.array([[np.cos(theta_C), -np.sin(theta_C)], [np.sin(theta_C), np.cos(theta_C)]])\n        U = U @ J_C\n        A = A @ J_C.T\n        if np.abs(off_B) < tolerance and np.abs(off_C) < tolerance:\n            break\n    singular_values = np.sqrt(np.diag(A))\n    idx = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[idx]\n    U = U[:, idx]\n    V = V[:, idx]\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    V = np.round(V, 4)\n    return (U, singular_values, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    tol = 1e-10\n    max_iter = 1000\n    B = np.dot(A.T, A)\n    V = np.eye(2)\n    for _ in range(max_iter):\n        (a, b, c, d) = (B[0, 0], B[0, 1], B[1, 0], B[1, 1])\n        off_diag = b\n        if abs(off_diag) < tol:\n            break\n        theta = 0.5 * np.arctan2(2 * off_diag, d - a)\n        c_theta = np.cos(theta)\n        s_theta = np.sin(theta)\n        J = np.array([[c_theta, s_theta], [-s_theta, c_theta]])\n        B = np.dot(J.T, np.dot(B, J))\n        V = np.dot(V, J)\n    s1 = np.sqrt(B[0, 0])\n    s2 = np.sqrt(B[1, 1])\n    return (round(s1, 4), round(s2, 4))\nA = np.array([[1.0, 2.0], [3.0, 4.0]])"}
{"task_id": 12, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    U = np.eye(2)\n    V = np.eye(2)\n    B = A.T @ A\n    tol = 1e-10\n    max_iter = 1000\n    for _ in range(max_iter):\n        (a, b, c, d) = (B[0, 0], B[0, 1], B[1, 0], B[1, 1])\n        if abs(b) < tol:\n            break\n        theta = 0.5 * np.arctan2(2 * b, a - d)\n        c_theta = np.cos(theta)\n        s_theta = np.sin(theta)\n        J = np.array([[c_theta, s_theta], [-s_theta, c_theta]])\n        B = J.T @ B @ J\n        V = V @ J\n    (sigma1_squared, sigma2_squared) = np.diag(B)\n    sigma1 = np.sqrt(sigma1_squared)\n    sigma2 = np.sqrt(sigma2_squared)\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n        U[:, [0, 1]] = U[:, [1, 0]]\n        V[:, [0, 1]] = V[:, [1, 0]]\n    Sigma_inv = np.array([[1 / sigma1 if sigma1 != 0 else 0, 0], [0, 1 / sigma2 if sigma2 != 0 else 0]])\n    U = A @ V @ Sigma_inv\n    U = np.round(U, 4)\n    V = np.round(V, 4)\n    Sigma = np.round(np.diag([sigma1, sigma2]), 4)\n    return (U, Sigma, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input must be a 2x2 matrix.')\n    U = np.eye(2)\n    V = np.eye(2)\n    Ak = A.copy()\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    while iter_count < max_iter:\n        Uk = U.copy()\n        Uk[:, 0] = Ak[:, 0] / np.linalg.norm(Ak[:, 0])\n        Uk[:, 1] = Ak[:, 1] - np.dot(Uk[:, 0], Ak[:, 1]) * Uk[:, 0]\n        Uk[:, 1] = Uk[:, 1] / np.linalg.norm(Uk[:, 1])\n        Vk = V.copy()\n        Vk[:, 0] = Ak[0, :] / np.linalg.norm(Ak[0, :])\n        Vk[:, 1] = Ak[1, :] - np.dot(Vk[:, 0], Ak[1, :]) * Vk[:, 0]\n        Vk[:, 1] = Vk[:, 1] / np.linalg.norm(Vk[:, 1])\n        Ak_prev = Ak.copy()\n        Ak = Uk.T @ A @ Vk\n        if np.linalg.norm(Ak - Ak_prev) < tol:\n            break\n        U = Uk\n        V = Vk\n        iter_count += 1\n    sigma1 = round(abs(Ak[0, 0]), 4)\n    sigma2 = round(abs(Ak[1, 1]), 4)\n    return (sigma1, sigma2)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    U = np.eye(2)\n    V = np.eye(2)\n    max_iterations = 1000\n    tolerance = 1e-10\n    for _ in range(max_iterations):\n        B = U.T @ A @ V\n        alpha = B[0, 0]\n        beta = B[0, 1]\n        gamma = B[1, 0]\n        delta = B[1, 1]\n        c = alpha / np.sqrt(alpha ** 2 + gamma ** 2)\n        s = gamma / np.sqrt(alpha ** 2 + gamma ** 2)\n        G = np.array([[c, s], [-s, c]])\n        c = delta / np.sqrt(beta ** 2 + delta ** 2)\n        s = beta / np.sqrt(beta ** 2 + delta ** 2)\n        F = np.array([[c, s], [-s, c]])\n        U = U @ G.T\n        V = F @ V\n        if abs(beta) < tolerance:\n            break\n    singular_values = np.sqrt(np.diag(B @ B.T))\n    indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[indices]\n    U = U[:, indices]\n    V = V[:, indices]\n    return (np.round(singular_values, 4), np.round(U, 4), np.round(V.T, 4))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    (s1, s2) = np.linalg.norm(A, axis=0)\n    tol = 1e-08\n    max_iter = 1000\n    for _ in range(max_iter):\n        B = np.dot(A.T, A)\n        J11 = 2 * (B[0, 0] - s1 ** 2)\n        J12 = 2 * B[0, 1]\n        J21 = 2 * B[1, 0]\n        J22 = 2 * (B[1, 1] - s2 ** 2)\n        grad_s1 = J11 * s1 + J12 * s2\n        grad_s2 = J21 * s1 + J22 * s2\n        s1_new = s1 - grad_s1\n        s2_new = s2 - grad_s2\n        if np.linalg.norm(np.array([s1_new, s2_new]) - np.array([s1, s2])) < tol:\n            break\n        (s1, s2) = (s1_new, s2_new)\n    (s1, s2) = sorted([s1, s2], reverse=True)\n    return (round(s1, 4), round(s2, 4))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    B = np.dot(A.T, A)\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    U = np.eye(2)\n    while iter_count < max_iter:\n        (a, b, c, d) = (B[0, 0], B[0, 1], B[1, 0], B[1, 1])\n        if abs(b) < tol:\n            break\n        theta = 0.5 * np.arctan2(2 * b, d - a)\n        c_theta = np.cos(theta)\n        s_theta = np.sin(theta)\n        J = np.array([[c_theta, s_theta], [-s_theta, c_theta]])\n        B = np.dot(J.T, np.dot(B, J))\n        U = np.dot(U, J)\n        iter_count += 1\n    (s1_squared, s2_squared) = (B[0, 0], B[1, 1])\n    s1 = np.sqrt(s1_squared)\n    s2 = np.sqrt(s2_squared)\n    if s1 < s2:\n        (s1, s2) = (s2, s1)\n        U = U[:, ::-1]\n    S = np.diag([s1, s2])\n    V = np.dot(np.dot(A, U), np.linalg.inv(S))\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U, S, V)\nA = np.array([[1.0, 2.0], [3.0, 4.0]])"}
{"task_id": 12, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    B = np.dot(A.T, A)\n    (s1, s2) = np.sqrt(np.linalg.eigvals(B))\n    while iter_count < max_iter:\n        B = np.dot(A.T, A)\n        tr_B = np.trace(B)\n        det_B = np.linalg.det(B)\n        s1_new = (tr_B + np.sqrt(tr_B ** 2 - 4 * det_B)) / 2\n        s2_new = (tr_B - np.sqrt(tr_B ** 2 - 4 * det_B)) / 2\n        if np.abs(s1_new - s1) < tol and np.abs(s2_new - s2) < tol:\n            break\n        (s1, s2) = (s1_new, s2_new)\n        iter_count += 1\n    return (round(np.sqrt(s1), 4), round(np.sqrt(s2), 4))\nA = np.array([[1.0, 2.0], [3.0, 4.0]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        det += (-1) ** c * matrix[0][c] * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 1, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for col in range(4):\n        minor_matrix = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        cofactor = (-1) ** col * matrix[0][col] * determinant_3x3(minor_matrix)\n        det += cofactor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\ndet = determinant_4x4(matrix)"}
{"task_id": 13, "completion_id": 2, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n    def determinant_2x2(minor: list[list[int | float]]) -> float:\n        (a, b) = minor[0]\n        (c, d) = minor[1]\n        return a * d - b * c\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        minor_det = determinant_3x3(minor)\n        det += (-1) ** c * matrix[0][c] * minor_det\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 3, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n    def minor_matrix(matrix: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        return [row[:col] + row[col + 1:] for row in matrix[:row] + matrix[row + 1:]]\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        sign = (-1) ** c\n        det += sign * matrix[0][c] * determinant_3x3(minor_matrix(matrix, 0, c))\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 4, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        cofactor = matrix[0][c] * (-1) ** c * determinant_3x3(minor)\n        det += cofactor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 5, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        sign = (-1) ** c\n        det += sign * matrix[0][c] * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 6, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        (a, b, c) = mat[0]\n        (d, e, f) = mat[1]\n        (g, h, i) = mat[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        det += (-1) ** c * matrix[0][c] * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 7, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor_matrix = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        cofactor = (-1) ** c * matrix[0][c] * determinant_3x3(minor_matrix)\n        det += cofactor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 8, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant(matrix: list[list[int | float]]) -> float:\n        n = len(matrix)\n        if n == 1:\n            return matrix[0][0]\n        elif n == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        else:\n            det = 0\n            for c in range(n):\n                minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n                det += (-1) ** c * matrix[0][c] * determinant(minor)\n            return det\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    return determinant(matrix)"}
{"task_id": 13, "completion_id": 9, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix')\n    det = 0\n    for c in range(4):\n        minor_matrix = [[matrix[r][cc] for cc in range(4) if cc != c] for r in range(1, 4)]\n        cofactor = (-1) ** c * matrix[0][c] * determinant_3x3(minor_matrix)\n        det += cofactor\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": ""}
{"task_id": 14, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = [round(coef, 4) for coef in theta]\n    return theta_rounded"}
{"task_id": 14, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    XT_X = np.dot(X_b.T, X_b)\n    XT_X_inv = np.linalg.inv(XT_X)\n    XT_y = np.dot(X_b.T, y_np)\n    theta = np.dot(XT_X_inv, XT_y)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = 1 / m * X.T.dot(errors)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()\ny = np.array([2, 4, 6])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = 1 / m * X.T.dot(error)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()\ny = np.array([2, 3, 4, 5])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = 1 / m * X.T.dot(error)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        theta -= alpha / m * X.T.dot(errors)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = 1 / m * X.T.dot(error)\n        theta -= alpha * gradient\n    return theta.round(4).tolist()"}
{"task_id": 15, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    coefficients = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = X.dot(coefficients)\n        errors = predictions - y\n        gradient = 1 / m * X.T.dot(errors)\n        coefficients -= alpha * gradient\n    coefficients_rounded = np.round(coefficients, 4)\n    return coefficients_rounded.tolist()"}
{"task_id": 15, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        h = X.dot(theta)\n        error = h - y\n        gradient = 1 / m * X.T.dot(error)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    coefficients = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, coefficients)\n        errors = predictions - y\n        gradient = 1 / m * np.dot(X.T, errors)\n        coefficients -= alpha * gradient\n    return np.round(coefficients, 4).tolist()"}
{"task_id": 15, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = 1 / m * X.T.dot(errors)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()\ny = np.array([2, 3, 4, 5])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        theta -= alpha / m * X.T.dot(errors)\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_data = (data - min_val) / (max_val - min_val)\n    min_max_data_rounded = np.round(min_max_data, 4).tolist()\n    return (standardized_data_rounded, min_max_data_rounded)"}
{"task_id": 16, "completion_id": 1, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data_rounded, min_max_normalized_data_rounded)"}
{"task_id": 16, "completion_id": 2, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data_rounded, min_max_normalized_data_rounded)"}
{"task_id": 16, "completion_id": 3, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4)\n    standardized_data_list = standardized_data_rounded.tolist()\n    min_max_normalized_data_list = min_max_normalized_data_rounded.tolist()\n    return (standardized_data_list, min_max_normalized_data_list)"}
{"task_id": 16, "completion_id": 4, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data, min_max_normalized_data)"}
{"task_id": 16, "completion_id": 5, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data_rounded, min_max_normalized_data_rounded)"}
{"task_id": 16, "completion_id": 6, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4)\n    standardized_data_list = standardized_data_rounded.tolist()\n    min_max_normalized_data_list = min_max_normalized_data_rounded.tolist()\n    return (standardized_data_list, min_max_normalized_data_list)"}
{"task_id": 16, "completion_id": 7, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_scaled_data_rounded = np.round(min_max_scaled_data, 4).tolist()\n    return (standardized_data_rounded, min_max_scaled_data_rounded)"}
{"task_id": 16, "completion_id": 8, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data_rounded, min_max_normalized_data_rounded)"}
{"task_id": 16, "completion_id": 9, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    normalized_data = np.round(normalized_data, 4).tolist()\n    return (standardized_data, normalized_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [(round(centroid[0], 4), round(centroid[1], 4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 1, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = np.round(centroids, 4)\n    final_centroids = [tuple(centroid) for centroid in rounded_centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 2, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in rounded_centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 3, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (2.0, 2.0), (4.0, 0.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 4, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    centroids = np.round(centroids, 4)\n    return list(map(tuple, centroids))\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 5, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n\n    def euclidean_distance(point1, point2):\n        return np.sqrt(np.sum((point1 - point2) ** 2))\n    for _ in range(max_iterations):\n        labels = np.argmin(np.array([euclidean_distance(point, centroids) for centroid in centroids]), axis=0)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return rounded_centroids"}
{"task_id": 17, "completion_id": 6, "solution": "import numpy as np\nfrom collections import defaultdict\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in centroids]"}
{"task_id": 17, "completion_id": 7, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[cluster_assignments == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 8, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in rounded_centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 9, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in final_centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 3\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The feature dataset.\n    y (np.ndarray): The target dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Random seed for shuffling.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for that fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples')\n    data = np.column_stack((X, y))\n    if shuffle:\n        np.random.shuffle(data)\n    X = data[:, :-1]\n    y = data[:, -1]\n    n_samples = X.shape[0]\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size if i != k - 1 else n_samples\n        test_indices = list(range(start, end))\n        train_indices = list(range(start)) + list(range(end, n_samples))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 1, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    X (np.ndarray): The input feature data.\n    y (np.ndarray): The target data.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int or None): Random seed for shuffling data.\n    \n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if k < 2:\n        raise ValueError('K must be at least 2')\n    if k > len(X):\n        raise ValueError('K cannot be greater than the number of samples')\n    indices = np.arange(len(X))\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_sizes = np.full(k, len(X) // k, dtype=int)\n    fold_sizes[:len(X) % k] += 1\n    current = 0\n    splits = []\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate((indices[:start], indices[stop:]))\n        splits.append((train_indices, test_indices))\n        current = stop\n    return splits"}
{"task_id": 18, "completion_id": 2, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, the feature dataset.\n    - y: np.ndarray, the target dataset.\n    - k: int, the number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, the seed for random number generator for reproducibility.\n\n    Returns:\n    - A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if k < 2 or k > len(X):\n        raise ValueError('The number of folds must be between 2 and the number of samples.')\n    if shuffle:\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    fold_size = len(X) // k\n    remainder = len(X) % k\n    train_test_indices = []\n    start_index = 0\n    for fold in range(k):\n        end_index = start_index + fold_size + (1 if fold < remainder else 0)\n        test_indices = np.arange(start_index, end_index)\n        train_indices = np.concatenate((np.arange(0, start_index), np.arange(end_index, len(X))))\n        train_test_indices.append((train_indices, test_indices))\n        start_index = end_index\n    return train_test_indices"}
{"task_id": 18, "completion_id": 3, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    X : np.ndarray\n        The feature dataset.\n    y : np.ndarray\n        The target dataset.\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int, default=None\n        If shuffle is True, seed for random number generator.\n        \n    Returns:\n    cv_indices : list of tuples\n        Each tuple contains the indices for train and test sets for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    cv_indices = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i != k - 1 else n_samples\n        test_idx = indices[start:end]\n        train_idx = np.concatenate([indices[:start], indices[end:]])\n        cv_indices.append((train_idx, test_idx))\n    return cv_indices"}
{"task_id": 18, "completion_id": 4, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The feature dataset.\n    y (np.ndarray): The target dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the dataset before splitting.\n    random_seed (int): The random seed for shuffling.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    (start, stop) = (0, 0)\n    splits = []\n    for fold in range(k):\n        if fold < remainder:\n            stop += fold_size + 1\n        else:\n            stop += fold_size\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate((indices[:start], indices[stop:]))\n        splits.append((train_indices, test_indices))\n        start = stop\n    return splits"}
{"task_id": 18, "completion_id": 5, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The feature dataset.\n    y (np.ndarray): The target dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Random seed for shuffling.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if k <= 1 or k > len(X):\n        raise ValueError('Number of folds must be greater than 1 and less than or equal to the number of samples.')\n    if shuffle:\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    fold_size = len(X) // k\n    splits = []\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size if i != k - 1 else len(X)\n        test_indices = np.arange(start, end)\n        train_indices = np.concatenate((np.arange(0, start), np.arange(end, len(X))))\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 18, "completion_id": 6, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, the feature dataset.\n    - y: np.ndarray, the target dataset.\n    - k: int, the number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int or None, seed for the random number generator for shuffling.\n\n    Returns:\n    - A list of tuples, where each tuple contains two arrays:\n      the first array contains the indices for the training set,\n      and the second array contains the indices for the test set.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size if i < k - 1 else n_samples\n        test_indices = indices[start:end]\n        train_indices = np.delete(indices, np.arange(start, end))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 7, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X : np.ndarray\n        The input features dataset.\n    y : np.ndarray\n        The target labels dataset.\n    k : int, optional\n        The number of folds. Default is 5.\n    shuffle : bool, optional\n        Whether to shuffle the data before splitting into batches. Default is True.\n    random_seed : int, optional\n        The seed for the random number generator. Default is None.\n\n    Returns:\n    list of tuples\n        Each tuple contains the train and test indices for the fold.\n    \"\"\"\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.arange(len(X))\n    fold_sizes = np.full(k, len(X) // k, dtype=int)\n    fold_sizes[:len(X) % k] += 1\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        test_indices = indices[start:stop]\n        train_indices = np.concatenate((indices[:start], indices[stop:]))\n        folds.append((train_indices, test_indices))\n        current = stop\n    return folds"}
{"task_id": 18, "completion_id": 8, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, feature matrix.\n    - y: np.ndarray, target vector.\n    - k: int, number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, seed for the random number generator for reproducibility.\n\n    Returns:\n    - A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    folds = [(fold * fold_size, (fold + 1) * fold_size) for fold in range(k)]\n    folds[-1] = (folds[-1][0], n_samples)\n    train_test_indices = []\n    for i in range(k):\n        (test_start, test_end) = folds[i]\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n        train_test_indices.append((train_indices, test_indices))\n    return train_test_indices"}
{"task_id": 18, "completion_id": 9, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, the feature dataset.\n    - y: np.ndarray, the target dataset.\n    - k: int, the number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, optional, random seed for shuffling.\n\n    Returns:\n    - A list of tuples, where each tuple contains the train and test indices for each fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if k < 2 or k > len(X):\n        raise ValueError('Invalid number of folds: k must be between 2 and the number of samples.')\n    indices = np.arange(len(X))\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = len(X) // k\n    folds = []\n    start = 0\n    for i in range(k):\n        end = start + fold_size if i < k - 1 else len(X)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        folds.append((train_indices, test_indices))\n        start = end\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 1, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 2, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4)\n    return principal_components_rounded.tolist()"}
{"task_id": 19, "completion_id": 3, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4).tolist()\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 4, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 5, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 6, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    data_standardized = (data - mean) / std\n    covariance_matrix = np.cov(data_standardized, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4)\n    return principal_components_rounded.tolist()"}
{"task_id": 19, "completion_id": 7, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4)\n    return principal_components_rounded.tolist()"}
{"task_id": 19, "completion_id": 8, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    return principal_components.round(4).tolist()"}
{"task_id": 19, "completion_id": 9, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(class_counts: Counter) -> float:\n    \"\"\"Calculate the entropy of a list of class labels.\"\"\"\n    if not class_counts:\n        return 0.0\n    total = sum(class_counts.values())\n    return -sum((count / total * math.log2(count / total) for count in class_counts.values()))\ndef information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain from splitting on a given attribute.\"\"\"\n    parent_counts = Counter((example[target_attr] for example in examples))\n    parent_entropy = entropy(parent_counts)\n    subsets = {}\n    for example in examples:\n        key = example[attribute]\n        if key not in subsets:\n            subsets[key] = []\n        subsets[key].append(example)\n    subset_entropy = 0.0\n    for subset in subsets.values():\n        subset_count = len(subset)\n        subset_entropy += subset_count / len(examples) * entropy(Counter((ex[class_attr] for ex in subset)))\n    return parent_entropy - subset_entropy\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"Recursively learn a decision tree using information gain.\"\"\"\n    class_counts = Counter((example[target_attr] for example in examples))\n    if len(class_counts) == 1:\n        return next(iter(class_counts))\n    if not attributes:\n        return max(class_counts, key=class_counts.get)\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    for value in set((example[best_attribute] for example in examples)):\n        subset = [example for example in examples if example[best_attribute] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 1, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: Counter) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        total_entropy = entropy(Counter((example[target_attr] for example in examples)))\n        attribute_values = set((example[attribute] for example in examples))\n        weighted_entropy = 0.0\n        for value in attribute_values:\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            weighted_entropy += len(subset) / len(examples) * subset_entropy\n        return total_entropy - weighted_entropy\n\n    def best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n        return max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n\n    def majority_value(examples: list[dict], target_attr: str) -> str:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return None\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attr = best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    best_attr_values = set((example[best_attr] for example in examples))\n    for value in best_attr_values:\n        subset = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(subset, [attr for attr in attributes if attr != best_attr], target_attr)\n        tree[best_attr][value] = subtree\n    return tree\nexamples = [{'color': 'red', 'size': 'small', 'label': 'apple'}, {'color': 'green', 'size': 'small', 'label': 'grape'}, {'color': 'red', 'size': 'medium', 'label': 'apple'}, {'color': 'red', 'size': 'large', 'label': 'apple'}, {'color': 'green', 'size': 'medium', 'label': 'grape'}, {'color': 'green', 'size': 'large', 'label': 'grape'}, {'color': 'yellow', 'size': 'medium', 'label': 'banana'}, {'color': 'yellow', 'size': 'large', 'label': 'banana'}]\nattributes = ['color', 'size']\ntarget_attr = 'label'"}
{"task_id": 20, "completion_id": 2, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: dict) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count))\n\n    def information_gain(data: list[dict], attribute: str, target_attr: str) -> float:\n        target_counts = Counter((example[target_attr] for example in data))\n        total_entropy = entropy(target_counts)\n        attr_values = set((example[attribute] for example in data))\n        attr_value_counts = {value: len([e for e in data if e[attribute] == value]) for value in attr_values}\n        weighted_entropy = 0\n        for value in attr_values:\n            subset = [example for example in data if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            weighted_entropy += attr_value_counts[value] / len(data) * subset_entropy\n        return total_entropy - weighted_entropy\n\n    def choose_best_attribute(data: list[dict], attributes: list[str], target_attr: str) -> str:\n        return max(attributes, key=lambda attr: information_gain(data, attr, target_attr))\n\n    def decision_tree_learning(data: list[dict], attributes: list[str], default_class: str, target_attr: str) -> dict:\n        if not data:\n            return default_class\n        elif len(set((example[target_attr] for example in data))) == 1:\n            return data[0][target_attr]\n        elif not attributes:\n            return Counter((example[target_attr] for example in data)).most_common(1)[0][0]\n        else:\n            best_attr = choose_best_attribute(data, attributes, target_attr)\n            tree = {best_attr: {}}\n            for value in set((example[best_attr] for example in data)):\n                subset = [example for example in data if example[best_attr] == value]\n                subtree = decision_tree_learning(subset, [attr for attr in attributes if attr != best_attr], Counter((example[target_attr] for example in data)).most_common(1)[0][0], target_attr)\n                tree[best_attr][value] = subtree\n            return tree\n    default_class = Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    return decision_tree_learning(examples, attributes, default_class, target_attr)"}
{"task_id": 20, "completion_id": 3, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(examples):\n        if not examples:\n            return 0\n        count = Counter((example[target_attr] for example in examples))\n        total = len(examples)\n        return -sum((count[label] / total * math.log2(count[label] / total) for label in count))\n\n    def information_gain(examples, attr):\n        total_entropy = entropy(examples)\n        val_counter = Counter((example[attr] for example in examples))\n        total = len(examples)\n        val_entropy = sum((val_counter[val] / total * entropy([ex for ex in examples if ex[attr] == val]) for val in val_counter))\n        return total_entropy - val_entropy\n\n    def most_common_label(examples):\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return most_common_label(examples)\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_label(examples)\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr))\n    tree = {best_attr: {}}\n    for val in set((example[best_attr] for example in examples)):\n        subtree_examples = [ex for ex in examples if ex[best_attr] == val]\n        subtree_attributes = [attr for attr in attributes if attr != best_attr]\n        tree[best_attr][val] = learn_decision_tree(subtree_examples, subtree_attributes, target_attr)\n    return tree"}
{"task_id": 20, "completion_id": 4, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: dict) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\n\n    def information_gain(examples: list[dict], attribute: str) -> float:\n        class_counts = Counter((example[target_attr] for example in examples))\n        total_entropy = entropy(class_counts)\n        split_entropy = 0.0\n        for value in set((example[attribute] for example in examples)):\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            split_entropy += len(subset) / len(examples) * subset_entropy\n        return total_entropy - split_entropy\n\n    def choose_best_attribute(examples: list[dict], attributes: list[str]) -> str:\n        return max(attributes, key=lambda attr: information_gain(examples, attr))\n\n    def most_common_class(examples: list[dict]) -> str:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return {}\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_class(examples)\n    best_attr = choose_best_attribute(examples, attributes)\n    tree = {best_attr: {}}\n    for value in set((example[best_attr] for example in examples)):\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implement a decision tree learning algorithm using recursive binary splitting based on entropy and information gain.\n    \n    Parameters:\n    examples (list[dict]): List of examples where each example is a dictionary of attribute-value pairs.\n    attributes (list[str]): List of attribute names.\n    target_attr (str): The name of the target attribute.\n    \n    Returns:\n    dict: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(class_counts: dict) -> float:\n        total = sum(class_counts.values())\n        ent = 0.0\n        for count in class_counts.values():\n            p = count / total\n            ent -= p * math.log2(p) if p > 0 else 0\n        return ent\n\n    def information_gain(examples, attribute, target_attr):\n        class_counts = Counter((example[target_attr] for example in examples))\n        total_entropy = entropy(class_counts)\n        partitions = {}\n        for example in examples:\n            attr_value = example[attribute]\n            if attr_value not in partitions:\n                partitions[attr_value] = []\n            partitions[attr_value].append(example)\n        weighted_entropy = 0.0\n        for partition in partitions.values():\n            partition_entropy = entropy(Counter((example[target_attr] for example in partition)))\n            weighted_entropy += len(partition) / len(examples) * partition_entropy\n        return total_entropy - weighted_entropy\n    class_values = [example[target_attr] for example in examples]\n    if len(set(class_values)) == 1:\n        return class_values[0]\n    if not attributes:\n        return Counter(class_values).most_common(1)[0][0]\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attr: {}}\n    partitions = {}\n    for example in examples:\n        attr_value = example[best_attr]\n        if attr_value not in partitions:\n            partitions[attr_value] = []\n        partitions[attr_value].append(example)\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for (attr_value, partition) in partitions.items():\n        tree[best_attr][attr_value] = learn_decision_tree(partition, remaining_attributes, target_attr)\n    return tree"}
{"task_id": 20, "completion_id": 6, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(label_counts: Counter) -> float:\n        total = sum(label_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in label_counts.values()))\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        label_counts = Counter((example[target_attr] for example in examples))\n        total_entropy = entropy(label_counts)\n        value_split = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in value_split:\n                value_split[value] = []\n            value_split[value].append(example)\n        remainder = 0\n        for subset in value_split.values():\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            remainder += len(subset) / len(examples) * subset_entropy\n        return total_entropy - remainder\n\n    def most_common_label(examples: list[dict], target_attr: str) -> str:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n\n    def build_tree(examples: list[dict], attributes: list[str], default: str) -> dict:\n        labels = [example[target_attr] for example in examples]\n        if not examples or not attributes:\n            return default\n        if labels.count(labels[0]) == len(labels):\n            return labels[0]\n        best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n        tree = {best_attribute: {}}\n        for value in set((example[best_attribute] for example in examples)):\n            subset = [example for example in examples if example[best_attribute] == value]\n            subtree = build_tree(subset, [attr for attr in attributes if attr != best_attribute], most_common_label(examples, target_attr))\n            tree[best_attribute][value] = subtree\n        return tree\n    return build_tree(examples, attributes, most_common_label(examples, target_attr))\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'"}
{"task_id": 20, "completion_id": 7, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: dict) -> float:\n        \"\"\"Calculate the entropy of a set of class counts.\"\"\"\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        \"\"\"Calculate the information gain of an attribute with respect to the target attribute.\"\"\"\n        total_entropy = entropy(Counter((example[target_attr] for example in examples)))\n        value_counts = Counter((example[attribute] for example in examples))\n        weighted_entropy = 0.0\n        for value in value_counts:\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            weighted_entropy += value_counts[value] / len(examples) * subset_entropy\n        return total_entropy - weighted_entropy\n\n    def majority_class(examples: list[dict], target_attr: str) -> str:\n        \"\"\"Return the majority class of the examples.\"\"\"\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return {}\n    first_target_value = examples[0][target_attr]\n    if all((example[target_attr] == first_target_value for example in examples)):\n        return first_target_value\n    if not attributes:\n        return majority_class(examples, target_attr)\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    for value in Counter((example[best_attribute] for example in examples)):\n        subset = [example for example in examples if example[best_attribute] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'color': 'red', 'size': 'small', 'label': 'apple'}, {'color': 'red', 'size': 'large', 'label': 'cherry'}, {'color': 'green', 'size': 'medium', 'label': 'grape'}, {'color': 'green', 'size': 'small', 'label': 'apple'}, {'color': 'red', 'size': 'medium', 'label': 'cherry'}, {'color': 'green', 'size': 'large', 'label': 'grape'}, {'color': 'red', 'size': 'small', 'label': 'apple'}]\nattributes = ['color', 'size']\ntarget_attr = 'label'"}
{"task_id": 20, "completion_id": 8, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: dict) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\n\n    def information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n        class_counts = Counter((example[target_attr] for example in examples))\n        initial_entropy = entropy(class_counts)\n        attr_values = set((example[attr] for example in examples))\n        total_entropy = 0\n        for value in attr_values:\n            subset = [example for example in examples if example[attr] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            total_entropy += len(subset) / len(examples) * subset_entropy\n        return initial_entropy - total_entropy\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return {target_values[0]: {}}\n    if not attributes:\n        return {Counter(target_values).most_common(1)[0][0]: {}}\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attr: {}}\n    attr_values = set((example[best_attr] for example in examples))\n    for value in attr_values:\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\nexamples = [{'color': 'red', 'size': 'small', 'label': 'apple'}, {'color': 'red', 'size': 'medium', 'label': 'apple'}, {'color': 'green', 'size': 'small', 'label': 'grape'}, {'color': 'green', 'size': 'large', 'label': 'grape'}, {'color': 'red', 'size': 'large', 'label': 'apple'}, {'color': 'yellow', 'size': 'medium', 'label': 'banana'}, {'color': 'yellow', 'size': 'large', 'label': 'banana'}]\nattributes = ['color', 'size']\ntarget_attr = 'label'"}
{"task_id": 20, "completion_id": 9, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts):\n        total_count = sum(class_counts.values())\n        return -sum((count / total_count * math.log2(count / total_count) for count in class_counts.values() if count != 0))\n\n    def split_examples(examples, attribute):\n        partitions = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in partitions:\n                partitions[value] = []\n            partitions[value].append(example)\n        return partitions\n\n    def information_gain(examples, attribute, target_attr):\n        target_counts = Counter((example[target_attr] for example in examples))\n        original_entropy = entropy(target_counts)\n        partitions = split_examples(examples, attribute)\n        weighted_entropy = 0\n        for partition in partitions.values():\n            partition_target_counts = Counter((example[target_attr] for example in partition))\n            partition_entropy = entropy(partition_target_counts)\n            weighted_entropy += len(partition) / len(examples) * partition_entropy\n        return original_entropy - weighted_entropy\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    partitions = split_examples(examples, best_attribute)\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    for (value, partition) in partitions.items():\n        if not partition:\n            tree[best_attribute][value] = Counter(target_values).most_common(1)[0][0]\n        else:\n            tree[best_attribute][value] = learn_decision_tree(partition, remaining_attributes, target_attr)\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def rbf_kernel(x1, x2, sigma=1.0):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n\n    def compute_kernel_matrix(data, kernel_func):\n        n_samples = data.shape[0]\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = kernel_func(data[i], data[j])\n        return K\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = lambda x1, x2: rbf_kernel(x1, x2, sigma)\n    else:\n        raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n    (n_samples, _) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    K = compute_kernel_matrix(data, kernel_func)\n    labels = np.where(labels == 0, -1, labels)\n    for t in range(1, iterations + 1):\n        step_size = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = np.sum(alpha * labels * K[i]) + bias\n            if labels[i] * prediction < 1:\n                alpha[i] += step_size * labels[i]\n                bias += step_size * labels[i]\n        alpha *= min(1, (1 / (lambda_val * np.linalg.norm(alpha) ** 2)) ** 0.5)\n    alpha_rounded = np.round(alpha, 4).tolist()\n    bias_rounded = np.round(bias, 4).tolist()\n    return (alpha_rounded, bias_rounded)"}
{"task_id": 21, "completion_id": 1, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def kernel_function(x1, x2):\n        if kernel == 'linear':\n            return np.dot(x1, x2)\n        elif kernel == 'rbf':\n            return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n        else:\n            raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0\n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            xi = data[i]\n            yi = labels[i]\n            sum_kernel = sum((alpha[j] * labels[j] * kernel_function(xi, data[j]) for j in range(n_samples)))\n            f_value = sum_kernel + b\n            if yi * f_value < 1:\n                alpha[i] = alpha[i] + learning_rate * yi\n                b = b + learning_rate * yi\n            else:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i]\n        alpha = np.clip(alpha, 0, 1 / (lambda_val * n_samples))\n    alpha_rounded = np.round(alpha, 4)\n    b_rounded = np.round(b, 4)\n    return (alpha_rounded.tolist(), b_rounded.tolist())"}
{"task_id": 21, "completion_id": 2, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def kernel_function(x1, x2):\n        if kernel == 'linear':\n            return np.dot(x1, x2)\n        elif kernel == 'rbf':\n            return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n        else:\n            raise ValueError('Unsupported kernel type')\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        margin_errors = []\n        for i in range(n_samples):\n            prediction = 0.0\n            for j in range(n_samples):\n                prediction += alphas[j] * labels[j] * kernel_function(data[j], data[i])\n            prediction += bias\n            margin_error = labels[i] * prediction\n            margin_errors.append(margin_error)\n            if margin_error < 1:\n                alphas[i] = alphas[i] + learning_rate * labels[i]\n                bias = bias + learning_rate * labels[i]\n        alphas = alphas * (1 - learning_rate * lambda_val)\n    alphas_rounded = np.round(alphas, 4)\n    bias_rounded = np.round(bias, 4)\n    return (alphas_rounded.tolist(), bias_rounded.tolist())"}
{"task_id": 21, "completion_id": 3, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    b = 0.0\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def rbf_kernel(x1, x2, sigma=sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = rbf_kernel\n    else:\n        raise ValueError(\"Kernel type not recognized. Use 'linear' or 'rbf'.\")\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            sum_k = 0\n            for j in range(n_samples):\n                sum_k += alpha[j] * labels[j] * kernel_func(data[i], data[j])\n            decision_function = sum_k + b\n            if labels[i] * decision_function < 1:\n                alpha[i] = (1 - eta * lambda_val) * alpha[i] + eta * labels[i]\n                b = b + eta * labels[i]\n            else:\n                alpha[i] = (1 - eta * lambda_val) * alpha[i]\n    alpha_rounded = np.round(alpha, 4)\n    b_rounded = round(b, 4)\n    return (alpha_rounded.tolist(), b_rounded)"}
{"task_id": 21, "completion_id": 4, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def kernel_function(x1, x2):\n        if kernel == 'linear':\n            return np.dot(x1, x2)\n        elif kernel == 'RBF':\n            return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n        else:\n            raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'RBF'.\")\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel_function(data[i], data[j])\n    for t in range(1, iterations + 1):\n        step_size = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = np.sum(alphas * labels * K[i]) + bias\n            if labels[i] * prediction < 1:\n                alphas[i] = (1 - step_size * lambda_val) * alphas[i] + step_size * labels[i]\n            else:\n                alphas[i] = (1 - step_size * lambda_val) * alphas[i]\n        sum_term = np.sum(alphas * labels * K)\n        bias = bias - step_size * sum_term\n    alphas = np.round(alphas, 4)\n    bias = np.round(bias, 4)\n    return (alphas.tolist(), bias.tolist())"}
{"task_id": 21, "completion_id": 5, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        K = np.exp(-np.sum((data[:, np.newaxis] - data) ** 2, axis=2) / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        predictions = np.sum(alpha * labels * K, axis=1) + b\n        margin = labels * predictions\n        for i in range(n_samples):\n            if margin[i] < 1:\n                alpha[i] = (1 - eta_t * lambda_val) * alpha[i] + eta_t * labels[i]\n            else:\n                alpha[i] = (1 - eta_t * lambda_val) * alpha[i]\n        b = b + eta_t * np.sum(labels * (1 - predictions) * (margin < 1))\n        norm_alpha = np.sqrt(np.sum((alpha * labels) ** 2 * K.diagonal()))\n        if norm_alpha > 1.0 / lambda_val:\n            alpha = alpha / (lambda_val * norm_alpha)\n    return (alpha.round(4).tolist(), round(b, 4))"}
{"task_id": 21, "completion_id": 6, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def linear_kernel(x, y):\n        return np.dot(x, y.T)\n\n    def rbf_kernel(x, y, sigma):\n        return np.exp(-np.linalg.norm(x[:, np.newaxis] - y, axis=2) ** 2 / (2 * sigma ** 2))\n\n    def compute_kernel_matrix(data, kernel_type):\n        if kernel_type == 'linear':\n            return linear_kernel(data, data)\n        elif kernel_type == 'rbf':\n            return rbf_kernel(data, data, sigma)\n        else:\n            raise ValueError('Unsupported kernel type')\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    t = 0\n    K = compute_kernel_matrix(data, kernel)\n    for t in range(1, iterations + 1):\n        lr = 1.0 / (lambda_val * t)\n        grad_alpha = np.maximum(0, 1 - labels * (K.dot(alpha) + b) * labels)\n        gradient = (lambda_val * alpha - np.sum(grad_alpha * labels)[:, np.newaxis] * K.T, lambda_val * b - np.sum(grad_alpha * labels))\n        alpha -= lr * gradient[0]\n        b -= lr * gradient[1]\n        alpha = np.maximum(0, alpha)\n        alpha = np.minimum(alpha, 1.0 / (lambda_val * n_samples))\n    alpha = np.round(alpha, 4)\n    b = np.round(b, 4)\n    return (alpha.tolist(), b.tolist())"}
{"task_id": 21, "completion_id": 7, "solution": "import numpy as np\ndef kernel_function(x1, x2, sigma=1.0, kernel_type='linear'):\n    if kernel_type == 'linear':\n        return np.dot(x1, x2)\n    elif kernel_type == 'rbf':\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Unsupported kernel type')\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    t = 0\n    for iteration in range(1, iterations + 1):\n        t += 1\n        learning_rate = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = 0.0\n            for j in range(n_samples):\n                prediction += alphas[j] * labels[j] * kernel_function(data[j], data[i], sigma, kernel)\n            prediction += bias\n            if labels[i] * prediction < 1:\n                alphas[i] = alphas[i] + learning_rate * labels[i]\n            alphas = alphas * (1 - learning_rate * lambda_val)\n        bias_term_sum = 0.0\n        non_zero_count = 0\n        for i in range(n_samples):\n            if alphas[i] > 0:\n                bias_term_sum += labels[i] - prediction\n                non_zero_count += 1\n        if non_zero_count > 0:\n            bias = bias_term_sum / non_zero_count\n    alphas = np.round(alphas, 4)\n    bias = np.round(bias, 4)\n    return (alphas.tolist(), bias.tolist())"}
{"task_id": 21, "completion_id": 8, "solution": "import numpy as np\ndef kernel_function(x1, x2, kernel_type='linear', sigma=1.0):\n    if kernel_type == 'linear':\n        return np.dot(x1, x2)\n    elif kernel_type == 'rbf':\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Unsupported kernel type')\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        sum_alpha_y_k = np.zeros(n_samples)\n        for i in range(n_samples):\n            for j in range(n_samples):\n                sum_alpha_y_k[i] += alpha[j] * labels[j] * kernel_function(data[i], data[j], kernel, sigma)\n        for i in range(n_samples):\n            update_term = learning_rate * labels[i] * (1 - labels[i] * (sum_alpha_y_k[i] + b))\n            if update_term > 0:\n                alpha[i] += update_term * learning_rate\n        alpha = np.clip(alpha, 0, 1 / (n_samples * lambda_val))\n        b = np.mean([labels[i] - sum_alpha_y_k[i] for i in range(n_samples)])\n    alpha_rounded = np.round(alpha, 4)\n    b_rounded = np.round(b, 4)\n    return (alpha_rounded.tolist(), b_rounded.tolist())"}
{"task_id": 21, "completion_id": 9, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def rbf_kernel(x1, x2, sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n\n    def kernel_function(x1, x2):\n        if kernel == 'linear':\n            return linear_kernel(x1, x2)\n        elif kernel == 'rbf':\n            return rbf_kernel(x1, x2, sigma)\n        else:\n            raise ValueError('Unsupported kernel type')\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    t = 0\n    for _ in range(iterations):\n        t += 1\n        learning_rate = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            margin = 0.0\n            for j in range(n_samples):\n                margin += alphas[j] * labels[j] * kernel_function(data[i], data[j])\n            margin += bias * labels[i]\n            if labels[i] * margin < 1:\n                alphas[i] = alphas[i] + learning_rate * labels[i]\n                bias = bias + learning_rate * labels[i]\n        norm_alpha = 0.0\n        for j in range(n_samples):\n            norm_alpha += alphas[j] ** 2 * labels[j] ** 2 * kernel_function(data[j], data[j])\n        norm_alpha = np.sqrt(norm_alpha)\n        if norm_alpha > 1.0 / lambda_val:\n            alphas *= 1.0 / (lambda_val * norm_alpha)\n    alphas = np.round(alphas, 4)\n    bias = np.round(bias, 4)\n    return (alphas.tolist(), bias.tolist())"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 1, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n        \n    Returns:\n        float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 2, "solution": "import math\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 3, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Computes the output of the sigmoid activation function for a given input z.\"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 4, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\n    \n    Args:\n        z (float): The input value to the sigmoid function.\n    \n    Returns:\n        float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    output = 1 / (1 + math.exp(-z))\n    return round(output, 4)"}
{"task_id": 22, "completion_id": 5, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    output = 1 / (1 + math.exp(-z))\n    return round(output, 4)"}
{"task_id": 22, "completion_id": 6, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 7, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 8, "solution": "import math\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 9, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 23, "completion_id": 1, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 23, "completion_id": 2, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 3, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    return [round(value, 4) for value in softmax_values]\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 23, "completion_id": 4, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 23, "completion_id": 5, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 23, "completion_id": 6, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 23, "completion_id": 7, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 23, "completion_id": 8, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 9, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predicted_probabilities = []\n    total_error = 0.0\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(predicted_probability)\n        error = (predicted_probability - label) ** 2\n        total_error += error\n    mean_squared_error = total_error / len(labels)\n    predicted_probabilities_rounded = [round(prob, 4) for prob in predicted_probabilities]\n    mean_squared_error_rounded = round(mean_squared_error, 4)\n    return (np.array(predicted_probabilities_rounded).tolist(), mean_squared_error_rounded)"}
{"task_id": 24, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_probability = 1 / (1 + math.exp(-weighted_sum))\n        predicted_probabilities.append(predicted_probability)\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predicted_probabilities, labels))) / len(labels)\n    predicted_probabilities_rounded = [round(p, 4) for p in predicted_probabilities]\n    mse_rounded = round(mse, 4)\n    return (np.array(predicted_probabilities_rounded).tolist(), mse_rounded)"}
{"task_id": 24, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature in features:\n        weighted_sum = sum((w * f for (w, f) in zip(weights, feature))) + bias\n        probability = sigmoid(weighted_sum)\n        predictions.append(round(probability, 4))\n    mse = sum(((p - t) ** 2 for (p, t) in zip(predictions, labels))) / len(labels)\n    mse = round(mse, 4)\n    predictions = np.array(predictions).tolist()\n    return (predictions, mse)\nfeatures = [[0.5, 0.1], [0.2, 0.3], [0.3, 0.6]]\nlabels = [0, 1, 1]\nweights = [0.4, 0.2]\nbias = 0.1"}
{"task_id": 24, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum((x * w for (x, w) in zip(feature_vector, weights))) + bias\n        probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(probability)\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predicted_probabilities, labels))) / len(labels)\n    predicted_probabilities_rounded = [round(p, 4) for p in predicted_probabilities]\n    mse_rounded = round(mse, 4)\n    predicted_probabilities_rounded = np.array(predicted_probabilities_rounded).tolist()\n    return (predicted_probabilities_rounded, mse_rounded)\nfeatures = [[0.5, 1.2], [1.1, 0.8], [0.3, 0.4]]\nlabels = [0, 1, 0]\nweights = [0.6, 0.4]\nbias = -0.2"}
{"task_id": 24, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features: List of feature vectors (each vector representing multiple features for an example).\n    - labels: True binary labels associated with the feature vectors.\n    - weights: Weights for each feature.\n    - bias: Bias for the neuron.\n    \n    Returns:\n    - Predicted probabilities after sigmoid activation, rounded to four decimal places.\n    - Mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predictions, labels))) / len(labels)\n    predictions_rounded = [round(p, 4) for p in predictions]\n    mse_rounded = round(mse, 4)\n    return (np.array(predictions_rounded).tolist(), mse_rounded)\nfeatures = [[0.5, 0.3], [0.2, 0.8], [0.7, 0.6]]\nlabels = [0, 1, 1]\nweights = [0.1, 0.2]\nbias = 0.0"}
{"task_id": 24, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x: float) -> float:\n        return 1 / (1 + math.exp(-x))\n    predicted_probabilities = []\n    mse = 0.0\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_prob = sigmoid(weighted_sum)\n        predicted_probabilities.append(round(predicted_prob, 4))\n        mse += (predicted_prob - label) ** 2\n    mse /= len(labels)\n    mse = round(mse, 4)\n    predicted_probabilities = np.array(predicted_probabilities).tolist()\n    return (predicted_probabilities, mse)"}
{"task_id": 24, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(round(predicted_probability, 4))\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predicted_probabilities, labels))) / len(labels)\n    mse = round(mse, 4)\n    predicted_probabilities = np.array(predicted_probabilities).tolist()\n    return (predicted_probabilities, mse)"}
{"task_id": 24, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    mse_loss = 0.0\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n        mse_loss += (prediction - label) ** 2\n    mse_loss /= len(labels)\n    predictions = [round(p, 4) for p in predictions]\n    mse_loss = round(mse_loss, 4)\n    return (predictions, mse_loss)\nfeatures = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\nlabels = [0, 1, 1]\nweights = [0.5, -0.3]\nbias = 0.1"}
{"task_id": 24, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    squared_errors = []\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n        error = (prediction - label) ** 2\n        squared_errors.append(error)\n    mean_squared_error = sum(squared_errors) / len(squared_errors)\n    predictions_rounded = np.round(predictions, 4).tolist()\n    mean_squared_error_rounded = round(mean_squared_error, 4)\n    return (predictions_rounded, mean_squared_error_rounded)"}
{"task_id": 24, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predictions, labels))) / len(labels)\n    predictions_rounded = [round(p, 4) for p in predictions]\n    mse_rounded = round(mse, 4)\n    predictions_rounded = np.array(predictions_rounded).tolist()\n    return (predictions_rounded, mse_rounded)\nfeatures = [[0.5, 0.3], [0.2, 0.8], [0.7, 0.1]]\nlabels = [0, 1, 0]\nweights = [0.1, 0.2]\nbias = 0.5"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(predictions, targets):\n    return np.mean((predictions - targets) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_predictions = error * sigmoid_derivative(predictions)\n        weights_update = np.dot(features.T, d_predictions)\n        bias_update = np.sum(d_predictions)\n        weights -= learning_rate * weights_update\n        bias -= learning_rate * bias_update\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(predictions, targets):\n    return ((predictions - targets) ** 2).mean()\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = np.array(initial_weights, dtype=float)\n    bias = float(initial_bias)\n    mse_history = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_history.append(round(mse, 4))\n        error = predictions - labels\n        d_predicted_output = error * sigmoid_derivative(predictions)\n        weights -= learning_rate * np.dot(features.T, d_predicted_output)\n        bias -= learning_rate * np.sum(d_predicted_output)\n    return (weights.tolist(), round(bias, 4), mse_history)"}
{"task_id": 25, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(predictions, labels):\n    return ((predictions - labels) ** 2).mean()\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_prediction = error * sigmoid_derivative(predictions)\n        weights_update = np.dot(features.T, d_prediction) * learning_rate\n        bias_update = np.sum(d_prediction) * learning_rate\n        weights -= weights_update\n        bias -= bias_update\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    return x * (1 - x)\ndef mean_squared_error(predictions: np.ndarray, true_labels: np.ndarray) -> float:\n    return np.mean((predictions - true_labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_loss_d_pred = 2 * error / len(labels)\n        d_pred_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_pred * d_pred_d_z\n        d_loss_d_w = np.dot(features.T, d_loss_d_z)\n        d_loss_d_b = np.sum(d_loss_d_z)\n        weights -= learning_rate * d_loss_d_w\n        bias -= learning_rate * d_loss_d_b\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    return x * (1 - x)\ndef mean_squared_error(predictions: np.ndarray, labels: np.ndarray) -> float:\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_history = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_history.append(round(mse, 4))\n        error = predictions - labels\n        d_sigmoid = sigmoid_derivative(predictions)\n        delta = error * d_sigmoid\n        weight_gradients = np.dot(features.T, delta)\n        bias_gradient = np.sum(delta)\n        weights -= learning_rate * weight_gradients\n        bias -= learning_rate * bias_gradient\n    return (weights.round(4), round(bias, 4), mse_history)"}
{"task_id": 25, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\ndef sigmoid_derivative(z):\n    return sigmoid(z) * (1 - sigmoid(z))\ndef mean_squared_error(predictions, labels):\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_loss_d_predictions = error\n        d_predictions_d_z = sigmoid_derivative(z)\n        d_loss_d_z = d_loss_d_predictions * d_predictions_d_z\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(predictions, labels):\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_predictions = error * sigmoid_derivative(predictions)\n        dw = np.dot(features.T, d_predictions)\n        db = np.sum(d_predictions)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(labels, predictions)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_loss_d_predictions = 2 * error / len(labels)\n        d_predictions_d_weighted_sum = sigmoid_derivative(predictions)\n        d_loss_d_weighted_sum = d_loss_d_predictions * d_predictions_d_weighted_sum\n        d_weighted_sum_d_weights = features\n        d_loss_d_weights = np.dot(d_weighted_sum_d_weights.T, d_loss_d_weighted_sum)\n        d_weighted_sum_d_bias = np.ones_like(labels)\n        d_loss_d_bias = np.dot(d_weighted_sum_d_bias, d_loss_d_weighted_sum)\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 8, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(x):\n        return x * (1 - x)\n\n    def mse_loss(predictions, actual):\n        return np.mean((predictions - actual) ** 2)\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        loss = mse_loss(predictions, labels)\n        mse_values.append(round(loss, 4))\n        error = predictions - labels\n        d_loss_d_pred = 2 * (predictions - labels) / len(labels)\n        d_pred_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_pred * d_pred_d_z\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 9, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(x):\n        return x * (1 - x)\n\n    def mse_loss(y_true, y_pred):\n        return np.mean((y_true - y_pred) ** 2)\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n        loss = mse_loss(labels, predictions)\n        mse_values.append(round(loss, 4))\n        error = predictions - labels\n        dPredictions = error * sigmoid_derivative(predictions)\n        weights -= learning_rate * np.dot(features.T, dPredictions)\n        bias -= learning_rate * np.sum(dPredictions)\n    return (weights.round(4), round(bias, 4), mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n\n        def topological_sort(node):\n            topo = []\n            visited = set()\n\n            def build_topo(v):\n                if v not in visited:\n                    visited.add(v)\n                    for child in v._prev:\n                        build_topo(child)\n                    topo.append(v)\n            build_topo(node)\n            return topo\n        self.grad = 1.0\n        for node in reversed(topological_sort(self)):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 1, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 2, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'only supporting int/float powers for now'\n        out = Value(self.data ** other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += other * self.data ** (other - 1) * out.grad\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 3, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 4, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 5, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 6, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 7, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 8, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 9, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    P = np.linalg.inv(C_matrix) @ B_matrix\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 1, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 1], [0, 1, 1], [0, 0, 1]]"}
{"task_id": 27, "completion_id": 2, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 3, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 27, "completion_id": 4, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    B_matrix_inv = np.linalg.inv(B_matrix)\n    P = np.dot(C_matrix, B_matrix_inv)\n    P_rounded = np.around(P, decimals=4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 1], [1, 0, 1], [0, 1, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 5, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 2, 3], [0, 1, 4], [5, 6, 0]]\nC = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]"}
{"task_id": 27, "completion_id": 6, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    C_inv = np.linalg.inv(C_matrix)\n    P = np.dot(C_inv, B_matrix)\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list"}
{"task_id": 27, "completion_id": 7, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    C_inv = np.linalg.inv(C_matrix)\n    P = C_inv @ B_matrix\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 1], [1, 1, 0], [1, 0, 0]]"}
{"task_id": 27, "completion_id": 8, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 9, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    B_matrix_inv = np.linalg.inv(B_matrix)\n    P = np.dot(C_matrix, B_matrix_inv)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    idx = singular_values.argsort()[::-1]\n    S = S[idx, :][:, idx]\n    U = U[:, idx]\n    V = V[:, idx]\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    return (U, S, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_transpose_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_transpose_A)\n    singular_values = np.sqrt(np.abs(eigenvalues))\n    sort_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sort_indices]\n    eigenvectors = eigenvectors[:, sort_indices]\n    V = eigenvectors\n    S = np.diag(singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 28, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2), 'Input matrix must be 2x2'\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    singular_values = np.sqrt(eigenvalues)\n    sort_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sort_indices]\n    eigenvectors = eigenvectors[:, sort_indices]\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = A @ V[:, i] / singular_values[i]\n        else:\n            u_i = np.zeros(2)\n        U[:, i] = u_i\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(len(singular_values)):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    (U, _) = np.linalg.qr(U)\n    U_rounded = np.round(U, 4)\n    S_rounded = np.round(S, 4)\n    V_rounded = np.round(V, 4)\n    return (U_rounded.tolist(), S_rounded.tolist(), V_rounded.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_transpose_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A_transpose_A)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(len(singular_values)):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n        else:\n            U[:, i] = np.zeros(2)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(np.abs(eigenvalues))\n    sorted_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sorted_indices]\n    V = eigenvectors[:, sorted_indices]\n    S = np.diag(singular_values)\n    U = np.zeros_like(A)\n    for i in range(len(singular_values)):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n    if np.linalg.det(U) < 0:\n        U[:, -1] = -U[:, -1]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2')\n    A_transpose_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_transpose_A)\n    singular_values = np.sqrt(eigenvalues)\n    idx = singular_values.argsort()[::-1]\n    singular_values = singular_values[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n        else:\n            U[:, i] = np.zeros(2)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_transpose_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_transpose_A)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.zeros((2, 2))\n    for i in range(2):\n        S[i, i] = singular_values[i]\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in the numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature matrix.\n    y (numpy.ndarray): The target array.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    list: A list containing the shuffled X and y as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples')\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])"}
{"task_id": 29, "completion_id": 1, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in two numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The input feature array.\n    y (numpy.ndarray): The target array.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list containing the shuffled X and y arrays reshaped to lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return (X_shuffled_list, y_shuffled_list)"}
{"task_id": 29, "completion_id": 2, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    - X: numpy array of features.\n    - y: numpy array of labels.\n    - seed: optional seed value for reproducibility.\n    \n    Returns:\n    - A list of shuffled X and y samples as numpy arrays converted to a list.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples.'\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 29, "completion_id": 3, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    Parameters:\n    - X: numpy array, the feature dataset.\n    - y: numpy array, the labels corresponding to the feature dataset.\n    - seed: int, optional, seed for the random number generator for reproducibility.\n    \n    Returns:\n    - A list of tuples, where each tuple contains a reshaped sample from X and its corresponding label from y.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    result = list(zip(X_shuffled.tolist(), y_shuffled.tolist()))\n    return result"}
{"task_id": 29, "completion_id": 4, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two numpy arrays X and y in unison while maintaining the corresponding order.\n    \n    Parameters:\n    X (numpy.ndarray): The first numpy array to be shuffled.\n    y (numpy.ndarray): The second numpy array to be shuffled, must have the same number of rows as X.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples, where each tuple contains the corresponding row from the shuffled X and y.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of rows.')\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    combined = list(zip(shuffled_X.tolist(), shuffled_y.tolist()))\n    return combined\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])"}
{"task_id": 29, "completion_id": 5, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The first numpy array, typically the features.\n    y (numpy.ndarray): The second numpy array, typically the labels.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    list: A list of two lists, the shuffled X and y as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 29, "completion_id": 6, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of X and y in the same order while maintaining correspondence.\n    \n    Parameters:\n        X (np.ndarray): The feature dataset.\n        y (np.ndarray): The labels dataset.\n        seed (int, optional): Seed for the random number generator for reproducibility.\n        \n    Returns:\n        list: A list of shuffled X and y as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    num_samples = X.shape[0]\n    indices = np.random.permutation(num_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return (X_shuffled_list, y_shuffled_list)"}
{"task_id": 29, "completion_id": 7, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in the numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The target dataset.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples containing the shuffled samples and their corresponding targets.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    shuffled_data = list(zip(X_shuffled.tolist(), y_shuffled.tolist()))\n    return shuffled_data"}
{"task_id": 29, "completion_id": 8, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    Parameters:\n    - X: numpy array of samples\n    - y: numpy array of labels corresponding to the samples\n    - seed: optional seed for random number generator for reproducibility\n    \n    Returns:\n    - A tuple of shuffled X and y as python lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    shuffled_X = X[indices]\n    shuffled_y = y[indices]\n    shuffled_X_list = shuffled_X.tolist()\n    shuffled_y_list = shuffled_y.tolist()\n    return (shuffled_X_list, shuffled_y_list)"}
{"task_id": 29, "completion_id": 9, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle the samples in numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The label dataset.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    list: A list of shuffled X and y datasets reshaped by using numpy's tolist() method.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from numpy arrays X and optional y.\n\n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array, optional, the target data.\n    - batch_size: int, size of each batch.\n\n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch) as lists.\n    - If y is not provided, yields X_batch as a list.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    X = X[indices]\n    if y is not None:\n        y = y[indices]\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = start_idx + batch_size\n        X_batch = X[start_idx:end_idx].tolist()\n        if y is not None:\n            y_batch = y[start_idx:end_idx].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 30, "completion_id": 1, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset X and optionally y.\n    \n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array (optional), the target data.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - A tuple of (X_batch, y_batch) if y is not None, otherwise yields X_batch.\n    Each batch is returned as a Python list after reshaping using numpy's tolist() method.\n    \"\"\"\n    if y is not None:\n        assert len(X) == len(y), 'X and y must have the same number of samples.'\n    num_samples = len(X)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[indices[start_idx:end_idx]]\n        if y is not None:\n            y_batch = y[indices[start_idx:end_idx]]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 2, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Iterates over the dataset X (and optionally y) in batches of a specified size.\n    \n    Parameters:\n    - X: numpy array of shape (num_samples, num_features)\n    - y: optional numpy array of shape (num_samples,)\n    - batch_size: integer, size of the batches to yield\n    \n    Yields:\n    - batches of (X, y) if y is provided, otherwise batches of X\n    \"\"\"\n    if y is not None:\n        assert len(X) == len(y), 'X and y must have the same number of samples'\n    num_samples = len(X)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        batch_X = X[batch_indices]\n        if y is not None:\n            batch_y = y[batch_indices]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 30, "completion_id": 3, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Batch iterator function that samples from numpy array X and an optional numpy array y.\n    \n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array, optional, the target data.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch).\n    - If y is not provided, yields X_batch.\n    \"\"\"\n    assert isinstance(X, np.ndarray), 'X must be a numpy array'\n    if y is not None:\n        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n        assert len(X) == len(y), 'X and y must have the same length'\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        batch_indices = indices[start_idx:end_idx]\n        X_batch = X[batch_indices]\n        if y is not None:\n            y_batch = y[batch_indices]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 4, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of a specified size from the dataset.\n    \n    Parameters:\n    - X: numpy array, the input dataset.\n    - y: optional numpy array, the labels for the dataset.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch).\n    - If y is not provided, yields X_batch.\n    Each batch is returned as a Python list after reshaping using numpy's tolist() method.\n    \"\"\"\n    if y is not None:\n        assert len(X) == len(y), 'X and y must have the same number of samples.'\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    for i in range(0, len(indices), batch_size):\n        batch_indices = indices[i:i + batch_size]\n        X_batch = X[batch_indices].tolist()\n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 30, "completion_id": 5, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset. If y is provided, yields batches of (X, y) pairs.\n    Otherwise, yields batches of X only.\n\n    Parameters:\n    X (numpy.ndarray): The input data array.\n    y (numpy.ndarray, optional): The target data array.\n    batch_size (int, optional): The size of each batch.\n\n    Yields:\n    list or tuple of lists: Batches of data. If y is provided, yields tuples of lists (X_batch, y_batch).\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        X_batch = X[batch_indices]\n        if y is not None:\n            y_batch = y[batch_indices]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 6, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator function to yield batches of data from numpy arrays X and y.\n    \n    Parameters:\n    - X: numpy array of input data\n    - y: optional numpy array of target data\n    - batch_size: integer, size of each batch to yield\n    \n    Yields:\n    - Batch of X data (and y data if provided) as a tuple of lists.\n    \"\"\"\n    assert isinstance(X, np.ndarray), 'X must be a numpy array'\n    if y is not None:\n        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n        assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        batch_X = X[batch_indices].tolist()\n        if y is not None:\n            batch_y = y[batch_indices].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 30, "completion_id": 7, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from numpy arrays X and optionally y.\n    \n    Parameters:\n    - X: numpy array, the main dataset.\n    - y: optional numpy array, the labels/dependant variables.\n    - batch_size: integer, the size of each batch to yield.\n    \n    Yields:\n    - If y is provided, yields tuples of lists (X_batch, y_batch).\n    - If y is not provided, yields lists X_batch.\n    \"\"\"\n    assert isinstance(X, np.ndarray), 'X must be a numpy array'\n    if y is not None:\n        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n        assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        X_batch = X[indices[start:end]].tolist()\n        if y is not None:\n            y_batch = y[indices[start:end]].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 30, "completion_id": 8, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterator that yields batches of X and optionally y.\n\n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array, the target data (optional).\n    - batch_size: int, the size of each batch.\n\n    Yields:\n    - If y is provided, yields tuples (X_batch, y_batch).\n    - If y is not provided, yields X_batch.\n    Each batch is a reshaped numpy array converted to a list.\n    \"\"\"\n    assert isinstance(X, np.ndarray), 'X must be a numpy array'\n    if y is not None:\n        assert isinstance(y, np.ndarray), 'y must be a numpy array'\n        assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[indices[start_idx:end_idx]]\n        if y is not None:\n            y_batch = y[indices[start_idx:end_idx]]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 9, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the given numpy arrays X and (optionally) y.\n    \n    Parameters:\n    - X: numpy array of input data.\n    - y: numpy array of target values (optional).\n    - batch_size: integer specifying the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (batch_X, batch_y) as lists.\n    - If y is not provided, yields batch_X as a list.\n    \"\"\"\n    X = np.array(X)\n    if y is not None:\n        y = np.array(y)\n        assert len(X) == len(y), 'X and y must have the same number of samples.'\n    num_batches = len(X) // batch_size\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        end_idx = start_idx + batch_size\n        batch_X = X[start_idx:end_idx]\n        if y is not None:\n            batch_y = y[start_idx:end_idx]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()\n    if len(X) % batch_size != 0:\n        last_batch_start = num_batches * batch_size\n        batch_X = X[last_batch_start:]\n        if y is not None:\n            batch_y = y[last_batch_start:]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on a feature threshold.\n    \n    Parameters:\n    - X: numpy array, the dataset to be divided.\n    - feature_i: int, the index of the feature to use for division.\n    - threshold: float, the threshold value for the feature.\n    \n    Returns:\n    - X_above: list, samples where the feature value is greater than or equal to the threshold.\n    - X_below: list, samples where the feature value is less than the threshold.\n    \"\"\"\n    X = np.array(X)\n    mask_above = X[:, feature_i] >= threshold\n    mask_below = X[:, feature_i] < threshold\n    X_above = X[mask_above]\n    X_below = X[mask_below]\n    return (X_above.tolist(), X_below.tolist())"}
{"task_id": 31, "completion_id": 1, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on whether the value of the specified feature\n    is greater than or equal to the given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value for division.\n\n    Returns:\n    tuple: Two lists, each containing the samples that meet and do not meet the condition.\n    \"\"\"\n    X = np.array(X)\n    mask = X[:, feature_i] >= threshold\n    subset_ge = X[mask].tolist()\n    subset_lt = X[~mask].tolist()\n    return (subset_ge, subset_lt)"}
{"task_id": 31, "completion_id": 2, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on the condition\n    whether the value of the feature at index feature_i is greater\n    than or equal to a given threshold.\n\n    Parameters:\n    - X: numpy.ndarray, the dataset to be divided.\n    - feature_i: int, the index of the feature to base the division on.\n    - threshold: float, the threshold value for the feature.\n\n    Returns:\n    - subset_1: list, samples where the feature value is >= threshold.\n    - subset_2: list, samples where the feature value is < threshold.\n    \"\"\"\n    X = np.array(X)\n    subset_1 = X[X[:, feature_i] >= threshold]\n    subset_2 = X[X[:, feature_i] < threshold]\n    return (subset_1.tolist(), subset_2.tolist())"}
{"task_id": 31, "completion_id": 3, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on a feature threshold.\n    \n    Parameters:\n    - X: numpy array, the dataset to be divided.\n    - feature_i: int, the index of the feature to use for division.\n    - threshold: float, the threshold value for dividing the dataset.\n    \n    Returns:\n    - left: list, samples where the feature value is less than the threshold.\n    - right: list, samples where the feature value is greater than or equal to the threshold.\n    \"\"\"\n    X = np.array(X)\n    left = X[X[:, feature_i] < threshold]\n    right = X[X[:, feature_i] >= threshold]\n    left = left.tolist()\n    right = right.tolist()\n    return (left, right)"}
{"task_id": 31, "completion_id": 4, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on a feature threshold.\n    \n    Parameters:\n    - X: numpy array, the dataset to divide.\n    - feature_i: int, the index of the feature to use for division.\n    - threshold: float, the threshold value for the feature.\n    \n    Returns:\n    - subset_1: list, samples where the feature value is greater than or equal to the threshold.\n    - subset_2: list, samples where the feature value is less than the threshold.\n    \"\"\"\n    X = np.array(X)\n    index_1 = X[:, feature_i] >= threshold\n    index_2 = X[:, feature_i] < threshold\n    subset_1 = X[index_1]\n    subset_2 = X[index_2]\n    return (subset_1.tolist(), subset_2.tolist())"}
{"task_id": 31, "completion_id": 5, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on whether the value of a specified feature\n    is greater than or equal to a given threshold.\n\n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value for division.\n\n    Returns:\n    tuple: A tuple of two lists, the first containing samples where the feature value is\n           greater than or equal to the threshold, and the second containing samples where\n           the feature value is less than the threshold.\n    \"\"\"\n    X = np.array(X)\n    mask_greater_equal = X[:, feature_i] >= threshold\n    mask_less = X[:, feature_i] < threshold\n    greater_equal = X[mask_greater_equal]\n    less = X[mask_less]\n    return (greater_equal.tolist(), less.tolist())"}
{"task_id": 31, "completion_id": 6, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on the feature at index feature_i\n    and the given threshold.\n    \n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for dividing the dataset.\n    threshold (float): The threshold value to use for dividing the dataset.\n    \n    Returns:\n    list: A list containing two numpy arrays. The first array contains the subset\n          of X where the feature at index feature_i is greater than or equal to the\n          threshold. The second array contains the subset where the feature is less\n          than the threshold.\n    \"\"\"\n    X = np.array(X)\n    indices_greater_equal = X[:, feature_i] >= threshold\n    indices_less = X[:, feature_i] < threshold\n    subset_greater_equal = X[indices_greater_equal]\n    subset_less = X[indices_less]\n    subset_greater_equal_list = subset_greater_equal.tolist()\n    subset_less_list = subset_less.tolist()\n    return [subset_greater_equal_list, subset_less_list]"}
{"task_id": 31, "completion_id": 7, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on the threshold for a given feature.\n    \n    Parameters:\n    X (numpy.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value to compare the feature against.\n    \n    Returns:\n    list: A list containing two sublists - samples where the feature is >= threshold and samples where the feature is < threshold.\n    \"\"\"\n    X = np.array(X)\n    mask_ge = X[:, feature_i] >= threshold\n    subset_ge = X[mask_ge]\n    mask_lt = X[:, feature_i] < threshold\n    subset_lt = X[mask_lt]\n    subset_ge_list = subset_ge.tolist()\n    subset_lt_list = subset_lt.tolist()\n    return (subset_ge_list, subset_lt_list)"}
{"task_id": 31, "completion_id": 8, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset into two subsets based on a feature threshold.\n\n    Parameters:\n    X (np.ndarray): The dataset to divide.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value for the feature.\n\n    Returns:\n    list: A list containing two subsets of the dataset. The first subset contains\n          samples where the feature value is greater than or equal to the threshold,\n          and the second subset contains samples where the feature value is less than\n          the threshold.\n    \"\"\"\n    X = np.array(X)\n    mask = X[:, feature_i] >= threshold\n    subset_ge = X[mask]\n    subset_lt = X[~mask]\n    subset_ge_list = subset_ge.tolist()\n    subset_lt_list = subset_lt.tolist()\n    return [subset_ge_list, subset_lt_list]"}
{"task_id": 31, "completion_id": 9, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on whether the value of the specified feature\n    is greater than or equal to the given threshold.\n\n    Parameters:\n    - X: np.ndarray, the dataset to be divided.\n    - feature_i: int, the index of the feature to check.\n    - threshold: float, the threshold value for dividing the dataset.\n\n    Returns:\n    - X1: list, subset of X where the feature value is >= threshold.\n    - X2: list, subset of X where the feature value is < threshold.\n    \"\"\"\n    X = np.array(X)\n    indices1 = X[:, feature_i] >= threshold\n    indices2 = X[:, feature_i] < threshold\n    X1 = X[indices1]\n    X2 = X[indices2]\n    X1 = X1.tolist()\n    X2 = X2.tolist()\n    return (X1, X2)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for combo in combinations:\n        feature_product = np.prod(X[:, combo], axis=1)\n        poly_features.append(feature_product)\n    poly_features = np.column_stack(poly_features)\n    bias_term = np.ones((n_samples, 1))\n    poly_features = np.column_stack((bias_term, poly_features))\n    return poly_features.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 1, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combs = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for comb in combs:\n        feature = np.prod(X[:, comb], axis=1)\n        poly_features.append(feature)\n    poly_features = np.column_stack([np.ones(n_samples)] + poly_features)\n    return poly_features.tolist()"}
{"task_id": 32, "completion_id": 2, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    feature_combinations = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for combination in feature_combinations:\n        product = np.ones(n_samples)\n        for index in combination:\n            product *= X[:, index]\n        poly_features.append(product)\n    poly_features = np.column_stack(poly_features)\n    poly_features = np.column_stack((np.ones(n_samples), poly_features))\n    return poly_features.tolist()"}
{"task_id": 32, "completion_id": 3, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for combo in combinations:\n        feature_product = np.ones(n_samples)\n        for index in combo:\n            feature_product *= X[:, index]\n        poly_features.append(feature_product)\n    poly_features = np.column_stack(poly_features)\n    bias_term = np.ones((n_samples, 1))\n    poly_features = np.hstack((bias_term, poly_features))\n    return poly_features.tolist()"}
{"task_id": 32, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features for a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): Input data with shape (n_samples, n_features).\n    degree (int): Degree of the polynomial features to generate.\n\n    Returns:\n    list: List of polynomial features with shape (n_samples, n_polynomial_features).\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    comb = combinations_with_replacement(range(n_features), degree)\n    polynomial_feature_array = np.ones((n_samples, 1))\n    for indices in comb:\n        feature_product = np.prod(X[:, indices], axis=1).reshape(-1, 1)\n        polynomial_feature_array = np.hstack((polynomial_feature_array, feature_product))\n    return polynomial_feature_array.tolist()"}
{"task_id": 32, "completion_id": 5, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for combo in combinations:\n        feature_product = np.prod(X[:, combo], axis=1)\n        poly_features.append(feature_product)\n    poly_features_array = np.column_stack(poly_features)\n    poly_features_array = np.hstack((np.ones((n_samples, 1)), poly_features_array))\n    return poly_features_array.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 6, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = list(combinations_with_replacement(range(n_features), degree))\n    polynomial_feature_list = []\n    for sample in X:\n        for combo in combinations:\n            feature_value = np.prod(sample[list(combo)])\n            polynomial_feature_list.append(feature_value)\n    polynomial_feature_array = np.array(polynomial_feature_list).reshape(n_samples, -1)\n    return polynomial_feature_array.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 7, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    comb = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for indices in comb:\n        feature_product = np.ones(n_samples)\n        for index in indices:\n            feature_product *= X[:, index]\n        poly_features.append(feature_product)\n    poly_features = np.column_stack(poly_features)\n    poly_features = np.hstack([np.ones((n_samples, 1)), poly_features])\n    return poly_features.tolist()"}
{"task_id": 32, "completion_id": 8, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    polynomial_feature_list = []\n    for combination in combinations:\n        feature = np.prod(X[:, combination], axis=1)\n        polynomial_feature_list.append(feature)\n    polynomial_feature_list.insert(0, np.ones(n_samples))\n    polynomial_features_array = np.column_stack(polynomial_feature_list)\n    return polynomial_features_array.tolist()"}
{"task_id": 32, "completion_id": 9, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = list(combinations_with_replacement(range(n_features), degree))\n    poly_features = []\n    for sample in X:\n        feature_vector = [np.prod(sample ** combo) for combo in combinations]\n        poly_features.append(feature_vector)\n    return np.array(poly_features).tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (np.ndarray): 2D numpy array of features.\n    y (np.ndarray): 1D numpy array of target values.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to sample with replacements.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: List of n_subsets tuples, each containing (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 1, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): A 2D numpy array representing the dataset features.\n    y (numpy.ndarray): A 1D numpy array representing the dataset labels.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool): If True, sampling is done with replacement. If False, sampling is done without replacement.\n    seed (int): Seed for the random number generator for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 2, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of target values.\n    - n_subsets: Integer number of subsets to generate.\n    - replacements: Boolean indicating whether to sample with or without replacements.\n    - seed: Integer seed for random number generator for reproducibility.\n\n    Returns:\n    - List of n_subsets tuples, where each tuple is a random subset (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 3, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): 2D numpy array representing the dataset features.\n    y (numpy.ndarray): 1D numpy array representing the dataset labels.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool): If True, sampling is done with replacements.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets, each a tuple (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 4, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (np.ndarray): 2D numpy array of features.\n    y (np.ndarray): 1D numpy array of labels.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool): Whether to sample with or without replacement.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets, each a tuple (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    subsets = []\n    for _ in range(n_subsets):\n        sampled_indices = np.random.choice(indices, size=len(indices), replace=replacements)\n        X_subset = X[sampled_indices]\n        y_subset = y[sampled_indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 5, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): 2D numpy array of features.\n    y (numpy.ndarray): 1D numpy array of target values.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool): Whether to sample with replacement.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: List of n_subsets random subsets, each a tuple (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 33, "completion_id": 6, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (numpy.ndarray): A 2D numpy array representing the dataset features.\n    y (numpy.ndarray): A 1D numpy array representing the dataset labels.\n    n_subsets (int): The number of random subsets to generate.\n    replacements (bool): Whether to sample with replacements.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets, each a tuple (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 7, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 8, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (np.ndarray): 2D numpy array of the dataset features.\n    y (np.ndarray): 1D numpy array of the dataset labels.\n    n_subsets (int): Number of random subsets to generate.\n    replacements (bool): Whether to sample with replacements.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets, each a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 9, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()"}
{"task_id": 34, "completion_id": 1, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_matrix = np.zeros((len(x), n_col), dtype=int)\n    one_hot_matrix[np.arange(len(x)), x] = 1\n    return one_hot_matrix.tolist()"}
{"task_id": 34, "completion_id": 2, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()\nx = np.array([0, 1, 2, 1, 0])"}
{"task_id": 34, "completion_id": 3, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_matrix = np.zeros((len(x), n_col), dtype=int)\n    one_hot_matrix[np.arange(len(x)), x] = 1\n    return one_hot_matrix.tolist()"}
{"task_id": 34, "completion_id": 4, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()\nx = np.array([0, 1, 2, 1])"}
{"task_id": 34, "completion_id": 5, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.eye(n_col)[x]\n    return one_hot_encoded.tolist()"}
{"task_id": 34, "completion_id": 6, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((len(x), n_col), dtype=int)\n    one_hot_encoded[np.arange(len(x)), x] = 1\n    return one_hot_encoded.tolist()\nx = np.array([0, 1, 2, 1, 0])"}
{"task_id": 34, "completion_id": 7, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()"}
{"task_id": 34, "completion_id": 8, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()"}
{"task_id": 34, "completion_id": 9, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((len(x), n_col), dtype=int)\n    one_hot_encoded[np.arange(len(x)), x] = 1\n    return one_hot_encoded.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (numpy.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 1, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 2, "solution": "import numpy as np\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 3, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 4, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 5, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 6, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 7, "solution": "import numpy as np\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 8, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 9, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (np.array): 1D numpy array containing the true labels.\n    y_pred (np.array): 1D numpy array containing the predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 1, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy.ndarray): 1D array containing the true labels.\n    y_pred (numpy.ndarray): 1D array containing the predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 2, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 3, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 4, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (np.array): 1D numpy array containing the true labels.\n    y_pred (np.array): 1D numpy array containing the predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 5, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (1D numpy array): True labels.\n    y_pred (1D numpy array): Predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / y_true.size\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 6, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n\n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 7, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (np.array): 1D numpy array containing the true labels.\n    y_pred (np.array): 1D numpy array containing the predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 8, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 9, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array containing true labels.\n    y_pred (numpy.ndarray): 1D array containing predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): A 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: The correlation matrix as a Python list.\n    \"\"\"\n    if Y is None:\n        correlation_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        combined = np.concatenate((X, Y), axis=0)\n        correlation_matrix = np.corrcoef(combined, rowvar=False)\n        correlation_matrix = correlation_matrix[:X.shape[1], X.shape[1]:]\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 1, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n\n    Parameters:\n    - X: 2D numpy array\n    - Y: Optional 2D numpy array\n\n    Returns:\n    - Correlation matrix as a 2D numpy array rounded to the nearest 4th decimal, returned as a python list.\n    \"\"\"\n    if Y is None:\n        corr_matrix = np.corrcoef(X, rowvar=False)\n    else:\n        combined = np.hstack((X, Y))\n        corr_matrix = np.corrcoef(combined, rowvar=False)\n    corr_matrix_rounded = np.around(corr_matrix, decimals=4).tolist()\n    return corr_matrix_rounded"}
{"task_id": 37, "completion_id": 2, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): A 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: The correlation matrix as a python list.\n    \"\"\"\n    if Y is None:\n        data = X\n    else:\n        data = np.hstack((X, Y))\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 3, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        data = X\n    else:\n        data = np.hstack((X, Y))\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 4, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features_X).\n    Y (numpy.ndarray, optional): A 2D numpy array of shape (n_samples, n_features_Y).\n    \n    Returns:\n    list: The correlation matrix as a reshaped python list.\n    \"\"\"\n    if Y is None:\n        data = X\n    else:\n        data = np.hstack((X, Y))\n    centered_data = data - np.mean(data, axis=0)\n    covariance_matrix = np.cov(centered_data, rowvar=False)\n    std_devs = np.std(data, axis=0, ddof=0)\n    correlation_matrix = covariance_matrix / np.outer(std_devs, std_devs)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 5, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    covariance_matrix = np.cov(X, Y, rowvar=False)\n    std_devs_X = np.std(X, axis=0, ddof=0)\n    std_devs_Y = np.std(Y, axis=0, ddof=0)\n    std_dev_matrix = np.outer(std_devs_X, std_devs_Y)\n    correlation_matrix = covariance_matrix / std_dev_matrix\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 6, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 7, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 8, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 37, "completion_id": 9, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    Y (numpy.ndarray, optional): A 2D numpy array of shape (n_samples, m_features).\n    \n    Returns:\n    list: The correlation matrix as a 2D list with values rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        data = X\n    else:\n        data = np.hstack((X, Y))\n    corr_matrix = np.corrcoef(data, rowvar=False)\n    corr_matrix_rounded = np.around(corr_matrix, decimals=4)\n    corr_matrix_list = corr_matrix_rounded.tolist()\n    return corr_matrix_list"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_clf = {}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            feature_values = np.unique(X[:, feature_i])\n            for threshold in feature_values:\n                p = 1\n                predictions = np.ones(n_samples)\n                predictions[X[:, feature_i] < threshold] = -1\n                weighted_error = sample_weights[y != predictions].sum()\n                if weighted_error > 0.5:\n                    weighted_error = 1 - weighted_error\n                    p = -1\n                if weighted_error < min_error:\n                    min_error = weighted_error\n                    best_clf = {'feature_index': feature_i, 'threshold': threshold, 'polarity': p}\n        alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(n_samples)\n        feature_i = best_clf['feature_index']\n        threshold = best_clf['threshold']\n        polarity = best_clf['polarity']\n        predictions[X[:, feature_i] < threshold] = -1\n        predictions *= polarity\n        sample_weights *= np.exp(-alpha * y * predictions)\n        sample_weights /= sample_weights.sum()\n        best_clf['alpha'] = alpha\n        classifiers.append(best_clf)\n    return [{k: round(v, 4) if isinstance(v, float) else v for (k, v) in clf.items()} for clf in classifiers]"}
{"task_id": 38, "completion_id": 1, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n\n    def _sign(x, threshold):\n        return 1 if x <= threshold else -1\n\n    def weighted_error(y, prediction, weights):\n        return round(np.sum(weights[y != prediction]), 4)\n    for _ in range(n_clf):\n        clf = {'feature_idx': None, 'threshold': None, 'alpha': None}\n        min_error = float('inf')\n        predictions = np.zeros(n_samples)\n        for feature_i in range(n_features):\n            (thresholds, classes) = np.unique(X[:, feature_i], return_index=True)\n            y_sample = y[classes]\n            for threshold in thresholds:\n                p1 = np.ones(n_samples)\n                p1[X[:, feature_i] > threshold] = -1\n                p2 = np.ones(n_samples)\n                p2[X[:, feature_i] <= threshold] = -1\n                error1 = weighted_error(y_sample, p1[classes], sample_weights[classes])\n                error2 = weighted_error(y_sample, p2[classes], sample_weights[classes])\n                error = min(error1, error2)\n                predictions_temp = p1 if error == error1 else p2\n                if error < min_error:\n                    clf['feature_idx'] = feature_i\n                    clf['threshold'] = threshold\n                    min_error = error\n                    predictions[:] = predictions_temp\n        clf['alpha'] = round(0.5 * math.log((1.0 - min_error) / (min_error + 1e-10)), 4)\n        sample_weights *= np.exp(-clf['alpha'] * y * predictions)\n        sample_weights /= round(np.sum(sample_weights), 4)\n        classifiers.append(clf)\n    return classifiers"}
{"task_id": 38, "completion_id": 2, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n\n    def weak_clf_predict(X, feature, threshold, polarity):\n        predictions = np.ones(X.shape[0])\n        if polarity == 1:\n            predictions[X[:, feature] < threshold] = -1\n        else:\n            predictions[X[:, feature] >= threshold] = -1\n        return predictions\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        clf = {'feature': None, 'threshold': None, 'polarity': None, 'alpha': None}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            thresholds = np.unique(X[:, feature_i])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = weak_clf_predict(X, feature_i, threshold, polarity)\n                    weighted_error = np.sum(sample_weights[y != predictions])\n                    if weighted_error > 0.5:\n                        weighted_error = 1 - weighted_error\n                    if weighted_error < min_error:\n                        min_error = weighted_error\n                        clf['feature'] = feature_i\n                        clf['threshold'] = threshold\n                        clf['polarity'] = polarity\n        clf['alpha'] = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = weak_clf_predict(X, clf['feature'], clf['threshold'], clf['polarity'])\n        sample_weights *= np.exp(-clf['alpha'] * y * predictions)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append(clf)\n    return [{k: round(v, 4) if isinstance(v, (float, np.float64)) else v for (k, v) in clf.items()} for clf in classifiers]"}
{"task_id": 38, "completion_id": 3, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n\n    def find_best_threshold(feature_values, y, sample_weights):\n        thresholds = np.unique(feature_values)\n        best_threshold = None\n        min_error = float('inf')\n        best_polarity = 1\n        for threshold in thresholds:\n            for polarity in [1, -1]:\n                predictions = np.ones_like(y)\n                predictions[feature_values * polarity < threshold * polarity] = -1\n                weighted_error = np.sum(sample_weights[y != predictions])\n                if weighted_error < min_error:\n                    min_error = weighted_error\n                    best_threshold = threshold\n                    best_polarity = polarity\n        return (best_threshold, best_polarity, min_error)\n    for _ in range(n_clf):\n        (best_feature, best_threshold, best_polarity, best_error) = (None, None, None, float('inf'))\n        for feature in range(n_features):\n            (threshold, polarity, error) = find_best_threshold(X[:, feature], y, sample_weights)\n            if error < best_error:\n                best_feature = feature\n                best_threshold = threshold\n                best_polarity = polarity\n                best_error = error\n        alpha = 0.5 * math.log((1 - best_error) / (best_error + 1e-10))\n        predictions = np.ones_like(y)\n        feature_values = X[:, best_feature]\n        predictions[feature_values * best_polarity < best_threshold * best_polarity] = -1\n        misclassified = (predictions != y).astype(int)\n        sample_weights *= np.exp(alpha * misclassified)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append({'feature': best_feature, 'threshold': round(best_threshold, 4), 'polarity': best_polarity, 'alpha': round(alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 4, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_clf = {}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            feature_values = X[:, feature_i]\n            unique_values = np.unique(feature_values)\n            for threshold in unique_values:\n                p = 1\n                error = sum(sample_weights[y != p])\n                if error > 0.5:\n                    error = 1 - error\n                    p = 0\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature_i': feature_i, 'threshold': threshold, 'p': p}\n        EPS = 1e-10\n        clf_alpha = 0.5 * math.log((1.0 - min_error + EPS) / (min_error + EPS))\n        best_clf['alpha'] = clf_alpha\n        classifiers.append(best_clf)\n        new_weights = []\n        for i in range(n_samples):\n            prediction = 1 if X[i, best_clf['feature_i']] < best_clf['threshold'] else 0\n            weight = sample_weights[i] * math.exp(-clf_alpha * y[i] * prediction)\n            new_weights.append(weight)\n        sample_weights = new_weights / sum(new_weights)\n    for clf in classifiers:\n        clf['alpha'] = round(clf['alpha'], 4)\n        clf['threshold'] = round(clf['threshold'], 4)\n    return classifiers"}
{"task_id": 38, "completion_id": 5, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n\n    def weighted_error(weights, y_pred):\n        return round(np.sum(weights[y != y_pred]) / np.sum(weights), 4)\n\n    def predict_stump(feature_i, threshold, sample, polarity=1):\n        prediction = 1 if sample[feature_i] >= threshold else -1\n        return prediction if polarity == 1 else -prediction\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_class = {}\n        stump_predictions = np.array([1] * n_samples)\n        for feature_i in range(n_features):\n            thresholds = np.unique(X[:, feature_i])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.array([predict_stump(feature_i, threshold, sample, polarity) for sample in X])\n                    error = weighted_error(sample_weights, predictions)\n                    if error > 0.5:\n                        error = 1 - error\n                        polarity = -polarity\n                    if error < min_error:\n                        min_error = error\n                        stump_predictions = predictions\n                        best_class = {'feature_i': feature_i, 'threshold': round(threshold, 4), 'polarity': polarity}\n        classifier_weight = round(0.5 * math.log((1.0 - min_error) / (min_error + 1e-10)), 4)\n        classifiers.append((best_class, classifier_weight))\n        sample_weights *= np.exp(-classifier_weight * y * stump_predictions)\n        sample_weights /= np.sum(sample_weights)\n    return classifiers"}
{"task_id": 38, "completion_id": 6, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        sample_weights /= sample_weights.sum()\n        best_clf = {'feature': None, 'threshold': None, 'polarity': None, 'error': float('inf')}\n        for feature_i in range(n_features):\n            feature_values = X[:, feature_i]\n            unique_values = np.unique(feature_values)\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples) * polarity\n                    predictions[feature_values < threshold] *= -polarity\n                    weighted_error = sample_weights[predictions != y].sum()\n                    if weighted_error < best_clf['error']:\n                        best_clf = {'feature': feature_i, 'threshold': threshold, 'polarity': polarity, 'error': weighted_error}\n        clf_weight = 0.5 * math.log((1.0 - best_clf['error']) / (best_clf['error'] + 1e-10))\n        sample_weights *= np.exp(-clf_weight * y * (2 * (best_clf['polarity'] * (X[:, best_clf['feature']] < best_clf['threshold'])).astype(int) - 1))\n        sample_weights = np.round(sample_weights, 4)\n        classifiers.append({'feature': best_clf['feature'], 'threshold': np.round(best_clf['threshold'], 4), 'polarity': best_clf['polarity'], 'clf_weight': np.round(clf_weight, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 7, "solution": "import numpy as np\nimport math"}
{"task_id": 38, "completion_id": 8, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n\n    def find_best_threshold(X, y, sample_weights):\n        (n_samples, n_features) = X.shape\n        best_feature = None\n        best_threshold = None\n        min_error = float('inf')\n        best_predictions = None\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                predictions = np.where(X[:, feature] < threshold, -1, 1)\n                weighted_error = np.sum(sample_weights[predictions != y])\n                if weighted_error > 0.5:\n                    weighted_error = 1 - weighted_error\n                    predictions = -predictions\n                if weighted_error < min_error:\n                    best_error = weighted_error\n                    best_predictions = predictions.copy()\n                    best_threshold = threshold\n                    best_feature = feature\n        return (best_feature, best_threshold, best_error, best_predictions)\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        (feature, threshold, error, predictions) = find_best_threshold(X, y, sample_weights)\n        if error == 0:\n            break\n        alpha = 0.5 * math.log((1 - error) / error)\n        misclassified = np.where(predictions != y, 1, 0)\n        sample_weights *= np.exp(alpha * misclassified)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append({'feature': feature, 'threshold': round(threshold, 4), 'alpha': round(alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 9, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_clf = {}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            feature_values = X[:, feature_i]\n            unique_values = np.unique(feature_values)\n            for threshold in unique_values:\n                p = 1\n                predictions = np.ones(n_samples)\n                predictions[X[:, feature_i] < threshold] = -1\n                error = np.sum(sample_weights[predictions != y])\n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature_index': feature_i, 'threshold': threshold, 'polarity': p}\n        alpha = 0.5 * math.log((1 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(n_samples)\n        predicted_correctly = predictions[y == n_clf]\n        predicted_incorrectly = predictions[y != n_clf]\n        sample_weights *= np.exp(-alpha * y * (predictions * best_clf['polarity']))\n        sample_weights /= np.sum(sample_weights)\n        best_clf['alpha'] = alpha\n        classifiers.append(best_clf)\n    for clf in classifiers:\n        clf['threshold'] = round(clf['threshold'], 4)\n        clf['alpha'] = round(clf['alpha'], 4)\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_scores = np.max(scores_array)\n    stable_scores = scores_array - max_scores\n    log_softmax_values = stable_scores - np.log(np.sum(np.exp(stable_scores)))\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    result_list = rounded_log_softmax_values.tolist()\n    return result_list"}
{"task_id": 39, "completion_id": 1, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    scores_array_max_subtracted = scores_array - np.max(scores_array)\n    softmax_values = np.exp(scores_array_max_subtracted) / np.sum(np.exp(scores_array_max_subtracted))\n    log_softmax_values = np.log(softmax_values)\n    log_softmax_values_rounded = np.round(log_softmax_values, 4)\n    log_softmax_list = log_softmax_values_rounded.tolist()\n    return log_softmax_list"}
{"task_id": 39, "completion_id": 2, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    exp_scores = np.exp(stable_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_softmax_values = stable_scores - np.log(sum_exp_scores)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    return rounded_log_softmax_values.tolist()"}
{"task_id": 39, "completion_id": 3, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stabilized_scores = scores_array - max_score\n    softmax_probs = np.exp(stabilized_scores) / np.sum(np.exp(stabilized_scores))\n    log_softmax_values = np.log(softmax_probs)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    result_list = rounded_log_softmax_values.tolist()\n    return result_list\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 39, "completion_id": 4, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_np = np.array(scores)\n    max_score = np.max(scores_np)\n    stable_scores = scores_np - max_score\n    softmax = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax)\n    log_softmax_rounded = np.round(log_softmax_values, 4)\n    return log_softmax_rounded.tolist()\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 39, "completion_id": 5, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    stable_scores = scores - max_score\n    softmax_probs = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax_probs)\n    log_softmax_values_rounded = np.round(log_softmax_values, 4)\n    return log_softmax_values_rounded.tolist()"}
{"task_id": 39, "completion_id": 6, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    softmax = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax)\n    log_softmax_rounded = np.round(log_softmax_values, 4)\n    result = log_softmax_rounded.tolist()\n    return result\nscores = [10.0, 20.0, 30.0]"}
{"task_id": 39, "completion_id": 7, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stabilized_scores = scores_array - max_score\n    softmax_probs = np.exp(stabilized_scores) / np.sum(np.exp(stabilized_scores))\n    log_softmax_values = np.log(softmax_probs)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    return rounded_log_softmax_values.tolist()"}
{"task_id": 39, "completion_id": 8, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores, dtype=np.float64)\n    max_score = np.max(scores_array)\n    stabilized_scores = scores_array - max_score\n    softmax_probs = np.exp(stabilized_scores) / np.sum(np.exp(stabilized_scores))\n    log_softmax_values = np.log(softmax_probs)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    return rounded_log_softmax_values.tolist()\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 39, "completion_id": 9, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stabilized_scores = scores_array - max_score\n    softmax_scores = np.exp(stabilized_scores) / np.sum(np.exp(stabilized_scores))\n    log_softmax_scores = np.log(softmax_scores)\n    rounded_log_softmax_scores = np.round(log_softmax_scores, 4)\n    result = rounded_log_softmax_scores.tolist()\n    return result\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        X_grad = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, W_grad)\n            self.w0 = self.w0_opt.update(self.w0, w0_grad)\n        return X_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 1, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Optimizer(object):\n\n    def update_params(self, layer):\n        raise NotImplementedError()\nclass SGD(Optimizer):\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update_params(self, layer):\n        layer.W -= self.learning_rate * layer.grad_W\n        layer.w0 -= self.learning_rate * layer.grad_w0\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.grad_W = None\n        self.grad_w0 = None\n        self.optimizer = SGD(learning_rate=0.01)\n\n    def initialize(self):\n        if self.input_shape is not None:\n            limit = 1 / math.sqrt(self.input_shape[0])\n            self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n            self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        self.grad_W = np.dot(self.layer_input.T, accum_grad)\n        self.grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.optimizer.update_params(self)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 2, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass SimpleSGD:\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, params, grads):\n        for (param, grad) in zip(params, grads):\n            param -= self.learning_rate * grad\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = SimpleSGD()\n        self.w0_optimizer = SimpleSGD()\n\n    def initialize(self, optimizer=None):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        if optimizer:\n            self.W_optimizer = copy.deepcopy(optimizer)\n            self.w0_optimizer = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        X_grad = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, W_grad)\n            self.w0 = self.w0_optimizer.update(self.w0, w0_grad)\n        return X_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 3, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.dW = None\n        self.dw0 = None\n        self.optimizer = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        self.dW = np.dot(self.layer_input.T, accum_grad)\n        self.dw0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimizer.update(self.W, self.dW)\n            self.w0 = self.optimizer_w0.update(self.w0, self.dw0)\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 4, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass SGD:\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, w, grad_w):\n        return w - self.learning_rate * grad_w\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = SGD()\n        self.w0_opt = SGD()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        grad_input = np.dot(accum_grad, W.T)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 5, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass SGD(object):\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, w, grad_wrt_w):\n        return w - self.learning_rate * grad_wrt_w\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = SGD()\n        self.w0_opt = SGD()\n\n    def initialize(self, optimizer=None):\n        if optimizer:\n            self.W_opt = copy.copy(optimizer)\n            self.w0_opt = copy.copy(optimizer)\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_wrt_W = np.dot(self.layer_input.T, accum_grad)\n            grad_wrt_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_wrt_W)\n            self.w0 = self.w0_opt.update(self.w0, grad_wrt_w0)\n        grad_wrt_input = np.dot(accum_grad, W.T)\n        return grad_wrt_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 6, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_w = np.dot(self.layer_input.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        grad_out = np.dot(accum_grad, self.W.T)\n        return grad_out\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 7, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        grad_wrt_w = np.dot(self.layer_input.T, accum_grad)\n        grad_wrt_x = np.dot(accum_grad, self.W.T)\n        grad_wrt_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, grad_wrt_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_wrt_w0)\n        return grad_wrt_x.reshape(self.layer_input.shape)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 8, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizers = {}\n        self.grad_W = None\n        self.grad_w0 = None\n\n    def initialize(self, optimizer=None):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        if optimizer is not None:\n            self.optimizers['W'] = copy.deepcopy(optimizer)\n            self.optimizers['w0'] = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        self.grad_W = np.dot(self.layer_input.T, accum_grad)\n        self.grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.optimizers['W'].update_params(self)\n            self.optimizers['w0'].update_params(self)\n        grad_X = np.dot(accum_grad, self.W.T)\n        return grad_X\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 9, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + self.w0.shape[1]\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        weights = self.W\n        if self.layer_input.ndim == 2:\n            grad_input = np.dot(accum_grad, weights.T)\n        else:\n            grad_input = np.tensordot(accum_grad, weights.T, axes=([2], [0]))\n            grad_input = np.expand_dims(grad_input, axis=0)\n        self.grad_W = np.dot(self.layer_input.T, accum_grad)\n        self.grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, self.grad_W)\n            self.w0 = self.w0_opt.update(self.w0, self.grad_w0)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_rows, input_cols) = input_matrix.shape\n    (kernel_rows, kernel_cols) = kernel.shape\n    output_rows = (input_rows + 2 * padding - kernel_rows) // stride + 1\n    output_cols = (input_cols + 2 * padding - kernel_cols) // stride + 1\n    output = np.zeros((output_rows, output_cols))\n    for i in range(0, output_rows):\n        for j in range(0, output_cols):\n            row_start = i * stride\n            row_end = row_start + kernel_rows\n            col_start = j * stride\n            col_end = col_start + kernel_cols\n            output[i, j] = np.sum(padded_matrix[row_start:row_end, col_start:col_end] * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[0.1, 0.2], [0.3, 0.4]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 1, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_matrix = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            roi = padded_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(roi * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()"}
{"task_id": 41, "completion_id": 2, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = int((input_height - kernel_height + 2 * padding) / stride) + 1\n    output_width = int((input_width - kernel_width + 2 * padding) / stride) + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            roi = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(roi * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 3, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_height, input_width) = padded_input.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            roi = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(roi * kernel)\n    return np.round(output, decimals=4).tolist()"}
{"task_id": 41, "completion_id": 4, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 5, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    if input_matrix.ndim != 2 or kernel.ndim != 2:\n        raise ValueError('Input matrix and kernel must be 2D arrays.')\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n    out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n    padded_matrix = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_matrix = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            region = padded_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]])\nkernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 6, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            roi = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(roi * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 7, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    return np.round(output, 4).tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 8, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height + 2 * padding - kernel_height) // stride + 1\n    out_width = (in_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((out_height, out_width))\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    for i in range(out_height):\n        for j in range(out_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1"}
{"task_id": 41, "completion_id": 9, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Apply the Rectified Linear Unit (ReLU) activation function to the input.\n\n    Parameters:\n    z (float): The input value to which the ReLU function is applied.\n\n    Returns:\n    float: The output value after applying the ReLU function.\n    \"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 1, "solution": "def relu(z: float) -> float:\n    \"\"\"Apply the Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 2, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to the input.\n\n    Parameters:\n    z (float): The input value to which the ReLU function is applied.\n\n    Returns:\n    float: The output value after applying the ReLU function.\n    \"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 3, "solution": "def relu(z: float) -> float:\n    \"\"\"Apply the Rectified Linear Unit (ReLU) activation function to the input.\"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 4, "solution": "def relu(z: float) -> float:\n    \"\"\"Compute the ReLU activation of the input.\"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 5, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function.\n    \"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 6, "solution": "def relu(z: float) -> float:\n    \"\"\"Implement the Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 7, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 8, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 9, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return np.round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 1, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization_term = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization_term\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 2, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 3, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 4, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 5, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 6, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 7, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 8, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 43, "completion_id": 9, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative input values. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 1, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implement the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 2, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    - z: The input value (float).\n    - alpha: The slope for negative inputs (float, default is 0.01).\n\n    Returns:\n    - The value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 44, "completion_id": 3, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 44, "completion_id": 4, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs, default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 5, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input z.\n\n    Parameters:\n    - z (float): The input value to the activation function.\n    - alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    - float: The result of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 6, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the Leaky ReLU function.\n    alpha (float): The slope for negative inputs, default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 7, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    if z < 0:\n        return alpha * z\n    else:\n        return z"}
{"task_id": 44, "completion_id": 8, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    - z: float, the input value.\n    - alpha: float, the slope for negative inputs, default is 0.01.\n\n    Returns:\n    - float, the output value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 44, "completion_id": 9, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    - z (float): The input value.\n    - alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    - float: The value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel (dot product) of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 1, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 2, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): First input vector.\n    x2 (np.array): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 3, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 4, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): The first input vector.\n    x2 (np.ndarray): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 5, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): The first input vector.\n    x2 (np.ndarray): The second input vector.\n    \n    Returns:\n    float: The linear kernel (dot product) of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 6, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): First input vector.\n    x2 (np.array): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 7, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (numpy array): First input vector.\n    x2 (numpy array): Second input vector.\n    \n    Returns:\n    float: The linear kernel value (dot product of x1 and x2).\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 8, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 9, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): First input vector.\n    x2 (np.array): Second input vector.\n    \n    Returns:\n    float: The result of the linear kernel, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy.array): True binary labels.\n    y_pred (numpy.array): Predicted binary labels.\n\n    Returns:\n    float: Precision score.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_score = true_positives / (true_positives + false_positives)\n    return precision_score"}
{"task_id": 46, "completion_id": 1, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    precision_value = TP / (TP + FP)\n    return precision_value\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 1])"}
{"task_id": 46, "completion_id": 2, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    else:\n        return TP / (TP + FP)"}
{"task_id": 46, "completion_id": 3, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_score = true_positives / (true_positives + false_positives)\n    return precision_score"}
{"task_id": 46, "completion_id": 4, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision score.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_score = true_positives / (true_positives + false_positives)\n    return precision_score"}
{"task_id": 46, "completion_id": 5, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    precision_value = TP / (TP + FP)\n    return precision_value\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 1])"}
{"task_id": 46, "completion_id": 6, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 46, "completion_id": 7, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n\n    Returns:\n    float: Precision metric.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value"}
{"task_id": 46, "completion_id": 8, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value"}
{"task_id": 46, "completion_id": 9, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    precision_value = TP / (TP + FP) if TP + FP != 0 else 0\n    return precision_value\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 0, 1, 0, 0]"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.column_stack((np.ones(len(X)), X))\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradients = 2 / n_samples * X.T.dot(X.dot(weights) - y)\n            weights -= learning_rate * gradients\n        elif method == 'stochastic':\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index + 1]\n                yi = y[random_index:random_index + 1]\n                gradients = 2 * xi.T.dot(xi.dot(weights) - yi)\n                weights -= learning_rate * gradients\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                gradients = 2 / len(xi) * xi.T.dot(xi.dot(weights) - yi)\n                weights -= learning_rate * gradients\n        else:\n            raise ValueError(\"Invalid method. Please choose 'batch', 'stochastic', or 'mini-batch'.\")\n    return weights.round(4).tolist()"}
{"task_id": 47, "completion_id": 1, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization for a given dataset and weights using the specified method.\n\n    Parameters:\n    - X: np.ndarray, input feature matrix of shape (n_samples, n_features)\n    - y: np.ndarray, target values of shape (n_samples,)\n    - weights: np.ndarray, initial weight values of shape (n_features,)\n    - learning_rate: float, learning rate for gradient descent\n    - n_iterations: int, number of iterations to perform\n    - batch_size: int, size of the mini-batch (only used for mini-batch gradient descent)\n    - method: str, specifies the type of gradient descent ('batch', 'stochastic', 'mini-batch')\n\n    Returns:\n    - list: final weights after completing the iterations, rounded to the nearest 4th decimal place\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            y_predicted = np.dot(X, weights)\n            error = y_predicted - y\n            grad = 2 / n_samples * np.dot(X.T, error)\n            weights -= learning_rate * grad\n        elif method == 'stochastic':\n            for i in range(n_samples):\n                x_i = X[i:i + 1]\n                y_i = y[i:i + 1]\n                y_predicted = np.dot(x_i, weights)\n                error = y_predicted - y_i\n                grad = 2 * np.dot(x_i.T, error)\n                weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                x_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                y_predicted = np.dot(x_batch, weights)\n                error = y_predicted - y_batch\n                grad = 2 / batch_size * np.dot(x_batch.T, error)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"Invalid method specified. Please use 'batch', 'stochastic', or 'mini-batch'.\")\n    return weights.round(4).tolist()"}
{"task_id": 47, "completion_id": 2, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    y = y.reshape(-1, 1)\n    weights = weights.reshape(-1, 1)\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            indices = np.random.permutation(X_b.shape[0])\n            X_b_shuffled = X_b[indices]\n            y_shuffled = y[indices]\n            for i in range(X_b.shape[0]):\n                random_index = i\n                xi = X_b_shuffled[random_index:random_index + 1]\n                yi = y_shuffled[random_index:random_index + 1]\n                gradients = 2 * xi.T.dot(xi.dot(weights) - yi)\n                weights = weights - learning_rate * gradients\n        elif method == 'batch':\n            gradients = 2 / X_b.shape[0] * X_b.T.dot(X_b.dot(weights) - y)\n            weights = weights - learning_rate * gradients\n        elif method == 'mini_batch':\n            shuffled_indices = np.random.permutation(X_b.shape[0])\n            for i in range(0, X_b.shape[0], batch_size):\n                X_b_mini = X_b[shuffled_indices[i:i + batch_size]]\n                y_mini = y[shuffled_indices[i:i + batch_size]]\n                gradients = 2 / batch_size * X_b_mini.T.dot(X_b_mini.dot(weights) - y_mini)\n                weights = weights - learning_rate * gradients\n        else:\n            raise ValueError(\"Invalid method specified. Choose 'stochastic', 'batch', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 3, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights)\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            indices = np.arange(n_samples)\n        elif method == 'stochastic':\n            indices = np.random.randint(0, n_samples, 1)\n        elif method == 'mini-batch':\n            indices = np.random.randint(0, n_samples, batch_size)\n        else:\n            raise ValueError(\"Method should be one of 'batch', 'stochastic', or 'mini-batch'\")\n        X_batch = X[indices]\n        y_batch = y[indices]\n        predictions = np.dot(X_batch, weights)\n        errors = predictions - y_batch\n        gradient = 2 / len(X_batch) * np.dot(X_batch.T, errors)\n        weights -= learning_rate * gradient\n    return weights.round(4).tolist()"}
{"task_id": 47, "completion_id": 4, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights)\n\n    def mse_loss(y_true, y_pred):\n        return np.mean((y_true - y_pred) ** 2)\n\n    def mse_gradient(X, y_true, y_pred):\n        return -2 * np.dot(X.T, y_true - y_pred) / len(y_true)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            y_pred = np.dot(X, weights)\n            gradient = mse_gradient(X, y, y_pred)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for i in range(len(X)):\n                y_pred = np.dot(X[i], weights)\n                gradient = mse_gradient(X[i].reshape(1, -1), y[i].reshape(1, -1), y_pred)\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, len(X), batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                y_pred = np.dot(X_batch, weights)\n                gradient = mse_gradient(X_batch, y_batch, y_pred)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Unsupported method. Please choose 'batch', 'stochastic', or 'mini-batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 5, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n    n_samples = X.shape[0]\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 / n_samples * np.dot(X.T, errors)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            indices = np.random.permutation(n_samples)\n            for idx in indices:\n                xi = X[idx:idx + 1]\n                yi = y[idx:idx + 1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * np.dot(xi.T, error)\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                predictions = np.dot(xi, weights)\n                errors = predictions - yi\n                gradient = 2 / batch_size * np.dot(xi.T, errors)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 6, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 / n_samples * np.dot(X.T, errors)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for i in range(n_samples):\n                xi = X[i:i + 1]\n                yi = y[i:i + 1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * np.dot(xi.T, error)\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                predictions = np.dot(xi, weights)\n                errors = predictions - yi\n                gradient = 2 / batch_size * np.dot(xi.T, errors)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method specified. Use 'batch', 'stochastic', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 7, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n    m = len(y)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            y_pred = X @ weights\n            error = y_pred - y\n            grad = 1 / m * (X.T @ error)\n        elif method == 'stochastic':\n            indices = np.random.permutation(m)\n            for i in indices:\n                xi = X[i].reshape(1, -1)\n                yi = y[i].reshape(1, -1)\n                y_pred = xi @ weights\n                error = y_pred - yi\n                grad = xi.T @ error\n                weights -= learning_rate * grad\n        elif method == 'mini-batch':\n            indices = np.random.permutation(m)\n            for start_idx in range(0, m, batch_size):\n                end_idx = min(start_idx + batch_size, m)\n                xi = X[indices[start_idx:end_idx]]\n                yi = y[indices[start_idx:end_idx]]\n                y_pred = xi @ weights\n                error = y_pred - yi\n                grad = 1 / batch_size * (xi.T @ error)\n                weights -= learning_rate * grad\n        else:\n            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'mini-batch'.\")\n        if method in ['batch', 'mini-batch']:\n            weights -= learning_rate * grad\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 8, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent on the given dataset to minimize Mean Squared Error (MSE).\n    \n    Parameters:\n    - X: numpy array of shape (m, n) representing the input features (with a bias term added if needed).\n    - y: numpy array of shape (m, 1) representing the target values.\n    - weights: numpy array of shape (n, 1) representing the initial weights.\n    - learning_rate: float representing the step size for each iteration.\n    - n_iterations: int representing the number of iterations to perform.\n    - batch_size: int representing the number of samples to use in each batch (only relevant for Mini-Batch and SGD).\n    - method: str, either 'batch', 'sgd', or 'mini-batch', specifying the gradient descent variant to use.\n    \n    Returns:\n    - A list representing the final weights after performing gradient descent, rounded to the nearest 4th decimal place.\n    \"\"\"\n    (m, n) = X.shape\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            gradient = 1 / m * X.T @ (X @ weights - y)\n        elif method == 'sgd':\n            indices = np.random.permutation(m)\n            for idx in indices:\n                xi = X[idx:idx + 1]\n                yi = y[idx:idx + 1]\n                gradient = xi.T @ (xi @ weights - yi)\n                weights -= learning_rate * gradient\n            continue\n        elif method == 'mini-batch':\n            for i in range(0, m, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                gradient = 1 / batch_size * xi.T @ (xi @ weights - yi)\n        else:\n            raise ValueError(\"Invalid method specified. Choose 'batch', 'sgd', or 'mini-batch'.\")\n        weights -= learning_rate * gradient\n    return weights.round(4).tolist()"}
{"task_id": 47, "completion_id": 9, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.insert(X, 0, 1, axis=1)\n    (n_samples, n_features) = X.shape\n    y = np.array(y)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            y_pred = X.dot(weights)\n            errors = y_pred - y\n            gradient = 2 / n_samples * X.T.dot(errors)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for i in range(n_samples):\n                xi = X[i:i + 1]\n                yi = y[i:i + 1]\n                y_pred = xi.dot(weights)\n                errors = y_pred - yi\n                gradient = 2 * xi.T.dot(errors)\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                y_pred = xi.dot(weights)\n                errors = y_pred - yi\n                gradient = 2 / batch_size * xi.T.dot(errors)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    (r, c) = (0, 0)\n    while r < rows and c < cols:\n        pivot_row = r\n        while pivot_row < rows and A[pivot_row, c] == 0:\n            pivot_row += 1\n        if pivot_row < rows:\n            A[[r, pivot_row]] = A[[pivot_row, r]]\n            if A[r, c] != 0:\n                A[r] = A[r] / A[r, c]\n            for i in range(rows):\n                if i != r:\n                    A[i] = A[i] - A[i, c] * A[r]\n            r += 1\n        c += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 1, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    r = 0\n    for c in range(cols):\n        max_row = np.argmax(np.abs(A[r:rows, c])) + r\n        if A[max_row, c] != 0:\n            A[[r, max_row]] = A[[max_row, r]]\n            A[r] = A[r] / A[r, c]\n            for i in range(rows):\n                if i != r:\n                    A[i] = A[i] - A[i, c] * A[r]\n            r += 1\n    return A.tolist()\nmatrix = [[1, 2, 3, 4], [2, 4, 6, 8], [1, 0, 1, 0]]"}
{"task_id": 48, "completion_id": 2, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    r = 0\n    for col in range(cols):\n        pivot_row = r\n        while pivot_row < rows and A[pivot_row, col] == 0:\n            pivot_row += 1\n        if pivot_row < rows:\n            A[[r, pivot_row]] = A[[pivot_row, r]]\n            pivot_value = A[r, col]\n            A[r] = A[r] / pivot_value\n            for other_row in range(rows):\n                if other_row != r:\n                    factor = A[other_row, col]\n                    A[other_row] = A[other_row] - factor * A[r]\n            r += 1\n    return A.tolist()"}
{"task_id": 48, "completion_id": 3, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            return matrix.tolist()\n        i = r\n        while matrix[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return matrix.tolist()\n        matrix[[i, r]] = matrix[[r, i]]\n        div = matrix[r, lead]\n        matrix[r] /= div\n        for i in range(rows):\n            if i != r:\n                matrix[i] -= matrix[i, lead] * matrix[r]\n        lead += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 4, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    r = 0\n    for c in range(cols):\n        if r >= rows:\n            break\n        pivot_row = r\n        while pivot_row < rows and A[pivot_row, c] == 0:\n            pivot_row += 1\n        if pivot_row == rows:\n            continue\n        A[[r, pivot_row]] = A[[pivot_row, r]]\n        A[r] = A[r] / A[r, c]\n        for i in range(rows):\n            if i != r:\n                A[i] = A[i] - A[i, c] * A[r]\n        r += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 5, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A matrix represented as a list of lists.\n    \n    Returns:\n    list of lists: The matrix in RREF.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (m, n) = A.shape\n    lead = 0\n    for r in range(m):\n        if lead >= n:\n            return A.tolist()\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == m:\n                i = r\n                lead += 1\n                if n == lead:\n                    return A.tolist()\n        A[[i, r]] = A[[r, i]]\n        A[r] = A[r] / A[r, lead]\n        for i in range(m):\n            if i != r:\n                A[i] = A[i] - A[i, lead] * A[r]\n        lead += 1\n    return A.tolist()\nmatrix = [[1, 2, 1, -1], [2, 2, -1, 3], [-1, -2, 2, -1]]"}
{"task_id": 48, "completion_id": 6, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    r = 0\n    for c in range(cols):\n        if r >= rows:\n            break\n        pivot_row = r\n        while pivot_row < rows and A[pivot_row, c] == 0:\n            pivot_row += 1\n        if pivot_row < rows:\n            A[[r, pivot_row]] = A[[pivot_row, r]]\n            pivot = A[r, c]\n            A[r] = A[r] / pivot\n            for i in range(rows):\n                if i != r:\n                    factor = A[i, c]\n                    A[i] = A[i] - factor * A[r]\n            r += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 7, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    r = 0\n    for c in range(cols):\n        if r >= rows:\n            break\n        pivot_row = r\n        while pivot_row < rows and A[pivot_row, c] == 0:\n            pivot_row += 1\n        if pivot_row == rows:\n            continue\n        A[[r, pivot_row]] = A[[pivot_row, r]]\n        pivot = A[r, c]\n        A[r] = A[r] / pivot\n        for i in range(rows):\n            if i != r:\n                A[i] = A[i] - A[i, c] * A[r]\n        r += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 8, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    pivot_row = 0\n    for col in range(cols):\n        if pivot_row >= rows:\n            break\n        max_row = pivot_row + np.argmax(np.abs(A[pivot_row:, col]))\n        A[[pivot_row, max_row]] = A[[max_row, pivot_row]]\n        if A[pivot_row, col] != 0:\n            A[pivot_row] = A[pivot_row] / A[pivot_row, col]\n            for other_row in range(rows):\n                if other_row != pivot_row:\n                    A[other_row] = A[other_row] - A[other_row, col] * A[pivot_row]\n            pivot_row += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 9, "solution": "import numpy as np\ndef rref(matrix):\n    m = np.array(matrix, dtype=float)\n    (rows, cols) = m.shape\n    row = 0\n    for col in range(cols):\n        pivot_row = np.argmax(np.abs(m[row:, col])) + row\n        if m[pivot_row, col] != 0:\n            m[[row, pivot_row]] = m[[pivot_row, row]]\n            m[row] = m[row] / m[row, col]\n            for other_row in range(rows):\n                if other_row != row:\n                    m[other_row] = m[other_row] - m[other_row, col] * m[row]\n            row += 1\n    return m.tolist()\nmatrix = [[1, -2, 1, 0], [-2, 4, -2, 1], [0, 0, 0, 1]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g * g)\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return [2 * x[0], 2 * x[1]]\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [1.0, 2.0]"}
{"task_id": 49, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for i in range(num_iterations):\n        t += 1\n        g = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return [2 * x[0], 2 * x[1]]\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = x0.copy()\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = np.array([1.0, 1.0])"}
{"task_id": 49, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return [2 * x[0], 2 * x[1]]\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=float)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [0.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0.0\n    for iteration in range(max_iter):\n        y_pred = X.dot(w) + b\n        gradient_w = 1 / n * X.T.dot(y_pred - y)\n        gradient_b = 1 / n * np.sum(y_pred - y)\n        w = np.sign(w) * np.maximum(np.abs(w) - alpha * learning_rate, 0)\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        if np.linalg.norm(gradient_w) < tol:\n            break\n    return (w.round(4).tolist(), round(b, 4))"}
{"task_id": 50, "completion_id": 1, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for iteration in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        error = predictions - y\n        grad_weights = 1 / n_samples * np.dot(X.T, error) + alpha * np.sign(weights)\n        grad_bias = 1 / n_samples * np.sum(error)\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n        if np.linalg.norm(grad_weights) < tol:\n            break\n    return (weights.round(4).tolist(), bias.round(4))"}
{"task_id": 50, "completion_id": 2, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    weights = np.zeros(p)\n    bias = 0.0\n    prev_cost = float('inf')\n    for iteration in range(max_iter):\n        predictions = X @ weights + bias\n        error = predictions - y\n        gradient_weights = 1 / n * X.T @ error\n        gradient_bias = 1 / n * np.sum(error)\n        weights -= learning_rate * (gradient_weights + alpha * np.sign(weights))\n        bias -= learning_rate * gradient_bias\n        cost = 1 / (2 * n) * np.sum(error ** 2) + alpha * np.sum(np.abs(weights))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (weights.round(4).tolist(), bias.round(4))"}
{"task_id": 50, "completion_id": 3, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y) + alpha * np.sign(weights)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        if iteration > 0 and abs(loss - prev_loss) < tol:\n            break\n        prev_loss = loss\n    return (np.round(weights, 4).tolist(), np.round(bias, 4))"}
{"task_id": 50, "completion_id": 4, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    X_transpose = X.T\n    prev_loss = float('inf')\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        gradient_weights = 1 / n_samples * np.dot(X_transpose, error)\n        gradient_bias = 1 / n_samples * np.sum(error)\n        weights -= learning_rate * (gradient_weights + alpha * np.sign(weights))\n        bias -= learning_rate * gradient_bias\n        current_loss = 1 / (2 * n_samples) * np.sum(error ** 2) + alpha * np.sum(np.abs(weights))\n        if np.abs(current_loss - prev_loss) < tol:\n            break\n        prev_loss = current_loss\n    return (np.round(weights, 4).tolist(), np.round(bias, 4))"}
{"task_id": 50, "completion_id": 5, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for i in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        dw_l1 = np.sign(weights) * alpha\n        weights -= learning_rate * (dw + dw_l1)\n        bias -= learning_rate * db\n        if np.linalg.norm(dw) < tol:\n            break\n    return (weights.round(4).tolist(), bias.round(4))"}
{"task_id": 50, "completion_id": 6, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    X_b = np.c_[np.ones((n_samples, 1)), X]\n    weights_b = np.insert(weights, 0, bias)\n    for iteration in range(max_iter):\n        predictions = X_b.dot(weights_b)\n        errors = predictions - y\n        gradient = 1 / n_samples * X_b.T.dot(errors)\n        l1_penalty = alpha * np.sign(weights_b)\n        l1_penalty[0] = 0\n        gradient += l1_penalty\n        weights_b -= learning_rate * gradient\n        if np.linalg.norm(gradient) < tol:\n            break\n    bias = weights_b[0]\n    weights = weights_b[1:]\n    bias_rounded = round(bias, 4)\n    weights_rounded = [round(weight, 4) for weight in weights]\n    return (bias_rounded, weights_rounded)"}
{"task_id": 50, "completion_id": 7, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    y = np.array(y)\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -(1 / n_samples) * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        db = -(1 / n_samples) * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        if iteration > 0 and np.abs(loss - prev_loss) < tol:\n            break\n        prev_loss = loss\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = round(bias, 4)\n    weights_list = weights_rounded.tolist()\n    return (weights_list, bias_rounded)"}
{"task_id": 50, "completion_id": 8, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        l1_grad = alpha * np.sign(weights)\n        weights = weights - learning_rate * (dw + l1_grad)\n        bias = bias - learning_rate * db\n        if np.linalg.norm(dw) < tol:\n            break\n    return (weights.round(4).tolist(), bias.round(4))"}
{"task_id": 50, "completion_id": 9, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    weights = np.zeros(p)\n    bias = 0\n    for iteration in range(max_iter):\n        predictions = X.dot(weights) + bias\n        errors = predictions - y\n        gradient_weights = 1 / n * X.T.dot(errors)\n        gradient_bias = 1 / n * np.sum(errors)\n        weights -= learning_rate * (gradient_weights + alpha * np.sign(weights))\n        bias -= learning_rate * gradient_bias\n        cost = 1 / (2 * n) * np.sum(errors ** 2) + alpha * np.sum(np.abs(weights))\n        if iteration > 0 and abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (np.round(weights, 4).tolist(), np.round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    dp = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n    for i in range(len(source) + 1):\n        dp[i][0] = i\n    for j in range(len(target) + 1):\n        dp[0][j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len(source)][len(target)]"}
{"task_id": 51, "completion_id": 1, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_s = len(source)\n    len_t = len(target)\n    d = np.zeros((len_s + 2, len_t + 2), dtype=int)\n    d[:, 0] = np.inf\n    d[0, :] = np.inf\n    d[1, 0] = 0\n    d[0, 1] = 0\n    for i in range(1, len_s + 1):\n        for j in range(1, len_t + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            d[i + 1, j + 1] = min(d[i, j + 1] + 1, d[i + 1, j] + 1, d[i, j] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                d[i + 1, j + 1] = min(d[i + 1, j + 1], d[i - 1, j - 1] + 1)\n    return d[len_s + 1, len_t + 1]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 2, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 3, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 4, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return dp[m, n]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 5, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 6, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[m][n]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 7, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    d = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n    for i in range(len(source) + 1):\n        d[i, 0] = i\n    for j in range(len(target) + 1):\n        d[0, j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            d[i, j] = min(d[i - 1, j] + 1, d[i, j - 1] + 1, d[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                d[i, j] = min(d[i, j], d[i - 2, j - 2] + cost)\n    return d[len(source), len(target)]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 8, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (len_src, len_tgt) = (len(source), len(target))\n    d = np.zeros((len_src + 1, len_tgt + 1), dtype=int)\n    for i in range(1, len_src + 1):\n        d[i][0] = i\n    for j in range(1, len_tgt + 1):\n        d[0][j] = j\n    for i in range(1, len_src + 1):\n        for j in range(1, len_tgt + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            d[i][j] = min(d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                d[i][j] = min(d[i][j], d[i - 2][j - 2] + 1)\n    return d[len_src][len_tgt]"}
{"task_id": 51, "completion_id": 9, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i, 0] = i\n    for j in range(len_target + 1):\n        dp[0, j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return dp[len_source, len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 1, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = tp / denominator\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 2, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 3, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 4, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 5, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n        return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 52, "completion_id": 6, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 7, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        recall_value = TP / denominator\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 8, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 9, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4).tolist()\n    return output_rounded\nX = np.random.rand(5, 3)\nW_q = np.random.rand(3, 3)\nW_k = np.random.rand(3, 3)\nW_v = np.random.rand(3, 3)"}
{"task_id": 53, "completion_id": 1, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 53, "completion_id": 2, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 53, "completion_id": 3, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(W_k.shape[0])\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return np.round(output, 4).tolist()"}
{"task_id": 53, "completion_id": 4, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    output_list = output_rounded.tolist()\n    return output_list\nX = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\nW_v = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 0.1]])"}
{"task_id": 53, "completion_id": 5, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n\n    Parameters:\n    X (numpy.ndarray): Input matrix of shape (seq_len, d_model)\n    W_q (numpy.ndarray): Query weight matrix of shape (d_model, d_k)\n    W_k (numpy.ndarray): Key weight matrix of shape (d_model, d_k)\n    W_v (numpy.ndarray): Value weight matrix of shape (d_model, d_v)\n\n    Returns:\n    list: Self-attention output as a python list with shape (seq_len, d_v)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(W_k.shape[1])\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 53, "completion_id": 6, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    dot_product = np.dot(Q, K.T)\n    d_k = K.shape[-1]\n    scaled_dot_product = dot_product / np.sqrt(d_k)\n    attention_weights = np.softmax(scaled_dot_product, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    output_list = output_rounded.tolist()\n    return output_list\nX = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\nW_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_k = np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]])\nW_v = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])"}
{"task_id": 53, "completion_id": 7, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    output_list = output_rounded.tolist()\n    return output_list\nX = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\nW_v = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 0.1]])"}
{"task_id": 53, "completion_id": 8, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[1])\n    attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4).tolist()\n    return output_rounded"}
{"task_id": 53, "completion_id": 9, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return np.round(output, 4).tolist()\nX = np.array([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\nW_v = np.array([[0.5, 0.4, 0.3], [0.2, 0.1, 0.0], [0.7, 0.6, 0.5]])\noutput = self_attention(X, W_q, W_k, W_v)"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 1, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = initial_hidden_state\n    for x in input_sequence:\n        x = np.array(x)\n        h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 2, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = initial_hidden_state\n    for x in input_sequence:\n        h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 3, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    h_rounded = np.round(h, 4)\n    return h_rounded.tolist()"}
{"task_id": 54, "completion_id": 4, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = initial_hidden_state\n    for x in input_sequence:\n        h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 5, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = initial_hidden_state\n    for x in input_sequence:\n        h_prev = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 6, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h_prev = initial_hidden_state\n    for x_t in input_sequence:\n        h_t = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n        h_prev = h_t\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 7, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    return h.round(4).tolist()"}
{"task_id": 54, "completion_id": 8, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h_prev = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 9, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for input_vector in input_sequence:\n        hidden_state = np.tanh(np.dot(input_vector, Wx) + np.dot(hidden_state, Wh) + b)\n    return hidden_state.round(4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_array = np.array([(x, y, 1) for (x, y) in points])\n    translated_points_array = np.dot(points_array, translation_matrix.T)\n    translated_points = translated_points_array[:, :2].tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 1, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([[x, y, 1] for (x, y) in points]).T\n    translated_points_homogeneous = translation_matrix @ points_homogeneous\n    translated_points = translated_points_homogeneous[:2].T\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 2, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.array([[x, y, 1] for (x, y) in points])\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = translated_points[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 3, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.array([point + [1] for point in points]).T\n    translated_homogeneous_points = translation_matrix @ homogeneous_points\n    translated_points = translated_homogeneous_points[:2].T\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 4, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.array([[x, y, 1] for (x, y) in points]).T\n    translated_homogeneous_points = np.dot(translation_matrix, homogeneous_points)\n    translated_points = translated_homogeneous_points[:2, :].T\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 5, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([[x, y, 1] for (x, y) in points]).T\n    translated_points_homogeneous = np.dot(translation_matrix, points_homogeneous)\n    translated_points = translated_points_homogeneous[:2, :].T\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 6, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([[x, y, 1] for (x, y) in points])\n    translated_points_homogeneous = np.dot(points_homogeneous, translation_matrix.T)\n    translated_points = translated_points_homogeneous[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 7, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.array([point + [1] for point in points]).T\n    translated_homogeneous_points = np.dot(translation_matrix, homogeneous_points)\n    translated_points = translated_homogeneous_points[:2].T\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 8, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([[x, y, 1] for (x, y) in points])\n    translated_points_homogeneous = np.dot(points_homogeneous, translation_matrix.T)\n    translated_points = translated_points_homogeneous[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 9, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([[x, y, 1] for (x, y) in points])\n    translated_points_homogeneous = np.dot(points_homogeneous, translation_matrix.T)\n    translated_points = translated_points_homogeneous[:, :2].tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first normal distribution P\n    - sigma_p: Standard deviation of the first normal distribution P\n    - mu_q: Mean of the second normal distribution Q\n    - sigma_q: Standard deviation of the second normal distribution Q\n    \n    Returns:\n    - kl_div: KL divergence between P and Q\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_div = 0.5 * (np.log(var_q / var_p) + (var_p + (mu_p - mu_q) ** 2) / var_q - 1)\n    return kl_div\nkl_div = kl_divergence_normal(0, 1, 1, 2)"}
{"task_id": 56, "completion_id": 1, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nkl_div = kl_divergence_normal(0, 1, 1, 2)"}
{"task_id": 56, "completion_id": 2, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence"}
{"task_id": 56, "completion_id": 3, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first normal distribution P.\n    - sigma_p: Standard deviation of the first normal distribution P.\n    - mu_q: Mean of the second normal distribution Q.\n    - sigma_q: Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    - KL divergence as a floating-point number.\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_divergence = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return kl_divergence"}
{"task_id": 56, "completion_id": 4, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first normal distribution P\n    - sigma_p: Standard deviation of the first normal distribution P\n    - mu_q: Mean of the second normal distribution Q\n    - sigma_q: Standard deviation of the second normal distribution Q\n    \n    Returns:\n    - KL divergence from Q to P as a floating-point number\n    \"\"\"\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2"}
{"task_id": 56, "completion_id": 5, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: The KL divergence from Q to P.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2"}
{"task_id": 56, "completion_id": 6, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: The KL divergence from Q to P.\n    \"\"\"\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2"}
{"task_id": 56, "completion_id": 7, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    - mu_p: Mean of the first normal distribution P\n    - sigma_p: Standard deviation of the first normal distribution P\n    - mu_q: Mean of the second normal distribution Q\n    - sigma_q: Standard deviation of the second normal distribution Q\n\n    Returns:\n    - kl_divergence: KL divergence between the two normal distributions\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_divergence = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return kl_divergence"}
{"task_id": 56, "completion_id": 8, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 56, "completion_id": 9, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence D_KL(P || Q).\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_div = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    N = len(b)\n    if x_ini is None:\n        x = np.zeros(N)\n    else:\n        x = x_ini\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            sum1 = np.dot(A[i, :i], x_new[:i])\n            sum2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - sum1 - sum2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 1, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be a square matrix')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('The number of rows in A must be equal to the number of elements in b')\n    if x_ini is None:\n        x = np.zeros(b.shape)\n    else:\n        x = np.array(x_ini, dtype=float)\n    num_unknowns = len(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(num_unknowns):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()\nA = np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 4]])\nb = np.array([1, 2, 3])\nx_ini = [0, 0, 0]\nn = 10"}
{"task_id": 57, "completion_id": 2, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(N):\n            s = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s) / A[i, i]\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded"}
{"task_id": 57, "completion_id": 3, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    size = len(b)\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros(size)\n        for i in range(size):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 4, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros(N, dtype=float)\n        for i in range(N):\n            s = np.dot(A[i, :i], x_new[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 5, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A)\n    b = np.array(b)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Incompatible dimensions of A and b.')\n    if x_ini is not None:\n        x = np.array(x_ini)\n    else:\n        x = np.zeros_like(b, dtype=float)\n    N = len(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x, dtype=float)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded"}
{"task_id": 57, "completion_id": 6, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros(N, dtype=float)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 7, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be a square matrix.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('The number of rows in A must be equal to the number of elements in b.')\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros_like(x, dtype=float)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return x.round(4).tolist()"}
{"task_id": 57, "completion_id": 8, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    (m, n_m) = A.shape\n    if m != n_m:\n        raise ValueError('Matrix A must be a square matrix.')\n    if b.size != m:\n        raise ValueError('Vector b must have the same number of elements as the rows of A.')\n    if x_ini is None:\n        x = np.zeros(m, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n        if x.size != m:\n            raise ValueError('Initial guess vector x_ini must have the same number of elements as the rows of A.')\n    for _ in range(n):\n        x_new = np.copy(x)\n        for i in range(m):\n            sigma = np.dot(A[i, :i], x_new[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - sigma) / A[i, i]\n        x = x_new\n    return x.round(4).tolist()"}
{"task_id": 57, "completion_id": 9, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = len(b)\n    if x_ini is None:\n        x = np.zeros(size, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = x.copy()\n        for i in range(size):\n            s1 = sum((A[i, j] * x_new[j] for j in range(i)))\n            s2 = sum((A[i, j] * x[j] for j in range(i + 1, size)))\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        if Ab[i, i] == 0:\n            raise ValueError('Matrix is singular or nearly singular')\n        Ab[i] /= Ab[i, i]\n        for j in range(i + 1, n):\n            factor = Ab[j, i]\n            Ab[j] -= factor * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = Ab[i, -1]\n        for j in range(i + 1, n):\n            x[i] -= Ab[i, j] * x[j]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]], dtype=float)\nb = np.array([1, -2, 0], dtype=float)"}
{"task_id": 58, "completion_id": 1, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    return x.round(4).tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\nb = np.array([1, -2, 0])"}
{"task_id": 58, "completion_id": 2, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        max_row = np.argmax(abs(Ab[i:n, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    return x.round(4).tolist()\nA = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]\nb = [8, -11, -3]"}
{"task_id": 58, "completion_id": 3, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(n, 1)))\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 58, "completion_id": 4, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(abs(Ab[i:n, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:n + 1] -= factor * Ab[i, i:n + 1]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, n] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\nb = np.array([1, -2, 0])"}
{"task_id": 58, "completion_id": 5, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    return x.round(4).tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 58, "completion_id": 6, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(A)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        pivot_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        pivot = Ab[i, i]\n        Ab[i] /= pivot\n        for j in range(i + 1, n):\n            factor = Ab[j, i]\n            Ab[j] -= factor * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 7, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    return x.round(4).tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 58, "completion_id": 8, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).reshape(-1, 1)\n    n = len(b)\n    Ab = np.hstack([A, b])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros((n, 1))\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 9, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3.0, 2.0, -1.0], [2.0, -2.0, 4.0], [-1.0, 0.5, -1.0]])\nb = np.array([1.0, -2.0, 0.0])"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        h_states = []\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)\n            concat = np.vstack((x_t, h_prev))\n            f_t = self._sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self._sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde_t = self._tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c_prev + i_t * c_tilde_t\n            o_t = self._sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * self._tanh(c_t)\n            h_prev = h_t\n            c_prev = c_t\n            h_states.append(h_t.flatten().tolist())\n        return (h_t.flatten().round(4).tolist(), c_t.flatten().round(4).tolist(), [hs_state.round(4).tolist() for hs_state in h_states])\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def _tanh(self, x):\n        return np.tanh(x)"}
{"task_id": 59, "completion_id": 1, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        x = np.array(x)\n        initial_hidden_state = np.array(initial_hidden_state).reshape(-1, 1)\n        initial_cell_state = np.array(initial_cell_state).reshape(-1, 1)\n        hidden_states = []\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        for t in range(x.shape[0]):\n            x_t = x[t].reshape(-1, 1)\n            combined = np.vstack((x_t, h_prev))\n            f_t = self._sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self._sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o_t = self._sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_next = f_t * c_prev + i_t * c_tilde\n            h_next = o_t * np.tanh(c_next)\n            hidden_states.append(h_next)\n            h_prev = h_next\n            c_prev = c_next\n        hidden_states_rounded = [np.round(h.flatten(), 4).tolist() for h in hidden_states]\n        final_hidden_state_rounded = np.round(h_next.flatten(), 4).tolist()\n        final_cell_state_rounded = np.round(c_next.flatten(), 4).tolist()\n        return (hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded)\n\n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))"}
{"task_id": 59, "completion_id": 2, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_prev = initial_hidden_state.reshape(-1, 1)\n        c_prev = initial_cell_state.reshape(-1, 1)\n        hidden_states = []\n        for input_t in x:\n            input_t = input_t.reshape(-1, 1)\n            combined = np.vstack((input_t, h_prev))\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_tilda_t = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            c_t = f_t * c_prev + i_t * c_tilda_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t.flatten().tolist())\n            h_prev = h_t\n            c_prev = c_t\n        return [np.round(np.array(hidden_states).tolist(), 4), np.round(h_t.flatten().tolist(), 4), np.round(c_t.flatten().tolist(), 4)]\n\n    @staticmethod\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\ninput_size = 10\nhidden_size = 20\nsequence_length = 5\nx = np.random.randn(sequence_length, input_size)"}
{"task_id": 59, "completion_id": 3, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_prev = initial_hidden_state.reshape(-1, 1)\n        c_prev = initial_cell_state.reshape(-1, 1)\n        h_states = []\n        for t in range(len(x)):\n            x_t = x[t].reshape(-1, 1)\n            concat = np.concatenate((x_t, h_prev), axis=0)\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_t_hat = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c_prev + i_t * c_t_hat\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            h_states.append(h_t.flatten().tolist())\n            h_prev = h_t\n            c_prev = c_t\n        return [np.array(h_states).round(4).tolist(), h_prev.flatten().round(4).tolist(), c_prev.flatten().round(4).tolist()]\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 4, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        h = initial_hidden_state\n        c = initial_cell_state\n        for x_t in x:\n            concat = np.vstack((x_t, h))\n            f_t = self._sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self._sigmoid(np.dot(self.Wi, concat) + self.bi)\n            o_t = self._sigmoid(np.dot(self.Wo, concat) + self.bo)\n            c_tilde_t = self._tanh(np.dot(self.Wc, concat) + self.bc)\n            c = f_t * c + i_t * c_tilde_t\n            h = o_t * self._tanh(c)\n            hidden_states.append(h)\n        hidden_states_rounded = [h.round(4) for h in hidden_states]\n        final_hidden_state_rounded = h.round(4)\n        final_cell_state_rounded = c.round(4)\n        hidden_states_list = [h.tolist() for h in hidden_states_rounded]\n        final_hidden_state_list = final_hidden_state_rounded.tolist()\n        final_cell_state_list = final_cell_state_rounded.tolist()\n        return (hidden_states_list, final_hidden_state_list, final_cell_state_list)\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def _tanh(self, x):\n        return np.tanh(x)"}
{"task_id": 59, "completion_id": 5, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        for xt in x:\n            xt = xt.reshape(-1, 1)\n            combined = np.vstack((h_prev, xt))\n            ft = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            ct = self.tanh(np.dot(self.Wc, combined) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_current = ft * c_prev + it * ct\n            h_current = ot * self.tanh(c_current)\n            hidden_states.append(h_current)\n            h_prev = h_current\n            c_prev = c_current\n        hidden_states = [hs.round(4).tolist() for hs in hidden_states]\n        final_hidden_state = h_prev.round(4).tolist()\n        final_cell_state = c_prev.round(4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)"}
{"task_id": 59, "completion_id": 6, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(T):\n            x_t = x[t]\n            x_t = x_t.reshape(-1, 1)\n            combined = np.vstack((x_t, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_hat_t = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_t = f_t * c_t + i_t * c_hat_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t.flatten())\n        return ([np.round(hs, 4).tolist() for hs in hidden_states], np.round(h_t.flatten(), 4).tolist(), np.round(c_t.flatten(), 4).tolist())\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 7, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        x = np.array(x)\n        sequence_length = x.shape[0]\n        h_prev = initial_hidden_state.reshape((self.hidden_size, 1))\n        c_prev = initial_cell_state.reshape((self.hidden_size, 1))\n        h_states = []\n        for t in range(sequence_length):\n            x_t = x[t].reshape((self.input_size, 1))\n            concat = np.vstack((x_t, h_prev))\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilda = self.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_curr = f_t * c_prev + i_t * c_tilda\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_curr = o_t * self.tanh(c_curr)\n            h_states.append(h_curr.T.tolist()[0])\n            (h_prev, c_prev) = (h_curr, c_curr)\n        return (h_states, h_prev.T.tolist()[0], c_prev.T.tolist()[0])"}
{"task_id": 59, "completion_id": 8, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_states = []\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state"}
{"task_id": 59, "completion_id": 9, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[0]\n        batch_size = x.shape[1]\n        hidden_states = np.zeros((sequence_length, batch_size, self.hidden_size))\n        cell_states = np.zeros((sequence_length, batch_size, self.hidden_size))\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        for t in range(sequence_length):\n            input_concat = np.vstack((x[t].T, h_prev))\n            f_t = self.sigmoid(np.dot(self.Wf, input_concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, input_concat) + self.bi)\n            c_hat_t = np.tanh(np.dot(self.Wc, input_concat) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, input_concat) + self.bo)\n            c_t = f_t * c_prev + i_t * c_hat_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_states[t] = h_t.T\n            cell_states[t] = c_t.T\n            h_prev = h_t\n            c_prev = c_t\n        hidden_states_rounded = np.round(hidden_states, 4).tolist()\n        final_hidden_state_rounded = np.round(h_t.T, 4).tolist()\n        final_cell_state_rounded = np.round(c_t.T, 4).tolist()\n        return (hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded)\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty.')\n    term_freqs = []\n    for doc in corpus:\n        doc_dict = {}\n        for word in doc:\n            if word in doc_dict:\n                doc_dict[word] += 1\n            else:\n                doc_dict[word] = 1\n        term_freqs.append(doc_dict)\n    doc_freqs = {}\n    for doc in term_freqs:\n        for word in doc:\n            if word in doc_freqs:\n                doc_freqs[word] += 1\n            else:\n                doc_freqs[word] = 1\n    N = len(corpus)\n    tf_idf_scores = []\n    for doc_dict in term_freqs:\n        doc_scores = []\n        for word in query:\n            tf = doc_dict.get(word, 0) / max(doc_dict.values(), 1)\n            df = doc_freqs.get(word, 0)\n            idf = np.log((N + 1) / (df + 1)) + 1\n            tf_idf = tf * idf\n            doc_scores.append(tf_idf)\n        tf_idf_scores.append(doc_scores)\n    tf_idf_scores_rounded = np.round(tf_idf_scores, 4).tolist()\n    return tf_idf_scores_rounded"}
{"task_id": 60, "completion_id": 1, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    num_docs = len(corpus)\n    unique_words = set((word for doc in corpus for word in doc))\n    tf_idf_scores = np.zeros((num_docs, len(query)))\n    df = {word: sum((1 for doc in corpus if word in doc)) for word in query}\n    for (doc_idx, doc) in enumerate(corpus):\n        tf = {word: doc.count(word) / len(doc) if doc else 0 for word in unique_words}\n        for (query_idx, word) in enumerate(query):\n            smoothed_idf = np.log((num_docs + 1) / (df.get(word, 0) + 1)) + 1\n            tf_idf_scores[doc_idx, query_idx] = tf[word] * smoothed_idf\n    return tf_idf_scores.round(4).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'lazy', 'dog'], ['the', 'quick', 'brown', 'dog']]\nquery = ['the', 'quick', 'lazy', 'cat']"}
{"task_id": 60, "completion_id": 2, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide a non-empty list of documents.')\n    num_docs = len(corpus)\n    unique_terms = set((word for doc in corpus for word in doc))\n    df = {term: sum((1 for doc in corpus if term in doc)) for term in unique_terms}\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_tf = {term: doc.count(term) for term in set(doc)}\n        doc_tf_idf = []\n        for term in query:\n            if term in doc_tf:\n                idf = np.log((num_docs + 1) / (df.get(term, 0) + 1))\n                tf_idf = doc_tf[term] * idf\n            else:\n                tf_idf = 0.0\n            doc_tf_idf.append(tf_idf)\n        tf_idf_scores.append(doc_tf_idf)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 3, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide a list of documents.')\n    if not query:\n        raise ValueError('Query is empty. Please provide a list of words for the query.')\n    documents = [' '.join(doc) for doc in corpus]\n    unique_words = set((word for doc in corpus for word in doc))\n    num_docs = len(corpus)\n    tf = {}\n    for (doc_index, doc) in enumerate(corpus):\n        word_count = len(doc)\n        tf[doc_index] = {}\n        for word in doc:\n            if word not in tf[doc_index]:\n                tf[doc_index][word] = 0\n            tf[doc_index][word] += 1 / word_count\n    idf = {}\n    for word in unique_words:\n        df = sum((1 for doc in corpus if word in doc))\n        idf[word] = np.log((num_docs + 1) / (df + 1)) + 1\n    tf_idf_scores = []\n    for doc_index in range(num_docs):\n        doc_scores = []\n        for word in query:\n            tf_value = tf[doc_index].get(word, 0)\n            idf_value = idf.get(word, 0)\n            tf_idf_score = tf_value * idf_value\n            doc_scores.append(tf_idf_score)\n        tf_idf_scores.append(doc_scores)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores\ncorpus = [['the', 'dog', 'chased', 'the', 'cat'], ['the', 'cat', 'sat', 'on', 'the', 'mat'], ['a', 'dog', 'and', 'a', 'cat', 'are', 'friends']]\nquery = ['dog', 'cat']"}
{"task_id": 60, "completion_id": 4, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    all_words = set((word for doc in corpus for word in doc))\n    N = len(corpus)\n    tf = {}\n    df = {word: 0 for word in all_words}\n    for doc in corpus:\n        doc_words = set(doc)\n        for word in doc:\n            if word not in tf:\n                tf[word] = {}\n            if doc not in tf[word]:\n                tf[word][doc] = 0\n            tf[word][doc] += 1\n        for word in doc_words:\n            df[word] += 1\n    idf = {word: np.log((N + 1) / (df[word] + 1)) + 1 for word in all_words}\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_tf_idf = []\n        for word in query:\n            if word in tf and doc in tf[word]:\n                term_tf = tf[word][doc] / len(doc) if doc else 0\n                term_tfidf = term_tf * idf[word]\n            else:\n                term_tfidf = 0\n            doc_tf_idf.append(term_tfidf)\n        tf_idf_scores.append(doc_tf_idf)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 5, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n\n    def term_frequency(term, document):\n        return document.count(term) / len(document) if document else 0\n\n    def inverse_document_frequency(term, corpus):\n        df = sum((1 for document in corpus if term in document))\n        return np.log((len(corpus) + 1) / (df + 1)) + 1\n    tf_idf_scores = []\n    for document in corpus:\n        doc_scores = []\n        for term in query:\n            tf = term_frequency(term, document)\n            idf = inverse_document_frequency(term, corpus)\n            tf_idf = tf * idf\n            doc_scores.append(tf_idf)\n        tf_idf_scores.append(doc_scores)\n    tf_idf_scores_rounded = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores_rounded"}
{"task_id": 60, "completion_id": 6, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide a list of documents.')\n    num_docs = len(corpus)\n    query_terms = set(query)\n    tf_matrix = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        word_freq = {}\n        for word in doc:\n            if word in query_terms:\n                word_freq[word] = word_freq.get(word, 0) + 1\n        tf_scores = {word: freq / doc_word_count for (word, freq) in word_freq.items()}\n        tf_matrix.append(tf_scores)\n    df = {word: 0 for word in query_terms}\n    for doc in corpus:\n        doc_terms = set(doc)\n        for word in query_terms:\n            if word in doc_terms:\n                df[word] += 1\n    idf = {word: np.log((num_docs + 1) / (df[word] + 1)) for word in query_terms}\n    tf_idf_scores = []\n    for tf_scores in tf_matrix:\n        doc_tf_idf = []\n        for word in query:\n            tf = tf_scores.get(word, 0)\n            tf_idf = tf * idf[word]\n            doc_tf_idf.append(tf_idf)\n        tf_idf_scores.append(doc_tf_idf)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 7, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide at least one document.')\n    document_sets = [set(doc) for doc in corpus]\n    tf = []\n    for doc in corpus:\n        doc_tf = {}\n        doc_length = len(doc)\n        for word in set(doc):\n            doc_tf[word] = doc.count(word) / doc_length\n        tf.append(doc_tf)\n    df = {}\n    total_docs = len(corpus)\n    for doc_set in document_sets:\n        for word in doc_set:\n            if word in df:\n                df[word] += 1\n            else:\n                df[word] = 1\n    idf = {}\n    for word in df:\n        idf[word] = np.log((total_docs + 1) / (df[word] + 1)) + 1\n    tf_idf_scores = []\n    for doc_tf in tf:\n        scores = []\n        for word in query:\n            tf_value = doc_tf.get(word, 0)\n            idf_value = idf.get(word, 0)\n            tf_idf = tf_value * idf_value\n            scores.append(tf_idf)\n        tf_idf_scores.append(scores)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 8, "solution": "import numpy as np\nfrom collections import defaultdict\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    num_docs = len(corpus)\n    tf_dicts = []\n    for doc in corpus:\n        tf_dict = defaultdict(int)\n        for word in doc:\n            tf_dict[word] += 1\n        tf_dicts.append(tf_dict)\n    df_dict = defaultdict(int)\n    for doc in corpus:\n        for word in set(doc):\n            df_dict[word] += 1\n    idf_dict = {}\n    for (word, df) in df_dict.items():\n        idf_dict[word] = np.log((num_docs + 1) / (df + 1)) + 1\n    tf_idf_scores = []\n    for tf_dict in tf_dicts:\n        doc_scores = []\n        for term in query:\n            tf = tf_dict[term] / len(corpus) if corpus else 0\n            idf = idf_dict.get(term, 0)\n            tf_idf = tf * idf\n            doc_scores.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_scores)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['hello', 'world', 'example'], ['hello', 'example', 'example'], ['world', 'world']]\nquery = ['hello', 'world', 'example']"}
{"task_id": 60, "completion_id": 9, "solution": "import numpy as np\nfrom collections import defaultdict\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide a non-empty list of documents.')\n    num_docs = len(corpus)\n    doc_tfs = []\n    for doc in corpus:\n        word_count = defaultdict(int)\n        for word in doc:\n            word_count[word] += 1\n        total_words = len(doc)\n        doc_tf = {word: count / total_words for (word, count) in word_count.items()}\n        doc_tfs.append(doc_tf)\n    doc_freq = defaultdict(int)\n    for doc in corpus:\n        for word in set(doc):\n            doc_freq[word] += 1\n    idf = {word: np.log((num_docs + 1) / (freq + 1)) for (word, freq) in doc_freq.items()}\n    tf_idf_scores = []\n    for doc_tf in doc_tfs:\n        scores = []\n        for word in query:\n            tf = doc_tf.get(word, 0.0)\n            scores.append(tf * idf.get(word, 0.0))\n        tf_idf_scores.append(scores)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores\ncorpus = [['word1', 'word2', 'word3', 'word1'], ['word2', 'word3', 'word4'], ['word1', 'word4', 'word4', 'word4']]\nquery = ['word1', 'word2', 'word5']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 0, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 1, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    if precision + recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 2, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 61, "completion_id": 3, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 4, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 61, "completion_id": 5, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 61, "completion_id": 6, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 61, "completion_id": 7, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\ny_true = np.array([0, 1, 0, 1, 1, 0, 1, 0, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 0, 0, 1, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 8, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 9, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if precision + recall > 0 else 0\n    return round(f_score_value, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = []\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for x in input_sequence:\n            x = x.reshape(-1, 1)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            hidden_states.append(h)\n            outputs.append(y)\n            h_prev = h\n        return (outputs, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t]\n            db_y += dy\n            dW_hy += np.dot(dy, hidden_states[t].T)\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - hidden_states[t] * hidden_states[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].reshape(1, -1))\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T) if t > 0 else 0\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate, epochs):\n        \"\"\"\n        Trains the RNN using BPTT for a specified number of epochs.\n        \"\"\"\n        for epoch in range(epochs):\n            (outputs, hidden_states) = self.forward(input_sequence)\n            self.backward(input_sequence, expected_output, outputs, hidden_states, learning_rate)"}
{"task_id": 62, "completion_id": 1, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        :param input_sequence: A list of input vectors, each of shape (input_size, 1)\n        :return: outputs - list of output vectors, each of shape (output_size, 1),\n                 last_inputs - list of Input vectors,\n                 last_hiddens - list of hidden states\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_hiddens = []\n        last_inputs = []\n        for x in input_sequence:\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_hiddens.append(h)\n            last_inputs.append(x)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) for the RNN.\n        \n        :param input_sequence: A list of input vectors, each of shape (input_size, 1)\n        :param expected_output: A list of expected output vectors, each of shape (output_size, 1)\n        :param outputs: List of actual output vectors from the forward pass\n        :param last_inputs: List of input vectors from the forward pass\n        :param last_hiddens: List of hidden states from the forward pass\n        :param learning_rate: Learning rate for weight updates\n        \"\"\"\n        (dW_xh, dW_hh, dW_hy) = (np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy))\n        (db_h, db_y) = (np.zeros_like(self.b_h), np.zeros_like(self.b_y))\n        dh_next = np.zeros_like(last_hiddens[0])\n        T = len(input_sequence)\n        for t in reversed(range(T)):\n            dy = outputs[t] - expected_output[t]\n            db_y += dy\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else 0\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, epochs, learning_rate):\n        \"\"\"\n        Trains the RNN on the given input sequence and expected output.\n        \n        :param input_sequence: A list of input vectors, each of shape (input_size, 1)\n        :param expected_output: A list of expected output vectors, each of shape (output_size, 1)\n        :param epochs: Number of training iterations\n        :param learning_rate: Learning rate for weight updates\n        \"\"\"\n        for epoch in range(epochs):\n            (outputs, last_inputs, last_hiddens) = self.forward(input_sequence)\n            loss = 0\n            for t in range(len(input_sequence)):\n                loss += 0.5 * np.sum((outputs[t] - expected_output[t]) ** 2)\n            loss /= len(input_sequence)\n            self.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n            if epoch % 1000 == 0:\n                print(f'Epoch {epoch}, Loss: {loss}')"}
{"task_id": 62, "completion_id": 2, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x in input_sequence:\n            x = x.reshape(-1, 1)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        loss = 0\n        for (y, y_hat) in zip(expected_output, outputs):\n            loss += 0.5 * np.sum((y_hat - y) ** 2)\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t].reshape(-1, 1)\n            db_y += dy\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else 0\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\ninput_sequence = [np.array([0, 1, 0]), np.array([1, 0, 0])]\nexpected_output = [np.array([0, 0, 1]), np.array([1, 0, 0])]\nloss = rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01)"}
{"task_id": 62, "completion_id": 3, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = []\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for x in input_sequence:\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            hidden_states.append(h)\n            outputs.append(y)\n            h_prev = h\n        return (outputs, input_sequence, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        T = len(input_sequence)\n        loss = 0\n        for t in reversed(range(T)):\n            dy = outputs[t] - expected_output[t]\n            loss += 0.5 * np.sum(dy * dy)\n            dW_hy += np.dot(dy, hidden_states[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - hidden_states[t] * hidden_states[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].T)\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 62, "completion_id": 4, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        hidden_states = []\n        for t in range(len(x)):\n            h_prev = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h_prev) + self.b_h)\n            output = np.dot(self.W_hy, h_prev) + self.b_y\n            outputs.append(output)\n            last_inputs.append(x[t])\n            hidden_states.append(h_prev)\n        return (outputs, last_inputs, hidden_states)\n\n    def backward(self, x, y_true, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        (dW_xh, dW_hh, dW_hy) = (np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy))\n        (db_h, db_y) = (np.zeros_like(self.b_h), np.zeros_like(self.b_y))\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(len(x))):\n            dy = outputs[t] - y_true[t]\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else np.zeros_like(self.W_hh)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for (param, dparam) in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n            param -= learning_rate * dparam\n\n    def compute_loss(self, outputs, y_true):\n        \"\"\"\n        Computes the 1/2 * Mean Squared Error (MSE) loss for the outputs.\n        \"\"\"\n        loss = 0.0\n        for (y_pred, y) in zip(outputs, y_true):\n            loss += 0.5 * np.sum((y_pred - y) ** 2)\n        return loss"}
{"task_id": 62, "completion_id": 5, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns the outputs, last inputs, and last hidden states.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x in input_sequence:\n            x = x.reshape(-1, 1)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h_prev)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t].reshape(-1, 1)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else np.zeros_like(self.W_hh)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def compute_loss(self, outputs, expected_output):\n        \"\"\"\n        Computes the 1/2 * Mean Squared Error (MSE) loss.\n        \"\"\"\n        loss = 0\n        for t in range(len(outputs)):\n            loss += 0.5 * np.sum((outputs[t] - expected_output[t].reshape(-1, 1)) ** 2)\n        return loss"}
{"task_id": 62, "completion_id": 6, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        x: A list of input vectors, each of shape (input_size, 1)\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for t in range(len(x)):\n            h_t = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n            outputs.append(y_t)\n            last_inputs.append(x[t])\n            last_hiddens.append(h_t)\n            h_prev = h_t\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(len(outputs))):\n            dy = outputs[t] - expected_output[t]\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else 0\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate, epochs):\n        for epoch in range(epochs):\n            (outputs, last_inputs, last_hiddens) = self.forward(input_sequence)\n            self.backward(expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n            loss = 0.5 * np.sum((np.array(outputs) - np.array(expected_output)) ** 2)\n            if epoch % 100 == 0:\n                print(f'Epoch {epoch}, Loss: {loss}')"}
{"task_id": 62, "completion_id": 7, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = []\n        current_hidden_state = np.zeros((self.hidden_size, 1))\n        outputs = []\n        for input_data in input_sequence:\n            input_data = np.array(input_data).reshape(-1, 1)\n            current_hidden_state = np.tanh(np.dot(self.W_xh, input_data) + np.dot(self.W_hh, current_hidden_state) + self.b_h)\n            output = np.dot(self.W_hy, current_hidden_state) + self.b_y\n            outputs.append(output)\n            hidden_states.append(current_hidden_state)\n        return (outputs, input_sequence, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backpropagation through time to adjust weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        for t in reversed(range(len(input_sequence))):\n            input_data = np.array(input_sequence[t]).reshape(-1, 1)\n            dh = np.dot(self.W_hy.T, outputs[t] - expected_output[t]) + dh_next\n            dh_raw = (1 - hidden_states[t] ** 2) * dh\n            db_h += dh_raw\n            db_y += outputs[t] - expected_output[t]\n            dW_hy += np.dot(outputs[t] - expected_output[t], hidden_states[t].T)\n            dW_xh += np.dot(dh_raw, input_data.T)\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def mse_loss(self, outputs, expected_output):\n        \"\"\"\n        Computes the Mean Squared Error loss for the outputs.\n        \"\"\"\n        return 0.5 * np.sum((np.array(outputs) - np.array(expected_output)) ** 2)\ninput_size = 2\nhidden_size = 4\noutput_size = 2\ninput_sequence = [[1, 0], [0, 1], [1, 1]]\nexpected_output = [[0, 1], [1, 0], [0, 0]]\nlearning_rate = 0.01"}
{"task_id": 62, "completion_id": 8, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = []\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for x in input_sequence:\n            x = x.reshape(-1, 1)\n            h_next = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h_next) + self.b_y\n            hidden_states.append(h_next)\n            outputs.append(y)\n            h_prev = h_next\n        return (outputs, input_sequence, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t].reshape(-1, 1)\n            dW_hy += np.dot(dy, hidden_states[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - hidden_states[t] * hidden_states[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].reshape(1, -1))\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 62, "completion_id": 9, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = []\n        outputs = []\n        h_prev = np.zeros((self.hidden_size, 1))\n        for x in input_sequence:\n            h_prev = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h_prev) + self.b_y\n            hidden_states.append(h_prev)\n            outputs.append(y)\n        return (outputs, input_sequence, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backpropagation through time for the RNN.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        loss = 0.0\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t]\n            loss += 0.5 * np.sum(dy * dy)\n            dW_hy += np.dot(dy, hidden_states[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - hidden_states[t] * hidden_states[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].T)\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T) if t > 0 else 0\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        dW_xh = np.clip(dW_xh, -5, 5)\n        dW_hh = np.clip(dW_hh, -5, 5)\n        dW_hy = np.clip(dW_hy, -5, 5)\n        db_h = np.clip(db_h, -5, 5)\n        db_y = np.clip(db_y, -5, 5)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - A @ x\n    p = r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 1, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    r_norm = np.linalg.norm(r)\n    for _ in range(n):\n        if r_norm < tol:\n            break\n        Ap = A @ p\n        alpha = r_norm ** 2 / (p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        r_new_norm = np.linalg.norm(r)\n        beta = r_new_norm ** 2 / r_norm ** 2\n        p = r + beta * p\n        r_norm = r_new_norm\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 2, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.tolist()"}
{"task_id": 63, "completion_id": 3, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = r.T @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / (p.T @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r.T @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 4, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - A @ x\n    p = r\n    rsold = r.T @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / (p.T @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r.T @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.tolist()"}
{"task_id": 63, "completion_id": 5, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - A @ x\n    p = r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.tolist()"}
{"task_id": 63, "completion_id": 6, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = r.T @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / (p.T @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r.T @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 7, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 8, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    r_k_norm_squared = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = r_k_norm_squared / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_k_plus_1_norm_squared = np.dot(r, r)\n        beta = r_k_plus_1_norm_squared / r_k_norm_squared\n        p = r + beta * p\n        r_k_norm_squared = r_k_plus_1_norm_squared\n        if np.sqrt(r_k_norm_squared) < tol:\n            break\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 9, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_samples = len(y)\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / n_samples\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = Counter(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)\ny = [1, 1, 2, 2, 2, 3, 3, 3, 3]"}
{"task_id": 64, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = Counter(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 3, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = np.bincount(y)\n    total_samples = len(y)\n    gini = 1.0 - np.sum((class_counts / total_samples) ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = Counter(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 5, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_samples = len(y)\n    (unique_labels, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / n_samples\n    gini = 1.0 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = Counter(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 7, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)\ny = [1, 1, 2, 2, 2, 3]"}
{"task_id": 64, "completion_id": 8, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = np.bincount(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts:\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 9, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_samples = len(y)\n    class_counts = np.unique(y, return_counts=True)[1]\n    probabilities = class_counts / n_samples\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = []\n    current_row_start = 0\n    for row in dense_matrix:\n        for (col, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col)\n        row_pointers.append(len(values))\n        current_row_start = len(values)\n    return (values, column_indices, row_pointers)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 0, 0, 0], [1, 2, 0, 0]]"}
{"task_id": 65, "completion_id": 1, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        current_row_non_zeros = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_row_non_zeros += 1\n        row_pointer.append(row_pointer[-1] + current_row_non_zeros)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 7], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 2, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        current_nnz = 0\n        for (col, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col)\n                current_nnz += 1\n        row_pointer.append(row_pointer[-1] + current_nnz)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 0, 0, 0], [6, 7, 0, 8]]"}
{"task_id": 65, "completion_id": 3, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        current_nnz = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_nnz += 1\n        row_pointer.append(row_pointer[-1] + current_nnz)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0]]"}
{"task_id": 65, "completion_id": 4, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                non_zero_count += 1\n        row_pointer.append(row_pointer[-1] + non_zero_count)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 0], [5, 6, 0, 7]]"}
{"task_id": 65, "completion_id": 5, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        initial_len = len(values)\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        row_pointer.append(len(values) - initial_len + row_pointer[-1])\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0], [0, 0, 0, 7]]"}
{"task_id": 65, "completion_id": 6, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        current_row_non_zero_count = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_row_non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + current_row_non_zero_count)\n    return (values, column_indices, row_pointers)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 5, 0, 6], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 7, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        current_non_zeros = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_non_zeros += 1\n        row_pointers.append(row_pointers[-1] + current_non_zeros)\n    return (values, column_indices, row_pointers)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 8, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        nnz_in_row = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                nnz_in_row += 1\n        row_pointers.append(row_pointers[-1] + nnz_in_row)\n    return (values, column_indices, row_pointers)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 7], [0, 0, 8, 0]]"}
{"task_id": 65, "completion_id": 9, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                non_zero_count += 1\n        row_pointer.append(row_pointer[-1] + non_zero_count)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 5], [0, 6, 0, 7]]"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection = scalar_projection * L\n    return [round(coord, 3) for coord in projection]"}
{"task_id": 66, "completion_id": 1, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection_vector = scalar_projection * L\n    return projection_vector.round(3).tolist()"}
{"task_id": 66, "completion_id": 2, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_np = np.array(v)\n    L_np = np.array(L)\n    dot_product_vL = np.dot(v_np, L_np)\n    dot_product_LL = np.dot(L_np, L_np)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection_vector = scalar_projection * L_np\n    return projection_vector.round(3).tolist()\nv = [3, 4]\nL = [1, 1]"}
{"task_id": 66, "completion_id": 3, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v, dtype=float)\n    L = np.array(L, dtype=float)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection = scalar_projection * L\n    return [round(x, 3) for x in projection.tolist()]\nv = [3, 4]\nL = [1, 0]"}
{"task_id": 66, "completion_id": 4, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_np = np.array(v)\n    L_np = np.array(L)\n    dot_product = np.dot(v_np, L_np)\n    norm_L_squared = np.dot(L_np, L_np)\n    scalar_projection = dot_product / norm_L_squared\n    projection_vector = scalar_projection * L_np\n    return projection_vector.round(3).tolist()\nv = [3, 4]\nL = [1, 0]"}
{"task_id": 66, "completion_id": 5, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_np = np.array(v)\n    L_np = np.array(L)\n    dot_product = np.dot(v_np, L_np)\n    dot_product_L_L = np.dot(L_np, L_np)\n    scalar_projection = dot_product / dot_product_L_L\n    projection_vector = scalar_projection * L_np\n    projection_vector_rounded = [round(x, 3) for x in projection_vector]\n    return projection_vector_rounded"}
{"task_id": 66, "completion_id": 6, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    dot_product_L_L = np.dot(L, L)\n    scalar_projection = dot_product / dot_product_L_L\n    projection = scalar_projection * L\n    return np.round(projection, 3).tolist()"}
{"task_id": 66, "completion_id": 7, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    dot_product_L_L = np.dot(L, L)\n    scalar_projection = dot_product / dot_product_L_L\n    projection = scalar_projection * L\n    return projection.round(3).tolist()"}
{"task_id": 66, "completion_id": 8, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_np = np.array(v)\n    L_np = np.array(L)\n    dot_product = np.dot(v_np, L_np)\n    dot_L_L = np.dot(L_np, L_np)\n    scalar_projection = dot_product / dot_L_L\n    projection_vector = scalar_projection * L_np\n    return [round(x, 3) for x in projection_vector]\nv = [3, 4]\nL = [1, 2]"}
{"task_id": 66, "completion_id": 9, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    dot_product_L_L = np.dot(L, L)\n    scalar_projection = dot_product / dot_product_L_L\n    projection_vector = scalar_projection * L\n    return projection_vector.round(3).tolist()\nv = [3, 4]\nL = [1, 0]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3], [4, 0, 0], [0, 5, 0], [6, 0, 0]]"}
{"task_id": 67, "completion_id": 1, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_pointer[col + 1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 0, 0], [5, 8, 0, 0], [0, 0, 3, 0], [0, 6, 0, 9]]"}
{"task_id": 67, "completion_id": 2, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0], [7, 0, 0, 8]]"}
{"task_id": 67, "completion_id": 3, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_pointer[col + 1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 4, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3], [4, 0, 0], [0, 5, 0]]"}
{"task_id": 67, "completion_id": 5, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(len(dense_matrix[0])):\n        column_start = len(values)\n        for row in range(len(dense_matrix)):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(column_start + len(values) - column_start)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 5, 0, 6], [0, 0, 0, 0]]"}
{"task_id": 67, "completion_id": 6, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_pointer[col + 1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 7, 0, 0]]"}
{"task_id": 67, "completion_id": 7, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 5, 6], [0, 7, 0, 0]]"}
{"task_id": 67, "completion_id": 8, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 9, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_pointer[col + 1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 0, 0, 5], [0, 6, 0, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (Q, R) = np.linalg.qr(A, mode='reduced')\n    pivot_columns = []\n    tol = 1e-10\n    num_cols = R.shape[1]\n    row_index = 0\n    for col_index in range(num_cols):\n        if row_index < R.shape[0] and abs(R[row_index, col_index]) > tol:\n            pivot_columns.append(col_index)\n            row_index += 1\n    basis_vectors = A[:, pivot_columns]\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    return basis_vectors_rounded\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 68, "completion_id": 1, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (q, r, p) = np.linalg.qr(A.T, pivoting=True)\n    rank = np.sum(np.abs(np.diag(r)) > 1e-10)\n    pivot_indices = p[:rank]\n    basis_vectors = A[:, pivot_indices]\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    return basis_vectors_rounded"}
{"task_id": 68, "completion_id": 2, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A)\n    (_, pivots) = np.linalg.qr(A.T, mode='r')\n    pivot_columns = pivots >= 1e-08\n    basis_vectors = A[:, pivot_columns]\n    return basis_vectors.round(8).tolist()"}
{"task_id": 68, "completion_id": 3, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, _, vh) = np.linalg.svd(A)\n    rank = np.linalg.matrix_rank(A)\n    independent_columns = vh.T[:, :rank]\n    (_, pivot_columns) = np.linalg.qr(A, mode='reduced')\n    basis_vectors = A[:, pivot_columns[:rank]]\n    basis_vectors = np.round(basis_vectors, 8)\n    return basis_vectors.tolist()\nA = [[1, 2, 1, 3], [2, 4, 2, 6], [1, 2, 0, 4]]"}
{"task_id": 68, "completion_id": 4, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A)\n    (_, pivots) = np.linalg.qr(A, mode='r', pivoting=True)\n    basis_vectors = A[:, pivots[:np.sum(pivots >= 0)]]\n    basis_vectors = np.round(basis_vectors, 8)\n    return basis_vectors.tolist()"}
{"task_id": 68, "completion_id": 5, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    for i in range(min(m, n)):\n        if A[i, i] == 0:\n            for k in range(i + 1, m):\n                if A[k, i] != 0:\n                    A[[i, k]] = A[[k, i]]\n                    break\n        if A[i, i] != 0:\n            A[i] = A[i] / A[i, i]\n        for k in range(i + 1, m):\n            factor = A[k, i]\n            A[k] -= factor * A[i]\n    pivot_columns = []\n    for i in range(n):\n        if np.any(A[:, i] == 1):\n            pivot_columns.append(i)\n    basis_vectors = A[:, pivot_columns].tolist()\n    basis_vectors = [[round(num, 8) for num in vector] for vector in basis_vectors]\n    return basis_vectors"}
{"task_id": 68, "completion_id": 6, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, pivots) = np.linalg.qr(A.T, mode='economic')\n    rank = len(pivots)\n    basis = A[:, pivots]\n    return basis.round(8).tolist()"}
{"task_id": 68, "completion_id": 7, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, _, Vt) = np.linalg.svd(A, full_matrices=False)\n    rank = np.sum(np.abs(Vt) > 1e-10)\n    basis = A[:, :rank]\n    pivot_columns = []\n    for i in range(rank):\n        pivot_index = np.argmax(np.abs(Vt[i]))\n        pivot_columns.append(pivot_index)\n    image_basis = A[:, pivot_columns]\n    return np.round(image_basis, 8).tolist()"}
{"task_id": 68, "completion_id": 8, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, pivots) = np.linalg.qr(A, mode='r')\n    rank = np.linalg.matrix_rank(A)\n    independent_columns = A[:, :rank]\n    basis_columns = A[:, pivots[:rank]]\n    return basis_columns.round(8).tolist()\nA = [[1, 2, 3], [2, 4, 6], [1, 0, 1]]"}
{"task_id": 68, "completion_id": 9, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (row_echelon, pivot_columns) = row_echelon_form(A)\n    basis_vectors = A[:, pivot_columns]\n    return basis_vectors.round(8).tolist()\ndef row_echelon_form(A):\n    A = A.astype(float)\n    (m, n) = A.shape\n    pivot_columns = []\n    row = 0\n    col = 0\n    while row < m and col < n:\n        if A[row, col] == 0:\n            swap_row = row\n            for r in range(row + 1, m):\n                if A[r, col] != 0:\n                    swap_row = r\n                    break\n            if swap_row != row:\n                A[[row, swap_row]] = A[[swap_row, row]]\n        if A[row, col] != 0:\n            pivot_columns.append(col)\n            pivot = A[row, col]\n            A[row] = A[row] / pivot\n            for r in range(row + 1, m):\n                factor = A[r, col]\n                A[r] = A[r] - factor * A[row]\n            row += 1\n        col += 1\n    return (A, pivot_columns)"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 1, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 2, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 3, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 4, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 5, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 6, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 7, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 8, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 9, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 1, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    row_length = len(img[0])\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 2, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list):\n        return -1\n    total_pixel_value = 0\n    pixel_count = 0\n    for row in img:\n        if not row or not isinstance(row, list):\n            return -1\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_pixel_value += pixel\n            pixel_count += 1\n    average_brightness = round(total_pixel_value / pixel_count, 2)\n    return average_brightness"}
{"task_id": 70, "completion_id": 3, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list):\n        return -1\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if not isinstance(row, list):\n            return -1\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_pixels += 1\n            total_brightness += pixel\n    average_brightness = round(total_brightness / total_pixels, 2) if total_pixels > 0 else -1\n    return average_brightness"}
{"task_id": 70, "completion_id": 4, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list) or (not all((isinstance(row, list) for row in img))):\n        return -1\n    row_length = len(img[0])\n    if not all((len(row) == row_length for row in img)):\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = round(total_brightness / total_pixels, 2)\n    return average_brightness"}
{"task_id": 70, "completion_id": 5, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_pixels = 0\n    sum_pixels = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            sum_pixels += pixel\n            total_pixels += 1\n    average_brightness = round(sum_pixels / total_pixels, 2)\n    return average_brightness"}
{"task_id": 70, "completion_id": 6, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    rows = len(img)\n    cols = len(img[0])\n    for row in img:\n        if len(row) != cols:\n            return -1\n    total_brightness = 0\n    pixel_count = 0\n    for i in range(rows):\n        for j in range(cols):\n            pixel_value = img[i][j]\n            if pixel_value < 0 or pixel_value > 255:\n                return -1\n            total_brightness += pixel_value\n            pixel_count += 1\n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 7, "solution": "def calculate_brightness(img):\n    if not img or not all((row for row in img)):\n        return -1\n    row_length = len(img[0])\n    if any((len(row) != row_length for row in img)):\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 8, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_pixels = 0\n    total_brightness = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 9, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list) or (not all((isinstance(row, list) for row in img))):\n        return -1\n    total_pixel_value = 0\n    pixel_count = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_pixel_value += pixel\n            pixel_count += 1\n    average_brightness = round(total_pixel_value / pixel_count, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Arrays y_true and y_pred must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.shape[0]\n    rmse_value = np.sqrt(np.sum((y_true - y_pred) ** 2) / n)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 1, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    n = y_true.size\n    residuals = y_true - y_pred\n    rmse_value = np.sqrt(np.sum(residuals ** 2) / n)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 2, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Inputs must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    n = y_true.size\n    residuals = y_true - y_pred\n    mse = np.sum(residuals ** 2) / n\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 3, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('y_true and y_pred must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.shape[0]\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 4, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('y_true and y_pred must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.shape[0]\n    mse = np.sum((y_true - y_pred) ** 2) / n\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 5, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('y_true and y_pred must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 6, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    n = y_true.shape[0]\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 71, "completion_id": 7, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.shape[0]\n    residuals = y_true - y_pred\n    mse = np.sum(residuals ** 2) / n\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 8, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n    \n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n    except Exception as e:\n        raise ValueError('Invalid input types. Ensure y_true and y_pred are array-like.') from e\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    rmse_value = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 9, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Inputs must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Arrays must have the same shape.')\n    n = y_true.shape[0]\n    residuals = y_true - y_pred\n    rmse_value = np.sqrt(np.sum(residuals ** 2) / n)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 72, "completion_id": 1, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 2, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    if union == 0:\n        return 1.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 72, "completion_id": 3, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 72, "completion_id": 4, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true & y_pred)\n    union = np.sum(y_true | y_pred)\n    if union == 0:\n        return 0.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 72, "completion_id": 5, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    j_index = intersection / union\n    return round(j_index, 3)"}
{"task_id": 72, "completion_id": 6, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = [0, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 0, 1]"}
{"task_id": 72, "completion_id": 7, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 8, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 9, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    j_index = intersection / union\n    return round(j_index, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        return 0.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n        return round(dice, 3)"}
{"task_id": 73, "completion_id": 1, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    dice_score = 2 * intersection / (sum_true + sum_pred)\n    return round(dice_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 73, "completion_id": 2, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (array-like): True binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must be of the same shape')\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice_score = 2 * intersection / (sum_true + sum_pred)\n    return round(dice_score, 3)\ny_true = [1, 0, 1, 1, 0, 0]\ny_pred = [1, 1, 0, 1, 0, 1]"}
{"task_id": 73, "completion_id": 3, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 4, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        return 0.0\n    else:\n        dice = 2 * intersection / (sum_true + sum_pred)\n        return round(dice, 3)"}
{"task_id": 73, "completion_id": 5, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice_score_value = 2 * intersection / (sum_true + sum_pred)\n    return round(dice_score_value, 3)"}
{"task_id": 73, "completion_id": 6, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 7, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 73, "completion_id": 8, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    true_positives = np.sum(y_true)\n    predicted_positives = np.sum(y_pred)\n    if true_positives == 0 and predicted_positives == 0:\n        return 1.0\n    dice = 2 * intersection / (true_positives + predicted_positives)\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 9, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    sum_true = np.sum(y_true == 1)\n    sum_pred = np.sum(y_pred == 1)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_random_hv(seed, dim):\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n\n    def bind_hvs(hv1, hv2):\n        return np.multiply(hv1, hv2)\n    composite_hv = np.ones(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_hv = generate_random_hv(feature_seed, dim)\n        value_seed = hash(f'{feature}_{value}') % (2 ** 32 - 1)\n        value_hv = generate_random_hv(value_seed, dim)\n        bound_hv = bind_hvs(feature_hv, value_hv)\n        composite_hv = bind_hvs(composite_hv, bound_hv)\n    return composite_hv.tolist()\nrow = {'age': 30, 'gender': 'male', 'city': 'New York'}\ndim = 1000\nrandom_seeds = {'age': 100, 'gender': 200, 'city': 300}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 1, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        np.random.seed(seed)\n        return 2 * (np.random.rand(dim) > 0.5).astype(int) - 1\n\n    def bind_hypervectors(hv1, hv2):\n        return np.multiply(hv1, hv2)\n    composite_hv = np.ones(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_seed = random_seeds.get(feature_name, None)\n        if feature_seed is not None:\n            feature_hv = generate_hypervector(feature_seed, dim)\n            value_seed = hash(f'{feature_name}_{feature_value}') % (2 ** 32 - 1)\n            value_hv = generate_hypervector(value_seed, dim)\n            feature_value_hv = bind_hypervectors(feature_hv, value_hv)\n            composite_hv = bind_hypervectors(composite_hv, feature_value_hv)\n        else:\n            raise ValueError(f\"Seed for feature '{feature_name}' not provided in random_seeds.\")\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 2, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_random_hv(seed, dim):\n        np.random.seed(seed)\n        return (np.random.randint(0, 2, dim) * 2 - 1).astype(int)\n\n    def bind_hv(hv1, hv2):\n        return np.multiply(hv1, hv2)\n\n    def bundle_hv(hvs):\n        return np.sum(hvs, axis=0) > 0\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds.get(feature, None)\n        if feature_seed is None:\n            raise ValueError(f\"Seed for feature '{feature}' not provided in random_seeds.\")\n        feature_hv = generate_random_hv(feature_seed, dim)\n        value_seed = hash((feature_seed, value))\n        value_hv = generate_random_hv(value_seed, dim)\n        bound_hv = bind_hv(feature_hv, value_hv)\n        composite_hv += bound_hv\n    composite_hv = bundle_hv(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 3, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        \"\"\"Generate a random hypervector with the given seed and dimension.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds.get(feature, None)\n        if feature_seed is None:\n            raise ValueError(f'No seed provided for feature: {feature}')\n        feature_hv = generate_hypervector(feature_seed, dim)\n        value_seed = hash((feature_seed, value))\n        value_hv = generate_hypervector(value_seed, dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()\nrow = {'age': 25, 'height': 175, 'weight': 70}\ndim = 1000\nrandom_seeds = {'age': 42, 'height': 83, 'weight': 15}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 4, "solution": "import numpy as np\ndef create_hv(seed, dim):\n    \"\"\"Create a random hypervector using a given seed.\"\"\"\n    np.random.seed(seed)\n    return np.random.randint(0, 2, dim) * 2 - 1\ndef bind_hv(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n    return hv1 * hv2\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors using the majority vote mechanism.\"\"\"\n    return np.sign(np.sum(hvs, axis=0))\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n    feature_hvs = []\n    for (feature, value) in row.items():\n        if feature in random_seeds:\n            feature_name_seed = random_seeds[feature]\n            feature_name_hv = create_hv(feature_name_seed, dim)\n            feature_value_seed = hash(f'{feature}_{value}') ^ feature_name_seed\n            feature_value_hv = create_hv(feature_value_seed, dim)\n            bound_hv = bind_hv(feature_name_hv, feature_value_hv)\n            feature_hvs.append(bound_hv)\n    composite_hv = bundle_hvs(feature_hvs)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 5, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n    composite_hv = np.zeros(dim)\n    for (feature_name, value) in row.items():\n        feature_hv = generate_hypervector(random_seeds[feature_name], dim)\n        value_hv = generate_hypervector(hash((random_seeds[feature_name], value)) % (2 ** 32 - 1), dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()\nrow = {'age': 25, 'height': 175, 'weight': 70}\ndim = 1000\nrandom_seeds = {'age': 1, 'height': 2, 'weight': 3}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 6, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], dim)\n\n    def bind_hypervectors(hv1, hv2):\n        return hv1 * hv2\n    composite_hv = np.ones(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_hv = generate_hypervector(feature_seed, dim)\n        value_seed = hash(f'{feature}_{value}') % (2 ** 32 - 1)\n        value_hv = generate_hypervector(value_seed, dim)\n        feature_value_hv = bind_hypervectors(feature_hv, value_hv)\n        composite_hv = bind_hypervectors(composite_hv, feature_value_hv)\n    return composite_hv.tolist()\nrow = {'age': 25, 'height': 175, 'weight': 70}\ndim = 1000\nrandom_seeds = {'age': 123, 'height': 456, 'weight': 789}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 7, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        \"\"\"Generate a random hypervector using a given seed.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], dim)\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_name_seed = hash(feature_name) ^ random_seeds[feature_name]\n        feature_name_hv = generate_hypervector(feature_name_seed, dim)\n        feature_value_seed = hash(feature_value) ^ random_seeds[feature_name]\n        feature_value_hv = generate_hypervector(feature_value_seed, dim)\n        bound_hv = feature_name_hv * feature_value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 8, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def create_hypervector(seed, dim):\n        \"\"\"Create a random hypervector with a given seed and dimension.\"\"\"\n        rng = np.random.default_rng(seed)\n        return rng.choice([-1, 1], size=dim)\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_hv = create_hypervector(feature_seed, dim)\n        value_seed = hash(str(value)) ^ feature_seed\n        value_hv = create_hypervector(value_seed, dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()\nrow = {'age': 25, 'height': 175, 'weight': 70}\ndim = 1000\nrandom_seeds = {'age': 42, 'height': 84, 'weight': 126}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 9, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hv(seed, dim):\n        \"\"\"Generate a random hypervector with a given seed.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n\n    def bind_hv(hv1, hv2):\n        \"\"\"Bind two hypervectors using circular convolution.\"\"\"\n        return np.fft.ifft(np.fft.fft(hv1) * np.fft.fft(hv2)).real.astype(int)\n    composite_hv = np.ones(dim)\n    for (feature_name, value) in row.items():\n        feature_seed = random_seeds.get(feature_name, None)\n        if feature_seed is None:\n            raise ValueError(f\"Seed for feature '{feature_name}' not provided in random_seeds.\")\n        feature_hv = generate_hv(feature_seed, dim)\n        value_seed = hash((feature_seed, value))\n        value_hv = generate_hv(value_seed, dim)\n        feature_value_hv = bind_hv(feature_hv, value_hv)\n        composite_hv = bind_hv(composite_hv, feature_value_hv)\n    return composite_hv.tolist()\nrow = {'age': 25, 'height': 175, 'weight': 70}\ndim = 1000\nrandom_seeds = {'age': 123, 'height': 456, 'weight': 789}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    cm = [[true_negative, false_positive], [false_negative, true_positive]]\n    return cm\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [0, 1]]"}
{"task_id": 75, "completion_id": 1, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    cm = [[true_negative, false_positive], [false_negative, true_positive]]\n    return cm\ndata = [[1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 2, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    cm = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            cm[0][0] += 1\n        elif y_true == 1 and y_pred == 0:\n            cm[0][1] += 1\n        elif y_true == 0 and y_pred == 1:\n            cm[1][0] += 1\n        elif y_true == 0 and y_pred == 0:\n            cm[1][1] += 1\n    return cm\ndata = [[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 3, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    cm = [[true_negative, false_positive], [false_negative, true_positive]]\n    return cm\ndata = [[1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [0, 0], [1, 0]]"}
{"task_id": 75, "completion_id": 4, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    confusion_matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n    return confusion_matrix\ndata = [[1, 1], [0, 1], [1, 0], [0, 0], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 5, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    cm = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            cm[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            cm[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            cm[1][0] += 1\n        elif y_true == 1 and y_pred == 1:\n            cm[1][1] += 1\n    return cm\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1], [0, 1], [1, 0]]"}
{"task_id": 75, "completion_id": 6, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n    return [[tn, fp], [fn, tp]]\ndata = [[1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 7, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    tp = tn = fp = fn = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1:\n            if y_pred == 1:\n                tp += 1\n            else:\n                fn += 1\n        elif y_pred == 1:\n            fp += 1\n        else:\n            tn += 1\n    return [[tn, fp], [fn, tp]]\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 8, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    counts = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    tp = counts[1, 1]\n    tn = counts[0, 0]\n    fp = counts[0, 1]\n    fn = counts[1, 0]\n    cm = [[tn, fp], [fn, tp]]\n    return cm\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [0, 1]]"}
{"task_id": 75, "completion_id": 9, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positives += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negatives += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positives += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negatives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    return confusion_matrix\ndata = [[1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [0, 0], [1, 0]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 76, "completion_id": 1, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors cannot have zero magnitude')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 76, "completion_id": 2, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)"}
{"task_id": 76, "completion_id": 3, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 76, "completion_id": 4, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors cannot have zero magnitude')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 5, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)"}
{"task_id": 76, "completion_id": 6, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Vectors cannot have zero magnitude')\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)"}
{"task_id": 76, "completion_id": 7, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_sim, 3)"}
{"task_id": 76, "completion_id": 8, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_sim, 3)"}
{"task_id": 76, "completion_id": 9, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import Tuple, List\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positives += 1\n        elif a == 0 and p == 0:\n            true_negatives += 1\n        elif a == 0 and p == 1:\n            false_positives += 1\n        elif a == 1 and p == 0:\n            false_negatives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives > 0 else 0\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives > 0 else 0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives > 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 1, "solution": "from collections import Counter\nfrom typing import Tuple\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> Tuple[np.array, float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 2, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((a in [0, 1] for a in actual)) or not all((p in [0, 1] for p in predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 3, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positives += 1\n        elif a == 0 and p == 0:\n            true_negatives += 1\n        elif a == 0 and p == 1:\n            false_positives += 1\n        elif a == 1 and p == 0:\n            false_negatives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives > 0 else 0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives > 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 4, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    true_positives = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    true_negatives = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    false_positives = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    false_negatives = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n    accuracy = (true_positives + true_negatives) / len(actual)\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives > 0 else 0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives > 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 5, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 6, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted must be either 0 or 1.')\n    true_positives = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    true_negatives = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    false_positives = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    false_negatives = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n    accuracy = (true_positives + true_negatives) / len(actual)\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives > 0 else 0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives > 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 7, "solution": "from collections import Counter\nfrom typing import Tuple\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> Tuple[np.array, float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    specificity = tn / (tn + fp) if tn + fp != 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn != 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 8, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in (0, 1) for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positive += 1\n        elif a == 0 and p == 0:\n            true_negative += 1\n        elif a == 0 and p == 1:\n            false_positive += 1\n        elif a == 1 and p == 0:\n            false_negative += 1\n    confusion_matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n    if true_positive == 0:\n        f1_score = 0.0\n    else:\n        precision = true_positive / (true_positive + false_positive)\n        recall = true_positive / (true_positive + false_negative)\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    if true_negative + false_positive == 0:\n        specificity = 0.0\n    else:\n        specificity = true_negative / (true_negative + false_positive)\n    if true_negative + false_negative == 0:\n        negative_predictive_value = 0.0\n    else:\n        negative_predictive_value = true_negative / (true_negative + false_negative)\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 9, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0] if len(data) > 0 else None\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    mean = round(mean, 4)\n    median = round(median, 4)\n    mode = round(mode, 4) if mode is not None else None\n    variance = round(variance, 4)\n    standard_deviation = round(standard_deviation, 4)\n    percentile_25 = round(percentile_25, 4)\n    percentile_50 = round(percentile_50, 4)\n    percentile_75 = round(percentile_75, 4)\n    interquartile_range = round(interquartile_range, 4)\n    result = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}\n    return result"}
{"task_id": 78, "completion_id": 1, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 0 else np.nan\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentiles = np.percentile(data, [25, 50, 75])\n    percentile_25 = percentiles[0]\n    percentile_50 = percentiles[1]\n    percentile_75 = percentiles[2]\n    interquartile_range = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results"}
{"task_id": 78, "completion_id": 2, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.mode.size > 0 else None\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentiles = np.percentile(data, [25, 50, 75])\n    (p25, p50, p75) = percentiles\n    iqr = p75 - p25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if mode is not None else mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return results"}
{"task_id": 78, "completion_id": 3, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0] if len(data) > 0 else None\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if mode is not None else None, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results"}
{"task_id": 78, "completion_id": 4, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else np.nan\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    mean = round(mean, 4)\n    median = round(median, 4)\n    variance = round(variance, 4)\n    standard_deviation = round(standard_deviation, 4)\n    percentile_25 = round(percentile_25, 4)\n    percentile_50 = round(percentile_50, 4)\n    percentile_75 = round(percentile_75, 4)\n    interquartile_range = round(interquartile_range, 4)\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}"}
{"task_id": 78, "completion_id": 5, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=False).mode[0]\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = percentiles[2] - percentiles[0]\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles[0], 4), '50th_percentile': round(percentiles[1], 4), '75th_percentile': round(percentiles[2], 4), 'interquartile_range': round(iqr, 4)}\n    return result"}
{"task_id": 78, "completion_id": 6, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else np.nan\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if not np.isnan(mode) else mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return results\ndata = [10, 20, 20, 30, 40, 50, 60, 70, 80, 90, 100]"}
{"task_id": 78, "completion_id": 7, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else np.nan\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    percentiles = np.percentile(data, [25, 50, 75])\n    _25th_percentile = percentiles[0]\n    _50th_percentile = percentiles[1]\n    _75th_percentile = percentiles[2]\n    iqr = _75th_percentile - _25th_percentile\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if not np.isnan(mode) else np.nan, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(_25th_percentile, 4), '50th_percentile': round(_50th_percentile, 4), '75th_percentile': round(_75th_percentile, 4), 'interquartile_range': round(iqr, 4)}\n    return results\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]"}
{"task_id": 78, "completion_id": 8, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if len(mode_result.mode) > 0 else np.nan\n    variance = np.var(data, ddof=1)\n    std_dev = np.std(data, ddof=1)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if not np.isnan(mode) else mode, 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\n    return results\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]"}
{"task_id": 78, "completion_id": 9, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0] if stats.mode(data)[1][0] > 1 else np.nan\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentiles = np.percentile(data, [25, 50, 75])\n    iqr = percentiles[2] - percentiles[0]\n    mean = round(mean, 4)\n    median = round(median, 4)\n    mode = round(mode, 4) if not np.isnan(mode) else mode\n    variance = round(variance, 4)\n    standard_deviation = round(standard_deviation, 4)\n    percentiles = [round(p, 4) for p in percentiles]\n    iqr = round(iqr, 4)\n    results = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentiles[0], '50th_percentile': percentiles[1], '75th_percentile': percentiles[2], 'interquartile_range': iqr}\n    return results\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 1, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 2, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 3, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 4, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 5, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 6, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 7, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 8, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 9, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 1, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than 0.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 2, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than 0.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 3, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 4, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 5, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 6, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 7, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 8, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than zero.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 9, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('Number of events k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('Mean rate of occurrences lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 1, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('Number of events k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('The mean rate of occurrences lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 2, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = math.exp(-lam) * math.pow(lam, k) / math.factorial(k)\n    return round(probability, 5)\nk = 3\nlam = 2.5"}
{"task_id": 81, "completion_id": 3, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if lam <= 0:\n        raise ValueError('lam must be a positive number')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 4, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 5, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 6, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 7, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 8, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('Number of events k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('The average rate lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 9, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nk = 3\nlam = 2"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nsample_img = np.array([[50, 100, 150], [200, 250, 200], [100, 50, 0]], dtype=np.uint8)\ncontrast = calculate_contrast(sample_img)"}
{"task_id": 82, "completion_id": 1, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 2, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale image).')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 3, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nsample_image = np.array([[50, 100, 150], [20, 70, 120], [255, 200, 150]], dtype=np.uint8)\ncontrast = calculate_contrast(sample_image)"}
{"task_id": 82, "completion_id": 4, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError('Input must be a 2D numpy array representing a grayscale image.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 5, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale image).')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 6, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('The input image must be a 2D grayscale image.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 7, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nsample_img = np.array([[50, 60, 100], [70, 80, 200], [90, 100, 250]])\ncontrast = calculate_contrast(sample_img)"}
{"task_id": 82, "completion_id": 8, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('The input image must be a 2D array (grayscale image).')\n    if not np.issubdtype(img.dtype, np.integer) and (not np.issubdtype(img.dtype, np.float)):\n        raise ValueError('The image array must contain integer or float pixel values.')\n    if img.min() < 0 or img.max() > 255:\n        raise ValueError('The pixel values must be in the range [0, 255].')\n    max_val = img.max()\n    min_val = img.min()\n    contrast = max_val - min_val\n    return contrast\nsample_img = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\ncontrast = calculate_contrast(sample_img)"}
{"task_id": 82, "completion_id": 9, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale image).')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nsample_image = np.array([[50, 60, 100], [90, 100, 150], [200, 250, 255]])\ncontrast = calculate_contrast(sample_image)"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 1, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 2, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 3, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 4, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D numpy arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must have the same length.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 83, "completion_id": 5, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 6, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Input vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 7, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vec1 and vec2 must be 1D arrays.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must have the same length.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 83, "completion_id": 8, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 9, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_data = np.vander(data_array, degree + 1, increasing=True)[:, 1:]\n    transformed_data = np.round(transformed_data, 8)\n    return transformed_data.tolist()"}
{"task_id": 84, "completion_id": 1, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists containing the polynomial features of the input data.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    if degree == 0:\n        return [[1.0] for _ in data]\n    vandermonde_matrix = np.vander(data_array, degree + 1, increasing=True)\n    vandermonde_matrix_rounded = np.round(vandermonde_matrix, 8)\n    return vandermonde_matrix_rounded.tolist()"}
{"task_id": 84, "completion_id": 2, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists containing the polynomial features of the input data.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    data_array_reshaped = data_array.reshape(-1, 1)\n    from sklearn.preprocessing import PolynomialFeatures\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    poly_features = poly.fit_transform(data_array_reshaped)\n    poly_features_rounded = np.round(poly_features, 8).tolist()\n    return poly_features_rounded"}
{"task_id": 84, "completion_id": 3, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    transformed_data = []\n    for x in data:\n        poly_features = [x ** i for i in range(degree + 1)]\n        poly_features_rounded = [round(feature, 8) for feature in poly_features]\n        transformed_data.append(poly_features_rounded)\n    return transformed_data"}
{"task_id": 84, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    num_features = data.shape[0]\n    feature_combinations = combinations_with_replacement(range(num_features), degree)\n    transformed_data = []\n    for point in data.T:\n        feature_vector = [1]\n        for combination in feature_combinations:\n            feature_value = np.prod(point[list(combination)])\n            feature_vector.append(feature_value)\n        transformed_data.append(feature_vector)\n    return [list(map(lambda x: round(x, 8), point)) for point in transformed_data]"}
{"task_id": 84, "completion_id": 5, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_data = np.vander(data_array, degree + 1, increasing=True)\n    transformed_data = np.round(transformed_data, 8)\n    return transformed_data.tolist()"}
{"task_id": 84, "completion_id": 6, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_data = []\n    for x in data_array:\n        features = [x ** i for i in range(degree + 1)]\n        transformed_data.append([round(feature, 8) for feature in features])\n    return transformed_data"}
{"task_id": 84, "completion_id": 7, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists containing the polynomial features of each data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_features = np.vander(data_array, degree + 1, increasing=True)\n    transformed_features = np.round(transformed_features, 8)\n    return transformed_features.tolist()"}
{"task_id": 84, "completion_id": 8, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_np = np.array(data).reshape(-1, 1)\n    from sklearn.preprocessing import PolynomialFeatures\n    poly = PolynomialFeatures(degree, include_bias=False)\n    transformed_data = poly.fit_transform(data_np)\n    transformed_data_rounded = np.round(transformed_data, 8).tolist()\n    return transformed_data_rounded"}
{"task_id": 84, "completion_id": 9, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists containing the polynomial features of each data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    transformed_data = []\n    for x in data:\n        feature_vector = [x ** i for i in range(degree + 1)]\n        transformed_data.append([round(feature, 8) for feature in feature_vector])\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                pos_enc[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_enc.tolist()"}
{"task_id": 85, "completion_id": 1, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_encoding_matrix = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pos_encoding_matrix[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_encoding_matrix[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_encoding_matrix.tolist()"}
{"task_id": 85, "completion_id": 2, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    PE = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                PE[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                PE[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return PE.tolist()"}
{"task_id": 85, "completion_id": 3, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                pos_enc[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_enc.tolist()"}
{"task_id": 85, "completion_id": 4, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    PE = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                PE[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model), dtype=np.float16)\n            if i + 1 < d_model:\n                PE[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model), dtype=np.float16)\n    return PE.tolist()"}
{"task_id": 85, "completion_id": 5, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                pos_enc[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_enc.tolist()"}
{"task_id": 85, "completion_id": 6, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pos_enc[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_enc.tolist()"}
{"task_id": 85, "completion_id": 7, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 8, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 9, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    PE = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            PE[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                PE[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return PE.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 1, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 2, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 3, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 4, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 5, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy > test_accuracy + 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 6, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 7, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 8, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 9, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter, dtype=float)\n    grad = np.array(grad, dtype=float)\n    m = np.array(m, dtype=float)\n    v = np.array(v, dtype=float)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = parameter.round(5).tolist()\n    m = m.round(5).tolist()\n    v = v.round(5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter.round(5).tolist()\n    updated_m = m.round(5).tolist()\n    updated_v = v.round(5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = updated_parameter.round(5).tolist()\n    updated_m = m.round(5).tolist()\n    updated_v = v.round(5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter_update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter - parameter_update\n    updated_parameter = updated_parameter.tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = parameter.round(5).tolist()\n    m = m.round(5).tolist()\n    v = v.round(5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (parameter.tolist(), m.tolist(), v.tolist())"}
{"task_id": 87, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter.tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter.tolist()\n    updated_m = m.tolist()\n    updated_v = v.tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps: float=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x = (x - mean) / np.sqrt(variance + eps)\n    return g * x + b\ndef multi_head_attention(q, k, v, num_heads):\n    d_model = q.shape[-1]\n    d_k = d_q = d_v = d_model // num_heads\n    q = q.reshape(q.shape[0], q.shape[1], num_heads, d_q).transpose(0, 2, 1, 3)\n    k = k.reshape(k.shape[0], k.shape[1], num_heads, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(v.shape[0], v.shape[1], num_heads, d_v).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention = attention / np.sum(attention, axis=-1, keepdims=True)\n    output = np.matmul(attention, v)\n    output = output.transpose(0, 2, 1, 3).reshape(output.shape[0], output.shape[1], d_model)\n    return output\ndef feed_forward(x, intermediate_size=2048):\n    intermediate = np.maximum(0, np.dot(x, np.random.rand(x.shape[-1], intermediate_size)))\n    output = np.dot(intermediate, np.random.rand(intermediate_size, x.shape[-1]))\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    input_ids = encoder.encode(prompt)\n    context_length = len(input_ids)\n    if context_length >= n_ctx:\n        raise ValueError('Prompt is too long')\n    for _ in range(n_tokens_to_generate):\n        context = np.array(input_ids[-n_ctx:])[None, :]\n        token_embeddings = params['wte'][context]\n        position_ids = np.arange(min(n_ctx, len(input_ids)))\n        positional_embeddings = params['wpe'][position_ids][None, :]\n        x = token_embeddings + positional_embeddings\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        q = k = v = x\n        attention_output = multi_head_attention(q, k, v, n_head)\n        x = layer_norm(attention_output, params['ln_f']['g'], params['ln_f']['b'])\n        ff_output = feed_forward(x)\n        x = x + ff_output\n        logits = np.dot(x[0, -1, :], params['wte'].T)\n        next_token = np.argmax(logits)\n        input_ids.append(next_token)\n    return encoder.decode(input_ids)"}
{"task_id": 88, "completion_id": 1, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mu = np.mean(x, axis=-1, keepdims=True)\n    sigma = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mu) / (sigma + eps) + b\ndef causal_attention_mask(n_ctx):\n    return 1 - np.triu(np.ones((n_ctx, n_ctx)), k=1)\ndef multi_head_attention(q, k, v, mask, n_head):\n    (n_ctx, embed_size) = q.shape\n    head_size = embed_size // n_head\n    q_heads = q.reshape(n_ctx, n_head, head_size)\n    k_heads = k.reshape(n_ctx, n_head, head_size)\n    v_heads = v.reshape(n_ctx, n_head, head_size)\n    att = np.einsum('thd,Thd->thT', q_heads, k_heads) / np.sqrt(head_size)\n    att = att * mask - 10000000000.0 * (1 - mask)\n    att_probs = np.exp(att - np.max(att, axis=-1, keepdims=True))\n    att_probs /= np.sum(att_probs, axis=-1, keepdims=True)\n    out = np.einsum('thT,Thd->thd', att_probs, v_heads)\n    return out.reshape(n_ctx, embed_size)\ndef feed_forward(x, W1, W2):\n    return W2 @ np.maximum(W1 @ x, 0)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    input_ids = encoder.encode(prompt)\n    context = np.array(input_ids, dtype=int)\n    position_ids = np.arange(len(context), dtype=int)\n    for _ in range(n_tokens_to_generate):\n        context = context[-n_ctx:]\n        position_ids = position_ids[-n_ctx:]\n        token_embeddings = params['wte'][context]\n        position_embeddings = params['wpe'][position_ids]\n        x = token_embeddings + position_embeddings\n        for block in params['blocks']:\n            q = k = v = x\n            mask = causal_attention_mask(n_ctx)\n            attn_out = multi_head_attention(q, k, v, mask, n_head)\n            x = layer_norm(attn_out + x, block['ln_1']['g'], block['ln_1']['b'])\n            ff_out = feed_forward(x, block['mlp']['c_fc'], block['mlp']['c_proj'])\n            x = layer_norm(ff_out + x, block['ln_2']['g'], block['ln_2']['b'])\n        logits = x @ params['wte'].T\n        next_token_id = np.argmax(logits[-1])\n        context = np.append(context, next_token_id)\n        position_ids = np.append(position_ids, len(context) - 1)\n    generated_text = encoder.decode(context)\n    return generated_text"}
{"task_id": 88, "completion_id": 2, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, embed_size) = q.shape\n    head_size = embed_size // n_head\n    q_heads = q.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    k_heads = k.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    v_heads = v.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    attn_scores = np.einsum('bhqd,bhkd->bhqk', q_heads, k_heads) / np.sqrt(head_size)\n    attn_probs = np.exp(attn_scores - np.max(attn_scores, axis=-1, keepdims=True))\n    attn_probs /= np.sum(attn_probs, axis=-1, keepdims=True)\n    attn_output_heads = np.einsum('bhqk,bhkd->bhqd', attn_probs, v_heads)\n    attn_output = attn_output_heads.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, embed_size)\n    return attn_output\ndef feed_forward(x, hidden_size):\n    hidden = np.maximum(0, np.dot(x, np.random.rand(10, hidden_size)) + np.random.rand(hidden_size))\n    output = np.dot(hidden, np.random.rand(hidden_size, 10)) + np.random.rand(10)\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    x = params['wte'][token_ids] + params['wpe'][:len(token_ids)]\n    for _ in range(n_tokens_to_generate):\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        attn_output = multi_head_attention(x, x, x, hparams['n_head'])\n        x = x + attn_output\n        ff_output = feed_forward(x, 20)\n        x = x + ff_output\n        last_token = np.argmax(x[-1], axis=-1)\n        token_ids.append(last_token)\n        x = np.concatenate([x, params['wte'][last_token:last_token + 1] + params['wpe'][len(token_ids) - 1:len(token_ids)]])\n    generated_text = encoder.decode(token_ids)\n    return generated_text\nprompt = 'hello'\ngenerated_text = gen_text(prompt, 10)"}
{"task_id": 88, "completion_id": 3, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\ndef linear(x, w):\n    return np.dot(x, w)\ndef multi_head_attention(q, k, v, n_head, mask=None):\n    (batch_size, seq_len, hidden_size) = q.shape\n    head_size = hidden_size // n_head\n    q = q.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, head_size).transpose(0, 2, 1, 3)\n    att = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(head_size)\n    if mask is not None:\n        att += mask\n    att = np.exp(att - np.max(att, axis=-1, keepdims=True))\n    att /= np.sum(att, axis=-1, keepdims=True)\n    out = np.matmul(att, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, hidden_size)\n    return out\ndef feed_forward(x, w1, w2, b1, b2):\n    return linear(gelu(linear(x, w1) + b1), w2) + b2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    wte = params['wte']\n    wpe = params['wpe']\n    ln_f_g = params['ln_f']['g']\n    ln_f_b = params['ln_f']['b']\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]\n    input_ids = np.array(input_ids)\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = wte[input_ids]\n        position_ids = np.arange(len(input_ids))\n        position_embeddings = wpe[position_ids]\n        x = token_embeddings + position_embeddings\n        qkv = linear(x, np.random.rand(10, 30))\n        (q, k, v) = np.split(qkv, 3, axis=-1)\n        att_output = multi_head_attention(q, k, v, n_head)\n        ff_output = feed_forward(att_output, np.random.rand(10, 100), np.random.rand(100, 10), np.random.rand(100), np.random.rand(10))\n        x = layer_norm(att_output + ff_output, ln_f_g, ln_f_b)\n        logits = linear(x[-1], np.random.rand(10, 3))\n        next_token = np.argmax(logits, axis=-1)\n        input_ids = np.append(input_ids, next_token)\n    generated_text = encoder.decode(input_ids.tolist())\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)"}
{"task_id": 88, "completion_id": 4, "solution": "def multi_head_attention(x, params, n_head):\n    n_embd = x.shape[-1]\n    d_k = n_embd // n_head\n    q = np.dot(x, params['q'])\n    k = np.dot(x, params['k'])\n    v = np.dot(x, params['v'])\n    q_split = np.concatenate([q[:, :, i * d_k:(i + 1) * d_k] for i in range(n_head)], axis=0)\n    k_split = np.concatenate([k[:, :, i * d_k:(i + 1) * d_k] for i in range(n_head)], axis=0)\n    v_split = np.concatenate([v[:, :, i * d_k:(i + 1) * d_k] for i in range(n_head)], axis=0)\n    scores = np.dot(q_split, k_split.T) / np.sqrt(d_k)\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights /= np.sum(weights, axis=-1, keepdims=True)\n    attended = np.dot(weights, v_split)\n    attended_split = [attended[i::n_head] for i in range(n_head)]\n    attended_concat = np.concatenate(attended_split, axis=-1)\n    output = np.dot(attended_concat, params['o'])\n    return output\ndef feed_forward_network(x, params):\n    hidden = np.maximum(0, np.dot(x, params['c_fc']) + params['b_fc'])\n    output = np.dot(hidden, params['c_proj']) + params['b_proj']\n    return output\ndef block(x, params, n_head):\n    x = x + multi_head_attention(x, params['attn'], n_head)\n    x = LayerNormalization(x, params['ln_1']['g'], params['ln_1']['b'])\n    x = x + feed_forward_network(x, params['mlp'])\n    x = LayerNormalization(x, params['ln_2']['g'], params['ln_2']['b'])\n    return x\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    input_ids = encoder.encode(prompt)\n    position_ids = np.arange(len(input_ids), dtype=np.int32)\n    token_embeddings = params['wte'][input_ids]\n    position_embeddings = params['wpe'][position_ids]\n    x = token_embeddings + position_embeddings\n    for _ in range(n_ctx - len(input_ids)):\n        for block_params in params['blocks']:\n            x = block(x, block_params, n_head)\n        logits = np.dot(x[-1], params['wte'].T) + params['b_fc'][-1]\n        next_token = np.argmax(logits)\n        input_ids.append(next_token)\n        position_ids = np.arange(len(input_ids), dtype=np.int32)\n        new_token_embedding = params['wte'][next_token]\n        new_position_embedding = params['wpe'][len(input_ids) - 1]\n        x = np.concatenate([x, [new_token_embedding + new_position_embedding]])\n        if len(input_ids) >= n_ctx:\n            break\n        if len(input_ids) >= len(prompt.split()) + n_tokens_to_generate:\n            break\n    generated_text = encoder.decode(input_ids[len(prompt.split()):])\n    return generated_text"}
{"task_id": 88, "completion_id": 5, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(30, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef multi_head_attention(x, c_attn, c_proj, n_head):\n    (batch_size, seq_len, d_model) = x.shape\n    (query, key, value) = np.split(x @ c_attn, 3, axis=-1)\n    query = query.reshape(batch_size, seq_len, n_head, d_model // n_head).transpose(0, 2, 1, 3)\n    key = key.reshape(batch_size, seq_len, n_head, d_model // n_head).transpose(0, 2, 1, 3)\n    value = value.reshape(batch_size, seq_len, n_head, d_model // n_head).transpose(0, 2, 1, 3)\n    scores = np.matmul(query, key.transpose(0, 1, 3, 2)) / np.sqrt(d_model // n_head)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, value).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output @ c_proj\ndef feed_forward(x, c_fc, c_proj):\n    x = np.maximum(x, 0) @ c_fc\n    x = np.maximum(x, 0) @ c_proj\n    return x\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_head = hparams['n_head']\n    block = params['blocks'][0]\n    input_ids = encoder.encode(prompt)\n    input_ids = np.array(input_ids).reshape(1, -1)\n    (batch_size, seq_len) = input_ids.shape\n    d_model = params['wte'].shape[-1]\n    token_embeddings = params['wte'][input_ids]\n    positional_embeddings = params['wpe'][:seq_len]\n    x = token_embeddings + positional_embeddings\n    for _ in range(n_tokens_to_generate):\n        attn_output = multi_head_attention(x, block['attn']['c_attn'], block['attn']['c_proj'], n_head)\n        x = x + attn_output\n        x = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n        mlp_output = feed_forward(x, block['mlp']['c_fc'], block['mlp']['c_proj'])\n        x = x + mlp_output\n        x = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n        logits = x[:, -1, :] @ params['wte'].T\n        next_token = np.argmax(logits, axis=-1).reshape(1, 1)\n        x = np.concatenate([x, params['wte'][next_token]], axis=1)\n    generated_ids = np.argmax(x, axis=-1).flatten()\n    generated_text = encoder.decode(generated_ids[len(input_ids[0]):-1])\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)"}
{"task_id": 88, "completion_id": 6, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x_norm = (x - mean) / np.sqrt(variance + eps)\n    return g * x_norm + b\ndef softmax(x, axis=-1):\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)\ndef multi_head_attention(q, k, v, n_head):\n    d_k = q.shape[-1] // n_head\n    q_h = q.reshape(q.shape[0], n_head, d_k)\n    k_h = k.reshape(k.shape[0], n_head, d_k)\n    v_h = v.reshape(v.shape[0], n_head, d_k)\n    scores = np.matmul(q_h, k_h.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = softmax(scores)\n    context = np.matmul(attention_weights, v_h)\n    context = context.reshape(q.shape)\n    return context\ndef feed_forward_network(x, n_hidden):\n    hidden = np.dot(x, np.random.rand(x.shape[-1], n_hidden)) + np.random.rand(n_hidden)\n    hidden = np.maximum(hidden, 0)\n    output = np.dot(hidden, np.random.rand(n_hidden, x.shape[-1])) + np.random.rand(x.shape[-1])\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    n_hidden = 20\n    for _ in range(n_tokens_to_generate):\n        if len(input_ids) > n_ctx:\n            input_ids = input_ids[-n_ctx:]\n        token_embeddings = params['wte'][input_ids]\n        positional_encodings = params['wpe'][:len(input_ids)]\n        x = token_embeddings + positional_encodings\n        attention_output = multi_head_attention(x, x, x, n_head)\n        x = x + attention_output\n        ln_f_g = params['ln_f']['g']\n        ln_f_b = params['ln_f']['b']\n        x = layer_norm(x, ln_f_g, ln_f_b)\n        ffn_output = feed_forward_network(x, n_hidden)\n        x = x + ffn_output\n        x = layer_norm(x, ln_f_g, ln_f_b)\n        logits = np.dot(x[-1], params['wte'].T)\n        next_token = np.argmax(logits, axis=-1)\n        input_ids.append(next_token)\n    generated_text = encoder.decode(input_ids)\n    return generated_text"}
{"task_id": 88, "completion_id": 7, "solution": "def multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, embed_size) = q.shape\n    head_dim = embed_size // n_head\n    q = q.reshape(batch_size, seq_len, n_head, head_dim).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, head_dim).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, head_dim).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(head_dim)\n    attention = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention = attention / np.sum(attention, axis=-1, keepdims=True)\n    output = np.matmul(attention, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, embed_size)\n    return output\ndef feed_forward(x, intermediate_size=2048):\n    embed_size = x.shape[-1]\n    w1 = np.random.rand(embed_size, intermediate_size)\n    b1 = np.zeros(intermediate_size)\n    w2 = np.random.rand(intermediate_size, embed_size)\n    b2 = np.zeros(embed_size)\n    intermediate = np.maximum(np.dot(x, w1) + b1, 0)\n    output = np.dot(intermediate, w2) + b2\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    wte = params['wte']\n    wpe = params['wpe']\n    ln_f_g = params['ln_f']['g']\n    ln_f_b = params['ln_f']['b']\n    token_ids = encoder.encode(prompt)\n    token_ids = token_ids[-n_ctx:]\n    token_ids = np.array(token_ids)\n    for _ in range(n_tokens_to_generate):\n        position_ids = np.arange(len(token_ids))\n        token_embeddings = wte[token_ids]\n        positional_embeddings = wpe[position_ids]\n        x = token_embeddings + positional_embeddings\n        x = multi_head_attention(x, x, x, n_head)\n        x = feed_forward(x)\n        x = layer_norm(x, ln_f_g, ln_f_b)\n        logits = np.dot(x[-1], wte.T)\n        next_token_id = np.argmax(logits, axis=-1)\n        token_ids = np.append(token_ids, next_token_id)\n        token_ids = token_ids[-n_ctx:]\n    generated_text = encoder.decode(token_ids)\n    return generated_text"}
{"task_id": 88, "completion_id": 8, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_normalization(x, g, b, epsilon=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + epsilon) + b\ndef multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, d_model) = q.shape\n    d_k = d_v = d_model // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_v).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, v)\n    output = output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\ndef feed_forward(x, params):\n    (w1, b1) = (params['w1'], params['b1'])\n    (w2, b2) = (params['w2'], params['b2'])\n    output = np.maximum(np.dot(x, w1) + b1, 0)\n    output = np.dot(output, w2) + b2\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    wte = params['wte']\n    wpe = params['wpe']\n    ln_f_g = params['ln_f']['g']\n    ln_f_b = params['ln_f']['b']\n    ff_params = {'w1': np.random.rand(10, 20), 'b1': np.zeros(20), 'w2': np.random.rand(20, 10), 'b2': np.zeros(10)}\n    input_tokens = encoder.encode(prompt)\n    current_ids = input_tokens\n    for _ in range(n_tokens_to_generate):\n        context = np.array(current_ids[-n_ctx:])\n        token_embeddings = wte[context]\n        positional_ids = np.arange(len(context))\n        positional_embeddings = wpe[positional_ids]\n        x = token_embeddings + positional_embeddings\n        attention_output = multi_head_attention(x, x, x, n_head)\n        x = layer_normalization(attention_output + x, ln_f_g, ln_f_b)\n        x = feed_forward(x, ff_params)\n        x = layer_normalization(x, ln_f_g, ln_f_b)\n        logits = np.dot(x, wte.T)\n        next_id = np.argmax(logits[-1])\n        current_ids.append(next_id)\n    return encoder.decode(current_ids)"}
{"task_id": 88, "completion_id": 9, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps: float=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    x_normalized = (x - mean) / np.sqrt(variance + eps)\n    return g * x_normalized + b\ndef multi_head_attention(query, key, value, n_head: int):\n    d_model = query.shape[-1]\n    d_k = d_model // n_head\n\n    def split_heads(x):\n        x = np.reshape(x, (x.shape[0], n_head, d_k))\n        return np.transpose(x, [1, 0, 2])\n\n    def merge_heads(x):\n        x = np.transpose(x, [1, 0, 2])\n        return np.reshape(x, (x.shape[0], d_model))\n    Q = split_heads(query)\n    K = split_heads(key)\n    V = split_heads(value)\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    return merge_heads(output)\ndef feed_forward(x, d_ff: int):\n    W1 = np.random.rand(x.shape[-1], d_ff)\n    W2 = np.random.rand(d_ff, x.shape[-1])\n    B1 = np.random.rand(d_ff)\n    B2 = np.random.rand(x.shape[-1])\n    return np.matmul(np.maximum(0, np.matmul(x, W1) + B1), W2) + B2\ndef softmax(x, axis=-1):\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    input_length = len(input_ids)\n    if input_length > hparams['n_ctx']:\n        input_ids = input_ids[-hparams['n_ctx']:]\n        input_length = hparams['n_ctx']\n    input_ids = input_ids + [0] * (hparams['n_ctx'] - input_length)\n    token_embeddings = params['wte'][input_ids]\n    position_ids = np.arange(hparams['n_ctx'])\n    positional_embeddings = params['wpe'][position_ids]\n    hidden_states = token_embeddings + positional_embeddings\n    hidden_states = multi_head_attention(hidden_states, hidden_states, hidden_states, hparams['n_head']) + hidden_states\n    hidden_states = feed_forward(hidden_states, d_ff=20) + hidden_states\n    hidden_states = layer_norm(hidden_states, params['ln_f']['g'], params['ln_f']['b'])\n    generated_ids = input_ids.copy()\n    for _ in range(n_tokens_to_generate):\n        logits = np.dot(hidden_states[-1], params['wte'].T)\n        next_id = np.argmax(logits, axis=-1)\n        generated_ids.append(next_id)\n        new_token_embedding = params['wte'][next_id]\n        new_positional_embedding = params['wpe'][len(generated_ids) - 1]\n        new_hidden_state = new_token_embedding + new_positional_embedding\n        new_hidden_state = multi_head_attention(new_hidden_state, hidden_states, hidden_states, hparams['n_head']) + new_hidden_state\n        new_hidden_state = feed_forward(new_hidden_state, d_ff=20) + new_hidden_state\n        new_hidden_state = layer_norm(new_hidden_state, params['ln_f']['g'], params['ln_f']['b'])\n        hidden_states = np.append(hidden_states, [new_hidden_state], axis=0)\n    generated_text = encoder.decode(generated_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    weighted_patterns = np.zeros(n)\n    crystal_values = np.array(crystal_values)\n    W = np.random.rand(dimension, dimension)\n    projected_values = np.dot(crystal_values.reshape(n, 1), np.ones((1, dimension)))\n    scored_values = np.dot(projected_values, W)\n    for i in range(n):\n        scores = np.dot(scored_values[i], scored_values.T)\n        attention_weights = softmax(scores)\n        weighted_patterns[i] = np.dot(attention_weights, crystal_values)\n    return [round(value, 4) for value in weighted_patterns]"}
{"task_id": 89, "completion_id": 1, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystals = np.array(crystal_values).reshape(n, 1) * np.ones((1, dimension))\n    attention_scores = np.dot(crystals, crystals.T)\n    attention_weights = np.apply_along_axis(softmax, 1, attention_scores)\n    weighted_patterns = np.dot(attention_weights, crystals)\n    final_patterns = np.sum(weighted_patterns, axis=1)\n    return [round(value, 4) for value in final_patterns]"}
{"task_id": 89, "completion_id": 2, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    weighted_patterns = []\n    crystal_values = np.array(crystal_values)\n    for i in range(n):\n        attention_scores = np.dot(crystal_values, crystal_values[i]) / dimension\n        attention_weights = softmax(attention_scores)\n        weighted_pattern = np.dot(attention_weights, crystal_values)\n        weighted_patterns.append(round(weighted_pattern, 4))\n    return weighted_patterns\nn = 4\ncrystal_values = [1.0, 2.0, 3.0, 4.0]\ndimension = 1.0"}
{"task_id": 89, "completion_id": 3, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    weights = np.random.randn(n, dimension)\n    attention_scores = np.dot(weights, weights.T)\n    attention_probs = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_probs, crystal_values)\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = [10, 20, 30, 40, 50]\ndimension = 3\nweighted_patterns = pattern_weaver(n, crystal_values, dimension)"}
{"task_id": 89, "completion_id": 4, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystals = np.array(crystal_values).reshape(n, 1)\n    weights = np.random.rand(dimension, 1)\n    projected_crystals = np.dot(crystals, weights.T)\n    attention_scores = np.dot(projected_crystals, projected_crystals.T)\n    attention_weights = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_weights, crystals)\n    final_patterns = [round(pattern[0], 4) for pattern in weighted_patterns]\n    return final_patterns\nn = 5\ndimension = 3"}
{"task_id": 89, "completion_id": 5, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystal_values = np.array(crystal_values)\n    weighted_patterns = np.zeros_like(crystal_values, dtype=float)\n    for i in range(n):\n        attention_scores = np.zeros(n)\n        for j in range(n):\n            if i != j:\n                relationship = np.dot(crystal_values[i, :dimension], crystal_values[j, :dimension])\n                attention_scores[j] = relationship\n        attention_weights = softmax(attention_scores)\n        for j in range(n):\n            if i != j:\n                weighted_patterns[i] += attention_weights[j] * crystal_values[j]\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 3\ncrystal_values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ndimension = 3"}
{"task_id": 89, "completion_id": 6, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystals = np.array(crystal_values).reshape(n, 1)\n    scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            scores[i, j] = np.dot(crystals[i].T, crystals[j]) / np.sqrt(dimension)\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=scores)\n    weighted_patterns = np.dot(attention_weights, crystals)\n    final_patterns = [round(pattern[0], 4) for pattern in weighted_patterns]\n    return final_patterns\nn = 3\ndimension = 1"}
{"task_id": 89, "completion_id": 7, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystal_values = np.array(crystal_values)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            projection_matrix = np.random.rand(dimension, 1)\n            crystal_i_proj = np.dot(projection_matrix.T, crystal_values[i:i + 1].reshape(-1, 1))\n            crystal_j_proj = np.dot(projection_matrix.T, crystal_values[j:j + 1].reshape(-1, 1))\n            attention_scores[i, j] = np.dot(crystal_i_proj.T, crystal_j_proj)\n    for i in range(n):\n        attention_scores[i] = softmax(attention_scores[i])\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = [1.0, 2.0, 3.0, 4.0, 5.0]\ndimension = 10"}
{"task_id": 89, "completion_id": 8, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / np.sum(e_values)\n    crystal_values = np.array(crystal_values).reshape(n, 1)\n    value_matrix = np.tile(crystal_values, (1, n))\n    raw_scores = np.dot(value_matrix, value_matrix.T) / np.sqrt(dimension)\n    attention_scores = softmax(raw_scores)\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(pattern[0], 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = [10, 20, 30, 40, 50]\ndimension = 64"}
{"task_id": 89, "completion_id": 9, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / np.sum(e_values)\n    final_patterns = np.zeros(n)\n    crystal_values = np.array(crystal_values)\n    weights = np.random.rand(dimension, dimension)\n    expanded_crystals = np.repeat(crystal_values[:, np.newaxis], dimension, axis=1)\n    for i in range(n):\n        current_crystal = expanded_crystals[i]\n        score = np.dot(weights, current_crystal)\n        scores = np.dot(expanded_crystals, score)\n        attention_scores = softmax(scores)\n        final_pattern = np.dot(attention_scores, expanded_crystals)\n        final_patterns[i] = np.sum(final_pattern)\n    return [round(pattern, 4) for pattern in final_patterns]\nn = 5\ncrystal_values = [10, 20, 30, 40, 50]\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def idf(N, n):\n        return np.log((N - n + 0.5) / (n + 0.5))\n\n    def avgdl(documents):\n        return sum((len(doc) for doc in documents)) / len(documents)\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    tokenized_query = query.lower().split()\n    N = len(tokenized_corpus)\n    doc_freqs = Counter((term for doc in tokenized_corpus for term in doc))\n    idfs = {term: idf(N, doc_freqs[term]) for term in doc_freqs}\n    avdl = avgdl(tokenized_corpus)\n    scores = []\n    for doc in tokenized_corpus:\n        score = 0\n        doc_len = len(doc)\n        doc_term_freqs = Counter(doc)\n        for term in tokenized_query:\n            if term in doc_term_freqs:\n                f_q_d = doc_term_freqs[term]\n                score += idfs[term] * (f_q_d * (k1 + 1)) / (f_q_d + k1 * (1 - b + b * doc_len / avdl))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the cat sat on the mat', 'the dog sat on the log', 'cats and dogs are friends']\nquery = 'cat sat'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    df = Counter()\n    num_docs = len(corpus)\n    for document in corpus:\n        terms = set(document.split())\n        for term in terms:\n            df[term] += 1\n    idf = {}\n    for (term, freq) in df.items():\n        idf[term] = np.log((num_docs - freq + 0.5) / (freq + 0.5))\n    avg_doc_length = sum((len(doc.split()) for doc in corpus)) / num_docs\n    scores = []\n    for document in corpus:\n        doc_length = len(document.split())\n        doc_terms = Counter(document.split())\n        score = 0.0\n        for term in query.split():\n            if term in doc_terms:\n                freq = doc_terms[term]\n                score += idf[term] * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * doc_length / avg_doc_length))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the cat sat on the mat', 'the dog sat on the log', 'cats and dogs are great']"}
{"task_id": 90, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def idf(term, corpus):\n        num_docs = len(corpus)\n        num_docs_with_term = sum((1 for doc in corpus if term in doc))\n        return np.log((num_docs - num_docs_with_term + 0.5) / (num_docs_with_term + 0.5))\n    avg_doc_length = np.mean([len(doc) for doc in corpus])\n    query_terms = set(query)\n    scores = []\n    for doc in corpus:\n        score = 0\n        doc_length = len(doc)\n        doc_term_freq = Counter(doc)\n        for term in query_terms:\n            if term in doc_term_freq:\n                term_freq = doc_term_freq[term]\n                idf_value = idf(term, corpus)\n                score_term = idf_value * (term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n                score += score_term\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the quick brown fox jumps over the lazy dog'.split(), 'never jump over the lazy dog quickly'.split(), 'a quick brown dog outpaces a quick fox'.split()]\nquery = 'quick brown fox'.split()"}
{"task_id": 90, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def idf(term, corpus):\n        num_docs_with_term = sum((1 for doc in corpus if term in doc))\n        return math.log((len(corpus) - num_docs_with_term + 0.5) / (num_docs_with_term + 0.5))\n    avg_doc_length = np.mean([len(doc) for doc in corpus])\n    query_terms = Counter(query)\n    scores = []\n    for doc in corpus:\n        doc_length = len(doc)\n        doc_term_count = Counter(doc)\n        score = 0\n        for term in query_terms:\n            f_qd = doc_term_count[term]\n            idf_value = idf(term, corpus)\n            score += idf_value * (f_qd * (k1 + 1)) / (f_qd + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['this', 'is', 'a', 'sample', 'document'], ['this', 'document', 'is', 'another', 'example'], ['information', 'retrieval', 'is', 'interesting'], ['sample', 'examples', 'from', 'the', 'field']]\nquery = ['sample', 'document']"}
{"task_id": 90, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    query = query.lower().split()\n    corpus = [' '.join(doc.lower().split()) for doc in corpus]\n    doc_lengths = np.array([len(doc.split()) for doc in corpus])\n    avg_doc_len = np.mean(doc_lengths)\n    num_docs = len(corpus)\n    idf = {}\n    for term in query:\n        num_docs_with_term = sum((1 for doc in corpus if term in doc.split()))\n        idf[term] = np.log((num_docs - num_docs_with_term + 0.5) / (num_docs_with_term + 0.5))\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0\n        doc_words = Counter(doc.split())\n        for term in query:\n            if term in doc_words:\n                tf = doc_words[term]\n                tf_saturation = (k1 + 1) * tf / (k1 * (1 - b + b * doc_lengths[i] / avg_doc_len) + tf)\n                score += idf[term] * tf_saturation\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['The quick brown fox jumps over the lazy dog', 'Never jump over the lazy dog quickly', 'A fast brown fox leaps over the sleeping dog', 'The dog was not amused by the quick brown fox']\nquery = 'quick brown fox'"}
{"task_id": 90, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    df = Counter()\n    for doc in corpus:\n        for word in set(doc):\n            df[word] += 1\n    avgdl = np.mean([len(doc) for doc in corpus])\n    scores = []\n    N = len(corpus)\n    for doc in corpus:\n        score = 0\n        doc_len = len(doc)\n        doc_term_count = Counter(doc)\n        for term in query:\n            if term in doc_term_count:\n                fqi = doc_term_count[term]\n                ni = df[term]\n                term_part1 = np.log((N - ni + 0.5) / (ni + 0.5))\n                term_part2 = (k1 + 1) * fqi / (k1 * (1 - b + b * (doc_len / avgdl)) + fqi)\n                score += term_part1 * term_part2\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the cat sat on the mat'.split(), 'the dog sat on the log'.split(), 'cats and dogs are friends'.split()]\nquery = 'cat dog'.split()"}
{"task_id": 90, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    doc_word_counts = [Counter(doc.split()) for doc in corpus]\n    avg_doc_len = sum((len(doc.split()) for doc in corpus)) / len(corpus)\n    query_counts = Counter(query.split())\n    idf = {}\n    N = len(corpus)\n    for term in query_counts:\n        df = sum((1 for doc in doc_word_counts if term in doc))\n        idf[term] = np.log((N - df + 0.5) / (df + 0.5))\n    scores = []\n    for (doc_index, doc) in enumerate(corpus):\n        score = 0\n        doc_len = len(doc.split())\n        for term in query_counts:\n            if term in doc:\n                f_qi_d = doc_word_counts[doc_index][term]\n                score += idf[term] * (f_qi_d * (k1 + 1)) / (f_qi_d + k1 * (1 - b + b * doc_len / avg_doc_len))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the quick brown fox jumps over the lazy dog', 'never jump over the lazy dog quickly', 'a quick brown dog outpaces a quick fox']"}
{"task_id": 90, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def idf(term, corpus):\n        num_docs_with_term = sum((1 for doc in corpus if term in doc))\n        if num_docs_with_term == 0:\n            return 0\n        return np.log((len(corpus) - num_docs_with_term + 0.5) / (num_docs_with_term + 0.5))\n    doc_lengths = [len(doc) for doc in corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    query_terms = Counter(query)\n    scores = []\n    for doc in corpus:\n        doc_score = 0\n        doc_term_count = Counter(doc)\n        for (term, query_term_freq) in query_terms.items():\n            idf_term = idf(term, corpus)\n            if term in doc_term_count:\n                doc_term_freq = doc_term_count[term]\n            else:\n                doc_term_freq = 0\n            numerator = idf_term * doc_term_freq * (k1 + 1)\n            denominator = doc_term_freq + k1 * (1 - b + b * (len(doc) / avg_doc_length))\n            if denominator == 0:\n                doc_score += 0\n            else:\n                doc_score += numerator / denominator\n        scores.append(round(doc_score, 3))\n    return scores\ncorpus = [['this', 'is', 'a', 'sample', 'document'], ['this', 'document', 'is', 'another', 'example'], ['and', 'this', 'is', 'yet', 'another', 'one'], ['sample', 'example', 'one', 'document', 'this']]\nquery = ['this', 'document', 'sample']"}
{"task_id": 90, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def avgdl(corpus):\n        total_length = sum((len(doc) for doc in corpus))\n        return total_length / len(corpus)\n    avgdl_value = avgdl(corpus)\n    doc_lengths = [len(doc) for doc in corpus]\n    corpus = [doc.lower().split() for doc in corpus]\n    query = query.lower().split()\n    doc_freqs = Counter((term for doc in corpus for term in set(doc)))\n    N = len(corpus)\n    query_terms = set(query)\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0.0\n        doc_length = doc_lengths[i]\n        for term in query_terms:\n            if term in doc:\n                f_q_d = doc.count(term)\n                n_q = doc_freqs[term]\n                idf = np.log((N - n_q + 0.5) / (n_q + 0.5))\n                saturation = k1 * f_q_d / (f_q_d + k1 * (1 - b + b * (doc_length / avgdl_value)))\n                score += idf * saturation\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['The sky is blue and beautiful.', 'Love this blue and beautiful sky!', 'The quick brown fox jumps over the lazy dog.', 'Never jump over the lazy dog quickly.']\nquery = 'blue sky'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def preprocess(text):\n        return text.lower().split()\n    query = preprocess(query)\n    corpus = [preprocess(doc) for doc in corpus]\n    doc_lengths = np.array([len(doc) for doc in corpus])\n    avg_doc_length = np.mean(doc_lengths)\n\n    def document_frequency(term):\n        return sum((1 for doc in corpus if term in doc))\n    scores = []\n    N = len(corpus)\n    for (doc, doc_length) in zip(corpus, doc_lengths):\n        score = 0\n        query_terms_count = Counter(query)\n        doc_terms_count = Counter(doc)\n        for (term, query_term_freq) in query_terms_count.items():\n            df = document_frequency(term)\n            idf = np.log((N - df + 0.5) / (df + 0.5))\n            tf = min(doc_terms_count[term], k1) / (doc_terms_count[term] + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n            score += idf * tf * query_term_freq\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['The quick brown fox jumps over the lazy dog', 'Never jump over the lazy dog quickly', 'A quick brown dog outpaces a quick fox']\nquery = 'quick brown fox'"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((1 for (y_t, y_p) in zip(y_true, y_pred) if y_t == 1 and y_p == 1))\n    fp = sum((1 for (y_t, y_p) in zip(y_true, y_pred) if y_t == 0 and y_p == 1))\n    fn = sum((1 for (y_t, y_p) in zip(y_true, y_pred) if y_t == 1 and y_p == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 1, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((yt == 1 and yp == 1 for (yt, yp) in zip(y_true, y_pred)))\n    fp = sum((yt == 0 and yp == 1 for (yt, yp) in zip(y_true, y_pred)))\n    fn = sum((yt == 1 and yp == 0 for (yt, yp) in zip(y_true, y_pred)))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 2, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from collections import Counter\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positives += 1\n        elif true == 0 and pred == 1:\n            false_positives += 1\n        elif true == 1 and pred == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [0, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 3, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    fp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 0 and pred == 1))\n    fn = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\ny_true = [0, 1, 1, 0, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 0, 1, 0, 1, 1]"}
{"task_id": 91, "completion_id": 4, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    fp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 0 and pred == 1))\n    fn = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 5, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 6, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    fp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 0 and pred == 1))\n    fn = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 7, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum(((y_t == 1) & (y_p == 1) for (y_t, y_p) in zip(y_true, y_pred)))\n    fp = sum(((y_t == 0) & (y_p == 1) for (y_t, y_p) in zip(y_true, y_pred)))\n    fn = sum(((y_t == 1) & (y_p == 0) for (y_t, y_p) in zip(y_true, y_pred)))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 91, "completion_id": 8, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from collections import Counter\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == pred == 1:\n            true_positives += 1\n        elif pred == 1 and true == 0:\n            false_positives += 1\n        elif pred == 0 and true == 1:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 9, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positives += 1\n        elif true == 0 and pred == 1:\n            false_positives += 1\n        elif true == 1 and pred == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\ny_pred = [0, 0, 0, 1, 1, 0, 1, 1, 0, 0]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended_data.append(consumption - fluctuation)\n    X = [[i] for i in range(1, len(consumption_data) + 1)]\n    y = detrended_data\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15_base_consumption = model.predict([[15]])[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_total_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_total_consumption * 1.05)\n    return final_consumption"}
{"task_id": 92, "completion_id": 1, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, 11)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15_base = model.predict(np.array([[15]]))[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_predicted = day_15_base + day_15_fluctuation\n    final_prediction = math.ceil(day_15_predicted * 1.05)\n    return final_prediction\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]"}
{"task_id": 92, "completion_id": 2, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15_base_consumption = model.predict(np.array([[15]]))[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [110, 125, 140, 160, 180, 200, 220, 240, 260, 280]"}
{"task_id": 92, "completion_id": 3, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15_base = model.predict(np.array([[15]]))[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_prediction = day_15_base + day_15_fluctuation\n    final_prediction = math.ceil(day_15_prediction * 1.05)\n    return final_prediction\nconsumption_data = [120, 145, 160, 180, 200, 220, 240, 260, 280, 300]"}
{"task_id": 92, "completion_id": 4, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_value = consumption - fluctuation\n        detrended_data.append(detrended_value)\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    final_consumption = math.ceil(predicted_consumption_day_15 * 1.05)\n    return final_consumption\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]"}
{"task_id": 92, "completion_id": 5, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15_base_consumption = model.predict(np.array([[15]]))[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [120, 130, 150, 140, 130, 150, 160, 170, 180, 190]"}
{"task_id": 92, "completion_id": 6, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom math import ceil\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.arange(1, len(consumption_data) + 1)\n    fluctuation = 10 * np.sin(2 * PI * days / 10)\n    detrended_data = np.array(consumption_data) - fluctuation\n    detrended_days = days.reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(detrended_days, detrended_data)\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    final_consumption = ceil(predicted_consumption_day_15[0] * 1.05)\n    return final_consumption\nconsumption_data = [120, 135, 150, 165, 180, 195, 210, 225, 240, 255]"}
{"task_id": 92, "completion_id": 7, "solution": "import math\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, len(consumption_data) + 1))\n    detrended_data = np.array(detrended_data)\n    coefficients = np.polyfit(days, detrended_data, 1)\n    linear_model = np.poly1d(coefficients)\n    day_15_base_consumption = linear_model(15)\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [120, 135, 150, 165, 180, 195, 210, 225, 240, 255]"}
{"task_id": 92, "completion_id": 8, "solution": "import math\nimport numpy as np\nfrom scipy import stats\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = np.array(range(1, len(consumption_data) + 1))\n    (slope, intercept, r_value, p_value, std_err) = stats.linregress(days, detrended_data)\n    day_15_base_consumption = slope * 15 + intercept\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_total_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_total_consumption * 1.05)\n    return final_consumption\nconsumption_data = [100, 120, 130, 150, 170, 180, 200, 210, 230, 250]"}
{"task_id": 92, "completion_id": 9, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.arange(1, 11)\n    fluctuations = 10 * np.sin(2 * PI * days / 10)\n    detrended_data = np.array(consumption_data) - fluctuations\n    X = days.reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(X, detrended_data)\n    day_15 = np.array([[15]])\n    day_15_base_consumption = model.predict(day_15)[0]\n    day_15_fluctuation = 10 * np.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    return final_consumption\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 1, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 2, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy ndarrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 3, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 4, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 5, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 6, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    mae_value = np.mean(np.abs(y_true - y_pred))\n    return round(mae_value, 3)"}
{"task_id": 93, "completion_id": 7, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    abs_errors = np.abs(y_true - y_pred)\n    mean_abs_error = np.mean(abs_errors)\n    return round(mean_abs_error, 3)"}
{"task_id": 93, "completion_id": 8, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 9, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices.\n    \n    :param X: Input data of shape (batch_size, seq_length, d_model)\n    :param W_q: Query weight matrix of shape (d_model, d_k)\n    :param W_k: Key weight matrix of shape (d_model, d_k)\n    :param W_v: Value weight matrix of shape (d_model, d_v)\n    :return: Tuple of Query, Key, and Value matrices each of shape (batch_size, seq_length, d_k/d_v)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    :param Q: Query matrix of shape (batch_size, seq_length, d_k)\n    :param K: Key matrix of shape (batch_size, seq_length, d_k)\n    :param V: Value matrix of shape (batch_size, seq_length, d_v)\n    :return: Context matrix of shape (batch_size, seq_length, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.bmm(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    context = np.bmm(attention_weights, V)\n    return context\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    :param X: Input data of shape (batch_size, seq_length, d_model)\n    :param W_q: Query weight matrix of shape (n_heads * d_k, d_model)\n    :param W_k: Key weight matrix of shape (n_heads * d_k, d_model)\n    :param W_v: Value weight matrix of shape (n_heads * d_v, d_model)\n    :param n_heads: Number of attention heads\n    :return: List of output after multi-head attention of shape (batch_size, seq_length, n_heads * d_v)\n    \"\"\"\n    d_k = W_q.shape[0] // n_heads\n    d_v = W_v.shape[0] // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q = Q.reshape(Q.shape[0], Q.shape[1], n_heads, d_k).transpose(0, 2, 1, 3)\n    K = K.reshape(K.shape[0], K.shape[1], n_heads, d_k).transpose(0, 2, 1, 3)\n    V = V.reshape(V.shape[0], V.shape[1], n_heads, d_v).transpose(0, 2, 1, 3)\n    contexts = [self_attention(Q[:, i], K[:, i], V[:, i]) for i in range(n_heads)]\n    context = np.concatenate(contexts, axis=-1)\n    context = context.reshape(context.shape[0], context.shape[1], -1)\n    context_rounded = np.round(context, 4)\n    return context_rounded.tolist()"}
{"task_id": 94, "completion_id": 1, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices.\n    \n    Args:\n    X (np.ndarray): Input feature matrix of shape (batch_size, seq_length, d_model).\n    W_q (np.ndarray): Query weights matrix of shape (d_model, d_k).\n    W_k (np.ndarray): Key weights matrix of shape (d_model, d_k).\n    W_v (np.ndarray): Value weights matrix of shape (d_model, d_k).\n    \n    Returns:\n    tuple: Query, Key, and Value matrices of shapes (batch_size, seq_length, d_k).\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism for a single head.\n    \n    Args:\n    Q (np.ndarray): Query matrix of shape (batch_size, seq_length, d_k).\n    K (np.ndarray): Key matrix of shape (batch_size, seq_length, d_k).\n    V (np.ndarray): Value matrix of shape (batch_size, seq_length, d_k).\n    \n    Returns:\n    np.ndarray: Self-attention output matrix of shape (batch_size, seq_length, d_k).\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    Args:\n    X (np.ndarray): Input feature matrix of shape (batch_size, seq_length, d_model).\n    W_q (np.ndarray): Query weights matrix of shape (n_heads, d_model, d_k).\n    W_k (np.ndarray): Key weights matrix of shape (n_heads, d_model, d_k).\n    W_v (np.ndarray): Value weights matrix of shape (n_heads, d_model, d_k).\n    n_heads (int): Number of attention heads.\n    \n    Returns:\n    list: Multi-head attention output as a flattened list of shape (batch_size, seq_length, d_model).\n    \"\"\"\n    (batch_size, seq_length, d_model) = X.shape\n    d_k = W_q.shape[-1]\n    heads = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        heads.append(self_attention(Q, K, V))\n    concatenated_heads = np.concatenate(heads, axis=-1)\n    output = concatenated_heads.reshape(batch_size, seq_length, d_model)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 94, "completion_id": 2, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices.\n    \n    Parameters:\n    - X: Input data matrix of shape (seq_len, embed_dim)\n    - W_q: Query weight matrix of shape (embed_dim, embed_dim)\n    - W_k: Key weight matrix of shape (embed_dim, embed_dim)\n    - W_v: Value weight matrix of shape (embed_dim, embed_dim)\n    \n    Returns:\n    - Q: Query matrix of shape (seq_len, embed_dim)\n    - K: Key matrix of shape (seq_len, embed_dim)\n    - V: Value matrix of shape (seq_len, embed_dim)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention for a single head.\n    \n    Parameters:\n    - Q: Query matrix of shape (seq_len, embed_dim)\n    - K: Key matrix of shape (seq_len, embed_dim)\n    - V: Value matrix of shape (seq_len, embed_dim)\n    \n    Returns:\n    - Z: Output of self-attention of shape (seq_len, embed_dim)\n    \"\"\"\n    scores = Q @ K.T / np.sqrt(Q.shape[1])\n    weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    weights /= np.sum(weights, axis=1, keepdims=True)\n    Z = weights @ V\n    return Z\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention.\n    \n    Parameters:\n    - X: Input data matrix of shape (seq_len, embed_dim)\n    - W_q: Query weight matrix of shape (embed_dim, embed_dim)\n    - W_k: Key weight matrix of shape (embed_dim, embed_dim)\n    - W_v: Value weight matrix of shape (embed_dim, embed_dim)\n    - n_heads: Number of attention heads\n    \n    Returns:\n    - result: Output of multi-head attention as a list after reshaping\n    \"\"\"\n    (seq_len, embed_dim) = X.shape\n    head_dim = embed_dim // n_heads\n    multi_head_output = np.zeros((seq_len, embed_dim))\n    for i in range(n_heads):\n        W_q_head = W_q[:, i * head_dim:(i + 1) * head_dim]\n        W_k_head = W_k[:, i * head_dim:(i + 1) * head_dim]\n        W_v_head = W_v[:, i * head_dim:(i + 1) * head_dim]\n        (Q, K, V) = compute_qkv(X, W_q_head, W_k_head, W_v_head)\n        Z_head = self_attention(Q, K, V)\n        multi_head_output[:, i * head_dim:(i + 1) * head_dim] = Z_head\n    result = np.round(multi_head_output, 4).tolist()\n    return result"}
{"task_id": 94, "completion_id": 3, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices.\n    \n    Parameters:\n    - X: Input matrix of shape (N, D) where N is the number of sequences and D is the dimension of each sequence.\n    - W_q: Query weights matrix of shape (D, Dk)\n    - W_k: Key weights matrix of shape (D, Dk)\n    - W_v: Value weights matrix of shape (D, Dv)\n    \n    Returns:\n    - Q: Query matrix of shape (N, Dk)\n    - K: Key matrix of shape (N, Dk)\n    - V: Value matrix of shape (N, Dv)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    Parameters:\n    - Q: Query matrix of shape (N, Dk)\n    - K: Key matrix of shape (N, Dk)\n    - V: Value matrix of shape (N, Dv)\n    \n    Returns:\n    - Z: Output matrix after applying self-attention, of shape (N, Dv)\n    \"\"\"\n    dot_product = Q @ K.T\n    d_k = K.shape[-1]\n    scaled_dot_product = dot_product / np.sqrt(d_k)\n    attention_weights = np.exp(scaled_dot_product) / np.sum(np.exp(scaled_dot_product), axis=-1, keepdims=True)\n    Z = attention_weights @ V\n    return Z\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    Parameters:\n    - X: Input matrix of shape (N, D) where N is the number of sequences and D is the dimension of each sequence.\n    - W_q: Query weights matrix of shape (D, n_heads * Dk)\n    - W_k: Key weights matrix of shape (D, n_heads * Dk)\n    - W_v: Value weights matrix of shape (D, n_heads * Dv)\n    - n_heads: Number of attention heads\n    \n    Returns:\n    - output: Output matrix after applying multi-head attention, of shape (N, n_heads * Dv)\n    \"\"\"\n    Dk = W_q.shape[1] // n_heads\n    Dv = W_v.shape[1] // n_heads\n    heads = []\n    for i in range(n_heads):\n        W_q_i = W_q[:, i * Dk:(i + 1) * Dk]\n        W_k_i = W_k[:, i * Dk:(i + 1) * Dk]\n        W_v_i = W_v[:, i * Dv:(i + 1) * Dv]\n        (Q, K, V) = compute_qkv(X, W_q_i, W_k_i, W_v_i)\n        Z = self_attention(Q, K, V)\n        heads.append(Z)\n    Z_concat = np.concatenate(heads, axis=-1)\n    output = Z_concat.round(4).tolist()\n    return output"}
{"task_id": 94, "completion_id": 4, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    Parameters:\n    - Q: Queries tensor of shape (batch_size, seq_len, d_k).\n    - K: Keys tensor of shape (batch_size, seq_len, d_k).\n    - V: Values tensor of shape (batch_size, seq_len, d_v).\n    \n    Returns:\n    - Attention scores tensor of shape (batch_size, seq_len, d_v).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, seq_len, d_model).\n    - W_q: Weight matrix for queries of shape (d_model, d_model).\n    - W_k: Weight matrix for keys of shape (d_model, d_model).\n    - W_v: Weight matrix for values of shape (d_model, d_model).\n    - n_heads: Number of attention heads.\n    \n    Returns:\n    - Multi-head attention output as a python list.\n    \"\"\"\n    (batch_size, seq_len, d_model) = X.shape\n    d_k = d_v = d_model // n_heads\n    W_q_heads = W_q.reshape(d_model, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads, batch_size, d_model, d_k)\n    W_k_heads = W_k.reshape(d_model, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads, batch_size, d_model, d_k)\n    W_v_heads = W_v.reshape(d_model, n_heads, d_v).transpose(1, 0, 2).reshape(n_heads, batch_size, d_model, d_v)\n    Q_heads = np.einsum('ihnm,bmd->bihd', W_q_heads, X)\n    K_heads = np.einsum('ihnm,bmd->bihd', W_k_heads, X)\n    V_heads = np.einsum('ihnm,bmd->bihd', W_v_heads, X)\n    output_heads = np.stack([self_attention(Q_heads[i], K_heads[i], V_heads[i]) for i in range(n_heads)], axis=1)\n    output = np.einsum('bihd,hndm->binm', output_heads, W_q.reshape(n_heads, d_k, d_model)).reshape(batch_size, seq_len, d_model)\n    return output.round(4).tolist()"}
{"task_id": 94, "completion_id": 5, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, and Value matrices.\n    \n    :param X: Input matrix (batch_size, seq_len, embed_size)\n    :param W_q: Query weight matrix (embed_size, model_dim)\n    :param W_k: Key weight matrix (embed_size, model_dim)\n    :param W_v: Value weight matrix (embed_size, model_dim)\n    :return: Tuple of Q, K, V matrices\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute self-attention for a single head.\n    \n    :param Q: Query matrix (batch_size, seq_len, model_dim)\n    :param K: Key matrix (batch_size, seq_len, model_dim)\n    :param V: Value matrix (batch_size, seq_len, model_dim)\n    :return: Attention output (batch_size, seq_len, model_dim)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement multi-head attention.\n    \n    :param X: Input matrix (batch_size, seq_len, embed_size)\n    :param W_q: Query weight matrix (embed_size, model_dim)\n    :param W_k: Key weight matrix (embed_size, model_dim)\n    :param W_v: Value weight matrix (embed_size, model_dim)\n    :param n_heads: Number of attention heads\n    :return: Multi-head attention output as a Python list\n    \"\"\"\n    (batch_size, seq_len, embed_size) = X.shape\n    model_dim = W_q.shape[1]\n    assert model_dim % n_heads == 0, 'model_dim must be divisible by n_heads'\n    W_q_heads = W_q.reshape(embed_size, n_heads, model_dim // n_heads).transpose(1, 0, 2)\n    W_k_heads = W_k.reshape(embed_size, n_heads, model_dim // n_heads).transpose(1, 0, 2)\n    W_v_heads = W_v.reshape(embed_size, n_heads, model_dim // n_heads).transpose(1, 0, 2)\n    head_outputs = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        head_output = self_attention(Q, K, V)\n        head_outputs.append(head_output)\n    concat_output = np.concatenate(head_outputs, axis=-1)\n    result = np.round(concat_output, 4).tolist()\n    return result"}
{"task_id": 94, "completion_id": 6, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n    \n    :param X: Input data of shape (batch_size, seq_length, d_model)\n    :param W_q: Weights for Query of shape (d_model, d_k)\n    :param W_k: Weights for Key of shape (d_model, d_k)\n    :param W_v: Weights for Value of shape (d_model, d_v)\n    :return: Tuple of Q, K, V matrices\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    :param Q: Query matrix of shape (batch_size, seq_length, d_k)\n    :param K: Key matrix of shape (batch_size, seq_length, d_k)\n    :param V: Value matrix of shape (batch_size, seq_length, d_v)\n    :return: Context matrix of shape (batch_size, seq_length, d_v)\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    context_matrix = np.dot(attention_weights, V)\n    return context_matrix\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    :param X: Input data of shape (batch_size, seq_length, d_model)\n    :param W_q: Weights for Query of shape (n_heads, d_model, d_k)\n    :param W_k: Weights for Key of shape (n_heads, d_model, d_k)\n    :param W_v: Weights for Value of shape (n_heads, d_model, d_v)\n    :param n_heads: Number of attention heads\n    :return: List of output after multi-head attention and concatenation, rounded to 4 decimal places\n    \"\"\"\n    (batch_size, seq_length, d_model) = X.shape\n    d_k = W_q.shape[-1]\n    d_v = W_v.shape[-1]\n    heads = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        context_matrix = self_attention(Q, K, V)\n        heads.append(context_matrix)\n    concatenated_heads = np.concatenate(heads, axis=-1)\n    W_o = np.random.randn(n_heads * d_v, d_model)\n    output = np.dot(concatenated_heads, W_o)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 94, "completion_id": 7, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"Compute Query, Key, and Value matrices.\"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the self-attention mechanism.\"\"\"\n    d_k = K.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Compute Multi-Head Attention.\"\"\"\n    (batch_size, seq_length, embed_size) = X.shape\n    head_dim = embed_size // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q = Q.reshape(batch_size, seq_length, n_heads, head_dim).transpose(0, 2, 1, 3)\n    K = K.reshape(batch_size, seq_length, n_heads, head_dim).transpose(0, 2, 1, 3)\n    V = V.reshape(batch_size, seq_length, n_heads, head_dim).transpose(0, 2, 1, 3)\n    head_outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q[:, i, :, :], K[:, i, :, :], V[:, i, :, :])\n        head_outputs.append(head_output)\n    output = np.concatenate(head_outputs, axis=-1).transpose(0, 2, 1, 3).reshape(batch_size, seq_length, embed_size)\n    return output.round(4).tolist()"}
{"task_id": 94, "completion_id": 8, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, and Value matrices.\n    \n    :param X: Input tensor of shape (seq_len, d_model).\n    :param W_q: Query weight matrix of shape (d_model, d_k).\n    :param W_k: Key weight matrix of shape (d_model, d_k).\n    :param W_v: Value weight matrix of shape (d_model, d_v).\n    :return: Tuple of Q, K, V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    :param Q: Query matrix of shape (seq_len, d_k).\n    :param K: Key matrix of shape (seq_len, d_k).\n    :param V: Value matrix of shape (seq_len, d_v).\n    :return: Output of self-attention mechanism.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scale = 1 / np.sqrt(d_k)\n    scores = np.dot(Q, K.T) * scale\n    attention_weights = softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\ndef softmax(x: np.ndarray, axis: int=-1) -> np.ndarray:\n    \"\"\"\n    Compute the softmax of vector x in a numerically stable way.\n    \n    :param x: Input tensor.\n    :param axis: Axis to apply softmax.\n    :return: Softmax of input tensor.\n    \"\"\"\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / np.sum(e_x, axis=axis, keepdims=True)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement Multi-Head Attention.\n    \n    :param X: Input tensor of shape (seq_len, d_model).\n    :param W_q: Query weight matrix of shape (d_model, d_k).\n    :param W_k: Key weight matrix of shape (d_model, d_k).\n    :param W_v: Value weight matrix of shape (d_model, d_v).\n    :param n_heads: Number of attention heads.\n    :return: Output of Multi-Head Attention as a list.\n    \"\"\"\n    (seq_len, d_model) = X.shape\n    W_q_heads = np.array_split(W_q, n_heads, axis=1)\n    W_k_heads = np.array_split(W_k, n_heads, axis=1)\n    W_v_heads = np.array_split(W_v, n_heads, axis=1)\n    head_outputs = []\n    for i in range(n_heads):\n        (Q_head, K_head, V_head) = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        head_output = self_attention(Q_head, K_head, V_head)\n        head_outputs.append(head_output)\n    concatenated_output = np.concatenate(head_outputs, axis=-1)\n    final_output = concatenated_output.reshape(seq_len, d_model)\n    return final_output.round(4).tolist()"}
{"task_id": 94, "completion_id": 9, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices.\n\n    :param X: Input data of shape (batch_size, seq_len, d_model)\n    :param W_q: Query weight matrix of shape (d_model, d_k)\n    :param W_k: Key weight matrix of shape (d_model, d_k)\n    :param W_v: Value weight matrix of shape (d_model, d_v)\n    :return: Tuple of Query, Key, and Value matrices of shape (batch_size, seq_len, d_k/d_v)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n\n    :param Q: Query matrix of shape (batch_size, seq_len, d_k)\n    :param K: Key matrix of shape (batch_size, seq_len, d_k)\n    :param V: Value matrix of shape (batch_size, seq_len, d_v)\n    :return: Self-attention output of shape (batch_size, seq_len, d_v)\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement the multi-head attention mechanism.\n\n    :param X: Input data of shape (batch_size, seq_len, d_model)\n    :param W_q: Query weight matrix of shape (d_model, d_k * n_heads)\n    :param W_k: Key weight matrix of shape (d_model, d_k * n_heads)\n    :param W_v: Value weight matrix of shape (d_model, d_v * n_heads)\n    :param n_heads: Number of attention heads\n    :return: Multi-head attention output as a Python list\n    \"\"\"\n    (batch_size, seq_len, d_model) = X.shape\n    d_k = W_q.shape[-1] // n_heads\n    d_v = W_v.shape[-1] // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    K = K.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    V = V.reshape(batch_size, seq_len, n_heads, d_v).transpose(0, 2, 1, 3)\n    attention_heads = []\n    for i in range(n_heads):\n        attention_head = self_attention(Q[:, i, :, :], K[:, i, :, :], V[:, i, :, :])\n        attention_heads.append(attention_head)\n    attention_output = np.concatenate(attention_heads, axis=-1)\n    attention_output = attention_output.transpose(0, 2, 1)\n    attention_output = attention_output.reshape(batch_size, d_v * n_heads, seq_len).transpose(0, 2, 1)\n    attention_output = np.round(attention_output, 4).tolist()\n    return attention_output"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n = len(x)\n    n11 = sum((x_i and y_i for (x_i, y_i) in zip(x, y)))\n    n10 = sum((x_i and (not y_i) for (x_i, y_i) in zip(x, y)))\n    n01 = sum((not x_i and y_i for (x_i, y_i) in zip(x, y)))\n    n00 = n - n11 - n10 - n01\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0]"}
{"task_id": 95, "completion_id": 1, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('The input lists must have the same length.')\n    a = sum((x_i and y_i for (x_i, y_i) in zip(x, y)))\n    b = sum((x_i and (not y_i) for (x_i, y_i) in zip(x, y)))\n    c = sum((not x_i and y_i for (x_i, y_i) in zip(x, y)))\n    d = sum((not x_i and (not y_i) for (x_i, y_i) in zip(x, y)))\n    n = len(x)\n    numerator = a * d - b * c\n    denominator = ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0, 1, 0]\ny = [1, 1, 0, 1, 0, 0, 1, 0]"}
{"task_id": 95, "completion_id": 2, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n11 = sum((xi == 1 and yi == 1 for (xi, yi) in zip(x, y)))\n    n10 = sum((xi == 1 and yi == 0 for (xi, yi) in zip(x, y)))\n    n01 = sum((xi == 0 and yi == 1 for (xi, yi) in zip(x, y)))\n    n00 = sum((xi == 0 and yi == 0 for (xi, yi) in zip(x, y)))\n    n = n11 + n10 + n01 + n00\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1, 0, 1]\ny = [1, 1, 0, 1, 0, 0, 1, 0]"}
{"task_id": 95, "completion_id": 3, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    a = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    b = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    c = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    d = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    n = len(x)\n    numerator = a * d - b * c\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 4, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n = len(x)\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 5, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n = len(x)\n    n11 = sum((xi and yi for (xi, yi) in zip(x, y)))\n    n10 = sum((xi and (not yi) for (xi, yi) in zip(x, y)))\n    n01 = sum((not xi and yi for (xi, yi) in zip(x, y)))\n    n00 = sum((not xi and (not yi) for (xi, yi) in zip(x, y)))\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n10 + n11) * (n00 + n01) * (n10 + n00) * (n11 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 6, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    a = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    b = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    c = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    d = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    n = len(x)\n    numerator = a * d - b * c\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 7, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n = len(x)\n    n11 = sum((xi and yi for (xi, yi) in zip(x, y)))\n    n10 = sum((xi and (not yi) for (xi, yi) in zip(x, y)))\n    n01 = sum((not xi and yi for (xi, yi) in zip(x, y)))\n    n00 = sum((not xi and (not yi) for (xi, yi) in zip(x, y)))\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [0, 1, 1, 0, 1]\ny = [1, 0, 1, 0, 1]"}
{"task_id": 95, "completion_id": 8, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    a = sum((x_i and y_i for (x_i, y_i) in zip(x, y)))\n    b = sum((x_i and (not y_i) for (x_i, y_i) in zip(x, y)))\n    c = sum((not x_i and y_i for (x_i, y_i) in zip(x, y)))\n    d = sum((not x_i and (not y_i) for (x_i, y_i) in zip(x, y)))\n    n = len(x)\n    numerator = a * d - b * c\n    denominator = ((a + b) * (c + d) * (a + c) * (b + d)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0]"}
{"task_id": 95, "completion_id": 9, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('The input lists must have the same length.')\n    n = len(x)\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = n - n11 - n10 - n01\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0, 1, 0]\ny = [1, 1, 0, 1, 0, 0, 1, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 1, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 2, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 3, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 4, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 5, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 6, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 7, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 8, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 9, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 1, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 2, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 3, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 4, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 5, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 6, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 7, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 8, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 97, "completion_id": 9, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 1, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 2, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 3, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 4, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 5, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 6, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 7, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 8, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 9, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 30:\n        return x\n    elif x < -30:\n        return math.exp(x)\n    else:\n        return round(math.log1p(math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 1, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 2, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return x\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 3, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 30:\n        return round(x, 4)\n    elif x < -30:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log1p(math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 4, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return x\n    elif x < -20:\n        return math.exp(x)\n    else:\n        return math.log(1 + math.exp(x))"}
{"task_id": 99, "completion_id": 5, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 6, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log1p(math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 7, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 30:\n        return x\n    elif x < -30:\n        return math.log1p(math.exp(x))\n    else:\n        return math.log1p(math.exp(x))"}
{"task_id": 99, "completion_id": 8, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return x\n    elif x < -20:\n        return math.exp(x)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 9, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return x\n    elif x < -20:\n        return math.exp(x)\n    else:\n        return math.log1p(math.exp(x))"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 1, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 2, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 3, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)"}
{"task_id": 100, "completion_id": 4, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 5, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 6, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 7, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 8, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)"}
{"task_id": 100, "completion_id": 9, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    term1 = np.minimum(rhos * A, clipped_rhos * A)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(term1) - beta * kl_divergence\n    return round(grpo_obj, 6)"}
{"task_id": 101, "completion_id": 1, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    unbiased_term = rhos * A\n    clipped_term = clipped_rhos * A\n    L_pi = np.minimum(unbiased_term, clipped_term)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(L_pi) - beta * kl_divergence\n    return round(float(grpo_obj), 6)\nrhos = [1.1, 0.8, 1.5, 0.9]\nA = [0.5, -0.2, 1.0, 0.3]\npi_theta_old = [0.25, 0.25, 0.25, 0.25]\npi_theta_ref = [0.3, 0.2, 0.2, 0.3]"}
{"task_id": 101, "completion_id": 2, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    pi_theta_old = np.clip(pi_theta_old, 1e-10, 1.0)\n    pi_theta_ref = np.clip(pi_theta_ref, 1e-10, 1.0)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    L = np.minimum(rhos * A, clipped_rhos * A)\n    J = np.mean(L) - beta * np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    return round(float(J), 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.5, 1.0, -1.0]\npi_theta_old = [0.2, 0.3, 0.3, 0.2]\npi_theta_ref = [0.25, 0.35, 0.25, 0.15]"}
{"task_id": 101, "completion_id": 3, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    objective_clipped = np.minimum(rhos * A, np.clip(rhos, 1 - epsilon, 1 + epsilon) * A)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(objective_clipped) - beta * kl_divergence\n    return round(grpo_obj, 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.3, 0.2, -0.4]\npi_theta_old = [0.3, 0.2, 0.1, 0.4]\npi_theta_ref = [0.25, 0.25, 0.25, 0.25]"}
{"task_id": 101, "completion_id": 4, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    p_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    primary_term = np.minimum(rhos * A, p_clipped * A).mean()\n    kl_divergence = (pi_theta_old * np.log(pi_theta_old / pi_theta_ref)).sum()\n    grpo_obj = primary_term - beta * kl_divergence\n    return round(float(grpo_obj), 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.3, 0.7, -0.2]\npi_theta_old = [0.2, 0.3, 0.1, 0.4]\npi_theta_ref = [0.25, 0.25, 0.25, 0.25]"}
{"task_id": 101, "completion_id": 5, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rho_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    grpo_surrogate = rhos * A - beta * pi_theta_old * np.log(pi_theta_old / pi_theta_ref)\n    clipped_grpo_surrogate = np.minimum(rhos * A, rho_clipped * A) - beta * pi_theta_old * np.log(pi_theta_old / pi_theta_ref)\n    grpo_obj = np.mean(clipped_grpo_surrogate)\n    return round(grpo_obj, 6)\nrhos = [1.5, 0.8, 1.2]\nA = [0.5, -0.3, 0.2]\npi_theta_old = [0.4, 0.3, 0.3]\npi_theta_ref = [0.3, 0.3, 0.4]\nepsilon = 0.2\nbeta = 0.01"}
{"task_id": 101, "completion_id": 6, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_term = clipped_rhos * A\n    unclipped_term = rhos * A\n    policy_loss = np.minimum(clipped_term, unclipped_term)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(policy_loss) - beta * kl_divergence\n    return round(grpo_obj, 6)"}
{"task_id": 101, "completion_id": 7, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    J_GRPO = np.minimum(rhos * A, clipped_rhos * A).mean()\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    J_GRPO -= beta * kl_divergence\n    return round(J_GRPO, 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.4, 0.3, 0.2, 0.1]\npi_theta_ref = [0.45, 0.25, 0.2, 0.1]"}
{"task_id": 101, "completion_id": 8, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    term1 = clipped_rhos * A\n    term2 = pi_theta_ref * A\n    kl_divergence = (pi_theta_old * np.log(pi_theta_old / pi_theta_ref)).sum()\n    objective = np.minimum(term1, term2).mean() - beta * kl_divergence\n    return round(objective, 6)\nrhos = [1.1, 0.9, 1.05, 0.85]\nA = [0.5, -0.2, 0.7, -0.1]\npi_theta_old = [0.3, 0.4, 0.2, 0.1]\npi_theta_ref = [0.25, 0.45, 0.15, 0.15]"}
{"task_id": 101, "completion_id": 9, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    p_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon) * A\n    p = rhos * A\n    L_clip = np.minimum(p, p_clipped)\n    kl_div = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(L_clip) - beta * kl_div\n    return round(grpo_obj, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    result = x * math.sigmoid(x)\n    return round(result, 4)"}
{"task_id": 102, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x / (1 + math.exp(-x))\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * (1 / (1 + math.exp(-x)))\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x / (1 + math.exp(-x))\n    return round(swish_value, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * (alpha * (math.exp(x) - 1))\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * (alpha * (math.exp(x) - 1))\n    return round(selu_value, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 1, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 2, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 3, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 4, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 5, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 6, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 7, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 8, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 9, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_classes = len(np.unique(y))\n    B = np.zeros((num_features, num_classes))\n    y_one_hot = np.eye(num_classes)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y_one_hot * np.log(probs)) / num_samples\n        losses.append(round(loss, 4))\n        error = probs - y_one_hot\n        grad = np.dot(X.T, error) / num_samples\n        B -= learning_rate * grad\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 1, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = len(np.unique(y))\n    M = X.shape[1]\n    B = np.random.rand(C, M)\n    y_encoded = np.eye(C)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y_encoded * np.log(probs)) / len(y)\n        losses.append(round(loss, 4))\n        dscores = probs - y_encoded\n        dW = np.dot(dscores.T, X) / len(y)\n        B -= learning_rate * dW\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 2, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Compute the cross-entropy loss.\"\"\"\n    m = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(m), y_true])\n    loss = np.sum(log_likelihood) / m\n    return loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    np.random.seed(0)\n    num_classes = np.max(y) + 1\n    B = np.random.randn(X.shape[1], num_classes)\n    y_one_hot = np.eye(num_classes)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        probs = softmax(scores)\n        gradient = np.dot(X.T, probs - y_one_hot) / X.shape[0]\n        B -= learning_rate * gradient\n        loss = cross_entropy_loss(y, probs)\n        losses.append(round(loss, 4))\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 3, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, C) = X.shape\n    K = np.unique(y).size\n    y_one_hot = np.eye(K)[y]\n    B = np.random.rand(C, K)\n    losses = []\n    for _ in range(iterations):\n        Z = np.dot(X, B)\n        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n        probabilities = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n        log_probabilities = np.log(probabilities[range(N), y])\n        loss = -np.mean(log_probabilities)\n        losses.append(round(loss, 4))\n        error = probabilities - y_one_hot\n        gradient = np.dot(X.T, error) / N\n        B -= learning_rate * gradient\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 4, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_classes = len(np.unique(y))\n    Y_one_hot = np.eye(num_classes)[y]\n    B = np.random.randn(num_features, num_classes)\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores)\n        softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(Y_one_hot * np.log(softmax_scores)) / num_samples\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, softmax_scores - Y_one_hot) / num_samples\n        B -= learning_rate * gradient\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 5, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = np.unique(y).size\n    M = X.shape[1]\n    B = np.random.rand(C, M) * 0.01\n    y_one_hot = np.eye(C)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y_one_hot * np.log(probs)) / y.size\n        losses.append(round(loss, 4))\n        dscores = probs - y_one_hot\n        dB = np.dot(dscores.T, X) / y.size\n        B -= learning_rate * dB\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 6, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = np.unique(y).size\n    B = np.random.rand(M, C)\n    Y_one_hot = np.eye(C)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        softmax_probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        data_loss = -np.sum(Y_one_hot * np.log(softmax_probs)) / N\n        losses.append(round(data_loss, 4))\n        dscores = softmax_probs - Y_one_hot\n        dB = np.dot(X.T, dscores) / N\n        B -= learning_rate * dB\n    return (B.T.flatten().tolist(), losses)"}
{"task_id": 105, "completion_id": 7, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = len(np.unique(y))\n    Y = np.eye(C)[y]\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(Y * np.log(probs)) / N\n        losses.append(round(loss, 4))\n        dScores = probs - Y\n        grad = np.dot(dScores.T, X) / N\n        B -= learning_rate * grad\n    return (B.tolist(), losses)"}
{"task_id": 105, "completion_id": 8, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = np.unique(y).size\n    y_encoded = np.eye(n_classes)[y]\n    B = np.random.randn(n_features, n_classes)\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_encoded * np.log(probabilities), axis=1))\n        losses.append(round(loss, 4))\n        error = probabilities - y_encoded\n        grad = X.T @ error / n_samples\n        B -= learning_rate * grad\n    return (B.round(4).tolist(), losses)"}
{"task_id": 105, "completion_id": 9, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = len(np.unique(y))\n    Y = np.eye(C)[y]\n    B = np.zeros((M, C))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(Y * np.log(probs)) / N\n        losses.append(round(loss, 4))\n        dScores = probs - Y\n        dB = np.dot(X.T, dScores) / N\n        B -= learning_rate * dB\n    return (B.tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.c_[np.ones(X.shape[0]), X]\n    weights = np.zeros(X.shape[1])\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def compute_loss(h, y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n    for i in range(iterations):\n        z = np.dot(X, weights)\n        h = sigmoid(z)\n        gradient = np.dot(X.T, h - y) / y.size\n        weights -= learning_rate * gradient\n        loss = compute_loss(h, y)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 1, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    for _ in range(iterations):\n        y_pred = sigmoid(np.dot(X, weights))\n        gradient = np.dot(X.T, y_pred - y) / n_samples\n        weights -= learning_rate * gradient\n        loss = -np.mean(y * np.log(y_pred + 1e-08) + (1 - y) * np.log(1 - y_pred + 1e-08))\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 2, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    n_features = X.shape[1]\n    weights = np.zeros(n_features)\n    bias = 0.0\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def binary_cross_entropy(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        dw = 1 / X.shape[0] * np.dot(X.T, y_pred - y)\n        db = 1 / X.shape[0] * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), bias, loss_values)"}
{"task_id": 106, "completion_id": 3, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    weights = np.zeros(X.shape[1])\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def compute_loss(y, y_hat):\n        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        y_hat = sigmoid(z)\n        error = y_hat - y\n        gradient = np.dot(X.T, error) / len(y)\n        weights -= learning_rate * gradient\n        loss = compute_loss(y, y_hat)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 4, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    coefficients = np.zeros(X.shape[1])\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def binary_cross_entropy(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    for _ in range(iterations):\n        z = np.dot(X, coefficients)\n        predictions = sigmoid(z)\n        loss = binary_cross_entropy(y, predictions)\n        loss_values.append(round(loss, 4))\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / y.size\n        coefficients -= learning_rate * gradient\n    return (coefficients.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 5, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    weights = np.zeros(X_b.shape[1])\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def compute_loss(y_true, y_pred):\n        epsilon = 1e-15\n        return np.mean(-y_true * np.log(y_pred + epsilon) - (1 - y_true) * np.log(1 - y_pred + epsilon))\n    for iteration in range(iterations):\n        z = np.dot(X_b, weights)\n        y_pred = sigmoid(z)\n        gradients = np.dot(X_b.T, y_pred - y) / len(y)\n        weights -= learning_rate * gradients\n        loss = compute_loss(y, y_pred)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 6, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    weights = np.zeros(X.shape[1])\n    loss_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def binary_cross_entropy(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        gradients = np.dot(X.T, y_pred - y) / y.shape[0]\n        weights -= learning_rate * gradients\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid of z.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\ndef compute_loss(h: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy loss.\"\"\"\n    m = y.size\n    return -1.0 / m * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    loss_values = []\n    for _ in range(iterations):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        gradient = 1.0 / m * X.T @ (h - y)\n        theta -= learning_rate * gradient\n        loss = compute_loss(h, y)\n        loss_values.append(round(float(loss), 4))\n    return (theta.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid function to convert logits to probabilities.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(h: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy loss.\"\"\"\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    weights = np.zeros(X.shape[1])\n    loss_values = []\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        h = sigmoid(z)\n        gradient = np.dot(X.T, h - y) / y.size\n        weights -= learning_rate * gradient\n        loss = compute_loss(h, y)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 106, "completion_id": 9, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    (m, n) = X_b.shape\n    theta = np.random.randn(n, 1)\n    loss_history = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    for i in range(iterations):\n        y_pred = sigmoid(X_b.dot(theta))\n        errors = y_pred - y.reshape(-1, 1)\n        gradient = X_b.T.dot(errors) / m\n        theta -= learning_rate * gradient\n        loss = -1 / m * (y.dot(np.log(y_pred + 1e-08)) + (1 - y).dot(np.log(1 - y_pred + 1e-08)))\n        loss_history.append(round(float(loss), 4))\n    return (theta.flatten().tolist(), loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": ""}
{"task_id": 107, "completion_id": 1, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = scores + mask\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(softmax_scores, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 2, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = scores + mask\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = weights / np.sum(weights, axis=-1, keepdims=True)\n    output = np.dot(weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 3, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask, scores, -1000000000.0)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 4, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask * -1000000000.0\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 5, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = np.where(mask, scores, -np.inf)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 6, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    masked_scores = scores + mask\n    attention_weights = np.exp(masked_scores - np.max(masked_scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 7, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    dot_product = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    dot_product = dot_product + mask * -1000000000.0\n    attention_scores = np.exp(dot_product) / np.sum(np.exp(dot_product), axis=-1, keepdims=True)\n    output = np.dot(attention_scores, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 8, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n    scores = scores + mask\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(softmax_scores, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 9, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * (p + 1e-10).log10() for p in probabilities))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 1, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 2, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is measured using the concept of entropy from information theory.\n    \"\"\"\n    from collections import Counter\n    import math\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 3, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The function uses the Gini impurity as a measure of disorder.\n    \"\"\"\n    if not apples:\n        return 0.0\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    gini = 1.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        gini -= probability ** 2\n    return round(gini, 4)"}
{"task_id": 108, "completion_id": 4, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is measured using the Shannon entropy, which is 0 when all apples\n    are the same color and increases with the variety of colors.\n    \"\"\"\n    from collections import Counter\n    import math\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 5, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * p ** 0.5 for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 6, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    disorder_value = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        disorder_value -= probability * (probability * total_apples).bit_length()\n    max_disorder = (total_apples * total_apples.bit_length()).bit_length()\n    normalized_disorder = disorder_value / max_disorder if max_disorder != 0 else 0\n    return round(normalized_disorder, 4)"}
{"task_id": 108, "completion_id": 7, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * p ** 0.5 for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 8, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    disorder_value = 0.0\n    for count in color_counts.values():\n        p = count / total_apples\n        disorder_value -= p * (p if p != 0 else 0)\n    return round(disorder_value, 4)"}
{"task_id": 108, "completion_id": 9, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Args:\n    - X: A 3D numpy array of shape (batch_size, sequence_length, feature_dim).\n    - gamma: A 1D numpy array of scaling parameters of shape (feature_dim,).\n    - beta: A 1D numpy array of shifting parameters of shape (feature_dim,).\n    - epsilon: A small float for numerical stability.\n    \n    Returns:\n    - A 3D list of the normalized and scaled tensor.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized_scaled = gamma * X_normalized + beta\n    return X_normalized_scaled.round(5).tolist()"}
{"task_id": 109, "completion_id": 1, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scaling parameters of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameters of shape (feature_dim,).\n    epsilon (float): Small constant for numerical stability.\n\n    Returns:\n    list: Normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 109, "completion_id": 2, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - normalized X as a list rounded to 5 decimal places\n    \"\"\"\n    mean = X.mean(axis=2, keepdims=True)\n    variance = X.var(axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 109, "completion_id": 3, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on an input tensor X.\n    \n    Args:\n    - X: A 3D numpy array of shape (batch_size, sequence_length, feature_dim)\n    - gamma: A 1D numpy array of scaling parameters of shape (feature_dim,)\n    - beta: A 1D numpy array of shifting parameters of shape (feature_dim,)\n    - epsilon: A small constant for numerical stability\n    \n    Returns:\n    - A 3D numpy array of the same shape as X, after applying layer normalization and rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    result = np.round(X_normalized, 5).tolist()\n    return result"}
{"task_id": 109, "completion_id": 4, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - normalized_X: List representation of the normalized X rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    normalized_X = gamma * X_normalized + beta\n    return normalized_X.round(5).tolist()"}
{"task_id": 109, "completion_id": 5, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - Normalized X rounded to 5 decimal places as a list\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 109, "completion_id": 6, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on an input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: a small float value to avoid division by zero\n    \n    Returns:\n    - A list of the normalized X rounded to 5 decimal places.\n    \"\"\"\n    mu = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mu) / np.sqrt(var + epsilon)\n    X_transformed = gamma * X_normalized + beta\n    return X_transformed.round(5).tolist()"}
{"task_id": 109, "completion_id": 7, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, feature_dim)\n    - gamma: Scaling parameters of shape (feature_dim,)\n    - beta: Shifting parameters of shape (feature_dim,)\n    - epsilon: Small constant for numerical stability\n    \n    Returns:\n    - Normalized tensor of shape (batch_size, sequence_length, feature_dim)\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 109, "completion_id": 8, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - normalized_X: List representation of the normalized tensor, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    normalized_X = gamma * X_normalized + beta\n    return normalized_X.round(5).tolist()"}
{"task_id": 109, "completion_id": 9, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n    - gamma: Scaling parameters of shape (feature_dimension,).\n    - beta: Shifting parameters of shape (feature_dimension,).\n    - epsilon: A small float added to variance to avoid dividing by zero.\n    \n    Returns:\n    - Normalized X as a list rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def word_matches(ref, cand):\n        ref_counts = Counter(ref)\n        cand_counts = Counter(cand)\n        matches = 0\n        for word in cand_counts:\n            if word in ref_counts:\n                matches += min(cand_counts[word], ref_counts[word])\n        return matches\n\n    def fragmentation(ref, cand):\n        ref_chunks = []\n        cand_chunks = []\n        ref_set = set(ref)\n        cand_set = set(cand)\n        i = 0\n        while i < len(ref):\n            if ref[i] in cand_set:\n                chunk = [ref[i]]\n                j = i + 1\n                while j < len(ref) and ref[j] in cand_set:\n                    chunk.append(ref[j])\n                    j += 1\n                ref_chunks.append(chunk)\n                i = j\n            else:\n                i += 1\n        i = 0\n        while i < len(cand):\n            if cand[i] in ref_set:\n                chunk = [cand[i]]\n                j = i + 1\n                while j < len(cand) and cand[j] in ref_set:\n                    chunk.append(cand[j])\n                    j += 1\n                cand_chunks.append(chunk)\n                i = j\n            else:\n                i += 1\n        return 1 - (len(cand_chunks) - 1) / max(len(ref_chunks) - 1, 1)\n    ref = reference.split()\n    cand = candidate.split()\n    matches = word_matches(ref, cand)\n    if not matches:\n        return 0.0\n    precision = matches / len(cand)\n    recall = matches / len(ref)\n    f_mean = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    frag = fragmentation(ref, cand)\n    score = (1 - gamma) * f_mean + gamma * frag\n    score = score ** alpha\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import combinations\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def _tokenize(s):\n        return s.split()\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    overlap = sum((ref_counter & cand_counter).values())\n    ref_len = len(ref_tokens)\n    cand_len = len(cand_tokens)\n    precision = overlap / cand_len if cand_len > 0 else 0\n    recall = overlap / ref_len if ref_len > 0 else 0\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    breaks = 0\n    for i in range(len(cand_tokens) - 1):\n        if cand_tokens[i] + ' ' + cand_tokens[i + 1] not in reference:\n            breaks += 1\n    frag_penalty = (1 - gamma) * (breaks / len(cand_tokens)) if len(cand_tokens) > 0 else 0\n    meteor = (1 - alpha) * f_mean - alpha * frag_penalty\n    meteor = max(0, min(1, meteor))\n    return round(meteor, 3)\nreference = 'the cat sat on the mat'\ncandidate = 'the cat laid on the mat'"}
{"task_id": 110, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def get_chunks(sequence):\n        chunks = set()\n        for i in range(len(sequence)):\n            for j in range(i + 1, len(sequence) + 1):\n                chunks.add(tuple(sequence[i:j]))\n        return chunks\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n    overlaps = sum((ref_counter & cand_counter).values())\n    ref_len = len(ref_words)\n    cand_len = len(cand_words)\n    precision = overlaps / cand_len if cand_len > 0 else 0\n    recall = overlaps / ref_len if ref_len > 0 else 0\n    if precision + recall > 0:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    else:\n        f_mean = 0\n    ref_chunks = get_chunks(ref_words)\n    cand_chunks = get_chunks(cand_words)\n    chunk_matches = ref_chunks.intersection(cand_chunks)\n    matches = sum((len(chunk) for chunk in chunk_matches))\n    possible_matches = sum((len(chunk) for chunk in ref_chunks))\n    frag_frac = 0 if matches == 0 else 1 - matches / possible_matches\n    frag_penalty = gamma * max(0, 1 - frag_frac ** alpha)\n    final_score = f_mean * (1 - frag_penalty)\n    return round(final_score, 3)"}
{"task_id": 110, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import permutations\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def get_ngrams(sequence, n=1):\n        return [' '.join(sequence[i:i + n]) for i in range(len(sequence) - n + 1)]\n\n    def get_chunks(sequence):\n        chunks = []\n        for n in range(1, 5):\n            chunks.extend(get_ngrams(sequence, n))\n        return chunks\n\n    def compute_fragmentation(ref_chunks, cand_chunks):\n        matches = 0\n        for chunk in cand_chunks:\n            if chunk in ref_chunks:\n                matches += 1\n                ref_chunks.remove(chunk)\n        return matches\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    ref_chunks = get_chunks(ref_tokens)\n    cand_chunks = get_chunks(cand_tokens)\n    chunk_matches = compute_fragmentation(ref_chunks, cand_chunks)\n    num_chunks = len(ref_chunks)\n    ref_len = len(ref_tokens)\n    cand_len = len(cand_tokens)\n    unigram_counter_ref = Counter(ref_tokens)\n    unigram_counter_cand = Counter(cand_tokens)\n    unigram_matches = sum((min(count, unigram_counter_cand[token]) for (token, count) in unigram_counter_ref.items()))\n    precision = unigram_matches / cand_len if cand_len > 0 else 0\n    recall = unigram_matches / ref_len if ref_len > 0 else 0\n    if precision + recall > 0:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    else:\n        f_mean = 0\n    fragmentation_penalty = gamma * (chunk_matches / num_chunks if num_chunks > 0 else 0)\n    meteor = (1 - alpha) * f_mean + alpha * fragmentation_penalty\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def get_ngrams(sequence, n):\n        return [tuple(sequence[i:i + n]) for i in range(len(sequence) - n + 1)]\n\n    def exact_match(reference, candidate):\n        ref_tokens = reference.split()\n        can_tokens = candidate.split()\n        ref_counter = Counter(ref_tokens)\n        can_counter = Counter(can_tokens)\n        match = 0\n        for word in ref_counter:\n            match += min(ref_counter[word], can_counter[word])\n        return match\n\n    def stem_match(reference, candidate):\n        from nltk.stem.porter import PorterStemmer\n        ps = PorterStemmer()\n        ref_tokens = reference.split()\n        can_tokens = candidate.split()\n        ref_stem = [ps.stem(word) for word in ref_tokens]\n        can_stem = [ps.stem(word) for word in can_tokens]\n        ref_counter = Counter(ref_stem)\n        can_counter = Counter(can_stem)\n        match = 0\n        for word in ref_counter:\n            match += min(ref_counter[word], can_counter[word])\n        return match\n\n    def fragmentation(reference, candidate):\n        ref_tokens = reference.split()\n        can_tokens = candidate.split()\n        if len(can_tokens) == 0:\n            return 0\n        chunks = 0\n        hits = 0\n        i = 0\n        for j in range(len(can_tokens)):\n            if can_tokens[j] in ref_tokens[i:]:\n                i = ref_tokens.index(can_tokens[j], i) + 1\n                hits += 1\n                if j == len(can_tokens) - 1 or can_tokens[j + 1] not in ref_tokens[i:]:\n                    chunks += 1\n        if hits == 0:\n            return 0\n        penalty = 1 - (chunks / hits) ** gamma\n        return penalty\n    exact_match_count = exact_match(reference, candidate)\n    stem_match_count = stem_match(reference, candidate)\n    ref_length = len(reference.split())\n    can_length = len(candidate.split())\n    precision = (exact_match_count + stem_match_count) / can_length\n    recall = (exact_match_count + stem_match_count) / ref_length\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    chunk_penalty = fragmentation(reference, candidate)\n    meteor = alpha * f_mean + (1 - alpha) * chunk_penalty\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def get_chunks(words):\n        chunks = []\n        ch = []\n        for w in words:\n            ch.append(w)\n            if not any((w.isdigit() for w in ch[-3:])):\n                chunks.append(' '.join(ch))\n                ch = []\n        if ch:\n            chunks.append(' '.join(ch))\n        return chunks\n\n    def fragment_penalty(candidate_chunks, reference_chunks):\n        candidate_chunks = get_chunks(candidate_chunks)\n        reference_chunks = get_chunks(reference_chunks)\n        candidate_chunks = set(candidate_chunks)\n        reference_chunks = set(reference_chunks)\n        common_chunks = candidate_chunks.intersection(reference_chunks)\n        fp = 1 - len(common_chunks) / max(len(candidate_chunks), len(reference_chunks))\n        return fp\n\n    def matching_and_stats(reference, candidate):\n        reference_counter = Counter(reference)\n        candidate_counter = Counter(candidate)\n        matching = sum((reference_counter & candidate_counter).values())\n        precision = matching / len(candidate) if len(candidate) > 0 else 0\n        recall = matching / len(reference) if len(reference) > 0 else 0\n        return (matching, precision, recall)\n    reference = reference.split()\n    candidate = candidate.split()\n    (matching, precision, recall) = matching_and_stats(reference, candidate)\n    if precision == 0 or recall == 0:\n        return 0.0\n    precision_x_recall = precision * recall\n    beta_square = beta ** 2\n    numerator = (1 + beta_square) * precision_x_recall\n    denominator = beta_square * precision + recall\n    if denominator == 0:\n        f_mean = 0\n    else:\n        f_mean = numerator / denominator\n    fp = fragment_penalty(candidate, reference)\n    final_score = (1 - gamma) * f_mean - gamma * fp\n    return round(final_score, 3)"}
{"task_id": 110, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    matches = 0\n    for word in cand_counter:\n        matches += min(ref_counter[word], cand_counter[word])\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    if precision + recall > 0:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    else:\n        f_mean = 0\n    chencherry = compute_chencherry(ref_tokens, cand_tokens)\n    if len(cand_tokens) > 0:\n        fragmentation_penalty = gamma * chencherry / len(cand_tokens)\n    else:\n        fragmentation_penalty = 0\n    meteor = (1 - alpha) * f_mean - alpha * fragmentation_penalty\n    return round(meteor, 3)\ndef compute_chencherry(ref_tokens, cand_tokens):\n    alignment = align(ref_tokens, cand_tokens)\n    ref_chunks = get_chunks(ref_tokens, alignment)\n    cand_chunks = get_chunks(cand_tokens, alignment)\n    changes = 0\n    for (ref_chunk, cand_chunk) in zip(ref_chunks, cand_chunks):\n        if ref_chunk != cand_chunk:\n            changes += 1\n    return changes\ndef align(ref_tokens, cand_tokens):\n    (m, n) = (len(ref_tokens), len(cand_tokens))\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if ref_tokens[i - 1] == cand_tokens[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n    alignment = [-1] * n\n    (i, j) = (m, n)\n    while i > 0 and j > 0:\n        if ref_tokens[i - 1] == cand_tokens[j - 1]:\n            alignment[j - 1] = i - 1\n            i -= 1\n            j -= 1\n        elif dp[i - 1][j] > dp[i][j - 1]:\n            i -= 1\n        else:\n            j -= 1\n    return alignment\ndef get_chunks(tokens, alignment):\n    chunks = []\n    current_chunk = []\n    last_aligned = -1\n    for (j, aligned_index) in enumerate(alignment):\n        if aligned_index != -1:\n            if aligned_index > last_aligned:\n                current_chunk.append(j)\n            else:\n                if current_chunk:\n                    chunks.append(current_chunk)\n                current_chunk = [j]\n            last_aligned = aligned_index\n    if current_chunk:\n        chunks.append(current_chunk)\n    return len(chunks)"}
{"task_id": 110, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    can_tokens = candidate.split()\n    ref_counter = Counter(ref_tokens)\n    can_counter = Counter(can_tokens)\n    matches = 0\n    for word in can_counter:\n        matches += min(can_counter[word], ref_counter[word])\n    prec = matches / len(can_tokens) if len(can_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    if prec + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (prec * recall) / (beta ** 2 * prec + recall)\n\n    def get_chunks(tokens):\n        chunks = []\n        start = 0\n        for i in range(1, len(tokens)):\n            if tokens[i] != tokens[i - 1] + 1:\n                chunks.append(tokens[start:i])\n                start = i\n        chunks.append(tokens[start:])\n        return chunks\n    aligned_chunks = []\n    for chunk in get_chunks(sorted([i for (i, word) in enumerate(can_tokens) if word in ref_tokens])):\n        if any((can_tokens[i] in ref_tokens for i in chunk)):\n            aligned_chunks.append(chunk)\n    fragmentation = len(aligned_chunks) / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    fragmentation_penalty = gamma * np.exp(-fragmentation)\n    meteor = (1 - alpha) * f_mean + alpha * fragmentation_penalty\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def stem(word):\n        return word.lower().rstrip('inges')\n\n    def get_tokens(sequence):\n        return sequence.split()\n\n    def word_matches(ref_tokens, cand_tokens):\n        ref_counts = Counter(ref_tokens)\n        cand_counts = Counter(cand_tokens)\n        matches = 0\n        for (word, count) in ref_counts.items():\n            if word in cand_counts:\n                matches += min(count, cand_counts[word])\n        return matches\n\n    def word_order_fragmentation(ref_tokens, cand_tokens):\n        ref_stems = [stem(word) for word in ref_tokens]\n        cand_stems = [stem(word) for word in cand_tokens]\n        match_counts = Counter(ref_stems)\n        match_indices = []\n        for (i, word) in enumerate(cand_stems):\n            if word in match_counts and match_counts[word] > 0:\n                match_indices.append(i)\n                match_counts[word] -= 1"}
{"task_id": 110, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\nfrom nltk.translate.meteor_score import meteor_score as nltk_meteor\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    match_count = sum((ref_counter & cand_counter).values())\n    prec = match_count / len(cand_tokens) if cand_tokens else 0\n    rec = match_count / len(ref_tokens) if ref_tokens else 0\n    if prec + rec == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (prec * rec) / (beta ** 2 * prec + rec)\n    chunk_count = len(list(groupby(zip(ref_tokens, cand_tokens), lambda x: x[0] == x[1])))\n    penalty = gamma * np.exp(-beta * ((len(ref_tokens) + len(cand_tokens)) / chunk_count - 1))\n    meteor = (1 - alpha) * f_mean + alpha * penalty\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    :param joint_counts: int, the number of times both events occur together\n    :param total_counts_x: int, the total number of times event X occurs\n    :param total_counts_y: int, the total number of times event Y occurs\n    :param total_samples: int, the total number of samples\n    :return: float, the PMI value rounded to 3 decimal places\n    \"\"\"\n    if joint_counts == 0:\n        return 0.0\n    expected_joint_counts = total_counts_x / total_samples * (total_counts_y / total_samples)\n    pmi = np.log2(joint_counts / expected_joint_counts)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 1, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    - joint_counts: int, the number of times both events X and Y occur together.\n    - total_counts_x: int, the number of times event X occurs.\n    - total_counts_y: int, the number of times event Y occurs.\n    - total_samples: int, the total number of samples.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 2, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    - joint_counts: int, the number of times both events X and Y occur together.\n    - total_counts_x: int, the total number of times event X occurs.\n    - total_counts_y: int, the total number of times event Y occurs.\n    - total_samples: int, the total number of samples or occurrences.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_count = total_counts_x / total_samples * (total_counts_y / total_samples)\n    if expected_joint_count == 0 or joint_counts == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(joint_counts / expected_joint_count)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 3, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    - joint_counts: int, the joint occurrence count of the two events.\n    - total_counts_x: int, the total occurrence count of event X.\n    - total_counts_y: int, the total occurrence count of event Y.\n    - total_samples: int, the total number of samples.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 4, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n\n    Parameters:\n    joint_counts (int): The joint occurrence count of the two events.\n    total_counts_x (int): The total occurrence count of event X.\n    total_counts_y (int): The total occurrence count of event Y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0:\n        return 0.0\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 5, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    :param joint_counts: int, the joint occurrence count of the two events\n    :param total_counts_x: int, the total occurrence count of event X\n    :param total_counts_y: int, the total occurrence count of event Y\n    :param total_samples: int, the total number of samples\n    :return: float, the PMI value rounded to 3 decimal places\n    \"\"\"\n    expected_joint_count = total_counts_x * total_counts_y / total_samples\n    if expected_joint_count == 0 or joint_counts == 0:\n        return 0.0\n    pmi = np.log2(joint_counts / expected_joint_count)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 6, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for two events.\n\n    Parameters:\n    - joint_counts: int, the number of times both events occur together.\n    - total_counts_x: int, the total number of times event X occurs.\n    - total_counts_y: int, the total number of times event Y occurs.\n    - total_samples: int, the total number of samples or events considered.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 7, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    - joint_counts: int, the number of times both events occur together.\n    - total_counts_x: int, the number of times event X occurs.\n    - total_counts_y: int, the number of times event Y occurs.\n    - total_samples: int, the total number of samples.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_counts = total_counts_x / total_samples * (total_counts_y / total_samples)\n    if expected_joint_counts == 0 or joint_counts == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(joint_counts / expected_joint_counts)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 8, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events occur together.\n    - total_counts_x: int, the total number of times event X occurs.\n    - total_counts_y: int, the total number of times event Y occurs.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = float('-inf')\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 9, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) for given counts.\n\n    Parameters:\n    - joint_counts: int, the joint occurrence count of events X and Y.\n    - total_counts_x: int, the total occurrence count of event X.\n    - total_counts_y: int, the total occurrence count of event Y.\n    - total_samples: int, the total number of samples.\n\n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 1, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 2, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 3, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 4, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 5, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 6, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 7, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 8, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 9, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = a2 + x\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()"}
{"task_id": 113, "completion_id": 1, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    h1 = np.maximum(0, np.dot(x, w1))\n    h2 = np.maximum(0, np.dot(h1, w2))\n    output = np.maximum(0, h2 + x)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\noutput = residual_block(x, w1, w2)"}
{"task_id": 113, "completion_id": 2, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = x + a2\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])"}
{"task_id": 113, "completion_id": 3, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    z3 = z2 + x\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nw2 = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])"}
{"task_id": 113, "completion_id": 4, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = a2 + x\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])"}
{"task_id": 113, "completion_id": 5, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    output = np.maximum(0, a2 + x)\n    return output.round(4).tolist()\nx = np.array([0.5, -0.2, 0.3])\nw1 = np.array([[0.1, -0.1, 0.2], [-0.2, 0.1, 0.3], [0.1, 0.2, -0.1]])\nw2 = np.array([[0.3, 0.2, -0.1], [-0.1, 0.3, 0.2], [0.2, -0.1, 0.3]])\noutput = residual_block(x, w1, w2)"}
{"task_id": 113, "completion_id": 6, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    assert x.shape[1] == w1.shape[0], 'Input dimension must match the first weight layer dimensions'\n    assert w1.shape[1] == w2.shape[0], 'The number of columns in the first weight layer must match the number of rows in the second weight layer'\n    z1 = np.matmul(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.matmul(a1, w2)\n    a2 = np.maximum(0, z2)\n    residue = a2 + x\n    output = np.maximum(0, residue)\n    return output.round(4).tolist()"}
{"task_id": 113, "completion_id": 7, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with a shortcut connection using NumPy.\n    \n    Parameters:\n    - x: np.ndarray, the input array.\n    - w1: np.ndarray, the weights for the first layer.\n    - w2: np.ndarray, the weights for the second layer.\n    \n    Returns:\n    - list: The output of the residual block after rounding to 4 decimal places.\n    \"\"\"\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = a2 + x\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()"}
{"task_id": 113, "completion_id": 8, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    shortcut = x + a2\n    output = np.maximum(0, shortcut)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])"}
{"task_id": 113, "completion_id": 9, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = a2 + x\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array representing feature maps.\n    \n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D NumPy array of shape (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 1, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the\n                average of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled"}
{"task_id": 114, "completion_id": 2, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Args:\n    x (np.ndarray): Input 3D array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D NumPy array of shape (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 3, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array with shape (height, width, channels).')\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output"}
{"task_id": 114, "completion_id": 4, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output"}
{"task_id": 114, "completion_id": 5, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input array must have 3 dimensions (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 6, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    x = np.asarray(x)\n    if x.ndim != 3:\n        raise ValueError('Input array must have 3 dimensions (height, width, channels).')\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output"}
{"task_id": 114, "completion_id": 7, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) with the average of each channel.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 8, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the\n                average of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 9, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Args:\n    x (np.ndarray): Input feature maps of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D NumPy array of shape (height, width, channels).')\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (batch_size, channels, height, width) = X.shape\n    X_reshaped = X.reshape(batch_size, channels, -1)\n    mean = X_reshaped.mean(axis=0, keepdims=True)\n    var = X_reshaped.var(axis=0, keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(batch_size, channels, height, width)\n    X_normalized = gamma.reshape(1, channels, 1, 1) * X_normalized + beta.reshape(1, channels, 1, 1)\n    return X_normalized.round(4).tolist()"}
{"task_id": 115, "completion_id": 1, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    x_normalized = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma.reshape(1, -1, 1, 1) * x_normalized + beta.reshape(1, -1, 1, 1)\n    return out.round(4).tolist()"}
{"task_id": 115, "completion_id": 2, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in BCHW format.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, channels, height, width)\n    - gamma: Scale parameters of shape (channels,)\n    - beta: Shift parameters of shape (channels,)\n    - epsilon: Small constant for numerical stability\n    \n    Returns:\n    - Normalized and scaled tensor as a list rounded to 4 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized_scaled = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    return X_normalized_scaled.round(4).tolist()"}
{"task_id": 115, "completion_id": 3, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format.\n    \n    Parameters:\n    - X: np.ndarray, input tensor of shape (batch, channels, height, width)\n    - gamma: np.ndarray, scale parameters of shape (channels,)\n    - beta: np.ndarray, shift parameters of shape (channels,)\n    - epsilon: float, a small value added to the variance for numerical stability\n    \n    Returns:\n    - normalized and transformed output tensor as a list rounded to 4 decimal places\n    \"\"\"\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(X.shape[1], -1)\n    mean = X_reshaped.mean(axis=1, keepdims=True)\n    var = X_reshaped.var(axis=1, keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(X.shape[1], X.shape[0], X.shape[2], X.shape[3]).transpose(1, 0, 2, 3)\n    Y = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return Y.round(4).tolist()"}
{"task_id": 115, "completion_id": 4, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    return X_normalized.round(4).tolist()"}
{"task_id": 115, "completion_id": 5, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    return X_normalized.round(4).tolist()"}
{"task_id": 115, "completion_id": 6, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma[:, np.newaxis, np.newaxis] * X_hat + beta[:, np.newaxis, np.newaxis]\n    return Y.round(4).tolist()"}
{"task_id": 115, "completion_id": 7, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (batch_size, channels, height, width) = X.shape\n    X_reshaped = X.reshape(batch_size, channels, -1)\n    mean = np.mean(X_reshaped, axis=(0, 2), keepdims=True)\n    var = np.var(X_reshaped, axis=(0, 2), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(batch_size, channels, height, width)\n    Y = gamma.reshape(1, channels, 1, 1) * X_normalized + beta.reshape(1, channels, 1, 1)\n    return Y.round(4).tolist()"}
{"task_id": 115, "completion_id": 8, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized_scaled = gamma * X_normalized + beta\n    return X_normalized_scaled.round(4).tolist()"}
{"task_id": 115, "completion_id": 9, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (batch_size, channels, height, width) = X.shape\n    X_reshaped = X.reshape(batch_size, channels, -1)\n    mean = np.mean(X_reshaped, axis=(0, 2), keepdims=True)\n    var = np.var(X_reshaped, axis=(0, 2), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(batch_size, channels, height, width)\n    X_normalized = gamma[:, np.newaxis, np.newaxis] * X_normalized + beta[:, np.newaxis, np.newaxis]\n    return X_normalized.round(4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 1, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 2, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 3, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 4, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 5, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which to evaluate the derivative.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 6, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at the given point, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 7, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    - c: Coefficient of the polynomial term.\n    - x: The point at which to evaluate the derivative.\n    - n: Exponent of the polynomial term.\n    \n    Returns:\n    - The value of the derivative at the point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 8, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 9, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at the given point, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    basis = []\n    for v in vectors:\n        w = v.astype(float)\n        for b in basis:\n            w -= np.dot(w, b) * b\n        if np.linalg.norm(w) > tol:\n            w = w / np.linalg.norm(w)\n            basis.append(w)\n    return [b.tolist() for b in np.round(basis, 4)]\nvectors = [[1, 0], [1, 1], [2, 2]]"}
{"task_id": 117, "completion_id": 1, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    V = np.array(vectors, dtype=float)\n    Q = []\n    for v in V:\n        u = v.copy()\n        for q in Q:\n            u -= np.dot(u, q) * q\n        norm_u = np.linalg.norm(u)\n        if norm_u > tol:\n            q = u / norm_u\n            Q.append(q)\n    Q_rounded = [q.round(4).tolist() for q in Q]\n    return Q_rounded\nvectors = [[1, 0], [1, 1], [0, 1]]"}
{"task_id": 117, "completion_id": 2, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_vectors = []\n    for v in vectors:\n        for u in orthonormal_vectors:\n            v -= np.dot(v, u) * u\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            orthonormal_vectors.append(v / norm)\n    orthonormal_vectors = [np.round(u, 4).tolist() for u in orthonormal_vectors]\n    return orthonormal_vectors\nvectors = [[1, 0], [1, 1]]"}
{"task_id": 117, "completion_id": 3, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_basis = []\n    for v in vectors:\n        for u in orthonormal_basis:\n            v -= np.dot(v, u) * u\n        if np.linalg.norm(v) > tol:\n            u = v / np.linalg.norm(v)\n            orthonormal_basis.append(u)\n    orthonormal_basis = [u.round(4).tolist() for u in orthonormal_basis]\n    return orthonormal_basis\nvectors = [[1, 2], [3, 4], [5, 6]]"}
{"task_id": 117, "completion_id": 4, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_vectors = []\n    for v in vectors:\n        u = np.array(v, dtype=float)\n        for orthonormal in orthonormal_vectors:\n            u -= np.dot(u, orthonormal) * orthonormal\n        if np.linalg.norm(u) > tol:\n            u = u / np.linalg.norm(u)\n            orthonormal_vectors.append(u)\n    orthonormal_vectors = [np.round(u, 4).tolist() for u in orthonormal_vectors]\n    return orthonormal_vectors\nvectors = [[1, 0], [1, 1], [0, 1]]"}
{"task_id": 117, "completion_id": 5, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthogonal_vectors = []\n    for v in vectors:\n        proj_sum = np.zeros_like(v)\n        for u in orthogonal_vectors:\n            proj_sum += np.dot(v, u) / np.dot(u, u) * u\n        v_hat = v - proj_sum\n        if np.linalg.norm(v_hat) > tol:\n            orthogonal_vectors.append(v_hat)\n    orthonormal_vectors = [u / np.linalg.norm(u) for u in orthogonal_vectors]\n    return [np.round(u, 4).tolist() for u in orthonormal_vectors]\nvectors = [[1, 2], [4, 5], [7, 8]]"}
{"task_id": 117, "completion_id": 6, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=float)\n        for u in basis:\n            proj = np.dot(v, u) * u\n            v -= proj\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            u = v / norm\n            basis.append(u)\n    orthonormal_basis = [u.round(4).tolist() for u in basis]\n    return orthonormal_basis\nvectors = [[1, 0], [1, 1], [2, 2]]"}
{"task_id": 117, "completion_id": 7, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_vectors = []\n    for v in vectors:\n        for u in orthonormal_vectors:\n            v = v - np.dot(v, u) * u\n        if np.linalg.norm(v) > tol:\n            v = v / np.linalg.norm(v)\n            orthonormal_vectors.append(v)\n    orthonormal_basis_list = [vec.round(4).tolist() for vec in orthonormal_vectors]\n    return orthonormal_basis_list"}
{"task_id": 117, "completion_id": 8, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    v = np.array(vectors, dtype=float)\n    u = []\n    for i in range(v.shape[0]):\n        vi = v[i]\n        for ui in u:\n            vi -= np.dot(vi, ui) / np.dot(ui, ui) * ui\n        if np.linalg.norm(vi) > tol:\n            u.append(vi / np.linalg.norm(vi))\n    return [ui.tolist() for ui in u]"}
{"task_id": 117, "completion_id": 9, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    V = np.array(vectors, dtype=float)\n    U = []\n    for v in V:\n        u = v.copy()\n        for ui in U:\n            projection = np.dot(u, ui) * ui\n            u -= projection\n        if np.linalg.norm(u) > tol:\n            u /= np.linalg.norm(u)\n            U.append(u)\n    return [u.round(4).tolist() for u in U]\nvectors = [[1, 1], [1, 0], [0, 1]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 1, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3-dimensional vectors.\n\n    Parameters:\n    a (list or np.array): The first 3D vector.\n    b (list or np.array): The second 3D vector.\n\n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return result.round(4).tolist()"}
{"task_id": 118, "completion_id": 2, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return result.round(4).tolist()"}
{"task_id": 118, "completion_id": 3, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return result.round(4).tolist()"}
{"task_id": 118, "completion_id": 4, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()"}
{"task_id": 118, "completion_id": 5, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    result_rounded = np.round(result, 4).tolist()\n    return result_rounded"}
{"task_id": 118, "completion_id": 6, "solution": "import numpy as np\ndef cross_product(a, b):\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cross_prod = np.cross(a_np, b_np)\n    result = np.round(cross_prod, 4).tolist()\n    return result"}
{"task_id": 118, "completion_id": 7, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 8, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 9, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    result_rounded = np.round(result, 4).tolist()\n    return result_rounded"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    n = len(b)\n    for i in range(n):\n        A_i = np.copy(A)\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()"}
{"task_id": 119, "completion_id": 1, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix.')\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = round(det_A_i / det_A, 4)\n        x.append(x_i)\n    return np.array(x).tolist()"}
{"task_id": 119, "completion_id": 2, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(len(b)):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 119, "completion_id": 3, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Coefficient matrix A must be square.')\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 119, "completion_id": 4, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 119, "completion_id": 5, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 119, "completion_id": 6, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    n = len(b)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()"}
{"task_id": 119, "completion_id": 7, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded"}
{"task_id": 119, "completion_id": 8, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    x = []\n    n = len(b)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 119, "completion_id": 9, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    return np.round(x, 4).tolist()\nA = [[3, 1], [1, 2]]\nb = [9, 8]"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 1, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 2, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 3, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 4, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else np.inf\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 5, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 6, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else 0.0\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 7, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 8, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 9, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    if not np.isclose(np.sum(p), 1.0) or not np.isclose(np.sum(q), 1.0):\n        return 0.0\n    bhattacharyya_coefficient = np.sum(np.sqrt(p * q))\n    distance = -np.log(bhattacharyya_coefficient)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 1, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 2, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 3, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 4, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 5, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 6, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 7, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 8, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 9, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        G = 0\n        T = len(episode)\n        for t in reversed(range(T)):\n            (s, a, r) = episode[t]\n            G = r + G if t == T - 1 else rewards[t + 1] + G\n            softmax_probs = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            grad_log_policy = np.eye(num_actions)[a] - softmax_probs\n            gradient[s, :] += grad_log_policy * G\n    gradient /= len(episodes)\n    return gradient.round(4).tolist()"}
{"task_id": 122, "completion_id": 1, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = sum((reward for (_, _, reward) in episode))\n        for (state, action, reward) in episode:\n            probs = np.exp(theta[state, :])\n            probs /= np.sum(probs)\n            grad = np.zeros(num_actions)\n            grad[action] = 1 - probs[action]\n            grad *= probs\n            gradient[state, :] += G * grad\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 122, "completion_id": 2, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        T = len(episode)\n        G = np.zeros(T, dtype=float)\n        cumulative_return = 0.0\n        for t in reversed(range(T)):\n            cumulative_return += episode[t][2]\n            G[t] = cumulative_return\n        for t in range(T):\n            (s, a, _) = episode[t]\n            probabilities = np.exp(theta[s, :]) / np.exp(theta[s, :]).sum()\n            for a_ in range(num_actions):\n                if a_ == a:\n                    gradient[s, a_] += G[t] * (probabilities[a_] - 1)\n                else:\n                    gradient[s, a_] -= G[t] * probabilities[a_]\n    gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [[(0, 0, 1.0), (1, 1, 1.0)], [(1, 0, -1.0), (0, 1, -1.0)]]"}
{"task_id": 122, "completion_id": 3, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        for t in range(len(episode) - 1, -1, -1):\n            (state_t, action_t, reward_t) = episode[t]\n            G = 0.99 * G + reward_t\n            action_probs = np.exp(theta[state_t, :]) / np.sum(np.exp(theta[state_t, :]))\n            grad_log_pi = np.zeros(num_actions)\n            grad_log_pi[action_t] = 1 - action_probs[action_t]\n            grad_log_pi -= action_probs\n            gradient[state_t, :] += G * grad_log_pi\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    gradient_list = gradient.tolist()\n    return gradient_list"}
{"task_id": 122, "completion_id": 4, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=float)\n    for episode in episodes:\n        G = 0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            G += reward\n            logits = theta[state, :]\n            softmax_probs = np.exp(logits - np.max(logits)) / np.sum(np.exp(logits - np.max(logits)))\n            grad = np.zeros(num_actions)\n            grad[action] = 1 - softmax_probs[action]\n            grad *= softmax_probs\n            grad *= G\n            gradient[state, :] += grad\n    average_gradient = gradient / len(episodes)\n    result = np.round(average_gradient, 4).tolist()\n    return result\ntheta = np.array([[0.1, -0.1, 0.2], [-0.5, 0.3, 0.4], [0.0, 0.0, 0.0]])\nepisodes = [[(0, 1, 1.0), (1, 2, 1.0), (2, 0, -1.0)], [(0, 2, 1.0), (1, 1, 1.0), (2, 0, 1.0)]]"}
{"task_id": 122, "completion_id": 5, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        for t in range(len(episode) - 1, -1, -1):\n            (state, action, reward) = episode[t]\n            G += reward\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            grad_log_policy = np.zeros(num_actions)\n            grad_log_policy[action] = 1 / probs[action] - 1\n            gradient[state, :] += grad_log_policy * G\n    average_gradient = gradient / len(episodes)\n    return np.round(average_gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 6, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        returns = []\n        for (state, action, reward) in reversed(episode):\n            G = reward + G\n            returns.append((state, action, G))\n        returns.reverse()\n        for (state, action, G) in returns:\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            for a in range(num_actions):\n                if a == action:\n                    gradient[state, a] += G * (1 - probs[a])\n                else:\n                    gradient[state, a] -= G * probs[a]\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 122, "completion_id": 7, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n\n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n    (num_states, num_actions) = theta.shape\n    policy_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        for t in range(len(episode) - 1, -1, -1):\n            (state, action, reward) = episode[t]\n            G = reward + G\n            probs = softmax(theta[state, :])\n            grad_log_policy = np.zeros(num_actions)\n            grad_log_policy[action] = 1 - probs[action]\n            policy_gradient[state, :] += grad_log_policy * G\n    policy_gradient /= len(episodes)\n    policy_gradient = np.round(policy_gradient, 4).tolist()\n    return policy_gradient"}
{"task_id": 122, "completion_id": 8, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=float)\n    for episode in episodes:\n        G = 0\n        for t in range(len(episode) - 1, -1, -1):\n            (state, action, reward) = episode[t]\n            G = 0.99 * G + reward\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            gradient[state, action] += G * (1 - probs[action])\n            for a in range(num_actions):\n                if a != action:\n                    gradient[state, a] -= G * probs[a]\n    gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 9, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    grad = np.zeros_like(theta)\n    for episode in episodes:\n        T = len(episode)\n        G = 0\n        grads = np.zeros((T, num_states, num_actions))\n        for t in range(T - 1, -1, -1):\n            (s, a, r) = episode[t]\n            G = r + G\n            softmax_probs = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            policy_grad = np.zeros(num_actions)\n            policy_grad[a] = 1 - softmax_probs[a]\n            policy_grad *= softmax_probs\n            grads[t, s, :] = policy_grad\n        for t in range(T):\n            (s, a, _) = episode[t]\n            grad[s, :] += grads[t, s, :] * G\n    grad /= len(episodes)\n    return grad.round(4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 1024"}
{"task_id": 123, "completion_id": 1, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nk_active = 4\nd_in = 1024\nd_out = 1024"}
{"task_id": 123, "completion_id": 2, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_rounded = round(savings, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_rounded)\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 1024"}
{"task_id": 123, "completion_id": 3, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nn_experts = 16\nk_active = 4\nd_in = 1024\nd_out = 1024"}
{"task_id": 123, "completion_id": 4, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out * 2\n    flops_moe = k_active * d_in * d_out * 2\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nk_active = 4\nd_in = 128\nd_out = 128"}
{"task_id": 123, "completion_id": 5, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 256"}
{"task_id": 123, "completion_id": 6, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active * (d_in * d_out)\n    savings_percentage = (1 - flops_moe / flops_dense) * 100 if flops_dense != 0 else 0\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nk_active = 4\nd_in = 512\nd_out = 1024"}
{"task_id": 123, "completion_id": 7, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out * 2\n    flops_moe = k_active * d_in * d_out * 2\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_rounded = round(savings, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_rounded)\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 123, "completion_id": 8, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_sparse = k_active * d_in * d_out\n    savings_flops = flops_dense - flops_sparse\n    savings_percentage = savings_flops / flops_dense * 100\n    flops_dense = round(flops_dense, 1)\n    flops_sparse = round(flops_sparse, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return (flops_dense, flops_sparse, savings_percentage)\nk_active = 4\nd_in = 128\nd_out = 256"}
{"task_id": 123, "completion_id": 9, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (1 - flops_moe / (n_experts * flops_dense)) * 100\n    return (round(flops_dense, 1), round(flops_moe, 1), round(savings_percentage, 1))\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 1024"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g.T)\n    noisy_logits = logits + np.dot(N, W_noise.T)\n    topk_indices = np.argsort(noisy_logits, axis=1)[:, -k:]\n    topk_mask = np.zeros_like(noisy_logits)\n    np.put_along_axis(topk_mask, topk_indices[:, :, np.newaxis], 1, axis=1)\n    topk_noisy_logits = noisy_logits * topk_mask\n    exp_topk_noisy_logits = np.exp(topk_noisy_logits)\n    sum_exp_topk_noisy_logits = np.sum(exp_topk_noisy_logits, axis=1, keepdims=True)\n    probabilities = exp_topk_noisy_logits / sum_exp_topk_noisy_logits\n    probabilities_rounded = np.round(probabilities, 4)\n    return probabilities_rounded.tolist()"}
{"task_id": 124, "completion_id": 1, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    noise = X @ W_noise\n    logits = X @ W_g + noise\n    logits += N\n    gates = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    top_k_indices = np.argsort(gates, axis=1)[:, -k:]\n    top_k_mask = np.zeros_like(gates, dtype=bool)\n    np.put_along_axis(top_k_mask, top_k_indices[:, :, np.newaxis], True, axis=1)\n    gates[~top_k_mask] = 0\n    gates = gates / np.sum(gates, axis=1, keepdims=True)\n    gates = np.round(gates, 4)\n    return gates.tolist()"}
{"task_id": 124, "completion_id": 2, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    G = np.dot(X, W_g) + N\n    exp_g = np.exp(G)\n    probs = exp_g / np.sum(exp_g, axis=-1, keepdims=True)\n    top_k_indices = np.argsort(probs, axis=-1)[:, -k:]\n    top_k_mask = np.zeros_like(probs, dtype=bool)\n    np.put_along_axis(top_k_mask, top_k_indices[:, :, np.newaxis], True, axis=-1)\n    top_k_probs = np.where(top_k_mask, probs, 0)\n    renorm_probs = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    return renorm_probs.round(4).tolist()"}
{"task_id": 124, "completion_id": 3, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Implements the Noisy Top-K Gating Function used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n    W_g (np.ndarray): Gating weights matrix of shape (input_dim, num_experts).\n    W_noise (np.ndarray): Noise weights matrix of shape (input_dim, num_experts).\n    N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    k (int): Sparsity constraint, the number of experts to which each input is routed.\n\n    Returns:\n    list: Final gating probabilities matrix after applying Top-K gating, rounded to the nearest 4th decimal.\n    \"\"\"\n    logits = np.matmul(X, W_g)\n    noisy_logits = logits + np.sqrt(N) * np.matmul(X, W_noise)\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    probabilities = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    topk_probabilities = np.zeros_like(probabilities)\n    topk_indices = np.argsort(probabilities, axis=1)[:, -k:]\n    for i in range(X.shape[0]):\n        topk_probabilities[i, topk_indices[i]] = probabilities[i, topk_indices[i]]\n    topk_probabilities = topk_probabilities / np.sum(topk_probabilities, axis=1, keepdims=True)\n    result = np.round(topk_probabilities, 4).tolist()\n    return result"}
{"task_id": 124, "completion_id": 4, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g)\n    noisy_logits = logits + np.dot(N, W_noise)\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    noisy_logits_sum = np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    softmax_noisy_logits = exp_noisy_logits / noisy_logits_sum\n    topk_values = np.zeros_like(softmax_noisy_logits)\n    topk_indices = np.argsort(softmax_noisy_logits, axis=1)[:, -k:]\n    for i in range(topk_values.shape[0]):\n        topk_values[i, topk_indices[i]] = softmax_noisy_logits[i, topk_indices[i]]\n    topk_sum = np.sum(topk_values, axis=1, keepdims=True)\n    gating_probabilities = topk_values / topk_sum\n    gating_probabilities_rounded = np.round(gating_probabilities, 4)\n    return gating_probabilities_rounded.tolist()"}
{"task_id": 124, "completion_id": 5, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X @ W_g.T + N @ W_noise.T\n    exp_logits = np.exp(logits)\n    exp_logits_sum = np.sum(exp_logits, axis=1, keepdims=True)\n    softmax_logits = exp_logits / exp_logits_sum\n    (topk_values, topk_indices) = (np.partition(-softmax_logits, k - 1, axis=1)[:, :k], np.argpartition(-softmax_logits, k - 1, axis=1)[:, :k])\n    topk_values = -topk_values\n    topk_values_sum = np.sum(topk_values, axis=1, keepdims=True)\n    topk_values /= topk_values_sum\n    G = np.zeros_like(softmax_logits)\n    np.put_along_axis(G, topk_indices, topk_values, axis=1)\n    G_rounded = np.round(G, decimals=4)\n    return G_rounded.tolist()"}
{"task_id": 124, "completion_id": 6, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g.T) + N\n    noisy_logits = logits + np.dot(N, W_noise.T)\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    probs = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    topk_indices = np.argsort(probs, axis=1)[:, -k:]\n    topk_mask = np.zeros_like(probs, dtype=int)\n    np.put_along_axis(topk_mask, topk_indices[:, :, None], 1, axis=1)\n    topk_probs = probs * topk_mask\n    topk_probs = topk_probs / np.sum(topk_probs, axis=1, keepdims=True)\n    return np.round(topk_probs, 4).tolist()"}
{"task_id": 124, "completion_id": 7, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X @ W_g.T\n    noisy_logits = logits + X @ W_noise.T * N\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    sum_exp_noisy_logits = np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    softmax_noisy_logits = exp_noisy_logits / sum_exp_noisy_logits\n    topk_indices = np.argpartition(-softmax_noisy_logits, k - 1, axis=1)[:, :k]\n    topk_probs = np.take_along_axis(softmax_noisy_logits, topk_indices, axis=1)\n    sum_topk_probs = np.sum(topk_probs, axis=1, keepdims=True)\n    gating_probs = np.zeros_like(softmax_noisy_logits)\n    np.put_along_axis(gating_probs, topk_indices, topk_probs / sum_topk_probs, axis=1)\n    gating_probs_rounded = np.round(gating_probs, 4)\n    return gating_probs_rounded.tolist()"}
{"task_id": 124, "completion_id": 8, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X @ W_g + X @ W_noise * N\n    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n    softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_k_indices = np.argsort(softmax_probs, axis=1)[:, -k:]\n    top_k_mask = np.zeros_like(softmax_probs, dtype=bool)\n    np.put_along_axis(top_k_mask, top_k_indices[:, :, np.newaxis], True, axis=1)\n    gated_probs = np.where(top_k_mask, softmax_probs, 0.0)\n    gated_probs /= np.sum(gated_probs, axis=1, keepdims=True)\n    gated_probs = np.round(gated_probs, 4)\n    return gated_probs.tolist()"}
{"task_id": 124, "completion_id": 9, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g.T) + np.dot(N, W_noise.T)\n    noisy_logits = logits + N\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=-1, keepdims=True))\n    probabilities = exp_noisy_logits / np.sum(exp_noisy_logits, axis=-1, keepdims=True)\n    topk_indices = np.argsort(probabilities, axis=-1)[:, -k:]\n    topk_mask = np.zeros_like(probabilities, dtype=int)\n    np.put_along_axis(topk_mask, topk_indices[:, :, np.newaxis], 1, axis=-1)\n    topk_probabilities = probabilities * topk_mask\n    topk_probabilities /= np.sum(topk_probabilities, axis=-1, keepdims=True)\n    topk_probabilities_rounded = np.round(topk_probabilities, 4)\n    result = topk_probabilities_rounded.tolist()\n    return result"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    \"\"\"\n    Computes the output of a Mixture-of-Experts layer with softmax gating and top-k routing.\n\n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim).\n    - We: Expert weight matrices of shape (n_experts, input_dim, expert_dim).\n    - Wg: Gating weight matrix of shape (input_dim, n_experts).\n    - n_experts: Number of experts.\n    - top_k: Number of top experts to consider for each token.\n\n    Returns:\n    - Output tensor of shape (batch_size, expert_dim) as a Python list.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    expert_dim = We.shape[2]\n    gates = np.dot(x, Wg)\n    exp_gates = np.exp(gates)\n    softmax_gates = exp_gates / np.sum(exp_gates, axis=1, keepdims=True)\n    (top_k_values, top_k_indices) = (np.partition(-gates, top_k, axis=1)[:, :top_k], np.argpartition(-gates, top_k, axis=1)[:, :top_k])\n    sorted_indices = np.argsort(-softmax_gates[np.arange(batch_size)[:, None], top_k_indices], axis=1)\n    top_k_indices = top_k_indices[np.arange(batch_size)[:, None], sorted_indices]\n    output = np.zeros((batch_size, expert_dim))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            gate_prob = softmax_gates[i, expert_idx]\n            expert_output = np.dot(x[i], We[expert_idx])\n            output[i] += gate_prob * expert_output\n    return np.round(output, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 1, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, input_features) = x.shape\n    (_, _, output_features) = We.shape\n    gating_scores = x @ Wg.T\n    gating_probs = np.softmax(gating_scores, axis=1)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_weights = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_weights /= top_k_weights.sum(axis=1, keepdims=True)\n    expert_outputs = np.zeros((batch_size, top_k, output_features))\n    for i in range(batch_size):\n        for (j, expert_idx) in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = x[i] @ We[expert_idx].T\n    moe_output = np.sum(expert_outputs * top_k_weights[:, :, np.newaxis], axis=1)\n    return np.round(moe_output, 4).tolist()"}
{"task_id": 125, "completion_id": 2, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, hidden_size) = x.shape\n    gate_outputs = np.dot(x, Wg.T)\n    gating_probs = np.exp(gate_outputs - np.max(gate_outputs, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    top_k_gating_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_gating_probs /= np.sum(top_k_gating_probs, axis=1, keepdims=True)\n    expert_outputs = []\n    for i in range(top_k):\n        expert_indices = top_k_indices[:, i:i + 1]\n        expert_indices = np.repeat(expert_indices, hidden_size, axis=1)\n        selected_We = np.take_along_axis(We, expert_indices, axis=0)\n        expert_output = x * selected_We\n        expert_outputs.append(expert_output)\n    weighted_expert_outputs = [top_k_gating_probs[:, i:i + 1] * expert_outputs[i] for i in range(top_k)]\n    moe_output = np.sum(weighted_expert_outputs, axis=0)\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()"}
{"task_id": 125, "completion_id": 3, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, d_model) = x.shape\n    d_output = We.shape[2]\n    gate_logits = np.dot(x, Wg)\n    gate_probs = np.softmax(gate_logits, axis=-1)\n    top_k_indices = np.argsort(gate_probs, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gate_probs, top_k_indices, axis=-1)\n    top_k_probs = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    output = np.zeros((batch_size, d_output))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            expert_weight = We[expert_idx]\n            output[i] += top_k_probs[i, j] * np.dot(x[i], expert_weight)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 125, "completion_id": 4, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Sparse Mixture of Experts (MoE) Layer using softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weight matrices of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weight matrix of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to use for each input token\n    \n    Returns:\n    - Output tensor of shape (batch_size, output_dim) as a Python list\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gating_scores = np.dot(x, Wg)\n    gating_scores -= np.max(gating_scores, axis=1, keepdims=True)\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1, keepdims=True)\n    (topk_values, topk_indices) = (np.partition(-gating_probs, top_k - 1, axis=1)[:, :top_k], np.argpartition(-gating_probs, top_k - 1, axis=1)[:, :top_k])\n    topk_values = -topk_values\n    topk_values /= np.sum(topk_values, axis=1, keepdims=True)\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for (j, expert_idx) in enumerate(topk_indices[i]):\n            expert_outputs[i, j] = np.dot(x[i], We[expert_idx])\n    weighted_outputs = expert_outputs * topk_values[:, :, np.newaxis]\n    final_output = np.sum(weighted_outputs, axis=1)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 125, "completion_id": 5, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    n_tokens = x.shape[0]\n    d_model = x.shape[1]\n    d_output = We.shape[2]\n    gate_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gate_scores - np.max(gate_scores, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    topk_indices = np.argsort(gating_probs, axis=1)[:, -top_k:][:, ::-1]\n    topk_values = np.take_along_axis(gating_probs, topk_indices, axis=1)\n    moe_output = np.zeros((n_tokens, d_output))\n    for token in range(n_tokens):\n        for i in range(top_k):\n            expert_idx = topk_indices[token, i]\n            expert_weight = topk_values[token, i]\n            expert_matrix = We[expert_idx]\n            moe_output[token] += expert_weight * np.dot(x[token], expert_matrix)\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()"}
{"task_id": 125, "completion_id": 6, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    n_tokens = x.shape[0]\n    input_dim = x.shape[1]\n    output_dim = We.shape[2]\n    gates = np.dot(x, Wg.T)\n    softmax_gates = np.exp(gates) / np.sum(np.exp(gates), axis=1, keepdims=True)\n    (top_k_values, top_k_indices) = (np.partition(softmax_gates, -top_k, axis=1)[:, -top_k:], np.argpartition(softmax_gates, -top_k, axis=1)[:, -top_k:])\n    sorted_indices = np.argsort(-top_k_values, axis=1)\n    top_k_values = np.take_along_axis(top_k_values, sorted_indices, axis=1)\n    top_k_indices = np.take_along_axis(top_k_indices, sorted_indices, axis=1)\n    moe_output = np.zeros((n_tokens, output_dim))\n    for token_idx in range(n_tokens):\n        experts_indices = top_k_indices[token_idx]\n        experts_weights = top_k_values[token_idx]\n        experts_activations = np.array([np.dot(x[token_idx], We[expert_idx].T) for expert_idx in experts_indices])\n        weighted_sum = np.dot(experts_weights, experts_activations)\n        moe_output[token_idx] = weighted_sum\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()"}
{"task_id": 125, "completion_id": 7, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (n_tokens, hidden_size) = x.shape\n    gate_outputs = x @ Wg.T\n    gating_probs = np.exp(gate_outputs - np.max(gate_outputs, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_values = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_values /= np.sum(top_k_values, axis=1, keepdims=True)\n    expert_outputs = np.zeros((n_tokens, hidden_size))\n    for i in range(n_tokens):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            weight = top_k_values[i, j]\n            expert_outputs[i] += weight * (x[i] @ We[expert_idx].T)\n    result = np.round(expert_outputs, 4).tolist()\n    return result\nx = np.random.rand(5, 10)\ntop_k = 2"}
{"task_id": 125, "completion_id": 8, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gating_scores = x @ Wg.T\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    (top_k_values, top_k_indices) = (np.partition(-gating_probs, top_k - 1, axis=1)[:, :top_k], np.argpartition(-gating_probs, top_k - 1, axis=1)[:, :top_k])\n    top_k_values = -top_k_values\n    top_k_values /= np.sum(top_k_values, axis=1, keepdims=True)\n    moe_output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_index = top_k_indices[i, j]\n            expert_weight = We[expert_index]\n            moe_output[i] += top_k_values[i, j] * (x[i] @ expert_weight.T)\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()\nx = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nWe = np.random.rand(5, 3, 4)\ntop_k = 3"}
{"task_id": 125, "completion_id": 9, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    top_k_gating_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_gating_probs /= np.sum(top_k_gating_probs, axis=1, keepdims=True)\n    (batch_size, input_features) = x.shape\n    (_, _, output_features) = We.shape\n    transformed_inputs = x[:, np.newaxis, :]\n    transformed_inputs = np.repeat(transformed_inputs, top_k, axis=1)\n    top_k_experts = np.take_along_axis(We, top_k_indices[..., np.newaxis, np.newaxis], axis=0)\n    expert_outputs = np.matmul(transformed_inputs, top_k_experts)\n    expert_outputs = np.sum(expert_outputs * top_k_gating_probs[..., np.newaxis], axis=1)\n    result = np.round(expert_outputs, 4).tolist()\n    return result"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X: np.ndarray with shape (B, C, H, W)\n    - gamma: np.ndarray with shape (1, C, 1, 1)\n    - beta: np.ndarray with shape (1, C, 1, 1)\n    - num_groups: int, number of groups to separate the channels into\n    - epsilon: float, small constant to prevent division by zero\n\n    Returns:\n    - normalized tensor as a Python list after rounding to 4 decimal places\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    Y = gamma * X_normalized + beta\n    return Y.round(4).tolist()"}
{"task_id": 126, "completion_id": 1, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by the number of groups.')\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    Y = gamma * X_normalized + beta\n    Y_rounded = np.round(Y, 4)\n    result = Y_rounded.tolist()\n    return result"}
{"task_id": 126, "completion_id": 2, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by num_groups')\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    group_mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    group_var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - group_mean) / np.sqrt(group_var + epsilon)\n    X_normalized_reshaped = X_normalized.reshape(B, C, H, W)\n    X_normalized_scaled = gamma * X_normalized_reshaped + beta\n    result = np.round(X_normalized_scaled, 4)\n    return result.tolist()"}
{"task_id": 126, "completion_id": 3, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups.'\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = gamma * X_normalized + beta\n    X_normalized = np.round(X_normalized, decimals=4)\n    result = X_normalized.tolist()\n    return result\nX = np.random.rand(2, 6, 2, 2)\ngamma = np.random.rand(6, 1, 1)\nbeta = np.random.rand(6, 1, 1)\nresult = group_normalization(X, gamma, beta, num_groups=2)"}
{"task_id": 126, "completion_id": 4, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    Y = gamma * X_normalized + beta\n    result = np.round(Y, 4).tolist()\n    return result"}
{"task_id": 126, "completion_id": 5, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameter of shape (1, C, 1, 1)\n    - beta: Learnable shift parameter of shape (1, C, 1, 1)\n    - num_groups: Number of groups to separate the channels into\n    - epsilon: Small constant for numerical stability\n    \n    Returns:\n    - Normalized and scaled tensor as a Python list\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    group_mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    group_var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - group_mean) / np.sqrt(group_var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    Y = gamma * X_norm + beta\n    result = np.round(Y, 4).tolist()\n    return result"}
{"task_id": 126, "completion_id": 6, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups'\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    output = gamma * X_normalized + beta\n    output = np.round(output, 4)\n    return output.tolist()\nX = np.array([[[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]], [[9.0, 10.0], [11.0, 12.0]], [[13.0, 14.0], [15.0, 16.0]]]])\ngamma = np.array([1.0, 1.0, 1.0, 1.0])\nbeta = np.array([0.0, 0.0, 0.0, 0.0])\nnum_groups = 2"}
{"task_id": 126, "completion_id": 7, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by the number of groups'\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = gamma * X_normalized + beta\n    X_normalized = np.round(X_normalized, 4)\n    result = X_normalized.tolist()\n    return result"}
{"task_id": 126, "completion_id": 8, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    channels_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = gamma * X_normalized + beta\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()"}
{"task_id": 126, "completion_id": 9, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    Y = gamma * X_normalized + beta\n    result = np.round(Y, 4).tolist()\n    return result"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for i in range(max_iters):\n        gradient = df(x_current)\n        x_next = x_current - learning_rate * gradient\n        if abs(x_next - x_current) < tolerance:\n            break\n        x_current = x_next\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 1, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for _ in range(max_iters):\n        gradient = df(x_current)\n        x_next = x_current - learning_rate * gradient\n        if abs(x_next - x_current) < tolerance:\n            break\n        x_current = x_next\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 2, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for _ in range(max_iters):\n        gradient = df(x_current)\n        x_new = x_current - learning_rate * gradient\n        if abs(x_new - x_current) < tolerance:\n            break\n        x_current = x_new\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 3, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 4, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for _ in range(max_iters):\n        gradient = df(x_current)\n        x_next = x_current - learning_rate * gradient\n        if abs(x_next - x_current) < tolerance:\n            break\n        x_current = x_next\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 5, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)\nstart_x = np.random.rand()"}
{"task_id": 127, "completion_id": 6, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for i in range(max_iters):\n        gradient = df(x_current)\n        x_next = x_current - learning_rate * gradient\n        if abs(x_next - x_current) < tolerance:\n            break\n        x_current = x_next\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 7, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        gradient = df(current_x)\n        new_x = current_x - learning_rate * gradient\n        if abs(new_x - current_x) < tolerance:\n            break\n        current_x = new_x\n    return round(current_x, 4)"}
{"task_id": 127, "completion_id": 8, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x_current = start_x\n    for i in range(max_iters):\n        gradient = df(x_current)\n        x_next = x_current - learning_rate * gradient\n        if abs(x_next - x_current) < tolerance:\n            break\n        x_current = x_next\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 9, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if np.abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    tanh_output = np.tanh(alpha * x + gamma)\n    dyt_output = beta * tanh_output\n    rounded_output = np.round(dyt_output, 4)\n    result_list = rounded_output.tolist()\n    return result_list"}
{"task_id": 128, "completion_id": 1, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    x_hat = alpha * x\n    tanh_output = np.tanh(x_hat)\n    output = gamma * tanh_output + beta\n    rounded_output = np.round(output, 4).tolist()\n    return rounded_output"}
{"task_id": 128, "completion_id": 2, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    x_transformed = alpha * np.tanh(gamma * x + beta)\n    x_rounded = np.round(x_transformed, 4)\n    result = x_rounded.tolist()\n    return result"}
{"task_id": 128, "completion_id": 3, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    tanh_part = np.tanh(alpha * x + beta)\n    scaled_tanh = gamma * tanh_part\n    rounded_result = np.round(scaled_tanh, 4)\n    result_list = rounded_result.tolist()\n    return result_list"}
{"task_id": 128, "completion_id": 4, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    output = alpha * np.tanh(gamma * x + beta)\n    rounded_output = np.round(output, 4)\n    result_list = rounded_output.tolist()\n    return result_list"}
{"task_id": 128, "completion_id": 5, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    y = alpha * np.tanh(gamma * (x + beta))\n    y_rounded = np.round(y, 4)\n    y_list = y_rounded.tolist()\n    return y_list"}
{"task_id": 128, "completion_id": 6, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    tanh_component = np.tanh(alpha * x + beta)\n    dyt_output = gamma * (x * tanh_component)\n    dyt_output_rounded = np.round(dyt_output, 4)\n    dyt_output_list = dyt_output_rounded.tolist()\n    return dyt_output_list"}
{"task_id": 128, "completion_id": 7, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    tanh_input = alpha * x + beta\n    tanh_output = np.tanh(tanh_input)\n    dyt_output = gamma * tanh_output\n    rounded_output = np.round(dyt_output, 4)\n    result_list = rounded_output.tolist()\n    return result_list"}
{"task_id": 128, "completion_id": 8, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    transformed = alpha * np.tanh(gamma * x + beta)\n    rounded_result = np.round(transformed, 4)\n    return rounded_result.tolist()"}
{"task_id": 128, "completion_id": 9, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    dyt_output = alpha * np.tanh(gamma * (x + beta))\n    rounded_output = np.round(dyt_output, 4)\n    result_list = rounded_output.tolist()\n    return result_list"}
