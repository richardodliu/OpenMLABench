{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    res = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        res.append(dot_product)\n    return res"}
{"task_id": 1, "completion_id": 1, "solution": "from typing import List, Union\ndef matrix_dot_vector(a: List[List[Union[int, float]]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 2, "solution": "from typing import List, Union\ndef matrix_dot_vector(a: List[List[Union[int, float]]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    if len(a[0]) == len(b):\n        return [sum((x * y for (x, y) in zip(row, b))) for row in a]\n    else:\n        return -1"}
{"task_id": 1, "completion_id": 3, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    num_cols_a = len(a[0])\n    if num_cols_a != len(b):\n        return -1\n    result = []\n    for row in a:\n        product = 0\n        for i in range(len(b)):\n            product += row[i] * b[i]\n        result.append(product)\n    return result"}
{"task_id": 1, "completion_id": 4, "solution": "from typing import List, Union\ndef matrix_dot_vector(a: List[List[Union[int, float]]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    \"\"\"Compute the dot product of a matrix and a vector.\"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((a * b for (a, b) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 5, "solution": "def matrix_dot_vector(m: list[list[int | float]], v: list[int | float]) -> list[int | float]:\n    if not all((len(row) == len(v) for row in m)):\n        return -1\n    dot_product = []\n    for row in m:\n        product = 0\n        for i in range(len(v)):\n            product += row[i] * v[i]\n        dot_product.append(product)\n    return dot_product"}
{"task_id": 1, "completion_id": 6, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = 0\n        for i in range(len(row)):\n            dot_product += row[i] * b[i]\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 7, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    num_cols_a = len(a[0])\n    num_rows_a = len(a)\n    num_rows_b = len(b)\n    if num_cols_a != num_rows_b:\n        return -1\n    result_vector = []\n    for row in a:\n        result = 0\n        for i in range(num_cols_a):\n            result += row[i] * b[i]\n        result_vector.append(result)\n    return result_vector"}
{"task_id": 1, "completion_id": 8, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 9, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 1, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 2, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 3, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 4, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 5, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    rows = len(a)\n    if rows == 0:\n        return []\n    cols = len(a[0])\n    transpose = [[0 for _ in range(rows)] for _ in range(cols)]\n    for i in range(rows):\n        for j in range(cols):\n            transpose[j][i] = a[i][j]\n    return transpose"}
{"task_id": 2, "completion_id": 6, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    transpose = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transpose"}
{"task_id": 2, "completion_id": 7, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    transposed = []\n    for i in range(len(a[0])):\n        transposed_row = []\n        for row in a:\n            transposed_row.append(row[i])\n        transposed.append(transposed_row)\n    return transposed"}
{"task_id": 2, "completion_id": 8, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    transposed_matrix = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed_matrix\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 2, "completion_id": 9, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        a = np.array(a)\n        reshaped_a = a.reshape(new_shape)\n        return reshaped_a.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 1, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        arr = np.array(a)\n        arr_reshaped = arr.reshape(new_shape)\n        return arr_reshaped.tolist()\n    except ValueError:\n        return []\nnew_shape = (2, 3)"}
{"task_id": 3, "completion_id": 2, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        a = np.array(a)\n        reshaped_a = a.reshape(new_shape)\n        reshaped_a = reshaped_a.tolist()\n        return reshaped_a\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 3, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    a_np = np.array(a)\n    try:\n        reshaped_a = a_np.reshape(new_shape)\n        return reshaped_a.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 4, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        mat = np.array(a)\n        reshaped_mat = np.reshape(mat, new_shape)\n        return reshaped_mat.tolist()\n    except:\n        return []"}
{"task_id": 3, "completion_id": 5, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    a = np.array(a)\n    try:\n        a = a.reshape(new_shape)\n        a = a.tolist()\n    except:\n        a = []\n    return a"}
{"task_id": 3, "completion_id": 6, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    np_array = np.array(a)\n    try:\n        reshaped_array = np_array.reshape(new_shape)\n    except ValueError:\n        return []\n    reshaped_list = reshaped_array.tolist()\n    return reshaped_list"}
{"task_id": 3, "completion_id": 7, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        arr = np.array(a)\n        reshaped_arr = arr.reshape(new_shape)\n        return reshaped_arr.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 8, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        arr = np.array(a)\n        reshaped_arr = arr.reshape(new_shape)\n        reshaped_list = reshaped_arr.tolist()\n        return reshaped_list\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 9, "solution": "import numpy as np\nfrom typing import List\ndef reshape_matrix(a: List[List[int | float]], new_shape: tuple[int, int]) -> List[List[int | float]]:\n    np_mat = np.matrix(a)\n    try:\n        reshaped_mat = np_mat.reshape(new_shape)\n        return reshaped_mat.tolist()\n    except ValueError:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(col) / len(matrix) for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Invalid mode. Please specify 'row' or 'column'.\")"}
{"task_id": 4, "completion_id": 1, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(col) / len(matrix) for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Invalid mode. Please provide either 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 2, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    import statistics\n    if mode == 'row':\n        row_means = [statistics.mean(row) for row in matrix]\n        return row_means\n    elif mode == 'column':\n        column_means = [statistics.mean(column) for column in zip(*matrix)]\n        return column_means\n    else:\n        raise ValueError(\"Mode should be either 'row' or 'column'\")\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 3, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Invalid mode. Allowed values are 'row' or 'column'\")\n    if len(matrix) == 0 or len(matrix[0]) == 0:\n        raise ValueError('Matrix is empty')\n    mean_values = []\n    if mode == 'row':\n        for row in matrix:\n            mean_row = sum(row) / len(row)\n            mean_values.append(mean_row)\n    else:\n        for i in range(len(matrix[0])):\n            column_sum = sum((row[i] for row in matrix))\n            mean_column = column_sum / len(matrix)\n            mean_values.append(mean_column)\n    return mean_values"}
{"task_id": 4, "completion_id": 4, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    else:\n        return [sum(col) / len(matrix) for col in zip(*matrix)]"}
{"task_id": 4, "completion_id": 5, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(col) / len(matrix) for col in zip(*matrix)]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 6, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not all((isinstance(i, list) for i in matrix)):\n        raise ValueError('invalid input: matrix should be a list of lists')\n    if not all((isinstance(i, float) for sublist in matrix for i in sublist)):\n        raise ValueError('invalid input: matrix should contain only floats')\n    if mode not in ['row', 'column']:\n        raise ValueError(\"invalid input: mode should be either 'row' or 'column'\")\n    if mode == 'column':\n        matrix = list(map(list, zip(*matrix)))\n    return [sum(row) / len(row) for row in matrix]"}
{"task_id": 4, "completion_id": 7, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column.\n\n    Args:\n    - matrix: A list of lists (matrix representation).\n    - mode: A string specifying how to calculate the mean ('row' or 'column').\n\n    Returns:\n    - A list of means, either by rows (if mode is 'row') or by columns (if mode is 'column').\n\n    Raises:\n    - ValueError: If the specified mode is neither 'row' nor 'column'.\n    \"\"\"\n    if mode == 'column':\n        matrix = list(map(list, zip(*matrix)))\n    elif mode != 'row':\n        raise ValueError(\"Invalid mode: mode must be either 'row' or 'column'\")\n    return [sum(row) / len(row) for row in matrix]\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 8, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed_matrix = list(zip(*matrix))\n        return [sum(column) / len(column) for column in transposed_matrix]\n    else:\n        raise ValueError(\"Mode should be either 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 9, "solution": "from typing import List\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Invalid mode. Allowed modes are 'row' or 'column'\")\n    if not all((len(row) == len(matrix[0]) for row in matrix)):\n        raise ValueError('Matrix is not rectangular')\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    else:\n        means = [sum(column) / len(matrix) for column in zip(*matrix)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nscalar = 2\nresult = scalar_multiply(matrix, scalar)"}
{"task_id": 5, "completion_id": 1, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [elem * scalar for elem in row]\n        result.append(new_row)\n    return result"}
{"task_id": 5, "completion_id": 2, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    scalar_product = []\n    for row in matrix:\n        new_row = []\n        for element in row:\n            new_row.append(element * scalar)\n        scalar_product.append(new_row)\n    return scalar_product"}
{"task_id": 5, "completion_id": 3, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    if len(matrix) == 0:\n        return []\n    num_rows = len(matrix)\n    num_cols = len(matrix[0])\n    result = [[0 for _ in range(num_cols)] for _ in range(num_rows)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 5, "completion_id": 4, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 5, "completion_id": 5, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\" Multiply a matrix by a scalar \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 6, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    for i in range(len(matrix)):\n        for j in range(len(matrix[i])):\n            matrix[i][j] *= scalar\n    return matrix"}
{"task_id": 5, "completion_id": 7, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Function to multiply a matrix by a scalar and return the result.\n\n    Args:\n    matrix (list[list[int|float]]): The input matrix.\n    scalar (int|float): The scalar to multiply the matrix with.\n\n    Returns:\n    list[list[int|float]]: The result of the multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 5, "completion_id": 8, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for i in range(len(matrix)):\n        result.append([])\n        for j in range(len(matrix[i])):\n            result[i].append(matrix[i][j] * scalar)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nscalar = 2\nresult = scalar_multiply(matrix, scalar)"}
{"task_id": 5, "completion_id": 9, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n\n    Parameters:\n        matrix (list[list[int | float]]): A two-dimensional matrix of integers or floats.\n        scalar (int | float): The scalar multiplying factor.\n\n    Returns:\n        matrix (list[list[int | float]]): The result of the matrix multiplication.\n    \"\"\"\n    if not matrix:\n        return []\n    elif len(set((len(row) for row in matrix))) != 1:\n        raise ValueError('Matrix must be rectangular')\n    for i in range(len(matrix)):\n        for j in range(len(matrix[i])):\n            matrix[i][j] *= scalar\n    return matrix"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    return sorted(eigenvalues.tolist(), reverse=True)"}
{"task_id": 6, "completion_id": 1, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    np_mat = np.array(matrix)\n    eigvals = np.linalg.eigvals(np_mat)\n    eigvals = list(eigvals)\n    eigvals.sort(reverse=True)\n    return eigvals\nmatrix = [[1, 2], [3, 4]]"}
{"task_id": 6, "completion_id": 2, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Matrix must be 2x2')\n    np_matrix = np.array(matrix)\n    eigvals = np.linalg.eigvals(np_matrix)\n    eigvals = sorted(eigvals, reverse=True)\n    return eigvals"}
{"task_id": 6, "completion_id": 3, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eig(np_matrix)[0].tolist()\n    return sorted(eigenvalues, reverse=True)\nmatrix = [[4, -2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 6, "completion_id": 4, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('The given matrix is not a 2x2 matrix')\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    tr = a + d\n    det = a * d - b * c\n    evalue1 = (tr + (tr ** 2 - 4 * det) ** 0.5) / 2\n    evalue2 = (tr - (tr ** 2 - 4 * det) ** 0.5) / 2\n    eigenvalues = sorted([evalue1, evalue2], reverse=True)\n    return eigenvalues"}
{"task_id": 6, "completion_id": 5, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    np_matrix = np.array(matrix)\n    if np_matrix.shape != (2, 2):\n        raise ValueError('The matrix must be 2x2')\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues.tolist()"}
{"task_id": 6, "completion_id": 6, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"Calculate the eigenvalues of a 2x2 matrix.\n    \n    The eigenvalues are returned in a list, sorted in descending order.\n\n    Args:\n        matrix (list[list[float|int]]): The matrix for which to calculate the eigenvalues.\n    \n    Returns:\n        list[float]: The eigenvalues of the matrix.\n    \"\"\"\n    matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix)\n    return sorted(eigenvalues, reverse=True)\nmatrix = [[1, 2], [3, 4]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 6, "completion_id": 7, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import numpy as np\n    matrix = np.array(matrix)\n    eigenvalues = np.linalg.eig(matrix)[0]\n    eigenvalues.sort()\n    eigenvalues = eigenvalues[::-1]\n    return eigenvalues"}
{"task_id": 6, "completion_id": 8, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Matrix should be of size 2x2')\n    np_matrix = np.array(matrix)\n    (eig_values, _) = np.linalg.eig(np_matrix)\n    eig_values_sorted = sorted(eig_values, reverse=True)\n    return eig_values_sorted\nmatrix = [[1, 2], [3, 4]]"}
{"task_id": 6, "completion_id": 9, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    np_matrix = np.array(matrix)\n    (eigenvalues, _) = np.linalg.eig(np_matrix)\n    eigenvalues = np.real(eigenvalues)\n    eigenvalues = sorted(eigenvalues, reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    result = np.linalg.inv(T) @ A @ S\n    return np.round(result, 4).tolist()"}
{"task_id": 7, "completion_id": 1, "solution": "import numpy as np\ndef is_invertible(matrix):\n    if len(matrix) != len(matrix[0]):\n        return False\n    det = np.linalg.det(matrix)\n    return det != 0\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if not is_invertible(T) or not is_invertible(S):\n        return -1\n    result = np.dot(np.linalg.inv(T), np.dot(A, S))\n    result = np.round(result, 4)\n    return result.tolist()"}
{"task_id": 7, "completion_id": 2, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]] | int:\n    npA = np.array(A)\n    npT = np.array(T)\n    npS = np.array(S)\n    if np.linalg.det(npT) == 0 or np.linalg.det(npS) == 0:\n        return -1\n    transformed = np.linalg.inv(npT) @ npA @ npS\n    rounded_transformed = np.round(transformed, 4)\n    transformed_list = rounded_transformed.tolist()\n    return transformed_list"}
{"task_id": 7, "completion_id": 3, "solution": "import numpy as np\ndef is_invertible(matrix):\n    return np.linalg.det(matrix) != 0\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if is_invertible(T) and is_invertible(S):\n        result = np.linalg.inv(T) @ A @ S\n        result = np.around(result, decimals=4)\n        return result.tolist()\n    else:\n        return -1"}
{"task_id": 7, "completion_id": 4, "solution": "import numpy as np\ndef transform_matrix(A, T, S):\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    transformed_matrix = np.matmul(np.matmul(np.linalg.inv(T), A), S)\n    transformed_matrix = np.round(transformed_matrix, 4)\n    return transformed_matrix.tolist()"}
{"task_id": 7, "completion_id": 5, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    result = np.matmul(T_inv, np.matmul(A, S_inv))\n    result = np.round(result, 4)\n    return result.tolist()"}
{"task_id": 7, "completion_id": 6, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    if det_T == 0 or det_S == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    result = np.linalg.multi_dot([T_inv, A, S_inv])\n    result = np.round(result, decimals=4)\n    return result.tolist()"}
{"task_id": 7, "completion_id": 7, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    result = np.dot(np.dot(np.linalg.inv(T), A), S)\n    result = np.round(result, 4)\n    return result.tolist()"}
{"task_id": 7, "completion_id": 8, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    T_det = np.linalg.det(T)\n    S_det = np.linalg.det(S)\n    if np.allclose(T_det, 0) or np.allclose(S_det, 0):\n        return -1\n    A = np.array(A, dtype=float)\n    T_inv = np.array(T, dtype=float).T / T_det\n    S = np.array(S, dtype=float)\n    S_inv = np.array(S, dtype=float).T / S_det\n    result = np.dot(np.dot(T_inv, A), S_inv)\n    result = np.round(result, 4).tolist()\n    return result"}
{"task_id": 7, "completion_id": 9, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    try:\n        T_inv = np.linalg.inv(T)\n        S_inv = np.linalg.inv(S)\n    except np.linalg.LinAlgError:\n        print('Error: One of the matrices is not invertible.')\n        return -1\n    res = np.dot(np.dot(T_inv, A), S)\n    res = res.round(4)\n    return res.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    adj = [[matrix[1][1], -matrix[0][1]], [-matrix[1][0], matrix[0][0]]]\n    inv = [[x / det for x in row] for row in adj]\n    return inv"}
{"task_id": 8, "completion_id": 1, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    determinant = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if determinant == 0:\n        return None\n    inverse = [[matrix[1][1], -matrix[0][1]], [-matrix[1][0], matrix[0][0]]]\n    for i in range(2):\n        for j in range(2):\n            inverse[i][j] /= determinant\n    return inverse"}
{"task_id": 8, "completion_id": 2, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    determinant = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if determinant == 0:\n        return None\n    inverse = [[0, 0], [0, 0]]\n    inverse[0][0] = matrix[1][1] / determinant\n    inverse[0][1] = -1 * matrix[0][1] / determinant\n    inverse[1][0] = -1 * matrix[1][0] / determinant\n    inverse[1][1] = matrix[0][0] / determinant\n    return inverse"}
{"task_id": 8, "completion_id": 3, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    else:\n        inv_det = 1 / det\n        return [[inv_det * matrix[1][1], -inv_det * matrix[0][1]], [-inv_det * matrix[1][0], inv_det * matrix[0][0]]]"}
{"task_id": 8, "completion_id": 4, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | None:\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    adj = [[matrix[1][1], -matrix[0][1]], [-matrix[1][0], matrix[0][0]]]\n    return [[adj[i][j] / det for j in range(2)] for i in range(2)]\nmatrix = [[1, 2], [2, 4]]"}
{"task_id": 8, "completion_id": 5, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    else:\n        inverse = [[0, 0], [0, 0]]\n        inverse[0][0] = matrix[1][1] / det\n        inverse[0][1] = -matrix[0][1] / det\n        inverse[1][0] = -matrix[1][0] / det\n        inverse[1][1] = matrix[0][0] / det\n        return inverse\nmatrix = [[4.0, 7.0], [2.0, 6.0]]"}
{"task_id": 8, "completion_id": 6, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] | str:\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        return 'The matrix should be 2x2'\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inv_matrix = [[0, 0], [0, 0]]\n    inv_matrix[0][0] = matrix[1][1] / det\n    inv_matrix[0][1] = -matrix[0][1] / det\n    inv_matrix[1][0] = -matrix[1][0] / det\n    inv_matrix[1][1] = matrix[0][0] / det\n    return inv_matrix\nmatrix = [[3, 8], [4, 6]]"}
{"task_id": 8, "completion_id": 7, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2:\n        raise ValueError('The matrix should be 2x2')\n    det = matrix[0][0] * matrix[1][1] - matrix[1][0] * matrix[0][1]\n    if det == 0:\n        return None\n    else:\n        return [[matrix[1][1] / det, -1 * matrix[0][1] / det], [-1 * matrix[1][0] / det, matrix[0][0] / det]]\nmatrix = [[4.0, 7.0], [2.0, 6.0]]"}
{"task_id": 8, "completion_id": 8, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inverse = [[0, 0], [0, 0]]\n    inverse[0][0] = matrix[1][1] / det\n    inverse[0][1] = -matrix[0][1] / det\n    inverse[1][0] = -matrix[1][0] / det\n    inverse[1][1] = matrix[0][0] / det\n    return inverse\nmatrix = [[1, 2], [2, 4]]"}
{"task_id": 8, "completion_id": 9, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    if det == 0:\n        return None\n    else:\n        return [[d / det, -b / det], [-c / det, a / det]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    rows_A = len(a)\n    cols_A = len(a[0])\n    rows_B = len(b)\n    cols_B = len(b[0])\n    if cols_A != rows_B:\n        return -1\n    result = [[0 for row in range(cols_B)] for col in range(rows_A)]\n    for i in range(rows_A):\n        for j in range(cols_B):\n            for k in range(cols_A):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 1, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    else:\n        result = [[0 for j in range(len(b[0]))] for i in range(len(a))]\n        for i in range(len(a)):\n            for j in range(len(b[0])):\n                for k in range(len(b)):\n                    result[i][j] += a[i][k] * b[k][j]\n        return result"}
{"task_id": 9, "completion_id": 2, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for __ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 3, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    cols_a = len(a[0])\n    rows_a = len(a)\n    cols_b = len(b[0])\n    rows_b = len(b)\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for row in range(cols_b)] for col in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 4, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    if len(a[0]) != len(b):\n        return -1\n    res = [[0 for j in range(len(b[0]))] for i in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                res[i][j] += a[i][k] * b[k][j]\n    return res"}
{"task_id": 9, "completion_id": 5, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    rows_A = len(a)\n    cols_A = len(a[0])\n    rows_B = len(b)\n    cols_B = len(b[0])\n    if cols_A != rows_B:\n        return -1\n    C = [[0 for row in range(cols_B)] for col in range(rows_A)]\n    for i in range(rows_A):\n        for j in range(cols_B):\n            for k in range(cols_A):\n                C[i][j] += a[i][k] * b[k][j]\n    return C"}
{"task_id": 9, "completion_id": 6, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    a_rows = len(a)\n    a_cols = len(a[0])\n    b_rows = len(b)\n    b_cols = len(b[0])\n    if a_cols != b_rows:\n        return -1\n    result = [[0] * b_cols for _ in range(a_rows)]\n    for i in range(a_rows):\n        for j in range(b_cols):\n            for k in range(a_cols):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 7, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 8, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (a_rows, a_cols) = (len(a), len(a[0]))\n    (b_rows, b_cols) = (len(b), len(b[0]))\n    if a_cols != b_rows:\n        return -1\n    c = [[0 for _ in range(b_cols)] for _ in range(a_rows)]\n    for i in range(a_rows):\n        for j in range(b_cols):\n            for k in range(a_cols):\n                c[i][j] += a[i][k] * b[k][j]\n    return c"}
{"task_id": 9, "completion_id": 9, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    result = [[0 for j in range(len(b[0]))] for i in range(len(a))]\n    if len(a[0]) != len(b):\n        return -1\n    else:\n        for i in range(len(a)):\n            for j in range(len(b[0])):\n                for k in range(len(b)):\n                    result[i][j] += a[i][k] * b[k][j]\n        return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    means = [np.mean(feature) for feature in vectors]\n    vectors_centered = [np.array([x - mu for x in feature]) for (feature, mu) in zip(vectors, means)]\n    covariance_matrix = np.cov(vectors_centered)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 1, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    This function computes the covariance matrix of given vectors.\n    Vectors is a 2D list of floats where each inner list represents a feature with its observations.\n    Returns the covariance matrix as a 2D list of floats.\n    \"\"\"\n    transposed_vectors = np.array(vectors).T.tolist()\n    covariance_matrix = np.cov(transposed_vectors).tolist()\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 2, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    covariance_matrix = np.cov(vectors)\n    return covariance_matrix.tolist()"}
{"task_id": 10, "completion_id": 3, "solution": "import statistics as stats\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    means = [stats.mean(col) for col in zip(*vectors)]\n    centered = [[value - mean for (value, mean) in zip(row, means)] for row in vectors]\n    covariance = [[sum((x * y for (x, y) in zip(row, col))) / len(row) for col in zip(*centered)] for row in zip(*centered)]\n    return covariance"}
{"task_id": 10, "completion_id": 4, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors):\n    return np.cov(vectors).tolist()"}
{"task_id": 10, "completion_id": 5, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n_features = len(vectors)\n    cov_matrix = np.zeros((n_features, n_features))\n    for i in range(n_features):\n        for j in range(n_features):\n            cov_matrix[i][j] = np.cov(vectors[i], vectors[j])[0][1]\n    return cov_matrix.tolist()"}
{"task_id": 10, "completion_id": 6, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    np_vectors = np.array(vectors)\n    cov_matrix = np.cov(np_vectors)\n    return cov_matrix.tolist()"}
{"task_id": 10, "completion_id": 7, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors):\n    data = np.array(vectors)\n    normed_data = data - np.mean(data, axis=0)\n    covariance_matrix = 1 / data.shape[1] * np.matmul(normed_data.T, normed_data)\n    return covariance_matrix.tolist()\nvectors = [[1, 2, 3, 4], [2, 4, 6, 8], [3, 6, 9, 12]]\ncovariance_matrix = calculate_covariance_matrix(vectors)"}
{"task_id": 10, "completion_id": 8, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    np_vectors = np.array(vectors)\n    return np.cov(np_vectors).tolist()"}
{"task_id": 10, "completion_id": 9, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    data = np.array(vectors)\n    cov_matrix = np.cov(data, rowvar=False)\n    cov_matrix_list = cov_matrix.tolist()\n    return cov_matrix_list\nvectors = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\ncov_matrix = calculate_covariance_matrix(vectors)"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x_new = np.round(x_new, 4)\n        if np.allclose(x, x_new, atol=0.0001):\n            break\n        x = x_new\n    return x.tolist()"}
{"task_id": 11, "completion_id": 1, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:\n        raise ValueError('A should be a square matrix and the number of rows in A should be the same as the number of elements in b')\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for _ in range(n):\n        D_inv = np.diag(1 / D)\n        x = np.dot(D_inv, b - np.dot(R, b))\n        b = x\n    x = [round(i, 4) for i in x]\n    return x\nA = np.array([[10, 2, -1], [1, 10, 3], [2, 3, 10]])\nb = np.array([6, -11, 1])\nx = solve_jacobi(A, b, 100)"}
{"task_id": 11, "completion_id": 2, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    rows = len(A)\n    x = np.zeros(rows)\n    for _ in range(n):\n        new_x = np.array(x)\n        for i in range(rows):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n        x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 100"}
{"task_id": 11, "completion_id": 3, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x = np.array([(b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i] for i in range(b.shape[0])])\n    return np.round(x, 4).tolist()"}
{"task_id": 11, "completion_id": 4, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    assert A.shape[0] == A.shape[1], 'Matrix A should be square'\n    assert np.linalg.det(A) != 0, 'System has no solution or infinite solutions'\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    x = np.zeros_like(b).astype(np.float64)\n    for _ in range(n):\n        x = (b - np.dot(R, x)) / D\n    return np.round(x, 4).tolist()\nA = np.array([[3, 2], [1, 2]])\nb = np.array([11, 7])\nn = 100"}
{"task_id": 11, "completion_id": 5, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    n_var = A.shape[0]\n    x = np.zeros_like(b, dtype=np.float64)\n    D = np.zeros_like(A, dtype=np.float64)\n    np.fill_diagonal(D, A.diagonal())\n    I = np.identity(n_var, dtype=np.float64)\n    if not np.all(2 * np.diag(np.abs(A)) >= np.sum(np.abs(A), axis=1)):\n        raise ValueError('Matrix A is not diagonally dominant, the Jacobi method may not converge.')\n    D_inv = np.linalg.inv(D)\n    for _ in range(n):\n        x = np.dot(D_inv, b - np.dot(I - D_inv @ A, x))\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 25"}
{"task_id": 11, "completion_id": 6, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:\n        raise ValueError('A should be a square matrix and A and b should have the same number of rows')\n    n = b.shape[-1]\n    x = np.zeros(n, dtype=np.float32)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for i in range(n):\n        x = (b - np.dot(R, x)) / D\n        x = np.round(x, 4)\n        print('Iteration {0}: {1}'.format(i + 1, x))\n    return x.tolist()"}
{"task_id": 11, "completion_id": 7, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    assert A.shape[0] == A.shape[1], 'Matrix A should be a square matrix'\n    assert A.shape[0] == b.shape[0], 'Dimensions of A and b should match'\n    assert np.all(2 * np.diag(A) > np.sum(np.abs(A), axis=1)), 'Matrix A should be diagonally dominant'\n    x = np.zeros_like(b)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for _ in range(n):\n        x = (b - np.dot(R, x)) / D\n    return np.round(x, 4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])"}
{"task_id": 11, "completion_id": 8, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b, dtype=np.float)\n    D = np.diag(A)\n    D_inv = 1.0 / D\n    D_inv = np.diag(D_inv)\n    LU = A - np.diag(D)\n    for i in range(n):\n        x = np.dot(D_inv, b - np.dot(LU, x))\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 9, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if not np.all(2 * np.diag(A) > np.sum(np.abs(A), 1)):\n        raise ValueError('Matrix must be diagonally dominant')\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return list(np.round(x, 4))\nA = np.array([[16, 3], [7, -11]])\nb = np.array([11, 13])\nn = 100"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def determinant(a):\n        return a[0][0] * a[1][1] - a[0][1] * a[1][0]\n\n    def jacobian(a):\n        return [[a[1][1], -a[0][1]], [-a[1][0], a[0][0]]]\n\n    def eigenvectors(j):\n        a = 1\n        b = -(j[0][0] + j[1][1])\n        c = j[0][0] * j[1][1] - j[0][1] * j[1][0]\n        lambda1 = (-b + math.sqrt(b ** 2 - 4 * a * c)) / (2 * a)\n        lambda2 = (-b - math.sqrt(b ** 2 - 4 * a * c)) / (2 * a)\n        return sorted([lambda1, lambda2], reverse=True)\n\n    def singular_values(a):\n        return [math.sqrt(abs(l)) for l in eigenvectors(jacobian(a))]\n    return tuple([round(s, 4) for s in singular_values(A)])\nA = [[5, 1], [1, 3]]"}
{"task_id": 12, "completion_id": 1, "solution": "import numpy as np\nfrom scipy.linalg import eig\nfrom numpy.linalg import norm\nimport math\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    (ATA_eigvals, ATA_eigvecs) = eig(np.dot(A.T, A))\n    (AAT_eigvals, AAT_eigvecs) = eig(np.dot(A, A.T))\n    sorted_ATA_indices = np.argsort(ATA_eigvals)[::-1]\n    sorted_ATA_eigvals = ATA_eigvals[sorted_ATA_indices]\n    sorted_ATA_eigvecs = ATA_eigvecs[:, sorted_ATA_indices]\n    sorted_AAT_indices = np.argsort(AAT_eigvals)[::-1]\n    sorted_AAT_eigvals = AAT_eigvals[sorted_AAT_indices]\n    sorted_AAT_eigvecs = AAT_eigvecs[:, sorted_AAT_indices]\n    sigma = np.sqrt(sorted_ATA_eigvals)\n    U = sorted_ATA_eigvecs\n    V = sorted_AAT_eigvecs\n    return (np.round(U, 4), np.round(sigma, 4), np.round(V, 4))\nA = np.array([[3, 2], [2, 3]])"}
{"task_id": 12, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Function that approximates the Singular Value Decomposition (SVD) on a 2x2 matrix\n    by using the Jacobian method.\n\n    Parameters:\n    A: a 2x2 matrix of numpy ndarray type.\n\n    Returns:\n    A tuple of three elements:\n     - u: left singular vector of float type.\n     - s: singular values of float type.\n     - v: right singular vector of float type.\n\n    All the results are rounded to the nearest 4th decimal.\n    \"\"\"\n    T = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(T)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = np.sqrt(eigenvalues[idx])\n    eigenvectors = eigenvectors[:, idx]\n    u = A @ eigenvectors\n    s = np.diag(eigenvalues)\n    v = eigenvectors\n    return (np.round(u, 4), np.round(s, 4), np.round(v, 4))"}
{"task_id": 12, "completion_id": 3, "solution": "import math\ndef svd_2x2_singular_values(A):\n\n    def get_eigen(A, num_sweeps=50):\n\n        def rotate(A, p, q):\n            if A[p][p] == A[q][q]:\n                phi = math.pi / 4\n            else:\n                phi = math.atan2(2 * A[p][q], A[q][q] - A[p][p]) / 2\n            c = math.cos(phi)\n            s = math.sin(phi)\n            return np.array([[c, -s], [s, c]])\n        B = np.copy(A)\n        for sweep in range(num_sweeps):\n            (p, q) = max(((p, q) for p in range(2) for q in range(p)), key=lambda pq: abs(B[pq]))\n            R = rotate(B, *pq)\n            B = R.T @ B @ R\n        return np.diag(B)\n    if A.shape != (2, 2):\n        return 'Input matrix should be 2x2'\n    eigen_vals = get_eigen(A)\n    singular_vals = sorted([math.sqrt(abs(x)) for x in eigen_vals], reverse=True)\n    singular_vals_rounded = [round(x, 4) for x in singular_vals]\n    return tuple(singular_vals_rounded)"}
{"task_id": 12, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    B = np.array([[0, 1], [-1, 0]])\n    H = np.array([[A[0, 0] + A[1, 1], A[1, 0] - A[0, 1]], [A[1, 0] + A[0, 1], A[0, 0] - A[1, 1]]])\n    det = H[0][0] * H[1][1] - H[0][1] * H[1][0]\n    sqrt_det = det ** 0.5\n    eig_vals = [(H[0][0] + H[1][1] + sqrt_det) / 2, (H[0][0] + H[1][1] - sqrt_det) / 2]\n    eig_vecs = []\n    for eig_val in eig_vals:\n        vec = np.linalg.solve(H - eig_val * B, np.array([1, 0]))\n        if abs(np.linalg.norm(vec) - 1) > 1e-05:\n            vec = -vec / np.linalg.norm(vec)\n        eig_vecs.append(vec)\n    return (tuple([np.sqrt(abs(eig_val)) for eig_val in eig_vals]), eig_vecs)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Returns the singular values of a 2x2 matrix A using the Jacobi method.\n    \n    param A: The 2x2 matrix to decompose\n    return: The singular values as a tuple in descending order. \n            The values are rounded to the nearest 4th decimal.\n    \"\"\"\n    (u, v) = (np.array([[1, 0], [0, 1]]), A)\n    while True:\n        (max_val, p, q) = max(((abs(u[i][j]), i, j) for i in range(len(u) - 1) for j in range(i + 1, len(u))))\n        if max_val < 1e-10:\n            break\n        theta = (v[p][q] - v[q][p]) / (2 * max_val)\n        t = np.sign(theta) / (abs(theta) + np.sqrt(theta ** 2 + 1))\n        (c, s) = (1 / np.sqrt(t ** 2 + 1), t)\n        temp = v\n        v = np.dot(c, temp) - np.dot(s, temp.T)\n        v = np.dot(v, c).T - np.dot(s, temp)\n        temp[p][q] = 0\n        temp[q][p] = 0\n        v = v + np.dot(s, temp) + np.dot(c, temp.T)\n        temp = u\n        u = np.dot(c, temp) - np.dot(s, temp.T)\n        u = np.dot(u, c).T - np.dot(s, temp)\n        temp[p][q] = 0\n        temp[q][p] = 0\n        u = u + np.dot(s, temp[q][p]).T - np.dot(c, temp[p][q]).T\n    sigma = np.sqrt(sum((i ** 2 for i in v.flatten())))\n    sigma1 = round(sigma, 4)\n    sigma2 = round(sigma / np.sqrt(2), 4)\n    return (sigma1, sigma2)\nA = np.array([[1.0, 3.0], [2.0, 4.0]])"}
{"task_id": 12, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    (a, b, c, d) = A.flatten()\n    M = np.array([[a, b], [c, d]])\n    discriminant = math.sqrt((a ** 2 + 4 * b * c - 2 * a * d + d ** 2) ** 2 - 4 * (a * d - b * c) ** 2)\n    trace = a + d\n    lambda1 = (trace + discriminant) / 2\n    lambda2 = (trace - discriminant) / 2\n    v1 = np.array([lambda1 - d, b])\n    v2 = np.array([c, lambda2 - a])\n    v1_norm = np.linalg.norm(v1)\n    v2_norm = np.linalg.norm(v2)\n    v1 = np.round(v1 / v1_norm, 4)\n    v2 = np.round(v2 / v2_norm, 4)\n    u1 = np.array([-v2[1], v1[1]])\n    u2 = np.array([v2[0], -v1[0]])\n    u1_norm = np.linalg.norm(u1)\n    u2_norm = np.linalg.norm(u2)\n    u1 = np.round(u1 / u1_norm, 4)\n    u2 = np.round(u2 / u2_norm, 4)\n    return ((np.abs(np.round(np.sqrt(max(0, lambda1, lambda2)), 4)), np.abs(np.round(np.sqrt(min(0, lambda1, lambda2)), 4))), (u1, u2), (v1, v2))\nM = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def jacobi_method(A, num_iterations=1000, epsilon=1e-10):\n\n        def max_elem(A):\n            n = A.shape[0]\n            A_max = 0.0\n            x_max = y_max = 0\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    if abs(A[i][j]) > A_max:\n                        A_max = abs(A[i][j])\n                        x_max = i\n                        y_max = j\n            return (A_max, x_max, y_max)\n\n        def jacobi_rotation(A, p, q):\n            if abs(A[p][q]) < epsilon:\n                return A\n            else:\n                angle = np.pi / 4\n                if abs(A[p][p] - A[q][q]) < epsilon:\n                    angle = np.pi / 2\n                elif abs(A[p][p] - A[q][q]) > epsilon:\n                    angle = 0.5 * np.arctan(2.0 * A[p][q] / (A[p][p] - A[q][q]))\n                c = np.cos(angle)\n                s = np.sin(angle)\n                P = np.eye(A.shape[0])\n                P[p][p] = c\n                P[q][q] = c\n                P[p][q] = -s\n                P[q][p] = s\n                return P\n        for _ in range(num_iterations):\n            (A_max, x, y) = max_elem(A)\n            if A_max < epsilon:\n                return A\n            else:\n                P = jacobi_rotation(A, x, y)\n                A = np.dot(np.dot(P.T, A), P)\n        return A\n\n    def get_singular_values(A):\n        eigvals = np.linalg.eigvals(A)\n        singular_values = sorted((np.sqrt(abs(eigval)) for eigval in eigvals))\n        return tuple((round(sv, 4) for sv in singular_values))\n    B = np.dot(A.T, A)\n    B = jacobi_method(B)\n    return get_singular_values(B)"}
{"task_id": 12, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2), 'Matrix must be 2x2'\n    (_, singular_values, _) = np.linalg.svd(A)\n    eig_A_TA = np.linalg.eigvals(np.dot(A.T, A))\n    eig_AA_T = np.linalg.eigvals(np.dot(A, A.T))\n    eig_A_TA = np.square(eig_A_TA)\n    eig_AA_T = np.square(eig_AA_T)\n    singular_values.sort()\n    eig_A_TA.sort()\n    eig_AA_T.sort()\n    return (np.round(singular_values, 4), np.round(eig_A_TA, 4), np.round(eig_AA_T, 4))"}
{"task_id": 12, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    (a, b, c, d) = A.flatten()\n\n    def jacobi(x1, x2, x3, x4, count, tol):\n        e1 = (x1 + x4) / 2\n        e2 = e1\n        if count > 50 or np.abs(x1 - e1) < tol:\n            return (e1, e2, count)\n        f = np.abs(x1 - x4)\n        phi = np.arctan(2 * x3 / (x1 - x4)) / 2\n        (c, s) = (np.cos(phi), np.sin(phi))\n        x1_new = e1 + f * c\n        x4_new = e1 - f * c\n        return jacobi(x1_new, x2, x4_new, x4, count + 1, tol)\n    (e1, e2, _) = jacobi(a, b, c, d, 0, 0.0001)\n    return (round(np.sqrt(e1), 4), round(np.sqrt(e2), 4))"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(m: list[list[int | float]]) -> float:\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n    determinant = 0.0\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    else:\n        for current_col in range(len(matrix)):\n            sub_matrix = []\n            for i in range(1, len(matrix)):\n                row = []\n                for j in range(len(matrix)):\n                    if j != current_col:\n                        row.append(matrix[i][j])\n                sub_matrix.append(row)\n            sign = (-1) ** current_col\n            determinant += sign * matrix[0][current_col] * determinant_3x3(sub_matrix)\n    return determinant"}
{"task_id": 13, "completion_id": 1, "solution": "def determinant_3x3(matrix: list[list[int | float]]) -> float:\n    a = matrix[0][0] * (matrix[1][1] * matrix[2][2] - matrix[1][2] * matrix[2][1])\n    b = matrix[0][1] * (matrix[1][0] * matrix[2][2] - matrix[1][2] * matrix[2][0])\n    c = matrix[0][2] * (matrix[1][0] * matrix[2][1] - matrix[1][1] * matrix[2][0])\n    return a - b + c\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    det = 0\n    for i in range(4):\n        sub_matrix = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det += (-1) ** i * matrix[0][i] * determinant_3x3(sub_matrix)\n    return det"}
{"task_id": 13, "completion_id": 2, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    assert len(matrix) == 4 and len(matrix[0]) == 4, 'Input must be a 4x4 matrix.'\n\n    def det_3x3(mat):\n        return mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) - mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) + mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0])\n    det = 0\n    for c in range(4):\n        sub_matrix = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        sign = -1 if c % 2 else 1\n        det += sign * matrix[0][c] * determinant_4x4(sub_matrix) if sub_matrix else det_3x3(matrix)\n    return det"}
{"task_id": 13, "completion_id": 3, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def det3x3(m: list[list[int | float]]) -> float:\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    determinant = 0\n    for c in range(len(matrix)):\n        minor = matrix[1:]\n        height = len(minor)\n        for i in range(height):\n            minor[i] = minor[i][0:c] + minor[i][c + 1:]\n        sign = (-1) ** (0 + c)\n        sub_det = determinant_4x4(minor)\n        determinant += sign * matrix[0][c] * sub_det\n    return determinant"}
{"task_id": 13, "completion_id": 4, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(m: list[list[int | float]]) -> float:\n        diagonal_sign = 1\n        total = 0.0\n        for i in range(3):\n            diagonal = m[0][i] * diagonal_sign\n            minor_matrix = [row[:i] + row[i + 1:] for row in m[1:]]\n            recursive_determinant = determinant_3x3(minor_matrix) if len(minor_matrix) > 2 else determinant_2x2(minor_matrix)\n            total += diagonal * recursive_determinant\n            diagonal_sign *= -1\n        return total\n\n    def determinant_2x2(m: list[list[int | float]]) -> float:\n        return m[0][0] * m[1][1] - m[0][1] * m[1][0]\n    if len(matrix) == 2:\n        return determinant_2x2(matrix)\n    else:\n        return determinant_3x3(matrix)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 5, "solution": "def determinant_4x4(matrix):\n\n    def determinant_3x3(m):\n        return m[0][0] * (m[1][1] * m[2][2] - m[2][1] * m[1][2]) - m[0][1] * (m[1][0] * m[2][2] - m[2][0] * m[1][2]) + m[0][2] * (m[1][0] * m[2][1] - m[2][0] * m[1][1])\n\n    def delete_row_col(m, row, col):\n        return [m[i][:col] + m[i][col + 1:] for i in range(len(m)) if i != row]\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    determinant = 0\n    for c in range(len(matrix)):\n        determinant += (-1) ** c * matrix[0][c] * determinant_3x3(delete_row_col(matrix, 0, c))\n    return determinant"}
{"task_id": 13, "completion_id": 6, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    determinant = 0\n    for focus_column in range(len(matrix)):\n        sub_matrix = matrix[1:]\n        height = len(sub_matrix)\n        for i in range(height):\n            sub_matrix[i] = sub_matrix[i][0:focus_column] + sub_matrix[i][focus_column + 1:]\n        sign = (-1) ** (focus_column % 2)\n        determinant += sign * matrix[0][focus_column] * determinant_4x4(sub_matrix)\n    return determinant"}
{"task_id": 13, "completion_id": 7, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    det = 0\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    for i in range(len(matrix)):\n        minor_matx = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        sign = (-1) ** i\n        sub_det = determinant_4x4(minor_matx)\n        det += sign * matrix[0][i] * sub_det\n    return det"}
{"task_id": 13, "completion_id": 8, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) != 4 or not all((len(row) == 4 for row in matrix)):\n        raise ValueError('The matrix must be a 4x4 matrix')\n\n    def det_3x3(m):\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n\n    def det_4x4(m):\n        if len(m) == 2:\n            return m[0][0] * m[1][1] - m[0][1] * m[1][0]\n        else:\n            determinant = 0\n            for j in range(len(m)):\n                determinant += (-1) ** j * m[0][j] * det_4x4(minor(m, i=0, j=j))\n            return determinant\n    return det_4x4(matrix)\ndef minor(m, i, j):\n    return [row[:j] + row[j + 1:] for row in m[:i] + m[i + 1:]]\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 9, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def det_3x3(m):\n        if len(m) != 3 or len(m[0]) != 3:\n            raise ValueError('The matrix passed is not 3x3')\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n    if len(matrix) != 4 or len(matrix[0]) != 4:\n        raise ValueError('The matrix passed is not 4x4')\n    det = 0\n    for col in range(4):\n        sub_matrix = []\n        for row in range(1, 4):\n            sub_matrix.append(matrix[row][:col] + matrix[row][col + 1:])\n        cofactor = (-1) ** col * det_3x3(sub_matrix)\n        det += matrix[0][col] * cofactor\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X, y):\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    return [round(coef, 4) for coef in theta]"}
{"task_id": 14, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    ones = np.ones((X.shape[0], 1))\n    X = np.concatenate((ones, X), axis=1)\n    X_transpose = np.transpose(X)\n    theta = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n    theta = [round(coeff, 4) for coeff in theta]\n    return theta"}
{"task_id": 14, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n    xTx = np.dot(X.T, X)\n    xTx_inv = np.linalg.inv(xTx)\n    xTy = np.dot(X.T, y)\n    coefficients = np.dot(xTx_inv, xTy)\n    coefficients = [round(coefficient, 4) for coefficient in coefficients]\n    return coefficients"}
{"task_id": 14, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_normal_equation(X, y):\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.round(np.linalg.inv(X.T @ X) @ X.T @ y, 4)\n    return list(theta)"}
{"task_id": 14, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_arr = np.array(X).reshape(len(X), len(X[0]))\n    y_arr = np.array(y).reshape(len(y), 1)\n    ones = np.ones((len(X), 1))\n    X_arr = np.concatenate((ones, X_arr), axis=1)\n    theta = np.linalg.inv(X_arr.T @ X_arr) @ X_arr.T @ y_arr\n    theta = theta.round(4).tolist()\n    return theta"}
{"task_id": 14, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    beta = [round(coef, 4) for coef in beta]\n    return beta"}
{"task_id": 14, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones(X.shape[0]), X]\n    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    beta = [round(coef, 4) for coef in beta]\n    return beta"}
{"task_id": 14, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    part1 = np.linalg.inv(np.dot(X.T, X))\n    part2 = np.dot(part1, X.T)\n    theta = np.dot(part2, y)\n    return np.round(theta, decimals=4).tolist()"}
{"task_id": 14, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones(X.shape[0]), X]\n    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n    theta = [round(elem, 4) for elem in theta]\n    return theta"}
{"task_id": 14, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X)\n    y_array = np.array(y)\n    X_b = np.c_[np.ones((X_array.shape[0], 1)), X_array]\n    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_array)\n    return [round(coef, 4) for coef in theta_best]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        errors = X.dot(theta) - y\n        gradient = 1 / m * X.T.dot(errors)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()\ny = np.array([1, 2, 3])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for i in range(iterations):\n        h = X.dot(theta)\n        loss = h - y\n        gradient = X.T.dot(loss) / m\n        theta = theta - alpha * gradient\n    return list(np.round(theta, 4))"}
{"task_id": 15, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    thetas = np.zeros(X.shape[1])\n    m = X.shape[0]\n    for _ in range(iterations):\n        thetas = thetas - alpha / m * X.T @ (X @ np.array(thetas) - y)\n    thetas = np.around(thetas, decimals=4)\n    return thetas.tolist()"}
{"task_id": 15, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        gradient = 2.0 / m * X.T.dot(X.dot(theta) - y)\n        theta -= alpha * gradient\n    theta = [round(t, 4) for t in theta]\n    return theta"}
{"task_id": 15, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    beta = np.random.rand(n, 1)\n    for iter in range(iterations):\n        gradient = 2 / m * np.dot(X.T, np.dot(X, beta) - y)\n        beta = beta - alpha * gradient\n    return beta.flatten().tolist()\nX = np.hstack((ones, X))\nones = np.ones((100, 1))\ny = 2 * X[:, 1].reshape(-1, 1) + 3 + np.random.normal(0, 0.5, (100, 1))\nalpha = 0.01\niterations = 5000\nbeta = linear_regression_gradient_descent(X, y, alpha, iterations)"}
{"task_id": 15, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        loss = h - y\n        gradient = np.dot(X.T, loss) / m\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        loss = h - y\n        gradient = np.dot(X.T, loss) / m\n        theta = theta - alpha * gradient\n    return np.around(theta.tolist(), 4)"}
{"task_id": 15, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X, y, alpha, iterations):\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        grad = np.dot(X.T, h - y) / X.shape[0]\n        theta -= alpha * grad\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    Theta = np.zeros(X.shape[1])\n    m = y.size\n    cost_history = []\n    for i in range(iterations):\n        h = X.dot(Theta)\n        loss = h - y\n        gradient = X.T.dot(loss) / m\n        Theta = Theta - alpha * gradient\n        cost = np.sum(loss ** 2) / (2 * m)\n        cost_history.append(cost)\n        if i > 0 and cost_history[-2] - cost_history[-1] < 1e-09:\n            break\n    Theta = np.around(Theta, decimals=4)\n    return Theta.tolist()"}
{"task_id": 15, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for iteration in range(iterations):\n        h = np.dot(X, theta)\n        loss = h - y\n        gradient = np.dot(X.T, loss) / m\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    feature_scaling function performs two types of scaling, standardization and min-max normalization.\n\n    Parameters:\n        data (np.ndarray): A 2D numpy array where each row represents a data sample and each column represents a feature.\n\n    Returns:\n        list[list[float]]: A 2D list scaled by standardization.\n        list[list[float]]: A 2D list scaled by min-max normalization.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    normalized_data = np.round(normalized_data, 4).tolist()\n    return (standardized_data, normalized_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 16, "completion_id": 1, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    std_scaled_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    std_scaled_data = np.round(std_scaled_data, 4).tolist()\n    min_max_scaled_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_scaled_data = np.round(min_max_scaled_data, 4).tolist()\n    return (std_scaled_data, min_max_scaled_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 16, "completion_id": 2, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    std_scaler = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    std_scaled = std_scaler.tolist()\n    min_max_scaler = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_scaled = min_max_scaler.tolist()\n    return (std_scaled, min_max_scaled)\ndata = np.random.rand(100, 10)"}
{"task_id": 16, "completion_id": 3, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (np.ndarray, np.ndarray):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data = np.round(standardized_data, 4)\n    normalized_data = np.round(normalized_data, 4)\n    standardized_data = standardized_data.tolist()\n    normalized_data = normalized_data.tolist()\n    return (standardized_data, normalized_data)"}
{"task_id": 16, "completion_id": 4, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standard_scaler = preprocessing.StandardScaler()\n    data_standardized = standard_scaler.fit_transform(data)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    data_normalized = min_max_scaler.fit_transform(data)\n    data_standardized = np.round(data_standardized, 4)\n    data_normalized = np.round(data_normalized, 4)\n    data_standardized = data_standardized.tolist()\n    data_normalized = data_normalized.tolist()\n    return (data_standardized, data_normalized)"}
{"task_id": 16, "completion_id": 5, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = standardized_data.tolist()\n    min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_data = min_max_data.tolist()\n    standardized_data = [[round(i, 4) for i in row] for row in standardized_data]\n    min_max_data = [[round(i, 4) for i in row] for row in min_max_data]\n    return (standardized_data, min_max_data)"}
{"task_id": 16, "completion_id": 6, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    if not isinstance(data, np.ndarray):\n        raise TypeError('Input data must be a numpy array')\n    if len(data.shape) != 2:\n        raise ValueError('Input data must be a 2D array')\n    standardized_data = []\n    min_max_normalized_data = []\n    for i in range(data.shape[1]):\n        feature = data[:, i]\n        mean = np.mean(feature)\n        std = np.std(feature)\n        standardized_data.append(list(np.round((feature - mean) / std, 4).tolist()))\n    for i in range(data.shape[1]):\n        feature = data[:, i]\n        min_val = np.min(feature)\n        max_val = np.max(feature)\n        min_max_normalized_data.append(list(np.round((feature - min_val) / (max_val - min_val), 4).tolist()))\n    return (standardized_data, min_max_normalized_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 16, "completion_id": 7, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray):\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    standardized = ((data - means) / stds).round(4).tolist()\n    normalized = ((data - mins) / (maxs - mins)).round(4).tolist()\n    return (standardized, normalized)"}
{"task_id": 16, "completion_id": 8, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    range_vals = max_vals - min_vals\n    normalized_data = (data - min_vals) / range_vals\n    standardized_data_list = np.round(standardized_data, 4).tolist()\n    normalized_data_list = np.round(normalized_data, 4).tolist()\n    return (standardized_data_list, normalized_data_list)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 16, "completion_id": 9, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    std_scaled_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    std_scaled_data = np.round(std_scaled_data, 4).tolist()\n    min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_data = np.round(min_max_data, 4).tolist()\n    return (std_scaled_data, min_max_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        nearest_centroid_indices = []\n        for point in points:\n            distances = np.linalg.norm(centroids - point, axis=1)\n            nearest_centroid_index = np.argmin(distances)\n            nearest_centroid_indices.append(nearest_centroid_index)\n        new_centroids = np.empty_like(centroids)\n        for i in range(k):\n            assigned_points = points[np.where(np.array(nearest_centroid_indices) == i)]\n            if len(assigned_points) > 0:\n                new_centroids[i] = np.mean(assigned_points, axis=0)\n            else:\n                new_centroids[i] = centroids[i]\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 1, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    labels = np.zeros(points.shape[0], dtype=int)\n    for _ in range(max_iterations):\n        for (i, point) in enumerate(points):\n            distances = np.linalg.norm(centroids - point, axis=1)\n            labels[i] = np.argmin(distances)\n        new_centroids = np.array([points[labels == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]"}
{"task_id": 17, "completion_id": 2, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        clusters = [[] for _ in range(k)]\n        for point in points_array:\n            distances = np.linalg.norm(centroids_array - point, axis=1)\n            closest_centroid_index = np.argmin(distances)\n            clusters[closest_centroid_index].append(point)\n        for (i, cluster) in enumerate(clusters):\n            centroids_array[i] = np.mean(cluster, axis=0)\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    return final_centroids"}
{"task_id": 17, "completion_id": 3, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    centroids = np.array(initial_centroids)\n    points = np.array(points)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        for j in range(k):\n            centroids[j] = np.mean(points[labels == j], axis=0)\n    centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return centroids"}
{"task_id": 17, "completion_id": 4, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return [tuple(np.round(c, 4)) for c in centroids]"}
{"task_id": 17, "completion_id": 5, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    clusters = [[] for _ in range(k)]\n    for _ in range(max_iterations):\n        for point in points:\n            distances = np.linalg.norm(centroids - point, axis=1)\n            cluster = np.argmin(distances)\n            clusters[cluster].append(point)\n        new_centroids = []\n        for cluster in clusters:\n            new_centroid = np.mean(cluster, axis=0)\n            new_centroids.append(new_centroid)\n        new_centroids = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 6, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    closest_centroid = np.zeros(points_array.shape[0], dtype=int)\n    for iteration in range(max_iterations):\n        for (i, point) in enumerate(points_array):\n            closest_centroid[i] = np.argmin(np.linalg.norm(centroids - point, axis=1))\n        for j in range(k):\n            centroids[j] = np.mean(points_array[closest_centroid == j], axis=0)\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]"}
{"task_id": 17, "completion_id": 7, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    centroids = initial_centroids\n    for _ in range(max_iterations):\n        clusters = [[] for _ in range(k)]\n        for point in points:\n            distances = [np.sqrt((point[0] - c[0]) ** 2 + (point[1] - c[1]) ** 2) for c in centroids]\n            cluster_idx = np.argmin(distances)\n            clusters[cluster_idx].append(point)\n        for (i, cluster) in enumerate(clusters):\n            centroids[i] = np.mean(cluster, axis=0)\n    centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    return centroids"}
{"task_id": 17, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    new_centroids = np.array([0, 0])\n    for i in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        clusters = np.argmin(distances, axis=0)\n        for j in range(k):\n            new_centroids[j] = points[clusters == j].mean(axis=0)\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids.copy()\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]"}
{"task_id": 17, "completion_id": 9, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    labels = np.zeros(points.shape[0], dtype=np.int32)\n    for _ in range(max_iterations):\n        for (i, point) in enumerate(points):\n            distances = np.linalg.norm(centroids - point, axis=1)\n            labels[i] = np.argmin(distances)\n        for i in range(k):\n            centroids[i] = np.mean(points[labels == i], axis=0)\n    centroids = [(round(c[0], 4), round(c[1], 4)) for c in centroids]\n    return centroids"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    This function implements K-Fold Cross-Validation and returns a list of train-test indices for each fold.\n\n    Parameters:\n    X: Features dataset. Type: numpy ndarray\n    y: Target variable. Type: numpy ndarray\n    k: Number of folds. Type: Integer. Default value: 5\n    shuffle: If True, then shuffle the dataset. Type: Boolean. Default value: True\n    random_seed: Random seed for shuffling the data. Type: Integer. Default value: None\n\n    Returns:\n    indices: List of train-test indices for each fold. Type: List of tuples\n    \"\"\"\n    if shuffle:\n        if random_seed:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.array(range(len(X)))\n    fold_size = len(X) // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size\n        if i < k - 1:\n            train_idx = np.append(indices[:start], indices[end:])\n            test_idx = indices[start:end]\n        else:\n            train_idx = indices[:end]\n            test_idx = indices[start:]\n        folds.append((train_idx, test_idx))\n    return folds\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([1, 2, 3, 4, 5])\nfolds = k_fold_cross_validation(X, y, k=2)"}
{"task_id": 18, "completion_id": 1, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n    The function will divide the dataset into k folds and return a list of train-test indices for each fold.\n\n    Parameters:\n    X : np.ndarray\n        Input features.\n    y : np.ndarray\n        Target values.\n    k : int\n        Number of folds. Default is 5.\n    shuffle : bool\n        Whether to shuffle the dataset before splitting. Default is True.\n    random_seed : int\n        Random seed for shuffling the dataset. Default is None.\n\n    Returns:\n    list of tuples\n        Each tuple contains two lists, one for the training indices, the other for the test indices.\n    \"\"\"\n    if random_seed:\n        np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    folds = [indices[i * fold_size:(i + 1) * fold_size] for i in range(k)]\n    train_test_indices = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate(folds[:i] + folds[i + 1:])\n        train_test_indices.append((train_indices, test_indices))\n    return train_test_indices\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([0, 0, 1, 1, 1])\nk = 2\ntrain_test_indices = k_fold_cross_validation(X, y, k)"}
{"task_id": 18, "completion_id": 2, "solution": "import numpy as np"}
{"task_id": 18, "completion_id": 3, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise ValueError('X and y should be numpy arrays.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y should have the same number of samples.')\n    if shuffle:\n        np.random.seed(random_seed)\n        indices = np.random.permutation(X.shape[0])\n        (X, y) = (X[indices], y[indices])\n    fold_size = X.shape[0] // k\n    remainder = X.shape[0] % k\n    start = 0\n    train_test_indices = []\n    for i in range(k):\n        if i < remainder:\n            end = start + fold_size + 1\n        else:\n            end = start + fold_size\n        test_indices = np.arange(start, end)\n        train_indices = np.concatenate([np.arange(0, start), np.arange(end, X.shape[0])])\n        train_test_indices.append((train_indices, test_indices))\n        start = end\n    return train_test_indices"}
{"task_id": 18, "completion_id": 4, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n = len(X)\n    indices = np.arange(n)\n    if shuffle:\n        np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    fold_size = n // k\n    remainder = n % k\n    folds = []\n    start = 0\n    for i in range(remainder):\n        end = start + fold_size + 1\n        folds.append(indices[start:end])\n        start = end\n    for i in range(k - remainder):\n        end = start + fold_size\n        folds.append(indices[start:end])\n        start = end\n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate(folds[:i] + folds[i + 1:])\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 18, "completion_id": 5, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    current = 0\n    cross_val_indices = []\n    for fold_size in fold_sizes:\n        (start, stop) = (current, current + fold_size)\n        train_indices = np.concatenate([indices[:start], indices[stop:]])\n        test_indices = indices[start:stop]\n        cross_val_indices.append((train_indices, test_indices))\n        current = stop\n    return cross_val_indices\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\ny = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ncross_val_indices = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=None)"}
{"task_id": 18, "completion_id": 6, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.  \n    \n    Parameters:\n        X (np.ndarray of shape (n_samples, n_features)): Features of the dataset.\n        y (np.ndarray of shape (n_samples,)): Target of the dataset.\n        k (int): Number of folds. Default is 5.\n        shuffle (bool): Whether to shuffle the data or not. Default is True.\n        random_seed (int): Random seed for shuffling. Default is None.\n\n    Returns:\n        List of tuples, each containing train and test index for each split.\n    \"\"\"\n    np.random.seed(random_seed)\n    fold_size = len(y) // k\n    indices = list()\n    for i in range(k):\n        if i != k - 1:\n            indices.append((np.r_[0:i * fold_size, (i + 1) * fold_size:k * fold_size], np.r_[i * fold_size:(i + 1) * fold_size]))\n        else:\n            indices.append((np.r_[0:i * fold_size], np.r_[i * fold_size:]))\n    if shuffle:\n        np.random.shuffle(indices)\n    return indices"}
{"task_id": 18, "completion_id": 7, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    if shuffle:\n        if random_seed:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n    else:\n        indices = np.arange(len(X))\n    fold_size = len(X) // k\n    train_test_splits = []\n    for i in range(k):\n        test_start = i * fold_size\n        if i < k - 1:\n            test_end = test_start + fold_size\n        else:\n            test_end = len(X)\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n        train_test_splits.append((train_indices, test_indices))\n    return train_test_splits"}
{"task_id": 18, "completion_id": 8, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train/test splits for K-Fold Cross-Validation\n    \n    Parameters:\n        X (np.ndarray): Array of input features\n        y (np.ndarray): Array of target variable\n        k (int): Number of folds. Default is 5\n        shuffle (bool): Whether to shuffle the data before splitting. Default is True\n        random_seed (int or None): Seed for the random number generator. Default is None\n\n    Returns:\n        list: list of train-test splits for each fold. Each item in the list is a tuple where the first element is a list of indices for the training set and the second element is a list of indices for the test set\n    \"\"\"\n    if shuffle:\n        np.random.seed(random_seed)\n        indices = np.random.permutation(len(X))\n        (X, y) = (X[indices], y[indices])\n    fold_size = len(X) // k\n    folds = []\n    for i in range(k):\n        start_idx = i * fold_size\n        end_idx = start_idx + fold_size\n        if i == k - 1:\n            test_indices = np.arange(start_idx, len(X))\n        else:\n            test_indices = np.arange(start_idx, end_idx)\n        train_indices = np.setdiff1d(np.arange(len(X)), test_indices)\n        folds.append((train_indices, test_indices))\n    return folds\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\ny = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\nfolds = k_fold_cross_validation(X, y)"}
{"task_id": 18, "completion_id": 9, "solution": "import numpy as np\ndef k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=None):\n    n_obs = X.shape[0]\n    fold_size = n_obs // k\n    if shuffle:\n        np.random.seed(random_seed)\n        indices = np.random.permutation(n_obs)\n    for fold in range(k):\n        test_indices = indices[fold * fold_size:(fold + 1) * fold_size]\n        train_indices = np.concatenate([indices[:fold * fold_size], indices[(fold + 1) * fold_size:]])\n        (x_train, y_train) = (X[train_indices, :], y[train_indices])\n        (x_test, y_test) = (X[test_indices, :], y[test_indices])\n        yield (x_train, x_test, y_train, y_test)"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[indices]\n    eigenvectors = eigenvectors[:, indices]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 1, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nk = 2"}
{"task_id": 19, "completion_id": 2, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data_standardized.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n    eigen_pairs.sort(key=lambda x: x[0], reverse=True)\n    principal_components = np.hstack([eigen_pairs[i][1].reshape(len(eigen_pairs), 1) for i in range(k)])\n    principal_components_rounded = [[round(elem, 4) for elem in row] for row in principal_components.tolist()]\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 3, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[indices]\n    eigenvectors = eigenvectors[:, indices]\n    principal_components = eigenvectors[:, :k]\n    return np.round(principal_components.tolist(), 4)"}
{"task_id": 19, "completion_id": 4, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_mat = np.cov(data.T)\n    (eig_vals, eig_vecs) = np.linalg.eig(cov_mat)\n    ind = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[ind]\n    eig_vecs = eig_vecs[:, ind]\n    principal_components = eig_vecs[:, :k].tolist()\n    principal_components = [[round(num, 4) for num in sublist] for sublist in principal_components]\n    return principal_components"}
{"task_id": 19, "completion_id": 5, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_mat = np.cov(data.T)\n    (eig_vals, eig_vecs) = np.linalg.eig(cov_mat)\n    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))]\n    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n    result = [np.real(eig_pairs[i][1]) for i in range(k)]\n    result = [[round(val, 4) for val in feature] for feature in result]\n    return result"}
{"task_id": 19, "completion_id": 6, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    principal_components = eigenvectors[:, :k]\n    return np.round(principal_components, 4).tolist()\ndata = np.random.rand(100, 5)\nprincipal_components = pca(data, 2)"}
{"task_id": 19, "completion_id": 7, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    principal_components = eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 8, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_matrix = np.cov(data.T)\n    (eig_values, eig_vectors) = np.linalg.eig(cov_matrix)\n    idx = eig_values.argsort()[::-1]\n    eig_values = eig_values[idx]\n    eig_vectors = eig_vectors[:, idx]\n    principal_components = np.real(eig_vectors[:, :k]).T\n    principal_components = [np.round(pc, 4) for pc in principal_components]\n    return principal_components"}
{"task_id": 19, "completion_id": 9, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    eig_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]\n    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n    principal_components = np.array([eig_pairs[i][1] for i in range(k)])\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter"}
{"task_id": 20, "completion_id": 1, "solution": "import math\nfrom collections import Counter\ndef calculate_entropy(data):\n    entropy = 0\n    data_count = len(data)\n    item_labels = [item[target_attr] for item in data]\n    cntr = Counter(item_labels)\n    for count in cntr.values():\n        prob = count / data_count\n        entropy += -prob * math.log2(prob)\n    return entropy\ndef information_gain(data, attribute):\n    attr_values = set((record[attribute] for record in data))\n    information_gain = calculate_entropy(data)\n    for attr_value in attr_values:\n        subset = [record for record in data if record[attribute] == attr_value]\n        subset_entropy = calculate_entropy(subset)\n        subset_ratio = len(subset) / len(data)\n        information_gain -= subset_ratio * subset_entropy\n    return information_gain\ndef choose_attr_for_splitting(data, attributes):\n    max_info_gain = -1\n    best_attr = None\n    for attr in attributes:\n        info_gain = information_gain(data, attr)\n        if info_gain > max_info_gain:\n            max_info_gain = info_gain\n            best_attr = attr\n    return best_attr\ndef build_tree(data, attributes, target_attr):\n    data = data[:]\n    classes = [example[target_attr] for example in data]\n    default = majority_value(data, target_attr)\n    if classes.count(classes[0]) == len(classes):\n        return classes[0]\n    if len(data[0]) == 1:\n        return default\n    else:\n        best_attr = choose_attr_for_splitting(data, attributes)\n        tree = {best_attr: {}}\n        attributes.remove(best_attr)\n        attr_values = set((record[best_attr] for record in data))\n        for attr_val in attr_values:\n            attr_val = attr_val\n            examples_attr = [record for record in data if record[best_attr] == attr_val]\n            subtree = build_tree(examples_attr, attributes[:], target_attr)\n            tree[best_attr][attr_val] = subtree\n        return tree\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    attributes = attributes[:]\n    return build_tree(examples, attributes, target_attr)\nExamples = [{'Outlook': 'Sunny', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'No'}, {'Outlook': 'Sunny', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'Play': 'No'}, {'Outlook': 'Overcast', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': 'No'}, {'Outlook': 'Overcast', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'Play': 'Yes'}, {'Outlook': 'Sunny', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'Play': 'No'}]\nAttributes = ['Outlook', 'Temp', 'Humidity', 'Wind']\ntarget_attr = 'Play'\ntree = learn_decision_tree(Examples, Attributes, target_attr)"}
{"task_id": 20, "completion_id": 2, "solution": "import math\nfrom collections import Counter\ndef calculate_entropy(examples):\n    num_examples = len(examples)\n    counts = Counter((x[target_attr] for x in examples))\n    entropy = 0.0\n    for num in counts.values():\n        p = num / num_examples\n        entropy -= p * math.log2(p)\n    return entropy\ndef split_examples(examples, attr, value):\n    return [x for x in examples if x[attr] == value]\ndef choose_attribute(examples, attributes):\n    min_entropy = float('inf')\n    best_attr = None\n    for attr in attributes:\n        attr_values = set((x[attr] for x in examples))\n        entropy = 0.0\n        for value in attr_values:\n            subset = split_examples(examples, attr, value)\n            entropy += len(subset) / len(examples) * calculate_entropy(subset)\n        if entropy < min_entropy:\n            min_entropy = entropy\n            best_attr = attr\n    return best_attr\ndef plurality_value(examples):\n    counts = Counter((x[target_attr] for x in examples))\n    return max(counts, key=counts.get)\ndef learn_decision_tree(examples, attributes, target_attr):\n    if not examples:\n        return plurality_value(examples)\n    elif all((x[target_attr] == target_attr for x in examples)):\n        return examples[0][target_attr]\n    elif not attributes:\n        return plurality_value(examples)\n    else:\n        best_attr = choose_attribute(examples, attributes)\n        tree = {best_attr: {}}\n        attributes.remove(best_attr)\n        for value in set((x[best_attr] for x in examples)):\n            subset = split_examples(examples, best_attr, value)\n            subtree = learn_decision_tree(subset, attributes, target_attr)\n            tree[best_attr][value] = subtree\n        return tree"}
{"task_id": 20, "completion_id": 3, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples, attributes, target_attr):\n    target_count = Counter((x[target_attr] for x in examples))\n    if len(target_count) == 1:\n        return list(target_count.keys())[0]\n    if not attributes:\n        return most_common_class(examples, target_attr)\n    gain = [(attr, calculate_information_gain(attr, examples, target_attr)) for attr in attributes]\n    best_attr = max(gain, key=lambda x: x[1])[0]\n    tree = {best_attr: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for val in set((x[best_attr] for x in examples)):\n        subset = [example for example in examples if example[best_attr] == val]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][val] = subtree\n    return tree\ndef most_common_class(examples, target_attr):\n    classes = [example[target_attr] for example in examples]\n    return Counter(classes).most_common(1)[0][0]\ndef calculate_entropy(examples, target_attr):\n    classes = [example[target_attr] for example in examples]\n    probs = [classes.count(c) / len(classes) for c in set(classes)]\n    return -sum((p * math.log2(p) for p in probs))\ndef calculate_information_gain(attr, examples, target_attr):\n    values = set((x[attr] for x in examples))\n    entropy = calculate_entropy(examples, target_attr)\n    for val in values:\n        subset = [example for example in examples if example[attr] == val]\n        subset_entropy = calculate_entropy(subset, target_attr)\n        entropy -= len(subset) / len(examples) * subset_entropy\n    return entropy"}
{"task_id": 20, "completion_id": 4, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples, attributes, target_attr):\n    data = examples[:]\n    vals = [record[target_attr] for record in data]\n    default = majorClass(attributes, data, target_attr)\n    if not data or len(attributes) - 1 <= 0:\n        return default\n    elif vals.count(vals[0]) == len(vals):\n        return vals[0]\n    else:\n        best = choose_attr(data, attributes, target_attr)\n        tree = {best: {}}\n        for val in getValues(data, best):\n            examples_i = getExamples(data, best, val)\n            newAttr = attributes[:]\n            newAttr.remove(best)\n            subtree = learn_decision_tree(examples_i, newAttr, target_attr)\n            tree[best][val] = subtree\n    return tree\ndef majorClass(attributes, data, target_attr):\n    data = data[:]\n    freq = {}\n    index = attributes.index(target_attr)\n    values = [record[index] for record in data]\n    most = max(set(values), key=values.count)\n    return most\ndef choose_attr(data, attributes, target_attr):\n    data = data[:]\n    best = attributes[0]\n    maxGain = 0\n    for attr in attributes:\n        gain = info_gain(data, attr, target_attr)\n        if gain > maxGain:\n            maxGain = gain\n            best = attr\n    return best\ndef getValues(data, attr):\n    data = data[:]\n    return set([record[attr] for record in data])\ndef getExamples(data, best, val):\n    data = data[:]\n    rtn = [record for record in data if record[best] == val]\n    return rtn\ndef entropy(data, target_attr):\n    data = data[:]\n    val_freq = {}\n    data_entropy = 0.0\n    for record in data:\n        if record[target_attr] in val_freq:\n            val_freq[record[target_attr]] += 1.0\n        else:\n            val_freq[record[target_attr]] = 1.0\n    for freq in val_freq.values():\n        data_entropy += -freq / len(data) * math.log(freq / len(data), 2)\n    return data_entropy\ndef info_gain(data, attr, target_attr):\n    val_freq = {}\n    subset_entropy = 0.0\n    for record in data:\n        if record[attr] in val_freq:\n            val_freq[record[attr]] += 1.0\n        else:\n            val_freq[record[attr]] = 1.0\n    for val in val_freq.keys():\n        val_prob = val_freq[val] / sum(val_freq.values())\n        data_subset = [record for record in data if record[attr] == val]\n        subset_entropy += val_prob * entropy(data_subset, target_attr)\n    return entropy(data, target_attr) - subset_entropy"}
{"task_id": 20, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples, attributes, target_attr):\n\n    def entropy(examples):\n        count = Counter([x[target_attr] for x in examples])\n        total = len(examples)\n        return -sum((count[i] / total * math.log2(count[i] / total) for i in count))\n\n    def gain(examples, attr):\n        count = Counter([x[attr] for x in examples])\n        total = len(examples)\n        return sum((count[x] / total * entropy([y for y in examples if y[attr] == x]) for x in count))\n\n    def majority_value(examples):\n        count = Counter([x[target_attr] for x in examples])\n        return max(count, key=count.get)\n    if not examples:\n        return {}\n    elif all((x[target_attr] == examples[0][target_attr] for x in examples)):\n        return {'label': examples[0][target_attr]}\n    elif not attributes:\n        return {'label': majority_value(examples)}\n    else:\n        best_attr = max(attributes, key=lambda x: gain(examples, x))\n        tree = {best_attr: {}}\n        remaining_attr = [x for x in attributes if x != best_attr]\n        for val in set([x[best_attr] for x in examples]):\n            subset = [x for x in examples if x[best_attr] == val]\n            subtree = learn_decision_tree(subset, remaining_attr, target_attr)\n            tree[best_attr][val] = subtree\n        return tree"}
{"task_id": 20, "completion_id": 6, "solution": "import math\nfrom collections import Counter\ndef entropy(data):\n    total_instances = len(data)\n    if total_instances <= 0:\n        return 0\n    count = Counter(data)\n    entropy = 0.0\n    for instances in count.values():\n        proportion = instances / total_instances\n        entropy += -1 * proportion * math.log2(proportion)\n    return entropy\ndef information_gain(data, attr):\n    total_instances = len(data)\n    attr_values = set((record[attr] for record in data))\n    weighted_entropy = 0.0\n    for value in attr_values:\n        subset_size = sum((record[attr] == value for record in data))\n        subset_entropy = entropy([record[target_attr] for record in data if record[attr] == value])\n        weighted_entropy += subset_size / total_instances * subset_entropy\n    return entropy(data) - weighted_entropy\ndef majority_value(data):\n    count = Counter((record[target_attr] for record in data))\n    return count.most_common(1)[0][0]\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    data = examples[:]\n    if not data or len(attributes) <= 0:\n        return majority_value(examples)\n    elif len(set((record[target_attr] for record in data))) == 1:\n        return data[0][target_attr]\n    else:\n        best = max(attributes, key=lambda attr: information_gain(data, attr))\n        tree = {best: {}}\n        attributes.remove(best)\n        for val in set((record[best] for record in data)):\n            sub_data = [record for record in data if record[best] == val]\n            sub_tree = learn_decision_tree(sub_data, attributes, target_attr)\n            tree[best][val] = sub_tree\n        return tree"}
{"task_id": 20, "completion_id": 7, "solution": "import math\nfrom collections import Counter\ndef entropy(attributes: list[str]) -> float:\n    \"\"\"Compute the entropy of a list of attributes.\"\"\"\n    counts = Counter(attributes)\n    entropy_value = 0.0\n    for attr_count in counts.values():\n        p = attr_count / len(attributes)\n        entropy_value += -p * math.log2(p)\n    return entropy_value\ndef majority_value(attributes: list[str]) -> str:\n    \"\"\"Determine the majority value in a list of attributes.\"\"\"\n    counts = Counter(attributes)\n    return counts.most_common(1)[0][0]\ndef choose_best_attr(examples: list[dict], attributes: list[str]) -> str:\n    \"\"\"Choose the attribute with highest information gain.\"\"\"\n    entropies = [entropy([e[attr] for e in examples]) for attr in attributes]\n    entropies_dict = dict(zip(attributes, entropies))\n    max_gain = max(entropies_dict.values())\n    for (attr, entropy_value) in entropies_dict.items():\n        if entropy_value == max_gain:\n            return attr\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"Learn a decision tree from a list of examples.\"\"\"\n    values = [e[target_attr] for e in examples]\n    if not examples or len(set(values)) == 0:\n        return majority_value(values)\n    elif len(set(values)) == 1:\n        return values[0]\n    elif not attributes:\n        return majority_value(values)\n    else:\n        best_attr = choose_best_attr(examples, attributes)\n        tree = {best_attr: {}}\n        for (attr_value, examples_subset) in partition_examples(examples, best_attr).items():\n            subtree = learn_decision_tree(examples_subset, [attr for attr in attributes if attr != best_attr], target_attr)\n            tree[best_attr][attr_value] = subtree\n        return tree\nexamples = ...\nattributes = ...\ntarget_attr = ...\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 20, "completion_id": 8, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implementation of the decision tree learning algorithm for classification.\n  \n    Parameters:\n    examples (list): A list of examples (each example is a dict of attribute-value pairs).\n    attributes (list): A list of attribute names.\n    target_attr (str): The name of the attribute that we want to predict.\n\n    Returns:\n    dict: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples: list[dict]):\n        \"\"\"\n        Compute the entropy of a list of examples.\n        \"\"\"\n        counts = Counter((x[target_attr] for x in examples))\n        total = len(examples)\n        return -sum((count / total * math.log2(count / total) for count in counts.values()))\n\n    def information_gain(entropy_before: float, examples: list[dict], attribute: str):\n        \"\"\"\n        Compute the information gain after splitting the examples by a given attribute.\n        \"\"\"\n        if not examples:\n            return 0\n        counts = Counter((x[attribute] for x in examples))\n        total = len(examples)\n        entropy_after = sum((count / total * entropy([e for e in examples if e[attribute] == attr]) for (attr, count) in counts.items()))\n        return entropy_before - entropy_after\n\n    def most_common_target(examples: list[dict]):\n        \"\"\"\n        Get the most common target value among the examples.\n        \"\"\"\n        counts = Counter((x[target_attr] for x in examples))\n        return counts.most_common(1)[0][0]\n    if not examples:\n        return {}\n    if all((x[target_attr] == examples[0][target_attr] for x in examples)):\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_target(examples)\n    start_entropy = entropy(examples)\n    best_attribute = max(attributes, key=lambda attr: information_gain(start_entropy, examples, attr))\n    tree = {best_attribute: {}}\n    remaining_attr = [x for x in attributes if x != best_attribute]\n    for value in set((x[best_attribute] for x in examples)):\n        filtered_examples = [e for e in examples if e[best_attribute] == value]\n        subtree = learn_decision_tree(filtered_examples, remaining_attr, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 9, "solution": "import math\nfrom collections import Counter\nimport pandas as pd\ndef entropy(data):\n    total_instances = len(data)\n    label_counts = Counter(data)\n    entropy_value = 0.0\n    for label in label_counts:\n        prob = label_counts[label] / total_instances\n        entropy_value += -prob * math.log2(prob)\n    return entropy_value\ndef info_gain(data, attr):\n    total_instances = len(data)\n    attr_values = set(data[attr])\n    gain = entropy(data[target_attribute])\n    for value in attr_values:\n        subset = data[data[attr] == value]\n        gain -= len(subset) / total_instances * entropy(subset[target_attribute])\n    return gain\ndef learn_decision_tree(data, attributes, target_attribute):\n    data_values = data[target_attribute]\n    if len(attributes) == 0 or len(data) == len(data_values.unique()):\n        return data_values.mode()[0]\n    elif len(data) == len(data_values.unique()):\n        return data_values.mode()[0]\n    else:\n        best = attributes[0]\n        for attr in attributes:\n            gain = info_gain(data, attr)\n            if gain > info_gain(data, best):\n                best = attr\n        tree = {best: {}}\n        remaining = [a for a in attributes if a != best]\n        for value in data[best].unique():\n            subset = data[data[best] == value]\n            subtree = learn_decision_tree(subset, remaining, target_attribute)\n            tree[best][value] = subtree\n        return tree\nattributes = ['age', 'income', 'maritial_status']\ntarget_attribute = 'buys_computer'\ndata = pd.DataFrame(data)\ntree = learn_decision_tree(data, attributes, target_attribute)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n    bias = 0\n    if kernel == 'rbf':\n        k = lambda x, y: np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n    elif kernel == 'linear':\n        k = lambda x, y: np.dot(x, y)\n    else:\n        raise ValueError(\"Invalid kernel. Supported kernels are 'linear' and 'rbf'.\")\n    K = np.array([[k(x1, x2) for x2 in data] for x1 in data])\n    for _ in range(iterations):\n        for n in range(num_samples):\n            pred_n = bias + np.dot(K[n, :], alpha * labels)\n            if labels[n] * pred_n < 1:\n                alpha[n] += 1 / (lambda_val * num_samples)\n            else:\n                alpha[n] *= 1 - 1 / (lambda_val * num_samples)\n        bias = 1 / lambda_val * np.sum([n * labels[i] * K[i, i] for (i, n) in enumerate(alpha)])\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 1, "solution": "import numpy as np\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2.T)\ndef rbf_kernel(x, y, sigma=1.0):\n    if np.ndim(x) == 1 and np.ndim(y) == 1:\n        result = np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n    elif np.ndim(x) > 1 and np.ndim(y) == 1 or (np.ndim(x) == 1 and np.ndim(y) > 1):\n        result = np.exp(-np.linalg.norm(x - y, axis=1) ** 2 / (2 * sigma ** 2))\n    elif np.ndim(x) > 1 and np.ndim(y) > 1:\n        result = np.exp(-np.linalg.norm(x[:, np.newaxis] - y[np.newaxis, :], axis=2) ** 2 / (2 * sigma ** 2))\n    return result\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n    bias = 0\n    kernel_func = linear_kernel\n    if kernel == 'rbf':\n        kernel_func = rbf_kernel\n    kernel_matrix = kernel_func(data, data, sigma)\n    for _ in range(iterations):\n        for i in range(num_samples):\n            value = labels[i] * (np.sum(alpha * labels * kernel_matrix[i]) + bias)\n            if labels[i] * value < 1:\n                alpha[i] += 1\n                bias += labels[i]\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 2, "solution": "import numpy as np\ndef linear_kernel(x, y, **kwargs):\n    return np.dot(x, y)\ndef rbf_kernel(x, y, sigma=1.0):\n    return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n    kernel_func = linear_kernel if kernel == 'linear' else rbf_kernel\n    for _ in range(iterations):\n        for i in range(num_samples):\n            hypothesis = np.sum([alpha[j] * labels[j] * kernel_func(data[j], data[i], sigma) for j in range(num_samples)])\n            if labels[i] * hypothesis < 1:\n                alpha[i] += 1 / (lambda_val * num_samples) * labels[i]\n    bias = np.mean([labels[i] - np.sum([alpha[j] * labels[j] * kernel_func(data[j], data[i], sigma) for j in range(num_samples)]) for i in range(num_samples)])\n    return (np.round(alpha, 4), np.round(bias, 4))\ny = np.array([-1, 1, 1, -1])"}
{"task_id": 21, "completion_id": 3, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n\n    def kernel_func(x_i, x_j):\n        if kernel == 'linear':\n            return np.dot(x_i, x_j)\n        elif kernel == 'RBF':\n            delta = np.linalg.norm(x_i - x_j) ** 2\n            return np.exp(-delta / (2 * sigma ** 2))\n    K = np.zeros((num_samples, num_samples))\n    for i in range(num_samples):\n        for j in range(num_samples):\n            K[i, j] = kernel_func(data[i], data[j])\n    for _ in range(iterations):\n        for i in range(num_samples):\n            error = labels[i] - np.dot(alpha * labels, K[i])\n            alpha[i] += lambda_val * labels[i] * error\n    support_vectors = alpha > 1e-05\n    bias = np.mean(labels[support_vectors] - np.dot(K[support_vectors].T, alpha * labels[support_vectors]))\n    alpha = alpha.tolist()\n    return (alpha, round(bias, 4))"}
{"task_id": 21, "completion_id": 4, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (nsamples, nfeatures) = data.shape\n    labels[labels == 0] = -1\n    K = np.zeros((nsamples, nsamples))\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'RBF':\n        for i in range(nsamples):\n            for j in range(nsamples):\n                K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n    alpha = np.zeros(nsamples)\n    for _ in range(iterations):\n        schedule = 1.0 / (lambda_val * (_ + 1))\n        for n in range(nsamples):\n            decision = labels[n] * np.dot(alpha * labels, K[:, n])\n            if decision < 1:\n                alpha[n] += schedule * (labels[n] - decision)\n    bias = labels - np.dot(alpha * labels, K)\n    bias = np.mean(bias)\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 5, "solution": "import numpy as np\ndef linear_kernel(x, y):\n    return np.dot(x, y)\ndef rbf_kernel(x, y, sigma=1.0):\n    diff = x - y\n    return np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n    bias = 0.0\n    if kernel == 'linear':\n        K = linear_kernel\n    elif kernel == 'rbf':\n        K = lambda x, y: rbf_kernel(x, y, sigma)\n    else:\n        raise ValueError('Invalid kernel function')\n    for _ in range(iterations):\n        for i in range(num_samples):\n            pred = 0.0\n            for j in range(num_samples):\n                pred += alpha[j] * labels[j] * K(data[i], data[j])\n            pred += bias\n            if labels[i] * pred < 1:\n                alpha[i] += 1\n                bias += lambda_val * labels[i]\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 6, "solution": "import numpy as np\ndef linear_kernel(x, y):\n    return np.dot(x, y)\ndef rbf_kernel(x, y, sigma=1.0):\n    diff = x - y\n    return np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0\n    if kernel == 'linear':\n        kernel_function = linear_kernel\n    elif kernel == 'rbf':\n        kernel_function = lambda x, y: rbf_kernel(x, y, sigma)\n    else:\n        raise ValueError('Invalid kernel')\n    for _ in range(iterations):\n        for i in range(n_samples):\n            output = sum((alpha[j] * labels[j] * kernel_function(data[j], data[i]) for j in range(n_samples))) + bias\n            if labels[i] * output < 1:\n                alpha[i] += lambda_val * labels[i]\n    for i in range(n_samples):\n        if alpha[i] != 0:\n            bias = labels[i] - sum((alpha[j] * labels[j] * kernel_function(data[j], data[i]) for j in range(n_samples)))\n            break\n    return (alpha.round(4).tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 7, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    k = Kernel.rbf(sigma) if kernel == 'rbf' else Kernel.linear()\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = k(data[i], data[j])\n    for t in range(1, iterations + 1):\n        for i in range(n_samples):\n            error = labels[i] * (np.inner(K[i], alpha) - labels[i] * alpha[i])\n            if error < 1:\n                alpha[i] = alpha[i] + labels[i] * 1 / (lambda_val * (t + 1))\n            else:\n                alpha[i] = alpha[i]\n    bias = np.mean([labels[i] - np.inner(alpha, K[i]) for i in range(n_samples)])\n    return (alpha.tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 8, "solution": "import numpy as np\ndef gaussian_kernel(x, y, sigma=1.0):\n    return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    alpha = np.zeros(len(data))\n    labels = np.where(labels <= 0, -1, 1)\n    if kernel == 'linear':\n        kernel_fun = linear_kernel\n    elif kernel == 'rbf':\n        kernel_fun = lambda x, y: gaussian_kernel(x, y, sigma)\n    else:\n        raise ValueError('Kernel not supported')\n    for iteration in range(iterations):\n        for j in range(len(data)):\n            kj = kernel_fun(data[j], data[j])\n            ayj = sum([alpha[i] * labels[i] * kernel_fun(data[i], data[j]) for i in range(len(data))])\n            aj_new = aj + 1 / (lambda_val * (iteration + 1)) * (labels[j] - ayj)\n            alpha[j] = np.sign(aj_new) * min(abs(aj_new), 1)\n    b = sum([alpha[i] * labels[i] * kernel_fun(data[i], data[j]) for i in range(len(data))])\n    return (alpha.tolist(), round(b, 4))"}
{"task_id": 21, "completion_id": 9, "solution": "import numpy as np\ndef linear_kernel(x, y):\n    return np.dot(x, y)\ndef rbf_kernel(x, y, sigma=1.0):\n    diff = x - y\n    return np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (num_samples, num_features) = data.shape\n    labels = labels.reshape(-1, 1)\n    alpha = np.zeros((num_samples, 1))\n    bias = np.zeros((num_samples, 1))\n    if kernel == 'linear':\n        kernel_func = linear_kernel\n    elif kernel == 'rbf':\n        kernel_func = lambda x, y: rbf_kernel(x, y, sigma)\n    else:\n        raise ValueError('Unknown kernel function')\n    for _ in range(iterations):\n        for i in range(num_samples):\n            kernel_val = kernel_func(data[i, :], data[i, :])\n            pred_val = bias[i] + np.sum(alpha * labels * kernel_val)\n            if labels[i] * pred_val < 1:\n                alpha[i] += 1\n                bias[i] += lambda_val * labels[i]\n    final_bias = np.mean(labels - np.sum(alpha * labels * kernel_val))\n    return (alpha.ravel().astype('float32'), final_bias)\ndata = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\nlabels = np.array([1, -1, 1, -1, 1])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 1, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 2, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 3, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the output of the sigmoid activation function.\n  \n    Parameters:\n    z (float): The input value.\n  \n    Returns:\n    float: The output of the activation function.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 4, "solution": "import math\ndef sigmoid(z: float) -> float:\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 5, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 6, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 7, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 8, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input.\n\n    Args:\n    z: A float representing the input to the sigmoid function.\n\n    Returns:\n    The output of the sigmoid function for the input, rounded to four decimal places.\n    \"\"\"\n    sig = 1 / (1 + math.exp(-z))\n    sig = round(sig, 4)\n    return sig"}
{"task_id": 22, "completion_id": 9, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Computes the sigmoid activation function.\"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 1, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n\n    Args:\n    scores: list of scores for which to compute the softmax values.\n\n    Returns:\n    list of softmax values rounded to four decimal places.\n    \"\"\"\n    total = sum((math.exp(i) for i in scores))\n    softmax_values = [round(math.exp(i) / total, 4) for i in scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 2, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_list = [math.exp(i) for i in scores]\n    sum_exp_list = sum(exp_list)\n    return [round(i / sum_exp_list, 4) for i in exp_list]"}
{"task_id": 23, "completion_id": 3, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 4, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 5, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 23, "completion_id": 6, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(i) for i in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [i / sum_exp_scores for i in exp_scores]\n    return [round(value, 4) for value in softmax_values]\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 7, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_scores = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_scores\nscores = [1.6, 2.7, 3.4, 1.0]"}
{"task_id": 23, "completion_id": 8, "solution": "import math\ndef softmax(scores):\n    m = max(scores)\n    total = sum((math.exp(i - m) for i in scores))\n    return [round(math.exp(i - m) / total, 4) for i in scores]\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 9, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    softmax_values = []\n    scores_exp = [math.exp(i) for i in scores]\n    scores_exp_sum = sum(scores_exp)\n    for score in scores:\n        softmax_value = round(math.exp(score) / scores_exp_sum, 4)\n        softmax_values.append(softmax_value)\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Calculate the sigmoid function for a given input\n    \"\"\"\n    return 1 / (1 + math.exp(-x))\ndef predict(features: list[float], weights: list[float], bias: float) -> float:\n    \"\"\"\n    Generate prediction for a single example by taking a weighted sum of its features\n    and applying the sigmoid activation function.\n    \"\"\"\n    dot_product = 0.0\n    dot_product = sum([feature * weight for (feature, weight) in zip(features, weights)]) + bias\n    return sigmoid(dot_product)\ndef mse(y_true: list[float], y_pred: list[float]) -> float:\n    \"\"\"\n    Calculate the mean squared error between the true and predicted labels.\n    \"\"\"\n    return sum([(yt - yp) ** 2 for (yt, yp) in zip(y_true, y_pred)]) / len(y_true)\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Single Neuron Function\n    \"\"\"\n    assert len(features[0]) == len(weights), 'Number of features and weights do not match'\n    predictions = [predict(feature, weights, bias) for feature in features]\n    loss = mse(labels, predictions)\n    predictions = [round(pred, 4) for pred in predictions]\n    loss = round(loss, 4)\n    return (predictions, loss)"}
{"task_id": 24, "completion_id": 1, "solution": "import numpy as np\nimport math\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float):\n    features = np.array(features)\n    weights = np.array(weights)\n    labels = np.array(labels)\n    z = np.dot(features, weights) + bias\n    predicted_probabilities = [sigmoid(z_i) for z_i in z]\n    mse = np.mean((np.array(predicted_probabilities) - labels) ** 2)\n    predicted_probabilities = [round(prob, 4) for prob in predicted_probabilities]\n    mse = round(mse, 4)\n    return (predicted_probabilities, mse)"}
{"task_id": 24, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predictions = []\n    errors = []\n    for i in range(len(features)):\n        activation = np.dot(features[i], weights) + bias\n        prediction = 1.0 / (1.0 + math.exp(-activation))\n        predictions.append(round(prediction, 4))\n        error = round(pow(labels[i] - prediction, 2), 4)\n        errors.append(error)\n    mse = round(sum(errors) / len(errors), 4)\n    return (predictions, mse)"}
{"task_id": 24, "completion_id": 3, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef single_neuron_model(features: List[List[float]], labels: List[int], weights: List[float], bias: float) -> Tuple[List[float], float]:\n    features = np.array(features)\n    weights = np.array(weights)\n    results = []\n    for feature in features:\n        result = np.dot(feature, weights) + bias\n        results.append(sigmoid(result))\n    predicted_probs = np.around(results, decimals=4).tolist()\n    mse = np.mean(np.square(np.subtract(labels, predicted_probs)))\n    mse = round(mse, 4)\n    return (predicted_probs, mse)"}
{"task_id": 24, "completion_id": 4, "solution": "import numpy as np"}
{"task_id": 24, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    output = []\n    for i in range(len(features)):\n        feature = features[i]\n        result = bias\n        for j in range(len(feature)):\n            result += feature[j] * weights[j]\n        output.append(sigmoid(result))\n    mse = np.mean(np.square(np.subtract(labels, output)))\n    return (np.round(output, 4).tolist(), round(mse, 4))"}
{"task_id": 24, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    weights = np.array(weights)\n    features = np.array(features)\n    labels = np.array(labels)\n    epochs = 1000\n    learning_rate = 0.1\n    n = len(features)\n    for epoch in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        weights = weights - learning_rate * (1 / n * np.dot(features.T, predictions - labels))\n        bias = bias - learning_rate * (1 / n * np.sum(predictions - labels))\n    final_output = sigmoid(np.dot(features, weights) + bias)\n    mse = np.mean((final_output - labels) ** 2)\n    return (final_output.round(4).tolist(), round(mse, 4))\nfeatures = [[0.1, 0.5], [0.2, 0.3], [0.7, 0.4], [0.8, 0.6]]\nlabels = [1, 0, 1, 0]\nweights = [0.1, 0.2]\nbias = 0.3"}
{"task_id": 24, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predictions = [sum([feature[i] * weights[i] for i in range(len(weights))]) for feature in features]\n    predictions = [prediction + bias for prediction in predictions]\n    predictions = [1 / (1 + math.exp(-pred)) for pred in predictions]\n    mse = np.mean(np.square(np.array(labels) - np.array(predictions)))\n    predictions = [round(pred, 4) for pred in predictions]\n    mse = round(mse, 4)\n    return (predictions, mse)"}
{"task_id": 24, "completion_id": 8, "solution": "import numpy as np\nimport math\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = np.dot(feature_vector, weights) + bias\n        predicted_probabilities.append(sigmoid(weighted_sum))\n    mse = np.mean((np.array(predicted_probabilities) - np.array(labels)) ** 2)\n    return ([round(prob, 4) for prob in predicted_probabilities], round(mse, 4))"}
{"task_id": 24, "completion_id": 9, "solution": "import numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    features = np.array(features)\n    weights = np.array(weights)\n    weighted_sums = np.dot(features, weights) + bias\n    probabilities = sigmoid(weighted_sums).round(4).tolist()\n    labels = np.array(labels)\n    mse = 1 / len(labels) * np.sum((labels - probabilities) ** 2).round(4)\n    return (probabilities, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_per_epoch = []\n    for i in range(epochs):\n        activations = np.dot(features, weights) + bias\n        outputs = sigmoid(activations)\n        mse = np.mean((outputs - labels) ** 2)\n        mse_per_epoch.append(round(mse, 4))\n        error = outputs - labels\n        d_outputs = error * sigmoid_derivative(outputs)\n        weights -= learning_rate * np.dot(features.T, d_outputs)\n        bias -= learning_rate * np.sum(d_outputs, axis=0)\n    return (weights.tolist(), round(bias, 4), mse_per_epoch)"}
{"task_id": 25, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mse(y_true, y_pred):\n    return np.mean(np.power(y_true - y_pred, 2))\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    errors = []\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        error = mse(labels, predictions)\n        errors.append(round(error, 4))\n        error_formula = 2 * (labels - predictions) * sigmoid_derivative(predictions)\n        weights += learning_rate * np.dot(features.T, error_formula)\n        bias += learning_rate * np.sum(error_formula)\n    return (weights.tolist(), round(bias, 4), errors)"}
{"task_id": 25, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef derivative_sigmoid(x):\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mse_loss(predicted, actual):\n    return np.mean((predicted - actual) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        error = predictions - labels\n        derivative_loss_prediction = error * derivative_sigmoid(predictions)\n        derivative_prediction_weights = features\n        derivative_loss_weights = np.dot(derivative_prediction_weights.T, derivative_loss_prediction)\n        derivative_loss_bias = np.sum(derivative_loss_prediction)\n        weights -= learning_rate * derivative_loss_weights\n        bias -= learning_rate * derivative_loss_bias\n        mse = mse_loss(predictions, labels)\n        mse_values.append(np.round(mse, 4))\n    return (np.round(weights, 4).tolist(), np.round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef mse(y, y_hat):\n    return np.mean((y - y_hat) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for i in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_hat = sigmoid(z)\n        mse_value = np.round(mse(labels, y_hat), 4)\n        mse_values.append(mse_value)\n        dz = y_hat - labels\n        dw = 1 / len(labels) * np.dot(features.T, dz)\n        db = 1 / len(labels) * np.sum(dz)\n        weights = weights - learning_rate * dw\n        bias = bias - learning_rate * db\n    return (np.round(weights, 4), np.round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predicted_output = sigmoid(linear_output)\n        mse = np.mean((predicted_output - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        error = predicted_output - labels\n        dcost_dpred = error\n        dpred_dz = sigmoid_derivative(predicted_output)\n        dz_dw = features\n        dcost_dz = dcost_dpred * dpred_dz\n        weights -= learning_rate * np.dot(dz_dw.T, dcost_dz)\n        bias -= learning_rate * np.sum(dcost_dz)\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1.0 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear = np.dot(features, weights) + bias\n        predicted = sigmoid(linear)\n        mse = np.sum((predicted - labels) ** 2) / len(labels)\n        mse_values.append(round(mse, 4))\n        dL_dpred = 2 * (predicted - labels)\n        dpred_dz = sigmoid_derivative(predicted)\n        dz_dw1 = features[:, 0]\n        dz_dw2 = features[:, 1]\n        dz_db = 1\n        dL_dz = dL_dpred * dpred_dz\n        dL_dw1 = dL_dz * dz_dw1\n        dL_dw2 = dL_dz * dz_dw2\n        dL_db = dL_dz * dz_db\n        weights[0] -= learning_rate * dL_dw1.mean()\n        weights[1] -= learning_rate * dL_dw2.mean()\n        bias -= learning_rate * dL_db.mean()\n    return (weights.round(4).tolist(), round(bias, 4), mse_values)"}
{"task_id": 25, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: float) -> float:\n    return sigmoid(x) * (1 - sigmoid(x))\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_list = []\n    for i in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        mse = np.square(predictions - labels).mean()\n        mse_list.append(np.round(mse, 4))\n        dmse = 2 * (predictions - labels)\n        dsigmoid = sigmoid_derivative(z)\n        dz = dmse * dsigmoid\n        dw = np.dot(features.T, dz) / len(features)\n        db = np.sum(dz) / len(features)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (weights, np.round(bias, 4), mse_list)"}
{"task_id": 25, "completion_id": 7, "solution": "import numpy as np\ndef train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n    weights = initial_weights\n    bias = initial_bias\n    mse_log = []\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(x):\n        return x * (1 - x)\n    for _ in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = np.mean((labels - predictions) ** 2)\n        mse_log.append(round(mse, 4))\n        error = labels - predictions\n        d_predictions = 2 * error * sigmoid_derivative(predictions)\n        derivatives_weights = np.dot(features.T, d_predictions)\n        derivative_bias = np.sum(d_predictions)\n        weights += derivatives_weights * learning_rate\n        bias += derivative_bias * learning_rate\n    return (weights.tolist(), round(bias, 4), mse_log)"}
{"task_id": 25, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef derivative_sigmoid(x):\n    return x * (1 - x)\ndef mse_loss(predicted, actual):\n    return np.mean((predicted - actual) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.reshape(-1, 1)\n    bias = initial_bias\n    mses = []\n    for _ in range(epochs):\n        a = np.dot(features, weights) + bias\n        predicted = sigmoid(a)\n        loss = mse_loss(predicted, labels)\n        mses.append(round(loss, 4))\n        dL_dpred = 2 * (predicted - labels)\n        dpred_da = derivative_sigmoid(predicted)\n        dL_da = dL_dpred * dpred_da\n        dL_db = np.sum(dL_da)\n        dL_dw = np.dot(features.T, dL_da)\n        dL_db = np.sum(dL_da)\n        weights -= learning_rate * dL_dw\n        bias -= learning_rate * dL_db\n    return (weights.flatten(), bias, mses)"}
{"task_id": 25, "completion_id": 9, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1.0 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_list = []\n    n_features = features.shape[1]\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        mse = np.mean((predictions - labels.reshape(-1, 1)) ** 2)\n        mse_list.append(np.round(mse, 4))\n        error = predictions - labels.reshape(-1, 1)\n        d_weights = np.dot(features.T, error * sigmoid_derivative(predictions)) / n_features\n        d_bias = np.sum(error * sigmoid_derivative(predictions)) / n_features\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights.tolist(), np.round(bias, 4), mse_list)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 1, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 2, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 3, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 4, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 5, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 6, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 7, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 8, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 9, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._prev = set(_children)\n        self._op = _op\n        self._backward = lambda : None\n\n    def __add__(self, other):\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B, dtype=float)\n    C = np.array(C, dtype=float)\n    P = np.dot(C, np.linalg.inv(B))\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P"}
{"task_id": 27, "completion_id": 1, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    P = np.matmul(C, np.linalg.inv(B))\n    P = P.round(4)\n    return P.tolist()"}
{"task_id": 27, "completion_id": 2, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Transforms a basis B to a basis C.\n\n    Parameters:\n    B (list[list[int]]): A list of list where each sub list represents a basis vector. Example: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    C (list[list[int]]): A list of list where each sub list represents a basis vector. Example: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    \n    Returns:\n    list[list[float]]: The transformation matrix applied to basis B to get basis C.\n    \"\"\"\n    B = np.array(B, dtype=float)\n    C = np.array(C, dtype=float)\n    P = np.linalg.inv(B.T) @ C.T\n    P = np.round(P, 4)\n    return P.tolist()"}
{"task_id": 27, "completion_id": 3, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    try:\n        P = np.matmul(C, np.linalg.inv(B))\n    except np.linalg.LinAlgError:\n        return 'Basis vectors are not linearly independent, cannot compute transformation matrix.'\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P"}
{"task_id": 27, "completion_id": 4, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    B_inv = np.linalg.inv(B)\n    P_BC = np.dot(C, B_inv)\n    P_BC = np.round(P_BC, 4)\n    return P_BC.tolist()"}
{"task_id": 27, "completion_id": 5, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]):\n    B = np.array(B)\n    C = np.array(C)\n    if np.linalg.matrix_rank(B) != 3:\n        raise ValueError('Bases B and C must be independent.')\n    P = C @ np.linalg.inv(B)\n    P = np.round(P, 4)\n    P = P.tolist()\n    return P"}
{"task_id": 27, "completion_id": 6, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    P = np.linalg.inv(C_matrix) @ B_matrix\n    rounded_P = np.round(P, 4)\n    return rounded_P.tolist()"}
{"task_id": 27, "completion_id": 7, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.solve(B.T, C.T).T\n    P = np.round(P, 4)\n    return P.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 3, 4], [0, 1, 0], [2, 5, 1]]"}
{"task_id": 27, "completion_id": 8, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    B_inv = np.linalg.inv(B)\n    P = np.dot(C, B_inv)\n    P = np.round(P, 4)\n    return P.tolist()"}
{"task_id": 27, "completion_id": 9, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]):\n    B = np.array(B)\n    C = np.array(C)\n    P = np.dot(C, np.linalg.inv(B))\n    return np.round(P, 4).tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.matmul(A.T, A)\n    (eig_vals, eig_vecs) = np.linalg.eig(ATA)\n    sing_vals = np.sqrt(eig_vals)\n    order = np.argsort(sing_vals)[::-1]\n    sing_vals = sing_vals[order]\n    eig_vecs = eig_vecs[:, order]\n    S = np.zeros_like(ATA)\n    S[np.diag_indices_from(S)] = sing_vals\n    j = 0\n    V = eig_vecs.copy()\n    U = np.zeros((A.shape[0], A.shape[0]))\n    for s in sing_vals:\n        if s != 0:\n            v = V[:, j]\n            u = 1 / s * np.matmul(A, v)\n            U[:, j] = u\n            j += 1\n    U = np.around(U, decimals=4)\n    S = np.around(S, decimals=4)\n    V = np.around(V, decimals=4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 28, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2)\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.diag(np.sqrt(eigenvalues))\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for (i, sigma) in enumerate(S.diagonal()):\n        if sigma != 0:\n            u = np.dot(A, V[:, i])\n            U[:, i] = u / sigma\n    S = np.around(S, 4)\n    U = np.around(U, 4)\n    V = np.around(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = np.sqrt(eigenvalues[idx])\n    eigenvectors = eigenvectors[:, idx]\n    S = np.diag(eigenvalues)\n    V = eigenvectors\n    U = np.zeros_like(A, dtype=np.float64)\n    for i in range(min(A.shape)):\n        if eigenvalues[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / eigenvalues[i]\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 28, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A, dtype=np.float64)\n    for i in range(V.shape[1]):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    return (U.round(4).tolist(), S.round(4).tolist(), V.round(4).tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 4, "solution": "import numpy as np\nfrom numpy.linalg import eigh\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigvals, eigvecs) = eigh(ATA)\n    idx = eigvals.argsort()[::-1]\n    eigvals = eigvals[idx]\n    eigvecs = eigvecs[:, idx]\n    S = np.diag(np.sqrt(eigvals))\n    V = eigvecs\n    U = np.zeros_like(A)\n    for i in range(2):\n        if eigvals[i] != 0:\n            U[:, i] = np.dot(A, eigvecs[:, i]) / np.sqrt(eigvals[i])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U = U.tolist()\n    S = S.tolist()\n    V = V.tolist()\n    return (U, S, V)"}
{"task_id": 28, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Matrix must be 2x2')\n    ATA = np.matmul(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = np.sqrt(np.real(eigenvalues[idx]))\n    eigenvectors = eigenvectors[:, idx]\n    V = eigenvectors\n    S = np.diag(eigenvalues)\n    U = np.zeros_like(A, dtype=np.float64)\n    for i in range(2):\n        U[:, i] = np.ravel(np.matmul(A, V[:, i]) / S[i, i])\n    if np.isnan(U).any() or np.isnan(S).any() or np.isnan(V).any():\n        raise ValueError('SVD for the given 2x2 matrix is undefined')\n    return (U.round(4).tolist(), S.round(4).tolist(), V.round(4).tolist())"}
{"task_id": 28, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2), 'Input matrix must be a 2x2 numpy array'\n    A_transpose_A = np.dot(A.T, A)\n    (eig_values, eig_vectors) = np.linalg.eig(A_transpose_A)\n    idx = eig_values.argsort()[::-1]\n    eig_values = eig_values[idx]\n    eig_vectors = eig_vectors[:, idx]\n    sigma = np.sqrt(eig_values)\n    V = eig_vectors\n    U = np.zeros_like(A, dtype=np.float64)\n    for i in range(2):\n        if sigma[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / sigma[i]\n    U = np.round(U, 4)\n    sigma = np.round(sigma, 4)\n    V = np.round(V, 4)\n    U = U.tolist()\n    sigma = sigma.tolist()\n    V = V.tolist()\n    return (U, sigma, V)"}
{"task_id": 28, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.matmul(A.T, A)\n    (eig_values, eig_vectors) = np.linalg.eigh(ATA)\n    idx = eig_values.argsort()[::-1]\n    eig_values = eig_values[idx]\n    eig_vectors = eig_vectors[:, idx]\n    S = np.zeros_like(ATA)\n    for i in range(min(A.shape)):\n        S[i, i] = np.sqrt(eig_values[i])\n    V = eig_vectors\n    U = np.zeros_like(A)\n    for i in range(min(A.shape)):\n        u = np.matmul(A, eig_vectors[:, i].reshape(-1, 1))\n        u /= S[i, i]\n        U[:, i] = u.flatten()\n    return (U.round(4).tolist(), S.round(4).tolist(), V.round(4).tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    (eig_val, V) = np.linalg.eig(np.matmul(A.T, A))\n    U = []\n    S = [0, 0]\n    singular_values = np.sqrt(eig_val)\n    singular_value_indices = np.argsort(-singular_values)\n    for i in singular_value_indices:\n        sigma = singular_values[i]\n        if sigma != 0:\n            v = V[:, i].reshape(2, 1)\n            u = A @ v / sigma\n            U.append(u.tolist())\n        else:\n            U.append([np.nan, np.nan])\n        S[i] = sigma\n    U = np.array(U)\n    S_diag = np.diag(S)\n    V = np.array(V)\n    return (U.round(4).tolist(), S_diag.round(4).tolist(), V.round(4).tolist())"}
{"task_id": 28, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A = A.astype(float)\n    ATA = np.dot(A.T, A)\n    (eig_vals, eig_vecs) = np.linalg.eig(ATA)\n    sort_indices = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[sort_indices]\n    eig_vecs = eig_vecs[:, sort_indices]\n    S = np.sqrt(np.maximum(eig_vals, 0))\n    V = eig_vecs\n    U = np.array([np.dot(A, V[:, i]) / S[i] for i in range(2)]).T\n    U = U / np.linalg.norm(U, axis=0)\n    U = U.round(4).tolist()\n    S = np.diag(S.round(4)).tolist()\n    V = V.round(4).tolist()\n    return (U, S, V)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X = X[indices].tolist()\n    y = y[indices].tolist()\n    return (X, y)"}
{"task_id": 29, "completion_id": 1, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 2, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    X = np.array(X)\n    y = np.array(y)\n    np.random.seed(seed)\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 3, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices].tolist()\n    y_shuffled = y[indices].tolist()\n    return (X_shuffled, y_shuffled)"}
{"task_id": 29, "completion_id": 4, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    data = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n    np.random.shuffle(data)\n    X = data[:, :-1]\n    y = data[:, -1]\n    return (X.tolist(), y.tolist())"}
{"task_id": 29, "completion_id": 5, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    dataset = np.column_stack((X, y))\n    np.random.seed(seed)\n    np.random.shuffle(dataset)\n    X_shuffled = dataset[:, :-1].tolist()\n    y_shuffled = dataset[:, -1].tolist()\n    return (X_shuffled, y_shuffled)"}
{"task_id": 29, "completion_id": 6, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    This function will shuffle the samples in two numpy arrays while maintaining the corresponding order between them.\n    Parameters:\n    X (numpy array): The first numpy array.\n    y (numpy array): The second numpy array.\n    seed (int, optional): The seed to use for the random number generator. Defaults to None.\n    Returns:\n    shuffled_X (numpy array): The shuffled X array.\n    shuffled_y (numpy array): The shuffled y array.\n    \"\"\"\n    np.random.seed(seed)\n    assert len(X) == len(y)\n    p = np.random.permutation(len(X))\n    shuffled_X = X[p]\n    shuffled_y = y[p]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nX = np.array([1, 2, 3, 4, 5])\ny = np.array(['a', 'b', 'c', 'd', 'e'])"}
{"task_id": 29, "completion_id": 7, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    shuffle_indices = np.random.permutation(len(X))\n    return (X[shuffle_indices].tolist(), y[shuffle_indices].tolist())\nX = np.array([1, 2, 3, 4, 5])"}
{"task_id": 29, "completion_id": 8, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    np.random.seed(seed)\n    permutation = np.random.permutation(len(X))\n    X_shuffled = X[permutation].tolist()\n    y_shuffled = y[permutation].tolist()\n    return (X_shuffled, y_shuffled)"}
{"task_id": 29, "completion_id": 9, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    permutation = np.random.permutation(len(X))\n    X_shuffled = X[permutation].tolist()\n    y_shuffled = y[permutation].tolist()\n    return (X_shuffled, y_shuffled)"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    assert X.shape[0] == (y.shape[0] if y is not None else batch_size), 'X and y should have same number of samples'\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for i in range(0, X.shape[0], batch_size):\n        batch_indices = indices[i:i + batch_size]\n        if y is not None:\n            yield (X[batch_indices].tolist(), y[batch_indices].tolist())\n        else:\n            yield X[batch_indices].tolist()"}
{"task_id": 30, "completion_id": 1, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    num_full_batches = num_samples // batch_size\n    start_index = num_full_batches * batch_size\n    remaining_samples = num_samples - start_index\n    for i in range(num_full_batches):\n        start = i * batch_size\n        end = (i + 1) * batch_size\n        if y is not None:\n            yield (X[start:end].tolist(), y[start:end].tolist())\n        else:\n            yield X[start:end].tolist()\n    if remaining_samples > 0:\n        if y is not None:\n            yield (X[start_index:].tolist(), y[start_index:].tolist())\n        else:\n            yield X[start_index:].tolist()"}
{"task_id": 30, "completion_id": 2, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for i in range(0, num_samples, batch_size):\n        x_batch = X[i:min(i + batch_size, num_samples)]\n        if y is not None:\n            y_batch = y[i:min(i + batch_size, num_samples)]\n            yield (x_batch.tolist(), y_batch.tolist())\n        else:\n            yield x_batch.tolist()"}
{"task_id": 30, "completion_id": 3, "solution": "import numpy as np\nimport random\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = len(X)\n    batch_size = min(batch_size, num_samples)\n    indices = list(range(num_samples))\n    random.shuffle(indices)\n    for i in range(0, num_samples, batch_size):\n        batch_size_i = min(batch_size, num_samples - i)\n        indices_batch = indices[i:i + batch_size_i]\n        if y is not None:\n            yield (X[indices_batch].tolist(), y[indices_batch].tolist())\n        else:\n            yield X[indices_batch].tolist()"}
{"task_id": 30, "completion_id": 4, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    X = np.array(X)\n    y = np.array(y) if y is not None else None\n    num_samples = X.shape[0]\n    num_batches = int(np.ceil(num_samples / batch_size))\n    batches = []\n    if y is not None:\n        for i in range(num_batches):\n            start = i * batch_size\n            end = min((i + 1) * batch_size, num_samples)\n            batches.append((X[start:end].tolist(), y[start:end].tolist()))\n    else:\n        for i in range(num_batches):\n            start = i * batch_size\n            end = min((i + 1) * batch_size, num_samples)\n            batches.append(X[start:end].tolist())\n    return batches\nnum_samples = 1000\nfeatures = 100\nX = np.random.rand(num_samples, features)\ny = np.random.rand(num_samples)\nbatch_size = 64\nbatches = batch_iterator(X, y, batch_size)"}
{"task_id": 30, "completion_id": 5, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batch iterable for a dataset X and an optional dataset y.\n\n    Parameters\n    ----------\n    X : numpy array\n        An array of samples.\n    y : numpy array\n        An array of corresponding labels. Optional (default is None).\n    batch_size : int\n        The batch size for each yielded batch.\n\n    Yields\n    ------\n    numpy arrays\n        If y is None, yields batches of X only. If y is given, yields batches with both X and y in tuples.\n    \"\"\"\n    num_samples = X.shape[0]\n    num_full_batches = num_samples // batch_size\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for batch_idx in range(num_full_batches):\n        batch_indices = indices[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n        batch_X = X[batch_indices]\n        if y is None:\n            yield batch_X.tolist()\n        else:\n            batch_y = y[batch_indices]\n            yield (batch_X.tolist(), batch_y.tolist())"}
{"task_id": 30, "completion_id": 6, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    if y is None:\n        np.random.shuffle(X)\n        for i in range(0, X.shape[0], batch_size):\n            yield X[i:i + batch_size].tolist()\n    else:\n        assert X.shape[0] == y.shape[0], 'X and y must have same number of instances'\n        indices = np.arange(X.shape[0])\n        np.random.shuffle(indices)\n        for i in range(0, X.shape[0], batch_size):\n            batch_indices = indices[i:i + batch_size]\n            yield (X[batch_indices].tolist(), y[batch_indices].tolist())"}
{"task_id": 30, "completion_id": 7, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for i in range(0, num_samples, batch_size):\n        (start, end) = (i, min(i + batch_size, num_samples))\n        if y is not None:\n            yield (X[start:end].tolist(), y[start:end].tolist())\n        else:\n            yield X[start:end].tolist()\ny = np.random.randint(0, 2, 100)"}
{"task_id": 30, "completion_id": 8, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    if y is None:\n        num_samples = X.shape[0]\n        num_batches = int(np.ceil(num_samples / batch_size))\n        for i in range(num_batches):\n            start_index = i * batch_size\n            end_index = min((i + 1) * batch_size, num_samples)\n            batch_X = X[start_index:end_index].tolist()\n            yield batch_X\n    else:\n        num_samples = X.shape[0]\n        num_batches = int(np.ceil(num_samples / batch_size))\n        assert num_samples == y.shape[0], 'The number of samples in X and y is not equal'\n        for i in range(num_batches):\n            start_index = i * batch_size\n            end_index = min((i + 1) * batch_size, num_samples)\n            batch_X = X[start_index:end_index].tolist()\n            batch_y = y[start_index:end_index].tolist()\n            yield (batch_X, batch_y)"}
{"task_id": 30, "completion_id": 9, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_samples = X.shape[0]\n    for i in np.arange(0, num_samples, batch_size):\n        end_i = min(i + batch_size, num_samples)\n        if y is not None:\n            yield (X[i:end_i].tolist(), y[i:end_i].tolist())\n        else:\n            yield X[i:end_i].tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold. \n\n    Parameters:\n    X (np.array): 2D array in which each row represents an example and each column represents a feature.\n    feature_i (int): The index of the feature to be used for the division.\n    threshold (float): The threshold value to consider when splitting the dataset. \n\n    Returns:\n    (list, list, list):\n    A list of samples where the feature value is greater than or equal to the threshold,\n    A list of samples where the feature value is less than the threshold,\n    A list containing the original dataset.\n    \"\"\"\n    result = [[], []]\n    for i in range(X.shape[0]):\n        if X[i, feature_i] >= threshold:\n            result[0].append(X[i].tolist())\n        else:\n            result[1].append(X[i].tolist())\n    result.append(X.tolist())\n    return result\nX = np.array([[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [7.0, 3.2, 4.7, 1.4], [6.4, 3.2, 4.5, 1.5]])"}
{"task_id": 31, "completion_id": 1, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    subset_true = X[np.where(X[:, feature_i] >= threshold)].tolist()\n    subset_false = X[np.where(X[:, feature_i] < threshold)].tolist()\n    return (subset_true, subset_false)"}
{"task_id": 31, "completion_id": 2, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"Divide dataset X based on if an example's feature indexed by feature_i being larger than or equal to threshold.\"\"\"\n    if feature_i >= X.shape[1]:\n        raise ValueError(f'The feature index {feature_i} is larger than the # of features in the dataset.')\n    X_greater = X[X[:, feature_i] >= threshold].tolist()\n    X_less_equal = X[X[:, feature_i] < threshold].tolist()\n    return (X_greater, X_less_equal)"}
{"task_id": 31, "completion_id": 3, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset X based on whether the value of a specified feature \n    is greater than or equal to a given threshold.\n\n    Parameters:\n    X (list of lists): The dataset to divide, with each inner list representing a sample \n                        and each element in the inner lists representing a feature value.\n    feature_i (int): The index of the feature to use for division (0-indexed).\n    threshold (float): The value to use for comparison.\n\n    Returns:\n    A tuple containing two subsets of the dataset, divided based on the specified condition. \n    Each subset is a list of lists, where each inner list represents a sample.\n    \"\"\"\n    np_X = np.array(X)\n    subset1 = np_X[np_X[:, feature_i] >= threshold].tolist()\n    subset2 = np_X[np_X[:, feature_i] < threshold].tolist()\n    return (subset1, subset2)\nX = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]"}
{"task_id": 31, "completion_id": 4, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset based on a feature's value compared to a threshold.\n    \n    Parameters:\n    X (list of lists): A 2D list representing the dataset where each inner list represents a sample.\n    feature_i (int): The index of the feature to be compared to the threshold.\n    threshold (float): The threshold value to compare with the feature.\n    \n    Returns:\n    list, list: Two lists of samples, one with samples where the feature's value is greater than or equal to the threshold, and the other with samples where the value is less than the threshold.\n    \"\"\"\n    X_greater_equal = []\n    X_less = []\n    for sample in X:\n        if sample[feature_i] >= threshold:\n            X_greater_equal.append(sample)\n        else:\n            X_less.append(sample)\n    return (np.array(X_greater_equal).tolist(), np.array(X_less).tolist())\nX = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]"}
{"task_id": 31, "completion_id": 5, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    X_np = np.array(X)\n    subset_1 = X_np[X_np[:, feature_i] >= threshold]\n    subset_2 = X_np[X_np[:, feature_i] < threshold]\n    subset_1 = subset_1.tolist()\n    subset_2 = subset_2.tolist()\n    return (subset_1, subset_2)\nX = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nfeature_i = 1\nthreshold = 5"}
{"task_id": 31, "completion_id": 6, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    X = np.transpose(X)\n    if feature_i >= len(X[0]):\n        raise IndexError('Feature index out of range.')\n    X_new1 = []\n    X_new2 = []\n    for sample in X:\n        if sample[feature_i] >= threshold:\n            X_new1.append(sample)\n        else:\n            X_new2.append(sample)\n    X_new1 = np.transpose(np.array(X_new1))\n    X_new2 = np.transpose(np.array(X_new2))\n    return (X_new1.tolist(), X_new2.tolist())"}
{"task_id": 31, "completion_id": 7, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    meets_condition = X[X[:, feature_i] >= threshold]\n    not_meets_condition = X[X[:, feature_i] < threshold]\n    return (meets_condition.tolist(), not_meets_condition.tolist())"}
{"task_id": 31, "completion_id": 8, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    X_T = X.T\n    mask = X_T[feature_i] >= threshold\n    X_meets_condition = X_T[mask].T\n    X_doesnt_meet_condition = X_T[~mask].T\n    return (X_meets_condition.tolist(), X_doesnt_meet_condition.tolist())"}
{"task_id": 31, "completion_id": 9, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    meets_condition = X[X[:, feature_i] >= threshold]\n    does_not_meet_condition = X[X[:, feature_i] < threshold]\n    return (meets_condition.tolist(), does_not_meet_condition.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    combinations = index_combinations()\n    new_X = np.empty((n_samples, len(combinations)))\n    for (i, index_combs) in enumerate(combinations):\n        new_X[:, i] = np.prod(X[:, index_combs], axis=1)\n    return new_X.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])"}
{"task_id": 32, "completion_id": 1, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [combi for sublist in combs for combi in sublist]\n        return flat_combs\n    new_X = np.empty((n_samples, len(index_combinations())))\n    for (i, index_combs) in enumerate(index_combinations()):\n        new_X[:, i] = np.prod(X[:, index_combs], axis=1)\n    return new_X.tolist()"}
{"task_id": 32, "completion_id": 2, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    new_features = [np.prod(X[:, list(combs)], axis=2, keepdims=False) for combs in index_combinations()]\n    return np.transpose(np.array(new_features)).tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2"}
{"task_id": 32, "completion_id": 3, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    new_features = index_combinations()\n    n_output_features = len(new_features)\n    X_new = np.empty((n_samples, n_output_features))\n    for (i, index_combs) in enumerate(new_features):\n        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n    return X_new.tolist()\ndegree = 2"}
{"task_id": 32, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    if X.ndim == 1:\n        X = X[:, None]\n    poly_features = [np.ones(X.shape[0])]\n    for d in range(1, degree + 1):\n        for (idx, x) in enumerate(X.T):\n            poly_features.extend([np.abs(x) ** d if i == 0 else x ** d for i in range(d + 1)])\n    return np.asarray(poly_features).T.tolist()"}
{"task_id": 32, "completion_id": 5, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [combi for sublist in combs for combi in sublist]\n        return flat_combs\n    new_features = [np.prod(X[:, combs], axis=1) for combs in index_combinations()]\n    return np.array(new_features).T.tolist()\ndegree = 2"}
{"task_id": 32, "completion_id": 6, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    new_features = []\n    combinations = index_combinations()\n    for items in combinations:\n        new_features.append(np.prod(X[:, items], axis=1))\n    return np.array(new_features).T.tolist()"}
{"task_id": 32, "completion_id": 7, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nimport numpy as np\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations(features, degree):\n        start_idx = np.zeros((degree,)).astype(int)\n        end_idx = np.array([degree] * degree).astype(int)\n        return combinations_with_replacement(range(n_features), degree)\n    combinations = list(index_combinations(n_features, degree))\n    n_output_features = len(combinations)\n    PF = np.empty((n_samples, n_output_features))\n    for (i, index_combs) in enumerate(combinations):\n        PF[:, i] = np.prod(X[:, index_combs], axis=1)\n    return PF.tolist()\nX = np.random.rand(5, 2)\ndegree = 3\nPF = polynomial_features(X, degree)"}
{"task_id": 32, "completion_id": 8, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    new_X = np.empty((n_samples, sum((1 for _ in combinations_with_replacement(range(n_features), degree)))))\n    for (i, index_combs) in enumerate(index_combinations()):\n        new_X[:, i] = np.prod(np.prod((X[:, comb] for comb in index_combs)), axis=1)\n    return new_X.tolist()"}
{"task_id": 32, "completion_id": 9, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\npolynomial_features = polynomial_features(X, degree)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    random_subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(len(X), size=len(X), replace=replacements)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        random_subsets.append((X_subset, y_subset))\n    return random_subsets"}
{"task_id": 33, "completion_id": 1, "solution": "import numpy as np\nfrom sklearn.utils import resample\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    for i in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(len(X), len(X))\n        else:\n            indices = np.random.choice(len(X), len(X), replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 2, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    dataset = np.hstack((X, np.expand_dims(y, axis=1)))\n    n_rows = dataset.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_rows, size=n_rows, replace=replacements)\n        subsets.append(dataset[indices, :])\n    return [[np.delete(subset, obj=-1, axis=1).tolist(), np.take(subset, indices=-1, axis=1).tolist()] for subset in subsets]"}
{"task_id": 33, "completion_id": 3, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 4, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=True)\n        else:\n            indices = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n        subsets.append((X[indices, :].tolist(), y[indices].tolist()))\n    return subsets\nX = np.random.rand(100, 5)\ny = np.random.randint(0, 2, 100)\nsubsets = get_random_subsets(X, y, 10)"}
{"task_id": 33, "completion_id": 5, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    row_indices = np.arange(X.shape[0])\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            sample_indices = np.random.choice(row_indices, size=X.shape[0])\n        else:\n            sample_indices = np.random.choice(row_indices, size=X.shape[0], replace=False)\n        X_subset = X[sample_indices]\n        y_subset = y[sample_indices]\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 33, "completion_id": 6, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([0, 1, 0, 1, 0])\nsubsets = get_random_subsets(X, y, 3, replacements=False)"}
{"task_id": 33, "completion_id": 7, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    if replacements:\n        subsets = []\n        for _ in range(n_subsets):\n            indices = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=True)\n            X_subset = X[indices]\n            y_subset = y[indices]\n            subsets.append((X_subset.tolist(), y_subset.tolist()))\n    else:\n        indices = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n        subsets = []\n        for _ in range(n_subsets):\n            np.random.shuffle(indices)\n            X_subset = X[indices]\n            y_subset = y[indices]\n            subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 8, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    if replacements:\n        for _ in range(n_subsets):\n            indexes = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=True)\n            subsets.append((X[indexes].tolist(), y[indexes].tolist()))\n    else:\n        indexes = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n        for i in range(n_subsets):\n            np.random.shuffle(indexes)\n            subsets.append((X[indexes].tolist(), y[indexes].tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 9, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_rows = X.shape[0]\n    row_indices = np.arange(n_rows)\n    for _ in range(n_subsets):\n        if replacements:\n            selected_row_indices = np.random.choice(row_indices, size=n_rows)\n        else:\n            if n_rows < n_subsets:\n                raise ValueError('Number of rows less than number of subsets')\n            selected_row_indices = np.random.choice(row_indices, size=n_rows, replace=False)\n        X_subset = X[selected_row_indices]\n        y_subset = y[selected_row_indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.amax(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 1, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 2, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 3, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 4, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 5, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"One-hot encoding of nominal values.\n\n    Parameters\n    ----------\n    x : numpy array\n        1D array of integer values.\n    n_col : int, optional\n        The number of columns for the one-hot encoded array. If not\n        provided, it is automatically determined from the input array.\n\n    Returns\n    -------\n    numpy array\n        The one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.amax(x) + 1\n    x_one_hot = np.zeros((x.shape[0], n_col))\n    x_one_hot[np.arange(x.shape[0]), x] = 1\n    return x_one_hot.tolist()\nx = np.array([0, 1, 2, 1])"}
{"task_id": 34, "completion_id": 6, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()\nx = np.array([0, 2, 1, 3, 1, 2])"}
{"task_id": 34, "completion_id": 7, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.amax(x) + 1\n    onehot = np.zeros((x.size, n_col))\n    onehot[np.arange(x.size), x] = 1\n    return onehot.tolist()"}
{"task_id": 34, "completion_id": 8, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.shape[0], n_col))\n    one_hot_encoded[np.arange(x.shape[0]), x] = 1\n    return one_hot_encoded.tolist()"}
{"task_id": 34, "completion_id": 9, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a numpy array to its categorical version\n\n    Args:\n    x: numpy array, input array to be converted into categorical.\n    n_col: An optional integer representing the number of columns for the one-hot encoded array. If not provided, it is automatically determined from the input array.\n    \n    Returns:\n    Python list of categorical version of the input array, by using numpy's tolist() method.\n    \"\"\"\n    if not n_col:\n        n_col = np.max(x) + 1\n    out = np.zeros((x.shape[0], n_col))\n    out[np.arange(x.shape[0]), x] = 1\n    return out.tolist()\nx = np.array([0, 1, 2, 1, 0])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diagflat(x).tolist()"}
{"task_id": 35, "completion_id": 1, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diagflat(x).tolist()"}
{"task_id": 35, "completion_id": 2, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diag(x).tolist()"}
{"task_id": 35, "completion_id": 3, "solution": "import numpy as np\ndef make_diagonal(x):\n    x = np.array(x)\n    diagonal_matrix = np.diagflat(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 4, "solution": "import numpy as np\nimport numpy as np\ndef make_diagonal(x):\n    return np.diagflat(x).tolist()"}
{"task_id": 35, "completion_id": 5, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diagflat(x).tolist()"}
{"task_id": 35, "completion_id": 6, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diagflat(x).tolist()"}
{"task_id": 35, "completion_id": 7, "solution": "import numpy as np\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 35, "completion_id": 8, "solution": "import numpy as np\ndef make_diagonal(x):\n    return np.diag(x).tolist()"}
{"task_id": 35, "completion_id": 9, "solution": "import numpy as np\ndef make_diagonal(x):\n    if x.ndim != 1:\n        raise ValueError('Input should be a 1D numpy array.')\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3, 4, 5])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred should have the same length')\n    true_positive = np.sum(y_true == y_pred)\n    accuracy = true_positive / len(y_true)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny_pred = np.array([0, 2, 1, 3, 4, 5, 7, 6, 8, 9])"}
{"task_id": 36, "completion_id": 1, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('The two arrays must have the same length')\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 2, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct = 0\n    total = len(y_true)\n    for i in range(0, total):\n        if y_true[i] == y_pred[i]:\n            correct += 1\n    accuracy = round(correct / total, 4)\n    return accuracy\ny_true = np.array([0, 1, 0, 1, 0, 1, 1, 1, 1, 1])\ny_pred = np.array([0, 1, 1, 1, 0, 1, 0, 0, 1, 0])"}
{"task_id": 36, "completion_id": 3, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)\n    assert y_true.ndim == y_pred.ndim == 1\n    assert len(y_true) == len(y_pred)\n    correct_preds = np.sum(y_true == y_pred)\n    total_preds = len(y_true)\n    accuracy = correct_preds / total_preds\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 4, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true : numpy.ndarray\n        The true labels.\n    y_pred : numpy.ndarray\n        The predicted labels.\n\n    Returns:\n    float\n        The accuracy score.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    accuracy = np.around(accuracy, decimals=4)\n    return accuracy"}
{"task_id": 36, "completion_id": 5, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('The input arrays must have the same length')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = round(correct / total, 4)\n    return accuracy"}
{"task_id": 36, "completion_id": 6, "solution": "import numpy as np\nimport numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = round(correct_predictions / total_predictions, 4)\n    return accuracy\ny_true = np.array([0, 1, 2, 3, 4, 5])\ny_pred = np.array([0, 1, 2, 0, 4, 5])"}
{"task_id": 36, "completion_id": 7, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct = y_true == y_pred\n    accuracy = np.mean(correct)\n    return round(accuracy, 4)\ny_true = np.array([1, 2, 1, 2, 1, 2])\ny_pred = np.array([2, 1, 2, 1, 2, 1])"}
{"task_id": 36, "completion_id": 8, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 9, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Both inputs must be numpy arrays.')\n    if y_true.ndim != 1 or y_pred.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays.')\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    cov_matrix = np.cov(X, Y)\n    var_X = np.var(X)\n    var_Y = np.var(Y)\n    sd_X = np.std(X)\n    sd_Y = np.std(Y)\n    corr_matrix = cov_matrix / (sd_X * sd_Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()"}
{"task_id": 37, "completion_id": 1, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X_mean = np.mean(X, axis=0)\n    Y_mean = np.mean(Y, axis=0)\n    X_var = np.var(X, axis=0)\n    Y_var = np.var(Y, axis=0)\n    covariance = np.mean((X - X_mean).T.dot(Y - Y_mean), axis=1)\n    correlation = covariance / np.sqrt(X_var * Y_var)\n    return np.around(correlation, decimals=4).tolist()"}
{"task_id": 37, "completion_id": 2, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        return np.corrcoef(X, rowvar=False).round(4).tolist()\n    else:\n        return np.corrcoef(X, Y, rowvar=False).round(4).tolist()"}
{"task_id": 37, "completion_id": 3, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X = X.astype(float)\n    Y = Y.astype(float)\n    X -= np.mean(X, axis=0)\n    Y -= np.mean(Y, axis=0)\n    cov_matrix = np.dot(X.T, Y) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    correlation_matrix = cov_matrix / np.outer(std_X, std_Y)\n    return correlation_matrix.round(4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 37, "completion_id": 4, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    n = X.shape[0]\n    cov_matrix = 1 / (n - 1) * np.dot((X - np.mean(X)).T, Y - np.mean(Y))\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    corr_matrix = cov_matrix / np.outer(std_X, std_Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()"}
{"task_id": 37, "completion_id": 5, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is not None:\n        X = np.concatenate((X, Y), axis=1)\n    n_vars = X.shape[1]\n    corr_matrix = np.zeros((n_vars, n_vars))\n    for i in range(n_vars):\n        for j in range(n_vars):\n            corr_matrix[i, j] = np.corrcoef(X[:, i], X[:, j])[0, 1]\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 6, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X = X - X.mean(axis=0)\n    Y = Y - Y.mean(axis=0)\n    covariance = np.dot(X.T, Y) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    correlation_matrix = covariance / np.outer(std_X, std_Y)\n    return correlation_matrix.round(4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 7, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X_mean = np.mean(X, axis=0)\n    Y_mean = np.mean(Y, axis=0)\n    X_std = np.std(X, axis=0)\n    Y_std = np.std(Y, axis=0)\n    X_zero_mean = X - X_mean\n    Y_zero_mean = Y - Y_mean\n    correlation_matrix = np.matmul(X_zero_mean.T, Y_zero_mean)\n    correlation_matrix /= len(X)\n    correlation_matrix /= X_std.reshape(-1, 1)\n    correlation_matrix /= Y_std\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()"}
{"task_id": 37, "completion_id": 8, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        return np.corrcoef(X, rowvar=False).round(4).tolist()\n    else:\n        return np.corrcoef(X, Y, rowvar=False).round(4).tolist()"}
{"task_id": 37, "completion_id": 9, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    corr_matrix = np.corrcoef(X, Y)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nclass DecisionStump:\n\n    def __init__(self):\n        self.feature = None\n        self.threshold = None\n        self.alpha = None\n\n    def fit(self, X, y, sample_weight):\n        (n_samples, n_features) = X.shape\n        min_error = float('inf')\n        for feature in range(n_features):\n            for threshold in np.unique(X[:, feature]):\n                predictions = np.ones(y.shape)\n                predictions[X[:, feature] < threshold] = -1\n                error = np.sum(sample_weight[predictions != y])\n                if error > 0.5:\n                    error = 1 - error\n                    predictions *= -1\n                if error < min_error:\n                    min_error = error\n                    self.feature = feature\n                    self.threshold = threshold\n                    self.alpha = 0.5 * np.log((1 - min_error) / (min_error + 1e-10))\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        predictions = np.ones(n_samples)\n        predictions[X[:, self.feature] < self.threshold] = -1\n        return predictions\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, _) = X.shape\n    sample_weight = np.full(n_samples, 1.0 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        classifier = DecisionStump()\n        classifier.fit(X, y, sample_weight)\n        classifiers.append(classifier)\n        predictions = classifier.predict(X)\n        sample_weight *= np.exp(-classifier.alpha * y * predictions)\n        sample_weight /= np.sum(sample_weight)\n    return classifiers"}
{"task_id": 38, "completion_id": 1, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weight = np.full((n_samples,), 1 / n_samples)\n    clfs = []\n    for _ in range(n_clf):\n        errors = np.zeros((n_features,))\n        thresholds = np.zeros((n_features,))\n        preds = np.zeros((n_features,))\n        for feature_idx in range(n_features):\n            feature = X[:, feature_idx]\n            min_error = float('inf')\n            for threshold in np.unique(feature):\n                pred = np.ones(np.shape(y))\n                pred[X[:, feature_idx] <= threshold] = -1\n                error = np.sum(sample_weight[pred != y])\n                if error > 0.5:\n                    error = 1 - error\n                    pred = -pred\n                if error < min_error:\n                    min_error = error\n                    thresholds[feature_idx] = threshold\n                    errors[feature_idx] = error\n                    preds[feature_idx] = pred\n        EPS = 1e-10\n        clf_weight = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n        sample_weight *= np.exp(-clf_weight * y * preds)\n        sample_weight /= np.sum(sample_weight)\n        import collections\n        clf = collections.namedtuple('classifier', ['feature_index', 'threshold', 'weight'])\n        clfs.append(clf(np.argmin(errors), thresholds[np.argmin(errors)], clf_weight))\n    return clfs"}
{"task_id": 38, "completion_id": 2, "solution": "import numpy as np\nimport math\nclassifier_list = []\ndef find_thresholds(X, y, weights):\n    thresholds = [-math.inf]\n    min_errors = [float('inf')]\n    sort_indices = np.argsort(X)\n    X_sorted = X[sort_indices]\n    y_sorted = y[sort_indices]\n    weights_sorted = weights[sort_indices]\n    midpoints = [(X_sorted[i] + X_sorted[i + 1]) / 2.0 for i in range(len(X_sorted) - 1)]\n    midpoint_weights = (weights_sorted[:-1] + weights_sorted[1:]) / 2.0\n    sort_order = np.argsort(midpoint_weights)\n    midpoints = midpoints[sort_order]\n    midpoint_weights = midpoint_weights[sort_order]\n    for i in range(len(midpoints)):\n        left_errors = np.sum(weights_sorted[:i + 1][y_sorted[:i + 1] == 1])\n        right_errors = np.sum(weights_sorted[i + 1:][y_sorted[i + 1:] == -1])\n        error = (left_errors + right_errors) / np.sum(weights_sorted)\n        if error < min_errors[-1]:\n            thresholds.append(midpoints[i])\n            min_errors.append(error)\n    return (thresholds, min_errors[1:])\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    for _ in range(n_clf):\n        best_feature = None\n        best_threshold = None\n        best_error = float('inf')\n        best_polarity = 1\n        for feature in range(n_features):\n            (thresholds, errors) = find_thresholds(X[:, feature], y, weights)\n            for i in range(len(thresholds)):\n                if errors[i] < best_error:\n                    best_feature = feature\n                    best_threshold = thresholds[i]\n                    best_error = errors[i]\n                    best_polarity = 1 if np.sum(weights[X[:, feature] < best_threshold] * y[X[:, feature] < best_threshold]) > 0 else -1\n        classifier_list.append((best_feature, best_threshold, best_polarity))\n        error = 1 - np.sum(weights[X[:, best_feature] * best_polarity < best_threshold * best_polarity] == y[X[:, best_feature] * best_polarity < best_threshold * best_polarity]) / np.sum(weights)\n        alpha = math.log((1 - error) / max(error, 1e-16))\n        weights = weights * np.exp(-alpha * y * np.sign(X[:, best_feature] - best_threshold))\n        weights = weights / np.sum(weights)\n    return classifier_list"}
{"task_id": 38, "completion_id": 3, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n\n    def calc_error(feature_index, t, weight, pred):\n        pred[np.where(X[:, feature_index] > t)] = -1\n        pred[np.where(X[:, feature_index] <= t)] = 1\n        error = np.sum(weight[y != pred])\n        return (error, pred)\n    (n_samples, n_features) = X.shape\n    weight = np.ones(n_samples) / n_samples\n    clfs = []\n    for _ in range(n_clf):\n        min_err = np.inf\n        for feature_index in range(n_features):\n            thresholds = np.unique(X[:, feature_index])\n            for t in thresholds:\n                pred = np.ones(y.shape)\n                (err, pred) = calc_error(feature_index, t, weight, pred)\n                if err < min_err:\n                    min_err = err\n                    clf = {'feature_index': feature_index, 'threshold': t, 'alpha': 0.5 * np.log((1 - min_err) / (min_err + 1e-10)), 'pred': pred.astype(int)}\n        clfs.append(clf)\n        weight = weight * np.exp(-clf['alpha'] * y * clf['pred'])\n        weight /= np.sum(weight)\n    return clfs"}
{"task_id": 38, "completion_id": 4, "solution": "import numpy as np\nimport math\ndef entropy(p):\n    \"\"\" Calculate entropy of a list of probabilities. \"\"\"\n    return np.sum([-p_i * math.log2(p_i) for p_i in p if p_i != 0])\ndef weighted_entropy(y, weights):\n    \"\"\" Calculate the weighted entropy of a list of targets using a list of weights. \"\"\"\n    classes = np.unique(y)\n    n = len(y)\n    entropy_val = 0.0\n    for c in classes:\n        mask = y == c\n        p = np.sum(weights[mask]) / np.sum(weights)\n        entropy_val += p * entropy([np.sum(weights[mask * (y == c_i)]) / np.sum(weights[mask]) for c_i in classes])\n    return entropy_val\ndef find_best_threshold(x, y, weights):\n    \"\"\" Find the best threshold for a given feature. \"\"\"\n    sorted_x = np.sort(np.unique(x))\n    best_threshold = None\n    min_entropy = float('inf')\n    for i in range(1, len(sorted_x)):\n        threshold = (sorted_x[i - 1] + sorted_x[i]) / 2\n        mask_left = x < threshold\n        mask_right = x >= threshold\n        entropy_left = weighted_entropy(y[mask_left], weights[mask_left])\n        entropy_right = weighted_entropy(y[mask_right], weights[mask_right])\n        total_entropy = np.sum(weights[mask_left]) / np.sum(weights) * entropy_left + np.sum(weights[mask_right]) / np.sum(weights) * entropy_right\n        if total_entropy < min_entropy:\n            min_entropy = total_entropy\n            best_threshold = threshold\n    return best_threshold\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    original_weights = 1 / n_samples * np.ones(n_samples)\n    weights = np.ones(n_samples)\n    classifiers = []\n    print(n_clf)\n    for _ in range(n_clf):\n        best_thresholds = []\n        best_entropies = []\n        for i in range(n_features):\n            best_thresholds.append(find_best_threshold(X[:, i], y, weights))\n            best_entropies.append(weighted_entropy(y, weights))\n        best_feature = np.argmin(best_entropies)\n        best_threshold = best_thresholds[best_feature]\n        error = best_entropies[best_feature]\n        classifier_weight = 0.5 * math.log((1 - error) / (error + 1e-09))\n        mask = X[:, best_feature] < best_threshold\n        weights[mask] *= math.exp(classifier_weight)\n        weights[~mask] *= math.exp(-classifier_weight)\n        weights /= np.sum(weights)\n        classifiers.append((best_feature, best_threshold, classifier_weight))\n    return classifiers"}
{"task_id": 38, "completion_id": 5, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import make_classification\ndef adaboost_fit(X, y, n_clf):\n    y = LabelEncoder().fit_transform(y)\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        stump = DecisionTreeClassifier(max_depth=1, random_state=0)\n        stump.fit(X, y, sample_weight=w)\n        y_pred = stump.predict(X)\n        error = sum(w[y != y_pred])\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        classifiers.append((alpha, stump))\n        w *= np.exp(-alpha * y * y_pred)\n        w /= np.sum(w)\n    return classifiers\ndef adaboost_predict(X, classifiers):\n    y_pred = np.array([clf[1].predict(X) for (alpha, clf) in classifiers]).T\n    y_pred = (np.sum(y_pred == -1, axis=1) > 0).astype(int)\n    y_pred[y_pred == 0] = -1\n    return y_pred\nclassifiers = adaboost_fit(X, y, n_clf=100)\ny_pred = adaboost_predict(X, classifiers)"}
{"task_id": 38, "completion_id": 6, "solution": "import numpy as np\nimport math\nclass DecisionStump:\n\n    def __init__(self):\n        self.polarity = 1\n        self.threshold = None\n        self.feature_index = None\n        self.alpha = None\n\n    def predict(self, X):\n        n_samples = np.shape(X)[0]\n        X_column = X[:, self.feature_index]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X_column < self.threshold] = -1\n        else:\n            predictions[X_column > self.threshold] = -1\n        return predictions\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = np.shape(X)\n    w = np.full(n_samples, 1 / n_samples)\n    clfs = []\n    for _ in range(n_clf):\n        clf = DecisionStump()\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            feature_values = np.expand_dims(X[:, feature_i], axis=1)\n            unique_values = np.unique(feature_values)\n            for threshold in unique_values:\n                p = 1\n                prediction = np.ones(np.shape(y))\n                prediction[X[:, feature_i] < threshold] = -1\n                error = np.sum(w[y != prediction])\n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n                if error < min_error:\n                    clf.threshold = threshold\n                    clf.feature_index = feature_i\n                    clf.polarity = p\n                    min_error = error\n        clf.alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = clf.predict(X)\n        w *= np.exp(-clf.alpha * y * predictions)\n        w /= np.sum(w)\n        clfs.append(clf)\n    return clfs"}
{"task_id": 38, "completion_id": 7, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        clf_params = {'weights': np.zeros(n_features), 'threshold': np.zeros(n_features), 'alpha': 0}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            X_column = X[:, feature_i]\n            thresholds = np.unique(X_column)\n            for threshold in thresholds:\n                p = 1\n                predictions = np.ones(np.shape(y))\n                predictions[X_column < threshold] = -1\n                error = np.sum(w[predictions != y])\n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n                if error < min_error:\n                    clf_params['weights'] = np.zeros(n_features)\n                    clf_params['threshold'] = np.zeros(n_features)\n                    clf_params['weights'][feature_i] = np.round(error / (1 - error), 4)\n                    clf_params['threshold'][feature_i] = np.round(threshold, 4)\n                    clf_params['alpha'] = np.round(0.5 * np.log((1.0 - error) / (error + 1e-10)), 4)\n                    min_error = error\n        w *= np.exp(-1 * y * predict([clf_params], X))\n        w /= np.sum(w)\n        classifiers.append(clf_params)\n    return classifiers\ndef predict(classifiers, X):\n    n_samples = X.shape[0]\n    y_pred = np.zeros((n_samples, 1))\n    for clf in classifiers:\n        predictions = np.ones(np.shape(y_pred))\n        predictions[X < clf['threshold']] = -1\n        y_pred += clf['alpha'] * clf['weights'] * predictions\n    y_pred = np.sign(y_pred).flatten()\n    return y_pred"}
{"task_id": 38, "completion_id": 8, "solution": "import numpy as np\nclass DecisionStump:\n\n    def __init__(self):\n        self.feature = None\n        self.threshold = None\n        self.min_error = np.inf\n        self.polarity = None\n\n    def fit(self, X, y, sample_weights):\n        (n_samples, n_features) = X.shape\n        total_error = float('inf')\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                polarity = 1\n                predictions = np.ones(np.shape(y))\n                predictions[X[:, feature] < threshold] = -1\n                error = np.sum(sample_weights[predictions != y])\n                if error > 0.5:\n                    error = 1 - error\n                    polarity = -1\n                if error < total_error:\n                    total_error = error\n                    self.feature = feature\n                    self.threshold = threshold\n                    self.polarity = polarity\n        self.min_error = total_error\nclass AdaBoost:\n\n    def __init__(self):\n        self.clfs = None\n\n    def fit(self, X, y, n_clf):\n        (n_samples, n_features) = X.shape\n        clfs = []\n        sample_weights = np.full(n_samples, 1 / n_samples)\n        for _ in range(n_clf):\n            clf = DecisionStump()\n            clf.fit(X, y, sample_weights)\n            clfs.append(clf)\n            error = np.sum(sample_weights[self.predict(X) != y])\n            alpha = 0.5 * np.log((1 - error) / error)\n            sample_weights *= np.exp(-alpha * y * self.predict(X))\n            sample_weights /= np.sum(sample_weights)\n        self.clfs = clfs\n\n    def predict(self, X):\n        n_samples = np.shape(X)[0]\n        y_pred = np.zeros((n_samples, 1))\n        for clf in self.clfs:\n            predictions = np.ones(np.shape(y_pred))\n            negative_idx = clf.polarity * X[:, clf.feature] < clf.polarity * clf.threshold\n            predictions[negative_idx] = -1\n            y_pred += clf.min_error * predictions\n        y_pred = np.sign(y_pred).flatten()\n        return y_pred\ndef adaboost_fit(X, y, n_clf):\n    clf = AdaBoost()\n    clf.fit(X, y, n_clf)\n    return np.round(clf.clfs, 4)"}
{"task_id": 38, "completion_id": 9, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    clfs = [{} for _ in range(n_clf)]\n    for clf_idx in range(n_clf):\n        min_err = math.inf\n        for feat_idx in range(n_features):\n            X_feat = X[:, feat_idx]\n            thresholds = np.unique(X_feat)\n            thresholds.sort()\n            for threshold in thresholds:\n                pred = np.ones(np.shape(y))\n                pred[X_feat < threshold] = -1\n                err = np.sum(w[pred != y])\n                if err > 0.5:\n                    err = 1 - err\n                    pred *= -1\n                if err < min_err:\n                    clfs[clf_idx]['threshold'] = threshold\n                    clfs[clf_idx]['feature'] = feat_idx\n                    clfs[clf_idx]['alpha'] = 0.5 * np.log((1.0 - err) / err)\n                    min_err = err\n        Z = np.sum([w * np.exp(-clf['alpha'] * y * predict(X, clf)) for clf in clfs])\n        for i in range(n_samples):\n            w[i] = w[i] * np.exp(-clf['alpha'] * y[i] * predict(X[i], clf)) / Z\n    return clfs\ndef predict(X, clf):\n    return 1 if X[clf['feature']] < clf['threshold'] else -1"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    arr = np.array(scores)\n    arr_max = np.max(arr)\n    arr_norm = arr - arr_max\n    numerator = np.exp(arr_norm)\n    denominator = np.sum(numerator)\n    s = numerator / denominator\n    log_s = np.log(s)\n    log_s_rounded = np.round(log_s, 4)\n    return log_s_rounded.tolist()"}
{"task_id": 39, "completion_id": 1, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    ds = scores - max_score\n    ds_exp = np.exp(ds)\n    sum_ds_exp = np.sum(ds_exp)\n    log_sum_ds_exp = np.log(sum_ds_exp)\n    res = ds - log_sum_ds_exp\n    return np.round(res, 4).tolist()"}
{"task_id": 39, "completion_id": 2, "solution": "import numpy as np\ndef log_softmax(scores: list) -> list:\n    scores = np.array(scores)\n    e_scores = np.exp(scores - np.max(scores))\n    probabilities = e_scores / e_scores.sum()\n    log_probabilities = np.log(probabilities)\n    return np.round(log_probabilities, 4).tolist()"}
{"task_id": 39, "completion_id": 3, "solution": "import numpy as np\ndef log_softmax(scores: list) -> list:\n    scores = np.subtract(scores, np.max(scores))\n    exp_score = np.exp(scores)\n    total_sum = np.sum(exp_score)\n    log_softmax = np.log(exp_score / total_sum)\n    log_softmax_rounded = [round(i, 4) for i in log_softmax.tolist()]\n    return log_softmax_rounded\nscores = [1, 2, 3, 4, 1, 2, 3]"}
{"task_id": 39, "completion_id": 4, "solution": "import numpy as np\ndef log_softmax(scores):\n    scores = np.array(scores)\n    scores -= np.max(scores)\n    exp_scores = np.exp(scores)\n    probs = exp_scores / np.sum(exp_scores)\n    log_probs = np.log(probs)\n    log_probs = [round(x, 4) for x in log_probs]\n    return log_probs\nscores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]"}
{"task_id": 39, "completion_id": 5, "solution": "import numpy as np\ndef log_softmax(scores):\n    scores = np.array(scores)\n    scores = scores - np.max(scores)\n    softmax = np.exp(scores) / np.sum(np.exp(scores))\n    log_softmax = np.log(softmax)\n    log_softmax = np.round(log_softmax, decimals=4)\n    log_softmax = log_softmax.tolist()\n    return log_softmax"}
{"task_id": 39, "completion_id": 6, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    scores = scores - np.max(scores)\n    num = np.exp(scores)\n    den = np.sum(num)\n    log_softmax = np.log(num / den)\n    log_softmax = np.round(log_softmax, 4)\n    return log_softmax.tolist()"}
{"task_id": 39, "completion_id": 7, "solution": "import numpy as np\ndef log_softmax(scores):\n    scores = np.array(scores)\n    scores = scores - np.max(scores)\n    scores_exp = np.exp(scores)\n    scores_exp_sum = np.sum(scores_exp)\n    softmax_scores = scores_exp / scores_exp_sum\n    log_softmax_scores = np.log(softmax_scores)\n    log_softmax_scores = np.round(log_softmax_scores, 4)\n    log_softmax_scores = log_softmax_scores.tolist()\n    return log_softmax_scores"}
{"task_id": 39, "completion_id": 8, "solution": "import numpy as np\ndef log_softmax(scores):\n    scores = np.array(scores)\n    exp_scores = np.exp(scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_probs = np.log(exp_scores / sum_exp_scores)\n    return np.round(log_probs, 4).tolist()"}
{"task_id": 39, "completion_id": 9, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    exp_scores = np.exp(scores - np.max(scores))\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return log_softmax_scores.round(4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W0_optimizer = None\n        self.W_optimizer = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W0_optimizer = np.zeros_like(self.w0)\n        self.W_optimizer = np.zeros_like(self.W)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_optimizer.update(self.W, grad_w)\n            self.w0 = self.W0_optimizer.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 1, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W - grad_w\n            self.w0 = self.w0 - grad_w0\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 2, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.initialize(n_units, input_shape)\n\n    def initialize(self, n_units, input_shape=None):\n        limit = 1 / math.sqrt(input_shape[0]) if input_shape else None\n        self.W = np.random.uniform(-limit, limit, (input_shape[0] if input_shape else n_units, n_units))\n        self.w0 = np.zeros((1, n_units))\n\n    def parameters(self):\n        return np.size(self.W) + np.size(self.w0)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W - 0.01 * grad_w\n            self.w0 = self.w0 - 0.01 * grad_w0\n        accum_grad = np.dot(accum_grad, W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 3, "solution": "import numpy as np\nimport copy\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W - grad_w\n            self.w0 = self.w0 - grad_w0\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 4, "solution": "import numpy as np\nfrom optimizer import Adam\nclass Dense(Layer):\n    \"\"\"A fully-connected NN layer.\n    Parameters:\n    -----------\n    n_units: int\n        The number of neurons in the layer.\n    input_shape: tuple\n        The expected input shape of the layer.\n    \"\"\"\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = Adam()\n        self.w0_opt = Adam()\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 5, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W - grad_w\n            self.w0 = self.w0 - grad_w0\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 6, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        \"\"\" Set the shape that the layer expects of the input in the forward\n        pass method \"\"\"\n        self.input_shape = shape\n\n    def layer_name(self):\n        \"\"\" The name of the layer. This is used in model summary. \"\"\"\n        return self.__class__.__name__\n\n    def parameters(self):\n        \"\"\" The number of trainable parameters used by the layer \"\"\"\n        return 0\n\n    def forward_pass(self, X, training):\n        \"\"\" Propogate the signal forward \"\"\"\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        \"\"\" Propogate the accumulated gradient backward \"\"\"\n        raise NotImplementedError()\n\n    def output_shape(self):\n        \"\"\" The shape of the output produced by forward_pass \"\"\"\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.copy(optimizer)\n        self.w0_opt = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 7, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.copy(optimizer)\n        self.w0_opt = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 8, "solution": "import numpy as np\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.vW = None\n        self.vw0 = None\n\n    def initialize(self, optimizer):\n        limit = 1.0 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.vW = np.zeros_like(self.W)\n        self.vw0 = np.zeros_like(self.w0)\n        self.optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.vW = self.optimizer.update(self.vW, grad_w)\n            self.vw0 = self.optimizer.update(self.vw0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 9, "solution": "import numpy as np\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n        self.initialize()\n\n    def initialize(self):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = np.zeros_like(self.W)\n        self.w0_opt = np.zeros_like(self.w0)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W_opt = grad_w\n            self.w0_opt = grad_w0\n            gradient = accum_grad.dot(W.T)\n        else:\n            gradient = accum_grad.dot(W.T)\n        return gradient\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.pad(input_matrix, padding)\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = int((input_height - kernel_height + 2 * padding) / stride + 1)\n    output_width = int((input_width - kernel_width + 2 * padding) / stride + 1)\n    output_matrix = np.zeros((output_height, output_width))\n    for y in range(output_height):\n        for x in range(output_width):\n            convolution_step = y * stride\n            output_matrix[y, x] = np.sum(np.multiply(input_matrix[convolution_step:convolution_step + kernel_height, x * stride:x * stride + kernel_width], kernel))\n    return [list(map(lambda x: round(x, 4), lst)) for lst in list(output_matrix)]"}
{"task_id": 41, "completion_id": 1, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (input_h, input_w) = padded_input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    output_h = (input_h - kernel_h) // stride + 1\n    output_w = (input_w - kernel_w) // stride + 1\n    output_matrix = np.zeros((output_h, output_w))\n    for i in range(0, input_h - kernel_h + 1, stride):\n        for j in range(0, input_w - kernel_w + 1, stride):\n            sub_matrix = padded_input_matrix[i:i + kernel_h, j:j + kernel_w]\n            output_matrix[i // stride, j // stride] = np.sum(sub_matrix * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    output_list = output_matrix.tolist()\n    return output_list"}
{"task_id": 41, "completion_id": 2, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = padded_input.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            output[i, j] = np.sum(padded_input[i:i + kernel_height, j:j + kernel_width] * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 3, "solution": "import numpy as np\nfrom scipy import signal\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(0, input_height - kernel_height + 1, stride):\n        for j in range(0, input_width - kernel_width + 1, stride):\n            patch = input_matrix[i:i + kernel_height, j:j + kernel_width]\n            output_matrix[i // stride, j // stride] = np.sum(patch * kernel)\n    output_matrix = np.round(output_matrix, 4).tolist()\n    return output_matrix"}
{"task_id": 41, "completion_id": 4, "solution": "import numpy as np\nfrom typing import List\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> List[List[float]]:\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            (h_start, w_start) = (i * stride, j * stride)\n            (h_end, w_end) = (h_start + kernel_height, w_start + kernel_width)\n            output_matrix[i, j] = np.sum(input_matrix[h_start:h_end, w_start:w_end] * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()"}
{"task_id": 41, "completion_id": 5, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    output_height = (padded_input.shape[0] - kernel.shape[0]) // stride + 1\n    output_width = (padded_input.shape[1] - kernel.shape[1]) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for h in range(0, output_height):\n        for w in range(0, output_width):\n            h_start = h * stride\n            h_end = h_start + kernel.shape[0]\n            w_start = w * stride\n            w_end = w_start + kernel.shape[1]\n            output_matrix[h, w] = np.sum(padded_input[h_start:h_end, w_start:w_end] * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]])\nkernel = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])"}
{"task_id": 41, "completion_id": 6, "solution": "import numpy as np\ndef pad_input_matrix(input_matrix, padding):\n    return np.pad(input_matrix, padding, mode='constant')\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_input_matrix = pad_input_matrix(input_matrix, padding)\n    (input_height, input_width) = padded_input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(0, input_height - kernel_height + 1, stride):\n        for j in range(0, input_width - kernel_width + 1, stride):\n            output_matrix[i, j] = np.sum(padded_input_matrix[i:i + kernel_height, j:j + kernel_width] * kernel)\n    return output_matrix.round(4).tolist()"}
{"task_id": 41, "completion_id": 7, "solution": "import numpy as np\ndef simple_conv2d(input_matrix, kernel, padding, stride):\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height - kernel_height) // stride + 1\n    out_width = (in_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((out_height, out_width))\n    for i in range(0, in_height - kernel_height + 1, stride):\n        for j in range(0, in_width - kernel_width + 1, stride):\n            x = i // stride\n            y = j // stride\n            output_matrix[x, y] = np.sum(input_matrix[i:i + kernel_height, j:j + kernel_width] * kernel)\n    return output_matrix.round(4).tolist()"}
{"task_id": 41, "completion_id": 8, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    convolved_matrix = np.zeros((output_height, output_width))\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            convolved_matrix[i, j] = np.sum(input_matrix[i:i + kernel_height, j:j + kernel_width] * kernel)\n    convolved_matrix = np.round(convolved_matrix, 4)\n    return convolved_matrix.tolist()"}
{"task_id": 41, "completion_id": 9, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.pad(input_matrix, [(padding, padding), (padding, padding)], mode='constant')\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    output_h = int((input_h - kernel_h) / stride + 1)\n    output_w = int((input_w - kernel_w) / stride + 1)\n    output_matrix = np.zeros((output_h, output_w))\n    for i in range(0, output_h, stride):\n        for j in range(0, output_w, stride):\n            region = input_matrix[i:i + kernel_h, j:j + kernel_w]\n            output_matrix[i // stride, j // stride] = np.sum(np.multiply(region, kernel))\n    output_matrix = np.round(output_matrix, 4)\n    output_matrix = output_matrix.tolist()\n    return output_matrix\ninput_matrix = np.array([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]])\nkernel = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 1, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implement the ReLU activation function.\n\n    Args:\n        z: The input value for the function.\n\n    Returns:\n        The value after applying the ReLU function.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 2, "solution": "import numpy as np\ndef relu(z: float) -> float:\n    return max(0, z)"}
{"task_id": 42, "completion_id": 3, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0"}
{"task_id": 42, "completion_id": 4, "solution": "def relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 42, "completion_id": 5, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 6, "solution": "def relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 42, "completion_id": 7, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 8, "solution": "def relu(z: float) -> float:\n    \"\"\"ReLU (Rectified Linear Unit) activation function implementation.\"\"\"\n    if z > 0:\n        return z\n    else:\n        return 0"}
{"task_id": 42, "completion_id": 9, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    mse = np.mean((np.dot(X, w) - y_true) ** 2)\n    ridge_loss = mse + alpha * np.dot(w.T, w)\n    ridge_loss = round(ridge_loss, 4)\n    return ridge_loss\nX = np.array([[1, 2], [3, 4], [5, 6]])\nw = np.array([0.1, 0.2])\ny_true = np.array([0.3, 0.4, 0.5])\nalpha = 0.01"}
{"task_id": 43, "completion_id": 1, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    ridge = alpha * np.dot(w.T, w)\n    total_loss = mse + ridge\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 2, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    se = np.square(y_true - y_pred)\n    loss = np.mean(se) + alpha * np.dot(w.T, w)\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 3, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    mse = np.mean((y_true - np.dot(X, w)) ** 2)\n    loss = mse + alpha * np.linalg.norm(w) ** 2\n    loss = round(loss, 4)\n    return loss"}
{"task_id": 43, "completion_id": 4, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_predicted = np.dot(X, w)\n    mse = np.mean((y_true - y_predicted) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)\nX = np.random.randn(n_samples, n_features)\nw = np.random.randn(n_features)\ny_true = np.dot(X, w) + np.random.randn(n_samples)\nalpha = 0.1"}
{"task_id": 43, "completion_id": 5, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    mse = np.mean((np.dot(X, w) - y_true) ** 2)\n    reg_term = alpha * np.dot(w.T, w)\n    loss = mse + reg_term\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 6, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss using the Mean Squared Error (MSE) and a regularization term.\n\n    Args:\n        X: A 2D numpy array representing the feature matrix.\n        w: A 1D numpy array representing the coefficients.\n        y_true: A 1D numpy array representing the true labels.\n        alpha: A float representing the regularization parameter.\n\n    Returns:\n        The Ridge Regression loss.\n    \"\"\"\n    y_predicted = X.dot(w)\n    mse = np.mean((y_true - y_predicted) ** 2)\n    ridge_loss = mse + alpha * np.sum(w ** 2)\n    return round(ridge_loss, 4)\nw = np.array([0.5, 0.5])\ny_true = np.array([1, 2, 3])\nalpha = 0.1"}
{"task_id": 43, "completion_id": 7, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    mse = np.mean(np.square(y_true - y_pred))\n    reg = alpha * np.sum(np.square(w))\n    loss = mse + reg\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 8, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    ridge_loss = mse + alpha * np.dot(w.T, w)\n    return round(ridge_loss, 4)\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nw = np.array([1, 2, 3])\ny_true = np.array([7, 8, 9])\nalpha = 0.5"}
{"task_id": 43, "completion_id": 9, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    mse = np.mean((np.dot(X, w) - y_true) ** 2)\n    ridge = alpha * np.sum(w ** 2)\n    loss = mse + ridge\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 1, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 2, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 3, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 4, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    if z < 0:\n        return alpha * z\n    else:\n        return z"}
{"task_id": 44, "completion_id": 5, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 6, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Leaky Rectified Linear Unit (Leaky ReLU) Activation Function. \n    Parameters:\n        z (float): Input float\n        alpha (float): Slope for negative inputs, default value is 0.01\n    Returns:\n        The value after applying the Leaky ReLU function\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 44, "completion_id": 7, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    This function implements Leaky ReLU activation function.\n    Args:\n        z: A float type input to the function.\n        alpha: A float type slope for negative values, default value is 0.01.\n    Returns:\n        The output of the function after applying Leaky ReLU.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 44, "completion_id": 8, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 9, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 1, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 2, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Calculate linear kernel between two input vectors. \n    The linear kernel is defined as the dot product (or inner product) of two vectors. \n\n    Parameters:\n    x1 (ndarray): Input vector 1. \n    x2 (ndarray): Input vector 2. \n\n    Returns:\n    float: The inner product (dot product) of the two input vectors. \n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 3, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 4, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 5, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 6, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 7, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    This function computes the linear kernel between two input vectors x1 and x2.\n    The linear kernel is defined as the dot product (inner product) of two vectors.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 8, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 9, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision of a given binary label vector\n    y_true: numpy array, true binary labels\n    y_pred: numpy array, predicted binary labels\n    \"\"\"\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_positives = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    else:\n        return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 1, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    try:\n        precision = tp / (tp + fp)\n    except ZeroDivisionError:\n        print('Warning: Both true positives and false positives are equal to 0. Precision is undefined.')\n        precision = np.nan\n    return precision"}
{"task_id": 46, "completion_id": 2, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    (y_true, y_pred) = (y_true.astype(bool), y_pred.astype(bool))\n    true_positives = np.sum(np.bitwise_and(y_true, y_pred))\n    false_positives = np.sum(np.bitwise_and(np.bitwise_not(y_true), y_pred))\n    precision = true_positives / (true_positives + false_positives)\n    return precision"}
{"task_id": 46, "completion_id": 3, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    true_positives = np.sum(y_true * y_pred)\n    false_positives = np.sum(y_pred - y_true > 0)\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 4, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_positives = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 'No true or false positives found, cannot compute precision.'\n    precision = true_positives / (true_positives + false_positives)\n    return precision"}
{"task_id": 46, "completion_id": 5, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have same shape')\n    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    FP = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    return TP / (TP + FP)"}
{"task_id": 46, "completion_id": 6, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    precision = tp / (tp + fp)\n    return precision"}
{"task_id": 46, "completion_id": 7, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    fp = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n    if tp + fp == 0:\n        return 0\n    return tp / (tp + fp)\ny_true = np.array([1, 0, 0, 1, 0, 1, 0, 1, 1, 0])\ny_pred = np.array([1, 0, 1, 1, 0, 0, 1, 0, 1, 0])"}
{"task_id": 46, "completion_id": 8, "solution": "import numpy as np\nprecision = precision(y_true, y_pred)\ny_true = np.random.randint(2, size=100)\ny_pred = np.random.randint(2, size=100)"}
{"task_id": 46, "completion_id": 9, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    if tp == 0 and fp == 0:\n        return 0\n    precision = tp / (tp + fp)\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef mean_squared_error(y_true, y_pred):\n    return np.mean(np.power(y_true - y_pred, 2))\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    history = np.zeros(n_iterations)\n    m = len(y)\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            gradient = 1 / m * X.T.dot(mean_squared_error(X.dot(weights), y))\n            weights = weights - learning_rate * gradient\n            loss = mean_squared_error(X.dot(weights), y)\n            history[iteration] = loss\n        elif method == 'stochastic':\n            for i in range(m):\n                random_index = np.random.randint(m)\n                x_i = X[random_index:random_index + 1]\n                y_i = y[random_index:random_index + 1]\n                gradient = 2 * x_i.T.dot(mean_squared_error(x_i.dot(weights), y_i) - y_i)\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            if batch_size > m:\n                batch_size = m\n            batch_indices = np.random.choice(m, batch_size)\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            gradient = 2 / batch_size * X_batch.T.dot(mean_squared_error(X_batch.dot(weights), y_batch) - y_batch)\n            weights = weights - learning_rate * gradient\n            loss = mean_squared_error(X_batch.dot(weights), y_batch)\n            history[iteration] = loss\n        else:\n            raise ValueError(\"Invalid method parameter. Should be 'batch', 'stochastic', or 'mini-batch'.\")\n    weights = weights.round(4)\n    return (weights.tolist(), history.tolist())"}
{"task_id": 47, "completion_id": 1, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Function to perform gradient descent\n\n    Arguments:\n    X: numpy array of predictors\n    y: numpy array of response variable\n    weights: initial weights for the predictors\n    learning_rate: step size for each iteration\n    n_iterations: total number of iterations\n    batch_size: optional argument; size of the batch for mini-batch gradient descent\n    method: method to use; can be 'batch', 'stochastic' or 'mini-batch'\n\n    Returns:\n    numpy array of weights after performing gradient descent\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if batch_size > n_samples:\n        raise ValueError('Batch size cannot be greater than number of samples')\n    X_transpose = X.T\n    new_weights = weights\n    if method == 'batch':\n        for i in range(n_iterations):\n            prediction = np.dot(X, new_weights)\n            error = prediction - y\n            gradient = np.dot(X_transpose, error) / n_samples\n            new_weights -= learning_rate * gradient\n    elif method == 'stochastic':\n        for _ in range(n_iterations):\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                X_rand = X[random_index:random_index + 1]\n                y_rand = y[random_index:random_index + 1]\n                prediction = np.dot(X_rand, new_weights)\n                error = prediction - y_rand\n                gradient = np.dot(X_transpose, error)\n                new_weights -= learning_rate * gradient\n    elif method == 'mini-batch':\n        for _ in range(n_iterations):\n            batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            prediction = np.dot(X_batch, new_weights)\n            error = prediction - y_batch\n            gradient = np.dot(X_transpose, error) / batch_size\n            new_weights -= learning_rate * gradient\n    else:\n        raise ValueError('Incorrect method chosen. Please choose either \"batch\", \"stochastic\" or \"mini-batch\"')\n    new_weights = np.round(new_weights, 4)\n    return new_weights\nX = np.random.rand(100, 2)\ny = np.random.rand(100, 1)\nweights = np.random.rand(2, 1)"}
{"task_id": 47, "completion_id": 2, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    (m, n) = X.shape\n    weights = weights.reshape((n, 1))\n    X = X.reshape(m, n)\n    y = y.reshape((m, 1))\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradient = 2 / m * X.T.dot(X.dot(weights) - y)\n        elif method == 'stochastic':\n            rand_index = np.random.randint(m)\n            xi = X[rand_index:rand_index + 1]\n            yi = y[rand_index:rand_index + 1]\n            gradient = 2 * xi.T.dot(xi.dot(weights) - yi)\n        elif method == 'mini-batch':\n            rand_indices = np.random.choice(m, batch_size)\n            Xi = X[rand_indices]\n            yi = y[rand_indices]\n            gradient = 2 / batch_size * Xi.T.dot(Xi.dot(weights) - yi)\n        else:\n            raise ValueError(f\"Invalid method: {method}. Should be 'batch', 'stochastic', or 'mini-batch'.\")\n        weights = weights - learning_rate * gradient\n    return np.round(weights.flatten().tolist(), 4)"}
{"task_id": 47, "completion_id": 3, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    if len(X.shape) == 1:\n        X = X.reshape(-1, 1)\n    if len(y.shape) == 1:\n        y = y.reshape(-1, 1)\n    y = y.reshape(-1, 1)\n    weights = weights.reshape(-1, 1)\n\n    def stochastic_gradient(X, y, weights):\n        n_samples = X.shape[0]\n        random_index = np.random.randint(0, n_samples)\n        x_sample = X[random_index, :].reshape(1, -1)\n        y_sample = y[random_index].reshape(1, -1)\n        prediction = np.dot(x_sample, weights)\n        gradient = 2 * np.dot(x_sample.T, prediction - y_sample)\n        return gradient / n_samples\n\n    def mini_batch_gradient(X, y, weights, batch_size):\n        n_samples = X.shape[0]\n        batch_indices = np.random.choice(n_samples, size=batch_size)\n        x_batch = X[batch_indices, :]\n        y_batch = y[batch_indices].reshape(-1, 1)\n        prediction = np.dot(x_batch, weights)\n        gradient = 2 * np.dot(x_batch.T, prediction - y_batch)\n        return gradient / n_samples\n\n    def batch_gradient(X, y, weights):\n        n_samples = X.shape[0]\n        prediction = np.dot(X, weights)\n        gradient = 2 * np.dot(X.T, prediction - y)\n        return gradient / n_samples\n    gradient_functions = {'stochastic': stochastic_gradient, 'mini_batch': mini_batch_gradient, 'batch': batch_gradient}\n    gradient_func = gradient_functions[method]\n    for _ in range(n_iterations):\n        grad = gradient_func(X, y, weights)\n        weights = weights - learning_rate * grad\n    return np.round(weights.tolist(), 4)"}
{"task_id": 47, "completion_id": 4, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n\n    def batch_gradient_descent(X, y, weights, learning_rate):\n        N = len(X)\n        prediction = np.dot(X, weights)\n        grad = -2 / N * np.dot(X.T, y - prediction)\n        weights = weights - learning_rate * grad\n        return weights\n\n    def stochastic_gradient_descent(X, y, weights, learning_rate):\n        sample = np.random.randint(0, len(X))\n        prediction = np.dot(X[sample], weights)\n        grad = -2 * np.dot(X[sample].T.reshape(-1, 1), (y[sample] - prediction).reshape(-1, 1))\n        weights = weights - learning_rate * grad\n        return weights\n\n    def mini_batch_gradient_descent(X, y, weights, learning_rate, batch_size):\n        sample = np.random.choice(len(X), batch_size)\n        prediction = np.dot(X[sample], weights)\n        grad = -2 * np.dot(X[sample].T, y[sample] - prediction)\n        weights = weights - learning_rate / batch_size * grad\n        return weights\n    for _ in range(n_iterations):\n        if method == 'batch':\n            weights = batch_gradient_descent(X, y, weights, learning_rate)\n        elif method == 'stochastic':\n            weights = stochastic_gradient_descent(X, y, weights, learning_rate)\n        elif method == 'mini-batch':\n            weights = mini_batch_gradient_descent(X, y, weights, learning_rate, batch_size)\n        else:\n            raise ValueError(\"Invalid method. Available options are 'batch', 'stochastic', and 'mini-batch'.\")\n    return np.around(weights.tolist(), decimals=4)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([5, 7, 9, 11])\nweights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=2, method='mini-batch')\nlearning_rate = 0.01\nn_iterations = 1000"}
{"task_id": 47, "completion_id": 5, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    errorDiff = lambda w, X, y: np.dot(X, w) - y\n    m = len(y)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            gradients = 2 / m * np.dot(X.T, errorDiff(weights, X, y))\n            weights = weights - learning_rate * gradients\n        elif method == 'stochastic':\n            random_index = np.random.randint(m)\n            xi = X[random_index:random_index + 1]\n            yi = y[random_index:random_index + 1]\n            gradients = 2 * np.dot(xi.T, errorDiff(weights, xi, yi))\n            weights = weights - learning_rate * gradients\n        elif method == 'mini-batch':\n            random_indices = np.random.randint(m, size=batch_size)\n            Xi = X[random_indices]\n            yi = y[random_indices]\n            gradients = 2 / batch_size * np.dot(Xi.T, errorDiff(weights, Xi, yi))\n            weights = weights - learning_rate * gradients\n        else:\n            raise ValueError(\"Invalid method. Choose either 'batch', 'stochastic', or 'mini-batch'.\")\n    return np.around(weights, decimals=4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6]])\ny = np.array([5, 7])\nweights = np.array([1.0, 1.0, 1.0])"}
{"task_id": 47, "completion_id": 6, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    (n_samples, n_features) = X.shape\n    assert len(weights) == n_features, f'Weights vector should have the same size as the number of features.'\n    assert method.lower() in ['sgd', 'batch', 'mini-batch'], f\"Method should be one of 'sgd', 'batch', or 'mini-batch'.\"\n    X_transpose = X.T\n    if method.lower() == 'sgd':\n        for _ in range(n_iterations):\n            for i in range(n_samples):\n                prediction = np.dot(X[i], weights)\n                error = y[i] - prediction\n                weights += learning_rate * error * X[i]\n        return weights.round(4).tolist()\n    else:\n        for _ in range(n_iterations):\n            if method.lower() == 'batch':\n                batch_start = 0\n                batch_end = n_samples\n                batch_step = n_samples // batch_size\n            elif method.lower() == 'mini-batch':\n                batch_start = [i * batch_size % n_samples for i in range(n_samples // batch_size)]\n                batch_end = [(i + 1) * batch_size % n_samples for i in range(n_samples // batch_size)]\n                batch_step = 1\n            for batch in range(len(batch_start)):\n                prediction = np.dot(X[batch_start[batch]:batch_end[batch]], weights)\n                error = y[batch_start[batch]:batch_end[batch]] - prediction\n                weights += learning_rate / n_samples * np.dot(X_transpose[batch_start[batch]:batch_end[batch]], error)\n                batch_start = [(i + batch_step) % n_samples for i in batch_start]\n                batch_end = [(i + batch_step) % n_samples for i in batch_end]\n        return weights.round(4).tolist()"}
{"task_id": 47, "completion_id": 7, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\" Perform gradient descent with MSE loss\"\"\"\n    m = X.shape[0]\n    n = X.shape[1]\n    X_transpose = X.transpose()\n    weights = np.reshape(weights, (n, 1))\n    for i in range(n_iterations):\n        if method == 'batch':\n            hypothesis = np.dot(X, weights)\n            loss = hypothesis - y\n            gradient = np.dot(X_transpose, loss) / m\n        elif method == 'sgd':\n            for j in range(m):\n                sample_index = np.random.randint(0, m)\n                sample = X[sample_index, :].reshape(1, n)\n                sample_label = y[sample_index, :].reshape(1, 1)\n                hypothesis = np.dot(sample, weights)\n                loss = hypothesis - sample_label\n                gradient = np.dot(sample.transpose(), loss)\n        elif method == 'mini-batch':\n            indices = np.random.randint(0, m, batch_size)\n            sample = X[indices, :]\n            sample_label = y[indices, :].reshape(batch_size, 1)\n            hypothesis = np.dot(sample, weights)\n            loss = hypothesis - sample_label\n            gradient = np.dot(sample.transpose(), loss) / batch_size\n        else:\n            raise ValueError(\"Unknown method. Choose from 'batch', 'sgd', or 'mini-batch'.\")\n        weights = weights - learning_rate * gradient\n    return np.round(weights.flatten().tolist(), 4)"}
{"task_id": 47, "completion_id": 8, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n\n    def predict(X, weights):\n        return np.dot(X, weights)\n\n    def MSE(y_pred, y_true):\n        return np.mean((y_pred - y_true) ** 2)\n\n    def calculate_gradients(X_batch, y_batch, weights):\n        n_samples = X_batch.shape[0]\n        y_pred = predict(X_batch, weights)\n        residuals = y_pred - y_batch\n        gradient = np.dot(X_batch.T, residuals) / n_samples\n        return gradient\n    loss = np.zeros(n_iterations)\n    mse = np.zeros(n_iterations)\n    weights_history = []\n    for i in range(n_iterations):\n        if method.lower() == 'sgd':\n            random_index = np.random.randint(len(X))\n            (X_i, y_i) = (X[random_index:random_index + 1], y[random_index:random_index + 1])\n            gradients = calculate_gradients(X_i, y_i, weights)\n            weights = weights - learning_rate * gradients\n        elif method.lower() == 'batch':\n            gradients = calculate_gradients(X, y, weights)\n            weights = weights - learning_rate * gradients\n        elif method.lower() == 'mini-batch':\n            batch_indices = np.random.choice(X.shape[0], size=batch_size)\n            (X_i, y_i) = (X[batch_indices], y[batch_indices])\n            gradients = calculate_gradients(X_i, y_i, weights)\n            weights = weights - learning_rate * gradients\n        else:\n            raise ValueError(f\"Invalid method {method}. Supported methods are 'sgd', 'batch' and 'mini-batch'.\")\n        y_pred = predict(X, weights)\n        loss[i] = MSE(y_pred, y)\n        if (i + 1) % 50 == 0:\n            print(f'Iteration: {i + 1}, Loss: {loss[i]}')\n        weights_history.append(weights.tolist())\n    return (weights.round(4).tolist(), loss.round(4).tolist(), weights_history)"}
{"task_id": 47, "completion_id": 9, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    if method == 'batch':\n        return batch_gradient_descent(X, y, weights, learning_rate, n_iterations)\n    elif method == 'stochastic':\n        return stochastic_gradient_descent(X, y, weights, learning_rate, n_iterations)\n    elif method == 'mini-batch':\n        return mini_batch_gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size)\n    else:\n        raise ValueError(\"Invalid method. Supported methods are 'batch', 'stochastic', 'mini-batch'\")\ndef batch_gradient_descent(X, y, weights, learning_rate, n_iterations):\n    m = X.shape[0]\n    for _ in range(n_iterations):\n        gradient = 1 / m * np.dot(X.T, np.dot(X, weights) - y)\n        weights = weights - learning_rate * gradient\n    return weights.round(4).tolist()\ndef stochastic_gradient_descent(X, y, weights, learning_rate, n_iterations):\n    m = X.shape[0]\n    for _ in range(n_iterations):\n        for i in range(m):\n            random_index = np.random.randint(m)\n            xi = X[random_index:random_index + 1]\n            yi = y[random_index:random_index + 1]\n            gradient = np.dot(xi.T, np.dot(xi, weights) - yi)\n            weights = weights - learning_rate * gradient\n    return weights.round(4).tolist()\ndef mini_batch_gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size):\n    m = X.shape[0]\n    for _ in range(n_iterations):\n        for i in range(0, m, batch_size):\n            xi = X[i:i + batch_size]\n            yi = y[i:i + batch_size]\n            gradient = 1 / batch_size * np.dot(xi.T, np.dot(xi, weights) - yi)\n            weights = weights - learning_rate * gradient\n    return weights.round(4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    r = 0\n    for c in range(len(matrix[0])):\n        if r >= len(matrix):\n            break\n        j = r\n        while np.abs(matrix[j, c]) < 10 ** (-10):\n            j += 1\n            if j == len(matrix):\n                j = r\n                break\n            j = r if j < len(matrix) else r\n        matrix[[j, r]] = matrix[[r, j]]\n        pivot = matrix[r, c]\n        if np.abs(pivot) < 10 ** (-10):\n            continue\n        matrix[r] = matrix[r] / pivot\n        for i in range(len(matrix)):\n            if i != r:\n                matrix[i] = matrix[i] - matrix[i, c] * matrix[r]\n        r += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 1, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    This function converts a given matrix into its Reduced Row Echelon Form (RREF)\n    Parameters:\n    matrix (numpy.ndarray): The matrix to be converted.\n\n    Returns:\n    numpy.ndarray: The matrix after converting into its RREF.\n    \"\"\"\n    matrix = np.array(matrix, dtype=np.float64)\n    (r, c) = matrix.shape\n    for i in range(min(r, c)):\n        largest_element_idx = np.argmax(np.abs(matrix[i:, i])) + i\n        matrix[[i, largest_element_idx]] = matrix[[largest_element_idx, i]]\n        if matrix[i, i] == 0:\n            continue\n        matrix[i] = matrix[i] / matrix[i, i]\n        for j in range(i + 1, r):\n            matrix[j] -= matrix[j, i] * matrix[i]\n    for i in range(min(r, c))[::-1]:\n        if matrix[i, i] != 0:\n            for j in range(i):\n                matrix[j] -= matrix[j, i] * matrix[i]\n    return matrix"}
{"task_id": 48, "completion_id": 2, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=np.float64)\n    (r, c) = matrix.shape\n    (i, j) = (0, 0)\n    while i < r and j < c:\n        k = np.argmax(np.abs(matrix[i:, j])) + i\n        temp = np.copy(matrix[k, :])\n        matrix[k, :] = matrix[i, :]\n        matrix[i, :] = temp\n        if np.abs(matrix[i, j]) < 1e-05:\n            j += 1\n            continue\n        matrix[i, :] = matrix[i, :] / matrix[i, j]\n        for k in range(r):\n            if k != i:\n                matrix[k, :] = matrix[k, :] - matrix[i, :] * matrix[k, j]\n        i += 1\n        j += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 3, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=np.float64)\n    (r, c) = matrix.shape\n    i = 0\n    j = 0\n    while i < r and j < c:\n        non_zero_row = i\n        while non_zero_row < r and matrix[non_zero_row, j] == 0:\n            non_zero_row += 1\n        if non_zero_row != r:\n            matrix[[i, non_zero_row]] = matrix[[non_zero_row, i]]\n            matrix[i] = matrix[i] / matrix[i, j]\n            for k in range(i + 1, r):\n                if matrix[k, j] != 0:\n                    matrix[k] = matrix[k] - matrix[k, j] * matrix[i]\n            i += 1\n        j += 1\n    for i in range(r - 1):\n        for j in range(i + 1, c):\n            if matrix[i, j] != 0:\n                matrix[i] = matrix[i] - matrix[i, j] * matrix[j]\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 4, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=np.float64)\n    (r, c) = matrix.shape\n    j = 0\n    for i in range(r):\n        if j >= c:\n            break\n        pivot_found = False\n        while not pivot_found:\n            for k in range(i, r):\n                if matrix[k, j] != 0:\n                    matrix[[i, k]] = matrix[[k, i]]\n                    pivot_found = True\n                    break\n            j += 1\n        if not pivot_found:\n            continue\n        row_ratio = matrix[i] / matrix[i, j]\n        row_reduce_func = lambda x: x - row_ratio * matrix[x, j]\n        matrix = row_reduce_func(matrix)\n        matrix[i] = row_ratio * matrix[i]\n        j += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 5, "solution": "import numpy as np\ndef rref(matrix):\n    for lead in range(min(matrix.shape[0], matrix.shape[1])):\n        nonzero = np.nonzero(matrix[lead:, lead])[0] + lead\n        if len(nonzero) == 0:\n            continue\n        matrix[[lead, nonzero[0]]] = matrix[[nonzero[0], lead]]\n        matrix[lead] = matrix[lead] / matrix[lead, lead]\n        for row in range(lead + 1, matrix.shape[0]):\n            matrix[row] = matrix[row] - matrix[row, lead] * matrix[lead]\n    for lead in range(min(matrix.shape[0], matrix.shape[1])):\n        nonzero = np.nonzero(matrix[:, lead])[0]\n        if len(nonzero) == 0:\n            continue\n        matrix[nonzero[0]] = matrix[nonzero[0]] / matrix[nonzero[0], lead]\n        for row in range(lead):\n            matrix[row] = matrix[row] - matrix[row, lead] * matrix[nonzero[0]]\n    matrix[np.abs(matrix) < 1e-15] = 0\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 6, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (r, c) = matrix.shape\n    for i in range(r):\n        if matrix[i, i] == 0:\n            for j in range(i + 1, r):\n                if matrix[j, i] != 0:\n                    matrix[[i, j]] = matrix[[j, i]]\n                    break\n        if matrix[i, i] != 0:\n            matrix[i] = matrix[i] / matrix[i, i]\n            for j in range(r):\n                if j != i:\n                    matrix[j] = matrix[j] - matrix[i] * matrix[j, i]\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 7, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, float)\n    (r, c) = A.shape\n    for k in range(c):\n        max_i = np.argmax(np.abs(A[k:, k])) + k\n        if A[max_i, k] == 0:\n            continue\n        if max_i != k:\n            A[[k, max_i], k:] = A[[max_i, k], k:]\n        A[k] = A[k] / A[k, k]\n        for row in range(r):\n            if row != k:\n                A[row] = A[row] - A[row, k] * A[k]\n    return A.tolist()"}
{"task_id": 48, "completion_id": 8, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (num_rows, num_cols) = matrix.shape\n    current_pivot_row = 0\n    current_pivot_col = 0\n    while current_pivot_row < num_rows and current_pivot_col < num_cols:\n        pivot_row = np.argmax(np.abs(matrix[current_pivot_row:, current_pivot_col])) + current_pivot_row\n        if pivot_row != current_pivot_row:\n            matrix[[current_pivot_row, pivot_row]] = matrix[[pivot_row, current_pivot_row]]\n        if matrix[current_pivot_row, current_pivot_col] == 0:\n            current_pivot_col += 1\n        else:\n            matrix[current_pivot_row] = matrix[current_pivot_row] / matrix[current_pivot_row, current_pivot_col]\n            for i_row in range(num_rows):\n                if i_row != current_pivot_row:\n                    factor = matrix[i_row, current_pivot_col]\n                    matrix[i_row] -= factor * matrix[current_pivot_row]\n            current_pivot_row += 1\n            current_pivot_col += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 9, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, float)\n    (r, c) = np.shape(A)\n    if r == 0 or c == 0:\n        return A.tolist()\n    for i in range(len(A)):\n        if A[i, 0] != 0:\n            break\n    else:\n        B = np.hstack([A[:, 1:], A[:, 0:1]])\n        return rref(B.tolist())\n    if i > 0:\n        ith_row = A[i].copy()\n        A[i] = A[0]\n        A[0] = ith_row\n    A[0] = A[0] / A[0, 0]\n    A[1:, 0] = A[1:, 0] / A[0, 0]\n    A[1:, 0] = -A[1:, 0]\n    for i in range(1, len(A)):\n        for j in range(1, len(A[0])):\n            A[i, j] = round(A[i, j] + A[0, j] * A[i, 0], 2)\n    B = rref(A[1:, 1:].tolist())\n    return np.vstack([A[:1], np.hstack([A[1:, :1], B])]).tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros(np.shape(x0))\n    v = np.zeros(np.shape(x0))\n    for t in range(num_iterations):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.square(g)\n        m_hat = m / (1 - np.power(beta1, t + 1))\n        v_hat = v / (1 - np.power(beta2, t + 1))\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0.tolist(), 4)"}
{"task_id": 49, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x0).reshape(-1)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    x0_rounded = np.round(x0, 4).tolist()\n    return x0_rounded"}
{"task_id": 49, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = x0\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.square(g)\n        m_hat = m / (1 - np.power(beta1, t))\n        v_hat = v / (1 - np.power(beta2, t))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)"}
{"task_id": 49, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = 0\n    v = 0\n    x = x0\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0.tolist(), 4)"}
{"task_id": 49, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    mt = np.zeros(len(x0))\n    vt = np.zeros(len(x0))\n    t = 0\n    x0 = np.array(x0)\n    while t < num_iterations:\n        t += 1\n        g = grad(x0)\n        mt = beta1 * mt + (1 - beta1) * g\n        vt = beta2 * vt + (1 - beta2) * g ** 2\n        mt_hat = mt / (1 - np.power(beta1, t))\n        vt_hat = vt / (1 - np.power(beta2, t))\n        x0 = x0 - learning_rate * mt_hat / (np.sqrt(vt_hat) + epsilon)\n    return np.around(x0.tolist(), 4)"}
{"task_id": 49, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = x0\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for i in range(num_iterations):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** (i + 1))\n        v_hat = v / (1 - beta2 ** (i + 1))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)"}
{"task_id": 49, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    mt = np.zeros_like(x0)\n    vt = np.zeros_like(x0)\n    for t in range(1, num_iterations + 1):\n        gt = grad(x0)\n        mt = beta1 * mt + (1 - beta1) * gt\n        vt = beta2 * vt + (1 - beta2) * gt ** 2\n        mt_hat = mt / (1 - beta1 ** t)\n        vt_hat = vt / (1 - beta2 ** t)\n        x0 = x0 - learning_rate * mt_hat / (np.sqrt(vt_hat) + epsilon)\n    return np.round(x0.tolist(), 4)"}
{"task_id": 49, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m_t = np.zeros_like(x0)\n    v_t = np.zeros_like(x0)\n    t = 0\n    x = x0\n    for i in range(1, num_iterations + 1):\n        t = t + 1\n        g_t = grad(x)\n        m_t = beta1 * m_t + (1 - beta1) * g_t\n        v_t = beta2 * v_t + (1 - beta2) * np.multiply(g_t, g_t)\n        m_cap = m_t / (1 - beta1 ** t)\n        v_cap = v_t / (1 - beta2 ** t)\n        x = x - learning_rate * m_cap / (np.sqrt(v_cap) + epsilon)\n    return np.round(x.tolist(), 4)\nx0 = np.array([1.0, 2.0])"}
{"task_id": 49, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * np.square(g)\n        m_hat = m / (1 - np.power(beta1, t))\n        v_hat = v / (1 - np.power(beta2, t))\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0.tolist(), 4)"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Lasso Regression using Gradient Descent\n    :param X: 2D numpy array\n    :param y: 1D numpy array\n    :param alpha: float, default is 0.1\n    :param learning_rate: float, default is 0.01\n    :param max_iter: int, default is 1000\n    :param tol: float, default is 1e-4\n    :return: weights: list, bias: float\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    n = float(len(y))\n    for i in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        residuals = y - y_pred\n        d_w = -2 / n * np.dot(X.T, residuals) + alpha * np.sign(weights)\n        weights = weights - learning_rate * d_w\n        d_b = -2 / n * np.sum(residuals)\n        bias = bias - learning_rate * d_b\n        if np.all(np.abs(d_w) < tol):\n            break\n    return (weights.round(4).tolist(), round(bias, 4))"}
{"task_id": 50, "completion_id": 1, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    loss = np.inf\n    for i in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        cost = 1 / (2 * n_samples) * np.sum((predictions - y) ** 2)\n        reg_cost = cost + alpha * np.linalg.norm(weights, ord=1)\n        if np.abs(loss - reg_cost) <= tol:\n            break\n        loss = reg_cost\n        gradient_weights = 1 / n_samples * np.dot(X.T, predictions - y) + alpha * np.sign(weights)\n        gradient_bias = 1 / n_samples * np.sum(predictions - y)\n        weights = weights - learning_rate * gradient_weights\n        bias = bias - learning_rate * gradient_bias\n    weights = np.around(weights, decimals=4)\n    bias = np.around(bias, decimals=4)\n    weights = weights.tolist()\n    return (weights, bias)"}
{"task_id": 50, "completion_id": 2, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    n_samples = X.shape[0]\n    residuals = []\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        residual = y - y_pred\n        residuals.append(np.sum(residual ** 2) / (2 * n_samples))\n        dW = (-2 * np.dot(X.T, residual) + alpha * np.sign(weights)) / n_samples\n        db = -2 * np.sum(residual) / n_samples\n        weights -= learning_rate * dW\n        bias -= learning_rate * db\n        if len(residuals) > 1 and np.abs(residuals[-1] - residuals[-2]) <= tol:\n            break\n    return (weights.tolist(), np.round(bias, 4), np.round(residuals[-1], 4))"}
{"task_id": 50, "completion_id": 3, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n\n    def compute_loss(X, y, w, b, alpha):\n        n = len(y)\n        y_pred = X.dot(w) + b\n        loss = 1 / 2 * np.mean((y - y_pred) ** 2)\n        l1_contribution = alpha * np.sum(np.abs(w))\n        return loss + l1_contribution\n\n    def compute_gradients(X, y, w, b, alpha):\n        n = len(y)\n        y_pred = X.dot(w) + b\n        error = y - y_pred\n        grad_w = -1.0 / n * error.dot(X) + alpha * np.sign(w)\n        grad_b = -1.0 / n * np.sum(error)\n        return (grad_w, grad_b)\n    w = np.zeros(X.shape[1])\n    b = 0\n    loss = compute_loss(X, y, w, b, alpha)\n    for i in range(max_iter):\n        (grad_w, grad_b) = compute_gradients(X, y, w, b, alpha)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        new_loss = compute_loss(X, y, w, b, alpha)\n        if np.abs(new_loss - loss) < tol:\n            break\n        loss = new_loss\n    return (np.round(w, 4).tolist(), np.round(b, 4))"}
{"task_id": 50, "completion_id": 4, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    np.random.seed(42)\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    n = X.shape[0]\n    for i in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        y_diff = y - y_pred\n        old_weights = np.copy(weights)\n        dW = -(2 * np.dot(X.T, y_diff) + alpha * np.sign(weights)) / n\n        dB = -2 * np.sum(y_diff) / n\n        weights -= learning_rate * dW\n        bias -= learning_rate * dB\n        if np.sum(np.abs(old_weights - weights)) < tol:\n            print('Converged after', i, 'iterations')\n            break\n    return (weights.round(4).tolist(), round(bias, 4))\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])"}
{"task_id": 50, "completion_id": 5, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    n_iter = 0\n    while n_iter < max_iter:\n        y_predicted = np.dot(X, weights) + bias\n        dW = -(2 / n_samples) * np.dot(X.T, y - y_predicted) + alpha * np.sign(weights)\n        dB = -2 / n_samples * np.sum(y - y_predicted)\n        weights -= learning_rate * dW\n        bias -= learning_rate * dB\n        n_iter += 1\n        if np.allclose(y, y_predicted, atol=tol):\n            break\n    return (np.round(weights, 4).tolist(), round(bias, 4))\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)"}
{"task_id": 50, "completion_id": 6, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    weights = np.random.randn(X.shape[1])\n    bias = np.random.randn()\n    for _ in range(max_iter):\n        prediction = np.dot(X, weights) + bias\n        difference = prediction - y\n        d_weights = 1 / len(y) * np.dot(X.T, difference) + alpha * np.sign(weights)\n        d_bias = 1 / len(y) * np.sum(difference)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n        if np.mean(np.abs(d_weights)) < tol:\n            break\n    return [np.round(bias, 4), np.round(weights, 4).tolist()]"}
{"task_id": 50, "completion_id": 7, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    w = np.zeros(p)\n    b = 0\n    y = y.reshape(-1, 1)\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = -2 / n * np.dot(X.T, y - y_pred) + alpha * np.sign(w)\n        db = -2 / n * np.sum(y - y_pred)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        loss = np.mean((y - y_pred) ** 2) + alpha * np.sum(np.abs(w))\n        if loss < tol:\n            break\n    return (np.round(w.tolist(), 4), np.round(b, 4))"}
{"task_id": 50, "completion_id": 8, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for i in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred)\n        db = -2 / n_samples * np.sum(y - y_pred)\n        weights = weights - learning_rate * (dw + alpha * np.sign(weights))\n        bias = bias - learning_rate * db\n        if np.sum(np.abs(weights)) < tol and np.sum(np.abs(bias)) < tol:\n            break\n    return (weights.round(4).tolist(), round(bias, 4))"}
{"task_id": 50, "completion_id": 9, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    for _ in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        weights_gradient = np.dot(X.T, predictions - y) / y.shape[0]\n        weights_gradient += alpha * np.sign(weights)\n        bias_gradient = np.sum(predictions - y) / y.shape[0]\n        new_weights = weights - learning_rate * weights_gradient\n        new_bias = bias - learning_rate * bias_gradient\n        if np.sum(np.abs(new_weights - weights)) + abs(new_bias - bias) < tol:\n            break\n        (weights, bias) = (new_weights, new_bias)\n    return np.round([weights, bias], 4).tolist()"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    n = len(source)\n    m = len(target)\n    dp = np.zeros((n + 1, m + 1))\n    for i in range(n + 1):\n        for j in range(m + 1):\n            if not i:\n                dp[i][j] = j\n            elif not j:\n                dp[i][j] = i\n            elif source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                insert_cost = dp[i][j - 1]\n                delete_cost = dp[i - 1][j]\n                replace_cost = dp[i - 1][j - 1]\n                if (i > 1 and j > 1) and (source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]):\n                    transpose_cost = dp[i - 2][j - 2] + 1\n                    dp[i][j] = 1 + min(insert_cost, delete_cost, replace_cost, transpose_cost)\n                else:\n                    dp[i][j] = 1 + min(insert_cost, delete_cost, replace_cost)\n    return int(dp[n][m])"}
{"task_id": 51, "completion_id": 1, "solution": "import numpy as np\ndef OSA(s1: str, s2: str) -> int:\n    (len_s1, len_s2) = (len(s1), len(s2))\n    dist_matrix = np.zeros((len_s1 + 1, len_s2 + 1), dtype=int)\n    dist_matrix[1:, 0] = range(1, len_s1 + 1)\n    dist_matrix[0, 1:] = range(1, len_s2 + 1)\n    for i in range(1, len_s1 + 1):\n        for j in range(1, len_s2 + 1):\n            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n            dist_matrix[i, j] = min(dist_matrix[i - 1, j] + 1, dist_matrix[i, j - 1] + 1, dist_matrix[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (s1[i - 1] == s2[j - 2]) and (s1[i - 2] == s2[j - 1]):\n                dist_matrix[i, j] = min(dist_matrix[i, j], dist_matrix[i - 2, j - 2] + cost)\n    return dist_matrix[len_s1, len_s2]\ns1 = 'caper'\ns2 = 'acer'"}
{"task_id": 51, "completion_id": 2, "solution": "import numpy as np\ndef minDis(source, target, s_len, t_len):\n    if s_len == 0:\n        return t_len\n    if t_len == 0:\n        return s_len\n    cost = 0 if source[s_len - 1] == target[t_len - 1] else 1\n    return min(minDis(source, target, s_len - 1, t_len) + 1, minDis(source, target, s_len, t_len - 1) + 1, minDis(source, target, s_len - 1, t_len - 1) + cost)\ndef OSA(source: str, target: str) -> int:\n    s_len = len(source)\n    t_len = len(target)\n    return minDis(source, target, s_len, t_len)"}
{"task_id": 51, "completion_id": 3, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    n = len(source)\n    m = len(target)\n    dp = np.zeros((n + 1, m + 1))\n    for i in range(n + 1):\n        dp[i][0] = i\n    for j in range(m + 1):\n        dp[0][j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            replace = dp[i - 1][j - 1]\n            insert = dp[i][j - 1]\n            delete = dp[i - 1][j]\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = replace\n            else:\n                dp[i][j] = 1 + min(replace, insert, delete)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return int(dp[n][m])"}
{"task_id": 51, "completion_id": 4, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    INSERT_COST = 1\n    DELETE_COST = 1\n    SUBSTITUTE_COST = 1\n    TRANSPOSE_COST = 1\n    dp_table = np.zeros((len(source) + 1, len(target) + 1))\n    for i in range(len(source) + 1):\n        dp_table[i, 0] = i\n    for j in range(len(target) + 1):\n        dp_table[0, j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            if source[i - 1] == target[j - 1]:\n                substitute_cost = 0\n                transpose_cost = dp_table[i - 2, j - 2] + TRANSPOSE_COST if i >= 2 and j >= 2 else float('inf')\n            else:\n                substitute_cost = SUBSTITUTE_COST\n                transpose_cost = float('inf')\n            dp_table[i, j] = min(dp_table[i - 1, j] + DELETE_COST, dp_table[i, j - 1] + INSERT_COST, dp_table[i - 1, j - 1] + substitute_cost, transpose_cost)\n    return dp_table[len(source), len(target)]"}
{"task_id": 51, "completion_id": 5, "solution": "import numpy as np\ndef OSA(source, target):\n    n = len(source)\n    m = len(target)\n    D = np.zeros((n + 1, m + 1))\n    for i in range(n + 1):\n        D[i, 0] = i\n    for j in range(m + 1):\n        D[0, j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            D[i, j] = min(D[i - 1, j] + 1, D[i, j - 1] + 1, D[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                D[i, j] = min(D[i, j], D[i - 2, j - 2] + 1)\n    return int(D[n, m])"}
{"task_id": 51, "completion_id": 6, "solution": "import numpy as np\ndef min_cost(ins, dels, subs, trans):\n    \"\"\" Helper function to get min of four values \"\"\"\n    return min(ins, dels, subs, trans)\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    cost = np.zeros((m + 1, n + 1))\n    for i in range(m + 1):\n        cost[i][0] = i\n    for j in range(n + 1):\n        cost[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            ins_cost = cost[i][j - 1] + 1\n            del_cost = cost[i - 1][j] + 1\n            sub_cost = cost[i - 1][j - 1] + (source[i - 1] != target[j - 1])\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                trans_cost = cost[i - 2][j - 2] + 1\n            else:\n                trans_cost = float('inf')\n            cost[i][j] = min_cost(ins_cost, del_cost, sub_cost, trans_cost)\n    return int(cost[m][n])"}
{"task_id": 51, "completion_id": 7, "solution": "import numpy as np\ndef OSA(s1, s2):\n    len_s1 = len(s1)\n    len_s2 = len(s2)\n    dist = np.zeros((len_s1 + 1, len_s2 + 1), dtype=int)\n    for i in range(len_s1 + 1):\n        dist[i, 0] = i\n    for j in range(len_s2 + 1):\n        dist[0, j] = j\n    for i in range(1, len_s1 + 1):\n        for j in range(1, len_s2 + 1):\n            subst_cost = dist[i - 1, j - 1] + (s1[i - 1] != s2[j - 1])\n            insert_cost = dist[i, j - 1] + 1\n            delete_cost = dist[i - 1, j] + 1\n            transp_cost = float('inf')\n            if i > 1 and j > 1:\n                if s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n                    transp_cost = dist[i - 2, j - 2] + 1\n            dist[i, j] = min(subst_cost, insert_cost, delete_cost, transp_cost)\n    return dist[len_s1, len_s2]"}
{"task_id": 51, "completion_id": 8, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    matrix = np.zeros((len(source) + 1, len(target) + 1))\n    for i in range(len(source) + 1):\n        matrix[i, 0] = i\n    for j in range(len(target) + 1):\n        matrix[0, j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            matrix[i, j] = min(matrix[i - 1, j] + 1, matrix[i, j - 1] + 1, matrix[i - 1, j - 1] + cost, matrix[i - 2, j - 2] + 1)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                matrix[i, j] = min(matrix[i, j], matrix[i - 2, j - 2] + 1)\n    return matrix[len(source), len(target)]"}
{"task_id": 51, "completion_id": 9, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    matrix = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n    for i in range(len(source) + 1):\n        matrix[i, 0] = i\n    for j in range(len(target) + 1):\n        matrix[0, j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            cost_substitution = 0 if source[i - 1] == target[j - 1] else 1\n            matrix[i, j] = min(matrix[i - 1, j] + 1, matrix[i, j - 1] + 1, matrix[i - 1, j - 1] + cost_substitution)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                matrix[i, j] = min(matrix[i, j], matrix[i - 2, j - 2] + 1)\n    return matrix[len(source), len(target)]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n    return round(recall, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\ny_pred = [1, 0, 1, 0, 1, 1, 0, 1, 0, 0]"}
{"task_id": 52, "completion_id": 1, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_val = TP / (TP + FN)\n    recall_val = round(recall_val, 3)\n    return recall_val\ny_true = [1, 0, 0, 1, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 1, 0, 1, 0, 1, 0, 1, 0, 0]"}
{"task_id": 52, "completion_id": 2, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    recall_value = TP / (TP + FN) if TP + FN > 0 else 0.0\n    return round(recall_value, 3)\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1]"}
{"task_id": 52, "completion_id": 3, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        print('Denominator is zero. Recall is set to 0.0')\n        return 0.0\n    recall = TP / (TP + FN)\n    return round(recall, 3)"}
{"task_id": 52, "completion_id": 4, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    pos_ind = 1\n    neg_ind = 0\n    TP = np.sum((y_true == pos_ind) & (y_pred == pos_ind))\n    FN = np.sum((y_true == pos_ind) & (y_pred == neg_ind))\n    denominator = TP + FN\n    recall = TP / denominator if denominator != 0 else 0.0\n    return round(recall, 3)\ny_true = [0, 1, 0, 1, 0, 1]\ny_pred = [0, 1, 1, 1, 0, 1]"}
{"task_id": 52, "completion_id": 5, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    fn = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    recall_value = tp / (tp + fn) if tp + fn != 0 else 0.0\n    recall_value = round(recall_value, 3)\n    return recall_value\ny_true = [1, 1, 1, 1, 0, 0, 0, 0]\ny_pred = [0, 0, 1, 1, 0, 0, 0, 1]"}
{"task_id": 52, "completion_id": 6, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(TP / denominator, 3)\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 1, 1, 0, 1, 0, 1, 0]"}
{"task_id": 52, "completion_id": 7, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    recall_metric = TP / (TP + FN) if TP > 0 else 0.0\n    return round(recall_metric, 3)"}
{"task_id": 52, "completion_id": 8, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    if TP + FN == 0:\n        return 0.0\n    recall = TP / (TP + FN)\n    return round(recall, 3)"}
{"task_id": 52, "completion_id": 9, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    (TP, FP, FN, TN) = (0, 0, 0, 0)\n    for i in range(len(y_true)):\n        if y_true[i] == y_pred[i] == 1:\n            TP += 1\n        if y_pred[i] == 1 and y_true[i] != y_pred[i]:\n            FP += 1\n        if y_true[i] == y_pred[i] == 0:\n            TN += 1\n        if y_pred[i] == 0 and y_true[i] != y_pred[i]:\n            FN += 1\n    recall = TP / (TP + FN) if TP + FN != 0 else 0.0\n    return round(recall, 3)\ny_true = [0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\ny_pred = [0, 1, 1, 1, 1, 0, 0, 1, 1, 0]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T)\n    scores_normalized = scores / np.sqrt(K.shape[1])\n    softmax_scores = np.exp(scores_normalized) / np.sum(np.exp(scores_normalized), axis=1, keepdims=True)\n    attention = np.dot(softmax_scores, V)\n    attention_rounded = np.round(attention, 4)\n    return attention_rounded.tolist()"}
{"task_id": 53, "completion_id": 1, "solution": "import numpy as np\nimport math\ndef self_attention(X, W_q, W_k, W_v):\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    score_matrix = Q @ K.T\n    d_k = W_k.shape[0]\n    scaled_score_matrix = score_matrix / math.sqrt(d_k)\n    attention_matrix = np.exp(scaled_score_matrix - np.max(scaled_score_matrix, axis=-1, keepdims=True))\n    attention_matrix = attention_matrix / np.sum(attention_matrix, axis=-1, keepdims=True)\n    output = attention_matrix @ V\n    return output.round(4).tolist()"}
{"task_id": 53, "completion_id": 2, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(W_q, X)\n    K = np.dot(W_k, X)\n    V = np.dot(W_v, X)\n    scores = np.dot(Q.T, K)\n    A = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(V, A)\n    return np.around(output, decimals=4).tolist()"}
{"task_id": 53, "completion_id": 3, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    scores = np.matmul(Q, K.T)\n    alpha = np.round(scores / np.sqrt(K.shape[1]), 4)\n    o = np.round(np.matmul(alpha, V), 4)\n    return o.tolist()"}
{"task_id": 53, "completion_id": 4, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    Scores = np.matmul(Q, K.T) / np.sqrt(Q.shape[-1])\n    Scores = np.exp(Scores - np.max(Scores, axis=-1, keepdims=True))\n    Scores = Scores / np.sum(Scores, axis=-1, keepdims=True)\n    Output = np.matmul(Scores, V)\n    Output = np.round(Output, 4).tolist()\n    return Output"}
{"task_id": 53, "completion_id": 5, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    X = np.array(X)\n    Q = np.dot(W_q, X)\n    K = np.dot(W_k, X)\n    V = np.dot(W_v, X)\n    attn_scores = np.dot(Q.T, K)\n    norm_attn_scores = attn_scores / np.sqrt(X.shape[0])\n    softmax_attn_scores = np.exp(norm_attn_scores) / np.sum(np.exp(norm_attn_scores), axis=-1, keepdims=True)\n    output = np.dot(softmax_attn_scores, V.T)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 53, "completion_id": 6, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(W_q, X)\n    K = np.dot(W_k, X)\n    V = np.dot(W_v, X)\n    dk = np.sqrt(W_q.shape[-1])\n    scores = np.dot(Q.T, K) / dk\n    softmax = np.exp(scores) / np.sum(np.exp(scores), axis=-1)\n    self_attn_output = np.dot(softmax, V.T)\n    self_attn_output = np.round(self_attn_output, 4)\n    return self_attn_output.tolist()\nX = np.random.rand(5, 4)\nW_q = np.random.rand(3, 5)\nW_k = np.random.rand(3, 5)\nW_v = np.random.rand(3, 5)"}
{"task_id": 53, "completion_id": 7, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T)\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(weights, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 53, "completion_id": 8, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T)\n    d_k = W_k.shape[0]\n    scores = scores / np.sqrt(d_k)\n    softmax = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(softmax, V)\n    output = output.round(4)\n    output = output.tolist()\n    return output\nX = np.array([[0.5, 0.2, 0.3], [0.6, 0.8, 0.1], [0.4, 0.9, 0.5]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\nW_v = np.array([[0.7, 0.8, 0.9], [0.4, 0.5, 0.6], [0.1, 0.2, 0.3]])"}
{"task_id": 53, "completion_id": 9, "solution": "import numpy as np\nimport math\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(W_q, X)\n    K = np.dot(W_k, X)\n    V = np.dot(W_v, X)\n    d_k = Q.shape[-1]\n    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    self_attention_output = np.dot(attention_weights, V)\n    self_attention_output = np.round(self_attention_output, 4)\n    return self_attention_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        hidden_state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, hidden_state) + b)\n    return np.around(hidden_state, 4).tolist()"}
{"task_id": 54, "completion_id": 1, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    assert input_sequence.shape[1] == Wx.shape[0]\n    assert initial_hidden_state.shape[0] == Wh.shape[0] == Wh.shape[1] == Wx.shape[1] == b.shape[0]\n    hidden_state = initial_hidden_state\n    for x in input_sequence:\n        hidden_state = np.tanh(np.dot(Wx, x) + np.dot(Wh, hidden_state) + b)\n    return hidden_state.round(4).tolist()"}
{"task_id": 54, "completion_id": 2, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    (T, n) = input_sequence.shape\n    hidden_states = np.zeros((T + 1, n))\n    hidden_states[0] = initial_hidden_state\n    for t in range(T):\n        hidden_states[t + 1] = np.tanh(np.dot(Wx, input_sequence[t]) + np.dot(Wh, hidden_states[t]) + b)\n    return np.around(hidden_states[-1], 4).tolist()"}
{"task_id": 54, "completion_id": 3, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    sequence_length = len(input_sequence)\n    current_hidden_state = initial_hidden_state\n    for i in range(sequence_length):\n        current_input = input_sequence[i]\n        current_hidden_state = np.tanh(np.dot(Wx, current_input) + np.dot(Wh, current_hidden_state) + b)\n    final_hidden_state = np.around(current_hidden_state, decimals=4)\n    return final_hidden_state.tolist()"}
{"task_id": 54, "completion_id": 4, "solution": "import numpy as np\nfrom typing import List\ndef rnn_forward(input_sequence: List[List[float]], initial_hidden_state: List[float], Wx: List[List[float]], Wh: List[List[float]], b: List[float]) -> List[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    final_hidden_state = initial_hidden_state\n    for x in input_sequence:\n        input_current_state = np.dot(Wx, x)\n        input_current_state += np.dot(Wh, final_hidden_state)\n        input_current_state += b\n        final_hidden_state = np.tanh(input_current_state)\n    final_hidden_state = np.round(final_hidden_state, 4).tolist()\n    return final_hidden_state"}
{"task_id": 54, "completion_id": 5, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    h = np.around(h, 4)\n    return h.tolist()"}
{"task_id": 54, "completion_id": 6, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    state = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        state = np.tanh(np.dot(Wx, input_vector) + np.dot(Wh, state) + b)\n    return np.around(state.tolist(), 4)"}
{"task_id": 54, "completion_id": 7, "solution": "import numpy as np\ndef rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b) -> list:\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    accumulator = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector)\n        Z = np.dot(Wx.T, input_vector) + np.dot(Wh.T, accumulator) + b\n        accumulator = np.tanh(Z)\n    return list(np.around(accumulator, 4))\ninput_sequence = [[1, 2], [3, 4], [5, 6]]\ninitial_hidden_state = [0, 0]\nWx = [[1, 0], [0, 1]]\nWh = [[1, 0], [0, 1]]\nb = [0, 0]"}
{"task_id": 54, "completion_id": 8, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = np.array(initial_hidden_state)\n    for input_vector in input_sequence:\n        input_vector = np.array(input_vector).reshape(-1, 1)\n        hidden_state = np.tanh(np.dot(Wx.T, input_vector) + np.dot(Wh.T, hidden_state) + b)\n    return hidden_state.flatten().round(4).tolist()"}
{"task_id": 54, "completion_id": 9, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for x in input_sequence:\n        x = np.array(x)\n        hidden_state = np.tanh(Wx.T.dot(x) + Wh.T.dot(hidden_state) + b)\n    return np.around(hidden_state, decimals=4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = [np.dot(translation_matrix, np.append(point, 1)) for point in points]\n    return [point[:2].tolist() for point in translated_points]"}
{"task_id": 55, "completion_id": 1, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    T = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homogeneous = np.array([point + [1] for point in points])\n    points_translated = np.array([T @ point for point in points_homogeneous])\n    points_cartesian = np.array([point[:2] for point in points_translated])\n    return points_cartesian.tolist()\npoints = [[0, 0], [1, 0], [0, 1]]\ntx = 2\nty = 3"}
{"task_id": 55, "completion_id": 2, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    T = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_h = np.hstack((np.array(points), np.ones((len(points), 1))))\n    points_t = np.dot(T, points_h.T).T\n    return points_t[:, :2].tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 3\nty = -1"}
{"task_id": 55, "completion_id": 3, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translate_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = []\n    for point in points:\n        homogeneous_point = np.array([point[0], point[1], 1])\n        translated_homogeneous_point = np.dot(translate_matrix, homogeneous_point)\n        translated_point = [translated_homogeneous_point[0], translated_homogeneous_point[1]]\n        translated_points.append(translated_point)\n    return translated_points"}
{"task_id": 55, "completion_id": 4, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = []\n    for point in points:\n        hom_point = np.append(point, 1)\n        translated_point = np.dot(translation_matrix, hom_point)\n        translated_points.append(translated_point[:2].tolist())\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 2\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 5, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed_points = []\n    for point in points:\n        point = np.array([point[0], point[1], 1])\n        transformed_point = np.dot(translation_matrix, point)\n        transformed_point = transformed_point[[0, 1]]\n        transformed_points.append(transformed_point.tolist())\n    return transformed_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = -1"}
{"task_id": 55, "completion_id": 6, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    hom_points = np.hstack([np.array(points), np.ones((len(points), 1))])\n    translated_hom_points = np.dot(hom_points, translation_matrix.T)\n    translated_points = (translated_hom_points[:, :2] / translated_hom_points[:, 2:]).tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 1"}
{"task_id": 55, "completion_id": 7, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    This function applies a 2D translation matrix to a set of points.\n    :param points: A list of tuples, where each tuple represents a point in 2D space\n    :param tx: Translation distance in the x-direction\n    :param ty: Translation distance in the y-direction\n    :return: A new list of points after applying the translation matrix\n    \"\"\"\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = []\n    for point in points:\n        homogeneous_coords = np.append(point, 1)\n        translated_point = np.dot(translation_matrix, homogeneous_coords)\n        translated_points.append(translated_point[:2])\n    return np.array(translated_points).tolist()"}
{"task_id": 55, "completion_id": 8, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points).reshape(-1, 2)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_array = np.hstack((points_array, np.ones((len(points_array), 1))))\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    return translated_points[:, :2].tolist()"}
{"task_id": 55, "completion_id": 9, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Function to translate a 2D set of points (i.e., [x, y] coordinates)\n    by specified distances in the x and y directions.\n\n    The function uses a 2x3 translation matrix to perform the translation.\n\n    Args:\n    points: A list of [x, y] coordinates.\n    tx: The translation distance in the x direction.\n    ty: The translation distance in the y direction.\n\n    Returns:\n    A new list of points (i.e., [x, y] coordinates) after applying the translation.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n    translated_array = np.dot(translation_matrix, points_array.T).T\n    translated_points = translated_array.tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = -1\nty = 2"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = np.log(sigma_q) - np.log(sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    kl_divergence = term1 + term2 + term3\n    return kl_divergence"}
{"task_id": 56, "completion_id": 1, "solution": "import numpy as np\nimport scipy.stats as sp\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    k = len(mu_p)\n    det_sigma_p = np.linalg.det(sigma_p)\n    det_sigma_q = np.linalg.det(sigma_q)\n    sigma_q_inv = np.linalg.inv(sigma_q)\n    mu_term = (mu_q - mu_p).T.dot(sigma_q_inv).dot(mu_q - mu_p)\n    kl_div = 0.5 * (np.trace(sigma_q_inv.dot(sigma_p)) + mu_term - k + np.log(det_sigma_p / det_sigma_q))\n    return kl_div"}
{"task_id": 56, "completion_id": 2, "solution": "import numpy as np\nfrom scipy.special import kl_divergence\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    (log_sp, log_sq) = (np.log(sigma_p), np.log(sigma_q))\n    kl_div = 0.5 * (log_sq - log_sp - np.exp(log_sq - log_sp) + 1 + (mu_q - mu_p) ** 2 / np.exp(log_sq))\n    return kl_div"}
{"task_id": 56, "completion_id": 3, "solution": "import numpy as np\nimport math\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    log_det_sigma_p = np.log(abs(sigma_p))\n    log_det_sigma_q = np.log(abs(sigma_q))\n    trace_sigma_q_sigma_p = np.square(sigma_p) + np.square(mu_p - mu_q) + np.square(sigma_q)\n    quad_form = np.square(mu_p - mu_q) / np.square(sigma_q)\n    kl_div = (log_det_sigma_q - log_det_sigma_p + trace_sigma_q_sigma_p - len(mu_p) - quad_form) / 2.0\n    return np.sum(kl_div)"}
{"task_id": 56, "completion_id": 4, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 56, "completion_id": 5, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between P and Q for assumptions of a normal distribution. P ~ N(mu_P, sigma_P^2) \n    and Q ~ N(mu_Q, sigma_Q^2).\n    >>> \"{:.2f}\".format(kl_divergence_normal(0.0, 1.0, 0.0, 1.0))\n    '0.00'\n    >>> \"{:.2f}\".format(kl_divergence_normal(0.0, 1.0, 0.0, 2.0))\n    '0.50'\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 56, "completion_id": 6, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    This function calculates the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution\n    sigma_p (float): Standard deviation of the first normal distribution\n    mu_q (float): Mean of the second normal distribution\n    sigma_q (float): Standard deviation of the second normal distribution\n\n    Returns:\n    float: KL divergence between the two normal distributions\n    \"\"\"\n    kl = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl"}
{"task_id": 56, "completion_id": 7, "solution": "import numpy as np\nfrom scipy.stats import norm\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculates the KL divergence between two normal distributions.\n\n    Parameters:\n        mu_p (float): Mean of the first normal distribution.\n        sigma_p (float): Standard deviation of the first normal distribution.\n        mu_q (float): Mean of the second normal distribution.\n        sigma_q (float): Standard deviation of the second normal distribution.\n\n    Returns:\n        kl_divergence (float): The Kullback-Leibler divergence between the two normal distributions.\n    \"\"\"\n    dist_p = norm(mu_p, sigma_p)\n    dist_q = norm(mu_q, sigma_q)\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence"}
{"task_id": 56, "completion_id": 8, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    return 0.5 * ((mu_p - mu_q) ** 2 / sigma_q ** 2 + np.log(sigma_q ** 2 / sigma_p ** 2) - 1)"}
{"task_id": 56, "completion_id": 9, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Function to calculate the Kullback-Leibler (KL) divergence\n    between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first distribution.\n    sigma_p (float): Standard deviation of the first distribution.\n    mu_q (float): Mean of the second distribution.\n    sigma_q (float): Standard deviation of the second distribution.\n\n    Returns:\n    float: The KL divergence between the two distributions.\n    \"\"\"\n    return 0.5 * (((mu_p - mu_q) ** 2 + sigma_q ** 2) / sigma_p ** 2 - 1 + 2 * np.log(sigma_p) - 2 * np.log(sigma_q))"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    L = np.tril(A)\n    U = np.triu(A, 1)\n    b = b.reshape(-1, 1)\n    x = x_ini.reshape(-1, 1) if x_ini is not None else np.zeros_like(b)\n    for _ in range(n):\n        x = np.linalg.inv(L).dot(b - U.dot(x))\n    return x.flatten().round(4).tolist()"}
{"task_id": 57, "completion_id": 1, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    n = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(n)\n    else:\n        x = x_ini\n    for _ in range(n):\n        for i in range(n):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    x = np.round(x, 4)\n    if len(x) == 1:\n        return x[0]\n    else:\n        return x.tolist()"}
{"task_id": 57, "completion_id": 2, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"Gauss-Seidel method to solve linear systems.\"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for i in range(n):\n        for j in range(N):\n            x[j] = (b[j] - np.dot(A[j, :j], x[:j]) - np.dot(A[j, j + 1:], x[j + 1:])) / A[j, j]\n    return x.round(4).tolist()\nA = [[4, -1, 0, -1, 0], [-1, 4, -1, 0, -1], [0, -1, 4, -1, 0], [-1, 0, -1, 4, -1], [0, -1, 0, -1, 4]]\nb = [0, 0, 0, 0, 0]\nn = 20"}
{"task_id": 57, "completion_id": 3, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    L = np.tril(A)\n    U = A - L\n    D = np.diag(np.diag(A))\n    if np.all(np.abs(np.diag(A)) > np.sum(np.abs(A), axis=1) - np.abs(np.diagonal(A))) == False:\n        print('Matrix A is not diagonally dominant. Consider a different matrix')\n        return None\n    if np.all(np.abs(np.diag(A)) <= np.sum(np.abs(U), axis=1)) == False:\n        print('Matrix A is not strictly diagonally dominant. Consider a different matrix')\n        return None\n    D_inv = np.linalg.inv(D)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini\n    for _ in range(n):\n        x = np.round(np.dot(D_inv, np.dot(-(L + U), x) + b), 4)\n    return x.tolist()"}
{"task_id": 57, "completion_id": 4, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b dimensions do not fit'\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.float)\n    else:\n        x = x_ini\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return x.round(4).tolist()"}
{"task_id": 57, "completion_id": 5, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    L = np.tril(A)\n    U = np.triu(A, 1)\n    if x_ini is None:\n        x_ini = np.zeros_like(b)\n    for _ in range(n):\n        x_ini = np.dot(np.linalg.inv(L), b - np.dot(U, x_ini))\n    return np.round(x_ini, 4).tolist()"}
{"task_id": 57, "completion_id": 6, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini.copy()\n    for _ in range(n):\n        for i in range(len(A)):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 10"}
{"task_id": 57, "completion_id": 7, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    assert A.shape[0] == A.shape[1] == b.shape[0] == x.shape[0]\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()\nA = [[4, 3, 0], [3, 4, -1], [0, -1, 4]]\nb = [24, 30, -24]\nx_ini = [0, 0, 0]"}
{"task_id": 57, "completion_id": 8, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    if x_ini is None:\n        x_ini = np.zeros_like(b)\n    x = np.zeros_like(b)\n    for _ in range(n):\n        for i in range(b.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x_ini[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n        x_ini = x.copy()\n    return x.round(4).tolist()"}
{"task_id": 57, "completion_id": 9, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A)\n    b = np.array(b)\n    assert A.shape[0] == A.shape[1], 'Matrix A should be a square matrix'\n    assert len(b) == len(A), 'Size of b should match the number of rows in A'\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.float)\n    else:\n        x = np.array(x_ini, dtype=np.float)\n    for _ in range(n):\n        for i in range(len(b)):\n            x[i] = b[i]\n            for j in range(len(b)):\n                if i != j:\n                    x[i] -= A[i][j] * x[j]\n            x[i] /= A[i][i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = [[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]]\nb = [6.0, 25.0, -11.0, 15.0]\nn = 100"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef gaussian_elimination(A, b):\n    augmented_matrix = np.concatenate((A, np.array([b]).T), axis=1)\n    n = len(b)\n    for i in range(n):\n        max_row = max(range(i, n), key=lambda j: abs(augmented_matrix[j][i]))\n        (augmented_matrix[i], augmented_matrix[max_row]) = (augmented_matrix[max_row], augmented_matrix[i])\n        augmented_matrix[i] = augmented_matrix[i] / augmented_matrix[i][i]\n        for j in range(i + 1, n):\n            augmented_matrix[j] = augmented_matrix[j] - augmented_matrix[i] * augmented_matrix[j][i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = augmented_matrix[i][-1] - sum((augmented_matrix[i][j] * x[j] for j in range(i + 1, n)))\n    return np.round(x.tolist(), 4)\nA = np.array([[2, 1, -1], [-1, 1, 2], [3, -1, 1]])\nb = np.array([2, 5, 3])\nx = gaussian_elimination(A, b)"}
{"task_id": 58, "completion_id": 1, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    M = np.concatenate((A, np.array([b]).T), axis=1).astype(float)\n    n = len(b)\n    for i in range(n):\n        max_idx = np.argmax(np.abs(M[i:, i])) + i\n        if i != max_idx:\n            M[[i, max_idx]] = M[[max_idx, i]]\n        M[i] = M[i] / M[i, i]\n        for j in range(i + 1, n):\n            factor = M[j, i] / M[i, i]\n            M[j] = M[j] - factor * M[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = M[i, -1] - np.dot(M[i, :-1], x)\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 2, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(A)\n    for k in range(n - 1):\n        max_index = np.argmax(np.abs(A[k:n, k])) + k\n        if max_index != k:\n            A[[k, max_index]] = A[[max_index, k]]\n            b[[k, max_index]] = b[[max_index, k]]\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] = A[i, k:] - factor * A[k, k:]\n            b[i] = b[i] - factor * b[k]\n    x = np.zeros(n)\n    x[n - 1] = b[n - 1] / A[n - 1, n - 1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:n], x[i + 1:n])) / A[i, i]\n    return list(np.round(x, 4))"}
{"task_id": 58, "completion_id": 3, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    aug_matrix = np.concatenate((A, b.reshape(-1, 1)), axis=1)\n    n = aug_matrix.shape[0]\n    for k in range(n):\n        max_index = np.abs(aug_matrix[k:, k]).argmax() + k\n        if aug_matrix[max_index, k] == 0:\n            print('Matrix is singular!')\n            return\n        if max_index != k:\n            aug_matrix[[k, max_index]] = aug_matrix[[max_index, k]]\n        for row in range(k + 1, n):\n            f = aug_matrix[row, k] / aug_matrix[k, k]\n            aug_matrix[row, k:] = aug_matrix[row, k:] - aug_matrix[k, k:] * f\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (aug_matrix[i, -1] - np.dot(aug_matrix[i, i + 1:-1], x[i + 1:])) / aug_matrix[i, i]\n    return np.around(x.tolist(), 4)"}
{"task_id": 58, "completion_id": 4, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    Ab = np.concatenate((A, b.reshape(-1, 1)), axis=1).astype(np.float64)\n    n = len(Ab)\n    for i in range(n):\n        pivot_row = i + np.argmax(np.abs(Ab[i:, i]))\n        if pivot_row != i:\n            Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        if np.abs(Ab[i, i]) < 1e-10:\n            return None\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j] -= factor * Ab[i]\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 5, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(A)\n    for k in range(n - 1):\n        max_idx = abs(A[k:, k]).argmax() + k\n        if A[max_idx, k] == 0:\n            raise ValueError('Matrix is singular')\n        if max_idx != k:\n            A[[k, max_idx]] = A[[max_idx, k]]\n            b[[k, max_idx]] = b[[max_idx, k]]\n        for row in range(k + 1, n):\n            multiplier = A[row, k] / A[k, k]\n            A[row, k:] = A[row, k:] - multiplier * A[k, k:]\n            b[row] = b[row] - multiplier * b[k]\n    x = np.zeros(n)\n    for k in range(n - 1, -1, -1):\n        x[k] = (b[k] - np.dot(A[k, k + 1:], x[k + 1:])) / A[k, k]\n    return x.round(4).tolist()"}
{"task_id": 58, "completion_id": 6, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    Ab = np.concatenate((A, b.reshape(-1, 1)), axis=1)\n    n = len(Ab)\n    for i in range(n):\n        max_row = i\n        for j in range(i + 1, n):\n            if abs(Ab[j, i]) > abs(Ab[max_row, i]):\n                max_row = j\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        Ab[i] = Ab[i] / Ab[i, i]\n        for j in range(i + 1, n):\n            Ab[j] = Ab[j] - Ab[j, i] * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = Ab[i, -1] / Ab[i, i]\n        for j in range(i - 1, -1, -1):\n            Ab[j, -1] = Ab[j, -1] - Ab[j, i] * x[i]\n    return x.round(4).tolist()"}
{"task_id": 58, "completion_id": 7, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(A)\n    for i in range(n):\n        max_element = abs(A[i][i])\n        max_row = i\n        for k in range(i + 1, n):\n            if abs(A[k][i]) > max_element:\n                max_element = abs(A[k][i])\n                max_row = k\n        (A[i], A[max_row]) = (A[max_row], np.copy(A[i]))\n        (b[i], b[max_row]) = (b[max_row], np.copy(b[i]))\n        for k in range(i + 1, n):\n            c = A[k][i] / A[i][i]\n            for j in range(i, n):\n                A[k][j] -= c * A[i][j]\n            b[k] -= c * b[i]\n    x = [0 for i in range(n)]\n    for i in range(n - 1, -1, -1):\n        x[i] = b[i] / A[i][i]\n        for k in range(i - 1, -1, -1):\n            b[k] -= A[k][i] * x[i]\n    return np.round(x, 4).tolist()\nA = np.array([[2.0, 1.0, -1.0], [-3.0, -1.0, 2.0], [-2.0, 1.0, 2.0]])\nb = np.array([8.0, -11.0, -3.0])"}
{"task_id": 58, "completion_id": 8, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    M = np.concatenate((A, b.reshape((-1, 1))), axis=1)\n    n = len(M)\n    for i in range(n):\n        max_element_idx = abs(M[i:, i]).argmax() + i\n        if i != max_element_idx:\n            M[[i, max_element_idx]] = M[[max_element_idx, i]]\n        for j in range(i + 1, n):\n            factor = M[j, i] / M[i, i]\n            M[j] -= factor * M[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (M[i, -1] - np.dot(M[i, :-1], x)) / M[i, i]\n    return x.round(4).tolist()"}
{"task_id": 58, "completion_id": 9, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(A)\n    for i in range(n):\n        max_val = abs(A[i][i])\n        max_row = i\n        for k in range(i + 1, n):\n            if abs(A[k][i]) > max_val:\n                max_val = abs(A[k][i])\n                max_row = k\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for k in range(i + 1, n):\n            c = -A[k][i] / A[i][i]\n            A[k] = A[k] + c * A[i]\n            b[k] = b[k] + c * b[i]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        prev_hidden_state = initial_hidden_state\n        prev_cell_state = initial_cell_state\n        for i in range(len(x)):\n            input_data = np.column_stack((prev_hidden_state, x[i]))\n            forget_gate = sigmoid(np.dot(self.Wf, input_data) + self.bf)\n            input_gate = sigmoid(np.dot(self.Wi, input_data) + self.bi)\n            candidate_cell_state = np.tanh(np.dot(self.Wc, input_data) + self.bc)\n            output_gate = sigmoid(np.dot(self.Wo, input_data) + self.bo)\n            new_cell_state = forget_gate * prev_cell_state + input_gate * candidate_cell_state\n            new_hidden_state = output_gate * np.tanh(new_cell_state)\n            hidden_states.append(new_hidden_state)\n            prev_hidden_state = new_hidden_state\n            prev_cell_state = new_cell_state\n        return ([np.around(hidden_state, 4).tolist() for hidden_state in hidden_states], np.around(new_hidden_state, 4).tolist(), np.around(new_cell_state, 4).tolist())"}
{"task_id": 59, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"\n    Sigmoid function.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    \"\"\"\n    Hyperbolic tangent function.\n    \"\"\"\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[1]\n        hidden_states = np.zeros((sequence_length, self.hidden_size))\n        cell_states = np.zeros((sequence_length, self.hidden_size))\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(sequence_length):\n            input_concat = np.concatenate((x[:, t], hidden_state), axis=0).reshape(-1, 1)\n            forget_gate = sigmoid(np.dot(self.Wf, input_concat) + self.bf)\n            input_gate = sigmoid(np.dot(self.Wi, input_concat) + self.bi)\n            candidate_cell_state = tanh(np.dot(self.Wc, input_concat) + self.bc)\n            output_gate = sigmoid(np.dot(self.Wo, input_concat) + self.bo)\n            cell_state = np.multiply(forget_gate, cell_state) + np.multiply(input_gate, candidate_cell_state)\n            hidden_state = np.multiply(output_gate, tanh(cell_state))\n            hidden_states[t] = hidden_state.reshape(-1)\n            cell_states[t] = cell_state.reshape(-1)\n        return (hidden_states.tolist(), hidden_state.tolist(), cell_state.tolist())"}
{"task_id": 59, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[0]\n        hidden_states = np.zeros((sequence_length, self.hidden_size))\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for i in range(sequence_length):\n            z = np.hstack((hidden_state, x[i]))\n            ft = sigmoid(self.Wf.dot(z) + self.bf)\n            it = sigmoid(self.Wi.dot(z) + self.bi)\n            candidate_cell_state = tanh(self.Wc.dot(z) + self.bc)\n            ot = sigmoid(self.Wo.dot(z) + self.bo)\n            cell_state = ft * cell_state + it * candidate_cell_state\n            hidden_state = ot * tanh(cell_state)\n            hidden_states[i, :] = hidden_state.reshape(self.hidden_size)\n        return (hidden_states.tolist(), hidden_state.tolist(), cell_state.tolist())"}
{"task_id": 59, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[0]\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        hidden_state_sequence = np.zeros((sequence_length, self.hidden_size))\n        for t in range(sequence_length):\n            x_t = x[t].reshape(-1, 1)\n            combined_states_t = np.vstack((hidden_state, x_t))\n            ft = sigmoid(np.dot(self.Wf, combined_states_t) + self.bf)\n            it = sigmoid(np.dot(self.Wi, combined_states_t) + self.bi)\n            Ct_tilde = tanh(np.dot(self.Wc, combined_states_t) + self.bc)\n            cell_state = np.add(np.multiply(ft, cell_state), np.multiply(it, Ct_tilde))\n            ot = sigmoid(np.dot(self.Wo, combined_states_t) + self.bo)\n            hidden_state = np.multiply(ot, tanh(cell_state))\n            hidden_state_sequence[t] = hidden_state.flatten()\n        return (hidden_state_sequence.round(4).tolist(), hidden_state.tolist(), cell_state.tolist())"}
{"task_id": 59, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        sequence_length = x.shape[0]\n        hidden_states = np.zeros((sequence_length, self.hidden_size))\n        cell_states = np.zeros((sequence_length, self.hidden_size))\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(sequence_length):\n            x_t = x[t]\n            xh_t = np.concatenate((x_t, h_t), axis=0).reshape(-1, 1)\n            ft = sigmoid(self.Wf @ xh_t + self.bf)\n            it = sigmoid(self.Wi @ xh_t + self.bi)\n            ct_cand = tanh(self.Wc @ xh_t + self.bc)\n            ot = sigmoid(self.Wo @ xh_t + self.bo)\n            c_t = np.multiply(ft, c_t) + np.multiply(it, ct_cand)\n            h_t = np.multiply(ot, tanh(c_t))\n            hidden_states[t] = h_t.reshape(-1)\n            cell_states[t] = c_t.reshape(-1)\n        return (hidden_states.tolist(), h_t.reshape(-1).tolist(), c_t.reshape(-1).tolist())"}
{"task_id": 59, "completion_id": 5, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        sequence_length = len(x)\n        hidden_states = np.zeros((sequence_length, self.hidden_size))\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(sequence_length):\n            input_data = np.concatenate((x[t], hidden_state), axis=0)\n            input_data = np.reshape(input_data, (-1, 1))\n            ft = self.sigmoid(self.Wf @ input_data + self.bf)\n            it = self.sigmoid(self.Wi @ input_data + self.bi)\n            Ct_cand = self.tanh(self.Wc @ input_data + self.bc)\n            Ot = self.sigmoid(self.Wo @ input_data + self.bo)\n            cell_state = ft * cell_state + it * Ct_cand\n            hidden_state = Ot * self.tanh(cell_state)\n            hidden_states[t] = hidden_state.flatten()\n        return (hidden_states.tolist(), hidden_state.flatten().tolist(), cell_state.flatten().tolist())"}
{"task_id": 59, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        seq_len = x.shape[0]\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        hidden_states = []\n        for i in range(seq_len):\n            concat = np.concatenate((hidden_state, x[i]), axis=0).reshape(1, -1).T\n            forget_gate = sigmoid(np.dot(self.Wf, concat) + self.bf)\n            input_gate = sigmoid(np.dot(self.Wi, concat) + self.bi)\n            output_gate = sigmoid(np.dot(self.Wo, concat) + self.bo)\n            candidate_cell_state = tanh(np.dot(self.Wc, concat) + self.bc)\n            cell_state = np.multiply(forget_gate, cell_state) + np.multiply(input_gate, candidate_cell_state)\n            hidden_state = np.multiply(output_gate, tanh(cell_state))\n            hidden_states.append(hidden_state)\n        return [np.around(hidden_state.reshape(-1).tolist(), 4), np.around(cell_state.reshape(-1).tolist(), 4), [np.around(h.reshape(-1).tolist(), 4) for h in hidden_states]]"}
{"task_id": 59, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        hidden_states = []\n        prev_hidden_state = initial_hidden_state\n        prev_cell_state = initial_cell_state\n        sequence_length = len(x)\n        for t in range(sequence_length):\n            input_data = np.expand_dims(np.concatenate((x[t], prev_hidden_state)), axis=1)\n            forget_gate = sigmoid(np.dot(self.Wf, input_data) + self.bf)\n            input_gate = sigmoid(np.dot(self.Wi, input_data) + self.bi)\n            candidate_cell_state = tanh(np.dot(self.Wc, input_data) + self.bc)\n            output_gate = sigmoid(np.dot(self.Wo, input_data) + self.bo)\n            cell_state = np.add(np.multiply(forget_gate, prev_cell_state), np.multiply(input_gate, candidate_cell_state))\n            hidden_state = np.multiply(output_gate, tanh(cell_state))\n            hidden_states.append(hidden_state)\n            prev_hidden_state = hidden_state\n            prev_cell_state = cell_state\n        hidden_states = np.array(hidden_states)\n        final_hidden_state = hidden_states[-1]\n        final_cell_state = prev_cell_state\n        return (hidden_states.tolist(), final_hidden_state.tolist(), final_cell_state.tolist())"}
{"task_id": 59, "completion_id": 8, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        sequence_length = x.shape[0]\n        hidden_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(sequence_length):\n            input_data = np.concatenate((h_t, x[t]), axis=0)\n            input_data = np.reshape(input_data, (input_data.shape[0], 1))\n            ft = self.sigmoid(self.Wf @ input_data + self.bf)\n            it = self.sigmoid(self.Wi @ input_data + self.bi)\n            C_tilda = np.tanh(self.Wc @ input_data + self.bc)\n            ot = self.sigmoid(self.Wo @ input_data + self.bo)\n            c_t = ft * c_t + it * C_tilda\n            h_t = ot * np.tanh(c_t)\n            hidden_states.append(h_t.tolist())\n        return (hidden_states, h_t, c_t)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 9, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[0]\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        hidden_states = []\n        for t in range(sequence_length):\n            input_t = np.concatenate((hidden_state, x[t]), axis=0)\n            input_t = input_t.reshape(-1, 1)\n            forget_gate = self.sigmoid(np.dot(self.Wf, input_t) + self.bf)\n            input_gate = self.sigmoid(np.dot(self.Wi, input_t) + self.bi)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, input_t) + self.bc)\n            output_gate = self.sigmoid(np.dot(self.Wo, input_t) + self.bo)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n        return (hidden_states, hidden_state, cell_state)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\nfrom collections import Counter\ndef compute_tf(corpus, query):\n    tf_scores = []\n    for doc in corpus:\n        total_words = len(doc)\n        word_count = Counter(doc)\n        tf_doc = []\n        for q in query:\n            if total_words != 0:\n                tf = word_count[q] / total_words\n            else:\n                tf = 0\n            tf_doc.append(round(tf, 5))\n        tf_scores.append(tf_doc)\n    return tf_scores\ndef compute_idf(corpus, query):\n    total_docs = len(corpus)\n    word_appearance = {}\n    for q in query:\n        count = 0\n        for doc in corpus:\n            if q in doc:\n                count += 1\n        word_appearance[q] = count\n    idf_scores = {}\n    for q in query:\n        df = word_appearance[q]\n        idf = np.log((total_docs + 1) / (df + 1)) + 1\n        idf_scores[q] = idf\n    return idf_scores\ndef compute_tf_idf(corpus, query):\n    tf_scores = compute_tf(corpus, query)\n    idf_scores = compute_idf(corpus, query)\n    tf_idf_scores = []\n    for tf_score in tf_scores:\n        tf_idf_doc = []\n        for i in range(len(tf_score)):\n            tf_idf = round(tf_score[i] * idf_scores[query[i]], 5)\n            tf_idf_doc.append(round(tf_idf, 4))\n        tf_idf_scores.append(tf_idf_doc)\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 1, "solution": "import numpy as np\nfrom collections import defaultdict\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    df_dict = defaultdict(int)\n    for doc in corpus:\n        for word in set(doc):\n            df_dict[word] += 1\n    results = []\n    for doc in corpus:\n        doc_scores = []\n        tf_dict = defaultdict(int)\n        for word in doc:\n            tf_dict[word] += 1 / len(doc)\n        for term in query:\n            if term in df_dict:\n                idf_score = log(len(corpus) / df_dict[term])\n                tf_score = tf_dict[term]\n            else:\n                idf_score = 0\n                tf_score = 0\n            tf_idf_score = round(tf_score * idf_score, 4)\n            doc_scores.append(tf_idf_score)\n        results.append(doc_scores)\n    return np.array(results).tolist()"}
{"task_id": 60, "completion_id": 2, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if len(corpus) == 0:\n        return []\n    tf_idf_scores = []\n    N = len(corpus)\n    for doc in corpus:\n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term) / len(doc) if len(doc) != 0 else 0\n            df = sum([1 for document in corpus if term in document])\n            idf = np.log((N + 1) / (df + 1)) + 1\n            tf_idf = tf * idf\n            doc_tf_idf.append(tf_idf)\n        tf_idf_scores.append(doc_tf_idf)\n    return np.round(tf_idf_scores, 4).tolist()"}
{"task_id": 60, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\ndef flatten(lst):\n    return [item for sublist in lst for item in sublist]\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty')\n    words = flatten(corpus)\n    docs = [words[i:i + len(corpus)] for i in range(0, len(words), len(corpus))]\n    df = {}\n    tf = {}\n    results = []\n    for (index, document) in enumerate(docs):\n        tf[index] = dict(Counter(document))\n    for word in flatten(corpus):\n        df[word] = sum([1 for document in corpus if word in document])\n    for query_word in query:\n        temp = []\n        for (index, document) in enumerate(docs):\n            if query_word in document:\n                temp.append(round(tf[index][query_word] * np.log10(len(corpus) / df[query_word]), 4))\n            else:\n                temp.append(0.0)\n        results.append(temp)\n    return results"}
{"task_id": 60, "completion_id": 4, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Empty Corpus Provided')\n    tf_list = []\n    for doc in corpus:\n        tf_dict = dict.fromkeys(query, 0)\n        tf = {word: doc.count(word) / len(doc) for word in query if word in doc}\n        tf_list.append(tf)\n    idf_dict = dict.fromkeys(query, 0)\n    corpus_size = len(corpus)\n    for word in query:\n        idf_dict[word] = np.log10(corpus_size / sum([1 for doc in corpus if word in doc]))\n    tf_idf = []\n    for i in range(len(tf_list)):\n        tf_idf.append({k: round(tf_list[i].get(k, 0) * idf_dict[k], 4) for k in query})\n    return tf_idf"}
{"task_id": 60, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_tf(corpus, query):\n    tf_scores = []\n    for doc in corpus:\n        doc_len = len(doc)\n        doc_counts = Counter(doc)\n        doc_tf = [round(doc_counts[word] / doc_len, 5) for word in query]\n        tf_scores.append(doc_tf)\n    return np.array(tf_scores).T.tolist()\ndef compute_idf(corpus, query):\n    N = len(corpus)\n    idf_scores = []\n    for word in query:\n        word_count = sum([1 for doc in corpus if word in doc])\n        idf = np.log((N + 1) / (word_count + 1)) + 1\n        idf_scores.append(round(idf, 5))\n    return idf_scores\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    tf_scores = compute_tf(corpus, query)\n    idf_scores = compute_idf(corpus, query)\n    tf_idf_scores = []\n    for i in range(len(query)):\n        doc_tf_idf = [round(tf * idf, 5) for (tf, idf) in zip(tf_scores[i], idf_scores)]\n        tf_idf_scores.append(doc_tf_idf)\n    return tf_idf_scores\ncorpus = [['cat', 'dog', 'cat', 'dog', 'dog', 'cat'], ['dog', 'cat', 'mouse', 'cat', 'mouse', 'mouse'], ['cat', 'dog', 'mouse']]\nquery = ['cat', 'dog', 'mouse']"}
{"task_id": 60, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef compute_tf_idf(corpus, query):\n    if len(corpus) == 0:\n        raise ValueError('Corpus is empty.')\n    for doc in corpus:\n        if len(doc) == 0:\n            raise ValueError('One or more documents in corpus are empty.')\n\n    def term_frequency(term, document):\n        return document.count(term) / len(document)\n\n    def inverse_document_frequency(term, corpus):\n        num_documents_with_term = sum((1 for doc in corpus if term in doc))\n        if num_documents_with_term == 0:\n            num_documents_with_term = 1\n        return math.log(len(corpus) / num_documents_with_term)\n\n    def tf_idf(term, document, corpus):\n        return term_frequency(term, document) * inverse_document_frequency(term, corpus)\n    tf_idf_scores = []\n    for document in corpus:\n        document_scores = []\n        for term in query:\n            document_scores.append(np.round(tf_idf(term, document, corpus), 4))\n        tf_idf_scores.append(document_scores)\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 7, "solution": "import numpy as np\nfrom collections import defaultdict\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    df = defaultdict(int)\n    for document in corpus:\n        for word in set(document):\n            df[word] += 1\n    n = len(corpus)\n    tf_idf_list = []\n    for document in corpus:\n        tf = defaultdict(int)\n        for word in document:\n            tf[word] += 1\n        tf_idf_scores = []\n        for word in query:\n            if word not in tf or word not in df:\n                tf_idf_scores.append(0)\n                continue\n            word_tf = 0 if tf[word] == 0 else tf[word] / len(document)\n            word_idf = np.log((n + 1) / (df[word] + 1))\n            tf_idf = word_tf * word_idf\n            tf_idf_scores.append(round(tf_idf, 4))\n        tf_idf_list.append(tf_idf_scores)\n    return tf_idf_list"}
{"task_id": 60, "completion_id": 8, "solution": "import numpy as np\nimport math\nfrom collections import Counter\ndef compute_tf(document, query):\n    tf_scores = []\n    doc_freq = Counter(document)\n    for term in query:\n        tf = doc_freq[term] / len(document)\n        tf_scores.append(tf)\n    return tf_scores\ndef compute_idf(corpus, query):\n    idf_scores = []\n    doc_count = len(corpus)\n    for term in query:\n        term_doc_count = sum((1 for doc in corpus if term in doc))\n        idf = 0 if term_doc_count == 0 else math.log(doc_count / term_doc_count)\n        idf_scores.append(idf)\n    return idf_scores\ndef compute_tf_idf(corpus, query):\n    if len(corpus) == 0:\n        return []\n    tf_idf_scores = []\n    idf_scores = compute_idf(corpus, query)\n    for document in corpus:\n        tf_scores = compute_tf(document, query)\n        tf_idf_score = [round(tf * idf, 5) for (tf, idf) in zip(tf_scores, idf_scores)]\n        tf_idf_scores.append(tf_idf_score)\n    return np.array(tf_idf_scores).tolist()"}
{"task_id": 60, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if len(corpus) == 0:\n        raise ValueError('Corpus is empty')\n    all_words = [word for document in corpus for word in document]\n    word_counts = Counter(all_words)\n    num_docs = len(corpus)\n    idf = {}\n    for word in word_counts.keys():\n        idf[word] = np.log((1 + num_docs) / (1 + sum((1 for document in corpus if word in document)))) + 1\n    tf_idf = []\n    for document in corpus:\n        tf = Counter(document)\n        tf_idf.append([round(tf.get(word, 0) * idf.get(word, 1), 4) for word in query])\n    return np.array(tf_idf).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n    f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)"}
{"task_id": 61, "completion_id": 1, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    precision = tp / np.sum(y_pred == 1) if np.sum(y_pred == 1) > 0 else 0\n    recall = tp / np.sum(y_true == 1) if np.sum(y_true == 1) > 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 2, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall != 0 else 0\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 3, "solution": "import numpy as np\nfrom sklearn.metrics import precision_score, recall_score\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f_score = (1 + beta ** 2) * (precision * recall / (beta ** 2 * precision + recall))\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 4, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    FN = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    FP = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n    P = TP / (TP + FP)\n    R = TP / (TP + FN)\n    F = (1 + beta ** 2) * P * R / (beta ** 2 * P + R)\n    return round(F, 3)"}
{"task_id": 61, "completion_id": 5, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_positives = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    false_negatives = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall / (beta ** 2 * precision + recall)) if beta ** 2 * precision + recall != 0 else 0\n    f_score = round(f_score, 3)\n    return f_score"}
{"task_id": 61, "completion_id": 6, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    fp = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n    fn = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall > 0 else 0\n    return round(f_score, 3)\ny_true = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\ny_pred = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\nbeta = 0.5"}
{"task_id": 61, "completion_id": 7, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    fp = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n    fn = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    f_score = round(f_score, 3)\n    return f_score"}
{"task_id": 61, "completion_id": 8, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall > 0 else 0\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 9, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n    false_positive = np.sum((y_true == 0) & (y_pred == 1))\n    false_negative = np.sum((y_true == 1) & (y_pred == 0))\n    precision = true_positive / (true_positive + false_positive)\n    recall = true_positive / (true_positive + false_negative)\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\ny_true = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 1, 1, 0, 0, 1, 0, 0])"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        outputs = []\n        hiddens = []\n        for sequence in input_sequence:\n            hidden = np.tanh(np.dot(self.W_hh, sequence) + np.dot(self.W_xh, sequence) + self.b_h)\n            output = np.dot(self.W_hy, hidden) + self.b_y\n            hiddens.append(hidden)\n            outputs.append(output)\n        return (outputs, hiddens)\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, hiddens, learning_rate=0.01):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n        for t in reversed(range(len(input_sequence))):\n            dh = np.dot(self.W_hy.T, expected_output[t] - outputs[t]) + dh_next\n            dW_hy += np.dot(dh, hiddens[t].T)\n            db_y += dh\n            dh_raw = (1 - hiddens[t] ** 2) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_sequence[t].T)\n            dW_hh += np.dot(dh_raw, hiddens[t - 1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for (param, dparam) in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n            param += -learning_rate * dparam"}
{"task_id": 62, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef d_sigmoid(x):\n    y = sigmoid(x)\n    return y * (1 - y)\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n        self.h_t = np.zeros((hidden_size, 1))\n\n    def forward(self, inputs):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y) = ({}, {}, {})\n        h[-1] = np.copy(self.h_t)\n        self.last_inputs = {}\n        for t in range(len(inputs)):\n            x[t] = inputs[t].reshape(2, 1)\n            self.last_inputs[t] = np.copy(x[t])\n            h[t] = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h[t - 1]) + self.b_h)\n            y[t] = sigmoid(np.dot(self.W_hy, h[t]) + self.b_y)\n        self.h_t = h[t]\n        return (y, h, x)\n\n    def backward(self, inputs, outputs, expected_outputs, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y, output, d_W_xh, d_W_hh, d_W_hy, d_b_h, d_b_y) = ({}, {}, {}, {}, {}, {}, {}, {}, {})\n        h[-1] = np.copy(self.h_t)\n        for t in reversed(range(len(inputs))):\n            x[t] = self.last_inputs[t]\n            h[t] = np.copy(outputs[t])\n            if t == len(inputs) - 1:\n                y[t] = np.copy(outputs[t])\n                output[t] = expected_outputs[t]\n            else:\n                y[t] = np.copy(outputs[t + 1])\n                output[t] = np.copy(outputs[t])\n            d_h = d_sigmoid(y[t]) * (output[t] - y[t])\n            d_W_hy += np.dot(d_h, h[t].T)\n            d_b_y += d_h\n            if t > 0:\n                d_W_hh += np.dot(d_h * (1 - h[t] ** 2), h[t - 1].T)\n                d_b_h += d_h * (1 - h[t] ** 2)\n            d_W_xh += np.dot(np.dot(self.W_hh.T, d_h * (1 - h[t] ** 2)), x[t].T)\n        for d in [d_W_xh, d_W_hh, d_W_hy, d_b_h, d_b_y]:\n            np.clip(d, -1, 1, out=d)\n        self.W_xh -= learning_rate * d_W_xh\n        self.W_hh -= learning_rate * d_W_hh\n        self.W_hy -= learning_rate * d_W_hy\n        self.b_h -= learning_rate * d_b_h.sum(axis=1).reshape(self.hidden_size, -1)\n        self.b_y -= learning_rate * d_b_y.sum(axis=1).reshape(self.b_y.shape[0], -1)"}
{"task_id": 62, "completion_id": 2, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y) = ({}, {}, {})\n        h[-1] = np.zeros((self.hidden_size, 1))\n        for (t, x_t) in enumerate(input_sequence):\n            x[t] = np.reshape(x_t, (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h[t - 1]) + self.b_h)\n            y[t] = np.dot(self.W_hy, h[t]) + self.b_y\n        outputs = [np.asscalar(y_t) for y_t in y.values()]\n        last_input = np.reshape(x[len(x) - 1], (self.W_xh.shape[1], 1))\n        last_hidden = np.reshape(h[len(h) - 1], (self.hidden_size, 1))\n        return (outputs, last_input, last_hidden)\n\n    def backward(self, input_sequence, expected_output, outputs, last_input, last_hidden, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights.\n        \"\"\"\n        (x, h, y) = ({}, {}, {})\n        h[-1] = np.zeros((self.hidden_size, 1))\n        (grads_xh, grads_hh, grads_hy) = (np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy))\n        (grads_bh, grads_by) = (np.zeros_like(self.b_h), np.zeros_like(self.b_y))\n        grads_h_next = np.zeros_like(h[0])\n        for t in reversed(range(len(input_sequence))):\n            x[t] = np.reshape(input_sequence[t], (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h[t - 1]) + self.b_h)\n            y[t] = np.dot(self.W_hy, h[t]) + self.b_y\n            loss_y = expected_output[t] - outputs[t]\n            grad_y = -(expected_output[t] - y[t])\n            grad_h = (1 - h[t] ** 2).T * (self.W_hy.T @ grad_y + grads_h_next.T)\n            grad_W_hy = grad_y @ h[t].T\n            grad_W_xh = grad_h @ x[t].T\n            grad_W_hh = grad_h @ h[t - 1].T\n            grads_xh += grad_W_xh\n            grads_hh += grad_W_hh\n            grads_hy += grad_y\n            grads_bh += grad_h\n            grads_by += grad_y\n            grads_h_next = self.W_hh.T @ grad_h\n        for dparam in [grads_xh, grads_hh, grads_hy, grads_bh, grads_by]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * grads_xh\n        self.W_hh -= learning_rate * grads_hh\n        self.W_hy -= learning_rate * grads_hy\n        self.b_h -= learning_rate * grads_bh\n        self.b_y -= learning_rate * grads_by"}
{"task_id": 62, "completion_id": 3, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        (self.W_xh, self.W_hh, self.W_hy) = self.initialize_weights(input_size, hidden_size, output_size)\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def initialize_weights(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the weights and biases.\n        \"\"\"\n        W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        return (W_xh, W_hh, W_hy)\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y, h_t, h_all) = ({}, {}, {}, {}, {})\n        h[-1] = np.zeros((self.hidden_size, 1))\n        loss = 0\n        for (t, x_t) in enumerate(input_sequence):\n            x[t] = np.reshape(x_t, (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(np.dot(self.W_hh, h[t - 1]) + np.dot(self.W_xh, x[t]) + self.b_h)\n            y[t] = np.dot(self.W_hy, h[t]) + self.b_y\n            h_all[t] = h[t]\n        return (y, h_all)\n\n    def backward(self, expected_output, input_sequence, outputs, hiddens, learning_rate=0.01):\n        \"\"\"\n        Backward pass (BPTT) to calculate gradients and update the weights.\n        \"\"\"\n        (x, h, y, h_t, h_all) = ({}, {}, {}, {}, {})\n        grad_W_xh = np.zeros_like(self.W_xh)\n        grad_W_hh = np.zeros_like(self.W_hh)\n        grad_W_hy = np.zeros_like(self.W_hy)\n        grad_b_h = np.zeros_like(self.b_h)\n        grad_b_y = np.zeros_like(self.b_y)\n        h[-1] = np.zeros((self.hidden_size, 1))\n        for t in reversed(range(len(input_sequence))):\n            x[t] = np.reshape(input_sequence[t], (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(np.dot(self.W_hh, h[t + 1]) + np.dot(self.W_xh, x[t]) + self.b_h)\n            y[t] = np.dot(self.W_hy, h[t]) + self.b_y\n            h_all[t] = h[t]\n            grad_y = 2 * (y[t] - np.reshape(expected_output[t], (self.W_hy.shape[0], 1)))\n            grad_W_hy += np.dot(grad_y, h[t].T)\n            grad_b_y += grad_y\n            grad_h = np.dot(self.W_hy.T, grad_y)\n            grad_h = grad_h * (1 - h[t] ** 2)\n            for t_ in reversed(range(t, max(0, t - 100 + 1))):\n                grad_W_hh += np.dot(grad_h, h[t_ - 1].T)\n                grad_b_h += grad_h\n                grad_W_xh += np.dot(grad_h, x[t_].T)\n                grad_h = np.dot(self.W_hh.T, grad_h) * (1 - h[t_ - 1] ** 2)\n        for (param, grad) in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [grad_W_xh, grad_W_hh, grad_W_hy, grad_b_h, grad_b_y]):\n            param -= learning_rate * grad\n        return (outputs, hiddens)"}
{"task_id": 62, "completion_id": 4, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, inputs):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.W_hh.shape[0], 1))\n        outputs = []\n        hidden_states = []\n        for (i, x) in enumerate(inputs):\n            h = np.tanh(np.dot(self.W_hh, h) + np.dot(self.W_xh, x) + self.b_h)\n            out = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(out)\n            hidden_states.append(h)\n        return (np.array(outputs), np.array(hidden_states))\n\n    def backward(self, inputs, expected_outputs, outputs, hidden_states, learning_rate=0.01):\n        \"\"\"\n        Backward pass through the RNN to adjust weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        for t in reversed(range(len(inputs))):\n            dh = np.dot(self.W_hy.T, outputs[t] - expected_outputs[t]) + dh_next\n            dh_raw = (1 - hidden_states[t] * hidden_states[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, inputs[t].T)\n            dW_hh += np.dot(dh_raw, hidden_states[t - 1].T)\n            dW_hy += np.dot(expected_outputs[t] - outputs[t], hidden_states[t].T)\n            db_y += expected_outputs[t] - outputs[t]\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 62, "completion_id": 5, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, inputs):\n        h = np.zeros((self.hidden_size, 1))\n        outputs = []\n        for x in inputs:\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n        return (outputs, h)\n\n    def backward(self, inputs, outputs, expected_outputs, hiddens, learning_rate=0.1):\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n        for t in reversed(range(len(inputs))):\n            y = outputs[t]\n            x = inputs[t]\n            h = hiddens[t]\n            y_exp = expected_outputs[t]\n            loss = 0.5 * np.sum(np.power(y_exp - y, 2))\n            dW_hy += np.dot(y - y_exp, h.T)\n            db_y += y - y_exp\n            dh = np.dot(self.W_hy.T, y - y_exp) + dh_next\n            dt = dh * (1 - h * h)\n            dW_xh += np.dot(dt, x.T)\n            dW_hh += np.dot(dt, hiddens[t - 1].T)\n            db_h += dt\n            dh_next = np.dot(self.W_hh.T, dt)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\ninputs = np.random.randn(5, 5)\nexpected_outputs = np.random.randn(5, 1)"}
{"task_id": 62, "completion_id": 6, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n        self.h_prev = np.zeros((hidden_size, 1))\n\n    def forward(self, inputs):\n        \"\"\"\n        Implement forward pass for the RNN.\n        \"\"\"\n        self.inputs = inputs\n        self.hidden_layers = []\n        self.outputs = []\n        for x in inputs:\n            h = np.tanh(np.dot(self.W_hh, self.h_prev) + np.dot(self.W_xh, x) + self.b_h)\n            o = np.dot(self.W_hy, h) + self.b_y\n            self.hidden_layers.append(h)\n            self.outputs.append(o)\n            self.h_prev = h\n        return (self.outputs, self.hidden_layers)\n\n    def backward(self, outputs, expected_outputs, hidden_layers, lr=0.005):\n        \"\"\"\n        Implement backpropagation through time (BPTT) for the RNN.\n        \"\"\"\n        n = len(inputs)\n        dy = outputs[0] - expected_outputs[0]\n        dW_hy = np.dot(dy, hidden_layers[0].T)\n        self.W_hy -= lr * dW_hy\n        self.b_y -= lr * dy\n        dh_next = np.dot(self.W_hy.T, dy)\n        for t in reversed(range(n - 1)):\n            dh = dh_next + np.dot(self.W_hh.T, dh_next) * (1 - hidden_layers[t] ** 2)\n            dW_hh = np.dot(dh, hidden_layers[t - 1].T)\n            dW_xh = np.dot(dh, self.inputs[t].T)\n            self.W_hh -= lr * dW_hh\n            self.W_xh -= lr * dW_xh\n            self.b_h -= lr * dh\n            dy = outputs[t] - expected_outputs[t]\n            dW_hy = np.dot(dy, hidden_layers[t].T)\n            self.W_hy -= lr * dW_hy\n            self.b_y -= lr * dy\n            dh_next = np.dot(self.W_hh.T, dh)\ninput_size = 3\nhidden_size = 5\noutput_size = 4\ninputs = np.random.randn(2, input_size, 1)\nexpected_outputs = np.random.randn(2, output_size, 1)"}
{"task_id": 62, "completion_id": 7, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, inputs):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hiddens = np.zeros((inputs.shape[0], self.hidden_size))\n        outputs = np.zeros((inputs.shape[0], self.W_hy.shape[0]))\n        for t in np.arange(inputs.shape[0]):\n            hiddens[t] = np.tanh(np.dot(self.W_xh, inputs[t].T) + np.dot(self.W_hh, hiddens[t - 1].T) + self.b_h.T)\n            outputs[t] = np.dot(self.W_hy, hiddens[t].T) + self.b_y.T\n        return (outputs, hiddens)\n\n    def backward(self, inputs, outputs, hiddens, expected_outputs, learning_rate=0.1):\n        \"\"\"\n        Perform backpropagation through time to adjust weights\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n        for t in reversed(np.arange(inputs.shape[0])):\n            doutput = outputs[t] - expected_outputs[t]\n            dW_hy += np.dot(doutput.T, hiddens[t])\n            db_y += doutput\n            dh = np.dot(doutput, self.W_hy) + dh_next\n            dh_raw = (1 - hiddens[t] * hiddens[t]) * dh\n            dW_xh += np.dot(dh_raw.T, inputs[t])\n            dW_hh += np.dot(dh_raw.T, hiddens[t - 1])\n            db_h += dh_raw\n            dh_next = np.dot(dh_raw, self.W_hh.T)\n        for (param, dparam) in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n            param -= learning_rate * dparam"}
{"task_id": 62, "completion_id": 8, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y) = ({}, {}, {})\n        h[-1] = np.zeros((self.hidden_size, 1))\n        loss = 0\n        outputs = []\n        for t in range(len(input_sequence)):\n            x[t] = np.reshape(input_sequence[t], (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(self.W_hh @ h[t - 1] + self.W_xh @ x[t] + self.b_h)\n            y[t] = self.W_hy @ h[t] + self.b_y\n            outputs.append(y[t])\n        return outputs\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, learning_rate=0.1):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        (x, h, y) = ({}, {}, {})\n        h[-1] = np.zeros((self.hidden_size, 1))\n        (dW_xh, dW_hh, dW_hy) = (np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy))\n        (db_h, db_y) = (np.zeros_like(self.b_h), np.zeros_like(self.b_y))\n        dhnext = np.zeros_like(h[0])\n        for t in reversed(range(len(input_sequence))):\n            x[t] = np.reshape(input_sequence[t], (self.W_xh.shape[1], 1))\n            h[t] = np.tanh(self.W_hh @ h[t - 1] + self.W_xh @ x[t] + self.b_h)\n            y[t] = self.W_hy @ h[t] + self.b_y\n            loss = np.sum(-expected_output[t] * np.log(y[t]))\n            dy = np.copy(y[t])\n            dy -= 1\n            dW_hy += dy @ h[t].T\n            dW_hh += dy @ h[t - 1].T\n            dW_xh += dy @ x[t].T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dhnext\n            dhraw = (1 - h[t] * h[t]) * dh\n            db_h += dhraw\n            dW_xh += dhraw @ x[t].T\n            dW_hh += dhraw @ h[t - 1].T\n            dhnext = self.W_hh.T @ dhraw\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 62, "completion_id": 9, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"Initializes the RNN with random weights and zero biases.\"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x, h_prev):\n        \"\"\"Performs a forward pass through the RNN for a given sequence of inputs.\"\"\"\n        h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n        y = np.dot(self.W_hy, h) + self.b_y\n        return (y, h)\n\n    def backward(self, x, h, y, d_y, d_h_next, W_hh, W_hy):\n        \"\"\"Performs a backward pass through the RNN to adjust the weights.\"\"\"\n        d_y_pred = 2 * (y - d_y)\n        d_W_hy = np.dot(d_y_pred, h.T)\n        d_h = np.dot(W_hy.T, d_y_pred) + d_h_next\n        d_W_hh = np.dot(d_h * (1 - h ** 2), h_prev.T)\n        d_W_xh = np.dot(d_h * (1 - h ** 2), x.T)\n        d_b_h = 1.0 * d_h\n        d_b_y = np.sum(d_y_pred, axis=1, keepdims=True)\n        d_h_prev = np.dot(W_hh.T, d_h)\n        return (d_W_xh, d_W_hh, d_W_hy, d_b_h, d_b_y, d_h_prev)\n\n    def calculate_loss(self, y, y_pred):\n        \"\"\"Calculate the mean squared error loss.\"\"\"\n        return np.mean(np.square(y_pred - y))\n\n    def train(self, x, y, h_prev, learning_rate=0.01):\n        \"\"\"Trains the RNN on a single sequence of inputs.\"\"\"\n        (y_pred, h) = self.forward(x, h_prev)\n        loss = self.calculate_loss(y, y_pred)\n        (d_W_xh, d_W_hh, d_W_hy, d_b_h, d_b_y, d_h_prev) = self.backward(x, h, y_pred, y, 0, self.W_hh, self.W_hy)\n        self.W_xh -= learning_rate * d_W_xh\n        self.W_hh -= learning_rate * d_W_hh\n        self.W_hy -= learning_rate * d_W_hy\n        self.b_h -= learning_rate * d_b_h\n        self.b_y -= learning_rate * d_b_y\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x0)\n    r = b - np.dot(A, x)\n    d = r\n    for i in range(n):\n        alpha = np.dot(r, r) / np.dot(d, np.dot(A, d))\n        x = x + alpha * d\n        r_new = r - alpha * np.dot(A, d)\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        d = r_new + beta * d\n        r = r_new\n        if np.linalg.norm(r) < tol:\n            break\n    return x.round(8).tolist()\nA = [[4, 1], [1, 3]]\nb = [1, 2]\nx0 = [2, 1]\nx = conjugate_gradient(A, b, 20, x0)"}
{"task_id": 63, "completion_id": 1, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros(len(b))\n    r = b - np.dot(A, x0)\n    p = r\n    k = 0\n    r_norm_sq = np.dot(r, r)\n    while True:\n        Ap = np.dot(A, p)\n        alpha = r_norm_sq / np.dot(p, Ap)\n        x = x0 + alpha * p\n        r = r - alpha * Ap\n        r_norm_sq_new = np.dot(r, r)\n        if np.sqrt(r_norm_sq_new) < tol:\n            return np.round(x, 8).tolist()\n        p = r + r_norm_sq_new / r_norm_sq * p\n        r_norm_sq = r_norm_sq_new\n        x0 = x\n        if k > n:\n            break\n        k += 1\n    return None"}
{"task_id": 63, "completion_id": 2, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if not x0:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    k = 0\n    while np.linalg.norm(r) > tol and k < n:\n        Ap = np.dot(A, p)\n        rTr = np.dot(r, r)\n        alpha = rTr / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rNextTr = np.dot(r, r)\n        beta = rNextTr / rTr\n        p = r + beta * p\n        k += 1\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 3, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if not x0:\n        x0 = np.zeros(len(b))\n    r = b - np.dot(A, x0)\n    p = r\n    r_squared = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = r_squared / np.dot(p, Ap)\n        x0 = x0 + alpha * p\n        r = r - alpha * Ap\n        r_squared_new = np.dot(r, r)\n        if np.sqrt(r_squared_new) < tol:\n            break\n        else:\n            beta = r_squared_new / r_squared\n            p = r + beta * p\n            r_squared = r_squared_new\n    return np.round(x0, 8).tolist()"}
{"task_id": 63, "completion_id": 4, "solution": "import numpy as np\ndef conjugate_gradient(A, b, n, x0=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if np.all(np.linalg.eigvals(A) > 0) == False:\n        return 'Error: A is not positive-definite!'\n    if x0 is None:\n        x = np.zeros_like(b, dtype=np.float64)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    d = r\n    for i in range(n):\n        Ad = np.dot(A, d)\n        rTr = np.dot(r, r)\n        alpha = rTr / np.dot(d, Ad)\n        x = x + alpha * d\n        r = r - alpha * Ad\n        r_norm = np.linalg.norm(r)\n        if r_norm < tol:\n            print(f'Converged in {i + 1} iterations.')\n            return np.round(x, 8).tolist()\n        beta = np.dot(r, r) / rTr\n        d = r + beta * d\n    print('Failed to converge in the given iterations.')\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]], dtype=np.float64)\nb = np.array([1, 2], dtype=np.float64)\nx = conjugate_gradient(A, b, 1000, tol=1e-10)"}
{"task_id": 63, "completion_id": 5, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if not x0:\n        x = np.zeros(len(b))\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.around(x, 8).tolist()"}
{"task_id": 63, "completion_id": 6, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    r = b - np.dot(A, x0)\n    p = r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x0 = x0 + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x0.round(8).tolist()"}
{"task_id": 63, "completion_id": 7, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n    \n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = np.zeros_like(b, dtype=np.float64) if x0 is None else x0\n    r = b - np.dot(A, x)\n    p = r\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        r_squared = np.dot(r, r)\n        alpha = r_squared / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_squared_new = np.dot(r, r)\n        if r_squared_new ** 0.5 < tol:\n            break\n        beta = r_squared_new / r_squared\n        p = r + beta * p\n    return x.round(8).tolist()\nA = np.array([[3, 2], [2, 5]])\nb = np.array([5, 6])\nn = 20"}
{"task_id": 63, "completion_id": 8, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = np.zeros_like(b) if x0 is None else x0\n    r = b - np.dot(A, x)\n    p = r\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        r_dot = np.dot(r.T, r)\n        alpha = r_dot / np.dot(p.T, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        residual = np.linalg.norm(r)\n        if residual < tol:\n            break\n        beta = np.dot(r.T, r) / r_dot\n        p = r + beta * p\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 9, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol: float=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - np.dot(A, x)\n    p = r.copy()\n    r_prev = r.copy()\n    for _ in range(n):\n        rTr = np.dot(r, r)\n        Ap = np.dot(A, p)\n        alpha = rTr / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_norm = np.linalg.norm(r)\n        if r_norm < tol:\n            break\n        else:\n            beta = np.dot(r, r) / rTr\n            p = r + beta * p\n    return np.around(x.tolist(), 8)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, class_counts) = np.unique(y, return_counts=True)\n    class_probabilities = class_counts / class_counts.sum()\n    gini = 1 - np.sum(np.square(class_probabilities))\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 1, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    n = len(y)\n    if n == 0:\n        return 0.0\n    class_proportions = np.array([np.sum(y == i) for i in np.unique(y)]) / n\n    gini = 1 - np.sum(np.square(class_proportions))\n    gini = round(gini, 3)\n    return gini"}
{"task_id": 64, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    class_counts = Counter(y)\n    impurity = 1\n    for count in class_counts.values():\n        p = count / len(y)\n        impurity -= p ** 2\n    return round(impurity, 3)\ny = [0, 0, 1, 1, 1, 0, 0, 1]"}
{"task_id": 64, "completion_id": 3, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    total = len(y)\n    probabilities = [len(list(filter(lambda x: x == i, y))) / total for i in set(y)]\n    impurity = 1 - sum([p ** 2 for p in probabilities])\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 4, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(np.square(probabilities))\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    hist = np.array(list(Counter(y).values()))\n    n_instances = float(hist.sum())\n    return round(1.0 - np.sum((hist / n_instances) ** 2), 3)"}
{"task_id": 64, "completion_id": 6, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(np.square(probabilities))\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 7, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 8, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 9, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for (i, row) in enumerate(dense_matrix):\n        for (j, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(j)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 1, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for (i, row) in enumerate(dense_matrix):\n        for (j, elem) in enumerate(row):\n            if elem != 0:\n                values.append(elem)\n                col_indices.append(j)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [9, 0, 0, 0, 12]]"}
{"task_id": 65, "completion_id": 2, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (i, num) in enumerate(row):\n            if num != 0:\n                values.append(num)\n                col_indices.append(i)\n        row_pointer.append(len(values))\n    return (values, col_indices, row_pointer)\ndense_matrix = [[1, 0, 0, 0], [0, 2, 0, 3], [4, 0, 5, 0], [0, 0, 0, 6]]"}
{"task_id": 65, "completion_id": 3, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    prev_row_pointer = 0\n    for (i, row) in enumerate(dense_matrix):\n        for (j, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(j)\n                prev_row_pointer += 1\n        row_pointer.append(prev_row_pointer)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 65, "completion_id": 4, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col_idx)\n        row_pointer.append(len(values))\n    return (values, col_indices, row_pointer)"}
{"task_id": 65, "completion_id": 5, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    columnIndices = []\n    rowPointer = [0]\n    for (i, row) in enumerate(dense_matrix):\n        for (j, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                columnIndices.append(j)\n        rowPointer.append(len(values))\n    return (values, columnIndices, rowPointer)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 65, "completion_id": 6, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_indices = []\n    row_pointer = [0]\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    for row in range(num_rows):\n        for col in range(num_cols):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n                column_indices.append(col)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 0, 5, 0], [0, 8, 0, 0, 0], [0, 0, 3, 0, 0], [0, 6, 0, 0, 4], [0, 0, 2, 0, 0]]"}
{"task_id": 65, "completion_id": 7, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values_array = []\n    column_indices_array = []\n    row_pointer_array = [0]\n    num_rows = len(dense_matrix)\n    for row in range(num_rows):\n        non_zero_elements = [(col, val) for (col, val) in enumerate(dense_matrix[row]) if val != 0]\n        for (col, val) in non_zero_elements:\n            values_array.append(val)\n            column_indices_array.append(col)\n        row_pointer_array.append(len(values_array))\n    return (values_array, column_indices_array, row_pointer_array)"}
{"task_id": 65, "completion_id": 8, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 9, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for (i, row) in enumerate(dense_matrix):\n        for (j, elem) in enumerate(row):\n            if elem != 0:\n                values.append(elem)\n                column_indices.append(j)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    unit_L = L / np.linalg.norm(L)\n    projection = np.dot(v, unit_L) * unit_L\n    return np.round(projection, 3).tolist()"}
{"task_id": 66, "completion_id": 1, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    sqr_mag_L = sum((i ** 2 for i in L))\n    dot_product = sum((x * y for (x, y) in zip(v, L)))\n    proj_v_L = [dot_product / sqr_mag_L * i for i in L]\n    return [round(i, 3) for i in proj_v_L]\nv = [3, 1]\nL = [4, 0]"}
{"task_id": 66, "completion_id": 2, "solution": "import math\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum([v[i] * L[i] for i in range(len(v))])\n    L_magnitude = math.sqrt(sum([L[i] ** 2 for i in range(len(L))]))\n    p = [dot_product / L_magnitude ** 2 * L[i] for i in range(len(L))]\n    return [round(x, 3) for x in p]"}
{"task_id": 66, "completion_id": 3, "solution": "def dot_product(v1, v2):\n    \"\"\"\n    Compute the dot product of two vectors v1 and v2.\n    \"\"\"\n    return sum((x * y for (x, y) in zip(v1, v2)))\ndef length(v):\n    \"\"\"\n    Compute the length of a vector v.\n    \"\"\"\n    return dot_product(v, v) ** 0.5\ndef normalize(v):\n    \"\"\"\n    Normalize a vector v.\n    \"\"\"\n    v_len = length(v)\n    return [x / v_len for x in v]\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    L_norm = normalize(L)\n    projection_length = dot_product(v, L_norm)\n    projection = [projection_length * x for x in L_norm]\n    return [round(x, 3) for x in projection]\nv = [1, 2, 3]\nL = [4, 5, 6]"}
{"task_id": 66, "completion_id": 4, "solution": "import math\ndef dot_product(a, b):\n    \"\"\"\n    Compute the dot product of vectors a and b.\n    \"\"\"\n    return sum((x * y for (x, y) in zip(a, b)))\ndef multiply_vector_by_scalar(v, s):\n    \"\"\"\n    Multiply a vector by a scalar value.\n    \"\"\"\n    return [x * s for x in v]\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    proj_coeff = dot_product(v, L) / dot_product(L, L)\n    proj_vector = multiply_vector_by_scalar(L, proj_coeff)\n    proj_vector = [round(x, 3) for x in proj_vector]\n    return proj_vector\nv = [3, 1]\nL = [4, 0]"}
{"task_id": 66, "completion_id": 5, "solution": "import math\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((x * y for (x, y) in zip(v, L)))\n    L_norm_squared = sum((i ** 2 for i in L))\n    if L_norm_squared == 0:\n        return [0.0, 0.0]\n    scalar = dot_product / L_norm_squared\n    projection_vector = [round(scalar * L_i, 3) for L_i in L]\n    return projection_vector\nv = [3, 1]\nL = [4, 0]"}
{"task_id": 66, "completion_id": 6, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of coordinates)\n    :param L: The line vector defining the direction of projection (list of coordinates)\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    L_norm = L / np.linalg.norm(L)\n    projection = np.dot(v, L_norm) * L_norm\n    return projection.round(3).tolist()\nv = [1, 2, 3]\nL = [4, 5, 6]"}
{"task_id": 66, "completion_id": 7, "solution": "import math\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((v_i * L_i for (v_i, L_i) in zip(v, L)))\n    l_len_squared = sum((L_i ** 2 for L_i in L))\n    proj_factor = dot_product / l_len_squared\n    proj_vec = [proj_factor * L_i for L_i in L]\n    return [round(x, 3) for x in proj_vec]\nv = [1, 2, 3]\nL = [4, 5, 6]"}
{"task_id": 66, "completion_id": 8, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    projection = v - np.dot(v, L) / np.dot(L, L) * L\n    return [round(val, 3) for val in projection]\nv = [2, 3, -4]\nL = [1, 0, 1]\nprojection = orthogonal_projection(v, L)"}
{"task_id": 66, "completion_id": 9, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((i * j for (i, j) in zip(v, L)))\n    magnitude_L = sum((i * j for (i, j) in zip(L, L)))\n    proj = [round(dot_product / magnitude_L * i, 3) for i in L]\n    return proj\nv = [1, 2]\nL = [3, 4]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for column in zip(*dense_matrix):\n        for (row_index, value) in enumerate(column):\n            if value != 0:\n                values.append(value)\n                row_indices.append(row_index)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer[:-1])"}
{"task_id": 67, "completion_id": 1, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(len(dense_matrix[0])):\n        col_val = [dense_matrix[row][col] for row in range(len(dense_matrix))]\n        for (row, val) in enumerate(col_val):\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\ndense_matrix = [[1, 0, 2, 0], [0, 3, 0, 4], [5, 0, 6, 0], [0, 7, 0, 8]]"}
{"task_id": 67, "completion_id": 2, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    rows = []\n    column_pointer = []\n    num_columns = len(dense_matrix[0])\n    for column_index in range(num_columns):\n        column = [row[column_index] for row in dense_matrix]\n        column_pointer.append(len(values))\n        for (row_index, element) in enumerate(column):\n            if element != 0:\n                values.append(element)\n                rows.append(row_index)\n    return (values, rows, column_pointer)"}
{"task_id": 67, "completion_id": 3, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    n_rows = len(dense_matrix)\n    n_cols = len(dense_matrix[0])\n    value_list = []\n    row_indices = []\n    for c in range(n_cols):\n        for r in range(n_rows):\n            if dense_matrix[r][c] != 0:\n                value_list.append(dense_matrix[r][c])\n                row_indices.append(r)\n    col_pointer = [0]\n    curr_sum = 0\n    for c in range(n_cols):\n        for r in range(n_rows):\n            if dense_matrix[r][c] != 0:\n                curr_sum += 1\n        col_pointer.append(curr_sum)\n    return (value_list, row_indices, col_pointer)\ndense_matrix = [[1, 0, 0, 0], [0, 2, 0, 3], [0, 0, 4, 0], [5, 0, 0, 6]]"}
{"task_id": 67, "completion_id": 4, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    prev_index = 0\n    for column in zip(*dense_matrix):\n        for (row_index, value) in enumerate(column):\n            if value != 0:\n                values.append(value)\n                row_indices.append(row_index)\n        prev_index += len(values)\n        column_pointer.append(prev_index)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 5, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for column in zip(*dense_matrix):\n        for (row_index, value) in enumerate(column):\n            if value != 0:\n                values.append(value)\n                row_indices.append(row_index)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer[:-1])\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 6, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    for j in range(num_cols):\n        element_count = 0\n        for i in range(num_rows):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n                element_count += 1\n        column_pointer.append(column_pointer[-1] + element_count)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0, 4], [0, 0, 5, 7, 0], [0, 0, 0, 0, 0], [0, 2, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 7, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    col_pointers = [0]\n    for col in range(len(dense_matrix[0])):\n        for row in range(len(dense_matrix)):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_pointers.append(len(values))\n    return (values, row_indices, col_pointers)"}
{"task_id": 67, "completion_id": 8, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for col in dense_matrix:\n        col_val = []\n        col_row = []\n        for (row_idx, elem) in enumerate(col):\n            if elem != 0:\n                col_val.append(elem)\n                col_row.append(row_idx)\n        values.extend(col_val)\n        row_indices.extend(col_row)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)"}
{"task_id": 67, "completion_id": 9, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of three lists: (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for j in range(len(dense_matrix[0])):\n        for i in range(len(dense_matrix)):\n            if dense_matrix[i][j] != 0:\n                values.append(dense_matrix[i][j])\n                row_indices.append(i)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.linalg import rref\ndef matrix_image(A):\n    A = np.array(A)\n    (_, inds) = rref(A)\n    basis = A[:, inds].round(8).tolist()\n    return basis\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 68, "completion_id": 1, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Given a matrix A, this function returns a Python list of column vectors that span the\n    column space of A.\n\n    This is achieved by performing Row Echelon Form Reduction on the transpose of A, and\n    then transposing the reduced matrix back to obtain the column space vectors. It uses\n    the numpy library for the computations.\n\n    Args:\n    A: A 2D numpy array representing the input square matrix A.\n\n    Returns:\n    A Python list of 2D numpy arrays, where each 2D numpy array represents a column vector\n    that spans the column space of A. Each array is converted to a list, rounded to 8\n    decimal places, using the tolist() method of numpy arrays.\n    \"\"\"\n    reduced_transpose = np.around(np.linalg.qr(A.transpose())[0], 8)\n    column_space = np.transpose(reduced_transpose)\n    column_space_list = [np.around(column, 8).tolist() for column in column_space]\n    return column_space_list"}
{"task_id": 68, "completion_id": 2, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (rref, inds) = (np.linalg.matrix_rank(A), np.linalg.matrix_rank(np.c_[A, np.ones(A.shape[0])]))\n    inds_basis = inds - rref\n    basis = np.round(A[:, :inds_basis], 8)\n    return basis.tolist()"}
{"task_id": 68, "completion_id": 3, "solution": "import numpy as np\ndef matrix_image(A):\n    (rref_A, inds) = (np.around(A, 8).T.tolist(), np.around(A, 8).T)\n    basis = []\n    for (i, row) in enumerate(rref_A):\n        if set(row) != {0}:\n            basis.append(inds[i])\n    return basis"}
{"task_id": 68, "completion_id": 4, "solution": "import numpy as np\ndef matrix_image(A):\n    A = gaussian_elimination(A)\n    basis = []\n    for column in A.T:\n        if np.count_nonzero(column) != 0 and column.tolist() not in basis:\n            basis.append(column.tolist())\n    basis = [[round(elem, 8) for elem in vec] for vec in basis]\n    return basis\ndef gaussian_elimination(A):\n    for i in range(min(len(A), len(A[0]))):\n        max_index = i + np.argmax(abs(A[i:, i]))\n        if i != max_index:\n            A[[i, max_index]] = A[[max_index, i]]\n        for row in range(i + 1, len(A)):\n            factor = A[row][i] / A[i][i]\n            A[row] -= factor * A[i]\n    for i in range(len(A) - 1, -1, -1):\n        A[i] /= A[i][i]\n        for row in range(i):\n            factor = A[row][i] / A[i][i]\n            A[row] -= factor * A[i]\n    return A"}
{"task_id": 68, "completion_id": 5, "solution": "import numpy as np\ndef matrix_image(A):\n    rref_A = np.around(np.linalg.qr(A.T)[0].T, decimals=8)\n    independent_columns = []\n    for column in rref_A.T:\n        if np.count_nonzero(column) == 1:\n            independent_columns.append(column)\n    return [column.tolist() for column in independent_columns]"}
{"task_id": 68, "completion_id": 6, "solution": "import numpy as np\nfrom scipy.linalg import rref\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (A_ref, ind) = rref(A)\n    ind = np.where(np.sum(A_ref, axis=0) != 0)[0]\n    A_ind = A[:, ind]\n    return A_ind.round(8).tolist()"}
{"task_id": 68, "completion_id": 7, "solution": "import numpy as np\nfrom scipy.linalg import lu\ndef matrix_image(A):\n    (P, L, U) = lu(A.round(8))\n    basis_vectors = L[:, np.argmax(L != 0, axis=0)]\n    return [vec.round(8).tolist() for vec in basis_vectors.transpose()]"}
{"task_id": 68, "completion_id": 8, "solution": "import numpy as np\nfrom scipy.linalg import lu\ndef matrix_image(A):\n    A = A.astype(float)\n    (P, L, U) = lu(A)\n    pivot_columns = np.flatnonzero(np.abs(U[0]) >= np.finfo(float).eps)\n    basis_vectors = A[:, pivot_columns]\n    basis_vectors = np.round(basis_vectors, 8).tolist()\n    return basis_vectors"}
{"task_id": 68, "completion_id": 9, "solution": "import numpy as np\ndef matrix_image(A):\n    A = A.astype(float)\n    (rows, cols) = A.shape\n    for i in range(min(rows, cols)):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if i != max_row:\n            A[[i, max_row]] = A[[max_row, i]]\n        for j in range(i + 1, rows):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] -= factor * A[i, i:]\n    for i in range(min(rows, cols) - 1, -1, -1):\n        A[i] /= A[i, i]\n        for j in range(i):\n            factor = A[j, i] / A[i, i]\n            A[j] -= factor * A[i]\n    B = A[:, :min(rows, cols)]\n    basis_vectors = B.round(8).tolist()\n    return basis_vectors"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    rss = np.sum((y_true - y_pred) ** 2)\n    avg_true = np.mean(y_true)\n    tss = np.sum((y_true - avg_true) ** 2)\n    r_squared = 1 - rss / tss\n    return round(r_squared, 3)"}
{"task_id": 69, "completion_id": 1, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true_mean = np.mean(y_true)\n    SST = np.sum(np.square(y_true - y_true_mean))\n    SSE = np.sum(np.square(y_true - y_pred))\n    r2 = 1 - SSE / SST\n    return round(r2, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 69, "completion_id": 2, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    y_mean = np.mean(y_true)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_mean) ** 2)\n    return round(1 - ss_res / ss_tot, 3)"}
{"task_id": 69, "completion_id": 3, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    ss_res = sum((y_true - y_pred) ** 2)\n    ss_tot = sum((y_true - np.mean(y_true)) ** 2)\n    r_squared = 1 - ss_res / ss_tot\n    return round(r_squared, 3)"}
{"task_id": 69, "completion_id": 4, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    rss = np.sum((y_true - y_pred) ** 2)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    r_squared = 1 - rss / tss\n    r_squared = round(r_squared, 3)\n    return r_squared\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 69, "completion_id": 5, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true_mean = np.mean(y_true)\n    ss_total = sum((y_true - y_true_mean) ** 2)\n    ss_residuals = sum((y_true - y_pred) ** 2)\n    r_squared = 1 - ss_residuals / ss_total\n    return round(r_squared, 3)"}
{"task_id": 69, "completion_id": 6, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_true_mean = np.mean(y_true)\n    SS_res = np.sum((y_true - y_pred) ** 2)\n    SS_tot = np.sum((y_true - y_true_mean) ** 2)\n    r2 = 1 - SS_res / SS_tot\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 7, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    ssr = np.sum(np.square(y_true - y_pred))\n    sst = np.sum(np.square(y_true - np.mean(y_true)))\n    r2 = 1 - ssr / sst\n    r2 = round(r2, 3)\n    return r2"}
{"task_id": 69, "completion_id": 8, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    rss = np.sum((y_true - y_pred) ** 2)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    r_sq = 1 - rss / tss\n    r_sq = round(r_sq, 3)\n    return r_sq"}
{"task_id": 69, "completion_id": 9, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true_mean = np.mean(y_true)\n    total_sum_of_squares = np.sum((y_true - y_true_mean) ** 2)\n    residual_sum_of_squares = np.sum((y_true - y_pred) ** 2)\n    r_squared = 1 - residual_sum_of_squares / total_sum_of_squares\n    return round(r_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if len(img) == 0:\n        return -1\n    length = len(img[0])\n    for row in img:\n        if len(row) != length:\n            return -1\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    total = 0\n    count = 0\n    for row in img:\n        total += sum(row)\n        count += len(row)\n    avg = total / count\n    return round(avg, 2)"}
{"task_id": 70, "completion_id": 1, "solution": "def calculate_brightness(img):\n    try:\n        if not img:\n            return -1\n        row_length = len(img[0])\n        if any((len(row) != row_length for row in img)):\n            return -1\n        if any((pixel_value not in range(0, 256) for row in img for pixel_value in row)):\n            return -1\n        pixel_sum = sum((sum(row) for row in img))\n        total_pixels = len(img) * len(img[0])\n        average_brightness = round(pixel_sum / total_pixels, 2)\n        return average_brightness\n    except Exception as e:\n        print(f'An error occurred: {e}')\n        return -1"}
{"task_id": 70, "completion_id": 2, "solution": "def calculate_brightness(img):\n    if len(img) == 0:\n        return -1\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    pixel_sum = 0\n    pixel_count = 0\n    for row in img:\n        pixel_sum += sum(row)\n        pixel_count += len(row)\n    avg_brightness = pixel_sum / pixel_count\n    return round(avg_brightness, 2)"}
{"task_id": 70, "completion_id": 3, "solution": "def calculate_brightness(img):\n    if len(img) == 0:\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0])\n    for r in img:\n        if len(r) != num_cols:\n            return -1\n    pixel_sum = 0\n    for r in range(num_rows):\n        for c in range(num_cols):\n            pixel_value = img[r][c]\n            if pixel_value < 0 or pixel_value > 255:\n                return -1\n            pixel_sum += pixel_value\n    num_pixels = num_rows * num_cols\n    avg_brightness = pixel_sum / num_pixels\n    return round(avg_brightness, 2)"}
{"task_id": 70, "completion_id": 4, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0])\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    avg_brightness = total_brightness / (num_rows * num_cols)\n    avg_brightness = round(avg_brightness, 2)\n    return avg_brightness"}
{"task_id": 70, "completion_id": 5, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_pixels += 1\n            total_brightness += pixel\n    if total_pixels:\n        return round(total_brightness / total_pixels, 2)\n    else:\n        return -1"}
{"task_id": 70, "completion_id": 6, "solution": "def calculate_brightness(img):\n    if len(img) == 0:\n        return -1\n    row_length = len(img[0])\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        if len(row) != row_length or not all((0 <= p <= 255 for p in row)):\n            return -1\n        total_brightness += sum(row)\n        pixel_count += len(row)\n    return round(total_brightness / pixel_count, 2)"}
{"task_id": 70, "completion_id": 7, "solution": "def calculate_brightness(img):\n    if len(img) == 0:\n        return -1\n    row_lengths = [len(row) for row in img]\n    if min(row_lengths) != max(row_lengths):\n        return -1\n    for row in img:\n        for value in row:\n            if value < 0 or value > 255:\n                return -1\n    brightness = sum([sum(row) for row in img]) / (len(img) * len(img[0]))\n    return round(brightness, 2)"}
{"task_id": 70, "completion_id": 8, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    row_len = len(img[0])\n    for row in img:\n        if len(row) != row_len:\n            return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    avg_brightness = total_brightness / pixel_count\n    return round(avg_brightness, 2)"}
{"task_id": 70, "completion_id": 9, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    row_len = len(img[0])\n    for row in img:\n        if len(row) != row_len:\n            return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    avg_brightness = total_brightness / num_pixels\n    return round(avg_brightness, 2)\nimg = []"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Inputs should be numpy arrays')\n    if len(y_true.shape) != len(y_pred.shape) or y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('Arrays must have the same shape')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Arrays must not be empty')\n    differences = y_true - y_pred\n    sum_of_squared_differences = np.sum(differences ** 2)\n    mean_of_squared_differences = sum_of_squared_differences / y_true.size\n    rmse_value = np.sqrt(mean_of_squared_differences)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 1, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate Root Mean Square Error (RMSE).\n\n    Parameters:\n    y_true (array-like): True target values.\n    y_pred (array-like): Predicted target values.\n\n    Returns:\n    float: The Root Mean Square Error (RMSE) value rounded to three decimal places.\n    \"\"\"\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise TypeError('Inputs must be array-like objects.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Arrays must have the same shape.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return round(rmse, 3)"}
{"task_id": 71, "completion_id": 2, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    try:\n        (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n        if y_true.shape != y_pred.shape:\n            raise ValueError('Mismatched array shapes')\n        if y_true.size == 0:\n            raise ValueError('Empty arrays')\n    except ValueError as ve:\n        print('ValueError:', ve)\n        return None\n    except Exception as e:\n        print('Exception:', e)\n        return None\n    return np.sqrt(np.mean((y_true - y_pred) ** 2))\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 71, "completion_id": 3, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('Mismatched array shapes: y_true and y_pred must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Empty arrays are not accepted.')\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise ValueError('Invalid input types. The inputs must be either lists or numpy arrays.')\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse, 3)"}
{"task_id": 71, "completion_id": 4, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if isinstance(y_true, list):\n        y_true = np.array(y_true)\n    if isinstance(y_pred, list):\n        y_pred = np.array(y_pred)\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Inputs should be either Python lists or numpy arrays.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('The input arrays should have the same length.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 5, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise ValueError('Both y_true and y_pred must be either a list or a numpy array')\n    (y_true, y_pred) = (np.array(y_true).astype(float), np.array(y_pred).astype(float))\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    n = y_true.size\n    if n == 0:\n        raise ValueError('Both y_true and y_pred must not be empty')\n    mse = np.sum((y_true - y_pred) ** 2) / n\n    rmse = np.sqrt(mse)\n    return round(rmse, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 71, "completion_id": 6, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    n = y_true.shape[0]\n    if n > 0:\n        return round(np.sqrt(np.sum(np.square(y_true - y_pred)) / n), 3)\n    else:\n        raise ValueError('Input arrays must not be empty.')"}
{"task_id": 71, "completion_id": 7, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        if y_true.size == 0 or y_pred.size == 0:\n            return 0.0\n        if y_true.shape != y_pred.shape:\n            raise ValueError('Input arrays have different shapes.')\n        return round(np.sqrt(np.mean((y_true - y_pred) ** 2)), 3)\n    except ValueError as ve:\n        print('ValueError:', ve)\n        return None\n    except TypeError:\n        print('TypeError: Input should be array-like')\n        return None\n    except Exception as e:\n        print('Unexpected error:', e)\n        return None"}
{"task_id": 71, "completion_id": 8, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if y_true.shape != y_pred.shape or len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Input arrays must have the same non-zero length')\n    if not (isinstance(y_true[0], (int, float)) and isinstance(y_pred[0], (int, float))):\n        raise TypeError('Input arrays must contain numeric elements')\n    n = len(y_true)\n    sum_of_squares = np.sum(np.square(y_true - y_pred))\n    rmse = np.sqrt(sum_of_squares / n)\n    return round(rmse, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 71, "completion_id": 9, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    try:\n        (y_true, y_pred) = (np.array(y_true).astype(float), np.array(y_pred).astype(float))\n        if y_true.shape != y_pred.shape:\n            raise ValueError('Input arrays must have the same shape.')\n        if y_true.size == 0 or y_pred.size == 0:\n            raise ValueError('Input arrays must not be empty.')\n        square_err = np.square(y_true - y_pred)\n        mean_square_err = np.mean(square_err)\n        root_mean_square_err = np.sqrt(mean_square_err)\n        return round(root_mean_square_err, 3)\n    except ValueError as ve:\n        print(f'ValueError: {ve}')\n        return None\n    except TypeError as te:\n        print(f'TypeError: {te}')\n        return None"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    if union == 0:\n        return 0.0\n    return round(intersection / union, 3)\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 1]\ny_pred = [0, 0, 0, 1, 1, 1, 0, 1, 1]"}
{"task_id": 72, "completion_id": 1, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard_sim = intersection / union\n    return round(jaccard_sim, 3)"}
{"task_id": 72, "completion_id": 2, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true.shape) > 1 or len(y_pred.shape) > 1:\n        raise ValueError('Input arrays should be one-dimensional')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays should have the same shape')\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    return round(intersection / union, 3) if union != 0 else 0"}
{"task_id": 72, "completion_id": 3, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    jaccard_index = intersection.sum() / float(union.sum())\n    return round(jaccard_index, 3)"}
{"task_id": 72, "completion_id": 4, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    jaccard_index = intersection.sum() / float(union.sum())\n    return round(jaccard_index, 3)"}
{"task_id": 72, "completion_id": 5, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate Jaccard Index for Binary Classification\n\n    Parameters:\n    y_true (array): True binary labels\n    y_pred (array): Predicted binary labels\n\n    Returns:\n    Jaccard Index (float): The intersection divided by the union of both arrays.\n    \"\"\"\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(np.clip(y_true + y_pred, 0, 1))\n    if union == 0:\n        return 1.0\n    jaccard_index = intersection / union\n    return round(jaccard_index, 3)"}
{"task_id": 72, "completion_id": 6, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    if set(np.unique(y_true)) != {0, 1} or set(np.unique(y_pred)) != {0, 1}:\n        raise ValueError(\"Input arrays should only contain binary elements (0's and 1's).\")\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    jaccard_index = np.sum(intersection) / np.sum(union)\n    return round(jaccard_index, 3)\ny_true = np.array([1, 1, 0, 1, 0, 1, 0, 0, 0, 0])\ny_pred = np.array([0, 1, 0, 1, 0, 0, 1, 0, 0, 0])"}
{"task_id": 72, "completion_id": 7, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    if union == 0:\n        return 0.0\n    return round(intersection / union, 3)"}
{"task_id": 72, "completion_id": 8, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    jaccard = np.sum(intersection) / np.sum(union)\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 9, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true) + np.sum(y_pred) - intersection\n    if union == 0:\n        return 0.0\n    return round(intersection / union, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred)\n    num_intersection = np.sum(intersection)\n    num_union = np.sum(y_true) + np.sum(y_pred)\n    if num_union == 0:\n        return 0.0\n    dice_score = 2.0 * num_intersection / num_union\n    return round(dice_score, 3)\ny_true = np.array([1, 1, 0, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 1, 0, 1])"}
{"task_id": 73, "completion_id": 1, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = y_true.astype(bool)\n    y_pred = y_pred.astype(bool)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        print('No true or predicted positives found')\n        return 0\n    return round(2 * intersection / union, 3)"}
{"task_id": 73, "completion_id": 2, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true) > 0\n    y_pred = np.array(y_pred) > 0\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    dice_score = 2.0 * intersection.sum() / (union.sum() + intersection.sum())\n    return round(dice_score, 3)\ny_true = [0, 1, 0, 1, 0, 1, 0]\ny_pred = [1, 1, 1, 0, 0, 1, 0]"}
{"task_id": 73, "completion_id": 3, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred)\n    numerator = 2.0 * np.sum(intersection)\n    denominator = np.sum(y_true) + np.sum(y_pred)\n    if denominator == 0:\n        return 0.0\n    dice = numerator / denominator\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 4, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        return 'Error: Arrays must have the same length'\n    if not np.array_equal(np.unique(y_true), [0, 1]) or not np.array_equal(np.unique(y_pred), [0, 1]):\n        return 'Error: Arrays must contain only binary values'\n    intersection = np.logical_and(y_true, y_pred)\n    dice_score = 2.0 * intersection.sum() / (y_true.sum() + y_pred.sum())\n    return round(dice_score, 3)"}
{"task_id": 73, "completion_id": 5, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true, dtype=bool)\n    y_pred = np.array(y_pred, dtype=bool)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = y_true.sum() + y_pred.sum()\n    if union == 0:\n        return 1 if intersection == 0 else 0\n    dice_score = 2.0 * intersection / union\n    return round(dice_score, 3)"}
{"task_id": 73, "completion_id": 6, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true: binary array of the same length, representing true labels.\n    y_pred: binary array of the same length, representing predicted labels.\n\n    Returns:\n    Float value rounded to 3 decimal places, representing the Dice Score.\n    \"\"\"\n    intersection = np.logical_and(y_true, y_pred)\n    dice_score = 2.0 * intersection.sum() / (y_true.sum() + y_pred.sum())\n    if intersection.sum() == 0 and y_true.sum() == 0:\n        return 1.0\n    return round(dice_score, 3)"}
{"task_id": 73, "completion_id": 7, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred)\n    true_positives = np.sum(intersection)\n    sum_predictions = np.sum(y_pred)\n    sum_ground_truth = np.sum(y_true)\n    if true_positives == 0:\n        return 0.0\n    dice_score = 2.0 * true_positives / (sum_ground_truth + sum_predictions)\n    return round(dice_score, 3)"}
{"task_id": 73, "completion_id": 8, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    assert len(y_true) == len(y_pred), 'Lengths of y_true and y_pred should be equal'\n    y_true_pos = y_true == 1\n    y_pred_pos = y_pred == 1\n    true_pos = np.sum(y_true_pos & y_pred_pos)\n    false_pos = np.sum(~y_true_pos & y_pred_pos)\n    false_neg = np.sum(y_true_pos & ~y_pred_pos)\n    denominator = true_pos * 2.0 + false_pos + false_neg if true_pos > 0 or false_pos > 0 or false_neg > 0 else 1.0\n    dice = true_pos * 2.0 / denominator\n    dice = round(dice, 3)\n    return dice"}
{"task_id": 73, "completion_id": 9, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true, dtype=bool)\n    y_pred = np.array(y_pred, dtype=bool)\n    true_positive = np.sum(y_true & y_pred)\n    total_positive = np.sum(y_true) + np.sum(y_pred)\n    if total_positive == 0:\n        return 1.0\n    dice_score = 2.0 * true_positive / total_positive\n    return round(dice_score, 3)\ny_true = [0, 0, 1, 1, 1, 0, 1, 0, 0]\ny_pred = [0, 1, 1, 0, 1, 0, 0, 1, 0]"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\nimport hyperspace as hyp\ndef create_row_hv(row, dim, random_seeds):\n    H = hyp.create(dim)\n    hv_row = np.zeros(H.dim())\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_binding = H.random_Hv(seed=feature_seed)\n        value_binding = np.zeros(H.dim())\n        value_binding[H.encode(str(value))] = 1\n        hv_row += feature_binding.bdot(value_binding)\n    return hv_row.tolist()"}
{"task_id": 74, "completion_id": 1, "solution": "import numpy as np\nfrom hypervector import HyperVector\nfrom random import Random\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary \n    to ensure reproducibility.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n\n    Input:\n    - `row`: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - `dim`: The dimensionality of the hypervectors.\n    - `random_seeds`: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n\n    Output:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n    hv = HyperVector(dimension=dim)\n    for (feature, val) in row.items():\n        seed = random_seeds.get(feature, None)\n        if seed is None:\n            seed = np.random.randint(0, 10000)\n            random_seeds[feature] = seed\n        rng = Random(seed)\n        feature_hv = HyperVector(dimension=dim, rand_func=rng.random)\n        hv.bind(feature_hv)\n    hv_arr = np.array(hv.components)\n    return hv_arr.tolist()"}
{"task_id": 74, "completion_id": 2, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    from hdc import hypervector as hv\n    composite_hv = hv.Hypervector(dn=dim)\n    for (feature, seed) in random_seeds.items():\n        np.random.seed(seed)\n        if feature in row:\n            feature_hv = hv.bind(hv.utils.random(dn=dim), str(feature))\n            value_hv = hv.bind(hv.utils.random(dn=dim), str(row[feature]))\n            composite_hv = hv.bundle(composite_hv, feature_hv)\n            composite_hv = hv.bundle(composite_hv, value_hv)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 3, "solution": "import numpy as np\nimport random\ndef create_hypervector(dim, seed):\n    \"\"\"\n    Creates a hypervector of provided dimension using the provided seed.\n    \"\"\"\n    np.random.seed(seed)\n    return np.random.random(dim) - 0.5\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a dataset row using the HDC approach.\n    \"\"\"\n    row_hv = None\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_hv = create_hypervector(dim, feature_seed)\n        row_seed = hash(feature + str(value))\n        value_hv = create_hypervector(dim, row_seed)\n        if row_hv is None:\n            row_hv = feature_hv + value_hv\n        else:\n            row_hv += feature_hv + value_hv\n    return row_hv.tolist()\ndim = 10\nrandom_seeds = {'feature1': 1, 'feature2': 2, 'feature3': 3}"}
{"task_id": 74, "completion_id": 4, "solution": "import numpy as np\nimport hypervector as hv\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = hv.hv(dim)\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        np.random.seed(seed)\n        feature_hv = hv.hv(np.random.normal(size=dim))\n        value_hv = hv.hv(np.random.normal(size=dim))\n        composite_hv = composite_hv + feature_hv * value_hv\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 5, "solution": "import numpy as np\nfrom hypervector import HV\ndef create_row_hv(row, dim, random_seeds):\n    hv_row = HV(dim=dim)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, 0)\n        np.random.seed(seed)\n        hv_feature = HV(dim=dim, value=feature)\n        hv_value = HV(dim=dim, value=str(value))\n        hv_row = hv_row + hv_feature + hv_value\n    return hv_row.tolist()\ndim = 50\nrandom_seeds = {'feature1': 10, 'feature2': 20}\nrow = {'feature1': 1, 'feature2': 'value2'}\nhv_row = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 6, "solution": "import numpy as np\nimport random\ndef Hypervector(dim):\n    \"\"\"Generates a Hypervector of specified dimension\"\"\"\n    H = np.zeros(dim)\n    random.seed(1)\n    indices = random.sample(range(0, dim), dim // 2)\n    H[indices] = 1\n    return H\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generates a composite hypervector for a dataset row\"\"\"\n    composite_hv = np.zeros(dim)\n    for (key, value) in row.items():\n        seed = random_seeds[key]\n        random.seed(seed)\n        H_feature_name = Hypervector(dim)\n        H_feature_value = Hypervector(dim)\n        np.random.seed(seed)\n        composite_hv += H_feature_name * ord(key) + H_feature_value * value\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 7, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    row_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_seed = np.random.RandomState(random_seeds[feature_name])\n        feature_hv = feature_seed.normal(scale=1.0 / np.sqrt(dim), size=dim)\n        feature_value = int(feature_value)\n        feature_start = hash(feature_name) % dim\n        row_hv[feature_start:feature_start + len(feature_hv)] = feature_hv\n    return row_hv.tolist()"}
{"task_id": 74, "completion_id": 8, "solution": "import numpy as np\nfrom pyhdc import HDC\ndef create_row_hv(row, dim, random_seeds):\n    hdc = HDC(dim)\n    row_hv = hdc.null()\n    for (feature, value) in row.items():\n        feature_seed = random_seeds.get(feature)\n        hdc.seed(feature_seed)\n        feature_hv = hdc.alloc(feature)\n        hdc.seed(value)\n        value_hv = hdc.alloc(str(value))\n        binding_hv = feature_hv + value_hv\n        row_hv += binding_hv\n    return row_hv.tolist()"}
{"task_id": 74, "completion_id": 9, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    from emd import emd\n    hypervectors = []\n    for (feature, value) in row.items():\n        value_str = str(value)\n        feature_seed = np.random.RandomState(random_seeds[feature])\n        feature_vector = feature_seed.rand(dim)\n        value_seed = np.random.RandomState(hash(value_str))\n        value_vector = value_seed.rand(dim)\n        composite_hv = np.multiply(feature_vector, value_vector)\n        hypervectors.append(composite_hv)\n    sum_hv = np.sum(hypervectors, axis=0)\n    return sum_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for entry in data:\n        y_true = entry[0]\n        y_pred = entry[1]\n        if y_true == 1 and y_pred == 1:\n            true_positives += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negatives += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positives += 1\n        else:\n            false_negatives += 1\n    return [[true_negatives, false_positives], [false_negatives, true_positives]]\ndata = [[1, 1], [0, 1], [0, 0], [1, 1], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0], [0, 0], [0, 1], [1, 1]]"}
{"task_id": 75, "completion_id": 1, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            matrix[0][0] += 1\n        elif y_true == 0 and y_pred == 0:\n            matrix[1][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            matrix[1][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            matrix[0][1] += 1\n    return matrix\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [1, 1], [0, 1], [1, 0], [1, 1]]"}
{"task_id": 75, "completion_id": 2, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    matrix = [[0, 0], [0, 0]]\n    for item in data:\n        (y_true, y_pred) = (item[0], item[1])\n        matrix[y_true][y_pred] += 1\n    return matrix\ndata = [[0, 1], [1, 0], [1, 1], [0, 0], [1, 0], [1, 1], [0, 1], [0, 0], [1, 1]]"}
{"task_id": 75, "completion_id": 3, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    tp = fp = tn = fn = 0\n    for pair in data:\n        (y_true, y_pred) = pair\n        if y_true == 1 and y_pred == 1:\n            tp += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n    return [[tn, fp], [fn, tp]]\ndata = [[1, 1], [0, 1], [0, 0], [1, 0]]"}
{"task_id": 75, "completion_id": 4, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    (true_positive, true_negative, false_positive, false_negative) = (0, 0, 0, 0)\n    for pair in data:\n        if pair[0] == pair[1] == 1:\n            true_positive += 1\n        elif pair[0] == pair[1] == 0:\n            true_negative += 1\n        elif pair[0] == 0 and pair[1] == 1:\n            false_positive += 1\n        else:\n            false_negative += 1\n    return [[true_negative, false_positive], [false_negative, true_positive]]"}
{"task_id": 75, "completion_id": 5, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = sum((1 for pair in data if pair[0] == pair[1] == 1))\n    false_positive = sum((1 for pair in data if pair[0] == 0 and pair[1] == 1))\n    true_negative = sum((1 for pair in data if pair[0] == pair[1] == 0))\n    false_negative = sum((1 for pair in data if pair[0] == 1 and pair[1] == 0))\n    return [[true_negative, false_positive], [false_negative, true_positive]]\ndata = [[1, 0], [1, 1], [0, 0], [1, 1], [0, 1], [0, 0], [1, 1], [0, 1], [0, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 6, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    confusion_matrix = [[0, 0], [0, 0]]\n    for pair in data:\n        y_true = pair[0]\n        y_pred = pair[1]\n        confusion_matrix[y_true][y_pred] += 1\n    return confusion_matrix\ndata = [[1, 1], [0, 0], [1, 1], [0, 1], [0, 0], [1, 0]]"}
{"task_id": 75, "completion_id": 7, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    :param data: a list of lists, where each inner list holds y_true and y_pred\n    :return: a 2x2 confusion matrix represented as a list of lists\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true not in (0, 1) or y_pred not in (0, 1):\n            raise ValueError('y_true and y_pred must be either 0 or 1')\n        matrix[y_true][y_pred] += 1\n    return matrix\ndata = [[0, 1], [1, 0], [0, 1], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef confusion_matrix(data: List[Tuple[int, int]]) -> np.ndarray:\n    \"\"\"\n    This function takes a list of tuples, where each tuple represents a binary classification prediction.\n    It returns a confusion matrix for the data.\n    \"\"\"\n    matrix = np.zeros((2, 2))\n    for item in data:\n        matrix[item[0]][item[1]] += 1\n    return matrix\ndata = [(0, 0), (0, 1), (1, 0), (1, 1), (0, 0), (1, 1), (0, 0), (1, 1)]"}
{"task_id": 75, "completion_id": 9, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    y_true = [y[0] for y in data]\n    y_pred = [y[1] for y in data]\n    counts = Counter(zip(y_true, y_pred))\n    tn = counts.get((0, 0), 0)\n    fp = counts.get((0, 1), 0)\n    fn = counts.get((1, 0), 0)\n    tp = counts.get((1, 1), 0)\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors cannot be zero vectors')\n    similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    similarity = round(similarity, 3)\n    return similarity"}
{"task_id": 76, "completion_id": 1, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    assert v1.shape == v2.shape, 'Input vectors must have the same shape.'\n    assert v1.size != 0 and v2.size != 0, 'Input vectors cannot be empty.'\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    assert magnitude_v1 != 0 and magnitude_v2 != 0, 'Input vectors cannot have zero magnitude.'\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)"}
{"task_id": 76, "completion_id": 2, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    assert v1.shape == v2.shape, 'Both input vectors must have the same shape'\n    assert np.linalg.norm(v1) != 0, 'Input vector v1 cannot be zero magnitude'\n    assert np.linalg.norm(v2) != 0, 'Input vector v2 cannot be zero magnitude'\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 3, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    assert v1.shape == v2.shape, 'Both input vectors must have the same shape!'\n    assert np.linalg.norm(v1) * np.linalg.norm(v2) != 0, 'Vectors cannot be empty or have zero magnitude!'\n    dot_product = np.dot(v1, v2)\n    magnitude_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n    similarity = dot_product / magnitude_product\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 4, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        raise ValueError('Input vectors cannot be empty or have zero magnitude.')\n    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    return np.round(cos_sim, 3)"}
{"task_id": 76, "completion_id": 5, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        return 'Input vectors must have same shape.'\n    if not v1.size or not v2.size:\n        return 'Input vectors cannot be empty.'\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        return 'Input vectors cannot have zero magnitude.'\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity, 3)"}
{"task_id": 76, "completion_id": 6, "solution": "import numpy as np\nimport numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.size != v2.size:\n        return 'Both vectors should have the same size'\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        return 'Vectors cannot be empty or have zero magnitude'\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 7, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    return round(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)), 3)"}
{"task_id": 76, "completion_id": 8, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape[0] == 0 or v2.shape[0] == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        print('Error: Input vectors cannot be empty or have zero magnitude')\n        return None\n    if v1.shape != v2.shape:\n        print('Error: Both input vectors must have the same shape.')\n        return None\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    similarity = round(similarity, 3)\n    return similarity"}
{"task_id": 76, "completion_id": 9, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    assert v1.shape == v2.shape, 'Both vectors must have the same shape.'\n    assert np.any(v1) and np.any(v2), 'Input vectors should not be empty.'\n    assert not np.allclose(v1, np.zeros(v1.shape)) and (not np.allclose(v2, np.zeros(v2.shape))), 'Input vectors should not have zero magnitude.'\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    cosine_similarity = dot_product / (norm_v1 * norm_v2)\n    return round(cosine_similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert all((val in [0, 1] for val in actual)), '`actual` values must be either 0 or 1!'\n    assert all((val in [0, 1] for val in predicted)), '`predicted` values must be either 0 or 1!'\n    assert len(actual) == len(predicted), '`actual` and `predicted` must have the same length!'\n    actual = np.array(actual)\n    predicted = np.array(predicted)\n    confusion_matrix = confusion_matrix(actual, predicted).ravel()\n    accuracy = round(np.sum(actual == predicted) / len(actual), 3)\n    tp = confusion_matrix[3]\n    fp = confusion_matrix[1]\n    fn = confusion_matrix[2]\n    precision = round(tp / (tp + fp), 3)\n    recall = round(tp / (tp + fn), 3)\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3)\n    tn = confusion_matrix[0]\n    fp = confusion_matrix[1]\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 1, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert len(actual) == len(predicted), 'Length of actual and predicted lists must be the same'\n    assert set(actual).issubset({0, 1}) and set(predicted).issubset({0, 1}), 'Lists should only contain 0 and 1'\n    (tn, fp, fn, tp) = [0, 0, 0, 0]\n    for (a, p) in zip(actual, predicted):\n        if a == p:\n            if a == 0:\n                tn += 1\n            else:\n                tp += 1\n        elif a == 0:\n            fp += 1\n        else:\n            fn += 1\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = round(tp / (tp + fp) if tp + fp != 0 else 0, 3)\n    recall = round(tp / (tp + fn) if tp + fn != 0 else 0, 3)\n    f1_score = round(2 * precision * recall / (precision + recall) if precision + recall != 0 else 0, 3)\n    specificity = round(tn / (tn + fp) if tn + fp != 0 else 0, 3)\n    negative_predictive_value = round(tn / (tn + fn) if tn + fn != 0 else 0, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 2, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert len(actual) == len(predicted), 'Both lists must have the same length.'\n    assert all((i in [0, 1] for i in actual)), \"All elements in the 'actual' list must be either 0 or 1.\"\n    assert all((i in [0, 1] for i in predicted)), \"All elements in the 'predicted' list must be either 0 or 1.\"\n    conf_matrix = confusion_matrix(actual, predicted)\n    accuracy = (conf_matrix[0, 0] + conf_matrix[1, 1]) / conf_matrix.sum()\n    f1 = f1_score(actual, predicted)\n    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n    negative_predictive_value = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])\n    accuracy = round(accuracy, 3)\n    f1 = round(f1, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (conf_matrix, accuracy, f1, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 3, "solution": "from collections import Counter\nfrom sklearn import metrics\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert len(actual) == len(predicted), 'The actual and predicted lists should have the same length.'\n    for val in actual + predicted:\n        assert val in [0, 1], 'All elements in the actual and predicted lists must be either 0 or 1.'\n    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n    accuracy = round(metrics.accuracy_score(actual, predicted), 3)\n    f1_score = round(metrics.f1_score(actual, predicted), 3)\n    (tn, fp, fn, tp) = confusion_matrix.ravel()\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 4, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    (TN, FP, FN, TP) = confusion_matrix(actual, predicted).ravel()\n    confusion_matrix_val = [[TN, FP], [FN, TP]]\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    f1_score_val = f1_score(actual, predicted)\n    specificity = TN / (TN + FP)\n    NPV = TN / (TN + FN)\n    return (confusion_matrix_val, round(accuracy, 3), round(f1_score_val, 3), round(specificity, 3), round(NPV, 3))"}
{"task_id": 77, "completion_id": 5, "solution": "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    conf_matrix = confusion_matrix(actual, predicted)\n    accuracy = accuracy_score(actual, predicted)\n    f1 = f1_score(actual, predicted)\n    spec = recall_score(actual, predicted, pos_label=0)\n    precision = precision_score(actual, predicted, pos_label=0)\n    negativePredictiveValue = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0])\n    return (conf_matrix, round(accuracy, 3), round(f1, 3), round(spec, 3), round(negativePredictiveValue, 3))"}
{"task_id": 77, "completion_id": 6, "solution": "from collections import Counter\nimport math\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i] == 1:\n            true_positives += 1\n        elif actual[i] == predicted[i] == 0:\n            true_negatives += 1\n        elif actual[i] == 1 and predicted[i] == 0:\n            false_negatives += 1\n        else:\n            false_positives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives != 0 else 0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives != 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 7, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    (tn, fp, fn, tp) = [0, 0, 0, 0]\n    for (a, p) in zip(actual, predicted):\n        if a == p:\n            if a == 1:\n                tp += 1\n            else:\n                tn += 1\n        elif a == 1:\n            fn += 1\n        else:\n            fp += 1\n    confusion_matrix = np.array([[tp, fp], [fn, tn]])\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    specificity = tn / (tn + fp) if tn + fp != 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn != 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 8, "solution": "from collections import Counter\nfrom sklearn import metrics\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    (tn, fp, fn, tp) = metrics.confusion_matrix(actual, predicted).ravel()\n    confusion_matrix = [[tp, fp], [fn, tn]]\n    accuracy = round(metrics.accuracy_score(actual, predicted), 3)\n    f1_score = round(metrics.f1_score(actual, predicted), 3)\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 9, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert all((i in [0, 1] for i in actual)) and all((i in [0, 1] for i in predicted)), 'All elements in the input lists must be either 0 or 1.'\n    assert len(actual) == len(predicted), 'Both lists must have the same length.'\n    cm = confusion_matrix(actual, predicted)\n    (tn, fp, fn, tp) = cm.ravel()\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    f1_score_value = round(f1_score(actual, predicted), 3)\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3)\n    return (cm, accuracy, f1_score_value, specificity, negative_predictive_value)\nactual = [1, 1, 0, 1, 0, 1, 1, 0, 0, 0]\npredicted = [1, 0, 0, 1, 0, 1, 0, 0, 1, 1]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    (p25, p50, p75) = np.percentile(data, [25, 50, 75])\n    iqr = p75 - p25\n    return {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}"}
{"task_id": 78, "completion_id": 1, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    mean = round(np.mean(data), 4)\n    median = round(np.median(data), 4)\n    mode = round(stats.mode(data)[0][0], 4)\n    variance = round(np.var(data), 4)\n    standard_deviation = round(np.std(data), 4)\n    percentile_25 = round(np.percentile(data, 25), 4)\n    percentile_50 = round(np.percentile(data, 50), 4)\n    percentile_75 = round(np.percentile(data, 75), 4)\n    iqr = round(percentile_75 - percentile_25, 4)\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\ndata = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 6, 6]"}
{"task_id": 78, "completion_id": 2, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    stats_dict = {}\n    stats_dict['mean'] = round(np.mean(data), 4)\n    stats_dict['median'] = round(np.median(data), 4)\n    (unique_vals, counts) = np.unique(data, return_counts=True)\n    mode_index = counts.argmax()\n    stats_dict['mode'] = unique_vals[mode_index]\n    stats_dict['variance'] = round(np.var(data), 4)\n    stats_dict['standard_deviation'] = round(np.std(data), 4)\n    stats_dict['25th_percentile'] = round(np.percentile(data, 25), 4)\n    stats_dict['50th_percentile'] = round(np.percentile(data, 50), 4)\n    stats_dict['75th_percentile'] = round(np.percentile(data, 75), 4)\n    stats_dict['interquartile_range'] = round(stats_dict['75th_percentile'] - stats_dict['25th_percentile'], 4)\n    return stats_dict"}
{"task_id": 78, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = Counter(data).most_common(1)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    percentile_25th = np.percentile(data, 25)\n    percentile_50th = np.percentile(data, 50)\n    percentile_75th = np.percentile(data, 75)\n    iqr = percentile_75th - percentile_25th\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(percentile_25th, 4), '50th_percentile': round(percentile_50th, 4), '75th_percentile': round(percentile_75th, 4), 'interquartile_range': round(iqr, 4)}\n    return result"}
{"task_id": 78, "completion_id": 4, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\n    return stats_dict\ndata = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 6, 7, 8, 9, 9, 10]"}
{"task_id": 78, "completion_id": 5, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_dev = np.std(data)\n    (p25, p50, p75) = np.percentile(data, [25, 50, 75])\n    interquartile = p75 - p25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_dev, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(interquartile, 4)}\n    return stats_dict"}
{"task_id": 78, "completion_id": 6, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    percentiles = np.percentile(data, [25, 50, 75])\n    (pct25, pct50, pct75) = percentiles\n    iqr = pct75 - pct25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(pct25, 4), '50th_percentile': round(pct50, 4), '75th_percentile': round(pct75, 4), 'interquartile_range': round(iqr, 4)}\n    return stats_dict\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"}
{"task_id": 78, "completion_id": 7, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    twenty_fifth_percentile = np.percentile(data, 25)\n    fifty_percentile = np.percentile(data, 50)\n    seventy_fifth_percentile = np.percentile(data, 75)\n    interquartile_range = seventy_fifth_percentile - twenty_fifth_percentile\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(twenty_fifth_percentile, 4), '50th_percentile': round(fifty_percentile, 4), '75th_percentile': round(seventy_fifth_percentile, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return result"}
{"task_id": 78, "completion_id": 8, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    output = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(interquartile_range, 4)}\n    return output"}
{"task_id": 78, "completion_id": 9, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    (p25, p50, p75) = np.percentile(data, [25, 50, 75])\n    iqr = p75 - p25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return stats_dict"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    prob = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)"}
{"task_id": 79, "completion_id": 1, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coeff = math.comb(n, k)\n    success_prob = math.pow(p, k)\n    failure_prob = math.pow(1 - p, n - k)\n    prob = binomial_coeff * success_prob * failure_prob\n    return round(prob, 5)"}
{"task_id": 79, "completion_id": 2, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coeff = math.comb(n, k)\n    probability = binomial_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 3, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n   Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n   each with probability p of success, using the Binomial distribution formula.\n   :param n: Total number of trials\n   :param k: Number of successes\n   :param p: Probability of success on each trial\n   :return: Probability of k successes in n trials\n   \"\"\"\n    cnk = math.comb(n, k)\n    probability = cnk * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 5\np = 0.5\nprobability = binomial_probability(n, k, p)"}
{"task_id": 79, "completion_id": 4, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    nCk = math.comb(n, k)\n    prob = nCk * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)"}
{"task_id": 79, "completion_id": 5, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coeff = math.comb(n, k)\n    prob = binomial_coeff * p ** k * (1 - p) ** (n - k)\n    prob = round(prob, 5)\n    return prob"}
{"task_id": 79, "completion_id": 6, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coef = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    probability = binom_coef * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 7, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if k > n or n < 0 or k < 0 or (p < 0) or (p > 1):\n        return None\n    Cnk = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    p_to_k = math.pow(p, k)\n    q_to_n_minus_k = math.pow(1 - p, n - k)\n    P_X_equals_k = Cnk * p_to_k * q_to_n_minus_k\n    return round(P_X_equals_k, 5)"}
{"task_id": 79, "completion_id": 8, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    probability = math.factorial(n) / (math.factorial(k) * math.factorial(n - k)) * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 9, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    success_prob = p ** k\n    fail_prob = (1 - p) ** (n - k)\n    prob = binom_coeff * success_prob * fail_prob\n    return round(prob, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponential = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(std_dev, 2))))\n    pdf = coefficient * exponential\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 1, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF of the normal distribution at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -1 * (x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 2, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -1 / 2 * ((x - mean) / std_dev) ** 2\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 3, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    term1 = 1 / (std_dev * math.sqrt(2 * math.pi))\n    term2 = math.exp(-0.5 * ((x - mean) / std_dev) ** 2)\n    return round(term1 * term2, 5)"}
{"task_id": 80, "completion_id": 4, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2.0 / (2 * std_dev ** 2.0))\n    pdf = coeff * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 5, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    term = 1 / (std_dev * (2 * math.pi) ** 0.5)\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = term * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 6, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coeff = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -1 * ((x - mean) ** 2 / (2 * std_dev ** 2))\n    return round(coeff * math.exp(exponent), 5)"}
{"task_id": 80, "completion_id": 7, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nmean = 0\nstd_dev = 1\nx = 1\npdf_value = normal_pdf(x, mean, std_dev)"}
{"task_id": 80, "completion_id": 8, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    coefficient = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2 / (2 * std_dev ** 2))\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 9, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: PDF calculated value at x.\n    \"\"\"\n    pi = 3.14159\n    coefficient = 1.0 / (std_dev * math.sqrt(2 * pi))\n    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(std_dev, 2))))\n    return round(coefficient * exponent, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    poisson_prob = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(poisson_prob, 5)"}
{"task_id": 81, "completion_id": 1, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k should be a non-negative integer.')\n    if lam < 0:\n        raise ValueError('Lambda (Mean rate) should be a non-negative number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 2, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    prob = lam ** k * math.exp(-lam) / math.factorial(k)\n    prob = round(prob, 5)\n    return prob"}
{"task_id": 81, "completion_id": 3, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)\nk = 3\nlam = 2.5"}
{"task_id": 81, "completion_id": 4, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if not isinstance(k, int) or not isinstance(lam, (int, float)) or k < 0 or (lam < 0):\n        raise ValueError('Input values should be non-negative integers or floats.')\n    return round(math.exp(-lam) * lam ** k / math.factorial(k), 5)\nk = 5\nlam = 3.5"}
{"task_id": 81, "completion_id": 5, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability\n    \"\"\"\n    return round(math.exp(-lam) * lam ** k / math.factorial(k), 5)"}
{"task_id": 81, "completion_id": 6, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    poisson_prob = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(poisson_prob, 5)"}
{"task_id": 81, "completion_id": 7, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('lam must be a positive number.')\n    prob = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(prob, 5)"}
{"task_id": 81, "completion_id": 8, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    try:\n        if k < 0 or lam <= 0:\n            raise ValueError('k should be a non-negative integer and lam should be positive.')\n        probability = math.exp(-lam) * lam ** k / math.factorial(k)\n        return round(probability, 5)\n    except TypeError:\n        raise TypeError('Both k and lam parameters should be numbers.')\nk = 3\nlam = 2.5\nprobability = poisson_probability(k, lam)"}
{"task_id": 81, "completion_id": 9, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if not (isinstance(k, int) and k >= 0 and isinstance(lam, (int, float)) and (lam >= 0)):\n        return None\n    poisson_prob = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(poisson_prob, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if len(img.shape) != 2:\n        return 'Error!! Image is not grayscale'\n    contrast = img.max() - img.min()\n    return contrast"}
{"task_id": 82, "completion_id": 1, "solution": "import numpy as np\nimport cv2\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: contrast value of the image\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('Input img should be a numpy array.')\n    if len(img.shape) != 2:\n        raise ValueError('Input img should be a 2D gray image.')\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast\nimg = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)"}
{"task_id": 82, "completion_id": 2, "solution": "import numpy as np\nimport cv2\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image.\n    \"\"\"\n    min_pixel_value = np.min(img)\n    max_pixel_value = np.max(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\nimg = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\ncontrast = calculate_contrast(img)"}
{"task_id": 82, "completion_id": 3, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Image must be grayscale (2D array).')\n    max_value = np.max(img)\n    min_value = np.min(img)\n    contrast = (max_value - min_value) / (max_value + min_value)\n    return contrast\nimg = np.random.randint(0, 256, (100, 100))"}
{"task_id": 82, "completion_id": 4, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n\n    Returns:\n        float: The contrast of the image.\n    \"\"\"\n    assert img.ndim == 2, 'Image must be grayscale'\n    min_pixel = np.min(img)\n    max_pixel = np.max(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 5, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    img = np.clip(img, 0, 255)\n    if len(img.shape) > 2:\n        img = np.mean(img, axis=2)\n    max_value = np.max(img)\n    min_value = np.min(img)\n    contrast = max_value - min_value\n    return contrast"}
{"task_id": 82, "completion_id": 6, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n\n    Returns:\n        contrast (float): contrast of the grayscale image.\n    \"\"\"\n    assert len(img.shape) == 2, 'The image is not a grayscale image'\n    max_value = np.max(img)\n    min_value = np.min(img)\n    contrast = (max_value - min_value) / (max_value + min_value)\n    return contrast"}
{"task_id": 82, "completion_id": 7, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be 2-dimensional.')\n    if img.min() < 0 or img.max() > 255:\n        raise ValueError('Pixel values must be between 0 and 255.')\n    pixel_min = img.min()\n    pixel_max = img.max()\n    contrast = pixel_max - pixel_min\n    return contrast"}
{"task_id": 82, "completion_id": 8, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 9, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n\n    Returns:\n        contrast (float): The contrast of the image, which can be between 0 and 255.\n    \"\"\"\n    if len(img.shape) != 2:\n        raise ValueError('Input should be a 2D numpy array representing the image.')\n    if np.any(img < 0) or np.any(img > 255):\n        raise ValueError('All pixel values should be in the range 0-255.')\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs should be 1D arrays.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 83, "completion_id": 1, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    assert vec1.ndim == 1 and vec2.ndim == 1, 'Both vectors must be 1D arrays.'\n    assert vec1.shape == vec2.shape, 'Both vectors must have the same length.'\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 83, "completion_id": 2, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    if isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray) and (vec1.shape == vec2.shape):\n        return np.dot(vec1, vec2)\n    else:\n        raise TypeError('Inputs must be one-dimensional numpy arrays with the same shape.')"}
{"task_id": 83, "completion_id": 3, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        The dot product of vec1 and vec2.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 4, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise TypeError('Both inputs must be numpy arrays')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both inputs must have the same length')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 83, "completion_id": 5, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 83, "completion_id": 6, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 7, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must be of same dimensions')\n    return np.dot(vec1, vec2)\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 83, "completion_id": 8, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (np.ndarray): 1D array representing the first vector.\n        vec2 (np.ndarray): 1D array representing the second vector.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Input vectors must have the same shape')\n    return np.dot(vec1, vec2)\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 83, "completion_id": 9, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        dot_product (float): The dot product of the two input vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point. \n        The output is rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    poly_features = np.polynomial.polynomial.polyvander(data, degree)\n    return [list(map(lambda x: round(x, 8), feature)) for feature in poly_features]\ndata = [1, 2, 3, 4, 5]\ndegree = 3"}
{"task_id": 84, "completion_id": 1, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for point in data:\n        point_result = [1]\n        for _ in range(1, degree + 1):\n            point_result.append(round(point ** _, 8))\n        result.append(point_result)\n    return result"}
{"task_id": 84, "completion_id": 2, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    vander_matrix = np.vander(data, degree + 1)\n    vander_matrix = np.around(vander_matrix, decimals=8)\n    return vander_matrix.tolist()\ndata = [1, 2, 3]\ndegree = 3"}
{"task_id": 84, "completion_id": 3, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_features = []\n    for point in data:\n        transformed_point = [round(point ** i, 8) for i in range(1, degree + 1)]\n        transformed_features.append(transformed_point)\n    return transformed_features"}
{"task_id": 84, "completion_id": 4, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 1:\n        return []\n    transformed_data = []\n    for point in data:\n        transformed_point = [point ** i for i in range(1, degree + 1)]\n        transformed_data.append(transformed_point)\n    transformed_data = np.round(transformed_data, 8)\n    return transformed_data.tolist()"}
{"task_id": 84, "completion_id": 5, "solution": "import numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by \n    generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        numpy.ndarray: A 2D numpy array where each row represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return np.array([])\n    X = np.array(data).reshape(-1, 1)\n    poly = PolynomialFeatures(degree)\n    X_poly = poly.fit_transform(X)\n    return np.round(X_poly, 8).tolist()"}
{"task_id": 84, "completion_id": 6, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    if degree == 0:\n        return [[round(x, 8) for x in data]]\n    if degree == 1:\n        return [[round(x, 8) for x in data], [round(x ** 2, 8) for x in data]]\n    transformed_data = []\n    for i in range(1, degree + 1):\n        transformed_data.append([round(x ** i, 8) for x in data])\n    return [[round(y, 8) for y in data]] + transformed_data"}
{"task_id": 84, "completion_id": 7, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = [x ** i for i in range(1, degree + 1)]\n        transformed_data.append(features)\n    return np.round(transformed_data, 8).tolist()"}
{"task_id": 84, "completion_id": 8, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for datum in data:\n        inner = []\n        for deg in range(degree + 1):\n            inner.append(round(datum ** deg, 8))\n        result.append(inner)\n    return result"}
{"task_id": 84, "completion_id": 9, "solution": "import numpy as np\ndef phi_transform(data, degree):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: The transformed data in a nested list format. If the degree is less than 0, an empty list is returned.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for point in data:\n        transformed_features = [round(point ** i, 8) for i in range(0, degree + 1)]\n        transformed_data.append(transformed_features)\n    return transformed_data\ndata = [1, 2, 3, 4, 5]\ndegree = 3"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int) -> list:\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model))\n    for pos in range(position):\n        for i in range(d_model):\n            sine_part = np.sin(pos / 10000 ** (2 * i / d_model))\n            cosine_part = np.cos(pos / 10000 ** (2 * i / d_model))\n            if i % 2 == 0:\n                encoding[pos, i] = cosine_part\n            else:\n                encoding[pos, i] = sine_part\n    encoding = encoding.astype(np.float16).tolist()\n    return encoding"}
{"task_id": 85, "completion_id": 1, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.array([position / np.power(10000, 2 * (j // 2) / d_model) for j in range(d_model)])\n    pos_enc[::2] = np.sin(pos_enc[::2])\n    pos_enc[1::2] = np.cos(pos_enc[1::2])\n    return pos_enc.astype(np.float16).tolist()"}
{"task_id": 85, "completion_id": 2, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0:\n        return -1\n    if d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                encoding[pos, i] = np.sin(pos / np.power(10000, i / d_model))\n            else:\n                encoding[pos, i] = np.cos(pos / np.power(10000, (i - 1) / d_model))\n    return encoding.tolist()"}
{"task_id": 85, "completion_id": 3, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position <= 0 or d_model <= 0:\n        return -1\n    position = np.expand_dims(np.arange(position), 1)\n    i = np.expand_dims(np.arange(d_model), 0)\n    angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float16(d_model))\n    argument = np.matmul(position, angle_rates)\n    pe = np.where(i % 2 == 0, np.sin(argument), np.cos(argument)).astype(np.float16)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 4, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encodings = np.zeros([position, d_model], dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                encodings[pos, i] = np.sin(pos / np.power(10000, i / d_model))\n            else:\n                encodings[pos, i] = np.cos(pos / np.power(10000, (i - 1) / d_model))\n    return encodings.tolist()"}
{"task_id": 85, "completion_id": 5, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encoding for a Transformer model.\n\n    Parameters:\n        position (int): Sequence length.\n        d_model (int): Model dimensionality.\n\n    Returns:\n        list: Positional encoding array.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                encoding[pos, i] = np.sin(pos / 10000 ** (i / d_model))\n            else:\n                encoding[pos, i] = np.cos(pos / 10000 ** ((i - 1) / d_model))\n    return encoding.tolist()"}
{"task_id": 85, "completion_id": 6, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_vector = np.arange(position)[:, np.newaxis]\n    depth_vector = np.arange(d_model)[np.newaxis, :]\n    angle_rates = 1 / np.power(10000, 2 * (depth_vector // 2) / np.float64(d_model))\n    angle_rads = pos_vector * angle_rates\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    return angle_rads.astype(np.float16).tolist()"}
{"task_id": 85, "completion_id": 7, "solution": "import numpy as np\nimport math\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model))\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                encoding[pos, i] = math.sin(pos / 10000 ** (i / d_model))\n            else:\n                encoding[pos, i] = math.cos(pos / 10000 ** ((i - 1) / d_model))\n    return encoding.astype(np.float16).tolist()"}
{"task_id": 85, "completion_id": 8, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position <= 0 or d_model <= 0:\n        return -1\n    encodings = np.zeros([position, d_model], dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            encodings[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            encodings[pos, i + 1] = np.cos(pos / 10000 ** (2 * i / d_model))\n    return encodings.tolist()"}
{"task_id": 85, "completion_id": 9, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    positions = np.arange(position)[:, np.newaxis]\n    dimensions = np.arange(d_model)[np.newaxis, :]\n    angle_rates = 1 / np.power(10000, 2 * (dimensions // 2) / np.float16(d_model))\n    angle_rads = positions * angle_rates\n    positional_encodings = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n    return positional_encodings.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0' to indicate overfitting, underfitting and good fit respectively.\n    \"\"\"\n    if not 0 <= training_accuracy <= 1 or not 0 <= test_accuracy <= 1:\n        raise ValueError('The training and test accuracy values should be in the range [0, 1]')\n    diff = training_accuracy - test_accuracy\n    if diff > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 1, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if not 0 <= training_accuracy <= 1 or not 0 <= test_accuracy <= 1:\n        raise ValueError('training_accuracy and test_accuracy must be between 0 and 1')\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 2, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy < 0 or training_accuracy > 1 or test_accuracy < 0 or (test_accuracy > 1):\n        raise ValueError('Both training_accuracy and test_accuracy must be between 0 and 1.')\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 3, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    assert 0 <= training_accuracy <= 1, 'training_accuracy should be a float between 0 and 1'\n    assert 0 <= test_accuracy <= 1, 'test_accuracy should be a float between 0 and 1'\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 4, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 5, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 6, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy > 0.9 and test_accuracy < 0.7:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 7, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 8, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2 and training_accuracy > 0.7:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 9, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return (np.round(parameter.tolist(), 5), np.round(m.tolist(), 5), np.round(v.tolist(), 5))"}
{"task_id": 87, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (round(float(parameter), 5), round(float(m), 5), round(float(v), 5))\nm = 0\nv = 0\nt = 1\nlearning_rate = 0.01\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-08\nparameter = 0.5\ngrad = 1.0"}
{"task_id": 87, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corr = m / (1 - beta1 ** t)\n    v_corr = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_corr / (np.sqrt(v_corr) + epsilon)\n    parameter = np.round(parameter, 5).tolist()\n    m = np.round(m, 5).tolist()\n    v = np.round(v, 5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    mt = m / (1 - beta1 ** t)\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    vt = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * mt / (np.sqrt(vt) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 87, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    updated_parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return (np.round(updated_parameter.tolist(), 5), np.round(m_corrected.tolist(), 5), np.round(v_corrected.tolist(), 5))"}
{"task_id": 87, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    updated_parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return (np.round(updated_parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 87, "completion_id": 6, "solution": "def adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())"}
{"task_id": 87, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    parameter = np.around(parameter, decimals=5).tolist()\n    m = np.around(m, decimals=5).tolist()\n    v = np.around(v, decimals=5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = parameter.tolist()\n    parameter = [round(num, 5) for num in parameter]\n    m = m.tolist()\n    m = [round(num, 5) for num in m]\n    v = v.tolist()\n    v = [round(num, 5) for num in v]\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    parameter -= learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m_corrected, 5).tolist(), np.round(v_corrected, 5).tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n    params = {'wte': np.random.rand(3, hparams['n_embd']), 'wpe': np.random.rand(hparams['n_ctx'], hparams['n_embd']), 'blocks': [], 'ln_f': {'g': np.ones((hparams['n_embd'],)), 'b': np.zeros((hparams['n_embd'],))}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef scaled_dot_product_attention(q, k, v, mask=None):\n    d_k = q.shape[-1]\n    scores = np.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1000000000.0)\n    attention = np.softmax(scores, axis=-1)\n    output = np.matmul(attention, v)\n    return (output, attention)\ndef multi_head_attention(hparams, q, k, v, mask=None):\n    batch_size = q.shape[0]\n    seq_len = q.shape[1]\n    n_head = hparams['n_head']\n    d_k = d_v = hparams['n_embd'] // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k)\n    k = k.reshape(batch_size, seq_len, n_head, d_k)\n    v = v.reshape(batch_size, seq_len, n_head, d_v)\n    (q, k, v) = (q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2))\n    (output, attention) = scaled_dot_product_attention(q, k, v, mask)\n    output = output.transpose(1, 2).reshape(batch_size, seq_len, -1)\n    return (output, attention)\ndef forward(x, hparams, params):\n    (n, t) = x.shape\n    token_embeddings = np.take(params['wte'], x, axis=0)\n    position_embeddings = params['wpe'][np.arange(t), :]\n    x = token_embeddings + position_embeddings\n    (x, _) = multi_head_attention(hparams, x, x, x)\n    x_ln = (x - x.mean(axis=-1, keepdims=True)) / np.sqrt(x.var(axis=-1, keepdims=True) + 1e-05)\n    x_ln = x_ln * params['ln_f']['g'] + params['ln_f']['b']\n    return x_ln\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_ids = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        if len(prompt_ids) > 0:\n            prompt_ids.append(0)\n        output = forward(np.array([prompt_ids]), hparams, params)\n        logits = output[0, -1]\n        next_token_probabilities = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n        next_token = np.random.choice([0, 1, 2], p=next_token_probabilities)\n        prompt_ids.append(next_token)\n    return encoder.decode(prompt_ids)"}
{"task_id": 88, "completion_id": 1, "solution": "import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n    encoder = DummyBPE()\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': torch.rand(3, 10), 'wpe': torch.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    context = torch.tensor(encoder.encode(prompt[-hparams['n_ctx']:]), dtype=torch.long).to(device)\n    past = None\n    for _ in range(n_tokens_to_generate):\n        (logits, presents) = model(context, past=past)\n        (_, _, raw_token) = torch.topk(F.softmax(logits[:, -1], dim=-1), k=1)\n        context = raw_token\n        past = presents\n    return encoder.decode(context.tolist())\nclass DummyBPE:\n\n    def __init__(self):\n        self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n    def encode(self, text: str):\n        tokens = text.strip().split()\n        return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n    def decode(self, token_ids: list):\n        reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n        return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = DummyModel(params).to(device)"}
{"task_id": 88, "completion_id": 2, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef multihead_attention(query, key, value, d_model, num_heads):\n    assert d_model % num_heads == 0\n    depth = d_model // num_heads\n    q = np.reshape(query, (-1, num_heads, depth))\n    k = np.reshape(key, (-1, num_heads, depth))\n    v = np.reshape(value, (-1, num_heads, depth))\n    score = np.matmul(q, np.transpose(k, (0, 1, 3, 2))) / np.sqrt(d_model)\n    weights = np.softmax(score, axis=-1)\n    output = np.matmul(weights, v)\n    output = np.reshape(output, (-1, d_model))\n    return output\ndef feed_forward(x, weights, biases):\n    return np.maximum(0, np.matmul(x, weights) + biases)\ndef layer_normalization(x, gamma, beta, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    normalized = (x - mean) / np.sqrt(variance + eps)\n    return gamma * normalized + beta\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        position = len(token_ids)\n        query = params['wte'][token_ids[-1]] + params['wpe'][position]\n        key = params['wte']\n        value = params['wte']\n        attention = multihead_attention(query, key, value, 10, hparams['n_head'])\n        feed_forward_output = feed_forward(attention, params['blocks'], params['ln_f']['g'], params['ln_f']['b'])\n        output = layer_normalization(attention + feed_forward_output, 1, 0)\n        next_token_id = np.argmax(output)\n        token_ids.append(next_token_id)\n    generated_text = encoder.decode(token_ids)\n    return generated_text"}
{"task_id": 88, "completion_id": 3, "solution": "import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.functional import F\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\nclass SelfAttention(nn.Module):\n\n    def __init__(self, hparams, params):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Linear(hparams['n_head'], hparams['n_head'])\n        self.key = nn.Linear(hparams['n_head'], hparams['n_head'])\n        self.value = nn.Linear(hparams['n_head'], hparams['n_head'])\n\n    def forward(self, x):\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        return F.softmax(q @ k.T / np.sqrt(v.size(-1)), dim=-1) @ v\nclass Block(nn.Module):\n\n    def __init__(self, hparams, params):\n        super(Block, self).__init__()\n        self.attention = SelfAttention(hparams, params)\n        self.feed_forward = nn.Linear(hparams['n_head'], hparams['n_head'])\n        self.layer_norm1 = nn.LayerNorm(hparams['n_head'])\n        self.layer_norm2 = nn.LayerNorm(hparams['n_head'])\n\n    def forward(self, x):\n        x = x + self.layer_norm1(self.attention(x))\n        x = x + self.layer_norm2(self.feed_forward(x))\n        return x\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    block = Block(hparams, params)\n    tokens = encoder.encode(prompt)\n    tokens = torch.tensor(tokens)\n    for _ in range(n_tokens_to_generate):\n        tokens = block(tokens)\n        new_token = tokens[-1].argmax().item()\n        tokens = torch.cat([tokens, torch.tensor([new_token])])\n    return encoder.decode(tokens.tolist())"}
{"task_id": 88, "completion_id": 4, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef mult_attn(Q, K, V):\n    return np.matmul(np.matmul(Q, K.T), V)\ndef feed_forward_network(input_data):\n    return np.matmul(input_data.T, input_data)\ndef layer_norm(input_data, gain, bias):\n    return (input_data - np.mean(input_data)) / np.std(input_data) * gain + bias\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = params['wte'][token_ids, :]\n    pos_embeddings = np.array([params['wpe'][i, :] for i in range(len(token_embeddings))])\n    input_embeddings = token_embeddings + pos_embeddings\n    attention_output = mult_attn(input_embeddings, input_embeddings, input_embeddings)\n    feed_forward_output = feed_forward_network(attention_output)\n    layer_norm_output = layer_norm(feed_forward_output, params['ln_f']['g'], params['ln_f']['b'])\n    generated_text = encoder.decode([np.argmax(embedding) for embedding in layer_norm_output])\n    return generated_text"}
{"task_id": 88, "completion_id": 5, "solution": "import numpy as np\nimport torch\nfrom torch import nn\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': torch.rand(3, 10), 'wpe': torch.rand(1024, 10), 'blocks': [], 'ln_f': {'g': torch.ones(10), 'b': torch.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\nclass TransformerBlock(nn.Module):\n\n    def __init__(self, hparams):\n        super().__init__()\n        self.att = nn.MultiheadAttention(hparams['n_head'], hparams['n_ctx'])\n        self.ln_1 = nn.LayerNorm(hparams['n_ctx'])\n        self.ff_self = nn.Linear(hparams['n_ctx'], hparams['n_ctx'])\n        self.ln_2 = nn.LayerNorm(hparams['n_ctx'])\n\n    def forward(self, x):\n        x = x + self.att(x, x, x)[0]\n        x = self.ln_1(x)\n        x = x + self.ff_self(x)\n        x = self.ln_2(x)\n        return x\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    transformer = TransformerBlock(hparams)\n    tokens_ids = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        tokens_ids.append(0)\n    tokens_tensor = torch.LongTensor(tokens_ids).unsqueeze(0)\n    output = transformer(tokens_tensor)\n    last_output_ids = output[-1, :].topk(1).indices.item()\n    tokens_ids.append(last_output_ids)\n    return encoder.decode(tokens_ids)"}
{"task_id": 88, "completion_id": 6, "solution": "import numpy as np\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\ndef layer_norm(x, g, b, epsilon=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    inv = np.diag(1.0 / np.sqrt(variance + epsilon))\n    x_norm = (x - mean) @ inv\n    return g * x_norm + b\ndef feed_forward(x, params):\n    return x @ params['w1'].T + params['b1']\ndef multihead_attention(x, params):\n    q = x['q'] @ params['wq'].T + params['bq']\n    k = x['k'] @ params['wk'].T + params['bk']\n    v = x['v'] @ params['wv'].T + params['bv']\n    dots = np.sum(q * k, axis=-1) * params['scale']\n    attn = softmax(dots)\n    return attn @ v\ndef gpt2_block(x, params):\n    attn = multihead_attention(x, params['attn'])\n    attn = layer_norm(x + attn, params['ln0']['g'], params['ln0']['b'])\n    ff = feed_forward(attn, params['ff'])\n    ff = layer_norm(attn + ff, params['ln1']['g'], params['ln1']['b'])\n    return ff\ndef gpt2_init_params(n_vocab, n_ctx, n_embd, n_head, n_layer):\n    params = {'wte': np.random.randn(n_vocab, n_embd), 'wpe': np.random.randn(n_ctx, n_embd), 'blocks': [], 'ln_f': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}}\n    for _ in range(n_layer):\n        params['blocks'].append({'attn': {'wq': np.random.randn(n_embd, n_embd), 'wk': np.random.randn(n_embd, n_embd), 'wv': np.random.randn(n_embd, n_embd), 'bq': np.random.randn(n_embd), 'bk': np.random.randn(n_embd), 'bv': np.random.randn(n_embd), 'scale': np.sqrt(n_embd)}, 'ln0': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'ff': {'w1': np.random.randn(n_embd, 4 * n_embd), 'b1': np.random.randn(4 * n_embd)}, 'ln1': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}})\n    return params\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_embd': 10, 'n_head': 12, 'n_layer': 12}\n    encoder = DummyBPE()\n    params = gpt2_init_params(n_vocab=3, **hparams)\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        token_embed = params['wte'][token_ids[-1]]\n        positional_embed = params['wpe'][len(token_ids) - 1]\n        x_in = positional_embed + token_embed\n        for block in params['blocks']:\n            x_in = gpt2_block({'q': x_in, 'k': x_in, 'v': x_in}, block)\n        x_in = layer_norm(x_in + params['ln_f']['b'], params['ln_f']['g'], params['ln_f']['b'])\n        logits = x_in @ params['wte'].T\n        next_token_id = np.argmax(logits)\n        token_ids.append(next_token_id)\n    return encoder.decode(token_ids)"}
{"task_id": 88, "completion_id": 7, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (_, __, params) = load_encoder_hparams_and_params()\n\n    def encode_sequence(prompt: str, params: dict):\n        token_ids = params['encoder'].encode(prompt)\n        return np.array([params['wte'][token_id] + params['wpe'][i] for (i, token_id) in enumerate(token_ids)])\n\n    def decode_sequence(sequence: np.ndarray, params: dict):\n        token_ids = [np.argmax(params['wte'][token_id] + params['wpe'][i]) for (i, token_id) in enumerate(sequence)]\n        return params['encoder'].decode(token_ids)\n\n    def autoregressive_decode(input_sequence: np.ndarray, n_tokens_to_generate: int):\n        for _ in range(n_tokens_to_generate):\n            input_sequence = np.concatenate([input_sequence, np.dot(params['ln_f']['g'], input_sequence[-1]) + params['ln_f']['b']])\n        return input_sequence\n    input_sequence = encode_sequence(prompt, params)\n    output_sequence = autoregressive_decode(input_sequence, n_tokens_to_generate)\n    generated_text = decode_sequence(output_sequence, params)\n    return generated_text"}
{"task_id": 88, "completion_id": 8, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    tokens = encoder.encode(prompt)\n    wte = params['wte']\n    wpe = params['wpe']\n    token_embeddings = np.dot(wte[tokens], wpe)\n    blocks = params['blocks']\n    for _ in range(hparams['n_head']):\n        token_embeddings = np.dot(token_embeddings, wte.T) + blocks[:5].T\n    output = np.exp(token_embeddings) / np.exp(token_embeddings).sum(-1)\n    dummy_output = tokens + list(np.random.choice(len(tokens), size=n_tokens_to_generate, p=output[-1] / output[-1:].sum()))\n    return encoder.decode(dummy_output)\nprompt = 'Hello world!'\nn_tokens_to_generate = 40"}
{"task_id": 88, "completion_id": 9, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (_, hparams, params) = load_encoder_hparams_and_params()\n    wte = params['wte']\n    wpe = params['wpe']\n    encoded = params['encoder'].encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        ctx = np.array([np.random.rand(10) @ wte + np.random.rand(10) @ wpe for _ in encoded])\n        attention_result = np.random.rand(10)\n        ln_g = params['ln_f']['g']\n        ln_b = params['ln_f']['b']\n        ln_out = np.tanh(attention_result) * ln_g + ln_b\n        next_token = np.argmax(ln_out)\n        encoded.append(next_token)\n    return params['encoder'].decode(encoded)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    crystal_values = np.array(crystal_values)\n    assert crystal_values.shape[1] == dimension\n    scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i, n):\n            similarity = np.dot(crystal_values[i], crystal_values[j])\n            scores[i, j] = scores[j, i] = similarity\n    attentions = softmax(scores).T\n    final_patterns = np.dot(attentions, crystal_values)\n    return [round(val, 4) for val in final_patterns.flatten()]"}
{"task_id": 89, "completion_id": 1, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    padded_values = np.pad(crystal_values, (dimension - 1, dimension - 1), mode='wrap')\n    rolling_window_view = np.lib.stride_tricks.as_strided(padded_values, shape=(len(padded_values) - 2 * (dimension - 1), 2 * dimension - 1), strides=padded_values.strides)\n    attention_scores = np.apply_along_axis(softmax, 1, rolling_window_view)\n    weighted_values = np.multiply(rolling_window_view, attention_scores)\n    final_patterns = np.sum(weighted_values, axis=1)\n    return [round(val, 4) for val in final_patterns]"}
{"task_id": 89, "completion_id": 2, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum(axis=0)\n    crystal_values = np.array(crystal_values)\n    weights = np.random.rand(dimension, n)\n    query_key_values = np.dot(weights, crystal_values)\n    attn_weights = softmax(query_key_values)\n    weighted_patterns = np.dot(crystal_values, attn_weights)\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = [1.2, 2.3, 3.4, 4.5, 5.6]\ndimension = 3"}
{"task_id": 89, "completion_id": 3, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    weights = np.empty((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i, n):\n            similarity = np.dot(crystal_values[i], crystal_values[j]) / (np.linalg.norm(crystal_values[i]) * np.linalg.norm(crystal_values[j]))\n            weight = softmax(np.array([similarity]))[0]\n            weights[i][j] = weights[j][i] = weight\n    final_patterns = np.empty(n, dtype=float)\n    for i in range(n):\n        final_patterns[i] = round(np.dot(weights[i], crystal_values[i]), 4)\n    return final_patterns"}
{"task_id": 89, "completion_id": 4, "solution": "import numpy as np\ndef softmax(values):\n    e_x = np.exp(values - np.max(values))\n    return e_x / e_x.sum(axis=0)\ndef pattern_weaver(n, crystal_values, dimension):\n    crystal_values = np.array(crystal_values)\n    normalized_values = crystal_values / np.linalg.norm(crystal_values)\n    scores = []\n    for crystal in normalized_values:\n        attention_scores = softmax(np.dot(crystal, normalized_values.T))\n        weighted_pattern = np.dot(attention_scores, normalized_values)\n        scores.append(np.around(weighted_pattern, 4))\n    return scores"}
{"task_id": 89, "completion_id": 5, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exps = np.exp(values - np.max(values))\n        return exps / np.sum(exps)\n    weights = np.random.rand(n, dimension)\n    bias = np.random.rand(n)\n    scores = [bias[i] + np.dot(weights[i], crystal_values) for i in range(n)]\n    attention_weights = softmax(scores)\n    pattern = [round(np.dot(attention_weights, crystal_values), 4) for i in range(n)]\n    return pattern"}
{"task_id": 89, "completion_id": 6, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values)\n        sum_exp_values = np.sum(exp_values)\n        softmax_values = exp_values / sum_exp_values\n        return list(softmax_values)\n\n    def attention_score(a, b):\n        return np.dot(a, b)\n    crystal_patterns = np.zeros((n, dimension))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                score = attention_score(crystal_values[i], crystal_values[j])\n                softmax_score = softmax([score])[0]\n                crystal_patterns[i] += crystal_values[j] * softmax_score\n    weighted_patterns = list(np.sum(crystal_patterns, axis=1))\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = np.random.rand(n, 10)\ndimension = 10"}
{"task_id": 89, "completion_id": 7, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    crystal_values = np.array(crystal_values)\n    (n, _) = crystal_values.shape\n    crystal_values = crystal_values.T\n    crystal_scores = []\n    for crystal in crystal_values:\n        crystal = crystal.reshape(-1, 1)\n        attention_scores = softmax(crystal).reshape(1, -1)\n        weighted_pattern = attention_scores.dot(crystal_values).flatten()\n        crystal_scores.append(weighted_pattern)\n    crystal_scores = np.array(crystal_scores).T\n    return [round(score, 4) for score in crystal_scores.flatten().tolist()]\nn = 3\ncrystal_values = [[0.5, 0.7, 0.9], [0.6, 0.8, 1.0], [0.7, 0.9, 1.1]]"}
{"task_id": 89, "completion_id": 8, "solution": "import numpy as np\ndef softmax(values):\n    e_x = np.exp(values - np.max(values))\n    return e_x / e_x.sum(axis=0)\ndef pattern_weaver(n, crystal_values, dimension):\n    crystal_values = np.array(crystal_values)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(crystal_values[i], crystal_values[j])\n    attention_scores = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(pattern, 4) for pattern in weighted_patterns]"}
{"task_id": 89, "completion_id": 9, "solution": "import numpy as np\ndef softmax(values):\n    e_x = np.exp(values - np.max(values))\n    return e_x / e_x.sum(axis=0)\ndef pattern_weaver(n, crystal_values, dimension):\n    attention_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                attention_matrix[i][j] = np.dot(crystal_values[i], crystal_values[j])\n    attention_scores = np.array([softmax(row) for row in attention_matrix])\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(val, 4) for val in weighted_patterns.reshape((n * dimension,))]\nn = 3\ndimension = 2\ncrystal_values = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    avg_dl = sum([len(doc) for doc in corpus]) / len(corpus)\n    normalized_document_lengths = [len(doc) / avg_dl for doc in corpus]\n    term_counters = [Counter(doc) for doc in corpus]\n    collection_frequencies = Counter()\n    for counter in term_counters:\n        collection_frequencies.update(counter.keys())\n    idf = {term: math.log((len(corpus) - freq + 0.5) / (freq + 0.5)) for (term, freq) in collection_frequencies.items()}\n    scores = []\n    query_counter = Counter(query)\n    for (i, term) in enumerate(query):\n        score = 0\n        for (j, doc) in enumerate(corpus):\n            doc_len = len(doc)\n            tf = doc.count(term)\n            doc_len_norm = normalized_document_lengths[j]\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * doc_len_norm)\n            bm25_score = idf.get(term, 0) * (numerator / denominator)\n            if i == 0:\n                scores.append(bm25_score)\n            else:\n                scores[j] += bm25_score\n    return [round(score, 3) for score in scores]"}
{"task_id": 90, "completion_id": 1, "solution": "import numpy as np\nimport math\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    D = len(corpus)\n    avgdl = sum([len(doc) for doc in corpus]) / D\n    word_counts = [Counter(doc) for doc in corpus]\n    idf = {}\n    for word in query:\n        df = sum([1 for doc in corpus if word in doc])\n        idf[word] = math.log((D - df + 0.5) / (df + 0.5))\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0\n        for word in query:\n            tf = word_counts[i][word]\n            qtf = query.count(word)\n            doc_length = len(doc)\n            score += idf[word] * ((k1 + 1) * tf) / (k1 * (1 - b + b * (doc_length / avgdl)) + tf) * ((k1 + 1) * qtf)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 90, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    doc_lengths = [len(doc) for doc in corpus]\n    avg_doc_length = sum(doc_lengths) / len(doc_lengths)\n    word_freqs = {}\n    for doc in corpus:\n        word_counter = Counter(doc)\n        for (word, freq) in word_counter.items():\n            if word not in word_freqs:\n                word_freqs[word] = 0\n            word_freqs[word] += freq\n    word_idfs = {}\n    for (word, freq) in word_freqs.items():\n        word_idfs[word] = np.log((len(corpus) - freq + 0.5) / (freq + 0.5))\n    scores = []\n    for doc in corpus:\n        score = 0\n        word_counter = Counter(doc)\n        for word in query:\n            if word in word_counter:\n                tf = word_counter[word] / len(doc)\n                idf = word_idfs[word]\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * len(doc) / avg_doc_length)\n                score += idf * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 90, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    word_counts = [Counter(words.split()) for words in corpus]\n    avgdl = sum((len(wc) for wc in word_counts)) / len(word_counts)\n    N = len(corpus)\n    idf = {}\n    for words in corpus:\n        for word in words.split():\n            idf[word] = idf.get(word, 0) + 1\n    for (word, freq) in idf.items():\n        idf[word] = math.log((N - freq + 0.5) / (freq + 0.5))\n    scores = []\n    query = query.split()\n    for wc in word_counts:\n        score = 0.0\n        for word in query:\n            if word in wc:\n                freq = wc[word]\n                score += idf.get(word, 0) * (freq * (k1 + 1)) / (freq + k1 * (1 - b + b * (len(wc) / avgdl)))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['this is the first document', 'this is the second second best document', 'and this is the third one']\nquery = 'document second'"}
{"task_id": 90, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(corpus)\n    query_vector = vectorizer.transform([query])\n    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n    avg_doc_len = np.mean([len(text.split(' ')) for text in corpus])\n    corpus_size = len(corpus)\n    final_scores = []\n    for (text, score) in zip(corpus, scores):\n        doc_len = len(text.split(' '))\n        bm25_score = score * (k1 + 1) / (k1 * (1 - b + b * (doc_len / avg_doc_len)) + score)\n        final_scores.append(round(bm25_score, 3))\n    return final_scores\ncorpus = ['this is a foo bar sentence', 'this is another short sentence', 'the quick brown fox jumps over the lazy dog', 'hi there']\nquery = 'fox sentence'"}
{"task_id": 90, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus against the provided query.\n\n    Parameters:\n    corpus (list of list of str): List of documents where each document is a list of words.\n    query (list of str): Query to calculate scores against.\n    k1 (float): Constant that affects term saturation, defaults to 1.5.\n    b (float): Constant that controls document length normalization, defaults to 0.75.\n\n    Returns:\n    list: List of BM25 scores rounded to three decimal places for each document in the corpus.\n    \"\"\"\n\n    def calculate_idf(word):\n        \"\"\"\n        Calculate inverse document frequency for a given word.\n\n        Parameters:\n        word (str): Word to calculate IDF for.\n\n        Returns:\n        float: IDF of the word.\n        \"\"\"\n        document_count = len(corpus)\n        document_freq = sum([1 for document in corpus if word in document])\n        return math.log((document_count - document_freq + 0.5) / (document_freq + 0.5))\n    idf = {word: calculate_idf(word) for word in query}\n\n    def calculate_tf(word, document):\n        \"\"\"\n        Calculate term frequency for a word in a given document.\n\n        Parameters:\n        word (str): Word to calculate TF for.\n        document (list of str): Document to calculate TF in.\n\n        Returns:\n        float: TF of the word in the document.\n        \"\"\"\n        word_count = Counter(document)[word]\n        return (k1 + 1) * word_count / (k1 * (1 - b + b * (len(document) / avg_doc_length)) + word_count)\n    avg_doc_length = sum([len(document) for document in corpus]) / len(corpus)\n    scores = []\n    for document in corpus:\n        score = 0\n        for word in query:\n            score += idf[word] * calculate_tf(word, document)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 90, "completion_id": 6, "solution": "import numpy as np\nimport math\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def average_doc_len(corpus):\n        return sum((len(doc) for doc in corpus)) / len(corpus)\n\n    def K(doc, avgdl):\n        return k1 * (1 - b + b * (len(doc) / avgdl))\n\n    def term_freq(term, document):\n        return Counter(document)[term]\n\n    def inverse_doc_freq(term, corpus):\n        return math.log(len(corpus) / sum((term in doc for doc in corpus)))\n    avgdl = average_doc_len(corpus)\n    scores = []\n    freqs = {term: term_freq(term, query) for term in query}\n    for doc in corpus:\n        score = 0\n        for (term, freq) in freqs.items():\n            idf = inverse_doc_freq(term, corpus)\n            term_score = idf * (freq * (k1 + 1) / (freq + K(doc, avgdl)))\n            score += term_score\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 90, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    N = len(corpus)\n    corpus = [set(x) for x in corpus]\n    idf = {}\n    avgdl = sum((len(doc) for doc in corpus)) / N\n    for doc in corpus:\n        for word in doc:\n            if word not in idf:\n                idf[word] = 1\n            else:\n                idf[word] += 1\n    for word in idf:\n        idf[word] = math.log((N - idf[word] + 0.5) / (idf[word] + 0.5))\n    scores = []\n    for doc in corpus:\n        score = 0\n        for word in query:\n            if word in doc:\n                tf = doc.count(word)\n                qtf = query.count(word)\n                numerator = idf[word] * tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * len(doc) / avgdl)\n                score += numerator / denominator\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 90, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def idf(word):\n        df = sum([1 for doc in corpus if word in doc])\n        return np.log((len(corpus) - df + 0.5) / (df + 0.5))\n\n    def avgdl(corpus):\n        return sum([len(doc) for doc in corpus]) / len(corpus)\n\n    def bm25(doc, query):\n        score = 0.0\n        for word in query:\n            f = doc.count(word)\n            qf = query.count(word)\n            N = len(corpus)\n            avg_dl = avgdl(corpus)\n            score += idf(word) * (f * (k1 + 1)) / (f + k1 * (1 - b + b * (len(doc) / avg_dl))) * (k1 + 1) * qf / (k1 + qf)\n        return score\n    return [round(bm25(doc, query), 3) for doc in corpus]\ncorpus = [['apple', 'banana', 'apple', 'strawberry', 'kiwi'], ['banana', 'strawberry', 'kiwi'], ['apple', 'banana'], ['strawberry', 'orange', 'kiwi'], ['kiwi', 'passionfruit', 'grapefruit']]\nquery = ['apple', 'kiwi']"}
{"task_id": 90, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    DF = Counter([token for document in corpus for token in set(document)])\n    N = len(corpus)\n    avg_dl = np.mean([len(doc) for doc in corpus])\n    bm25_scores = []\n    for document in corpus:\n        tokens = document\n        doc_len = len(tokens)\n        freq = Counter(tokens)\n        scores = [freq[token] * (k1 + 1) / (freq[token] + k1 * (1 - b + b * doc_len / avg_dl)) for token in query if token in freq]\n        bm25_scores.append(round(sum((np.log((N - DF[i] + 0.5) / (DF[i] + 0.5)) * freq[i] * (k1 + 1) * scores[i] / (k1 * (1 - b + b * doc_len / avg_dl) + freq[i]) for i in range(len(scores)))), 3))\n    return bm25_scores\ncorpus = [['dog', 'bites', 'man'], ['man', 'bites', 'dog'], ['dog', 'eats', 'meat'], ['dog', 'eats', 'dog'], ['man', 'eats', 'food']]\nquery = ['dog', 'bites']"}
{"task_id": 91, "completion_id": 0, "solution": "from sklearn.metrics import precision_score, recall_score\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)\ny_true = [0, 1, 1, 0, 1, 1]\ny_pred = [0, 0, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 1, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n    The F1 score is a widely used metric in machine learning, combining precision and recall into a single measure.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    (tp, fp, fn) = (0, 0, 0)\n    for (y_t, y_p) in zip(y_true, y_pred):\n        if y_t == y_p and y_t == 1:\n            tp += 1\n        elif y_p == 1 and y_t != y_p:\n            fp += 1\n        elif y_p == 0 and y_t != y_p:\n            fn += 1\n    precision = tp / (tp + fp) if tp > 0 else 0\n    recall = tp / (tp + fn) if tp > 0 else 0\n    f1_score = 2 * (precision * recall / (precision + recall)) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 2, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    assert len(y_true) == len(y_pred)\n    true_positives = sum([1 for (t, p) in zip(y_true, y_pred) if t == p == 1])\n    false_positives = sum([1 for (t, p) in zip(y_true, y_pred) if t == 0 and p == 1])\n    false_negatives = sum([1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 0])\n    try:\n        precision = true_positives / (true_positives + false_positives)\n    except ZeroDivisionError:\n        precision = 0.0\n    try:\n        recall = true_positives / (true_positives + false_negatives)\n    except ZeroDivisionError:\n        recall = 0.0\n    try:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    except ZeroDivisionError:\n        f1_score = 0.0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 0, 0]"}
{"task_id": 91, "completion_id": 3, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    (tp, fp, fn) = (0, 0, 0)\n    for (true_out, pred_out) in zip(y_true, y_pred):\n        if true_out == 1 and pred_out == 1:\n            tp += 1\n        elif true_out == 0 and pred_out == 1:\n            fp += 1\n        elif true_out == 1 and pred_out == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    f1_score = round(f1_score, 3)\n    return f1_score"}
{"task_id": 91, "completion_id": 4, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    (tp, fp, fn) = (0, 0, 0)\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 5, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'Length of true labels and predicted labels should be same.'\n    true_positive = true_negative = false_positive = false_negative = 0\n    for i in range(len(y_true)):\n        if y_true[i] == y_pred[i] == 1:\n            true_positive += 1\n        elif y_true[i] == y_pred[i] == 0:\n            true_negative += 1\n        elif y_pred[i] == 1 and y_true[i] != y_pred[i]:\n            false_positive += 1\n        else:\n            false_negative += 1\n    precision = true_positive / (true_positive + false_positive)\n    recall = true_positive / (true_positive + false_negative)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    f1_score = round(f1_score, 3)\n    return f1_score\ny_true = [1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\ny_pred = [1, 1, 1, 0, 0, 0, 1, 1, 0, 1]\nf1_score = calculate_f1_score(y_true, y_pred)"}
{"task_id": 91, "completion_id": 6, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Both lists must have the same length.')\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    if tp + fp != 0:\n        precision = tp / (tp + fp)\n    else:\n        precision = 0\n    if tp + fn != 0:\n        recall = tp / (tp + fn)\n    else:\n        recall = 0\n    if precision + recall != 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_score = 0\n    return round(f1_score, 3)\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 0, 1, 0, 1, 0, 1, 0, 1]"}
{"task_id": 91, "completion_id": 7, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('True labels and predicted labels should have the same length.')\n    true_positives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == yp and yt == 1])\n    false_positives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt != yp and yp == 1])\n    false_negatives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt != yp and yp == 0])\n    try:\n        precision = true_positives / (true_positives + false_positives)\n    except ZeroDivisionError:\n        precision = 0\n    try:\n        recall = true_positives / (true_positives + false_negatives)\n    except ZeroDivisionError:\n        recall = 0\n    try:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    except ZeroDivisionError:\n        f1_score = 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 8, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum([y_true[i] == y_pred[i] and y_pred[i] == 1 for i in range(len(y_true))])\n    fp = sum([y_true[i] != y_pred[i] and y_pred[i] == 1 for i in range(len(y_true))])\n    fn = sum([y_true[i] != y_pred[i] and y_pred[i] == 0 for i in range(len(y_true))])\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 9, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'True labels and predicted labels should have the same length.'\n    (true_positives, false_positives, false_negatives) = (0, 0, 0)\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            true_positives += 1\n        elif true_label == 0 and pred_label == 1:\n            false_positives += 1\n        elif true_label == 1 and pred_label == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    f1_score = round(f1_score, 3)\n    return f1_score"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    PI = 3.14159\n    days = len(consumption_data)\n    fluctuation_values = [10 * math.sin(2 * PI * i / days) for i in range(1, days + 1)]\n    detrended_data = consumption_data - fluctuation_values\n    x = np.array(range(1, days + 1)).reshape(-1, 1)\n    y = np.array(detrended_data).reshape(-1, 1)\n    model = LinearRegression().fit(x, y)\n    day_15_prediction = model.predict([[15]])\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / days)\n    day_15_prediction_with_fluctuation = day_15_prediction + day_15_fluctuation\n    final_answer = round(day_15_prediction_with_fluctuation)\n    final_answer += final_answer * 0.05\n    final_answer = math.ceil(final_answer)\n    return final_answer"}
{"task_id": 92, "completion_id": 1, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    PI = 3.14159\n    D = 10\n    detrended_data = []\n    for (i, data) in enumerate(consumption_data):\n        detrended_data.append(data - 10 * math.sin(2 * PI * (i + 1) / D))\n    day_no = np.array(range(1, D + 1)).reshape((-1, 1))\n    consumption = np.array(detrended_data)\n    model = LinearRegression().fit(day_no, consumption)\n    day_15_base_consumption = model.predict([[15]])[0]\n    day_15_consumption = day_15_base_consumption + 10 * math.sin(2 * PI * 15 / D)\n    safety_margin = day_15_consumption * 0.05\n    rounded_consumption = round(day_15_consumption + safety_margin)\n    return int(rounded_consumption)"}
{"task_id": 92, "completion_id": 2, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = np.array([data - 10 * np.sin(2 * PI * i / 10) for (i, data) in enumerate(consumption_data, start=1)])\n    x = np.array(range(1, 11)).reshape((-1, 1))\n    model = LinearRegression().fit(x, detrended_data)\n    day_15 = np.array([[15]])\n    predicted_base_consumption = model.predict(day_15)\n    predicted_total_consumption = predicted_base_consumption + 10 * np.sin(2 * PI * 15 / 10)\n    final_prediction = math.ceil(predicted_total_consumption[0] * 1.05)\n    return final_prediction"}
{"task_id": 92, "completion_id": 3, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression"}
{"task_id": 92, "completion_id": 4, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = np.array([consumption_data[i] - fluctuation[i] for i in range(10)])\n    x = np.array(range(10)).reshape(-1, 1)\n    y = np.array(detrended_data)\n    model = LinearRegression().fit(x, y)\n    day15_base_consumption = model.predict([[14]])[0]\n    fluctuation_day15 = fluctuation[9]\n    day15_consumption = day15_base_consumption + fluctuation_day15\n    final_consumption = day15_consumption * 1.05\n    final_integer = math.ceil(final_consumption)\n    return final_integer"}
{"task_id": 92, "completion_id": 5, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    days = np.arange(1, 11).reshape(-1, 1)\n    fluctuation = 10 * np.sin(2 * np.pi * days / 10)\n    detrended_data = consumption_data - fluctuation\n    model = LinearRegression().fit(days, detrended_data)\n    day15_consumption = model.predict([[15]])[0]\n    fitration = 10 * math.sin(2 * math.pi * 15 / 10)\n    day15_consumption += fitration\n    final_consumption = math.ceil(day15_consumption * 1.05)\n    return final_consumption"}
{"task_id": 92, "completion_id": 6, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    PI = 3.14159\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * i / 10) for i in range(len(consumption_data))]\n    days = np.array([i + 1 for i in range(len(consumption_data))]).reshape(-1, 1)\n    regressor = LinearRegression()\n    regressor.fit(days, detrended_data)\n    day_15_base_consumption = regressor.predict([[15]])[0]\n    day_15_consumption = day_15_base_consumption + 10 * math.sin(2 * PI * 15 / 10)\n    final_consumption = day_15_consumption * 1.05\n    return math.ceil(final_consumption)\nconsumption_data = [100, 110, 120, 110, 100, 90, 80, 90, 100, 110]"}
{"task_id": 92, "completion_id": 7, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for i in range(10):\n        detrended_data.append(consumption_data[i] - 10 * math.sin(2 * math.pi * (i + 1) / 10))\n    X = np.array(range(1, 11)).reshape(-1, 1)\n    Y = np.array(detrended_data)\n    model = LinearRegression().fit(X, Y)\n    base_consumption_day15 = model.predict(np.array([[15]]))[0]\n    base_consumption_day15_with_fluc = base_consumption_day15 + 10 * math.sin(2 * math.pi * 15 / 10)\n    final_consumption = round(base_consumption_day15_with_fluc)\n    final_consumption += round(0.05 * final_consumption)\n    return final_consumption"}
{"task_id": 92, "completion_id": 8, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\nDAYS = 10\nFUTURE_DAY = 15\nMARGIN = 1.05\ndef power_grid_forecast(consumption_data):\n    consumption_data = consumption_data.copy()\n    fluctuation = [10 * math.sin(2 * PI * i / DAYS) for i in range(1, DAYS + 1)]\n    detrended_data = consumption_data - fluctuation\n    x = np.array(range(1, DAYS + 1)).reshape((-1, 1))\n    model = LinearRegression().fit(x, detrended_data)\n    intercept = model.intercept_\n    slope = model.coef_[0]\n    day_15_base_consumption = intercept + slope * FUTURE_DAY\n    day_15_fluctuation = 10 * math.sin(2 * PI * FUTURE_DAY / DAYS)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_integer = math.ceil(day_15_consumption * MARGIN)\n    return final_integer\nconsumption_data = np.array([10, 12, 15, 18, 21, 24, 27, 30, 33, 36])"}
{"task_id": 92, "completion_id": 9, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    n = len(consumption_data)\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, n + 1)]\n    detrended_data = np.array(consumption_data) - np.array(fluctuation)\n    x = np.array(range(1, n + 1)).reshape((-1, 1))\n    y = detrended_data\n    model = LinearRegression().fit(x, y)\n    day15_base_consumption = model.predict(np.array([[n + 1]]))[0]\n    day15_fluctuation = 10 * math.sin(2 * PI * (n + 1) / 10)\n    day15_consumption_pred = day15_base_consumption + day15_fluctuation\n    final_consumption = math.ceil(day15_consumption_pred * 1.05)\n    return int(final_consumption)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    differences = np.abs(y_true - y_pred)\n    mae = np.mean(differences)\n    return round(mae, 3)"}
{"task_id": 93, "completion_id": 1, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return 'Error: The arrays must have the same length.'\n    else:\n        return round(np.mean(np.abs(y_true - y_pred)), 3)\ny_true = np.array([0, 1, 2, 3, 4, 5])\ny_pred = np.array([0, 1.5, 1.7, 2.9, 4.1, 5.2])"}
{"task_id": 93, "completion_id": 2, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n    \n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    return np.round(np.mean(np.abs(y_true - y_pred)), 3)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.9, 5.1])"}
{"task_id": 93, "completion_id": 3, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Arrays are not of same length.')\n    errors = np.abs(y_true - y_pred)\n    mae = np.mean(errors)\n    return round(mae, 3)"}
{"task_id": 93, "completion_id": 4, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Arrays must have the same length')\n    abs_diff = np.abs(y_true - y_pred)\n    mean_abs_diff = np.mean(abs_diff)\n    return round(mean_abs_diff, 3)"}
{"task_id": 93, "completion_id": 5, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    abs_diff = np.abs(y_true - y_pred)\n    mae = np.mean(abs_diff)\n    mae_rounded = round(mae, 3)\n    return mae_rounded\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])"}
{"task_id": 93, "completion_id": 6, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    return np.mean(np.abs(y_true - y_pred)).round(3)"}
{"task_id": 93, "completion_id": 7, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    errors = np.abs(y_true - y_pred)\n    mae = np.mean(errors).round(3)\n    return mae\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 1.9, 3.2, 3.8, 5.1])"}
{"task_id": 93, "completion_id": 8, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values.\n    y_pred (numpy.ndarray): Array of predicted values.\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    abs_diff = np.abs(y_true - y_pred)\n    mae = abs_diff.mean()\n    return round(mae, 3)"}
{"task_id": 93, "completion_id": 9, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'Length of actual and predicted arrays do not match'\n    mae = np.sum(np.abs(y_true - y_pred)) / len(y_true)\n    return round(mae, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = Q.shape[-1]\n    attention_scores = np.dot(Q, K.transpose()) / np.sqrt(d_k)\n    attention_weights = softmax(attention_scores, axis=-1)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    d_model = Q.shape[-1]\n    head_dim = d_model // n_heads\n    Q_split = np.array(np.split(Q, n_heads, axis=-1))\n    K_split = np.array(np.split(K, n_heads, axis=-1))\n    V_split = np.array(np.split(V, n_heads, axis=-1))\n    output_split = [self_attention(Q_split[i], K_split[i], V_split[i]) for i in range(n_heads)]\n    output_concat = np.concatenate(output_split, axis=-1)\n    output_concat_re = np.round(output_concat, 4)\n    output_list = output_concat_re.tolist()\n    return output_list"}
{"task_id": 94, "completion_id": 1, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    def compute_qkv(X, W_q, W_k, W_v):\n        return (np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v))\n\n    def self_attention(Q, K, V):\n        d_k = Q.shape[-1]\n        scores = np.dot(Q, K.T) / np.sqrt(d_k)\n        attention = np.softmax(scores, axis=-1)\n        return np.dot(attention, V)\n    (n_seq, d_model) = X.shape\n    (d_k, d_v) = (d_model // n_heads, d_model // n_heads)\n    W_q = W_q.reshape(d_model, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads * d_k, d_model)\n    W_k = W_k.reshape(d_model, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads * d_k, d_model)\n    W_v = W_v.reshape(d_model, n_heads, d_v).transpose(1, 0, 2).reshape(n_heads * d_v, d_model)\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q = Q.reshape(n_seq, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads * n_seq, d_k)\n    K = K.reshape(n_seq, n_heads, d_k).transpose(1, 0, 2).reshape(n_heads * n_seq, d_k)\n    V = V.reshape(n_seq, n_heads, d_v).transpose(1, 0, 2).reshape(n_heads * n_seq, d_v)\n    output = self_attention(Q, K, V)\n    output = output.reshape(n_heads, n_seq, d_v).transpose(1, 0, 2).reshape(n_seq, n_heads * d_v)\n    return np.around(output, decimals=4).tolist()"}
{"task_id": 94, "completion_id": 2, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    heads = [self_attention(Q[i], K[i], V[i]) for i in range(n_heads)]\n    concat_res = np.concatenate(heads, axis=-1)\n    return concat_res.tolist()\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray):\n    return (np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v))\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attn_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    result = np.dot(attn_weights, V)\n    return result"}
{"task_id": 94, "completion_id": 3, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention mechanism\n    :param Q: Query matrix\n    :param K: Key matrix\n    :param V: Value matrix\n    :return: Self-attention matrix\n    \"\"\"\n    d_k = Q.shape[-1]\n    attention_scores = np.matmul(Q, K.transpose()) / np.sqrt(d_k)\n    self_attention = np.matmul(attention_scores, V)\n    return self_attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism\n    :param X: Input matrix\n    :param W_q: Query weight matrix\n    :param W_k: Key weight matrix\n    :param W_v: Value weight matrix\n    :param n_heads: Number of attention heads\n    :return: Multi-head attention matrix\n    \"\"\"\n    (Q, K, V) = ([], [], [])\n    (q_chunk_dim, k_chunk_dim, v_chunk_dim) = (W_q.shape[0] // n_heads, W_k.shape[0] // n_heads, W_v.shape[0] // n_heads)\n    for i in range(n_heads):\n        Q.append(np.dot(X, W_q[i * q_chunk_dim:(i + 1) * q_chunk_dim]))\n        K.append(np.dot(X, W_k[i * k_chunk_dim:(i + 1) * k_chunk_dim]))\n        V.append(np.dot(X, W_v[i * v_chunk_dim:(i + 1) * v_chunk_dim]))\n    self_attention_vals = []\n    for i in range(n_heads):\n        self_attention_vals.append(self_attention(Q[i], K[i], V[i]))\n    multi_head_output = np.concatenate(self_attention_vals, axis=1)\n    return multi_head_output.round(4).tolist()"}
{"task_id": 94, "completion_id": 4, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    weights = np.round_(np.softmax(scores, axis=-1), 4)\n    attention = np.round_(np.dot(weights, V), 4)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    head_outputs = []\n    for _ in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n        attention = self_attention(Q, K, V)\n        head_outputs.append(attention)\n    multi_head_output = np.concatenate(head_outputs, axis=-1)\n    return multi_head_output.tolist()"}
{"task_id": 94, "completion_id": 5, "solution": "import numpy as np\ndef compute_qkv(x: np.ndarray, w_q: np.ndarray, w_k: np.ndarray, w_v: np.ndarray, n_heads: int) -> list:\n    (_, n_features) = x.shape\n    d_k = n_features // n_heads\n    q = np.dot(x, w_q)\n    k = np.dot(x, w_k)\n    v = np.dot(x, w_v)\n    return [q, k, v, d_k]\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray, d_k: int) -> np.ndarray:\n    scores = np.matmul(q, k.transpose()) / np.sqrt(d_k)\n    attention = np.matmul(scores, v)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (q, k, v, d_k) = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    head_attention = []\n    for i in range(n_heads):\n        head_q = q[:, i * d_k:(i + 1) * d_k]\n        head_k = k[:, i * d_k:(i + 1) * d_k]\n        head_v = v[:, i * d_k:(i + 1) * d_k]\n        attention = self_attention(head_q, head_k, head_v, d_k)\n        head_attention.append(attention)\n    return [np.around(head_att.tolist(), 4) for head_att in head_attention]"}
{"task_id": 94, "completion_id": 6, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    def compute_qkv(X, W_q, W_k, W_v):\n        Q = np.matmul(X, W_q)\n        K = np.matmul(X, W_k)\n        V = np.matmul(X, W_v)\n        return (Q, K, V)\n\n    def self_attention(Q, K, V):\n        dk = np.sqrt(Q.shape[1])\n        weights = np.matmul(Q, K.T) / dk\n        weights = np.nan_to_num(weights)\n        attention = np.matmul(weights, V)\n        return attention\n    X_splits = np.split(X, n_heads, axis=1)\n    W_q_splits = np.split(W_q, n_heads, axis=1)\n    W_k_splits = np.split(W_k, n_heads, axis=1)\n    W_v_splits = np.split(W_v, n_heads, axis=1)\n    results = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X_splits[i], W_q_splits[i], W_k_splits[i], W_v_splits[i])\n        attention = self_attention(Q, K, V)\n        results.append(attention)\n    multi_head_result = np.concatenate(results, axis=1)\n    multi_head_result = np.round(multi_head_result, 4)\n    return multi_head_result.tolist()"}
{"task_id": 94, "completion_id": 7, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    This function computes Query (Q), Key (K), and Value (V) matrices.\n    - X is the input to get Q, K, V for, in the shape (n_seq, d_model)\n    - W_q, W_k, W_v are weight matrices of shape (d_model, d_model)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray):\n    \"\"\"\n    This function calculates scaled dot-product self_attention.\n    - Q, K, V after computing with compute_qkv function\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention = np.matmul(np.exp(scores), V)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int):\n    \"\"\"\n    This function computes multi-head attention\n    - X is the input sequence of shape (n_seq, d_model)\n    - W_q, W_k, W_v are the weights for each head, each of shape (d_model, d_model)\n    - n_heads is the number of heads\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    head_size = Q.shape[-1] // n_heads\n    heads_Q = np.array(np.split(Q, n_heads, axis=-1))\n    heads_K = np.array(np.split(K, n_heads, axis=-1))\n    heads_V = np.array(np.split(V, n_heads, axis=-1))\n    head_outputs = [self_attention(heads_Q[i], heads_K[i], heads_V[i]) for i in range(n_heads)]\n    output = np.concatenate(head_outputs, axis=-1)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 94, "completion_id": 8, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    def compute_qkv(X, W_q, W_k, W_v):\n        return (np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v))\n\n    def self_attention(q, k, v):\n        scores = np.matmul(q, k.T)\n        weights = np.matmul(scores, v)\n        return weights\n    results = []\n    n_batches = X.shape[0]\n    for i in range(n_heads):\n        (q, k, v) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        attention_weights = self_attention(q, k, v)\n        results.append(attention_weights.round(4).tolist())\n    return results\nn_batches = 2\nseq_length = 3\nd_model = 512\nn_heads = 8\nX = np.random.rand(n_batches, seq_length, d_model)\nW_q = [np.random.rand(d_model, d_model) for _ in range(n_heads)]\nW_k = [np.random.rand(d_model, d_model) for _ in range(n_heads)]\nW_v = [np.random.rand(d_model, d_model) for _ in range(n_heads)]"}
{"task_id": 94, "completion_id": 9, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    multi_head_attn = []\n    for i in range(n_heads):\n        q = Q[:, i * X.shape[1] // n_heads:(i + 1) * X.shape[1] // n_heads]\n        k = K[:, i * X.shape[1] // n_heads:(i + 1) * X.shape[1] // n_heads]\n        v = V[:, i * X.shape[1] // n_heads:(i + 1) * X.shape[1] // n_heads]\n        attention = self_attention(q, k, v)\n        multi_head_attn.append(attention)\n    multi_head_attn = np.concatenate(multi_head_attn, axis=1)\n    return multi_head_attn.tolist()\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray) -> np.ndarray:\n    scores = np.dot(q, k.T)\n    weights = np.dot(scores, v)\n    return weights"}
{"task_id": 95, "completion_id": 0, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    contingency_table = [[0, 0], [0, 0]]\n    for i in range(n):\n        if x[i] == 0 and y[i] == 0:\n            contingency_table[0][0] += 1\n        elif x[i] == 0 and y[i] == 1:\n            contingency_table[0][1] += 1\n        elif x[i] == 1 and y[i] == 0:\n            contingency_table[1][0] += 1\n        elif x[i] == 1 and y[i] == 1:\n            contingency_table[1][1] += 1\n    a = contingency_table[0][0]\n    b = contingency_table[0][1]\n    c = contingency_table[1][0]\n    d = contingency_table[1][1]\n    phi = (a * d - b * c) / math.sqrt((a + b) * (a + c) * (b + d) * (c + d))\n    phi = round(phi, 4)\n    return phi"}
{"task_id": 95, "completion_id": 1, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    assert len(x) == len(y), 'Input lists must be of equal length'\n    (num_00, num_01, num_10, num_11) = (0, 0, 0, 0)\n    for (xi, yi) in zip(x, y):\n        if xi == 0 and yi == 0:\n            num_00 += 1\n        elif xi == 0 and yi == 1:\n            num_01 += 1\n        elif xi == 1 and yi == 0:\n            num_10 += 1\n        else:\n            num_11 += 1\n    phi = (num_11 * num_00 - num_10 * num_01) / ((num_11 + num_10) * (num_11 + num_01) * (num_01 + num_00) * (num_10 + num_00)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 2, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    agree = 0\n    for i in range(n):\n        if x[i] == y[i]:\n            agree += 1\n    disagree = n - agree\n    total_agree = sum(x) * sum(y)\n    total_disagree = sum([1 - i for i in x]) * sum([1 - j for j in y])\n    phi = (agree * disagree - total_disagree) / ((n ** 2 * (n - 1)) ** 0.5 if n > 1 else 1)\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 3, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n00 = n01 = n10 = n11 = 0\n    for i in range(len(x)):\n        if x[i] == 1 and y[i] == 1:\n            n11 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n10 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n01 += 1\n        else:\n            n00 += 1\n    phi = (n11 * n00 - n10 * n01) / ((n11 + n10) * (n11 + n01) * (n01 + n00) * (n10 + n00)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 4, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    for val in x:\n        if val not in (0, 1):\n            raise ValueError('x list values can only be either 0 or 1.')\n    for val in y:\n        if val not in (0, 1):\n            raise ValueError('y list values can only be either 0 or 1.')\n    n1 = sum(y)\n    n0 = len(y) - n1\n    n11 = sum([i[0] * i[1] for i in zip(x, y)])\n    n10 = sum(x) - n11\n    n01 = n1 - n11\n    phi = (n11 * n00 - n10 * n01) / (n1 * n0 * (n * n1) * (n * n0)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 5, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n00 = n01 = n10 = n11 = 0\n    for i in range(len(x)):\n        if x[i] == 0 and y[i] == 0:\n            n00 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n01 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n10 += 1\n        elif x[i] == 1 and y[i] == 1:\n            n11 += 1\n    phi_coeff = (n00 * n11 - n01 * n10) / ((n11 + n01) * (n11 + n10) * (n01 + n00) * (n00 + n10)) ** 0.5\n    return round(phi_coeff, 4)"}
{"task_id": 95, "completion_id": 6, "solution": "from typing import List\nimport math\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if not (all((i in (0, 1) for i in x)) and all((i in (0, 1) for i in y))):\n        raise ValueError('Inputs must be lists of 0s and 1s')\n    if len(x) != len(y):\n        raise ValueError('Inputs must be of the same length')\n    n00 = sum((i == 0 and j == 0 for (i, j) in zip(x, y)))\n    n01 = sum((i == 0 and j == 1 for (i, j) in zip(x, y)))\n    n10 = sum((i == 1 and j == 0 for (i, j) in zip(x, y)))\n    n11 = sum((i == 1 and j == 1 for (i, j) in zip(x, y)))\n    phi = (n11 * n00 - n10 * n01) / math.sqrt((n11 + n10) * (n11 + n01) * (n00 + n01) * (n00 + n10)) if (n11 + n10) * (n11 + n01) * (n00 + n01) * (n00 + n10) != 0 else 0\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 7, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Lists must be the same length.')\n    n11 = sum((xi and yi for (xi, yi) in zip(x, y)))\n    n10 = sum((xi and (not yi) for (xi, yi) in zip(x, y)))\n    n01 = sum((not xi and yi for (xi, yi) in zip(x, y)))\n    n00 = sum((not xi and (not yi) for (xi, yi) in zip(x, y)))\n    n1_ = n11 + n10\n    n_1 = n11 + n01\n    n0_ = n00 + n01\n    n_0 = n00 + n10\n    phi = (n11 * n00 - n10 * n01) / math.sqrt(n1_ * n0_ * n_1 * n_0) if n1_ * n0_ * n_1 * n_0 != 0 else float('nan')\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [1, 1, 0, 0, 1, 0]"}
{"task_id": 95, "completion_id": 8, "solution": "import math\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    if n != len(y):\n        raise ValueError('x and y must have the same length.')\n    observed_agreement = 0.0\n    observed_disagreement = 0.0\n    for i in range(n):\n        if x[i] == y[i]:\n            if x[i] == 1:\n                observed_agreement += 1.0\n            else:\n                observed_agreement += 0.5\n        elif x[i] == 1:\n            observed_disagreement += 1.0\n        else:\n            observed_disagreement += 0.5\n    expected_agreement = x.count(1) * (y.count(1) / n) + x.count(0) * (y.count(0) / n)\n    expected_disagreement = n - expected_agreement\n    phi = (observed_agreement - expected_agreement) / math.sqrt(expected_agreement * expected_disagreement)\n    return round(phi, 4)\nx = [1, 1, 0, 0, 1, 1, 0, 0]\ny = [1, 1, 0, 0, 1, 0, 0, 1]"}
{"task_id": 95, "completion_id": 9, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n = len(x)\n    a_b_c_d = [[0, 0], [0, 0]]\n    for i in range(n):\n        a_b_c_d[x[i]][y[i]] += 1\n    a = a_b_c_d[0][0]\n    b = a_b_c_d[0][1]\n    c = a_b_c_d[1][0]\n    d = a_b_c_d[1][1]\n    phi = (a * d - b * c) / ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 1, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 2, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 3, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 4, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 5, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 6, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 7, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0\n    elif x > 2.5:\n        return 1\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 8, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 9, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 1, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    val = alpha * (math.exp(x) - 1) if x < 0 else x\n    val = round(val, 4)\n    return val"}
{"task_id": 97, "completion_id": 2, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 3, "solution": "import math\nimport math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value.\n        alpha (float): ELU parameter for negative values (default: 1.0).\n\n    Returns:\n        float: ELU activation value.\n    \"\"\"\n    return round(x if x >= 0 else alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 4, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 5, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    return round(x if x >= 0 else alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 6, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 7, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 8, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 9, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 98, "completion_id": 1, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 2, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 3, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 4, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 5, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 98, "completion_id": 6, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 7, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 8, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 9, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0.0, x) + alpha * min(0.0, x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    return round(math.log(1 + math.exp(-abs(x)) + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 1, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 700:\n        return x\n    if x < -700:\n        return 0.0001\n    softplus_val = math.log(1 + math.exp(x))\n    return round(softplus_val, 4)"}
{"task_id": 99, "completion_id": 2, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 500:\n        return math.log(1 + math.exp(500))\n    elif x < -500:\n        return 0.0\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 3, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    try:\n        if x > 500:\n            return float('inf')\n        elif x < -500:\n            return 0\n        else:\n            return round(math.log1p(math.exp(x)), 4)\n    except OverflowError:\n        return float('inf')\n    except (ValueError, TypeError):\n        print('Invalid input')"}
{"task_id": 99, "completion_id": 4, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    try:\n        result = math.log1p(math.exp(x))\n        return round(result, 4)\n    except OverflowError:\n        return float('inf')"}
{"task_id": 99, "completion_id": 5, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    Args:\n        x: Input value\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    try:\n        result = math.log(1 + math.exp(x))\n    except OverflowError:\n        result = float('inf')\n    except:\n        raise\n    return float('{:.4f}'.format(result))"}
{"task_id": 99, "completion_id": 6, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    try:\n        result = math.log1p(math.exp(x))\n        return round(result, 4)\n    except OverflowError:\n        return float('inf')"}
{"task_id": 99, "completion_id": 7, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return x\n    elif x < -100:\n        return math.exp(x)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 8, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the Softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return x\n    elif x < -100:\n        return 0\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 9, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value\n    \"\"\"\n    try:\n        result = math.log1p(math.exp(x))\n        if result > 709.78:\n            return float('inf')\n        elif result < -708.39:\n            return 0.0\n        else:\n            return round(result, 4)\n    except OverflowError:\n        return float('inf')"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 1, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 2, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    softsign = x / (1 + abs(x))\n    return round(softsign, 4)"}
{"task_id": 100, "completion_id": 3, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 4, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 5, "solution": "import math\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    numerator = x\n    denominator = 1 + math.fabs(x)\n    result = numerator / denominator\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 6, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 7, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 8, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 9, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01):\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, a_min=1 - epsilon, a_max=1 + epsilon)\n    surrogate_obj = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_div = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_obj = surrogate_obj - beta * kl_div\n    grpo_obj = np.round(grpo_obj, 6)\n    return grpo_obj"}
{"task_id": 101, "completion_id": 1, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, a_min=1 - epsilon, a_max=1 + epsilon)\n    surrogate_objective = np.minimum(rhos * A, clipped_rhos * A)\n    kl_div = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_objective = np.mean(surrogate_objective) + beta * kl_div\n    grpo_objective = round(grpo_objective, 6)\n    return grpo_objective"}
{"task_id": 101, "completion_id": 2, "solution": "import numpy as np\nfrom scipy.stats import entropy\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    J_clip = np.mean(clipped_rhos * A)\n    D_KL = entropy(pi_theta_old, qk=pi_theta_ref)\n    J_grpo = J_clip + beta * D_KL\n    return np.round(J_grpo, 6)"}
{"task_id": 101, "completion_id": 3, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    surrogate_objective = np.clip(rhos, 1 - epsilon, 1 + epsilon) * A\n    kl_penalty = beta * np.sum(pi_theta_ref * (np.log(pi_theta_ref) - np.log(pi_theta_old)))\n    grpo_obj = np.sum(surrogate_objective) + kl_penalty\n    grpo_obj = round(grpo_obj, 6)\n    return grpo_obj"}
{"task_id": 101, "completion_id": 4, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clip = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    p_A = clip * A\n    rho_new = pi_theta_ref / pi_theta_old\n    kl = np.where(pi_theta_old <= 0, np.log(pi_theta_old / pi_theta_ref), pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    kl_beta = beta * np.sum(kl)\n    objective = np.mean(p_A) - kl_beta\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 5, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rho_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    J_clip = np.minimum(rho_clipped * A, np.clip(rhos, 1 - epsilon, 1 + epsilon) * A).mean()\n    KL_divergence = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    final_objective = J_clip - beta * KL_divergence\n    return round(final_objective, 6)"}
{"task_id": 101, "completion_id": 6, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    (pi_theta_old, pi_theta_ref, rhos, A) = (np.array(pi_theta_old), np.array(pi_theta_ref), np.array(rhos), np.array(A))\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_objective = np.minimum(rhos * A, clipped_rhos * A)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    return (np.mean(surrogate_objective - beta * kl_divergence), np.round(kl_divergence, 6))"}
{"task_id": 101, "completion_id": 7, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_advantage = clipped_rhos * A\n    surrogate_gradients = clipped_rhos * (rhos - 1) / np.clip(rhos, epsilon, None)\n    mean_gradients_advantage = np.mean(surrogate_gradients * surrogate_advantage)\n    kl_divergence = np.mean(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_objective = mean_gradients_advantage - beta * kl_divergence\n    return grpo_objective\nrhos = np.array([0.5, 1.2, 1.5])\nA = np.array([0.2, 0.3, 0.4])\npi_theta_old = np.array([0.1, 0.2, 0.7])\npi_theta_ref = np.array([0.2, 0.3, 0.5])\nepsilon = 0.2\nbeta = 0.01"}
{"task_id": 101, "completion_id": 8, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    if len(rhos) != len(A) or len(rhos) != len(pi_theta_old) or len(rhos) != len(pi_theta_ref):\n        raise ValueError('All input lists should have the same length')\n    rhos_clipped = np.clip(rhos, a_min=1 - epsilon, a_max=1 + epsilon)\n    pi_theta_old_array = np.array(pi_theta_old)\n    pi_theta_ref_array = np.array(pi_theta_ref)\n    kl_div = np.sum(pi_theta_old_array * np.log(pi_theta_old_array / pi_theta_ref_array))\n    objective = np.mean(rhos_clipped * A) - beta * kl_div\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 9, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    assert rhos.shape == A.shape == pi_theta_old.shape == pi_theta_ref.shape\n    clipped_obj = np.minimum(rhos * A, np.clip(rhos, 1 - epsilon, 1 + epsilon) * A)\n    surrogate_obj = -np.minimum(pi_theta_old * A, np.clip(pi_theta_old, 1 - epsilon, 1 + epsilon) * A)\n    kl_div = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_obj = clipped_obj + surrogate_obj - beta * kl_div\n    return float(np.sum(grpo_obj))\nrhos = np.random.random(10)\nA = np.random.random(10)\npi_theta_old = np.random.random(10)\npi_theta_ref = np.random.random(10)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 102, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    beta = 1.0\n    swish_output = x * sigmoid(beta * x)\n    return round(swish_output, 4)\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Sigmoid activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Sigmoid activation value\n    \"\"\"\n    return 1 / (1 + math.exp(-x))"}
{"task_id": 102, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef swish(x):\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value. It can be a scalar, list or a numpy array.\n\n    Returns:\n        The Swish activation value. It will be same type as input.\n    \"\"\"\n    if type(x) == list:\n        return [round(x_i * sigmoid(x_i), 4) for x_i in x]\n    elif type(x) == np.ndarray:\n        return np.round(x * sigmoid(x), 4)\n    else:\n        return round(x * sigmoid(x), 4)"}
{"task_id": 102, "completion_id": 5, "solution": "import numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    swish_val = x / (1 + np.exp(-x))\n    swish_val = round(swish_val, 4)\n    return swish_val"}
{"task_id": 102, "completion_id": 6, "solution": "import math\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return float(np.round(x * sigmoid(x), 4))\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Compute sigmoid value.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The sigmoid value\n    \"\"\"\n    return 1 / (1 + math.exp(-x))"}
{"task_id": 102, "completion_id": 7, "solution": "import math\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 102, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * (alpha * np.exp(x) - alpha), 4)"}
{"task_id": 103, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * (alpha * np.exp(x) - alpha)\n    return round(selu_value, 4)"}
{"task_id": 103, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x <= 0:\n        return scale * alpha * (np.exp(x) - 1)\n    else:\n        return scale * x"}
{"task_id": 103, "completion_id": 5, "solution": "import math\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)"}
{"task_id": 103, "completion_id": 6, "solution": "import math\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\nx = 3.4"}
{"task_id": 103, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        value = scale * x\n    else:\n        value = scale * alpha * (np.exp(x) - 1)\n    return round(value, 4)"}
{"task_id": 103, "completion_id": 8, "solution": "import numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return scale * x\n    else:\n        return scale * alpha * (np.exp(x) - 1)"}
{"task_id": 103, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Compute the sigmoid of x\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_model = np.dot(X, weights) + bias\n    y_pred = sigmoid(linear_model)\n    y_pred[y_pred >= 0.5] = 1\n    y_pred[y_pred < 0.5] = 0\n    return y_pred.tolist()"}
{"task_id": 104, "completion_id": 1, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probs = 1 / (1 + np.exp(-z))\n    predictions = (probs >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Computes the sigmoid function for a given input.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Helper function to compute sigmoid function value.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Args:\n        z: A scalar or numpy array of any size.\n\n    Returns:\n        s: sigmoid(z)\n    \"\"\"\n    s = 1 / (1 + np.exp(-z))\n    return s\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Predicted class labels (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    A = sigmoid(z)\n    predictions = (A >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 6, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    sigmoid_output = 1 / (1 + np.exp(-linear_output))\n    y_predicted = (sigmoid_output >= 0.5).astype(int)\n    return y_predicted.tolist()"}
{"task_id": 104, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Computes the sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Computes the sigmoid function of the input.\n\n    Args:\n        z: Input value\n\n    Returns:\n        Computed value\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if p >= 0.5 else 0 for p in probabilities]\n    return predictions\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nweights = np.array([0.1, 0.2, 0.3])\nbias = 0.4\npredictions = predict_logistic(X, weights, bias)"}
{"task_id": 104, "completion_id": 9, "solution": "import numpy as np\nimport numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_step = np.dot(X, weights) + bias\n    class_probabilities = sigmoid(linear_step)\n    binary_predictions = (class_probabilities >= 0.5).astype(int).tolist()\n    return binary_predictions\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z):\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / np.sum(e_z, axis=1, keepdims=True)\ndef cross_entropy(p, y):\n    return -np.sum(y * np.log(p))\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int):\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_classes = y.shape[1]\n    np.random.seed(101)\n    W = np.random.rand(num_features, num_classes)\n    b = np.random.rand(num_classes)\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, W) + b\n        predictions = softmax(z)\n        loss = cross_entropy(predictions, y)\n        losses.append(loss)\n        error = predictions - y\n        gradient_W = np.dot(X.T, error)\n        gradient_b = np.sum(error, axis=0)\n        W -= learning_rate * gradient_W\n        b -= learning_rate * gradient_b\n    B = np.vstack([b, W]).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)"}
{"task_id": 105, "completion_id": 1, "solution": "import numpy as np\ndef softmax(z):\n    exp = np.exp(z - np.max(z))\n    return exp / exp.sum(axis=0)\ndef cross_entropy(predicted, actual):\n    indices = np.argmax(actual, axis=0)\n    predicted = predicted[indices, range(len(indices))]\n    return -np.mean(-np.log(predicted + 1e-30))\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    num_features = X.shape[1]\n    num_classes = y.shape[0]\n    weights = np.zeros((num_classes, num_features))\n    bias = np.zeros((num_classes, 1))\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(weights, X.T) + bias\n        prediction = softmax(z)\n        loss = cross_entropy(prediction, y)\n        losses.append(np.round(loss, 4))\n        dw = 1 / num_features * np.dot(prediction - y, X)\n        db = 1 / num_features * np.sum(prediction - y, axis=1, keepdims=True)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights.flatten(), 4).tolist()\n    bias = np.round(bias.flatten(), 4).tolist()\n    return (weights, bias, losses)"}
{"task_id": 105, "completion_id": 2, "solution": "import numpy as np\ndef train_softmaxreg(X, y, learning_rate, iterations):\n    (N, M) = X.shape\n    num_classes = len(set(y))\n    B = np.zeros(M)\n    C = np.zeros((num_classes, M))\n    X = np.array([B] * N).T\n    X = np.append(X, X, axis=0)\n    losses = []\n\n    def softmax(x):\n        exps = np.exp(x - np.max(x))\n        return exps / np.sum(exps)\n\n    def cross_entropy_loss(p, q):\n        return -np.sum(np.log(q) * p)\n    for _ in range(iterations):\n        Z = np.dot(C, X) + B\n        A = softmax(Z)\n        dZ = A - y\n        dB = np.sum(dZ, axis=1) / N\n        dC = np.dot(dZ, X.T) / N\n        B -= learning_rate * dB\n        C -= learning_rate * dC\n        loss = cross_entropy_loss(y, A)\n        losses.append(loss)\n    return (B.round(4).tolist(), C.round(4).tolist(), [round(l, 4) for l in losses])"}
{"task_id": 105, "completion_id": 3, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n    np.random.seed(10)\n    thetas = np.random.randn(X_bias.shape[1], len(np.unique(y)))\n    losses = []\n    m = len(X)\n    for iteration in range(iterations):\n        scores = np.dot(X_bias, thetas)\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        correct_logprobs = -np.log(probs[range(m), y])\n        loss = np.sum(correct_logprobs) / m\n        losses.append(round(float(loss), 4))\n        dscores = probs\n        dscores[range(m), y] -= 1\n        dscores /= m\n        thetas += -learning_rate * np.dot(X_bias.T, dscores)\n    return [np.round(thetas, 4).tolist(), losses]"}
{"task_id": 105, "completion_id": 4, "solution": "import numpy as np\ndef softmax(z):\n    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\ndef cross_entropy(y, y_hat):\n    return -np.sum(np.log(np.sum(y_hat * y, axis=1)))\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    num_features = X.shape[1]\n    num_classes = y.shape[1]\n    W = np.random.randn(num_features, num_classes) * 0.01\n    b = np.random.randn(1, num_classes) * 0.01\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, W) + b\n        y_hat = softmax(z)\n        loss = cross_entropy(y, y_hat)\n        losses.append(round(loss, 4))\n        dz = y_hat - y\n        dW = np.dot(X.T, dz)\n        db = np.sum(dz, axis=0, keepdims=True)\n        W -= learning_rate * dW\n        b -= learning_rate * db\n    return (np.round(W, 4).tolist(), np.round(b, 4).tolist(), losses)"}
{"task_id": 105, "completion_id": 5, "solution": "import numpy as np\ndef softmax(z):\n    return np.exp(z) / np.exp(z).sum(axis=1, keepdims=True)\ndef cross_entropy(y_pred, y):\n    m = y.shape[0]\n    log_likelihood = -np.log(y_pred[range(m), y])\n    loss = np.sum(log_likelihood) / m\n    return loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    X = np.insert(X, 0, 1, axis=1)\n    theta = np.zeros([X.shape[1], y.max() + 1])\n    losses = []\n    for _ in range(iterations):\n        prob = softmax(np.dot(X, theta))\n        grad = np.dot(X.T, prob)\n        grad[range(X.shape[0]), y] -= X.shape[0]\n        grad /= X.shape[0]\n        theta -= learning_rate * grad\n        loss = cross_entropy(prob, y)\n        losses.append(loss)\n    theta_rounded = np.round(theta, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (theta_rounded, losses_rounded)"}
{"task_id": 105, "completion_id": 6, "solution": "import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\ndef softmax(z):\n    exp = np.exp(z - np.max(z))\n    return exp / np.sum(exp, axis=0)\ndef cross_entropy(Y, Y_hat):\n    N = Y.shape[1]\n    loss = -np.sum(np.multiply(Y, np.log(Y_hat + 1e-06))) / N\n    return loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    onehot_encoder = OneHotEncoder(sparse=False)\n    y_onehot = onehot_encoder.fit_transform(y.reshape(-1, 1))\n    (n_samples, n_features) = X.shape\n    (_, n_classes) = y_onehot.shape\n    weights = np.random.randn(n_features, n_classes)\n    biases = np.random.randn(n_classes)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, weights) + biases\n        y_pred = softmax(z)\n        loss = cross_entropy(y_onehot, y_pred)\n        losses.append(loss)\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y_onehot)\n        db = 1 / n_samples * np.sum(y_pred - y_onehot, axis=0, keepdims=True)\n        weights -= learning_rate * dw\n        biases -= learning_rate * db\n    return (weights.round(4).tolist(), biases.round(4).tolist(), [round(loss, 4) for loss in losses])"}
{"task_id": 105, "completion_id": 7, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    weights = np.random.rand(X.shape[1], len(np.unique(y)))\n    losses = []\n    y_onehot = np.zeros((len(y), len(np.unique(y))))\n    y_onehot[np.arange(len(y)), y] = 1\n    for i in range(iterations):\n        z = np.dot(X, weights)\n        probs = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n        loss = -np.sum(np.log(np.sum(probs * y_onehot, axis=1)))\n        losses.append(loss)\n        grad = np.dot(X.T, probs - y_onehot)\n        weights -= learning_rate * grad\n    return ([round(val, 4) for val in weights.flatten().tolist()], [round(val, 4) for val in losses])"}
{"task_id": 105, "completion_id": 8, "solution": "import numpy as np\ndef softmax(z):\n    z -= np.max(z)\n    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1)).T\n    return sm\ndef cross_entropy(pred, real):\n    n_samples = real.shape[0]\n    res = pred - real\n    return np.sum(np.log(np.sum(np.exp(res), axis=1))) / n_samples\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        W : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    num_classes = y.shape[1]\n    W = np.random.randn(num_features, num_classes)\n    b = np.random.randn(num_classes)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, W) + b\n        predictions = softmax(z)\n        dW = 1 / num_samples * np.dot(X.T, predictions - y)\n        db = 1 / num_samples * np.sum(predictions - y, axis=0)\n        W -= learning_rate * dW\n        b -= learning_rate * db\n        loss = cross_entropy(predictions, y)\n        losses.append(loss)\n    return (W.round(4).tolist(), b.round(4).tolist(), [round(loss, 4) for loss in losses])"}
{"task_id": 105, "completion_id": 9, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n        Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n        Returns:\n            B : list[float], CxM updated parameter vector rounded to 4 floating points\n            losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n        \"\"\"\n    X = np.insert(X, 0, 1, axis=1)\n    num_features = X.shape[1]\n    num_classes = len(np.unique(y))\n    B = np.random.randn(num_features, num_classes)\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        correct_logprobs = -np.log(probs[range(len(y)), y])\n        data_loss = np.sum(correct_logprobs) / len(y)\n        losses.append(data_loss)\n        dscores = probs\n        dscores[range(len(y)), y] -= 1\n        dscores /= len(y)\n        dBo = X.T @ dscores\n        B -= learning_rate * dBo\n    B = np.round(B, 4).tolist()\n    losses = np.round(losses, 4).tolist()\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(y_pred, y):\n    return -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)).mean()\ndef gradient(X, y, y_pred):\n    error = y_pred - y\n    gradient = X.T.dot(error) / X.shape[0]\n    return gradient\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    loss_values = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        loss = binary_cross_entropy(y_pred, y)\n        loss_values.append(np.round(loss, 4))\n        grad = gradient(X, y, y_pred)\n        weights -= learning_rate * grad\n        bias -= learning_rate * np.mean(y_pred - y)\n    return (np.round(weights, 4).tolist(), np.round(bias, 4), loss_values)\nX = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 0, 1, 1])"}
{"task_id": 106, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Calculate sigmoid of a number\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"Calculate Binary Cross Entropy\"\"\"\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, \n    optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    loss_over_iterations = []\n    for i in range(iterations):\n        y_pred = sigmoid(np.dot(X, weights) + bias)\n        loss = binary_cross_entropy(y, y_pred)\n        loss_over_iterations.append(loss)\n        dw = 1 / X.shape[0] * np.dot(X.T, y_pred - y)\n        db = 1 / X.shape[0] * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = [round(weight, 4) for weight in weights]\n    bias = round(bias, 4)\n    loss_over_iterations = [round(loss, 4) for loss in loss_over_iterations]\n    return (weights, bias, loss_over_iterations)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])"}
{"task_id": 106, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy_loss(y_true, y_pred):\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    weights = np.zeros(X.shape[1])\n    loss_history = []\n    for _ in range(iterations):\n        y_pred = sigmoid(np.dot(X, weights))\n        weights -= learning_rate * np.dot(X.T, y_pred - y) / y.size\n        loss_history.append(round(binary_cross_entropy_loss(y, y_pred), 4))\n    return (weights.tolist(), loss_history)"}
{"task_id": 106, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(p: np.ndarray, y: np.ndarray) -> np.ndarray:\n    return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n    (num_examples, num_features) = X.shape\n    weights = np.random.rand(num_features)\n    weights = weights[:, np.newaxis]\n    loss_history = []\n    for iter in range(iterations):\n        p = sigmoid(np.dot(X, weights))\n        loss = binary_cross_entropy(p, y)\n        loss_history.append(loss)\n        dw = 1 / num_examples * np.dot(X.T, p - y)\n        weights -= learning_rate * dw\n    return (np.squeeze(weights).tolist(), [round(loss, 4) for loss in loss_history])"}
{"task_id": 106, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"\n    Sigmoid function to predict the probability of the instance being labeled 1.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef bce_loss(y_true, y_pred):\n    \"\"\"\n    Binary cross-entropy loss function.\n    \"\"\"\n    N = len(y_true)\n    loss = -1 / N * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\ndef train_logreg(X, y, learning_rate, iterations):\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with \n    Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    losses = []\n    for i in range(iterations):\n        linear_pred = np.dot(X, weights) + bias\n        predictions = sigmoid(linear_pred)\n        loss = bce_loss(y, predictions)\n        losses.append(loss)\n        dw = 1 / X.shape[0] * np.dot(X.T, predictions - y)\n        db = 1 / X.shape[0] * np.sum(predictions - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (weights.tolist(), round(bias, 4), [round(loss, 4) for loss in losses])"}
{"task_id": 106, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Compute sigmoid of array z.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy_loss(y_pred, y):\n    \"\"\"Compute binary cross-entropy loss.\"\"\"\n    m = y.shape[0]\n    loss = -(1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n    return np.squeeze(loss)\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros((n, 1))\n    losses = []\n    for iter in range(iterations):\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        loss = binary_cross_entropy_loss(y_pred, y)\n        gradient = np.dot(X.T, y_pred - y) / m\n        weights -= learning_rate * gradient\n        losses.append(round(loss, 4))\n    return (losses, weights.tolist())"}
{"task_id": 106, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function maps any value between 0 and 1\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef cost(h, y):\n    \"\"\"\n    Binary Cross Entropy cost function\n    \"\"\"\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\ndef train_logreg(X, y, learning_rate, iterations):\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    loss_history = []\n    for i in range(iterations):\n        z = np.dot(X, theta)\n        h = sigmoid(z)\n        gradient = np.dot(X.T, h - y) / m\n        theta -= learning_rate * gradient\n        loss_history.append(cost(h, y))\n    theta = [round(t, 4) for t in theta.tolist()]\n    loss_history = [round(loss, 4) for loss in loss_history]\n    return (theta, loss_history)"}
{"task_id": 106, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"\n    Returns sigmoid of x\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Returns Binary Cross Entropy loss\n    \"\"\"\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    loss_history = []\n    for i in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        loss = binary_cross_entropy(y, y_pred)\n        loss_history.append(loss)\n        dw = 1 / X.shape[0] * np.dot(X.T, y_pred - y)\n        db = 1 / X.shape[0] * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (np.round(weights, 4), np.round(bias, 4), np.round(loss_history, 4))"}
{"task_id": 106, "completion_id": 8, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    X = np.c_[np.ones(X.shape[0]), X]\n    loss_history = []\n    for i in range(iterations):\n        p = 1 / (1 + np.exp(-np.dot(X, weights)))\n        gradient = np.dot(X.T, p - y) / y.size\n        weights -= learning_rate * gradient\n        loss = (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n        loss_history.append(float(loss))\n    return (weights.round(4).tolist(), loss_history)"}
{"task_id": 106, "completion_id": 9, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef binary_cross_entropy_loss(y_pred, y):\n    n = len(y)\n    loss = -1 / n * np.sum(y * np.log(y_pred + 1e-09) + (1 - y) * np.log(1 - y_pred + 1e-09))\n    return loss\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Args:\n    @param X : feature data\n    @param y : labels\n    @param learning_rate : learning rate\n    @param iterations : number of iterations\n    \n    Returns:\n    @returns : Tuple containing list of loss values and coefficients\n    \"\"\"\n    coefs = np.zeros(X.shape[1])\n    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n    losses = []\n    for _ in range(iterations):\n        y_pred = sigmoid(np.dot(X, coefs))\n        loss = binary_cross_entropy_loss(y_pred, y)\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, y_pred - y) / y.shape[0]\n        coefs -= learning_rate * gradient\n    return (losses, np.round(coefs, 4))"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    scores = np.where(mask == 0, -np.inf, scores)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output.tolist()\nmask = np.random.randint(2, size=(3, 3))"}
{"task_id": 107, "completion_id": 1, "solution": "import numpy as np\nimport math\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    dot_product = np.dot(Q, K.transpose())\n    d_k = Q.shape[1]\n    scaled_attention_logits = dot_product / math.sqrt(d_k)\n    if mask is not None:\n        scaled_attention_logits += mask * -1000000000.0\n    attention_weights = np.exp(scaled_attention_logits - np.max(scaled_attention_logits, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 107, "completion_id": 2, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    score = np.dot(Q, K.T)\n    score_masked = score * mask - 1000000000.0 * (1 - mask)\n    score_softmaxed = np.exp(score_masked) / np.sum(np.exp(score_masked), axis=-1, keepdims=True)\n    attention = np.dot(score_softmaxed, V)\n    return attention.tolist()"}
{"task_id": 107, "completion_id": 3, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -np.inf, scores)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\nmask = np.random.randint(2, size=(4, 4))\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 107, "completion_id": 4, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    dot_product = np.dot(Q, np.transpose(K))\n    scaled_dot_product = dot_product / np.sqrt(K.shape[1])\n    scaled_dot_product[mask == 0] = -np.inf\n    attention_weights = np.exp(scaled_dot_product - np.max(scaled_dot_product))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 5, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q: The Query matrix, of shape (N, D).\n        K: The Key matrix, of shape (N, D).\n        V: The Value matrix, of shape (N, D).\n        mask: The mask matrix, of shape (N, N), where the mask is 1 where values are allowed and 0 where they are not.\n    \n    Returns:\n        The result of the masked self-attention, of shape (N, D).\n    \"\"\"\n    scores = np.matmul(Q, K.T) / np.sqrt(Q.shape[-1])\n    scores = np.where(mask == 1, scores, -1000000000.0)\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = weights / np.sum(weights, axis=-1, keepdims=True)\n    output = np.matmul(weights, V)\n    return output"}
{"task_id": 107, "completion_id": 6, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    attn_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attn_scores = np.where(mask, attn_scores, -np.inf)\n    attn_probs = np.exp(attn_scores - np.max(attn_scores, axis=-1, keepdims=True))\n    attn_probs = attn_probs / np.sum(attn_probs, axis=-1, keepdims=True)\n    output = np.dot(attn_probs, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 7, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    K = K / np.sqrt(d_k)\n    scores = np.dot(Q, K.T)\n    if mask is not None:\n        mask = mask.astype(bool)\n        scores = np.where(mask, scores, -np.inf)\n    scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    scores /= np.sum(scores, axis=-1, keepdims=True)\n    output = np.dot(scores, V)\n    return output.tolist()\nmask = np.array([[0, 1, 0], [0, 0, 1], [1, 1, 0]])\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 107, "completion_id": 8, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n    mask (np.ndarray): Attention mask.\n\n    Returns:\n    np.ndarray: Masked attention output.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.transpose()) / np.sqrt(d_k)\n    scores = np.where(mask, scores, -np.inf)\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = weights / np.sum(weights, axis=-1, keepdims=True)\n    output = np.matmul(weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 9, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    scores = np.where(mask, scores, -np.inf)\n    probabilities = softmax(scores, axis=-1)\n    attention = np.dot(probabilities, V)\n    return attention\ndef softmax(x: np.ndarray, axis=None) -> np.ndarray:\n    \"\"\"\n    Compute softmax values for each sets of scores in x.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    n = len(apples)\n    counts = Counter(apples)\n    entropy = -sum((count / n * math.log2(count / n) for count in counts.values()))\n    entropy = round(entropy, 4)\n    return entropy"}
{"task_id": 108, "completion_id": 1, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples or len(apples) == 0:\n        return 0.0\n    unique_colors = len(set(apples))\n    total_apples = len(apples)\n    disorder = unique_colors / total_apples\n    return round(disorder, 4)"}
{"task_id": 108, "completion_id": 2, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a bunch of apples based on their colors.\n    \"\"\"\n    total_num = len(apples)\n    count = Counter(apples)\n    disorder = 0.0\n    for num in count.values():\n        p = num / total_num\n        disorder += -p * math.log2(p)\n    return round(disorder, 4)"}
{"task_id": 108, "completion_id": 3, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    mean_color = sum(apples) / len(apples)\n    return round(sum(((color - mean_color) ** 2 for color in apples)) / len(apples), 4)"}
{"task_id": 108, "completion_id": 4, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    n = len(apples)\n    frequencies = [count / n for count in Counter(apples).values()]\n    entropy = -sum((p * math.log2(p) for p in frequencies))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    total_apples = len(apples)\n    count_colors = Counter(apples)\n    entropy = sum([-(count / total_apples) * math.log2(count / total_apples) for count in count_colors.values()])\n    disorder = round(entropy ** 2, 4)\n    return disorder"}
{"task_id": 108, "completion_id": 6, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    total_apples = len(apples)\n    apple_counter = Counter(apples)\n    entropy = -sum((apple_counter[i] / total_apples * math.log(apple_counter[i] / total_apples, 2) for i in apple_counter.keys()))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 7, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors using entropy.\n    \"\"\"\n    counter = Counter(apples)\n    num_apples = len(apples)\n    disorder_score = sum((-count / num_apples * math.log(count / num_apples) for count in counter.values()))\n    return round(disorder_score, 4)"}
{"task_id": 108, "completion_id": 8, "solution": "import math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    apple_colors = {}\n    for apple in apples:\n        if apple in apple_colors:\n            apple_colors[apple] += 1\n        else:\n            apple_colors[apple] = 1\n    total_apples = len(apples)\n    disorder = 0.0\n    for count in apple_colors.values():\n        p = count / total_apples\n        disorder += p * math.log2(p)\n    disorder = -disorder\n    return round(disorder, 4)"}
{"task_id": 108, "completion_id": 9, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    unique_colors = set(apples)\n    num_unique_colors = len(unique_colors)\n    total_apples = len(apples)\n    disorder = 1 - num_unique_colors / total_apples\n    return round(disorder, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized = (X - mean) / np.sqrt(variance + epsilon)\n    return np.round(gamma * normalized + beta, 5).tolist()\nX = np.random.randn(3, 4, 5)"}
{"task_id": 109, "completion_id": 1, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    std_dev = np.std(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(std_dev ** 2 + epsilon)\n    output = gamma * X_norm + beta\n    output = np.round(output, 5).tolist()\n    return output"}
{"task_id": 109, "completion_id": 2, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = X.mean(axis=-1, keepdims=True)\n    std = X.std(axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(std ** 2 + epsilon)\n    X_norm = gamma * X_norm + beta\n    return X_norm.round(5).tolist()"}
{"task_id": 109, "completion_id": 3, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(variance + epsilon)\n    return (gamma * normalized_X + beta).round(5).tolist()"}
{"task_id": 109, "completion_id": 4, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_hat = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * X_hat + beta\n    rounded_list = np.round(output, 5).tolist()\n    return rounded_list\nX = np.random.randn(2, 3, 4)\ngamma = np.ones((4,))\nbeta = np.zeros((4,))"}
{"task_id": 109, "completion_id": 5, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    sequence_means = np.mean(X, axis=-1, keepdims=True)\n    sequence_stds = np.std(X, axis=-1, keepdims=True)\n    normalized_X = (X - sequence_means) / np.sqrt(sequence_stds ** 2 + epsilon)\n    transformed_X = gamma * normalized_X + beta\n    return transformed_X.round(5).tolist()"}
{"task_id": 109, "completion_id": 6, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    X_mean = np.mean(X, axis=-1, keepdims=True)\n    X_var = np.var(X, axis=-1, keepdims=True)\n    X_hat = (X - X_mean) / np.sqrt(X_var + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(5).tolist()\nX = np.random.randn(2, 3, 4)\ngamma = np.ones(4)\nbeta = np.zeros(4)"}
{"task_id": 109, "completion_id": 7, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    X_mean = np.mean(X, axis=-1, keepdims=True)\n    X_var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - X_mean) / np.sqrt(X_var + epsilon)\n    output = gamma * X_norm + beta\n    return np.round(output, 5).tolist()"}
{"task_id": 109, "completion_id": 8, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    std_dev = np.std(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(std_dev ** 2 + epsilon)\n    output = gamma * X_normalized + beta\n    output = np.round(output, 5).tolist()\n    return output\nX = np.random.rand(2, 3, 4)\ngamma = np.random.rand(4)\nbeta = np.random.rand(4)\noutput = layer_normalization(X, gamma, beta)"}
{"task_id": 109, "completion_id": 9, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mu = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_hat = (X - mu) / np.sqrt(variance + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(5).tolist()\nX = np.array([[[1, 2], [3, 4], [5, 6]], [[7, 8], [9, 10], [11, 12]]], dtype=np.float32)\ngamma = np.array([1, 1], dtype=np.float32)\nbeta = np.array([0, 0], dtype=np.float32)"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef ngrams(tokens, n):\n    return list(zip(*[tokens[i:] for i in range(n)]))\ndef precision(reference, candidate, n):\n    return len([word for word in candidate if word in reference]) / len(candidate)\ndef recall(reference, candidate, n):\n    return len([word for word in candidate if word in reference]) / len(reference)\ndef f_mean(precision, recall, beta):\n    return (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\ndef penalty(fragments, candidate_length):\n    penalty_value = 0\n    for i in range(fragments):\n        penalty_value += fragments[i] ** 2 / candidate_length\n    return penalty_value\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    ref_len = len(reference)\n    can_len = len(candidate)\n    ref_bigram = ngrams(reference, 2)\n    can_bigram = ngrams(candidate, 2)\n    matching_terms = sum((Counter(ref_bigram) & Counter(can_bigram)).values())\n    fragmentation = abs(ref_len - can_len)\n    precision_val = precision(reference, candidate, 1)\n    recall_val = recall(reference, candidate, 1)\n    if precision_val == 0 or recall_val == 0:\n        f_mean_val = 0\n    else:\n        f_mean_val = f_mean(precision_val, recall_val, beta)\n    penalty_val = penalty(fragmentation, can_len)\n    final_score = alpha * f_mean_val * (1 - penalty_val)\n    return round(final_score, 3)"}
{"task_id": 110, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference_ngrams = []\n    reference_words = reference.strip().split()\n    for n in range(1, len(reference_words) + 1):\n        reference_ngrams.extend(zip(*[reference_words[i:] for i in range(n)]))\n    candidate_ngrams = []\n    candidate_words = candidate.strip().split()\n    for n in range(1, len(candidate_words) + 1):\n        candidate_ngrams.extend(zip(*[candidate_words[i:] for i in range(n)]))\n    match_count = 0\n    transposition_list = []\n    for ngram in candidate_ngrams:\n        if ngram in reference_ngrams:\n            match_count += 1\n        elif ngram[0] in reference_words and ngram[-1] in reference_words and (reference_words.index(ngram[-1]) > reference_words.index(ngram[0])):\n            transposition_list.append(ngram)\n    unigram_match_count = 0\n    for word in candidate_words:\n        if word in reference_words:\n            unigram_match_count += 1\n    P = match_count / len(candidate_ngrams)\n    R = unigram_match_count / len(reference_ngrams)\n    F = alpha * P * R / (alpha * P + (1 - alpha) * R) if alpha * P + (1 - alpha) * R != 0 else 0\n    T = len(transposition_list)\n    penalty = 1\n    for ngram in transposition_list:\n        cand_ngram_indices = [i for (i, n) in enumerate(candidate_words) if n in ngram]\n        ref_ngram_indices = [i for (i, n) in enumerate(reference_words) if n in ngram]\n        cand_lexical_distance = max((abs(i1 - i2) for i1 in cand_ngram_indices for i2 in ref_ngram_indices))\n        penalty *= np.power(gamma * cand_lexical_distance / len(candidate_words), beta)\n    return round(penalty * F, 3)"}
{"task_id": 110, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\nimport nltk\nfrom nltk.translate.meteor_score import meteor_score as nltk_meteor_score\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    if not isinstance(reference, str) or not isinstance(candidate, str):\n        print('Both reference and candidate should be strings')\n        return\n    ref_tokens = nltk.word_tokenize(reference.lower())\n    cand_tokens = nltk.word_tokenize(candidate.lower())\n    matching_ngrams = sum((Counter(ref_tokens) & Counter(cand_tokens)).values())\n    precision = matching_ngrams / (len(cand_tokens) + 1e-10)\n    recall = matching_ngrams / (len(ref_tokens) + 1e-10)\n    penalty = np.power(beta * precision * recall / (alpha * precision + (1 - alpha) * recall + 1e-10), gamma)\n    f_mean = (1 + np.power(beta, 2)) * (precision * recall / (np.power(beta, 2) * precision + recall + 1e-10))\n    return round(penalty * f_mean, 3)"}
{"task_id": 110, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def precision_recall(reference, candidate):\n        ref_counter = Counter(reference)\n        cand_counter = Counter(candidate)\n        term_count = sum(ref_counter.values())\n        match_count = sum((min(ref_counter[tok], cand_counter[tok]) for tok in reference if tok in cand_counter))\n        precision = match_count / max(len(candidate), 1.0)\n        recall = match_count / max(term_count, 1.0)\n        Fmean = (1 + gamma ** 2) * precision * recall / max(precision + gamma ** 2 * recall, 0.001)\n        return (precision, recall, Fmean)\n\n    def word_order(reference, candidate):\n        ref_length = len(reference)\n        cand_length = len(candidate)\n        match_count = 0\n        for i in range(ref_length):\n            if i + cand_length > ref_length:\n                break\n            if reference[i:i + cand_length] == candidate:\n                match_count += 1\n        word_order_frag = 1 - match_count / max(cand_length, 1.0)\n        return word_order_frag\n    (precision, recall, Fmean) = precision_recall(reference, candidate)\n    word_order_frag = word_order(reference, candidate)\n    score = Fmean * (1 - word_order_frag)\n    return round(score, 3)\nreference = ['this', 'is', 'a', 'test']\ncandidate = ['this', 'is', 'test']"}
{"task_id": 110, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def chunking(sequence, size):\n        return [sequence[i:i + size] for i in range(len(sequence) - size + 1)]\n\n    def unigram_match(reference, candidate):\n        ref_unigrams = Counter(reference)\n        cand_unigrams = Counter(candidate)\n        return sum((ref_unigrams & cand_unigrams).values())\n\n    def precision(reference, candidate):\n        return unigram_match(reference, candidate) / len(candidate)\n\n    def recall(reference, candidate):\n        return unigram_match(reference, candidate) / len(reference)\n\n    def fmean(precision, recall, beta):\n        return (1 + beta ** 2) * precision * recall / (recall + beta ** 2 * precision)\n\n    def fragmentation(reference, candidate):\n        ref_length = len(reference)\n        cand_length = len(candidate)\n        matching_count = 0\n        for chunk in chunking(reference, 3):\n            if chunk in candidate:\n                matching_count += 1\n        matching_count -= 6\n        penalty = np.exp(1 - matching_count / ref_length)\n        return penalty\n    ref = reference.lower().split()\n    cand = candidate.lower().split()\n    precision_val = precision(ref, cand)\n    recall_val = recall(ref, cand)\n    fmean_val = fmean(precision_val, recall_val, beta)\n    penalty_val = fragmentation(ref, cand)\n    meteor = alpha * fmean_val * penalty_val + (1 - alpha) * fmean_val\n    return round(meteor, 3)\nmeteor = meteor_score('Your reference translation here', 'Your candidate translation here')"}
{"task_id": 110, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef ngrams(sentence, n):\n    \"\"\"\n    This function returns the list of ngrams from a sentence.\n    \"\"\"\n    return list(zip(*[sentence[i:] for i in range(n)]))\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    This function calculates the METEOR score for evaluating machine translation quality.\n    \"\"\"\n    reference = reference.lower().split(' ')\n    candidate = candidate.lower().split(' ')\n    match_ngrams = len(set(ngrams(reference, 1)).intersection(ngrams(candidate, 1)))\n    precision = match_ngrams / len(candidate) if candidate else 0\n    recall = match_ngrams / len(reference) if reference else 0\n    fmean = (1 - alpha) * (precision * recall / (beta * precision + recall) if beta * precision + recall > 0 else 0)\n    penalty = gamma * sum([min((Counter(ngrams(candidate, i)) & Counter(ngrams(reference, i))).values()) for i in range(1, 5)]) / (4 * len(candidate)) if len(candidate) > 0 else 0\n    meteor = fmean * (1 - penalty)\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\nfrom nltk.translate.meteor_score import meteor_score as nltk_meteor_score"}
{"task_id": 110, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def exact_match(reference, candidate):\n        ref_words = Counter(reference.lower().split())\n        can_words = Counter(candidate.lower().split())\n        exact_match = sum((ref_words & can_words).values())\n        return exact_match\n\n    def precision(reference, candidate):\n        reference_count = len(reference.split())\n        return exact_match(reference, candidate) / reference_count\n\n    def recall(reference, candidate):\n        candidate_count = len(candidate.split())\n        return exact_match(reference, candidate) / candidate_count\n\n    def word_order_fragmentation(reference, candidate):\n        reference_count = len(reference.split())\n        reference_bigrams = [' '.join(x) for x in list(zip(reference.split()[:-1], reference.split()[1:]))]\n        candidate_bigrams = [' '.join(x) for x in list(zip(candidate.split()[:-1], candidate.split()[1:]))]\n        (ref_bigram_count, can_bigram_count) = (Counter(reference_bigrams), Counter(candidate_bigrams))\n        return sum((ref_bigram_count & can_bigram_count).values()) / reference_count\n    P = precision(reference, candidate)\n    R = recall(reference, candidate)\n    Frag = np.power(word_order_fragmentation(reference, candidate), beta)\n    return round(P * R / (alpha * R + (1 - alpha) * P) * (1 - gamma * Frag), 3)"}
{"task_id": 110, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference_words = reference.split()\n    candidate_words = candidate.split()\n    unigram_matches = sum((Counter(reference_words) & Counter(candidate_words)).values())\n    precision = unigram_matches / float(len(candidate_words))\n    recall = unigram_matches / float(len(reference_words))\n    F_mean = (1 - alpha) * (precision * recall / (beta * beta * precision + recall) if beta * beta * precision + recall != 0 else 0)\n    penalty = gamma * (abs(len(reference_words) - len(candidate_words)) / float(len(candidate_words))) if len(candidate_words) != 0 else 0\n    meteor_score = F_mean - penalty\n    return round(meteor_score, 3)"}
{"task_id": 110, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def _div_or_zero(numerator, denominator):\n        try:\n            return numerator / denominator\n        except ZeroDivisionError:\n            return 0\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    reference_counter = Counter(reference)\n    candidate_counter = Counter(candidate)\n    matching_ngrams = sum((reference_counter & candidate_counter).values())\n    precision = _div_or_zero(matching_ngrams, len(candidate))\n    recall = _div_or_zero(matching_ngrams, len(reference))\n    F_mean = _div_or_zero((1 + beta ** 2) * precision * recall, beta ** 2 * precision + recall)\n    penalty = gamma * sum((np.minimum(reference_counter[word], candidate_counter[word]) for word in candidate)) / len(candidate)\n    meteor_score = F_mean * (1 - penalty)\n    return round(meteor_score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 1, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    probability_x = total_counts_x / total_samples\n    probability_y = total_counts_y / total_samples\n    joint_probability = joint_counts / total_samples\n    expected_probability = probability_x * probability_y\n    pmi = np.log2(joint_probability / expected_probability)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 2, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    prob_x = total_counts_x / float(total_samples)\n    prob_y = total_counts_y / float(total_samples)\n    prob_xy = joint_counts / float(total_samples)\n    pmi = np.log2(prob_xy / float(prob_x * prob_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 3, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI)\n    \n    Args:\n    joint_counts (int): Joint occurrence count of two events\n    total_counts_x (int): Individual count of first event\n    total_counts_y (int): Individual count of second event\n    total_samples (int): Total number of samples\n    \n    Returns:\n    float: Pointwise Mutual Information value rounded to 3 decimal places\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 10\ntotal_counts_x = 50\ntotal_counts_y = 70\ntotal_samples = 100\npmi = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)"}
{"task_id": 111, "completion_id": 4, "solution": "import numpy as np\ndef compute_pmi(joint_counts: int, total_counts_x: int, total_counts_y: int, total_samples: int) -> float:\n    \"\"\"Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, \n       their individual counts, and the total number of samples.\n\n    Args:\n        joint_counts (int): Joint occurrence count of two events.\n        total_counts_x (int): Individual count of event X.\n        total_counts_y (int): Individual count of event Y.\n        total_samples (int): Total number of samples.\n\n    Returns:\n        float: Pointwise Mutual Information (PMI), rounded to 3 decimal places.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    pmi_rounded = round(pmi, 3)\n    return pmi_rounded"}
{"task_id": 111, "completion_id": 5, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    px = total_counts_x / total_samples\n    py = total_counts_y / total_samples\n    pxy = joint_counts / total_samples\n    pmi = np.log2(pxy / (px * py))\n    return round(pmi, 3)\njoint_counts = 10\ntotal_counts_x = 20\ntotal_counts_y = 30\ntotal_samples = 100\npmi = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)"}
{"task_id": 111, "completion_id": 6, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    p_x_y = p_x * p_y\n    pmi = np.log2(p_xy / p_x_y)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 7, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x_y = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_x_y / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 8, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 9, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = p_xy / (p_x * p_y)\n    pmi = np.log2(pmi)\n    pmi = round(pmi, 3)\n    return pmi"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    (min_x, max_x) = (min(x), max(x))\n    normalized_list = [round((i - min_x) / (max_x - min_x), 4) for i in x]\n    return normalized_list"}
{"task_id": 112, "completion_id": 1, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    return [round((i - min_x) / (max_x - min_x), 4) for i in x]"}
{"task_id": 112, "completion_id": 2, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    normalized = [round(i, 4) for i in normalized]\n    return normalized\nx = [15, 30, 45, 60, 75]"}
{"task_id": 112, "completion_id": 3, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    normalized_data = [round((val - min_val) / (max_val - min_val), 4) for val in x]\n    return normalized_data"}
{"task_id": 112, "completion_id": 4, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_value = min(x)\n    max_value = max(x)\n    normalized_values = [round((value - min_value) / (max_value - min_value), 4) for value in x]\n    return normalized_values\nvalues = [10, 20, 30, 40, 50]\nnormalized_values = min_max(values)"}
{"task_id": 112, "completion_id": 5, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_x = min(x)\n    max_x = max(x)\n    return [round((i - min_x) / (max_x - min_x), 4) for i in x]"}
{"task_id": 112, "completion_id": 6, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    return [round((i - min_val) / (max_val - min_val), 4) for i in x]"}
{"task_id": 112, "completion_id": 7, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    result = [round((i - min_x) / (max_x - min_x), 4) for i in x]\n    return result"}
{"task_id": 112, "completion_id": 8, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    normalized_x = [float('{:.4f}'.format((i - min_val) / (max_val - min_val))) for i in x]\n    return normalized_x"}
{"task_id": 112, "completion_id": 9, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    return [round((i - min_val) / (max_val - min_val), 4) for i in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x = x.reshape(-1)\n    w1 = np.atleast_2d(w1)\n    w2 = np.atleast_2d(w2)\n    x_l1 = np.matmul(w1, x)\n    x_l1[x_l1 < 0] = 0\n    x_l2 = np.matmul(w2, x_l1)\n    x_l2[x_l2 < 0] = 0\n    x_out = x_l2 + x\n    x_out[x_out < 0] = 0\n    return np.round(x_out, 4).tolist()\nx = np.array([1, 2, 3])\nw1 = np.array([[1, -1, 0], [0, 1, -1]])\nw2 = np.array([[1, -1], [1, 0], [0, 1]])"}
{"task_id": 113, "completion_id": 1, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"Implement a simple residual block using NumPy.\n\n    Args:\n        x: Input array.\n        w1: Weights for the first layer.\n        w2: Weights for the second layer.\n\n    Returns:\n        Result after processing x through the block.\n    \"\"\"\n    layer1_out = np.maximum(0, np.dot(w1, x))\n    layer2_out = np.dot(w2, layer1_out)\n    shortcut_out = x + layer2_out\n    block_out = np.maximum(0, shortcut_out)\n    result = block_out.round(4).tolist()\n    return result\nx = np.array([1, 2, 3])\nw1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nw2 = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])"}
{"task_id": 113, "completion_id": 2, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    output = np.matmul(x, w1)\n    output = np.maximum(output, 0)\n    output = np.matmul(output, w2)\n    output = np.maximum(output, 0)\n    output += x\n    output = np.maximum(output, 0)\n    return np.round(output, 4).tolist()"}
{"task_id": 113, "completion_id": 3, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    assert x.shape[0] == w1.shape[1], \"Dimensions of x and weights don't match\"\n    assert w1.shape[0] == w2.shape[1], \"Dimensions of weights don't match\"\n    x = np.maximum(0, np.dot(w1, x))\n    x = np.maximum(0, np.dot(w2, x))\n    x = np.maximum(0, x + x)\n    x = np.round(x, 4).tolist()\n    return x\nx = np.array([1, 2, 3, 4, 5])\nw1 = np.array([[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]])\nw2 = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5]])"}
{"task_id": 113, "completion_id": 4, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x = np.matmul(x, w1)\n    x = np.maximum(x, 0)\n    x = np.matmul(x, w2)\n    x = x + np.matmul(x, w1)\n    x = np.maximum(x, 0)\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 113, "completion_id": 5, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x = np.dot(x, w1)\n    x = np.maximum(x, 0)\n    x = np.dot(x, w2)\n    shortcut = np.dot(np.ones(x.shape[0]), w1[:, 0])\n    x += shortcut\n    x = np.maximum(x, 0)\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 113, "completion_id": 6, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Creates a simple residual block using NumPy.\n    Args:\n        x: 1D input array\n        w1: weight 1\n        w2: weight 2\n    Returns:\n        block output as a list\n    \"\"\"\n    x_weight1 = np.matmul(w1, x)\n    relu_weight1 = np.maximum(x_weight1, 0)\n    relu_weight2 = np.maximum(np.matmul(w2, relu_weight1), 0)\n    shortcut_connection = x + relu_weight2\n    block_output = np.maximum(shortcut_connection, 0)\n    return np.round(block_output, 4).tolist()\nx = np.array([1, 2, 3, 4, 5])\nw1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\nw2 = np.array([[16, 17, 18], [19, 20, 21], [22, 23, 24]])"}
{"task_id": 113, "completion_id": 7, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    z1 = np.maximum(0, np.dot(x, w1))\n    z2 = np.maximum(0, np.dot(z1, w2))\n    output = z2 + x\n    output = np.maximum(0, output)\n    return np.round(output, 4).tolist()\nx = np.array([1, 2, 3, 4, 5])\nw1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\nw2 = np.array([[16, 17, 18], [19, 20, 21], [22, 23, 24]])"}
{"task_id": 113, "completion_id": 8, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    output = np.dot(x, w1)\n    output = np.maximum(output, 0)\n    output = x + np.dot(output, w2)\n    output = np.maximum(output, 0)\n    output = np.around(output, decimals=4).tolist()\n    return output\nx = np.array([1, 2, 3])\nw1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nw2 = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])"}
{"task_id": 113, "completion_id": 9, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    y = np.dot(x, w1)\n    y = np.maximum(y, 0)\n    y = np.dot(y, w2)\n    y = np.maximum(y, 0)\n    y = y + x\n    y = np.maximum(y, 0)\n    return y.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 1, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Function to perform Global Average Pooling on a 3D NumPy array.\n\n    Args:\n    x: 3D NumPy array of shape (height, width, channels)\n\n    Returns:\n    1D array of shape (channels,) where each element is the average of all values in the corresponding feature map\n    \n    \"\"\"\n    return x.mean(axis=(0, 1))"}
{"task_id": 114, "completion_id": 2, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 3, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array representing\n    feature maps from a convolutional layer.\n\n    Args:\n    - x: A 3D NumPy array of shape (height, width, channels)\n\n    Returns:\n    - A 1D NumPy array of shape (channels,) where each element is the average of\n      all values in the corresponding feature map.\n    \"\"\"\n    assert len(x.shape) == 3, 'Input shape should be a 3D np array'\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 4, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 5, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 6, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 7, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Implementation of global average pooling.\n    \n    Args:\n    x: A 3D numpy array of shape (height, width, channels).\n    \n    Returns:\n    A 1D numpy array of shape (channels,), where each element is the average of all values \n    in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))\nx = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])"}
{"task_id": 114, "completion_id": 8, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))\nx = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]])"}
{"task_id": 114, "completion_id": 9, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    assert len(x.shape) == 3, 'Input should be a 3D numpy array'\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(4).tolist()\nX = np.random.randn(3, 2, 4, 4)\ngamma = np.ones(2)\nbeta = np.zeros(2)\nY = batch_normalization(X, gamma, beta)"}
{"task_id": 115, "completion_id": 1, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    num_channels = X.shape[1]\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    normalized = gamma * ((X - mean.reshape((1, num_channels, 1, 1))) / np.sqrt(var.reshape((1, num_channels, 1, 1)) + epsilon)) + beta\n    return np.round(normalized, 4).tolist()\nX = np.random.randn(3, 2, 4, 5)\ngamma = np.random.randn(2)\nbeta = np.random.randn(2)"}
{"task_id": 115, "completion_id": 2, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    batch_mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    batch_var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - batch_mean) / np.sqrt(batch_var + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(4).tolist()\nX = np.random.randn(3, 2, 2, 2)\ngamma = np.ones(2)\nbeta = np.zeros(2)"}
{"task_id": 115, "completion_id": 3, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(4).tolist()"}
{"task_id": 115, "completion_id": 4, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    batch_mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    batch_var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - batch_mean) / np.sqrt(batch_var + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(4).tolist()"}
{"task_id": 115, "completion_id": 5, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    output = gamma * X_hat + beta\n    return np.around(output, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 6, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = gamma * X_normalized + beta\n    output = output.round(4).tolist()\n    return output"}
{"task_id": 115, "completion_id": 7, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = gamma * normalized + beta\n    output = np.round(output.astype('float32'), 4)\n    output = output.tolist()\n    return output"}
{"task_id": 115, "completion_id": 8, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma * X_hat + beta\n    out = np.around(out, decimals=4)\n    return out.tolist()"}
{"task_id": 115, "completion_id": 9, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mu = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    sigma = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mu) / np.sqrt(sigma + epsilon)\n    Y = gamma * X_hat + beta\n    return Y.round(4).tolist()\nX = np.random.rand(2, 3, 4, 4)\ngamma = np.random.rand(3)\nbeta = np.random.rand(3)"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 1, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    return round(c * n * x ** (n - 1), 4)"}
{"task_id": 116, "completion_id": 2, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    result = c * n * x ** (n - 1)\n    return round(result, 4)"}
{"task_id": 116, "completion_id": 3, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative_value = float(c * n * x ** (n - 1))\n    return round(derivative_value, 4)\nc = 2\nx = 3\nn = 4"}
{"task_id": 116, "completion_id": 4, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = n * c * pow(x, n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 5, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    der = c * n * x ** (n - 1)\n    return round(der, 4)"}
{"task_id": 116, "completion_id": 6, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 7, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 8, "solution": "def poly_term_derivative(c: float, n: float, x: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 9, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    return round(n * c * x ** (n - 1), 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        v = np.array(v)\n        w = v - np.sum((np.dot(v, b) * b for b in basis))\n        if np.linalg.norm(w) > tol:\n            basis.append(w / np.linalg.norm(w))\n    return [b.round(4).tolist() for b in basis]"}
{"task_id": 117, "completion_id": 1, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"Compute orthonormal basis for 2D vectors\"\"\"\n    basis = []\n    for v in vectors:\n        w = np.array(v, dtype=float)\n        for vi in basis:\n            vi = np.array(vi)\n            w -= np.dot(vi, w) * vi\n        if np.sqrt(np.dot(w, w)) < tol:\n            continue\n        else:\n            w /= np.sqrt(np.dot(w, w))\n            basis.append(w.round(4).tolist())\n    return basis"}
{"task_id": 117, "completion_id": 2, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n\n    def normalize(v):\n        norm = np.linalg.norm(v)\n        if norm < tol:\n            raise ValueError('Vector norm is too small.')\n        return v / norm\n\n    def orthogonalize(v, basis):\n        for b in basis:\n            v -= np.dot(v, b) * b\n        return v\n    basis = []\n    for v in vectors:\n        v = np.array(v)\n        if basis:\n            v = orthogonalize(v, basis)\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            basis.append(normalize(v).round(4).tolist())\n    return basis\nvectors = [[1, 1], [1, -1]]\nbasis = orthonormal_basis(vectors)"}
{"task_id": 117, "completion_id": 3, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        w = np.array(v)\n        for b in basis:\n            w -= np.dot(w, b) * b\n        if np.linalg.norm(w) > tol:\n            basis.append(w / np.linalg.norm(w))\n    return [vec.round(4).tolist() for vec in basis]"}
{"task_id": 117, "completion_id": 4, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        basis_v = np.array(v, dtype=float)\n        for b in basis:\n            basis_v -= np.dot(b, v) * b\n        norm = np.linalg.norm(basis_v)\n        if norm > tol:\n            basis.append(list(np.round(basis_v / norm, 4).tolist()))\n    return basis\nvectors = [[3, 1], [2, 2]]\nbasis = orthonormal_basis(vectors)"}
{"task_id": 117, "completion_id": 5, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors)\n    basis = []\n    for v in vectors:\n        u = np.array(v, dtype=float)\n        for b in basis:\n            u -= np.dot(u, b) * b\n        if np.linalg.norm(u) > tol:\n            basis.append(u / np.linalg.norm(u))\n    basis = [np.round(b.tolist(), 4) for b in basis]\n    return basis\nvectors = [[1, 2], [3, 4], [5, 6]]\nbasis = orthonormal_basis(vectors)"}
{"task_id": 117, "completion_id": 6, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"Generates an orthonormal basis for the subspace spanned by the vectors using Gram-Schmidt process\"\"\"\n    basis = []\n    for v in vectors:\n        vector = np.array(v, dtype=float)\n        if basis == [] or np.linalg.norm(sum((np.dot(b, vector) for b in basis))) > tol:\n            w = vector\n            for b in basis:\n                w -= np.dot(b, vector) * b\n            w /= np.linalg.norm(w)\n            basis.append(w)\n    basis = [b.tolist() for b in basis]\n    basis = [[round(i, 4) for i in b] for b in basis]\n    return basis\nvectors = [[1, 2], [3, 4]]\ntol = 1e-10"}
{"task_id": 117, "completion_id": 7, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        w = np.array(v, dtype=float)\n        for b in basis:\n            proj_vec = np.dot(w, b) * b\n            w -= proj_vec\n        if np.linalg.norm(w) > tol:\n            basis.append(w / np.linalg.norm(w))\n    return [vec.round(4).tolist() for vec in basis]\nvectors = [[1, 1], [2, 2], [3, 3]]\ntol = 1e-10"}
{"task_id": 117, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List\ndef orthonormal_basis(vectors: List[List[float]], tol: float=1e-10) -> List[List[float]]:\n    basis = []\n    for v in vectors:\n        v = np.array(v)\n        for b in basis:\n            v = v - b.dot(v) * b\n        if np.linalg.norm(v) > tol:\n            basis.append(v / np.linalg.norm(v))\n    return [b.round(4).tolist() for b in basis]"}
{"task_id": 117, "completion_id": 9, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[np.ndarray]:\n    vectors = np.array(vectors, dtype=float)\n    n = vectors.shape[0]\n    orthonormal_vectors = []\n    for i in range(n):\n        v_i = vectors[i]\n        for j in range(i):\n            v_i = v_i - np.dot(v_i, orthonormal_vectors[j]) * orthonormal_vectors[j]\n        if np.linalg.norm(v_i) > tol:\n            orthonormal_vectors.append(v_i / np.linalg.norm(v_i))\n        else:\n            raise ValueError('Vectors are not linearly independent')\n    return [v.round(4).tolist() for v in orthonormal_vectors]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    if len(a) == 3 and len(b) == 3:\n        result = np.cross(a, b)\n        return result.round(decimals=4)\n    else:\n        return 'Both vectors must be 3-dimensional'\na = [1, 2, 3]\nb = [4, 5, 6]"}
{"task_id": 118, "completion_id": 1, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cross_product = np.cross(a, b)\n    cross_product = np.round(cross_product, 4)\n    cross_product = cross_product.tolist()\n    return cross_product"}
{"task_id": 118, "completion_id": 2, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cross_product_result = np.cross(a, b)\n    cross_product_result = cross_product_result.round(4).tolist()\n    return cross_product_result"}
{"task_id": 118, "completion_id": 3, "solution": "import numpy as np\ndef cross_product(a, b):\n    (a, b) = (np.array(a), np.array(b))\n    cross = np.cross(a, b)\n    cross = np.round(cross, 4).tolist()\n    return cross\na = [1, 2, 3]\nb = [4, 5, 6]"}
{"task_id": 118, "completion_id": 4, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    result = result.round(decimals=4)\n    return result.tolist()"}
{"task_id": 118, "completion_id": 5, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cross_product = np.cross(a, b)\n    cross_product = cross_product.round(4).tolist()\n    return cross_product\na = [1, 2, 3]\nb = [4, 5, 6]"}
{"task_id": 118, "completion_id": 6, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cross_prod = np.cross(a, b)\n    cross_prod = np.round(cross_prod, 4)\n    cross_prod = cross_prod.tolist()\n    return cross_prod"}
{"task_id": 118, "completion_id": 7, "solution": "import numpy as np\ndef cross_product(a, b):\n    if len(a) == 3 and len(b) == 3:\n        resultant_vector = np.cross(a, b)\n        return resultant_vector.round(decimals=4).tolist()\n    else:\n        print('Both vectors should be 3D.')\n        return None\na = [1, 2, 3]\nb = [4, 5, 6]"}
{"task_id": 118, "completion_id": 8, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    rounded_cross = [round(value, 4) for value in cross]\n    return rounded_cross"}
{"task_id": 118, "completion_id": 9, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return [round(elem, 4) for elem in result.tolist()]\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    n = len(b)\n    solutions = []\n    for i in range(n):\n        Ai = np.copy(A)\n        Ai[:, i] = b\n        detAi = np.linalg.det(Ai)\n        xi = detAi / detA\n        solutions.append(xi)\n    solutions = [round(solution, 4) for solution in solutions]\n    return solutions"}
{"task_id": 119, "completion_id": 1, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('A and b must have compatible dimensions')\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    x = []\n    for i in range(A.shape[1]):\n        Adet = A.copy()\n        Adet[:, i] = b\n        detAdet = np.linalg.det(Adet)\n        x_i = detAdet / detA\n        x.append(x_i)\n    return np.round(x, decimals=4).tolist()"}
{"task_id": 119, "completion_id": 2, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b.T\n        det_Ai = np.linalg.det(Ai)\n        xi = det_Ai / det_A\n        x.append(xi)\n    return np.round(x, 4).tolist()"}
{"task_id": 119, "completion_id": 3, "solution": "import numpy as np\nfrom math import sqrt\nimport copy\ndef cramers_rule(A, b):\n    n = len(A)\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-15:\n        return -1\n    res = []\n    for i in range(n):\n        B = copy.deepcopy(A)\n        for k in range(n):\n            B[k][i] = b[k][0]\n        det_B = np.linalg.det(B)\n        x_i = det_B / det_A\n        res.append(round(x_i, 4))\n    return res\nA = np.array([[2.0, 1.0, -1.0], [-3.0, -1.0, 2.0], [-2.0, 1.0, 2.0]])\nb = np.array([[8.0], [-11.0], [-3.0]])"}
{"task_id": 119, "completion_id": 4, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    new_matrices = [np.hstack((A[:, :j], np.reshape(b, (-1, 1)), A[:, j + 1:])) for j in range(A.shape[1])]\n    det_xi = np.array([np.linalg.det(new_mat) for new_mat in new_matrices])\n    x = det_xi / det_A\n    x = x.round(4).tolist()\n    return x"}
{"task_id": 119, "completion_id": 5, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    det = np.linalg.det(A)\n    if det == 0:\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        di = np.linalg.det(Ai)\n        x[i] = di / det\n    return np.round(x.tolist(), 4)"}
{"task_id": 119, "completion_id": 6, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    if np.linalg.det(A) == 0:\n        return -1\n    solution = []\n    A = np.array(A)\n    b = np.array(b)\n    n = len(b)\n    D = np.linalg.det(A)\n    for i in range(n):\n        new_A = np.array(A)\n        new_A[:, i] = b\n        new_D = np.linalg.det(new_A)\n        x_i = new_D / D\n        solution.append(x_i)\n    return np.round(solution, 4).tolist()"}
{"task_id": 119, "completion_id": 7, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    if A.shape[0] != A.shape[1]:\n        return -1\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        detAi = np.linalg.det(Ai)\n        x[i] = detAi / detA\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]])\nb = np.array([8, -11, -3])\nx = cramers_rule(A, b)"}
{"task_id": 119, "completion_id": 8, "solution": "import numpy as np\ndef determinant(matrix):\n    return np.linalg.det(matrix)\ndef cramers_rule(A, b):\n    detA = determinant(A)\n    if abs(detA) < 1e-10:\n        return -1\n    n = len(b)\n    x = []\n    for j in range(n):\n        A_j = np.array(A)\n        A_j[:, j] = b\n        detA_j = determinant(A_j)\n        x.append(round(detA_j / detA, 4))\n    return x"}
{"task_id": 119, "completion_id": 9, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    if np.linalg.det(A) == 0:\n        return -1\n    else:\n        n = A.shape[0]\n        x = np.zeros(n)\n        for i in range(n):\n            temp_A = np.copy(A)\n            temp_A[:, i] = b.transpose()\n            x[i] = round(np.linalg.det(temp_A) / np.linalg.det(A), 4)\n        return x.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or (len(p) == 0 and len(q) == 0):\n        return 0.0\n    else:\n        bc_distance = np.sum(np.sqrt(p * q))\n        dist = -np.log(bc_distance)\n        return round(dist, 4)"}
{"task_id": 120, "completion_id": 1, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    else:\n        bc_distance = np.sum([np.sqrt(px * qx) for (px, qx) in zip(p, q)])\n        return round(np.log(bc_distance), 4) if bc_distance > 0 else 0.0"}
{"task_id": 120, "completion_id": 2, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc_distance = -np.log(np.sum([np.sqrt(i * j) for (i, j) in zip(p, q)]))\n    return round(bc_distance, 4)"}
{"task_id": 120, "completion_id": 3, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    (p_array, q_array) = (np.array(p), np.array(q))\n    distance = -np.log(np.sum(np.sqrt(p_array * q_array)))\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 4, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0 or len(q) == 0:\n        return 0.0\n    else:\n        bc_distance = sum([np.sqrt(p_elem * q_elem) for (p_elem, q_elem) in zip(p, q)])\n        return round(np.log(bc_distance), 4) if bc_distance > 0 else 0.0"}
{"task_id": 120, "completion_id": 5, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) == len(q) != 0:\n        if abs(sum(p) - 1.0) < 1e-09 and abs(sum(q) - 1.0) < 1e-09:\n            return round(np.sum(np.sqrt(p * q)), 4)\n    return 0.0"}
{"task_id": 120, "completion_id": 6, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    This function computes the Bhattacharyya distance between two probability distributions.\n    \"\"\"\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    bc_num = np.sum([np.sqrt(p_i * q_i) for (p_i, q_i) in zip(p, q)])\n    bd = -np.log(bc_num)\n    bd = round(bd, 4)\n    return bd"}
{"task_id": 120, "completion_id": 7, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q):\n        return 0.0\n    if len(p) == 0 or len(q) == 0:\n        return 0.0\n    p_q = [np.sqrt(x * y) for (x, y) in zip(p, q)]\n    bc_distance = -np.log(np.sum(p_q))\n    return round(bc_distance, 4)"}
{"task_id": 120, "completion_id": 8, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Compute the Bhattacharyya distance between two discrete probability distributions\n    \"\"\"\n    (p, q) = (np.array(p), np.array(q))\n    if len(p) != len(q):\n        return 0.0\n    if len(p) == 0 or len(q) == 0:\n        return 0.0\n    distance = np.sum(np.sqrt(p * q))\n    return round(distance, 4)\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.15, 0.25, 0.35, 0.15]"}
{"task_id": 120, "completion_id": 9, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc_distance = sum([np.sqrt(p_i * q_i) for (p_i, q_i) in zip(p, q)])\n    bc_distance = -np.log(bc_distance)\n    return round(bc_distance, 4)\np = [0.2, 0.5, 0.3]\nq = [0.3, 0.4, 0.3]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    else:\n        return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 1, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 2, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 3, "solution": "from typing import List, Union\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    if len(a) != len(b):\n        return -1\n    else:\n        return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 4, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 5, "solution": "from typing import List, Union\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    if len(a) != len(b):\n        return -1\n    else:\n        return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 6, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 7, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 8, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [i + j for (i, j) in zip(a, b)]\na = [1, 2, 3]\nb = [4, 5]"}
{"task_id": 121, "completion_id": 9, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) == len(b):\n        return [x + y for (x, y) in zip(a, b)]\n    else:\n        return -1"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    result = []\n\n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    def policy_gradient(theta, state, action):\n        policy = softmax(theta[state, :])\n        one_hot = np.zeros(len(policy))\n        one_hot[action] = 1\n        return (one_hot - policy).reshape(-1, 1) * state\n    for episode in episodes:\n        episode_result = []\n        total_reward = sum((x[2] for x in episode))\n        for (state, action, _) in episode:\n            episode_result.append(policy_gradient(theta, state, action) * total_reward)\n        result.append(np.sum(episode_result, axis=0).tolist())\n    return np.array(result).mean(axis=0).round(4).tolist()"}
{"task_id": 122, "completion_id": 1, "solution": "import numpy as np\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    gradients = []\n    for episode in episodes:\n        state_actions = np.array([[s, a, r] for (s, a, r) in episode])\n        states = state_actions[:, 0]\n        actions = state_actions[:, 1]\n        rewards = state_actions[:, 2]\n        returns = np.array([np.sum(rewards[i:]) for i in range(len(rewards))])\n        returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-06)\n        for ((state, action, reward), ret) in zip(episode, returns):\n            policy = softmax(theta[state, :])\n            one_hot_action = np.zeros(len(policy))\n            one_hot_action[action] = 1\n            grad = (one_hot_action - policy).reshape(-1, 1)\n            gradients.append(grad * ret)\n    avg_grad = np.mean(gradients, axis=0)\n    avg_grad = np.round(avg_grad, 4)\n    return avg_grad.tolist()"}
{"task_id": 122, "completion_id": 2, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros((num_states, num_actions))\n    for episode in episodes:\n        T = len(episode)\n        states = np.array([state for (state, _, _) in episode])\n        actions = np.array([action for (_, action, _) in episode])\n        rewards = np.array([reward for (_, _, reward) in episode])\n        returns = np.empty(T)\n        running_return = 0\n        for t in reversed(range(T)):\n            running_return = rewards[t] + returns[t] if t + 1 < T else rewards[t]\n            returns[t] = running_return\n        for (state, action, _) in episode:\n            probs = softmax(theta[state, :])\n            gradient[state, :] -= probs.reshape(-1, 1) * np.ones((1, num_actions))\n            gradient[state, action] += probs[action]\n    gradient *= returns.reshape(-1, 1)\n    gradient /= len(episodes)\n    return gradient.tolist()"}
{"task_id": 122, "completion_id": 3, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    gradients = []\n    for episode in episodes:\n        for (t, (s, a, r)) in enumerate(episode):\n            policy = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            log_policy = np.log(policy)\n            gradient = log_policy[:, None] - policy[:, None] * log_policy[None, :]\n            gradients.append(gradient)\n    mean_gradient = np.mean(gradients, axis=0)\n    mean_gradient = mean_gradient.round(4)\n    return mean_gradient.tolist()"}
{"task_id": 122, "completion_id": 4, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n\n    def softmax(x):\n        num = np.exp(x)\n        den = np.sum(num, axis=1).reshape(-1, 1)\n        return num / den\n\n    def compute_gradient(s, a, theta):\n        probs = softmax(theta[s, :])\n        s = probs.reshape(-1, 1)\n        return np.diagflat(s) - np.dot(s, s.T)\n    gradients = []\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        returns = np.cumsum(rewards[::-1])[::-1]\n        for (s, a, G) in zip(states, actions, returns):\n            grad = compute_gradient(s, a, theta)\n            gradients.append(G * np.log(grad[a]))\n    avg_gradient = np.mean(gradients, axis=0)\n    return avg_gradient.tolist()"}
{"task_id": 122, "completion_id": 5, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    policy_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        for t in reversed(range(len(episode))):\n            (s, a, r) = episode[t]\n            policy_vec = np.exp(theta[s, :] - np.max(theta[s, :]))\n            sum_exp_theta = np.sum(policy_vec)\n            G = G + r\n            for a_ in range(num_actions):\n                prob = policy_vec[a_] / sum_exp_theta\n                if a_ == a:\n                    grad_func = (1 - prob) * policy_vec[a_]\n                else:\n                    grad_func = -policy_vec[a_] * prob\n                policy_gradient[s, a_] += grad_func * G\n    policy_gradient /= len(episodes)\n    policy_gradient_list = policy_gradient.tolist()\n    rounded_grad = [[round(value, 4) for value in grad] for grad in policy_gradient_list]\n    return rounded_grad"}
{"task_id": 122, "completion_id": 6, "solution": "import numpy as np"}
{"task_id": 122, "completion_id": 7, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n\n    def softmax(x):\n        num = np.exp(x)\n        den = np.sum(np.exp(x), axis=1)\n        return num / den[:, None]\n    total_gradient = 0\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        returns = np.cumsum(rewards[::-1])[::-1]\n        state_vectors = np.eye(len(theta))[states]\n        policy_grad = softmax(theta[states, :])[range(len(actions)), actions]\n        total_gradient += np.sum(returns * policy_grad)\n    avg_gradient = total_gradient / len(episodes)\n    avg_gradient_rounded = np.around(avg_gradient, decimals=4).tolist()\n    return avg_gradient_rounded"}
{"task_id": 122, "completion_id": 8, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n\n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n\n    def compute_gradient(state, action, theta):\n        policy = softmax(theta[state, :])\n        one_hot_action = np.zeros(num_actions)\n        one_hot_action[action] = 1\n        return (one_hot_action - policy).reshape(-1, 1) * theta[state, :].reshape(-1, 1)\n    episode_gradients = []\n    for episode in episodes:\n        episode_gradient = 0\n        G = 0\n        for (state, action, reward) in reversed(episode):\n            G = reward + G\n            episode_gradient += G * compute_gradient(state, action, theta)\n        episode_gradients.append(episode_gradient.tolist())\n    average_gradient = sum(episode_gradients) / len(episode_gradients)\n    return np.round(average_gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 9, "solution": "import numpy as np\ndef policy_fn(theta, state):\n    exp = np.exp(theta[state])\n    return exp / exp.sum()\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    grads = []\n    for episode in episodes:\n        grad_log_pi = []\n        (states, actions, rewards) = zip(*episode)\n        for (t, state) in enumerate(states):\n            policy = policy_fn(theta, state)\n            grad_log_pi_a = -policy\n            grad_log_pi_a[actions[t]] += 1\n            G = sum(rewards[t:])\n            weighted_grad_log_pi_a = G * grad_log_pi_a\n            grad_log_pi.append(weighted_grad_log_pi_a)\n        grads.append(np.array(grad_log_pi).sum(axis=0))\n    avg_grad_log_pi = np.array(grads).sum(axis=0)\n    avg_grad_log_pi /= len(episodes)\n    return avg_grad_log_pi.round(4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": ""}
{"task_id": 123, "completion_id": 1, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = n_experts * d_in * d_out\n    moe_flops = k_active * d_in * d_out\n    efficiency = (dense_flops - moe_flops) / dense_flops * 100\n    efficiency = round(efficiency, 1)\n    return efficiency"}
{"task_id": 123, "completion_id": 2, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out * k_active\n    moe_flops = 2 * n_experts * k_active + n_experts * d_in * d_out + (n_experts * d_in + 2 * n_experts * k_active)\n    efficiency = round(100 * (dense_flops / moe_flops), 1)\n    return efficiency\nn_experts = 1000\nk_active = 20\nd_in = 100\nd_out = 50\nefficiency = compute_efficiency(n_experts, k_active, d_in, d_out)"}
{"task_id": 123, "completion_id": 3, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = (2 * d_in - 1) * (d_out + d_in * (d_in - 1) / 2)\n    moe_flops = (2 * d_in - 1) * (d_out + d_in * (d_in - 1) / 2) + n_experts * d_in + k_active * n_experts * d_in * d_out\n    savings = 100 - moe_flops / dense_flops * 100\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings = round(savings, 1)\n    return (dense_flops, moe_flops, savings)\nn_experts = 10\nk_active = 5\nd_in = 2048\nd_out = 2048"}
{"task_id": 123, "completion_id": 4, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    if n_experts <= 0 or k_active <= 0 or d_in <= 0 or (d_out <= 0):\n        raise ValueError('All parameters must be positive integers.')\n    if n_experts < k_active:\n        raise ValueError('Number of experts must be greater than number of active experts.')\n    dense_FLOPs = 2 * d_in * k_active * d_out\n    gates_FLOPs = (d_in + d_in * k_active) * n_experts\n    routing_FLOPs = d_in * d_in\n    experts_FLOPs = n_experts * (d_in * d_out + d_out)\n    moe_FLOPs = gates_FLOPs + routing_FLOPs + experts_FLOPs\n    moe_params = n_experts * (d_in * d_out + d_out) + 2 * d_in * k_active + d_in * n_experts\n    efficiency = dense_FLOPs / moe_FLOPs * 100\n    efficiency = round(efficiency, 1)\n    (moe_FLOPs, dense_FLOPs, moe_params) = (round(moe_FLOPs, 1), round(dense_FLOPs, 1), round(moe_params, 1))\n    return (efficiency, moe_FLOPs, dense_FLOPs, moe_params)\nn_experts = 512\nk_active = 32\nd_in = 2048\nd_out = 4096"}
{"task_id": 123, "completion_id": 5, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    moe_flops = n_experts * d_in * d_out * (k_active / n_experts) + n_experts * d_in * d_out * (1 - k_active / n_experts)\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings = round(savings, 1)\n    return (dense_flops, moe_flops, savings)\nn_experts = 10\nk_active = 5\nd_in = 100\nd_out = 100"}
{"task_id": 123, "completion_id": 6, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    moe_flops = n_experts * (1 * d_in + 2 * d_in * d_out) + k_active * (1 * d_in + 2 * d_in * d_out)\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    return (round(dense_flops, 1), round(moe_flops, 1), round(savings, 1))\nn_experts = 32\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 123, "completion_id": 7, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    FLOPs_MoE = 4 * n_experts * d_in * d_out + n_experts * d_out + 4 * k_active * d_in * d_out\n    FLOPs_Dense = 2 * d_in * d_out + d_out\n    savings = (FLOPs_Dense - FLOPs_MoE) / FLOPs_Dense * 100\n    return (round(FLOPs_MoE, 1), round(FLOPs_Dense, 1), round(savings, 1))\nn_experts = 512\nk_active = 128\nd_in = 2048\nd_out = 1024"}
{"task_id": 123, "completion_id": 8, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out * n_experts\n    moe_flops = n_experts * d_out * (d_in + 2 * d_out)\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    (dense_flops, moe_flops, savings) = (round(dense_flops, 1), round(moe_flops, 1), round(savings, 1))\n    return (dense_flops, moe_flops, savings)\nn_experts = 8\nd_in = 512\nd_out = 512"}
{"task_id": 123, "completion_id": 9, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    g1_flops = 2 * d_in * n_experts\n    g2_flops = 2 * k_active * d_out\n    expert_flops = k_active * (d_in + 1) * d_out\n    moe_flops = g1_flops + g2_flops + expert_flops\n    dense_flops = 2 * d_in * d_out\n    savings = round(100 - moe_flops / dense_flops * 100, 1)\n    return (moe_flops, dense_flops, savings)\nn_experts = 1024\nk_active = 32\nd_in = 2048\nd_out = 4096"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X.dot(W_g) + W_noise + N\n    topk_indices = np.argpartition(logits, -k)[-k:]\n    gating_probs = np.zeros_like(logits)\n    gating_probs[topk_indices] = 1 / k\n    gating_probs = np.round(gating_probs, 4)\n    return gating_probs.tolist()"}
{"task_id": 124, "completion_id": 1, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    gate_values = np.dot(X, W_g)\n    noisy_gates = gate_values + np.dot(N, W_noise)\n    noisy_gates = np.squeeze(noisy_gates)\n    probabilities = np.exp(noisy_gates) / np.sum(np.exp(noisy_gates))\n    sorted_indices = np.argsort(-probabilities)\n    probabilities[sorted_indices[k:]] = 0.0\n    probabilities /= np.sum(probabilities)\n    probabilities = np.round(probabilities, 4)\n    return probabilities.tolist()"}
{"task_id": 124, "completion_id": 2, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    G = np.dot(X, W_g.T) + np.dot(N, W_noise.T)\n    G = np.clip(G, -1000000000000.0, 1000000000000.0)\n    G = 1.0 / (1.0 + np.exp(-G))\n    topk_indices = np.argpartition(G, -k, axis=1)[:, -k:]\n    topk_values = np.take_along_axis(G, topk_indices, axis=1)\n    topk_sum = topk_values.sum(axis=1, keepdims=True)\n    G_hat = topk_values / topk_sum\n    indices = np.expand_dims(np.arange(G.shape[0]), axis=-1)\n    G[indices, topk_indices] = G_hat\n    G[G != G_hat[indices, topk_indices]] = 0\n    G = np.round(G, 4).tolist()\n    return G"}
{"task_id": 124, "completion_id": 3, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    gate_score = X @ W_g.T + W_noise\n    gate_score = np.clip(gate_score, a_min=None, a_max=3.0)\n    top_k_indices = np.argpartition(gate_score, -k)[:, -k:]\n    gating_prob = np.zeros_like(gate_score)\n    for i in range(gate_score.shape[0]):\n        gating_prob[i, top_k_indices[i]] = 1\n    sum_prob = np.sum(gating_prob, axis=1, keepdims=True)\n    gating_prob /= sum_prob\n    return gating_prob.round(4).tolist()"}
{"task_id": 124, "completion_id": 4, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    G = X.dot(W_g)\n    G_noisy = G + W_noise\n    P = np.exp(G_noisy) / np.sum(np.exp(G_noisy))\n    (P_k, P_k_indices) = top_k(P, k)\n    P_remaining = (1 - sum(P_k)) / sum(1 - P_k[P_k_indices])\n    P_new = P\n    P_new[P_k_indices] = P_remaining\n    P_new = np.reshape(P_new, (1, -1))\n    return P_new.tolist()\ndef top_k(arr, k):\n    indices_part = np.argpartition(arr, -k)[-k:]\n    values_part = arr[indices_part]\n    return (values_part, indices_part)\nW_g = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_noise = np.array([[0.01, 0.02, 0.03], [0.04, 0.05, 0.06], [0.07, 0.08, 0.09]])\nk = 2"}
{"task_id": 124, "completion_id": 5, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    logits = np.dot(X, W_g.T) + np.dot(N, W_noise.T)\n    max_logit = np.max(logits, axis=1, keepdims=True)\n    logits -= max_logit\n    exp_logits = np.exp(logits)\n    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    topk_indices = np.argpartition(probs, -k, axis=-1)[:, -k:]\n    topk_values = np.take_along_axis(probs, topk_indices, axis=-1)\n    sparsified_values = topk_values / np.sum(topk_values, axis=-1, keepdims=True)\n    preserve_shape = np.zeros_like(probs)\n    np.put_along_axis(preserve_shape, topk_indices, sparsified_values, axis=-1)\n    gating_prob = preserve_shape.round(4).tolist()\n    return gating_prob"}
{"task_id": 124, "completion_id": 6, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    gating_scores = np.dot(X, W_g)\n    gating_scores += np.dot(N, W_noise)\n    top_k_indices = top_k(gating_scores, k)\n    gating_probabilities = np.zeros_like(gating_scores)\n    gating_probabilities[top_k_indices] = 1.0\n    gating_probabilities = softmax(gating_probabilities)\n    gating_probabilities = gating_probabilities.flatten().tolist()\n    gating_probabilities = [round(x, 4) for x in gating_probabilities]\n    return gating_probabilities\ndef top_k(scores, k):\n    indices = np.argpartition(scores, -k)[-k:]\n    return indices\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)"}
{"task_id": 124, "completion_id": 7, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    unnoisy_scores = np.dot(X, W_g)\n    noisy_scores = unnoisy_scores + np.dot(N, W_noise)\n    gating_probabilities = np.exp(noisy_scores) / np.sum(np.exp(noisy_scores), axis=1, keepdims=True)\n    num_experts = gating_probabilities.shape[1]\n    for i in range(len(gating_probabilities)):\n        top_k_indices = np.argpartition(gating_probabilities[i], -k)[-k:]\n        gating_probabilities[i][top_k_indices] = 0\n        sum_top_k = np.sum(gating_probabilities[i])\n        gating_probabilities[i] = [x / sum_top_k if x > 0 else 0 for x in gating_probabilities[i]]\n    return np.around(gating_probabilities, decimals=4).tolist()"}
{"task_id": 124, "completion_id": 8, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    scores = np.dot(X, W_g)\n    noisy_scores = scores + W_noise\n    sorted_indices = np.argsort(noisy_scores, axis=1)\n    sorted_indices = sorted_indices[:, ::-1]\n    mask = sorted_indices < k\n    topk_mask = np.zeros_like(mask)\n    for i in range(len(topk_mask)):\n        topk_mask[i, sorted_indices[i, :k]] = True\n    probs = topk_mask * np.exp(noisy_scores) / np.sum(np.exp(noisy_scores), axis=1, keepdims=True)\n    unif_noise = np.random.uniform(0, N, size=probs.shape)\n    probs = (1.0 - N) * probs + N * unif_noise\n    probs_list = np.round(probs.tolist(), 4)\n    return probs_list"}
{"task_id": 124, "completion_id": 9, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    logits = X.dot(W_g.T)\n    logits += W_noise\n    logits -= np.max(logits, axis=1, keepdims=True)\n    exponentials = np.exp(logits)\n    topk_probs = np.sort(exponentials, axis=1)[:, -k:]\n    sum_topk_probs = np.sum(topk_probs, axis=1)\n    normalized_probs = topk_probs / np.reshape(sum_topk_probs, (-1, 1))\n    return normalized_probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    E = [np.dot(x, We[i]) for i in range(n_experts)]\n    E = np.array(E).T\n    G = np.dot(x, Wg)\n    G = np.exp(G)\n    G = G / np.sum(G, axis=1)[:, None]\n    top_k_experts = np.argpartition(G, -top_k, axis=1)[:, -top_k:]\n    result = np.zeros_like(E)\n    for i in range(E.shape[0]):\n        for j in range(top_k):\n            result[i] += E[i, top_k_experts[i, j]] * G[i, top_k_experts[i, j]]\n    result = np.round(result, 4)\n    return result.tolist()"}
{"task_id": 125, "completion_id": 1, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    g = np.matmul(x, Wg.transpose())\n    g = np.exp(g) / np.sum(np.exp(g), axis=1, keepdims=True)\n    top_values = np.zeros_like(g)\n    top_indices = np.zeros_like(g, dtype=np.int32)\n    for i in range(g.shape[0]):\n        top_values[i, :] = (-g[i, :]).argsort()[:top_k]\n        top_values[i, :] = g[i, top_values[i, :].astype(np.int32)]\n        g[i, :] = 0\n        g[i, top_indices[i, :]] = top_values[i, :]\n    m = np.zeros_like(x)\n    for i in range(n_experts):\n        m += np.matmul(x, We[i]) * g[:, [i]]\n    return np.round(m, 4).tolist()"}
{"task_id": 125, "completion_id": 2, "solution": "import numpy as np\nfrom scipy.special import softmax\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"Implements a Sparse Mixture of Experts Layer\n\n    Args:\n        x (np.ndarray): Input tensor of shape [batch_size, input_size]\n        We (np.ndarray): Expert weight matrices of shape [input_size, n_experts, output_size]\n        Wg (np.ndarray): Gating weight matrix of shape [input_size, n_experts]\n        n_experts (int): Number of experts\n        top_k (int): Value of k\n\n    Returns:\n        np.ndarray: Aggregated results weighted by the normalized gating probabilities\n    \n    \"\"\"\n    activation = np.einsum('bi,ijk->bjk', x, We)\n    gating_prob = softmax(np.matmul(x, Wg), axis=1)\n    top_k_expert_indices = np.argpartition(gating_prob, -top_k)[:, -top_k:]\n    experts_output = np.zeros_like(gating_prob)\n    for idx in range(x.shape[0]):\n        expert_indices = top_k_expert_indices[idx]\n        experts_output[idx, expert_indices] = activation[idx, expert_indices]\n    aggregated_output = np.sum(gating_prob[:, :, np.newaxis] * experts_output[:, np.newaxis, :], axis=1)\n    aggregated_output = aggregated_output / top_k\n    return aggregated_output.round(4).tolist()"}
{"task_id": 125, "completion_id": 3, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each scores in x.\"\"\"\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    x: input tensor, shape: (num_features, num_tokens)\n    We: expert weight matrices, shape: (num_features, num_experts * num_features)\n    Wg: gating weight matrix, shape: (num_experts, num_features)\n    n_experts: number of experts\n    top_k: value of k for top-k routing\n    \"\"\"\n    (num_features, num_tokens) = x.shape\n    num_layers = We.shape[0] // num_features\n    gate_out = np.dot(Wg, x.T)\n    gate_out_norm = softmax(gate_out)\n    top_experts = np.argpartition(gate_out, -top_k, axis=0)[::-top_k]\n    expert_out = np.zeros((num_layers, num_tokens))\n    for l in range(num_layers):\n        for i in range(num_tokens):\n            expert_weights = np.zeros((n_experts, num_features))\n            for j in range(top_k):\n                expert_weights[top_experts[j], :] = We[num_features * l:num_features * (l + 1), top_experts[j] * num_features:(top_experts[j] + 1) * num_features]\n            expert_out[l, i] = np.sum(expert_weights * gate_out_norm[:, i])\n    return expert_out.round(4).tolist()"}
{"task_id": 125, "completion_id": 4, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    expert_outputs = []\n    for expert_id in range(n_experts):\n        expert_weight = np.reshape(We[expert_id], newshape=(We[expert_id].shape[0], 1))\n        output = np.dot(x, expert_weight)\n        expert_outputs.append(output)\n    expert_outputs = np.array(expert_outputs)\n    gating_scores = np.dot(x, Wg)\n    norm_gating_scores = gating_scores / gating_scores.sum()\n    gating_probabilities = np.exp(norm_gating_scores) / np.exp(norm_gating_scores).sum()\n    top_k_expert_ids = np.argsort(-gating_probabilities)[:top_k]\n    moe_output = np.zeros_like(expert_outputs[0])\n    for expert_id in top_k_expert_ids:\n        moe_output += expert_outputs[expert_id] * gating_probabilities[expert_id]\n    return np.round(moe_output.flatten(), 4).tolist()"}
{"task_id": 125, "completion_id": 5, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    gating_logits = np.matmul(x, np.transpose(Wg))\n    max_gating_logits = np.max(gating_logits, axis=1, keepdims=True)\n    exp_gating_logits = np.exp(gating_logits - max_gating_logits)\n    gating_probabilities = exp_gating_logits / np.sum(exp_gating_logits, axis=1, keepdims=True)\n    token_outputs = []\n    for (token, gating_prob) in zip(x, gating_logits):\n        top_k_experts = np.argsort(gating_prob)[-top_k:]\n        expert_outputs = np.dot(We[top_k_experts], token)\n        weighted_output = np.sum(expert_outputs * gating_prob[top_k_experts])\n        token_outputs.append(weighted_output)\n    return np.round(token_outputs, 4).tolist()"}
{"task_id": 125, "completion_id": 6, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    x_size = x.shape[0]\n    expert_size = x.shape[1]\n    g = np.dot(x, Wg)\n    g = np.exp(g) / np.exp(g).sum(axis=-1, keepdims=True)\n    top_experts = g.argsort()[:, -top_k:]\n    output = np.zeros((x_size, expert_size))\n    for i in range(x_size):\n        ei = []\n        for expert in range(n_experts):\n            if expert in top_experts[i]:\n                ei.append(np.dot(x[i, :], We[expert]))\n            else:\n                ei.append(np.zeros((1, expert_size)))\n        ei = np.vstack(ei)\n        oi = np.dot(g[i, top_experts[i]], ei)\n        output[i, :] = oi\n    return np.around(output, 4).tolist()"}
{"task_id": 125, "completion_id": 7, "solution": "import numpy as np\nimport numpy as np\nfrom moe import moe\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    g = np.dot(x, Wg.T)\n    p = np.exp(g) / np.sum(np.exp(g), axis=1, keepdims=True)\n    top_expert_indices = np.argpartition(p, -top_k, axis=1)[:, -top_k:]\n    output = np.zeros(x.shape)\n    for i in range(n_experts):\n        We_i = We[i]\n        expert_output = np.dot(x, We_i.T)\n        for j in range(top_k):\n            index_j = top_expert_indices[:, j]\n            output[index_j] += p[index_j, i] * expert_output[index_j]\n    output = np.round(output, 4).tolist()\n    return output\nx = np.random.randn(10, 20)\nWe = np.random.randn(10, 20, 30)\nWg = np.random.randn(30, 20)\noutput = moe(x, We, Wg, 10, 2)"}
{"task_id": 125, "completion_id": 8, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    (batch_size, in_features) = x.shape\n    (out_features, _) = We.shape\n    expert_inputs = np.dot(x, We.T)\n    expert_outputs = np.reshape(np.exp(expert_inputs), (batch_size, n_experts, out_features))\n    gating_inputs = np.dot(x, Wg.T)\n    gating_outputs = np.exp(gating_inputs) / np.sum(np.exp(gating_inputs), axis=1, keepdims=True)\n    topk_experts = np.argpartition(gating_outputs, -top_k)[:, -top_k:]\n    final_output = 0.0\n    for batch in range(batch_size):\n        for i in range(top_k):\n            expert = topk_experts[batch, i]\n            weight = gating_outputs[batch, expert]\n            final_output += weight * expert_outputs[batch, expert]\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 125, "completion_id": 9, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    x = x if len(x.shape) == 2 else x[:, np.newaxis]\n    We = We if len(We.shape) == 3 else We[:, :, np.newaxis]\n    Wg = Wg if len(Wg.shape) == 2 else Wg[:, np.newaxis]\n    g_scores = np.matmul(x, np.transpose(Wg, (1, 0)))\n    g_probs = np.exp(g_scores) / np.sum(np.exp(g_scores), axis=1)[:, np.newaxis]\n    sorted_gating_probs_indices = np.argsort(g_probs, axis=1)\n    top_k_expert_indices = sorted_gating_probs_indices[:, -top_k:]\n    output = np.zeros_like(x)\n    for token_idx in range(x.shape[0]):\n        expert_weights = We[top_k_expert_indices[token_idx]]\n        expert_outputs = np.matmul(x[[token_idx]], np.transpose(expert_weights, (0, 2, 1)))\n        gating_weights = g_probs[token_idx, top_k_expert_indices[token_idx]]\n        output[token_idx] = np.sum(expert_outputs * gating_weights[:, np.newaxis, np.newaxis], axis=0)\n    return np.round(output, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    assert X.ndim == 4, 'Input X must be a 4D tensor'\n    assert gamma.ndim == 1 and beta.ndim == 1, 'Gamma and beta must be 1D tensors'\n    assert X.shape[1] == gamma.shape[0] == beta.shape[0], 'Gamma and beta must have the same length as the number of channels in X'\n    assert X.shape[1] % num_groups == 0, 'Number of channels in X must be divisible by num_groups'\n    (B, C, H, W) = X.shape\n    group_channels = C // num_groups\n    X = X.reshape(B, num_groups, group_channels * H * W)\n    mean = X.mean(axis=-1, keepdims=True)\n    std = X.std(axis=-1, keepdims=True)\n    X_hat = (X - mean) / (std + epsilon)\n    X_hat = X_hat.reshape(B, C, H, W)\n    out = gamma * X_hat + beta\n    return out.round(4).tolist()\nX = np.random.randn(B, C, H, W)\ngamma = np.random.randn(C)\nbeta = np.random.randn(C)\nout = group_normalization(X, gamma, beta, num_groups, epsilon)"}
{"task_id": 126, "completion_id": 1, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> np.ndarray:\n    (B, C, H, W) = X.shape\n    groups = []\n    for i in range(0, C, num_groups):\n        group = X[:, i:i + num_groups, :, :]\n        groups.append(group)\n    norm_groups = []\n    for group in groups:\n        mean = np.mean(group, axis=(2, 3), keepdims=True)\n        var = np.var(group, axis=(2, 3), keepdims=True)\n        normalized = (group - mean) / np.sqrt(var + epsilon)\n        norm_groups.append(normalized)\n    X_norm = np.concatenate(norm_groups, axis=1)\n    X_norm = gamma * X_norm + beta\n    X_norm = X_norm.tolist()\n    return X_norm"}
{"task_id": 126, "completion_id": 2, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (C, H, W) = X.shape[1:]\n    assert C % num_groups == 0, 'Number of channels should be divisible by number of groups'\n    group_size = C // num_groups\n    X_reshaped = X.reshape(X.shape[0], num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    return (gamma * X_norm + beta).round(4).tolist()"}
{"task_id": 126, "completion_id": 3, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_norm + beta\n    return Y.reshape(B, C, H, W).round(4).tolist()"}
{"task_id": 126, "completion_id": 4, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    assert len(X.shape) == 4, 'Input tensor X must be 4 dimensional.'\n    (B, C, H, W) = X.shape\n    assert len(gamma.shape) == 1 and len(beta.shape) == 1, 'Gamma and beta should be 1 dimensional.'\n    assert C == gamma.shape[0] and C == beta.shape[0], 'Shapes of gamma and beta should be equal to the number of channels C.'\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    group_size = C // num_groups\n    Y = np.zeros_like(X)\n    for i in range(0, C, group_size):\n        start = i\n        end = min(i + group_size, C)\n        group_slice = slice(start, end)\n        X_group = X[:, group_slice, :, :]\n        mean = np.mean(X_group, axis=(2, 3), keepdims=True)\n        var = np.var(X_group, axis=(2, 3), keepdims=True)\n        X_group_norm = (X_group - mean) / np.sqrt(var + epsilon)\n        X_group_norm = gamma[0, group_slice, 0, 0] * X_group_norm + beta[0, group_slice, 0, 0]\n        Y[:, group_slice, :, :] = X_group_norm\n    Y = np.round(Y, 4)\n    return Y.tolist()"}
{"task_id": 126, "completion_id": 5, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'The number of channels should be divisible by num_groups'\n    slice_size = C // num_groups\n    Y = np.zeros_like(X)\n    for i in range(num_groups):\n        X_slice = X[:, i * slice_size:(i + 1) * slice_size, :, :]\n        mean = np.mean(X_slice, axis=(2, 3), keepdims=True)\n        var = np.var(X_slice, axis=(2, 3), keepdims=True)\n        X_slice_norm = (X_slice - mean) / np.sqrt(var + epsilon)\n        Y[:, i * slice_size:(i + 1) * slice_size, :, :] = gamma[i * slice_size:(i + 1) * slice_size, np.newaxis, np.newaxis] * X_slice_norm + beta[i * slice_size:(i + 1) * slice_size, np.newaxis, np.newaxis]\n    return Y.round(4).tolist()\nX = np.random.randn(2, 8, 4, 4)\ngamma = np.ones(8)\nbeta = np.zeros(8)"}
{"task_id": 126, "completion_id": 6, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    X_grouped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_normalized + beta\n    return np.round(Y, 4).tolist()"}
{"task_id": 126, "completion_id": 7, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    if X.ndim != 4:\n        raise ValueError('input should be 4D tensor. e.g. (B, C, H, W)')\n    (batch_size, num_channels, height, width) = X.shape\n    if num_channels % num_groups != 0:\n        raise ValueError('number of channels should be exactly divisible by num_groups')\n    group_size = num_channels // num_groups\n    num_pixels = height * width\n    X_reshaped = X.reshape(batch_size, num_groups, group_size, num_pixels)\n    mean = np.mean(X_reshaped, axis=(2, 3), keepdims=True)\n    variance = np.var(X_reshaped, axis=(2, 3), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    Y = gamma * X_normalized + beta\n    return np.round(Y.reshape(batch_size, num_channels, height, width).tolist(), 4)\ngamma = np.ones((1, C, 1, 1))\nbeta = np.zeros((1, C, 1, 1))\nnum_groups = 2"}
{"task_id": 126, "completion_id": 8, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    X = X.reshape(X.shape[0], num_groups, -1, *X.shape[2:])\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = ((X - mean) ** 2).mean(axis=(2, 3, 4), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    Y = gamma * X_hat + beta\n    Y = Y.reshape(*X.shape[:2], *X.shape[3:])\n    return [round(i, 4) for i in Y.flatten().tolist()]\nX = np.random.randn(4, 3, 2, 2)\ngamma = np.ones(3)\nbeta = np.zeros(3)\nnum_groups = 2"}
{"task_id": 126, "completion_id": 9, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"Function to perform Group Normalization\"\"\"\n    if len(X.shape) != 4:\n        raise ValueError('Tensor input X should have 4 dimensions - (B, C, H, W)')\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError(f'Number of channels should be divisible by num_groups. Number of channels: {C}, num_groups: {num_groups}')\n    gamma_r = gamma.reshape(1, C, 1, 1)\n    beta_r = beta.reshape(1, C, 1, 1)\n    groupnorm_X = np.zeros_like(X)\n    for b in range(B):\n        for c in range(0, C, num_groups):\n            group_slice = slice(c, c + num_groups)\n            X_group = X[b, group_slice, :, :]\n            mean_X_g = np.mean(X_group, axis=(1, 2), keepdims=True)\n            var_X_g = np.var(X_group, axis=(1, 2), keepdims=True)\n            X_group_norm = (X_group - mean_X_g) / np.sqrt(var_X_g + epsilon)\n            X_group_norm_scale_shift = gamma_r[:, group_slice, :, :] * X_group_norm + beta_r[:, group_slice, :, :]\n            groupnorm_X[b, group_slice, :, :] = X_group_norm_scale_shift\n    return groupnorm_X.round(4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        grad = df(x)\n        if np.abs(grad) < tolerance:\n            break\n        x = x - learning_rate * grad\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 1, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.01, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    f = lambda x: x ** 4 - 3 * x ** 3 + 2\n    df = lambda x: 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        grad = df(x)\n        if np.abs(grad) < tolerance:\n            return np.round(x, 4)\n        x = x - learning_rate * grad\n    raise ValueError(f'Function did not converge within {max_iters} iterations')"}
{"task_id": 127, "completion_id": 2, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    function = lambda x: x ** 4 - 3 * x ** 3 + 2\n    derivative = lambda x: 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    iterations = 0\n    while True:\n        previous_x = current_x\n        current_x = current_x - learning_rate * derivative(previous_x)\n        iterations += 1\n        if iterations > max_iters:\n            print('Max iterations reached. Result may not be accurate.')\n            return round(current_x, 4)\n        if np.abs(current_x - previous_x) < tolerance:\n            return round(current_x, 4)\nstart_x = 0"}
{"task_id": 127, "completion_id": 3, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        if abs(gradient) < tolerance:\n            break\n        x -= learning_rate * gradient\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 4, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    f = lambda x: x ** 4 - 3 * x ** 3 + 2\n    df = lambda x: 4 * x ** 3 - 9 * x ** 2\n    for _ in range(max_iters):\n        gradient = df(x)\n        if np.abs(gradient) < tolerance:\n            break\n        x = x - learning_rate * gradient\n    return round(x, 4)\nstart_x = np.random.uniform(-2, 2)"}
{"task_id": 127, "completion_id": 5, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def grad(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        grad_value = grad(x)\n        x = x - learning_rate * grad_value\n        if np.abs(grad_value) < tolerance:\n            break\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 6, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the value of x where f(x) reaches its minimum, starting from any random initial position.\n\n    Args:\n        start_x (float): Initial x position.\n        learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.1.\n        tolerance (float, optional): Tolerance for the change in x for terminating the algorithm. Defaults to 1e-6.\n        max_iters (int, optional): Maximum number of iterations for the algorithm. Defaults to 10000.\n\n    Returns:\n        float: The x position where f(x) reaches its minimum.\n    \"\"\"\n    x = start_x\n    df = lambda y: 4 * y ** 3 - 9 * y ** 2\n    for _ in range(max_iters):\n        delta_x = -learning_rate * df(x)\n        if np.abs(delta_x) < tolerance:\n            break\n        x += delta_x\n    return round(x, 4)\nstart_x = np.random.uniform(-10, 10)"}
{"task_id": 127, "completion_id": 7, "solution": "import numpy as np\ndef grad_f(x):\n    return 4 * x ** 3 - 9 * x ** 2\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = grad_f(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            return np.round(new_x, 4)\n        x = new_x\n    return np.round(x, 4)"}
{"task_id": 127, "completion_id": 8, "solution": "import numpy as np\ndef gradient(x):\n    return 4 * x ** 3 - 9 * x ** 2\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x_current = start_x\n    for _ in range(max_iters):\n        gradient_current = gradient(x_current)\n        if np.abs(gradient_current) < tolerance:\n            break\n        x_current = x_current - learning_rate * gradient_current\n    return round(x_current, 4)"}
{"task_id": 127, "completion_id": 9, "solution": "import numpy as np\ndef df(x):\n    return 4 * x ** 3 - 9 * x ** 2\ndef gradient_descent(start_x, learning_rate, tolerance, max_iters):\n    x = start_x\n    steps = 0\n    previous_step_size = 1\n    while previous_step_size > tolerance and steps < max_iters:\n        prev_x = x\n        x -= learning_rate * df(prev_x)\n        previous_step_size = abs(x - prev_x)\n        steps += 1\n    return round(x, 4)\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    return gradient_descent(start_x, learning_rate, tolerance, max_iters)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    dytanh = gamma * np.tanh(beta * alpha * x)\n    dytanh = np.round(dytanh, 4)\n    dytanh = dytanh.tolist()\n    return dytanh"}
{"task_id": 128, "completion_id": 1, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mu = np.mean(x)\n    sigma = np.std(x)\n    dyt_output = alpha * np.tanh((x - mu) / sigma) + gamma * x + beta\n    dyt_output = [round(i, 4) for i in dyt_output.tolist()]\n    return dyt_output"}
{"task_id": 128, "completion_id": 2, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    dyT = alpha * (2 / (1 + np.exp(-2 * (beta * x) / gamma)) - 1)\n    return np.around(dyT, 4).tolist()"}
{"task_id": 128, "completion_id": 3, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    dyt = list(alpha * np.tanh(gamma * x + beta).round(4))\n    return dyt"}
{"task_id": 128, "completion_id": 4, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mean = np.mean(x)\n    std = np.std(x)\n    x = alpha * ((x - mean) / std) * gamma + beta\n    x = np.tanh(x)\n    x = np.round(x, 4)\n    x = x.tolist()\n    return x"}
{"task_id": 128, "completion_id": 5, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mean = np.mean(x)\n    var = np.var(x)\n    normalized = (x - mean) / np.sqrt(var + beta)\n    tanh = 2 * alpha / (1 + np.exp(-2 * alpha * gamma * normalized)) - alpha\n    dy_tanh = gamma * (1 - (tanh / alpha) ** 2)\n    return np.round(dy_tanh, 4).tolist()"}
{"task_id": 128, "completion_id": 6, "solution": "from typing import List\nimport numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> List[float]:\n    \"\"\"Implement the Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\"\"\"\n    y = alpha * gamma * x - beta * np.tanh(alpha * gamma * x)\n    return y.round(4).tolist()\nx = np.array([1, 2, 3, 4, 5])\nalpha = 0.5\ngamma = 0.5\nbeta = 0.5"}
{"task_id": 128, "completion_id": 7, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh: Normalization-Free Transformer Activation\n\n    DyT function replaces layer normalization in Transformer architectures \n    while preserving squashing behavior and enabling stable training.\n\n    Parameters:\n    x (np.ndarray): input array\n    alpha (float): alpha parameter\n    gamma (float): gamma parameter\n    beta (float): beta parameter\n\n    Returns:\n    list[float]: list of dynamic tanh results\n    \"\"\"\n    x = x - np.mean(x)\n    x = np.tanh(alpha * x) * gamma + beta\n    return np.round(x, 4).tolist()\nx = np.array([1, 2, 3, 4, 5])\nalpha = 0.5\ngamma = 2.0\nbeta = -1.0"}
{"task_id": 128, "completion_id": 8, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh: Normalization-Free Transformer Activation\n\n    Args:\n    x: Input numpy ndarray.\n    alpha: alpha value for activation function.\n    gamma: gamma value for activation function.\n    beta: beta value for activation function.\n\n    Returns:\n    A list of rounded values after dynamic tanh transformation.\n    \"\"\"\n    result = alpha * np.tanh(gamma * x + beta)\n    return np.round(result, 4).tolist()\nx = np.array([-0.5, -0.7, 0, 0.6, 1.0])\nalpha = 0.2\ngamma = 1.5\nbeta = 0.1"}
{"task_id": 128, "completion_id": 9, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> np.ndarray:\n    mean = np.mean(x)\n    variance = np.var(x)\n    normalized = (x - mean) / np.sqrt(variance + np.finfo(np.float32).eps)\n    dy_tanh = alpha * np.tanh(gamma * normalized + beta)\n    return np.around(dy_tanh, decimals=4).tolist()\nx = np.array([1, 2, 3, 4, 5])\nalpha = 1.0\ngamma = 1.0\nbeta = 0.0"}
