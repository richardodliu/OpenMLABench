{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 1, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    Returns the resulting vector as a list if the operation is valid, or -1 if the dimensions are incompatible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product_sum = 0\n        for i in range(len(row)):\n            dot_product_sum += row[i] * b[i]\n        result.append(dot_product_sum)\n    return result"}
{"task_id": 1, "completion_id": 2, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float: The resulting vector if the operation is valid.\n    int: -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a or not a[0] or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 3, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the dot product of a matrix `a` and a vector `b`. If the number of columns\n    in `a` is not the same as the length of `b`, returns -1.\n    \n    Parameters:\n    a (list of list of int/float): The matrix to be used in the dot product calculation.\n    b (list of int/float): The vector to be used in the dot product calculation.\n    \n    Returns:\n    list of int/float or int: A list representing the dot product of the matrix and vector,\n                              or -1 if the dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 4, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    :param a: Matrix represented as a list of lists of integers or floats.\n    :param b: Vector represented as a list of integers or floats.\n    :return: A list representing the resulting vector or -1 if dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 5, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 6, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    :param a: Matrix, represented as a list of lists of integers or floats.\n    :param b: Vector, represented as a list of integers or floats.\n    :return: A list representing the resulting vector if the operation is valid, or -1 otherwise.\n    \"\"\"\n    num_rows = len(a)\n    if num_rows == 0:\n        return -1\n    num_cols = len(a[0])\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != num_cols:\n            return -1\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 7, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector. Returns a list representing the resulting vector,\n    or -1 if the matrix and vector dimensions are incompatible.\n    \n    :param a: A list of lists representing the matrix, with each inner list representing one row.\n    :param b: A list representing the vector.\n    :return: A list representing the result of the dot product or -1 if dimensions are incompatible.\n    \"\"\"\n    if not (a and b) or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        dot_product = sum((x * b[i] for (i, x) in enumerate(row)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 8, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * b[i] for (i, x) in enumerate(row)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 9, "solution": "import numpy as np\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Args:\n    a (list of list of int|float): The matrix.\n    b (list of int|float): The vector.\n    \n    Returns:\n    list of int|float: The resulting vector if operation is valid; -1 if dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transposes the given matrix.\n    \n    Parameters:\n    a (list of list of int|float): The input matrix.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 1, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function takes a matrix (list of lists) as input and returns its transpose.\n    The transpose of a matrix is a new matrix whose rows are the columns of the original.\n    \n    :param a: List of lists representing the input matrix where all inner lists have the same length.\n    :return: Transpose of the input matrix as a list of lists.\n    \"\"\"\n    transposed_matrix = [[row[i] for row in a] for i in range(len(a[0]))]\n    return transposed_matrix"}
{"task_id": 2, "completion_id": 2, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function takes a 2D matrix `a` as input and returns its transpose.\n    \n    :param a: 2D list of numbers (integers or floats)\n    :return: Transpose of the input matrix `a`\n    \"\"\"\n    rows = len(a)\n    cols = len(a[0]) if a else 0\n    transposed_matrix = []\n    for col in range(cols):\n        transposed_row = []\n        for row in range(rows):\n            transposed_row.append(a[row][col])\n        transposed_matrix.append(transposed_row)\n    return transposed_matrix"}
{"task_id": 2, "completion_id": 3, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Compute the transpose of a given matrix.\n\n    Parameters:\n    - a (list of list of int/float): The matrix to be transposed.\n\n    Returns:\n    - list of list of int/float: The transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 4, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix.\n    \n    Returns:\n    list of list of int/float: The transposed matrix.\n    \"\"\"\n    num_rows = len(a)\n    num_cols = len(a[0]) if a else 0\n    transpose = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transpose[j][i] = a[i][j]\n    return transpose"}
{"task_id": 2, "completion_id": 5, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Compute and return the transpose of the matrix a.\n    \n    Parameters:\n    - a: A list of lists, where each inner list represents a row of the matrix.\n    \n    Returns:\n    - A list of lists, where each inner list represents a row of the transposed matrix.\n    \"\"\"\n    num_rows = len(a)\n    num_cols = len(a[0]) if a else 0\n    transposed = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed[j][i] = a[i][j]\n    return transposed"}
{"task_id": 2, "completion_id": 6, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix for which the transpose is to be calculated.\n    \n    Returns:\n    list of list of int/float: The transpose of the input matrix.\n    \"\"\"\n    return [list(row) for row in zip(*a)]"}
{"task_id": 2, "completion_id": 7, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix to be transposed.\n    \n    Returns:\n    list of list of int/float: The transposed matrix.\n    \"\"\"\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transposed_matrix = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    return transposed_matrix"}
{"task_id": 2, "completion_id": 8, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transpose a given matrix.\n    \n    Args:\n    a: The matrix to be transposed. It's assumed that all sub-lists are of equal length.\n    \n    Returns:\n    A new list of lists representing the transposed matrix.\n    \"\"\"\n    return [list(row) for row in zip(*a)]"}
{"task_id": 2, "completion_id": 9, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Compute the transpose of a given matrix.\n    \n    The function takes a matrix `a` represented as a list of lists, where each inner list represents a row of the matrix.\n    It returns the transpose of the matrix, where rows and columns are swapped.\n    \n    Parameters:\n    a (list[list[int|float]]): The input matrix.\n    \n    Returns:\n    list[list[int|float]]: The transpose of the input matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes the given matrix a into the specified new_shape and returns the reshaped matrix as a python list.\n    If the matrix cannot be reshaped to the new_shape, it returns an empty list.\n    \n    Parameters:\n    a (list of list of int/float): The matrix to be reshaped.\n    new_shape (tuple of int): The new shape as a tuple of two integers (rows, columns).\n    \n    Returns:\n    list of list of int/float: The reshaped matrix as a python list or an empty list if reshaping is not possible.\n    \"\"\"\n    a_np = np.array(a)\n    try:\n        reshaped_np = a_np.reshape(new_shape)\n        return reshaped_np.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 1, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Given a matrix 'a' and a new shape 'new_shape', return a reshaped matrix if possible.\n    If the reshaping is not possible, return an empty list.\n    \n    :param a: List of lists representing the original matrix\n    :param new_shape: Tuple representing the new shape (rows, columns)\n    :return: List of lists representing the reshaped matrix, or an empty list if reshaping is not possible\n    \"\"\"\n    original_array = np.array(a)\n    original_shape = original_array.shape\n    if original_shape[0] * original_shape[1] != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = original_array.reshape(new_shape)\n    return reshaped_array.tolist()\nnew_shape = (1, 4)"}
{"task_id": 3, "completion_id": 2, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix a into a specified shape new_shape. If the reshape is not possible,\n    return an empty list.\n    \n    :param a: List of lists of integers or floats representing the matrix to be reshaped.\n    :param new_shape: A tuple of two integers representing the new shape.\n    :return: A reshaped list of lists or an empty list if reshape is not possible.\n    \"\"\"\n    try:\n        np_array = np.array(a)\n        reshaped_array = np_array.reshape(new_shape)\n        return reshaped_array.tolist()\n    except ValueError:\n        return []\nnew_shape = (1, 4)"}
{"task_id": 3, "completion_id": 3, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given matrix 'a' into the specified shape 'new_shape'.\n    If the reshape operation cannot be performed, return an empty list.\n    \n    Args:\n    a (list[list[int|float]]): The original matrix.\n    new_shape (tuple[int, int]): The new shape to reshape the matrix.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix or an empty list if reshaping is not possible.\n    \"\"\"\n    original_shape = (len(a), len(a[0]))\n    original_size = original_shape[0] * original_shape[1]\n    new_size = new_shape[0] * new_shape[1]\n    if original_size != new_size:\n        return []\n    try:\n        reshaped_matrix = np.array(a).reshape(new_shape).tolist()\n    except ValueError:\n        reshaped_matrix = []\n    return reshaped_matrix\na = [[1, 2, 3], [4, 5, 6]]\nnew_shape = (3, 2)\nreshaped_matrix = reshape_matrix(a, new_shape)"}
{"task_id": 3, "completion_id": 4, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix a into a specified shape defined by new_shape.\n    If the reshaping is not possible, returns an empty list.\n    \n    Args:\n    a: A list of lists representing the original matrix.\n    new_shape: A tuple representing the desired shape for the matrix.\n    \n    Returns:\n    A list of lists representing the reshaped matrix, or an empty list if reshaping is not possible.\n    \"\"\"\n    a_np = np.array(a)\n    original_shape = a_np.shape\n    if a_np.size == new_shape[0] * new_shape[1]:\n        reshaped_a = a_np.reshape(new_shape)\n        return reshaped_a.tolist()\n    else:\n        return []\nnew_shape = (3, 3)"}
{"task_id": 3, "completion_id": 5, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given 2D matrix `a` into the specified `new_shape`.\n    If the reshaping is not possible, return an empty list.\n    \n    :param a: 2D list representing the matrix to be reshaped\n    :param new_shape: A tuple of two elements (rows, columns) indicating the desired shape\n    :return: A reshaped 2D list if possible, otherwise an empty list\n    \"\"\"\n    np_arr = np.array(a)\n    if np_arr.size == new_shape[0] * new_shape[1]:\n        reshaped_arr = np_arr.reshape(new_shape)\n        return reshaped_arr.tolist()\n    else:\n        return []"}
{"task_id": 3, "completion_id": 6, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given 2D list 'a' into the new shape 'new_shape' if possible.\n    If the reshape operation cannot be performed, return an empty list.\n    \n    :param a: List of lists containing integers or floats representing a 2D matrix.\n    :param new_shape: Tuple containing the desired shape (rows, columns) for the matrix.\n    :return: Reshaped matrix as a list of lists if reshape is possible, otherwise an empty list.\n    \"\"\"\n    np_array = np.array(a)\n    try:\n        reshaped_array = np_array.reshape(new_shape)\n        reshaped_list = reshaped_array.tolist()\n        return reshaped_list\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 7, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes the given 2D matrix 'a' into the specified 'new_shape' if possible.\n    If the reshape is not possible due to size mismatch, returns an empty list.\n    \n    :param a: List of lists, representing the 2D matrix to be reshaped.\n    :param new_shape: A tuple indicating the desired shape (rows, columns).\n    :return: The reshaped matrix as a list of lists, or an empty list if the reshape is not possible.\n    \"\"\"\n    if not a:\n        return []\n    original_shape = (len(a), len(a[0]))\n    original_size = original_shape[0] * original_shape[1]\n    new_size = new_shape[0] * new_shape[1]\n    if original_size != new_size:\n        return []\n    a_np = np.array(a)\n    try:\n        reshaped_np = a_np.reshape(new_shape)\n    except ValueError:\n        return []\n    return reshaped_np.tolist()\nnew_shape = (3, 2)"}
{"task_id": 3, "completion_id": 8, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given matrix 'a' into the specified 'new_shape'. If the reshape operation cannot be performed,\n    return an empty list.\n    \n    :param a: List of lists representing the matrix to be reshaped.\n    :param new_shape: Tuple representing the new shape (rows, columns) of the matrix.\n    :return: The reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    a_np = np.array(a)\n    if a_np.size == new_shape[0] * new_shape[1]:\n        reshaped_np = np.reshape(a_np, new_shape)\n        return reshaped_np.tolist()\n    else:\n        return []"}
{"task_id": 3, "completion_id": 9, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given matrix into a specified shape.\n    If the reshaping is not possible, return an empty list.\n\n    :param a: List of lists representing the initial matrix.\n    :param new_shape: Tuple representing the desired shape (rows, columns).\n    :return: Reshaped matrix as a list of lists or an empty list if the reshape is not possible.\n    \"\"\"\n    np_array = np.array(a)\n    if np_array.size == new_shape[0] * new_shape[1]:\n        reshaped_array = np_array.reshape(new_shape)\n        return reshaped_array.tolist()\n    else:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the means of the rows or columns in a given matrix.\n    \n    Parameters:\n    - matrix: The matrix (list of lists of floats) to calculate means for.\n    - mode: A string ('row' or 'column') indicating whether to calculate means by row or by column.\n    \n    Returns:\n    - A list of floats containing the mean values for each row or column, depending on the mode.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Invalid mode. Mode must be 'row' or 'column'.\")\n    np_matrix = np.array(matrix)\n    if mode == 'row':\n        means = np_matrix.mean(axis=1).tolist()\n    else:\n        means = np_matrix.mean(axis=0).tolist()\n    return means"}
{"task_id": 4, "completion_id": 1, "solution": "from typing import List\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculates the mean of the given matrix by row or by column based on the mode.\n    \n    :param matrix: A 2D list of floats representing the matrix.\n    :param mode: A string, either 'row' or 'column', to specify the direction of mean calculation.\n    :return: A list of floats representing the mean values calculated according to the specified mode.\n    \"\"\"\n    if mode.lower() == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode.lower() == 'column':\n        col_sums = [0] * len(matrix[0])\n        for row in matrix:\n            for (index, value) in enumerate(row):\n                col_sums[index] += value\n        means = [total_sum / len(matrix) for total_sum in col_sums]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    return means"}
{"task_id": 4, "completion_id": 2, "solution": "import statistics\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of each row or each column in a matrix.\n    \n    Parameters:\n    - matrix: A 2D list of numerical values.\n    - mode: A string which can be 'row' or 'column' indicating whether to calculate mean by row or column\n    \n    Returns:\n    - A list of mean values calculated either for each row or each column.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'column':\n        matrix = list(zip(*matrix))\n    means = [statistics.mean(row) for row in matrix]\n    return means"}
{"task_id": 4, "completion_id": 3, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of each row or column of a matrix.\n    \n    Args:\n    matrix: A list of lists of floats representing the matrix.\n    mode: A string indicating whether to calculate the mean by 'row' or 'column'.\n    \n    Returns:\n    A list of floats representing the means.\n    \"\"\"\n    if mode.lower() == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode.lower() == 'column':\n        transposed_matrix = list(zip(*matrix))\n        return [sum(col) / len(col) for col in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 4, "completion_id": 4, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by either row or column.\n    \n    :param matrix: List of lists where each inner list represents a row.\n    :param mode: String indicating whether to calculate means by 'row' or 'column'.\n    :return: A list of means based on the specified mode.\n    :raises ValueError: If mode is neither 'row' nor 'column'.\n    \"\"\"\n    result_means = []\n    if mode == 'row':\n        for row in matrix:\n            result_means.append(sum(row) / len(row))\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        for i in range(num_columns):\n            column_sum = sum((row[i] for row in matrix))\n            result_means.append(column_sum / len(matrix))\n    else:\n        raise ValueError(\"Mode should be either 'row' or 'column'.\")\n    return result_means"}
{"task_id": 4, "completion_id": 5, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    This function calculates the mean of each row or each column of a given matrix.\n    \n    Args:\n    matrix: A list of lists (2D list) containing numerical data.\n    mode: A string specifying the mode of operation - 'row' for row-wise mean or 'column' for column-wise mean.\n    \n    Returns:\n    A list of means depending on the specified mode.\n    \"\"\"\n    if mode == 'row':\n        return [np.mean(row) for row in matrix]\n    elif mode == 'column':\n        columns = list(zip(*matrix))\n        return [np.mean(column) for column in columns]\n    else:\n        raise ValueError(\"Invalid mode. Choose either 'row' or 'column'.\")\nmatrix = [[10.0, 20.0], [30.0, 40.0]]\nmode = 'row'"}
{"task_id": 4, "completion_id": 6, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix by rows or columns.\n    \n    :param matrix: A 2D list of numerical values.\n    :param mode: A string indicating whether to calculate the mean by 'row' or 'column'.\n    :return: A list of means either for each row or each column.\n    \"\"\"\n    mat = np.array(matrix)\n    if mode.lower() == 'row':\n        mean_values = [np.mean(row) for row in mat]\n    elif mode.lower() == 'column':\n        mean_values = [np.mean(column) for column in mat.T]\n    else:\n        raise ValueError(\"Invalid mode specified. Choose either 'row' or 'column'.\")\n    return mean_values"}
{"task_id": 4, "completion_id": 7, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of each row or each column in a given matrix.\n    \n    :param matrix: A list of lists where each sublist represents a row in the matrix.\n    :param mode: A string that specifies whether to calculate the mean 'row'-wise or 'column'-wise.\n    :return: A list of mean values for each row or each column, depending on the mode.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    if mode == 'row':\n        return [np.mean(row) for row in matrix]\n    else:\n        transposed_matrix = zip(*matrix)\n        return [np.mean(column) for column in transposed_matrix]"}
{"task_id": 4, "completion_id": 8, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    This function calculates the mean of a matrix either by row or by column,\n    based on the given mode.\n    \n    Parameters:\n    - matrix (list of lists): The input matrix.\n    - mode (str): 'row' for mean by row or 'column' for mean by column.\n    \n    Returns:\n    - list of floats: A list of means calculated according to the specified mode.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    if mode == 'row':\n        means = np_matrix.mean(axis=1)\n    elif mode == 'column':\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    return means.tolist()\nmatrix = [[2.0, 4.0, 6.0], [3.0, 6.0, 9.0]]"}
{"task_id": 4, "completion_id": 9, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row ('row') or by column ('column').\n    \n    Args:\n    - matrix: A list of lists containing numerical values (floats).\n    - mode: A string that specifies the mode as either 'row' or 'column'.\n    \n    Returns:\n    - A list of means, either for each row or each column.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode should be 'row' or 'column'\")\n    if not matrix or not all((isinstance(row, list) for row in matrix)):\n        raise ValueError('Matrix should be a non-empty list of lists')\n    if mode == 'row':\n        return [np.mean(row) for row in matrix]\n    else:\n        transposed_matrix = np.transpose(matrix)\n        return [np.mean(column) for column in transposed_matrix]\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the scalar and returns the resulting matrix.\n    \n    :param matrix: A list of lists of integers or floats representing the matrix.\n    :param scalar: An integer or float that is the scalar to multiply the matrix by.\n    :return: A new matrix where each element is the product of the original element and the scalar.\n    \"\"\"\n    result = [[scalar * element for element in row] for row in matrix]\n    return result"}
{"task_id": 5, "completion_id": 1, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a given matrix by a scalar and returns the result matrix.\n\n    :param matrix: A list of lists representing the matrix to be multiplied.\n    :param scalar: An int or float representing the scalar to multiply the matrix by.\n    :return: A new matrix resulting from the scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 2, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply a matrix by a scalar.\n    \n    :param matrix: A 2D list representing the matrix.\n    :param scalar: An integer or float representing the scalar multiplier.\n    :return: A new 2D list representing the matrix after scalar multiplication.\n    \"\"\"\n    result_matrix = [[element * scalar for element in row] for row in matrix]\n    return result_matrix"}
{"task_id": 5, "completion_id": 3, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the given scalar.\n\n    Parameters:\n    matrix (list of list of int/float): The matrix to be multiplied.\n    scalar (int/float): The scalar to multiply with the matrix.\n\n    Returns:\n    list of list of int/float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 4, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element in the given matrix by a scalar value.\n\n    Parameters:\n    - matrix: A list of lists (2D list) where each sublist represents a row of the matrix.\n    - scalar: A number by which each element of the matrix will be multiplied.\n\n    Returns:\n    - A new matrix (2D list) with the result of the scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 5, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply each element of the given matrix by the scalar value.\n    \n    Args:\n    matrix: A list of lists, where each sublist represents a row in the matrix.\n    scalar: A number by which all elements in the matrix will be multiplied.\n    \n    Returns:\n    A new matrix where each element is a result of the multiplication of the original element by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 6, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element in the given matrix by the scalar value.\n    \n    :param matrix: A list of lists, where each inner list represents a row in the matrix.\n    :param scalar: The scalar value to multiply the matrix by.\n    :return: The result of the scalar multiplication of the matrix.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 7, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the given scalar.\n    \n    Parameters:\n    matrix (list of list of int|float): The matrix to be multiplied.\n    scalar (int|float): The scalar to multiply the matrix by.\n    \n    Returns:\n    list of list of int|float: The resulting matrix after multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 8, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply each element of the matrix by the scalar and return the new matrix.\n\n    :param matrix: A two-dimensional list where each sublist represents a row in the matrix.\n    :param scalar: A number (integer or float) by which to multiply each element of the matrix.\n    :return: A new matrix with each element multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 9, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiply a matrix by a scalar.\n    \n    Args:\n    - matrix (list of list of int or float): The matrix to be multiplied.\n    - scalar (int or float): The scalar to multiply the matrix by.\n    \n    Returns:\n    - list of list of int or float: The result matrix after scalar multiplication.\n    \"\"\"\n    result_matrix = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result_matrix.append(new_row)\n    return result_matrix\nmatrix = [[1, 2], [3, 4]]\nscalar = 3"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    Args:\n    matrix (list of list of float|int): a 2x2 matrix represented as a list of two rows, \n                                        each containing two numeric elements.\n    \n    Returns:\n    list[float]: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    return sorted(eigenvalues, reverse=True)"}
{"task_id": 6, "completion_id": 1, "solution": "import math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate and return the sorted eigenvalues (from highest to lowest) of a 2x2 matrix.\n    :param matrix: A 2x2 matrix represented as a list of lists\n    :return: A list of the calculated eigenvalues [lambda1, lambda2] sorted in descending order.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    eigenvalue1 = (trace + math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalue2 = (trace - math.sqrt(trace ** 2 - 4 * determinant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)"}
{"task_id": 6, "completion_id": 2, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n    matrix (list of list of float/int): A 2x2 matrix for which to find the eigenvalues.\n    \n    Returns:\n    list of float: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    return sorted(eigenvalues, reverse=True)"}
{"task_id": 6, "completion_id": 3, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Parameters:\n    - matrix (list[list[float|int]]): A 2x2 matrix provided as a list of lists.\n\n    Returns:\n    - list[float]: A list containing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    matrix_array = np.array(matrix)\n    (eigenvalues, _) = np.linalg.eig(matrix_array)\n    return sorted(eigenvalues, reverse=True)"}
{"task_id": 6, "completion_id": 4, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate and return the eigenvalues of a 2x2 matrix sorted in descending order.\n    \n    Args:\n    - matrix (list of list): A 2x2 matrix provided as a list of lists.\n    \n    Returns:\n    - list of float: Sorted list of eigenvalues in descending order.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    eigenvalues_sorted = sorted(eigenvalues, reverse=True)\n    return [float(e) if isinstance(e, np.float64) else complex(e) for e in eigenvalues_sorted]"}
{"task_id": 6, "completion_id": 5, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists with integers or floats.\n    :return: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues"}
{"task_id": 6, "completion_id": 6, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n    matrix (list of lists of float/int): A 2x2 matrix.\n    \n    Returns:\n    list of float: Sorted list containing the eigenvalues of the matrix, from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return [float(val) for val in sorted_eigenvalues]"}
{"task_id": 6, "completion_id": 7, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Args:\n    matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    list of float: A list containing the eigenvalues of the matrix, sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    eigenvalues_sorted = sorted(eigenvalues, reverse=True)\n    return eigenvalues_sorted"}
{"task_id": 6, "completion_id": 8, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: A list of floats representing the eigenvalues, sorted in descending order.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    eigenvalues_sorted = sorted(eigenvalues, reverse=True)\n    return list(eigenvalues_sorted)"}
{"task_id": 6, "completion_id": 9, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n    matrix (list of lists): A 2x2 matrix.\n    \n    Returns:\n    list of floats: A list containing the calculated eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    sorted_eigenvalues = np.sort(eigenvalues)[::-1]\n    return sorted_eigenvalues.tolist()"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^(-1)AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_matrix = T_inv.dot(A_np).dot(S_inv)\n        transformed_matrix = np.round(transformed_matrix, 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[2, 0], [0, 2]]\nT = [[1, 0], [0, 1]]\nS = [[0.5, 0], [0, 0.5]]"}
{"task_id": 7, "completion_id": 1, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> int | list[list[int | float]]:\n    \"\"\"\n    Calculates T^(-1)*A*S for 2D matrices A, T, and S. Checks if T and S are invertible.\n    Returns -1 if inverse does not exist, else returns the result rounded to 4 decimal places.\n    \"\"\"\n    try:\n        A = np.array(A)\n        T = np.linalg.inv(np.array(T))\n        S = np.linalg.inv(np.array(S))\n        transformed = np.round(T @ A @ S, 4)\n        return transformed.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[2, 0], [0, 2]]"}
{"task_id": 7, "completion_id": 2, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}*A*S, where T and S are invertible matrices.\n    \n    Parameters:\n    A (list of list of int|float): The matrix to be transformed.\n    T (list of list of int|float): The invertible matrix on the left.\n    S (list of list of int|float): The invertible matrix on the right.\n    \n    Returns:\n    list of list of int|float: The transformed matrix. If T or S are not invertible, return -1.\n    \"\"\"\n    try:\n        (A, T, S) = (np.array(A), np.array(T), np.array(S))\n        T_inv = np.linalg.inv(T)\n        transformed = T_inv @ A @ S\n        transformed = np.round(transformed, 4)\n        return transformed.tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 7, "completion_id": 3, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]):\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are\n    assumed to be invertible square matrices of appropriate dimensions.\n    Checks if T and S are invertible, performs the transformation, and\n    returns the result rounded to 4 decimal places, else returns -1 if\n    T or S are not invertible or dimensions are incompatible.\n    \"\"\"\n    try:\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            return -1\n        if T.shape[0] != A.shape[0] or S.shape[1] != A.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T)\n        transformed_matrix = T_inv @ A @ S\n        transformed_matrix = np.round(transformed_matrix, 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 4]]"}
{"task_id": 7, "completion_id": 4, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^{-1}AS, where T and S are\n    invertible matrices. Returns -1 if either T or S are not invertible.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        inv_T = np.linalg.inv(T_np)\n        inv_S = np.linalg.inv(S_np)\n        transformed_matrix = inv_T @ A_np @ S_np\n        result = np.round(transformed_matrix, 4).tolist()\n        return result\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[0, 1], [1, 0]]"}
{"task_id": 7, "completion_id": 5, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms a given matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If either T or S are not invertible or cannot be inverted due to precision issues, the function returns -1.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        if np.isnan(T_inv).any() or np.isnan(S_inv).any():\n            return -1\n        transformed_matrix = T_inv @ A_np @ S_inv\n        transformed_matrix = np.round(transformed_matrix, 4).tolist()\n        return transformed_matrix\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 7, "completion_id": 6, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        if A_np.shape[0] != T_np.shape[0] or A_np.shape[1] != S_np.shape[0]:\n            return -1\n        transformed_matrix = np.dot(np.dot(T_inv, A_np), S_inv)\n        transformed_matrix_rounded = np.around(transformed_matrix, decimals=4).tolist()\n        return transformed_matrix_rounded\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[1, 2], [2, 1]]\nS = [[3, 4], [5, 6]]"}
{"task_id": 7, "completion_id": 7, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Takes three matrices A, T, and S, checks if T and S are invertible,\n    then applies the transformation T^{-1}AS to A. If T or S are not invertible,\n    returns -1.\n    \n    Parameters:\n        A (list of list of int/float): The input matrix to be transformed.\n        T (list of list of int/float): The invertible transformation matrix.\n        S (list of list of int/float): The invertible transformation matrix.\n        \n    Returns:\n        list of list of int/float: The transformed matrix, or -1 if T or S is not invertible.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        result = np.round(T_inv @ A_np @ S_inv, 4)\n        return result.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 7, "completion_id": 8, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Performs the matrix transformation T^{-1}AS.\n    \n    Parameters:\n    A (list of list of int or float): The input matrix A.\n    T (list of list of int or float): The invertible matrix T.\n    S (list of list of int or float): The invertible matrix S.\n    \n    Returns:\n    list of list of int or float: The transformed matrix, or -1 if no solution exists.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_matrix = np.dot(np.dot(T_inv, A_np), S_np)\n        result = np.round(transformed_matrix, 4).tolist()\n        return result\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[5, 6], [7, 8]]\nS = [[10, 11], [12, 13]]\nresult = transform_matrix(A, T, S)"}
{"task_id": 7, "completion_id": 9, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    This function takes three matrices A, T, and S, checks if T and S are invertible,\n    and then returns the matrix T^(-1)*A*S if possible, rounded to the nearest 4th decimal.\n    \n    If T or S are not invertible, the function returns -1.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1] or np.linalg.det(T_np) == 0 or (np.linalg.det(S_np) == 0):\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        AS = np.dot(A_np, S_np)\n        transformed = np.dot(T_inv, AS)\n        transformed_rounded = np.round(transformed, 4)\n        return transformed_rounded.tolist()\n    except Exception as e:\n        print(f'An error occurred: {e}')\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, -1], [0, 1]]"}
{"task_id": 8, "completion_id": 0, "solution": "import sys\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    If the matrix is not invertible (i.e., determinant is zero), return 'None'.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: The inverse matrix if it exists, otherwise 'None'.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse_matrix"}
{"task_id": 8, "completion_id": 1, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    \n    Args:\n        matrix (list[list[float]]): A 2x2 matrix given as a list of two lists, \n                                    each containing two elements representing rows.\n    \n    Returns:\n        list[list[float]]: The inverse of the given matrix, or None if the matrix is not invertible.\n    \"\"\"\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix"}
{"task_id": 8, "completion_id": 2, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix if it exists, and return None if the matrix is not invertible.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix provided as a list of two lists.\n    \n    Returns:\n    list[list[float]] or None: The inverse of the matrix, or None if the matrix is not invertible.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix"}
{"task_id": 8, "completion_id": 3, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates and returns the inverse of a 2x2 matrix if it is invertible.\n    Returns None if the matrix is not invertible.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_a = d / determinant\n    inv_b = -b / determinant\n    inv_c = -c / determinant\n    inv_d = a / determinant\n    return [[inv_a, inv_b], [inv_c, inv_d]]"}
{"task_id": 8, "completion_id": 4, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix. Returns None if the matrix is not invertible.\n    \n    :param matrix: A 2x2 matrix represented as a list of two lists, each containing two floats.\n    :return: The inverse of the matrix if it's invertible, otherwise None.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input matrix must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = ([d / det, -b / det], [-c / det, a / det])\n    return inv_matrix"}
{"task_id": 8, "completion_id": 5, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    Returns None if the matrix is not invertible.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists\n    :return: The inverse of the matrix, or None if the matrix is not invertible\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    determinant = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if determinant == 0:\n        return None\n    inv_matrix = [[matrix[1][1] / determinant, -1 * (matrix[0][1] / determinant)], [-1 * (matrix[1][0] / determinant), matrix[0][0] / determinant]]\n    return inv_matrix"}
{"task_id": 8, "completion_id": 6, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix if it exists. Return None if the matrix is not invertible.\n    \n    :param matrix: A 2x2 matrix (list of lists) with float elements\n    :return: The inverse matrix as a list of lists or None if not invertible\n    \"\"\"\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inv_matrix = [[matrix[1][1] / det, -matrix[0][1] / det], [-matrix[1][0] / det, matrix[0][0] / det]]\n    return inv_matrix"}
{"task_id": 8, "completion_id": 7, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    If the matrix is not invertible, the function returns None.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists of floats.\n    :return: The inverse of the matrix as a 2x2 matrix, or None if not invertible.\n    \"\"\"\n    determinant = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if determinant == 0:\n        return None\n    else:\n        factor = 1 / determinant\n        inverse = [[factor * matrix[1][1], factor * -matrix[0][1]], [factor * -matrix[1][0], factor * matrix[0][0]]]\n        return inverse"}
{"task_id": 8, "completion_id": 8, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    \n    Parameters:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of two lists, each containing two elements.\n    \n    Returns:\n    list[list[float]]: The inverse of the given 2x2 matrix if it is invertible, otherwise returns None.\n    \"\"\"\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    adjugate = [[matrix[1][1], -matrix[0][1]], [-matrix[1][0], matrix[0][0]]]\n    inverse = [[x / det for x in row] for row in adjugate]\n    return inverse"}
{"task_id": 8, "completion_id": 9, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix.\n    If the matrix is not invertible, returns None.\n    \n    :param matrix: A list of two lists signifying a 2x2 matrix.\n    :return: The inverse of the matrix if invertible, else None.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix\nmatrix = [[4, 7], [2, 6]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices if they are compatible for multiplication. If not, returns -1.\n    \n    :param a: First matrix (list of lists of integers or floats)\n    :param b: Second matrix (list of lists of integers or floats)\n    :return: The resultant matrix if a and b can be multiplied, otherwise -1.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 1, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> int | list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices if possible and returns the result.\n    If the matrices cannot be multiplied (i.e., the number of columns in the first matrix\n    is not equal to the number of rows in the second matrix), return -1.\n    \n    Parameters:\n    a (list[list[int|float]]): The first matrix (2D list).\n    b (list[list[int|float]]): The second matrix (2D list).\n    \n    Returns:\n    -1 if matrices cannot be multiplied, otherwise the result of multiplication (list[list[int|float]]).\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nmatrix_a = [[1, 2, 3], [4, 5, 6]]\nmatrix_b = [[7, 8], [9, 10], [11, 12]]\nresult = matrixmul(matrix_a, matrix_b)"}
{"task_id": 9, "completion_id": 2, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices a and b.\n    \n    Parameters:\n        a (list of lists of int|float): First matrix.\n        b (list of lists of int|float): Second matrix.\n    \n    Returns:\n        list of lists of int|float: Resultant matrix from multiplying a and b, or -1 if the operation is not possible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 3, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8], [9, 10], [11, 12]]\nresult = matrixmul(A, B)"}
{"task_id": 9, "completion_id": 4, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices if they are compatible, else returns -1.\n    \n    :param a: First matrix\n    :param b: Second matrix\n    :return: The resulting matrix if multiplication is possible, otherwise -1\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 5, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n    Returns -1 if the matrices cannot be multiplied due to incompatible dimensions.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 6, "solution": "from typing import List, Union\ndef matrixmul(a: List[List[Union[int, float]]], b: List[List[Union[int, float]]]) -> Union[int, List[List[Union[int, float]]]]:\n    \"\"\"\n    Multiplies two matrices a and b. If the matrices cannot be multiplied, returns -1.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\na = [[1, 2], [3, 4]]\nb = [[1, 2, 3], [4, 5, 6]]"}
{"task_id": 9, "completion_id": 7, "solution": "from typing import List, Union\ndef matrixmul(a: List[List[Union[int, float]]], b: List[List[Union[int, float]]]) -> Union[List[List[Union[int, float]]], int]:\n    \"\"\"\n    Multiply two matrices if they are compatible for multiplication. The number of columns\n    in the first matrix must match the number of rows in the second matrix.\n    \n    Args:\n    a: First matrix as a list of lists.\n    b: Second matrix as a list of lists.\n    \n    Returns:\n    The product of the matrices as a list of lists, or -1 if the matrices cannot be multiplied.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0]) if a else 0\n    rows_b = len(b)\n    cols_b = len(b[0]) if b else 0\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\na = [[1, 2, 3], [4, 5, 6]]\nb = [[7, 8], [9, 10], [11, 12]]\ng = [[1]]\nh = [[2, 3, 4]]\ni = matrixmul(g, h)\nj = [[1]]\nk = [[2], [3]]"}
{"task_id": 9, "completion_id": 8, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 9, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices if they are compatible for multiplication.\n    If the matrices cannot be multiplied, returns -1.\n    \n    :param a: First matrix as a list of lists.\n    :param b: Second matrix as a list of lists.\n    :return: The result of the multiplication as a matrix or -1 if it cannot be calculated.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors: A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    A list of lists which represents the covariance matrix of the input vectors.\n    \"\"\"\n    vectors_t = np.array(vectors).T\n    cov_matrix = np.cov(vectors_t, bias=True)\n    return cov_matrix.tolist()"}
{"task_id": 10, "completion_id": 1, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Arguments:\n    vectors -- a list of lists where each inner list represents a feature with its observations\n    \n    Returns:\n    A list of lists representing the covariance matrix.\n    \"\"\"\n    matrix = np.array(vectors)\n    cov_matrix = np.cov(matrix)\n    return cov_matrix.astype(float).tolist()"}
{"task_id": 10, "completion_id": 2, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    vectors_np = np.array(vectors)\n    cov_matrix = np.cov(vectors_np)\n    return cov_matrix.tolist()"}
{"task_id": 10, "completion_id": 3, "solution": "from typing import List\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    Each inner list in `vectors` represents a feature with its observations.\n    \n    Args:\n    vectors: A list of lists, where each inner list contains observations for a feature.\n    \n    Returns:\n    A covariance matrix represented as a list of lists.\n    \"\"\"\n\n    def mean(nums: List[float]) -> float:\n        return sum(nums) / len(nums)\n    num_features = len(vectors)\n    observations = len(vectors[0])\n    means = [mean(vector) for vector in vectors]\n    covariance_matrix = []\n    for i in range(num_features):\n        temp = []\n        for j in range(num_features):\n            cov = sum(((vectors[i][k] - means[i]) * (vectors[j][k] - means[j]) for k in range(observations))) / (observations - 1)\n            temp.append(cov)\n        covariance_matrix.append(temp)\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 4, "solution": "from typing import List\nimport numpy as np\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (List[List[float]]): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    List[List[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    data_matrix = np.array(vectors)\n    means = np.mean(data_matrix, axis=1)\n    deviations = data_matrix - means[:, np.newaxis]\n    n = len(data_matrix[0])\n    cov_matrix = deviations @ deviations.T / (n - 1)\n    return cov_matrix.tolist()"}
{"task_id": 10, "completion_id": 5, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of lists where each inner list represents a vector of observations.\n    \n    Returns:\n    list of list of float: The covariance matrix for the given vectors.\n    \"\"\"\n\n    def mean(values):\n        return sum(values) / float(len(values))\n    column_means = [mean(column) for column in vectors]\n    deviations = [[vectors[i][row] - column_means[i] for row in range(len(vectors[i]))] for i in range(len(vectors))]\n    n = len(vectors[0])\n    covariance_matrix = [[0 for _ in range(len(vectors))] for _ in range(len(vectors))]\n    for i in range(len(vectors)):\n        for j in range(len(vectors)):\n            covariance_matrix[i][j] = sum((deviations[i][r] * deviations[j][r] for r in range(n))) / (n - 1)\n    return covariance_matrix\nvectors = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]"}
{"task_id": 10, "completion_id": 6, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix given a list of lists, where each inner list represents a feature.\n    \n    :param vectors: A list of lists where each inner list is a feature vector.\n    :return: A covariance matrix as a list of lists.\n    \"\"\"\n    data_matrix = np.array(vectors)\n    cov_matrix = np.cov(data_matrix, rowvar=False)\n    cov_matrix_as_list = cov_matrix.tolist()\n    return cov_matrix_as_list"}
{"task_id": 10, "completion_id": 7, "solution": "from typing import List\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    List[List[float]]: The covariance matrix as a matrix of lists.\n    \"\"\"\n    from numpy import cov\n    vectors_transposed = list(zip(*vectors))\n    covariance_matrix = cov(vectors_transposed).tolist()\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 8, "solution": "from typing import List\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (List[List[float]]): A list of lists, where each inner list represents a feature vector.\n    \n    Returns:\n    List[List[float]]: The covariance matrix.\n    \"\"\"\n    observations = list(zip(*vectors))\n    n = len(observations)\n    mean_vector = [sum(col) / n for col in observations]\n    normalized_data = [[x - mean_vector[i] for (i, x) in enumerate(col)] for col in observations]\n    covariance_matrix = [[sum((x * y for (x, y) in zip(vec1, vec2))) / (n - 1) for vec1 in normalized_data] for vec2 in normalized_data]\n    return list(map(list, zip(*covariance_matrix)))"}
{"task_id": 10, "completion_id": 9, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of lists of floats): A list of vectors, each vector is a list of observations.\n    \n    Returns:\n    list of lists of floats: The covariance matrix as a list of lists.\n    \"\"\"\n    matrix = np.array(vectors)\n    cov_matrix = np.cov(matrix)\n    return cov_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n    \n    Parameters:\n    A (np.ndarray): The coefficient matrix of the system.\n    b (np.ndarray): The solution matrix.\n    n (int): The number of iterations.\n    \n    Returns:\n    list: The approximate solution of the system of equations after n iterations.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    D = np.diag(np.diag(A))\n    L = -np.tril(A - D)\n    U = -np.triu(A - D)\n    T = np.dot(np.linalg.inv(D), L + U)\n    C = np.dot(np.linalg.inv(D), b)\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.dot(T, x) + C\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 10"}
{"task_id": 11, "completion_id": 1, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve the system of linear equations Ax = b using the Jacobi iterative method.\n    The method iterates n times, rounding each intermediate solution to four decimal places.\n    \n    :param A: The coefficient matrix for the system of equations.\n    :param b: The constant terms for the system of equations.\n    :param n: Number of iterations to perform.\n    :return: The approximate solution vector x as a python list.\n    \"\"\"\n    x = np.zeros_like(b, dtype=np.float64)\n    D = np.diag(np.diag(A))\n    LU = A - D\n    D_inv = np.diag(1 / np.diag(D))\n    for _ in range(n):\n        x_old = x\n        x = np.dot(D_inv, b - np.dot(LU, x))\n        x = np.round(x, 4)\n        if np.allclose(x, x_old, rtol=1e-06, atol=1e-06):\n            break\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 25"}
{"task_id": 11, "completion_id": 2, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): A square matrix representing the coefficients of the system of linear equations.\n    b (np.ndarray): A column vector representing the constants of the system of linear equations.\n    n (int): The number of iterations for the Jacobi method.\n    \n    Returns:\n    list: A list containing the approximate solution for the system of linear equations.\n    \"\"\"\n    D_inv = np.diag(1 / np.diag(A))\n    R = A - np.diag(np.diag(A))\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.dot(D_inv, b - np.dot(R, x))\n        x = np.round(x_new, decimals=4)\n    return x.tolist()\nA = np.array([[4.0, -1.0, 0.0, 0.0], [-1.0, 4.0, -1.0, 0.0], [0.0, -1.0, 4.0, -1.0], [0.0, 0.0, -1.0, 3.0]])\nb = np.array([11.0, 14.0, 14.0, 10.0])\nn = 20"}
{"task_id": 11, "completion_id": 3, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a given system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    - A: Coefficient matrix (2D numpy array).\n    - b: Resultant values (1D numpy array).\n    - n: Number of iterations.\n    \n    Returns:\n    - x: Approximate solution (1D list).\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square!')\n    x = np.zeros_like(b)\n    D = np.diag(A)\n    for i in range(n):\n        x_new = (b - np.dot(A, x) + D * x) / D\n        x = x_new\n        x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 25"}
{"task_id": 11, "completion_id": 4, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    x = np.zeros_like(b)\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    for _ in range(n):\n        x_new = (b - np.dot(R, x)) / D\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([3, 1])\nn = 100\nx = solve_jacobi(A, b, n)"}
{"task_id": 11, "completion_id": 5, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n    \n    Parameters:\n    A (np.ndarray): A square matrix representing the coefficients of the linear equations.\n    b (np.ndarray): A column vector representing constants of the equations.\n    n (int): Number of iterations for the Jacobi method.\n    \n    Returns:\n    list: Approximate solutions to the system of equations, rounded to 4 decimal places.\n    \"\"\"\n    N = len(b)\n    x = np.zeros_like(b)\n    for it_count in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, 1], [1, 3]], dtype=float)\nb = np.array([7, 12], dtype=float)\nn = 20"}
{"task_id": 11, "completion_id": 6, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using Jacobi method.\n    Returns an approximate solution vector x.\n    \n    :param A: Coefficient matrix as a numpy array.\n    :param b: Constant terms as a numpy array.\n    :param n: Number of iterations.\n    :return: Approximate solution vector as a list.\n    \"\"\"\n    n_eq = len(b)\n    x = np.zeros(n_eq)\n    for _ in range(n):\n        x_new = np.zeros(n_eq)\n        for i in range(n_eq):\n            s = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 7, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a linear system of equations in the form Ax = b using the Jacobi iterative method.\n    \n    :param A: A numpy ndarray representing the coefficient matrix.\n    :param b: A numpy ndarray representing the constant terms of the equations.\n    :param n: An integer specifying the number of iterations for the Jacobi method.\n    :return: A list containing the approximate solution vector x.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be a square matrix.'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b sizes must match.'\n    x = np.zeros_like(b)\n    for it in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            total = 0\n            for j in range(A.shape[1]):\n                if j != i:\n                    total += A[i, j] * x[j]\n            x_new[i] = (b[i] - total) / A[i, i]\n        x = x_new\n        x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5"}
{"task_id": 11, "completion_id": 8, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi Iteration method.\n    \n    Parameters:\n    A : np.ndarray\n        The coefficient matrix of the system of equations.\n    b : np.ndarray\n        The constant vector of the system of equations.\n    n : int\n        The number of iterations for the Jacobi method.\n        \n    Returns:\n    list\n        The approximate solution vector x as a python list with elements rounded to the 4th decimal place.\n    \"\"\"\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.copy(x)\n        for i in range(A.shape[0]):\n            s = sum((A[i][j] * x[j] for j in range(A.shape[1]) if i != j))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x_new = np.round(x_new, decimals=4)\n        x = x_new\n    return x.tolist()"}
{"task_id": 11, "completion_id": 9, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Uses Jacobi method to solve a system of linear equations given by Ax = b.\n    The function iterates 'n' times and returns the approximate solution x.\n    \n    Parameters:\n    - A : np.ndarray, square matrix of the system of equations\n    - b : np.ndarray, column vector of the right-hand side constants\n    - n : int, number of iterations\n    \n    Returns:\n    - x.tolist() : list, approximate solution to the system of equations\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square')\n    x = np.zeros_like(b)\n    D = np.diag(np.diag(A))\n    R = A - D\n    for _ in range(n):\n        x_new = np.dot(np.linalg.inv(D), b - np.dot(R, x))\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[5, 3, 1], [3, 5, 1], [1, 1, 4]], dtype=float)\nb = np.array([23, 17, 10], dtype=float)"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (tr, det) = (np.trace(ATA), np.linalg.det(ATA))\n    lambda1 = (tr + np.sqrt(tr ** 2 - 4 * det)) / 2\n    lambda2 = (tr - np.sqrt(tr ** 2 - 4 * det)) / 2\n    s1 = np.sqrt(lambda1)\n    s2 = np.sqrt(lambda2)\n    return (round(s1, 4), round(s2, 4))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 1, "solution": "import numpy as np\nfrom math import sqrt\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Calculate the singular values of a 2x2 matrix A without directly using numpy's svd function.\n    \n    Parameters:\n    A (numpy.ndarray): 2x2 matrix.\n    \n    Returns:\n    tuple: A tuple of two values (singular values of the 2x2 matrix A).\n    \"\"\"\n    (a, b, c, d) = (A[0, 0], A[0, 1], A[1, 0], A[1, 1])\n    m = np.array([[a * a + b * b, a * c + b * d], [a * c + b * d, c * c + d * d]])\n    tr_m = m[0, 0] + m[1, 1]\n    det_m = m[0, 0] * m[1, 1] - m[0, 1] * m[1, 0]\n    term = sqrt(tr_m ** 2 / 4 - det_m)\n    s1_squared = tr_m / 2 + term\n    s2_squared = tr_m / 2 - term\n    s1 = sqrt(s1_squared)\n    s2 = sqrt(s2_squared)\n    return (round(s1, 4), round(s2, 4))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix using an approximate Jacobian-based method.\n    \n    Args:\n        A (np.ndarray): A 2x2 matrix.\n        \n    Returns:\n        tuple: A tuple of two floating numbers representing the singular values of the matrix.\n    \"\"\"\n\n    def frobenius_norm(M):\n        return np.sqrt(np.sum(M * M))\n\n    def approximate_singular_values(A):\n        tol = 1e-06\n        max_iter = 1000\n        delta = 100\n        assert not np.all(A == 0), 'Input matrix should not be a zero matrix.'\n        A_norm = frobenius_norm(A)\n        U = np.eye(2)\n        V = np.eye(2)\n        for _ in range(max_iter):\n            UVt = U @ V.T\n            J = A - UVt\n            J_U = J @ V\n            J_V = J.T @ U\n            U_new = U + delta * J_U\n            V_new = V + delta * J_V\n            U = U_new\n            V = V_new\n            if frobenius_norm(J) < tol:\n                break\n        singular_values = tuple(np.sort((frobenius_norm(U[:, i]) for i in range(2))))\n        return singular_values\n    singular_values = approximate_singular_values(A)\n    return tuple((round(sv, 4) for sv in singular_values))\nA = np.array([[1, 2], [2, 1]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 12, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    eigenvalues_ATA = np.linalg.eigvalsh(ATA)\n    singular_values = np.sqrt(eigenvalues_ATA)\n    singular_values_rounded = np.round(singular_values, 4)\n    return tuple(sorted(singular_values_rounded, reverse=True))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the singular values of a 2x2 matrix A using the Jacobian method.\n    \n    Parameters:\n    - A: np.ndarray, a 2x2 matrix.\n    \n    Returns:\n    - tuple: Singular values rounded to the nearest 4th decimal.\n    \"\"\"\n    AtA = np.dot(A.T, A)\n    eigenvalues = np.linalg.eigvalsh(AtA)\n    singular_values = np.sqrt(eigenvalues)\n    return tuple((round(value, 4) for value in sorted(singular_values, reverse=True)))\nA = np.array([[3, 4], [5, 6]])"}
{"task_id": 12, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix A using eigenvalues without directly using numpy's svd method.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 square matrix.\n    \n    Returns:\n    tuple: A tuple of two singular values of matrix A, rounded to the nearest 4th decimal.\n    \"\"\"\n    ATA = A.T @ A\n    eigenvalues = np.linalg.eigvals(ATA)\n    singular_values = sorted(np.sqrt(eigenvalues), reverse=True)\n    return tuple((round(value, 4) for value in singular_values))"}
{"task_id": 12, "completion_id": 6, "solution": "import numpy as np\nfrom scipy.optimize import minimize\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2x2 matrix A without using numpy's svd function.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 numpy array representing the matrix to decompose.\n    \n    Returns:\n    tuple: A tuple containing the singular values rounded to the nearest 4th decimal.\n    \"\"\"\n    A_times_A_T = np.dot(A.T, A)\n    eigenvalues = np.linalg.eigvals(A_times_A_T)\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = sorted(singular_values, reverse=True)\n    singular_values_rounded = np.round(singular_values, 4)\n    return tuple(singular_values_rounded)\nA = np.array([[3, 4], [5, 6]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 12, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix A using its eigenvalues.\n    \n    Parameters:\n    - A: np.ndarray (2x2 matrix)\n    \n    Returns:\n    - Tuple with 2 singular values (rounded to 4 decimal places)\n    \"\"\"\n    ATA = np.dot(A, A.T)\n    eigenvalues = np.linalg.eigvals(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = np.round(singular_values, 4)\n    singular_values = tuple(sorted(singular_values, reverse=True))\n    return singular_values\nA = np.array([[2, 4], [1, 3]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 12, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) for a 2x2 matrix A using a formulaic approach.\n    \n    Args:\n        A: numpy.ndarray - A 2x2 matrix.\n        \n    Returns:\n        A tuple containing the singular values of A rounded to 4th decimal place.\n    \"\"\"\n    frob_square = A[0, 0] ** 2 + A[0, 1] ** 2 + A[1, 0] ** 2 + A[1, 1] ** 2\n    diff_square = (A[0, 0] * A[1, 1]) ** 2 - (A[0, 1] * A[1, 0]) ** 2\n    sing_val1 = np.sqrt((frob_square + np.sqrt(frob_square ** 2 - 4 * diff_square)) / 2)\n    sing_val2 = np.sqrt((frob_square - np.sqrt(frob_square ** 2 - 4 * diff_square)) / 2)\n    sing_val1 = round(sing_val1, 4)\n    sing_val2 = round(sing_val2, 4)\n    return (sing_val1, sing_val2)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 12, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    This function computes the singular values of a 2x2 matrix using the Jacobian method.\n    The singular values approximated are returned as a tuple.\n    \"\"\"\n\n    def rotate_matrix(A, theta):\n        \"\"\"Apply rotation to matrix A given an angle theta.\"\"\"\n        c = np.cos(theta)\n        s = np.sin(theta)\n        first_row = [c * A[0, 0] + s * A[1, 0], c * A[0, 1] + s * A[1, 1]]\n        second_row = [-s * A[0, 0] + c * A[1, 0], -s * A[0, 1] + c * A[1, 1]]\n        return np.array([first_row, second_row])\n\n    def find_theta(a, b, c):\n        \"\"\"Calculate theta based on the off-diagonal element c and diagonal elements a, b.\"\"\"\n        if a == b:\n            theta = 0.5 * np.pi\n        else:\n            theta = 0.5 * np.arctan(2 * c / (a - b))\n        return theta\n\n    def is_converging(off_diag):\n        \"\"\"Check if the off-diagonal element is close to zero.\"\"\"\n        return abs(off_diag) < 1e-06\n    A = np.array(A, dtype=float)\n    (a, b, c, d) = (A[0, 0], A[0, 1], A[1, 0], A[1, 1])\n    for _ in range(500):\n        theta = find_theta(a, d, b)\n        A = rotate_matrix(A, theta)\n        (a, b, c, d) = (A[0, 0], A[0, 1], A[1, 0], A[1, 1])\n        if is_converging(b):\n            break\n    singular_value_1 = np.sqrt(a ** 2 + c ** 2)\n    singular_value_2 = np.sqrt(b ** 2 + d ** 2)\n    return (round(singular_value_1, 4), round(singular_value_2, 4))\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        \"\"\"\n        Helper function to calculate the determinant of a 3x3 matrix.\n        :param minor: A 3x3 matrix.\n        :return: The determinant of the 3x3 matrix.\n        \"\"\"\n        return minor[0][0] * (minor[1][1] * minor[2][2] - minor[1][2] * minor[2][1]) - minor[0][1] * (minor[1][0] * minor[2][2] - minor[1][2] * minor[2][0]) + minor[0][2] * (minor[1][0] * minor[2][1] - minor[1][1] * minor[2][0])\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det += (-1) ** i * matrix[0][i] * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 1, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of 4 lists, each containing 4 elements.\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n\n    def determinant_3x3(sub_matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        :param sub_matrix: A 3x3 matrix represented as a list of 3 lists, each containing 3 elements.\n        :return: The determinant of the 3x3 matrix.\n        \"\"\"\n        return sub_matrix[0][0] * sub_matrix[1][1] * sub_matrix[2][2] + sub_matrix[0][1] * sub_matrix[1][2] * sub_matrix[2][0] + sub_matrix[0][2] * sub_matrix[1][0] * sub_matrix[2][1] - sub_matrix[0][2] * sub_matrix[1][1] * sub_matrix[2][0] - sub_matrix[0][1] * sub_matrix[1][0] * sub_matrix[2][2] - sub_matrix[0][0] * sub_matrix[1][2] * sub_matrix[2][1]\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det += (-1) ** i * matrix[0][i] * determinant_3x3(minor)\n    return det\nmatrix = [[2, 4, 5, 6], [1, 3, 4, 2], [3, 4, 6, 1], [1, 2, 3, 4]]"}
{"task_id": 13, "completion_id": 2, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(sub_matrix: list[list[int | float]]) -> float:\n        (a, b, c) = sub_matrix[0]\n        (d, e, f) = sub_matrix[1]\n        (g, h, i) = sub_matrix[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('The matrix must be a 4x4 square matrix.')\n    det = 0\n    for col in range(4):\n        sign = (-1) ** col\n        cofactor = matrix[0][col]\n        minor = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        det += sign * cofactor * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 3, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion Method.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix represented as a list of 4 lists, each with 4 elements.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix using the standard formula.\n        \n        Args:\n        minor (list of list of int|float): A 3x3 matrix represented as a list of 3 lists, each with 3 elements.\n        \n        Returns:\n        float: The determinant of the 3x3 matrix.\n        \"\"\"\n        det = 0\n        for col in range(3):\n            submatrix = [minor[i][0:col] + minor[i][col + 1:] for i in range(1, 3)]\n            det += (-1) ** col * minor[0][col] * determinant_2x2(submatrix)\n        return det\n\n    def determinant_2x2(minor: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 2x2 matrix.\n        \n        Args:\n        minor (list of list of int|float): A 2x2 matrix represented as a list of 2 lists, each with 2 elements.\n        \n        Returns:\n        float: The determinant of the 2x2 matrix.\n        \"\"\"\n        return minor[0][0] * minor[1][1] - minor[0][1] * minor[1][0]\n    det = 0\n    for col in range(4):\n        submatrix = [matrix[i][0:col] + matrix[i][col + 1:] for i in range(1, 4)]\n        det += (-1) ** col * matrix[0][col] * determinant_3x3(submatrix)\n    return det"}
{"task_id": 13, "completion_id": 4, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n\n    def minor(mat: list[list[int | float]], i: int, j: int) -> list[list[int | float]]:\n        \"\"\"\n        Compute the minor matrix of a given matrix by removing the ith row and jth column.\n        \n        :param mat: The original matrix.\n        :param i: The row index to remove.\n        :param j: The column index to remove.\n        :return: The minor matrix.\n        \"\"\"\n        return [row[:j] + row[j + 1:] for row in mat[:i] + mat[i + 1:]]\n\n    def det(mat: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a matrix using recursion.\n        \n        :param mat: A square matrix.\n        :return: The determinant of the matrix.\n        \"\"\"\n        if len(mat) == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        determinant = 0\n        for (column_index, value) in enumerate(mat[0]):\n            determinant += value * (-1) ** column_index * det(minor(mat, 0, column_index))\n        return determinant\n    return det(matrix)"}
{"task_id": 13, "completion_id": 5, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    :param matrix: A 4x4 matrix represented as a list of 4 lists, each containing 4 elements.\n    :return: The determinant of the matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Matrix must be 4x4')\n\n    def minor(mat, i, j):\n        \"\"\"Return the minor obtained by removing row i and column j from mat.\"\"\"\n        return [[mat[row][col] for col in range(4) if col != j] for row in range(4) if row != i]\n\n    def det(mat):\n        \"\"\"Calculate the determinant of a matrix using recursive Laplace Expansion.\"\"\"\n        size = len(mat)\n        if size == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        return sum(((-1) ** col * mat[0][col] * det(minor(mat, 0, col)) for col in range(size)))\n    return det(matrix)"}
{"task_id": 13, "completion_id": 6, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    - matrix: A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    - float: The determinant of the given matrix.\n    \"\"\"\n\n    def determinant_3x3(m: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix.\n        \n        Args:\n        - m: A 3x3 matrix.\n        \n        Returns:\n        - float: The determinant of the given 3x3 matrix.\n        \"\"\"\n        return sum((m[0][c] * (m[1][(c + 1) % 3] * m[2][(c + 2) % 3] - m[1][(c + 2) % 3] * m[2][(c + 1) % 3]) for c in range(3)))\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix')\n    det = 0\n    for col in range(4):\n        sign = (-1) ** col\n        minor = [[matrix[i][j] for j in range(4) if j != col] for i in range(1, 4)]\n        det += sign * matrix[0][col] * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 7, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    float: The determinant of the given 4x4 matrix.\n    \"\"\"\n\n    def determinant_3x3(m: list[list[int | float]]) -> float:\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n\n    def minor(matrix, row, col) -> list[list[int | float]]:\n        return [row[:col] + row[col + 1:] for row in matrix[:row] + matrix[row + 1:]]\n    n = len(matrix)\n    det = 0\n    for c in range(n):\n        sign = (-1) ** c\n        minor_matrix = minor(matrix, 0, c)\n        if len(minor_matrix) == 3:\n            minor_det = determinant_3x3(minor_matrix)\n        else:\n            minor_det = determinant_4x4(minor_matrix)\n        det += sign * matrix[0][c] * minor_det\n    return det\nmatrix = [[1, 3, 5, 9], [1, 3, 1, 7], [4, 3, 9, 7], [5, 2, 0, 9]]"}
{"task_id": 13, "completion_id": 8, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    matrix (list[list[int|float]]): A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \n    Raises:\n    ValueError: If the input is not a 4x4 matrix.\n    \"\"\"\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix')\n\n    def minor(mat: list[list[int | float]], i: int, j: int) -> list[list[int | float]]:\n        \"\"\"\n        Returns the minor matrix of a given matrix after removing the ith row and jth column.\n        \"\"\"\n        return [row[:j] + row[j + 1:] for row in mat[:i] + mat[i + 1:]]\n\n    def determinant_recursive(mat: list[list[int | float]]) -> int | float:\n        \"\"\"\n        Calculates the determinant of a nxn matrix recursively.\n        Base case: 2x2 matrix.\n        \"\"\"\n        if len(mat) == 2 and len(mat[0]) == 2:\n            return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n        det = 0\n        for c in range(len(mat)):\n            det += (-1) ** c * mat[0][c] * determinant_recursive(minor(mat, 0, c))\n        return det\n    return determinant_recursive(matrix)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 9, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's expansion.\n    \"\"\"\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix.\n        \"\"\"\n        return minor[0][0] * (minor[1][1] * minor[2][2] - minor[2][1] * minor[1][2]) - minor[0][1] * (minor[1][0] * minor[2][2] - minor[2][0] * minor[1][2]) + minor[0][2] * (minor[1][0] * minor[2][1] - minor[2][0] * minor[1][1])\n\n    def create_minor(matrix: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        \"\"\"\n        Create a minor matrix by removing the specified row and column from the matrix.\n        \"\"\"\n        minor = [row[:] for (i, row) in enumerate(matrix) if i != row]\n        return [row[:col] + row[col + 1:] for row in minor]\n    det = 0\n    for col in range(4):\n        sign = (-1) ** col\n        cofactor = matrix[0][col] * sign\n        minor = create_minor(matrix, 0, col)\n        det += cofactor * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Computes the coefficients for a linear regression model using the normal equation.\n    Rounds the result to 4 decimal places.\n    \n    :param X: A list of lists containing floats, representing the feature matrix.\n    :param y: A list of floats, representing the target vector.\n    :return: A list of floats representing the coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X_with_bias = np.hstack([np.ones((X.shape[0], 1)), X])\n    theta = np.linalg.inv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n    theta_rounded = [round(x, 4) for x in theta.tolist()]\n    return theta_rounded\nX_example = [[1, 2], [4, 5], [6, 7]]\ny_example = [5, 6, 7]\ntheta = linear_regression_normal_equation(X_example, y_example)"}
{"task_id": 14, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list of list of float): The feature matrix without the bias term.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model rounded to 4 decimal places.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    (m, n) = X_np.shape\n    X_b = np.c_[np.ones((m, 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    return [round(i, 4) for i in theta]"}
{"task_id": 14, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    - X (list of list of float): The features matrix.\n    - y (list of float): The target vector.\n    \n    Returns:\n    - list of float: The coefficients of the linear regression.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    (m, n) = X_np.shape\n    X_b = np.c_[np.ones((m, 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, decimals=4)\n    return theta_rounded.tolist()\nX = [[1, 1], [1, 2], [2, 2], [2, 3]]\ny = [1, 2, 2.3, 3.2]"}
{"task_id": 14, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation.\n    \n    Args:\n    X: A list of lists containing the feature data.\n    y: A list containing the target variable data.\n    \n    Returns:\n    A list of float containing the regression coefficients.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    theta_rounded = np.round(theta, decimals=4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation to find the coefficients.\n    \n    Parameters:\n    X : list of list of floats - Feature matrix.\n    y : list of floats - Target vector.\n    \n    Returns:\n    list of floats - Coefficients of the linear regression model.\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation to find the model coefficients.\n    \n    Parameters:\n    X (list of list of float): A list of lists where each inner list represents a feature vector.\n    y (list of float): A list of target values.\n    \n    Returns:\n    list of float: A list of model coefficients rounded to 4 decimal places.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    theta_rounded = np.round(theta, 4).tolist()\n    return theta_rounded\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 6]"}
{"task_id": 14, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation and return the model coefficients.\n    \n    Parameters:\n    X (list of lists of float): The feature matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_with_intercept = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_with_intercept.T.dot(X_with_intercept)).dot(X_with_intercept.T).dot(y_np)\n    theta_rounded = np.around(theta, decimals=4)\n    return theta_rounded.tolist()"}
{"task_id": 14, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    X (list of list of float): The features matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: Coefficients of the linear regression model rounded to 4 decimal places.\n    \"\"\"\n    X_array = np.array(X)\n    y_array = np.array(y)\n    X_array = np.insert(X_array, 0, 1, axis=1)\n    theta = np.linalg.inv(X_array.T @ X_array) @ X_array.T @ y_array\n    theta = np.round(theta, 4)\n    theta = theta.tolist()\n    return theta"}
{"task_id": 14, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list of lists of float): The feature matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    X_matrix = np.array(X)\n    y_vector = np.array(y)\n    X_matrix = np.insert(X_matrix, 0, 1, axis=1)\n    theta = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_vector\n    theta = np.round(theta, 4)\n    theta = theta.tolist()\n    return theta\nX = [[1, 2], [2, 3], [3, 5], [4, 1], [5, 3]]\ny = [2, 3, 5, 4, 5]"}
{"task_id": 14, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list of list of float): The feature matrix including a column of ones for the intercept.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: Coefficients of the linear regression model, rounded to four decimal places.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    theta_rounded = [round(coef, 4) for coef in theta]\n    theta_rounded = [-0.0 if -1e-05 < coef < 1e-05 else coef for coef in theta_rounded]\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    :param X: NumPy array of features with a column of ones for the intercept.\n    :param y: NumPy array of the target variable.\n    :param alpha: Learning rate for the gradient descent.\n    :param iterations: The number of iterations to run gradient descent.\n    :return: List of linear regression coefficients rounded to 4 decimal places.\n    \"\"\"\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / m\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / m\n        theta -= alpha * gradient\n    return np.round(theta, decimals=4).tolist()\nX = np.array([[1, 1], [1, 2], [1, 3]])\ny = np.array([2, 3, 4])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    Parameters:\n    - X: np.ndarray, a 2D array with a column of ones for the intercept and the features.\n    - y: np.ndarray, a 1D array representing the target variable.\n    - alpha: float, the learning rate for gradient descent.\n    - iterations: int, the number of iterations to update the coefficients.\n\n    Return:\n    - list, the coefficients of the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = 1 / m * np.dot(X.T, errors)\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        error = X.dot(theta) - y\n        gradient = 1 / m * X.T.dot(error)\n        theta -= alpha * gradient\n    return [round(x, 4) for x in theta.tolist()]"}
{"task_id": 15, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent and return the model coefficients.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix with a column of ones for the intercept.\n    y (np.ndarray): Target vector.\n    alpha (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations to perform gradient descent.\n    \n    Returns:\n    list: List of model coefficients rounded to four decimal places.\n    \"\"\"\n    coef = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, coef)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / len(y)\n        coef -= alpha * gradient\n    return np.around(coef, decimals=4).tolist()"}
{"task_id": 15, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    :param X: numpy.ndarray of shape (n_samples, n_features + 1) including a column of ones for intercept.\n    :param y: numpy.ndarray of shape (n_samples,) containing target values.\n    :param alpha: Learning rate, a positive float.\n    :param iterations: Number of iterations to perform gradient descent.\n    :return: List of coefficients rounded to the nearest 4th decimal.\n    \"\"\"\n    n = X.shape[0]\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(errors, X) / n\n        theta -= alpha * gradient\n    return np.round(theta, decimals=4).tolist()\nX = np.array([[1, 1], [1, 2], [1, 3]])\ny = np.array([1, 2, 3])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        y_pred = X.dot(theta)\n        residuals = y_pred - y\n        gradient = 1 / m * X.T.dot(residuals)\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()\nX = np.hstack([np.ones((X.shape[0], 1)), X])\ny = 2 * X[:, 1] + 3 + np.random.randn(100) * 2\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Performs linear regression using gradient descent.\n    \n    Parameters:\n    - X: a NumPy array of shape (m, n), features including a column of ones for the intercept.\n    - y: a NumPy array of shape (m,), target values.\n    - alpha: float, learning rate.\n    - iterations: int, number of iterations for gradient descent.\n    \n    Returns:\n    - A list of the model coefficients with 4 decimal places.\n    \"\"\"\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / m\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    - X: A NumPy array of shape (n_samples, n_features+1) where the first column is all ones for the intercept.\n    - y: A NumPy array of target values of shape (n_samples, ).\n    - alpha: The learning rate for the gradient descent.\n    - iterations: The number of iterations to run the gradient descent.\n    \n    Returns:\n    - A list of the model's coefficients rounded to the nearest 4th decimal place.\n    \"\"\"\n    n = X.shape[0]\n    m = X.shape[1]\n    theta = np.zeros(m)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / n\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Computes linear regression coefficients using gradient descent.\n    \n    :param X: NumPy array with features; includes a column of ones for the intercept.\n    :param y: Target NumPy array.\n    :param alpha: Learning rate as float.\n    :param iterations: Number of iterations as integer.\n    :return: List of coefficients rounded to four decimal places.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, weights)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / m\n        weights -= alpha * gradient\n    weights_rounded = np.round(weights, 4)\n    return weights_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input dataset using both standardization and min-max normalization.\n    \n    :param data: A 2D NumPy array where each row is a sample and each column is a feature.\n    :return: Two 2D lists. The first is the data standardized, the second is the data normalized by min-max.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.around(standardized_data, 4)\n    min_max_normalized_data = np.around(min_max_normalized_data, 4)\n    return (standardized_data.tolist(), min_max_normalized_data.tolist())"}
{"task_id": 16, "completion_id": 1, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function takes a numpy array and scales it using standardization and min-max normalization,\n    returning the results as 2D lists with all values rounded to 4 decimal places.\n    \n    Parameters:\n    - data: A 2D numpy array where each row represents a data sample and each column represents a feature.\n    \n    Returns:\n    - Two 2D lists: the first list contains data scaled by standardization, and the second one by min-max normalization.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data, min_max_normalized_data)"}
{"task_id": 16, "completion_id": 2, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the features in the input dataset using standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row is a data sample and each column is a feature.\n    \n    Returns:\n    tuple: A tuple of two elements, each a 2D list representing scaling by standardization and min-max normalization respectively.\n    \"\"\"\n    standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_range = np.ptp(data, axis=0)\n    minmax_normalized = (data - min_vals) / max_range\n    standardized = np.round(standardized, decimals=4).tolist()\n    minmax_normalized = np.round(minmax_normalized, decimals=4).tolist()\n    return (standardized, minmax_normalized)"}
{"task_id": 16, "completion_id": 3, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D numpy array and returns two 2D lists representing the data scaled\n    using standardization and min-max normalization respectively.\n    \"\"\"\n    scaled_standard = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    (data_min, data_max) = (np.min(data, axis=0), np.max(data, axis=0))\n    scaled_minmax = (data - data_min) / (data_max - data_min)\n    scaled_standard = np.around(scaled_standard, decimals=4).tolist()\n    scaled_minmax = np.around(scaled_minmax, decimals=4).tolist()\n    return (scaled_standard, scaled_minmax)"}
{"task_id": 16, "completion_id": 4, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling (standardization and min-max normalization) on the input data.\n    \n    Parameters:\n    data (np.ndarray): The input dataset as a 2D NumPy array.\n    \n    Returns:\n    tuple: A tuple containing two lists: standardized data and min-max normalized data.\n    \"\"\"\n    std_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    std_data_list = np.round(std_data, decimals=4).tolist()\n    min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_data_list = np.round(min_max_data, decimals=4).tolist()\n    return (std_data_list, min_max_data_list)"}
{"task_id": 16, "completion_id": 5, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on the dataset using both standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row is a data sample and each column is a feature.\n    \n    Returns:\n    Tuple[list[list[float]], list[list[float]]]: Two 2D lists; the first scaled by standardization,\n    the second scaled by min-max normalization, both rounded to 4 decimal places.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_norm_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data_list = np.round(standardized_data, 4).tolist()\n    min_max_norm_data_list = np.round(min_max_norm_data, 4).tolist()\n    return (standardized_data_list, min_max_norm_data_list)\ndata = np.array([[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]])"}
{"task_id": 16, "completion_id": 6, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a dataset using both standardization and min-max normalization.\n    \n    :param data: 2D NumPy array with each row as a data sample and each column as a feature.\n    :return: Two 2D lists - one standardized and one min-max normalized.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    normalized_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.round(standardized_data, decimals=4).tolist()\n    normalized_data = np.round(normalized_data, decimals=4).tolist()\n    return (standardized_data, normalized_data)\ndata = np.array([[1, 2], [3, 4], [5, 6]])"}
{"task_id": 16, "completion_id": 7, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform standardization and min-max normalization on the input data.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array with each row as a sample and each column as a feature.\n\n    Returns:\n    tuple: A tuple containing two elements:\n        - A 2D list with the data standarized.\n        - A 2D list with the data normalized using min-max.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    minmax_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    minmax_data = np.round(minmax_data, 4).tolist()\n    return (standardized_data, minmax_data)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 16, "completion_id": 8, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    This function takes a 2D NumPy array as input and scales the features using standardization\n    and min-max normalization. It returns two lists containing the scaled data.\n    \n    Parameters:\n    - data (np.ndarray): A 2D array where each row is a sample and each column is a feature.\n    \n    Returns:\n    - tuple: Two lists, the first is the data scaled using standardization and the second using min-max normalization.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_max_normalized_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_max_normalized_data = np.round(min_max_normalized_data, 4).tolist()\n    return (standardized_data, min_max_normalized_data)"}
{"task_id": 16, "completion_id": 9, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input 2D NumPy array using standardization and min-max normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array.\n    \n    Returns:\n    - A 2D list scaled by standardization.\n    - A 2D list scaled by min-max normalization.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    range_val = max_val - min_val\n    normalized_data = (data - min_val) / range_val\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    return (standardized_data, normalized_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        for i in range(k):\n            assigned_points = points[labels == i]\n            if len(assigned_points) > 0:\n                new_centroid = np.mean(assigned_points, axis=0)\n                centroids[i] = new_centroid\n    centroids_rounded = [tuple(map(lambda x: round(x, 4), centroid)) for centroid in centroids]\n    return centroids_rounded\npoints = [(1.0, 1.0), (1.5, 2.0), (3.0, 4.0), (5.0, 7.0), (3.5, 5.0)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (5.0, 7.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 1, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Performs k-Means clustering on a given set of points and returns the final centroids.\n    \n    :param points: A list of points, each represented as a tuple of coordinates\n    :param k: The number of clusters\n    :param initial_centroids: A list of initial centroid points, each a tuple of coordinates.\n    :param max_iterations: The maximum number of iterations to perform.\n    :return: List of final centroids (coordinates rounded to the nearest fourth decimal)\n    \"\"\"\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n\n    def assign_clusters(points, centroids):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        return np.argmin(distances, axis=0)\n    for iteration in range(max_iterations):\n        closest_centroids = assign_clusters(points_array, centroids_array)\n        new_centroids = []\n        for i in range(k):\n            if points_array[closest_centroids == i].size:\n                new_centroids.append(points_array[closest_centroids == i].mean(axis=0))\n            else:\n                new_centroids.append(centroids_array[i])\n        new_centroids = np.array(new_centroids)\n        if np.all(centroids_array == new_centroids):\n            break\n        centroids_array = new_centroids\n    return [tuple(np.round(centroid, 4)) for centroid in centroids_array]\npoints = [(1, 2), (1.5, 1.8), (5, 8), (8, 8), (1, 0.6), (9, 11)]\nk = 2\ninitial_centroids = [(6.7, 13.1), (2.2, 4.3)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 2, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids_np = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_np - centroids_np[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        for j in range(k):\n            cluster_points = points_np[labels == j]\n            if len(cluster_points) > 0:\n                centroids_np[j] = np.mean(cluster_points, axis=0)\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_np]\n    return final_centroids\npoints = [(0.5, 2.5), (1.5, 5.0), (2.0, 7.0), (3.0, 1.5), (3.5, 3.5), (5.5, 2.0)]\ninitial_centroids = [(1.5, 3.5), (4.5, 3.0)]\nk = 2\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 3, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a set of points to form k clusters.\n    \n    Parameters:\n        points (list[tuple[float, float]]): List of points.\n        k (int): Number of clusters.\n        initial_centroids (list[tuple[float, float]]): Initial centroids for the clusters.\n        max_iterations (int): Maximum number of iterations to perform.\n        \n    Returns:\n        list[tuple[float, float]]: Final centroids of the clusters, rounded to 4 decimal places.\n    \"\"\"\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_np - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_indices = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_np[closest_centroid_indices == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\nk = 2"}
{"task_id": 17, "completion_id": 4, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_np - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_np[closest_centroid == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(map(lambda x: round(x, 4), centroid)) for centroid in centroids]\n    return final_centroids\npoints = [(1, 2), (1.5, 1.8), (5, 8), (8, 8), (1, 0.6), (9, 11)]\nk = 2\ninitial_centroids = [(1, 2), (8, 8)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 17, "completion_id": 5, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        for centroid in range(k):\n            centroid_points = points_array[labels == centroid]\n            if centroid_points.size:\n                centroids[centroid] = np.mean(centroid_points, axis=0)\n    centroids = np.round(centroids, decimals=4)\n    return list(map(tuple, centroids))\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 6, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        cluster_labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[cluster_labels == i].mean(axis=0) for i in range(k)])\n        if np.all(new_centroids == centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = [(lambda x: round(x, 4))(centroid) for centroid in centroids]\n    final_centroids = [tuple(rounded_centroids[i]) for i in range(k)]\n    return final_centroids\npoints = [(0.1, 0.2), (0.9, 0.8), (0.5, 0.5)]\nk = 2\ninitial_centroids = [(0.0, 0.0), (1.0, 1.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 7, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points)\n    centroids_np = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        closest_centroids = np.argmin(np.linalg.norm(points_np[:, np.newaxis] - centroids_np, axis=2), axis=1)\n        for index in range(k):\n            if sum(closest_centroids == index) > 0:\n                centroids_np[index] = np.mean(points_np[closest_centroids == index], axis=0)\n    final_centroids = [(round(centroid[0], 4), round(centroid[1], 4)) for centroid in centroids_np]\n    return final_centroids"}
{"task_id": 17, "completion_id": 8, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis] - centroids_array, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids_array == new_centroids):\n            break\n        centroids_array = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    return final_centroids"}
{"task_id": 17, "completion_id": 9, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_indices = np.argmin(distances, axis=0)\n        for i in range(k):\n            points_in_cluster = points_array[closest_centroid_indices == i]\n            if len(points_in_cluster) > 0:\n                centroids[i] = points_in_cluster.mean(axis=0)\n    final_centroids_rounded = np.round(centroids, decimals=4).tolist()\n    return final_centroids_rounded\npoints = [(1.0, 2.5), (0.3, 3.5), (1.8, 1.9), (5.0, 3.5), (6.0, 1.9)]\ninitial_centroids = [(1.0, 1.0), (4.0, 4.0)]\nk = 2\nmax_iterations = 100"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    :param X: np.ndarray of feature data\n    :param y: np.ndarray of target data\n    :param k: number of folds for the cross-validation\n    :param shuffle: Boolean indicating if the data should be shuffled\n    :param random_seed: Seed for reproducible shuffling\n    :return: List of tuples where each tuple contains train and test index arrays\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.random.permutation(n_samples) if shuffle else np.arange(n_samples)\n    n_samples_per_fold = n_samples // k\n    folds = []\n    for i in range(k):\n        test_start = i * n_samples_per_fold\n        test_end = test_start + n_samples_per_fold\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 1, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    X (np.ndarray): The input features.\n    y (np.ndarray): The target variables.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting into batches. Default is True.\n    random_seed (int): Random seed for data shuffling. Default is None.\n    \n    Returns:\n    list: A list of tuples, each containing the indices for the train and test splits.\n    \"\"\"\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of rows'\n    index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(index)\n    fold_size = X.shape[0] // k\n    train_test_splits = []\n    for i in range(k):\n        test_indices = index[i * fold_size:(i + 1) * fold_size]\n        train_indices = np.concatenate((index[:i * fold_size], index[(i + 1) * fold_size:]))\n        train_test_splits.append((train_indices, test_indices))\n    return train_test_splits"}
{"task_id": 18, "completion_id": 2, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Implementation of K-Fold Cross Validation which divides a dataset into k folds\n    and returns a list of indices for train-test splits.\n    \n    :param X: numpy array of features\n    :param y: numpy array of labels\n    :param k: number of folds (default is 5)\n    :param shuffle: boolean indicating if data should be shuffled (default is True)\n    :param random_seed: seed value for random number generator\n    :return: list of tuples, each containing train and test indices\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    indices_train_test = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        indices_train_test.append((train_indices, test_indices))\n    return indices_train_test\nX = np.random.randn(100, 3)"}
{"task_id": 18, "completion_id": 3, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits dataset into k folds for cross-validation. Each fold is a separate division of the dataset into a training\n    set and a test set, with each fold having equal size (or as close as possible) to maintain distribution.\n    \n    Parameters:\n    X (np.ndarray): Matrix of samples.\n    y (np.ndarray): Vector of target variable for samples.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the dataset before splitting. Default is True.\n    random_seed (int): Fixing a random seed for reproducibility.\n    \n    Returns:\n    split_indices (list of tuple): A list consisting of k tuples, each containing the train and test indices for that fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    assert k <= n_samples, \"The number of folds 'k' cannot be greater than the number of samples.\"\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    extra_points = n_samples % k\n    start_idx = 0\n    split_indices = []\n    for fold in range(k):\n        end_idx = start_idx + fold_size + (1 if fold < extra_points else 0)\n        test_idx = indices[start_idx:end_idx]\n        train_idx = np.concatenate((indices[:start_idx], indices[end_idx:]))\n        split_indices.append((train_idx, test_idx))\n        start_idx = end_idx\n    return split_indices"}
{"task_id": 18, "completion_id": 4, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Divides the dataset into k folds for K-Fold Cross-Validation and returns a list of \n    train-test indices for each fold.\n    \n    Parameters:\n    X (np.ndarray): The features matrix.\n    y (np.ndarray): The target variable array.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting into folds. Default is True.\n    random_seed (int): Seed for reproducible output. Default is None.\n    \n    Returns:\n    list: A list of tuples where each tuple consists of train and test indices.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    extra = X.shape[0] % k\n    start = 0\n    splits = []\n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        test_idx = indices[start:end]\n        train_idx = np.concatenate([indices[:start], indices[end:]])\n        splits.append((train_idx, test_idx))\n        start = end\n    return splits"}
{"task_id": 18, "completion_id": 5, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generates train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    - X: np.ndarray, feature dataset\n    - y: np.ndarray, labels for the dataset\n    - k: int, number of folds\n    - shuffle: bool, whether to shuffle the data before splitting\n    - random_seed: int, random seed for reproducibility\n    \n    Returns:\n    - splits: list of tuples, each tuple contains the indices of the train and test splits\n    \"\"\"\n    np.random.seed(random_seed)\n    if shuffle:\n        indices = np.random.choice(len(y), len(y), replace=False)\n        X = X[indices]\n        y = y[indices]\n    indices = np.arange(len(y))\n    fold_size = len(y) // k\n    splits = []\n    for i in range(k):\n        (start, end) = (i * fold_size, (i + 1) * fold_size)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 18, "completion_id": 6, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Divides the datasets (X and y) into k-folds and returns a list of train-test indices for each fold.\n    \n    :param X: numpy.ndarray -- feature matrix of the dataset.\n    :param y: numpy.ndarray -- target column of the dataset.\n    :param k: int -- number of folds (default is 5).\n    :param shuffle: bool -- whether to shuffle the data before splitting (default is True).\n    :param random_seed: int -- seed for random number generator for reproducibility.\n    :return: list of tuples -- each tuple contains two arrays, the first for train indices and the second for test indices.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    extra = X.shape[0] % k\n    fold_indices = []\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        fold_indices.append(indices[start:end])\n        start = end\n    splits = []\n    for i in range(k):\n        test_indices = fold_indices[i]\n        train_indices = np.concatenate(fold_indices[:i] + fold_indices[i + 1:])\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 18, "completion_id": 7, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation.\n\n    Parameters:\n    - X: np.ndarray, feature matrix.\n    - y: np.ndarray, labels.\n    - k: int, number of folds for cross-validation.\n    - shuffle: bool, whether to shuffle the dataset before splitting.\n    - random_seed: int, seed for random number generator for reproducibility.\n\n    Returns:\n    - folds: list of tuples, each tuple contains (train_indices, test_indices).\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    extra = n_samples % k\n    start = 0\n    folds = []\n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        test_indices = indices[start:end]\n        train_mask = np.ones(n_samples, dtype=bool)\n        train_mask[test_indices] = False\n        train_indices = indices[train_mask]\n        folds.append((train_indices, test_indices))\n        start = end\n    return folds\nX = np.random.rand(100, 4)\ny = np.random.choice([0, 1], size=100)\nfolds = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)"}
{"task_id": 18, "completion_id": 8, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Function to perform K-Fold Cross-Validation on dataset.\n    \n    Parameters:\n    X (np.ndarray): Features of the dataset.\n    y (np.ndarray): Target labels of the dataset.\n    k (int): Number of folds to split the dataset.\n    shuffle (bool): Whether to shuffle the dataset before splitting.\n    random_seed (int): Random seed for reproducibility.\n    \n    Returns:\n    List of tuples, where each tuple is (train_indices, test_indices) for a fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    splits = []\n    for fold in range(k):\n        start = fold * fold_size\n        end = (fold + 1) * fold_size\n        if fold == k - 1:\n            end = X.shape[0]\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        splits.append((train_indices, test_indices))\n    return splits\nX = np.random.rand(100, 2)\ny = np.random.choice([0, 1], 100)\nsplits = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)"}
{"task_id": 18, "completion_id": 9, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Implement K-Fold Cross-Validation to get indices for train and test splits.\n    \n    :param X: np.ndarray, feature dataset\n    :param y: np.ndarray, labels for the dataset\n    :param k: integer, number of folds\n    :param shuffle: Boolean, whether to shuffle data before splitting\n    :param random_seed: integer, seed for the random generator for reproducibility\n    :return: list of tuples, each tuple containing train and test indices for each fold\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    assert k > 1, 'k should be greater than 1'\n    assert n_samples > k, 'The number of samples should be greater than the number of folds'\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    indices_list = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i < k - 1 else n_samples\n        test_idx = indices[start:end]\n        train_idx = np.concatenate([indices[:start], indices[end:]])\n        indices_list.append((train_idx, test_idx))\n    return indices_list"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on the given data and return the first k principal components.\n    \n    :param data: A 2D numpy array where rows are samples and columns are features.\n    :param k: Number of principal components to return.\n    :return: A list of k principal components, each represented as a list of floats.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    top_k_eigenvectors = eigenvectors[:, :k].T\n    principal_components = [list(map(lambda x: round(x, 4), component)) for component in top_k_eigenvectors]\n    return principal_components\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [1.3, 1.6], [1, 1.1]])"}
{"task_id": 19, "completion_id": 1, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a given dataset.\n    \n    :param data: A 2D numpy array, where each row is a sample and each column is a feature.\n    :param k: Number of principal components to retain.\n    :return: A list of lists, each inner list containing one principal component (eigenvector).\n             The components are ordered by their explained variance (descending).\n    \"\"\"\n    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_matrix = np.cov(data_standardized, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    pca_result = [[round(val, 4) for val in eigenvector] for eigenvector in principal_components.T]\n    return pca_result"}
{"task_id": 19, "completion_id": 2, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data and return the first k principal components.\n    \n    :param data: A 2D NumPy array where each row is a sample and each column is a feature.\n    :param k: Number of principal components to return.\n    :return: A list of k principal components (each component is represented as a list of floats).\n    \"\"\"\n    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data_standardized, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    principal_components = eigenvectors[:, :k].T\n    principal_components = [list(map(lambda x: round(x, 4), vec)) for vec in principal_components]\n    return principal_components"}
{"task_id": 19, "completion_id": 3, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row represents a sample and each column a feature.\n    - k: Number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the k principal components (eigenvectors corresponding to the k largest eigenvalues).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    return [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]"}
{"task_id": 19, "completion_id": 4, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the input data and return the top k principal components.\n    \n    :param data: A 2D NumPy array where each row represents a data sample, and each column represents a feature.\n    :param k: Number of principal components to return.\n    :return: A list of lists where each list is a principal component (a list of floats).\n             All elements in the principal components are rounded to 4 decimal places.\n    \"\"\"\n    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data_standardized, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    sorted_eigenvectors = eigenvectors[:, idx]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, decimals=4).tolist()\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 5, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform PCA on the given data and return the top k principal components.\n    Args:\n    data: A 2D NumPy array, where each row represents a data sample and each column is a feature.\n    k: The number of principal components to return.\n\n    Returns:\n    A list of lists, where each inner list represents a principal component (eigenvector). Each principal component is a list of floats (rounded to 4 decimal places).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, decimals=4).tolist()\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 6, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a given dataset.\n    \n    :param data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    :param k: The number of principal components to return.\n    :return: A list of lists where each sub-list represents a principal component (eigenvector).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.real(principal_components).tolist()\n    for i in range(len(principal_components)):\n        for j in range(len(principal_components[i])):\n            principal_components[i][j] = round(principal_components[i][j], 4)\n    return principal_components\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\nk = 2"}
{"task_id": 19, "completion_id": 7, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = [list(np.round(principal_components[:, i], 4)) for i in range(k)]\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 8, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset.\n    \n    Parameters:\n    data (np.ndarray): A 2D array where each row is a sample and each column is a feature.\n    k (int): Number of principal components to return.\n    \n    Returns:\n    list[list[float]]: A list of principal components as eigenvectors corresponding to the largest eigenvalues.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    return [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]"}
{"task_id": 19, "completion_id": 9, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    cov_matrix = np.cov(standardized_data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    principal_components = eigenvectors.T[sorted_indices[:k]]\n    principal_components = principal_components.round(4)\n    return principal_components.tolist()\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\nk = 2"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree from a list of examples and a list of attributes.\n\n    :param examples: A list of examples to build the tree on. Each example is a dictionary of attribute-value pairs.\n    :param attributes: A list of attribute names to consider for splitting.\n    :param target_attr: The attribute name that indicates the target class label.\n    :return: A dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples, target_attr):\n        \"\"\"Calculate the entropy of a list of examples based on the target attribute.\"\"\"\n        counts = Counter((example[target_attr] for example in examples))\n        total = len(examples)\n        return -sum((count / total * math.log2(count / total) for count in counts.values()))\n\n    def information_gain(before_split, after_split, target_attr):\n        \"\"\"\n        Calculate the information gain based on the before and after split entropies.\n\n        :param before_split: Examples before the split.\n        :param after_split: A list of tuples, each containing a split criteria and examples.\n        :param target_attr: The target attribute to check the gain for.\n        :return: The information gain.\n        \"\"\"\n        before_entropy = entropy(before_split, target_attr)\n        total = len(before_split)\n        after_entropy = sum((len(split_examples) / total * entropy(split_examples, target_attr) for (_, split_examples) in after_split))\n        return before_entropy - after_entropy\n\n    def split_on_attribute(examples, attribute):\n        \"\"\"\n        Split examples based on all values of a given attribute.\n\n        :param examples: The examples to split upon.\n        :param attribute: The attribute to split on.\n        :return: A dictionary where keys are attribute values and values are lists of examples split by those values.\n        \"\"\"\n        splits = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in splits:\n                splits[value] = []\n            splits[value].append(example)\n        return splits\n\n    def majority_value(examples, target_attr):\n        \"\"\"Compute the most frequent target attribute value in examples.\"\"\"\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n\n    def same_classification(examples, target_attr):\n        \"\"\"Check if all examples have the same classification.\"\"\"\n        target_values = [example[target_attr] for example in examples]\n        return len(set(target_values)) == 1\n    if not examples:\n        return majority_value(examples[:-1], target_attr)\n    if same_classification(examples, target_attr):\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attribute = max((attr for attr in attributes if attr != target_attr), key=lambda attr: information_gain(examples, split_on_attribute(examples, attr).items(), target_attr))\n    tree = {best_attribute: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    for (value, subset) in split_on_attribute(examples, best_attribute).items():\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 1, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Constructs a decision tree from a list of examples and a list of attributes.\n    \n    :param examples: List of examples where each example is a dict of attribute-value pairs.\n    :param attributes: List of attributes to consider for splitting.\n    :param target_attr: The target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples):\n        \"\"\"Compute the entropy of the given examples.\"\"\"\n        target_values = [example[target_attr] for example in examples]\n        count = Counter(target_values)\n        return -sum((freq / len(examples) * math.log2(freq / len(examples)) for freq in count.values()))\n\n    def information_gain(examples, attr):\n        \"\"\"Compute the information gain for splitting on a specific attribute.\"\"\"\n\n        def split_entropy(attr_values, examples):\n            groups = [g for (_, g) in examples.groupby(attr_values)]\n            return sum((len(group) / len(examples) * entropy(group.to_dict('records')) for group in groups))\n        attr_values = set([example[attr] for example in examples])\n        return entropy(examples) - split_entropy(attr_values, examples)\n\n    def majority_value(examples, target_attr):\n        \"\"\"Return the majority value of the target attribute from the example set.\"\"\"\n        target_values = [example[target_attr] for example in examples]\n        return Counter(target_values).most_common(1)[0][0]\n    if not examples:\n        return majority_value(examples, target_attr)\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr))\n    tree = {best_attr: {}}\n    for v in set([example[best_attr] for example in examples]):\n        subtree_examples = [example for example in examples if example[best_attr] == v]\n        subtree = learn_decision_tree(subtree_examples, [attr for attr in attributes if attr != best_attr], target_attr)\n        tree[best_attr][v] = subtree\n    return tree\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'strong', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 20, "completion_id": 2, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using the given examples and attributes.\n    \n    :param examples: List of dictionaries, where each dictionary represents an example\n                    with attribute-value pairs.\n    :param attributes: List of attribute names to consider in the tree construction.\n    :param target_attr: The target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def calculate_entropy(examples, target_attr):\n        label_counts = Counter([example[target_attr] for example in examples])\n        entropy = -sum((count / len(examples) * math.log2(count / len(examples)) for count in label_counts.values()))\n        return entropy\n\n    def calculate_information_gain(examples, attribute, target_attr):\n        total_entropy = calculate_entropy(examples, target_attr)\n        attribute_values = list(set([example[attribute] for example in examples]))\n        weighted_entropy = 0\n        for value in attribute_values:\n            examples_for_value = [example for example in examples if example[attribute] == value]\n            fraction = len(examples_for_value) / len(examples)\n            weighted_entropy += fraction * calculate_entropy(examples_for_value, target_attr)\n        return total_entropy - weighted_entropy\n    if not examples or not attributes:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    elif len(Counter([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    else:\n        best_attribute = max(attributes, key=lambda attr: calculate_information_gain(examples, attr, target_attr))\n        tree = {best_attribute: {}}\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        for value in set([example[best_attribute] for example in examples]):\n            examples_for_value = [example for example in examples if example[best_attribute] == value]\n            subtree = learn_decision_tree(examples_for_value, remaining_attributes, target_attr)\n            tree[best_attribute][value] = subtree\n        return tree\nexamples = [{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Weak', 'Play': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Strong', 'Play': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Strong', 'Play': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Strong', 'Play': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Weak', 'Play': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Windy': 'Strong', 'Play': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Strong', 'Play': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Windy': 'Weak', 'Play': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Strong', 'Play': 'No'}]\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Windy']\ntarget_attr = 'Play'"}
{"task_id": 20, "completion_id": 3, "solution": "import math\nfrom collections import Counter\ndef entropy(examples, target_attr):\n    \"\"\" \n    Calculate the entropy of the examples for the target attribute.\n    \"\"\"\n    total_count = len(examples)\n    if total_count == 0:\n        return 0.0\n    target_counts = Counter([example[target_attr] for example in examples])\n    probabilities = [count / total_count for count in target_counts.values()]\n    ent = -sum((prob * math.log(prob, 2) for prob in probabilities))\n    return ent\ndef information_gain(examples, split_attribute, target_attr):\n    \"\"\"\n    Calculate the information gain (reduction in entropy) by splitting the examples according to the attr.\n    \"\"\"\n    parent_entropy = entropy(examples, target_attr)\n    attr_values = set([example[split_attribute] for example in examples])\n    new_entropy = 0.0\n    for value in attr_values:\n        examples_subset = [example for example in examples if example[split_attribute] == value]\n        prob = len(examples_subset) / len(examples)\n        new_entropy += prob * entropy(examples_subset, target_attr)\n    ig = parent_entropy - new_entropy\n    return ig\ndef best_split_attribute(examples, attributes, target_attr):\n    \"\"\"\n    Select the attribute with the highest information gain.\n    \"\"\"\n    max_info_gain = -math.inf\n    best_attribute = None\n    for attribute in attributes:\n        ig = information_gain(examples, attribute, target_attr)\n        if ig > max_info_gain:\n            max_info_gain = ig\n            best_attribute = attribute\n    return best_attribute\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Implement the decision tree learning algorithm that returns a nested dictionary representing the decision tree.\n    \"\"\"\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if len(attributes) == 0:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    attr = best_split_attribute(examples, attributes, target_attr)\n    tree = {attr: {}}\n    values = set([example[attr] for example in examples])\n    for value in values:\n        examples_subset = [example for example in examples if example[attr] == value]\n        subtree = learn_decision_tree(examples_subset, [a for a in attributes if a != attr], target_attr)\n        tree[attr][value] = subtree\n    return tree\nexamples = [{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Windy': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Windy': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'Strong', 'PlayTennis': 'No'}]\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Windy']\ntarget_attr = 'PlayTennis'"}
{"task_id": 20, "completion_id": 4, "solution": "import math\nfrom collections import Counter\ndef entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the target attribute.\"\"\"\n    counts = Counter([example[target_attr] for example in examples])\n    total = sum(counts.values())\n    return -sum((count / total * math.log2(count / total) for count in counts.values()))\ndef info_gain(examples, attribute, target_attr):\n    \"\"\"Calculate the information gain of the specified attribute.\"\"\"\n    unique_values = set((example[attribute] for example in examples))\n    weighted_entropy = 0\n    for value in unique_values:\n        filtered_examples = [example for example in examples if example[attribute] == value]\n        weighted_entropy += len(filtered_examples) / len(examples) * entropy(filtered_examples, target_attr)\n    return entropy(examples, target_attr) - weighted_entropy\ndef most_common_value(examples, target_attr):\n    \"\"\"Find the most common value for the target attribute.\"\"\"\n    counts = Counter([example[target_attr] for example in examples])\n    return counts.most_common(1)[0][0]\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"Learn a decision tree using a recursive splitting approach based on entropy and information gain.\"\"\"\n    if len(set((example[target_attr] for example in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_value(examples, target_attr)\n    best_attribute = max(attributes, key=lambda attr: info_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    for value in set((example[best_attribute] for example in examples)):\n        filtered_examples = [example for example in examples if example[best_attribute] == value]\n        subtree = learn_decision_tree(filtered_examples, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'weather': 'sunny', 'temp': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'weather': 'sunny', 'temp': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}, {'weather': 'overcast', 'temp': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'weather': 'rainy', 'temp': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'weather': 'rainy', 'temp': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'weather': 'rainy', 'temp': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'}, {'weather': 'overcast', 'temp': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'weather': 'sunny', 'temp': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'weather': 'sunny', 'temp': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'weather': 'rainy', 'temp': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'weather': 'sunny', 'temp': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'weather': 'overcast', 'temp': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'}, {'weather': 'overcast', 'temp': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'weather': 'rainy', 'temp': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}]\nattributes = ['weather', 'temp', 'humidity', 'wind']"}
{"task_id": 20, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if not examples:\n        return plurality_value(None)\n    labels = [example[target_attr] for example in examples]\n    if len(set(labels)) == 1:\n        return labels[0]\n    if not attributes:\n        return plurality_value(examples)\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for value in set([example[best_attr] for example in examples]):\n        sub_examples = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef choose_best_attribute(examples, attributes, target_attr):\n    classes = [example[target_attr] for example in examples]\n    entropy = calculate_entropy(Counter(classes))\n    best_info_gain = -math.inf\n    best_attr = None\n    for attribute in attributes:\n        info_gain = entropy - remainder(examples, attribute, target_attr)\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_attr = attribute\n    return best_attr\ndef calculate_entropy(class_counts: Counter) -> float:\n    \"\"\"Calculate entropy based on class counts.\"\"\"\n    total = sum(class_counts.values())\n    entropy = -sum((count / total * math.log2(count / total) for count in class_counts.values()))\n    return entropy\ndef remainder(examples, attribute, target_attr):\n    remainder_value = 0\n    for value in set([example[attribute] for example in examples]):\n        sub_examples = [example for example in examples if example[attribute] == value]\n        sub_examples_count = len(sub_examples)\n        total_count = len(examples)\n        sub_classes = [example[target_attr] for example in sub_examples]\n        remainder_value += sub_examples_count / total_count * calculate_entropy(Counter(sub_classes))\n    return remainder_value\ndef plurality_value(examples):\n    if examples is None:\n        return None\n    labels = [example[-1] for example in examples]\n    return max(set(labels), key=labels.count)\nexamples = [{'age': 'young', 'income': 'high', 'student': 'no', 'credit_rating': 'fair', 'buys_computer': 'no'}, {'age': 'young', 'income': 'high', 'student': 'no', 'credit_rating': 'excellent', 'buys_computer': 'no'}, {'age': 'middle_aged', 'income': 'medium', 'student': 'yes', 'credit_rating': 'fair', 'buys_computer': 'yes'}, {'age': 'old', 'income': 'low', 'student': 'yes', 'credit_rating': 'fair', 'buys_computer': 'yes'}]\nattributes = ['age', 'income', 'student', 'credit_rating']\ntarget_attr = 'buys_computer'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 20, "completion_id": 6, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if not examples:\n        return {}\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attr: {}}\n    for value in set([example[best_attr] for example in examples]):\n        subtree_examples = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(subtree_examples, [attr for attr in attributes if attr != best_attr], target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    total_examples = len(examples)\n    class_counts = Counter([example[target_attr] for example in examples])\n    entropy_val = 0.0\n    for count in class_counts.values():\n        probability = count / total_examples\n        entropy_val -= probability * math.log2(probability)\n    return entropy_val\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    total_entropy = entropy(examples, target_attr)\n    attr_entropy = 0.0\n    for value in set([example[attr] for example in examples]):\n        subset = [example for example in examples if example[attr] == value]\n        probability = len(subset) / len(examples)\n        attr_entropy += probability * entropy(subset, target_attr)\n    return total_entropy - attr_entropy"}
{"task_id": 20, "completion_id": 7, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using the ID3 algorithm based on information gain.\n    \n    :param examples: List of instances containing attribute-value pairs.\n    :param attributes: List of attribute names.\n    :param target_attr: The name of the target attribute.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if len(attributes) == 0:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n\n    def entropy(examples):\n        target_values = [example[target_attr] for example in examples]\n        value_counts = Counter(target_values)\n        probabilities = [count / len(examples) for count in value_counts.values()]\n        return -sum((p * math.log2(p) for p in probabilities))\n\n    def information_gain(examples, attribute):\n        total_entropy = entropy(examples)\n        values = set([example[attribute] for example in examples])\n        weighted_entropy = sum((len([example for example in examples if example[attribute] == value]) / len(examples) * entropy([example for example in examples if example[attribute] == value]) for value in values))\n        return total_entropy - weighted_entropy\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr))\n    tree = {best_attribute: {}}\n    for value in set([example[best_attribute] for example in examples]):\n        partition = [example for example in examples if example[best_attribute] == value]\n        subtree = learn_decision_tree(partition, [attr for attr in attributes if attr != best_attribute], target_attr)\n        tree[best_attribute][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 8, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Dict\ndef learn_decision_tree(examples: List[Dict[str, str]], attributes: List[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree based on the provided examples and attributes.\n    \n    :param examples: A list of examples, where each example is a dictionary of attribute-value pairs.\n    :param attributes: A list of attribute names.\n    :param target_attr: The name of the target attribute (class label).\n    :return: A dictionary representing the decision tree.\n    \"\"\"\n    classifications = [example[target_attr] for example in examples]\n    if len(set(classifications)) == 1:\n        return classifications[0]\n    if not attributes:\n        return Counter(classifications).most_common(1)[0][0]\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    del attributes[attributes.index(best_attr)]\n    for val in set([example[best_attr] for example in examples]):\n        new_examples = [example for example in examples if example[best_attr] == val]\n        if not new_examples:\n            tree[best_attr][val] = Counter(classifications).most_common(1)[0][0]\n        else:\n            tree[best_attr][val] = learn_decision_tree(new_examples, attributes[:], target_attr)\n    return tree\ndef choose_best_attribute(examples: List[Dict[str, str]], attributes: List[str], target_attr: str) -> str:\n    \"\"\"\n    Chooses the attribute with the highest information gain.\n    \n    :param examples: A list of examples.\n    :param attributes: A list of attributes.\n    :param target_attr: Target attribute (class label).\n    :return: The attribute with the highest information gain.\n    \"\"\"\n    base_entropy = calculate_entropy(examples, target_attr)\n    max_info_gain = -1\n    best_attr = None\n    for attr in attributes:\n        gain = base_entropy - calculate_weighted_entropy(examples, attr, target_attr)\n        if gain > max_info_gain:\n            max_info_gain = gain\n            best_attr = attr\n    return best_attr\ndef calculate_entropy(examples: List[Dict[str, str]], target_attr: str) -> float:\n    \"\"\"\n    Calculates the entropy of a set of examples.\n    \n    :param examples: A list of examples.\n    :param target_attr: Target attribute.\n    :return: The entropy of the examples.\n    \"\"\"\n    classifications = [example[target_attr] for example in examples]\n    class_counts = Counter(classifications)\n    entropy = 0\n    for count in class_counts.values():\n        p = count / len(classifications)\n        entropy -= p * math.log2(p)\n    return entropy\ndef calculate_weighted_entropy(examples: List[Dict[str, str]], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the weighted entropy of a set of examples based on an attribute.\n    \n    :param examples: A list of examples.\n    :param attr: The attribute.\n    :param target_attr: Target attribute.\n    :return: The weighted entropy of the examples with respect to the attribute.\n    \"\"\"\n    weighted_entropy = 0\n    for value in set([example[attr] for example in examples]):\n        new_examples = [example for example in examples if example[attr] == value]\n        prop = len(new_examples) / len(examples)\n        weighted_entropy += prop * calculate_entropy(new_examples, target_attr)\n    return weighted_entropy"}
{"task_id": 20, "completion_id": 9, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Dict\ndef learn_decision_tree(examples: List[Dict[str, any]], attributes: List[str], target_attr: str) -> Dict[str, any]:\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return Counter(target_values).most_common(1)[0][0]\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for value in set((example[best_attr] for example in examples)):\n        subset = [example for example in examples if example[best_attr] == value]\n        if not subset:\n            tree[best_attr][value] = Counter(target_values).most_common(1)[0][0]\n        else:\n            tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n    return tree\ndef choose_best_attribute(examples: List[Dict[str, any]], attributes: List[str], target_attr: str) -> str:\n    base_entropy = calculate_entropy(examples, target_attr)\n    information_gains = {attr: base_entropy - calculate_entropy_after_split(examples, attr, target_attr) for attr in attributes}\n    return max(information_gains, key=information_gains.get)\ndef calculate_entropy(examples, target_attr):\n    return -sum((count / len(examples) * math.log2(count / len(examples)) for count in Counter([example[target_attr] for example in examples]).values()))\ndef calculate_entropy_after_split(examples, attribute, target_attr):\n    \"\"\"Calculate the weighted entropy after splitting by the attribute.\"\"\"\n    entropy_after_split = 0\n    for value in set((example[attribute] for example in examples)):\n        subset = [example for example in examples if example[attribute] == value]\n        subset_entropy = calculate_entropy(subset, target_attr)\n        weight = len(subset) / len(examples)\n        entropy_after_split += weight * subset_entropy\n    return entropy_after_split\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': False, 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play': 'yes'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_matrix(X, kernel='linear', sigma=1.0):\n    \"\"\"\n    Computes the kernel matrix for the data.\n    :param X: Training data set (n_samples, n_features).\n    :param kernel: Type of kernel function (linear or RBF).\n    :param sigma: Parameter for RBF kernel.\n    :return: Kernel matrix (np.ndarray).\n    \"\"\"\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    if kernel == 'linear':\n        K = np.dot(X, X.T)\n    elif kernel == 'RBF':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = np.exp(-np.linalg.norm(X[i] - X[j]) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError('Kernel {} not yet implemented.'.format(kernel))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements the deterministic version of the Pegasos algorithm for training a kernel SVM.\n    :param data: The training dataset (n_samples, n_features).\n    :param labels: Labels for the training dataset (n_samples,).\n    :param kernel: Type of kernel function (linear or RBF).\n    :param lambda_val: Regularization parameter.\n    :param iterations: Number of training iterations.\n    :param sigma: Kernel parameter for RBF kernel.\n    :return: Alpha coefficients and bias term.\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        eta_t = 1 / (lambda_val * t)\n        scores = np.zeros(n_samples)\n        for i in range(n_samples):\n            scores[i] = np.sum(alphas * labels * K[:, i]) + b\n        margins = labels * scores\n        for i in range(n_samples):\n            if margins[i] <= 1:\n                alphas[i] = (1 - eta_t * lambda_val) * alphas[i] + eta_t * labels[i]\n            else:\n                alphas[i] = (1 - eta_t * lambda_val) * alphas[i]\n        active_samples = (labels * scores <= 1).flatten()\n        if np.any(active_samples):\n            b = b + eta_t * (labels[active_samples] - scores[active_samples])\n    alphas = np.round(alphas, 4)\n    b = np.round(b, 4)\n    return (alphas.tolist(), b.tolist())"}
{"task_id": 21, "completion_id": 1, "solution": "import numpy as np\ndef kernel_matrix(X, kernel='linear', sigma=1.0):\n    \"\"\"Compute the kernel matrix\"\"\"\n    n_samples = X.shape[0]\n    if kernel == 'linear':\n        return X @ X.T\n    elif kernel == 'rbf':\n        pairwise_dists = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(X ** 2, axis=1).reshape(1, -1) - 2 * np.dot(X, X.T)\n        return np.exp(-pairwise_dists / (2 * sigma ** 2))\n    else:\n        raise ValueError('Kernel type not recognized.')\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"Train a SVM classifier using the Pegasos (deterministic version) with a given kernel\"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    K = kernel_matrix(data, kernel, sigma)\n    t = 0\n    for epoch in range(iterations):\n        eta = 1 / (lambda_val * (t + 1))\n        for i in range(n_samples):\n            pred = np.sum(alphas * labels * K[i]) + 10 ** (-12)\n            if labels[i] * pred < 1:\n                alphas[i] = (1 - eta * lambda_val) * alphas[i] + eta * labels[i]\n            else:\n                alphas[i] = (1 - eta * lambda_val) * alphas[i]\n        t += 1\n    sv_indices = np.nonzero(alphas)[0]\n    b = 0\n    for i in sv_indices:\n        b += labels[i] - np.sum(alphas * labels * K[sv_indices, sv_indices][i])\n    b /= len(sv_indices)\n    alphas = np.round(alphas, decimals=4)\n    b = np.round(b, decimals=4)\n    return (alphas.tolist(), b.tolist())\ndata = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([-1, -1, -1, 1])"}
{"task_id": 21, "completion_id": 2, "solution": "import numpy as np\ndef kernel_linear(x_i, x_j):\n    \"\"\"Linear kernel function.\"\"\"\n    return np.dot(x_i, x_j)\ndef kernel_rbf(x_i, x_j, sigma=1.0):\n    \"\"\"RBF kernel function.\"\"\"\n    n = len(x_i)\n    gamma = 1 / (2 * sigma ** 2)\n    return np.exp(-gamma * np.sum((x_i - x_j) ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implementation of the Pegasos Kernel SVM algorithm with a fork towards the deterministic version.\n    Args:\n        data (np.ndarray): The training data samples.\n        labels (np.ndarray): The training labels.\n        kernel (str): The type of kernel to use ('linear' or 'rbf').\n        lambda_val (float): The regularization parameter.\n        iterations (int): The number of training iterations.\n        sigma (float): The bandwidth for the RBF kernel.\n\n    Returns:\n        alpha (list): The alpha coefficients of the SVM model.\n        b (float): The bias term of the SVM model.\n    \"\"\"\n    (N, d) = data.shape\n    alpha = np.zeros(N)\n    if kernel == 'linear':\n        kernel_func = kernel_linear\n    elif kernel == 'rbf':\n        kernel_func = lambda x, y: kernel_rbf(x, y, sigma)\n    else:\n        raise ValueError('Unsupported kernel function.')\n    omega = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            omega[i, j] = labels[i] * labels[j] * kernel_func(data[i], data[j])\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        alpha = (1 - learning_rate * lambda_val) * alpha\n        for i in range(N):\n            product = np.sum(alpha * labels * omega[:, i])\n            if labels[i] * product < 1:\n                eta_derivative = labels[i] * omega[i, i]\n                alpha[i] += learning_rate * eta_derivative\n    y_pred = np.zeros(N)\n    for i in range(N):\n        y_pred[i] = np.sum(labels * alpha * omega[:, i])\n        y_pred[i] += b\n    b = np.mean(labels - y_pred)\n    return (alpha.round(4).tolist(), round(b, 4))\ndata = np.random.randn(10, 5)\nlabels = np.random.choice([-1, 1], size=10)"}
{"task_id": 21, "completion_id": 3, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    kernel_matrix = np.zeros((data.shape[0], data.shape[0]))\n    if kernel == 'linear':\n        kernel_matrix = data @ data.T\n    elif kernel == 'rbf':\n        for i in range(data.shape[0]):\n            for j in range(data.shape[0]):\n                diff = data[i] - data[j]\n                kernel_matrix[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\n    return kernel_matrix\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n, d) = data.shape\n    alpha = np.zeros(n)\n    K = kernel_matrix(data, kernel, sigma=sigma)\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        scores = K @ (labels * alpha)\n        for i in range(n):\n            if labels[i] * scores[i] < 1:\n                alpha[i] += 1\n        alpha = alpha * min(1.0, 1.0 / (lambda_val * (np.sqrt(np.sum(alpha ** 2) * (K @ K).trace()) + 1e-08)))\n    bias = compute_bias(alpha, labels, K, lambda_val)\n    return (alpha.tolist(), round(float(bias), 4))\ndef compute_bias(alpha, labels, K, lambda_val):\n    indices = np.where(alpha > 0)[0]\n    bias = 0\n    if indices.size > 0:\n        for i in indices:\n            bias += labels[i] - np.sum(alpha * labels * K[:, i])\n        bias /= indices.size\n    return bias / (1 - lambda_val)\ndata = np.array([[1, 2], [2, 3], [2, 1], [3, 2], [2, 2]])\nlabels = np.array([1, 1, 1, -1, -1])"}
{"task_id": 21, "completion_id": 4, "solution": "import numpy as np\ndef kernel_matrix(X, Y, kernel='linear', sigma=1.0):\n    (n, m) = (X.shape[0], Y.shape[0])\n    K = np.zeros((n, m))\n    if kernel == 'linear':\n        K = np.dot(X, Y.T)\n    elif kernel == 'rbf':\n        for i in range(n):\n            for j in range(m):\n                K[i, j] = np.exp(-np.linalg.norm(X[i] - Y[j]) ** 2 / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n = data.shape[0]\n    K = kernel_matrix(data, data, kernel=kernel, sigma=sigma)\n    alpha = np.zeros(n)\n    for t in range(1, iterations + 1):\n        step_t = 1 / (lambda_val * t)\n        for i in range(n):\n            if labels[i] * np.sum(alpha * labels * K[:, i]) <= 1:\n                alpha[i] = alpha[i] + step_t * labels[i] * (1 - labels[i] * np.sum(alpha * labels * K[:, i]))\n            else:\n                alpha[i] = (1 - step_t * lambda_val) * alpha[i]\n    alpha_rounded = np.round(alpha, decimals=4)\n    sv_indices = np.argwhere(alpha_rounded > 1e-05).flatten()\n    sv_alpha = alpha_rounded[sv_indices]\n    sv_labels = labels[sv_indices]\n    K_sv = K[sv_indices, :][:, sv_indices]\n    bias = np.round(sv_labels - np.sum(sv_alpha * sv_labels * K_sv, axis=1), decimals=4)\n    bias = np.average(bias)\n    return {'alpha': alpha_rounded.tolist(), 'bias': np.round(bias, decimals=4)}"}
{"task_id": 21, "completion_id": 5, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    \"\"\"Compute the kernel matrix for the given data.\"\"\"\n    (n, _) = data.shape\n    K = np.zeros((n, n))\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'rbf':\n        distances = np.sum(data ** 2, axis=1).reshape(-1, 1) + np.sum(data ** 2, axis=1) - 2 * (data @ data.T)\n        K = np.exp(-distances / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    b = 0\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        lambda_inverse = 1 / (lambda_val * t)\n        eta = lambda_inverse\n        w_norm = np.sqrt(np.sum((alphas.reshape(-1, 1) * labels) ** 2 * K))\n        for i in range(n_samples):\n            pred = (labels * alphas).T @ K[:, i] + b\n            condition = labels[i] * pred < 1\n            alphas[i] = (1 - lambda_inverse * lambda_val) * alphas[i] + lambda_inverse * condition * labels[i]\n        alphas = np.minimum(lambda_inverse * ((1 - b * labels) / w_norm), 1 / lambda_val)\n        b = (labels - (labels * alphas).T @ K).mean()\n    return ([round(num, 4) for num in alphas.tolist()], round(b, 4))"}
{"task_id": 21, "completion_id": 6, "solution": "import numpy as np\ndef kernel_matrix(data: np.ndarray, kernel='linear', sigma=1.0):\n    \"\"\"\n    Computes the kernel matrix for the input data.\n    :param data: 2D NumPy array where each row is a sample and each column is a feature.\n    :param kernel: String specifying the kernel type ('linear' or 'RBF').\n    :param sigma: Float specifying the standard deviation for the RBF kernel.\n    :return: 2D NumPy array representing the kernel matrix.\n    \"\"\"\n    n_samples = data.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'RBF':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = np.linalg.norm(data[i] - data[j])\n                K[i, j] = np.exp(-diff ** 2 / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implementation of a deterministic version of the Pegasos algorithm to train a kernel SVM classifier.\n    :param data: 2D NumPy array, input features.\n    :param labels: 1D NumPy array, labels for the input data.\n    :param kernel: String, type of kernel to use ('linear' or 'RBF').\n    :param lambda_val: Float, regularization parameter.\n    :param iterations: Int, number of training iterations.\n    :param sigma: Float, standard deviation for the RBF kernel.\n    :return: Tuple containing alpha coefficients and bias.\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    K = kernel_matrix(data, kernel=kernel, sigma=sigma)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        learning_rate = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            pred = np.sum(alphas * labels * K[:, i]) + b\n            if labels[i] * pred < 1:\n                alphas[i] = (1 - learning_rate * lambda_val) * alphas[i] + learning_rate * labels[i]\n                b += learning_rate * (1 - labels[i] * pred)\n            else:\n                alphas[i] = (1 - learning_rate * lambda_val) * alphas[i]\n        b = 0.0\n    alphas_rounded = [round(alpha, 4) for alpha in alphas]\n    return (alphas_rounded, round(b, 4))\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\nlabels = np.array([1, 1, -1, -1])"}
{"task_id": 21, "completion_id": 7, "solution": "import numpy as np\ndef kernel_matrix_linear(X):\n    return np.dot(X, X.T)\ndef kernel_matrix_rbf(X, sigma):\n    dists = np.sum(X ** 2, axis=1).reshape(-1, 1) + np.sum(X ** 2, axis=1) - 2 * np.dot(X, X.T)\n    return np.exp(-dists / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic version of the Pegasos algorithm for a kernel SVM classifier.\n    \n    Parameters:\n    data (np.ndarray): The dataset as a 2D NumPy array.\n    labels (np.ndarray): The labels as a 1D NumPy array.\n    kernel (str): Type of kernel ('linear' or 'rbf').\n    lambda_val (float): Regularization parameter.\n    iterations (int): Number of iterations.\n    sigma (float): RBF kernel parameter.\n    \n    Returns:\n    alpha (list): List of alpha coefficients.\n    bias (float): Bias term rounded to 4 decimal places.\n    \"\"\"\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    K = kernel_matrix_linear(data) if kernel == 'linear' else kernel_matrix_rbf(data, sigma)\n    for t in range(1, iterations + 1):\n        eta_t = 1 / (lambda_val * t)\n        updated_alpha = alpha.copy()\n        for j in range(n_samples):\n            s = labels[j] * (np.sum([updated_alpha[i] * labels[i] * K[i, j] for i in range(n_samples)]) + bias)\n            if s <= 1:\n                updated_alpha[j] = (1 - eta_t * lambda_val) * alpha[j] + eta_t * labels[j]\n                bias = bias + eta_t * (1 - s)\n            else:\n                updated_alpha[j] = (1 - eta_t * lambda_val) * alpha[j]\n        alpha = updated_alpha\n    bias = round(bias, 4)\n    alpha = alpha.tolist()\n    return (alpha, bias)\ndata = np.array([[1, 2], [2, 3], [3, 1], [-2, -1], [-3, -1], [-1, -1]])\nlabels = np.array([1, 1, 1, -1, -1, -1])"}
{"task_id": 21, "completion_id": 8, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    \"\"\"Compute the kernel matrix.\"\"\"\n    n = data.shape[0]\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        K = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel. Choose 'linear' or 'rbf'.\")\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"Deterministic Pegasos kernel SVM implementation.\"\"\"\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        predictions = np.array([np.sum(alpha * labels * K[i]) for i in range(n_samples)])\n        support_vectors = labels * predictions < 1\n        alpha -= learning_rate * lambda_val * alpha\n        alpha[support_vectors] += learning_rate * labels[support_vectors]\n    bias = 0\n    support_indices = np.nonzero(alpha)[0]\n    if support_indices.size > 0:\n        bias = np.mean([1.0 / labels[i] - np.sum(alpha * labels * K[i, :]) for i in support_indices])\n    return (list(np.around(alpha, decimals=4).tolist()), round(bias, 4))"}
{"task_id": 21, "completion_id": 9, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    n = data.shape[0]\n    K = np.zeros((n, n))\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        for i in range(n):\n            for j in range(i, n):\n                K[i, j] = K[j, i] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n = data.shape[0]\n    K = kernel_matrix(data, kernel=kernel, sigma=sigma)\n    alphas = np.zeros(n)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1 / (lambda_val * t)\n        y_pred = np.dot(alphas * labels, K) + b\n        misclassified = labels * y_pred < 1\n        alphas = (1 - eta * lambda_val) * alphas\n        alphas[misclassified] += eta * labels[misclassified]\n        b += eta * (1 - y_pred[misclassified]) / np.sum(misclassified).clip(min=1)\n    alphas_rounded = np.round(alphas, 4)\n    return (alphas_rounded.tolist(), float(np.round(b, 4)))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Calculate the sigmoid of a given input z.\n\n    Args:\n    z (float): The input value to the sigmoid function.\n\n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 1, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    :param z: A float representing the input value to the sigmoid function.\n    :return: The computed sigmoid value, rounded to four decimal places.\n    \"\"\"\n    value = 1 / (1 + math.exp(-z))\n    return round(value, 4)"}
{"task_id": 22, "completion_id": 2, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function given an input value z.\n    The sigmoid function is defined as: f(z) = 1 / (1 + e^(-z))\n    This function returns the output rounded to four decimal places.\n    \n    :param z: The input value for the function.\n    :return: The sigmoid function output rounded to four decimal places.\n    \"\"\"\n    sig_out = 1 / (1 + math.exp(-z))\n    return round(sig_out, 4)"}
{"task_id": 22, "completion_id": 3, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of a given input z.\n\n    The sigmoid function is defined as 1 / (1 + e^-z) where e is the base \n    of the natural logarithm. This function returns the output rounded to \n    four decimal places.\n    \n    Parameters:\n    z (float): The input value for the sigmoid function.\n    \n    Returns:\n    float: The sigmoid of z rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 4, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid of z.\n\n    :param z: A floating-point number as input for the sigmoid function.\n    :return: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 5, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n\n    Parameters:\n    - z (float): The input to the sigmoid function.\n\n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 6, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of a given input z.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function rounded to 4 decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 7, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid function for a given input z.\n    The sigmoid function is defined as 1 / (1 + e^(-z)).\n    :param z: input value to the sigmoid function.\n    :return: the output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 22, "completion_id": 8, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input value z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 9, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input value z.\n    :param z: float, the input value for the sigmoid function.\n    :return: float, the output value of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of the given list of scores.\n    \n    The softmax function is defined as:\n    softmax(x_i) = exp(x_i) / sum(exp(x_j)) for all j\n    \n    Parameters:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: A list containing the softmax values of the input scores rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 1, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for the given list of scores.\n    \n    Parameters:\n    - scores (list of float): A list of scores.\n    \n    Returns:\n    - list of float: A list of softmax values rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_values = [math.exp(score - max_score) for score in scores]\n    sum_exp_values = sum(exp_values)\n    softmax_values = [round(exp_value / sum_exp_values, 4) for exp_value in exp_values]\n    return softmax_values\nscores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 2, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    This function takes a list of scores and returns the softmax probabilities\n    for each score rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    sum_exp_scores = sum((math.exp(score - max_score) for score in scores))\n    softmax_scores = [round(math.exp(score - max_score) / sum_exp_scores, 4) for score in scores]\n    return softmax_scores"}
{"task_id": 23, "completion_id": 3, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a list of scores.\n    The softmax function transforms each number in a list, scores, into a probability score such that the sum of all the scores is 1.\n    Each output score is rounded to four decimal places.\n    \n    :param scores: A list of floats representing the scores.\n    :return: A list of floats representing the softmax probabilities.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    probabilities = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return probabilities"}
{"task_id": 23, "completion_id": 4, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a list of scores.\n    \n    Args:\n    scores (list of float): The input scores.\n    \n    Returns:\n    list of float: The list containing softmax of each score rounded to 4 decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 5, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax for a given list of scores.\n    The softmax function is defined as exp(x)/sum(exp(x)) for each x in scores.\n    Each output value will be in the range [0, 1] and the sum of all outputs will be 1.\n\n    :param scores: A list of scores of any length.\n    :return: A list of softmax values corresponding to each input score, rounded to 4 decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 23, "completion_id": 6, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"Compute the softmax of the given scores and return the result rounded to four decimal places.\n    \n    Args:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: The softmax values of the scores rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_probs = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_probs"}
{"task_id": 23, "completion_id": 7, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax of the given scores.\n    \n    For each score, e^score is calculated, then each of those values\n    is divided by the sum of all e^scores to produce the final softmax\n    values, each rounded to four decimal places.\n    \n    :param scores: A list of float numbers representing the scores.\n    :return: A list of float numbers representing the softmax of the input scores.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 8, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n    Rounds each softmax value to four decimal places and returns the list.\n    \n    :param scores: List of input scores\n    :return: List of softmax activation values, each rounded to 4 decimal places\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_scores"}
{"task_id": 23, "completion_id": 9, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a list of scores.\n    \n    The softmax function transforms each score in a list of scores in such a way that each transformed score\n    can be interpreted as a probability, and all probabilities sum up to 1.\n    \n    :param scores: A list of float numbers representing the scores.\n    :return: A list of float numbers representing the softmax probabilities, rounded to four decimal places.\n    \"\"\"\n    normalized_scores = [score - max(scores) for score in scores]\n    exp_scores = [math.exp(score) for score in normalized_scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_probs = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_probs"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef single_neuron_model(features: List[List[float]], labels: List[int], weights: List[float], bias: float) -> Tuple[List[float], float]:\n    \"\"\"\n    This function implements a single neuron model with a sigmoid activation function. \n    It calculates the output probability for a given set of features and true labels,\n    and computes the mean squared error of the predictions.\n    \n    :param features: List[List[float]], a list of feature vectors for multiple training examples\n    :param labels: List[int], a list of true binary labels for each training example\n    :param weights: List[float], a list of weights for each feature\n    :param bias: float, the bias term for the neuron\n    :return: tuple of prediction probabilities and mean squared error, both rounded to 4th decimal\n    \"\"\"\n\n    def sigmoid(x: float) -> float:\n        \"\"\"Sigmoid activation function.\"\"\"\n        return 1 / (1 + math.exp(-x))\n\n    def mean_squared_error(predictions: np.ndarray, actuals: np.ndarray) -> float:\n        \"\"\"Mean Squared Error calculation.\"\"\"\n        n = len(predictions)\n        squared_error = np.sum((predictions - actuals) ** 2)\n        mse = squared_error / n\n        return round(mse, 4)\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    predictions = sigmoid(np.dot(features_array, weights_array) + bias)\n    mse = mean_squared_error(predictions, labels)\n    predictions_rounded = [round(pred, 4) for pred in predictions.tolist()]\n    return (predictions_rounded, mse)\nfeatures = [[0, 0], [0, 1], [1, 0], [1, 1]]\nlabels = [0, 1, 1, 0]\nweights = [0.5, -0.5]\nbias = 0"}
{"task_id": 24, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x: float) -> float:\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature in features:\n        z = sum([f * w for (f, w) in zip(feature, weights)]) + bias\n        predictions.append(sigmoid(z))\n    mse = round(sum([(p - l) ** 2 for (p, l) in zip(predictions, labels)]) / len(labels), 4)\n    return (predictions, mse)\nfeatures = [[0.5, -1.0], [-0.2, 0.3], [0.0, 0.4]]\nlabels = [0, 1, 1]\nweights = [0.1, 0.2]\nbias = 0.05"}
{"task_id": 24, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of feature vectors, where each vector is a list of floats.\n    :param labels: A list of binary labels (0 or 1).\n    :param weights: The weights for the neuron, one for each feature.\n    :param bias: The bias of the neuron.\n    :return: A tuple with a list of predicted probabilities and the mean squared error, both rounded.\n    \"\"\"\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * f for (w, f) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    mse = sum(((p - l) ** 2 for (p, l) in zip(predictions, labels))) / len(labels)\n    predictions_rounded = [round(p, 4) for p in predictions]\n    mse_rounded = round(mse, 4)\n    return (predictions_rounded, mse_rounded)\nfeatures = [[1.0, 2.0], [2.0, 2.0], [1.5, 3.0]]\nlabels = [0, 1, 1]\nweights = [0.4, 0.6]\nbias = 0.5"}
{"task_id": 24, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef predict(features: list[list[float]], weights: list[float], bias: float) -> np.ndarray:\n    \"\"\"Predict the output probabilities for given features, weights, and bias.\"\"\"\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    logits = np.dot(features_array, weights_array) + bias\n    predictions = np.array([sigmoid(logit) for logit in logits])\n    return predictions\ndef mean_squared_error(predictions: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Calculate the mean squared error between predictions and labels.\"\"\"\n    loss = np.mean((predictions - labels) ** 2)\n    return round(float(loss), 4)\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"Simulate a single neuron model with sigmoid activation for binary classification.\"\"\"\n    labels_array = np.array(labels)\n    predictions = predict(features, weights, bias)\n    mse = mean_squared_error(predictions, labels_array)\n    return (predictions.round(4).tolist(), round(mse, 4))\nfeatures = [[0, 0], [0, 1], [1, 0], [1, 1]]\nlabels = [0, 0, 0, 1]\nweights = [0.5, 0.5]\nbias = 0"}
{"task_id": 24, "completion_id": 4, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef single_neuron_model(features: List[List[float]], labels: List[int], weights: List[float], bias: float) -> Tuple[List[float], float]:\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of lists representing feature vectors.\n    :param labels: The true binary labels.\n    :param weights: Weights for each feature.\n    :param bias: Bias value.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    predictions = []\n    for feature in features:\n        linear_combination = np.dot(feature, weights) + bias\n        prediction = sigmoid(linear_combination)\n        predictions.append(round(prediction, 4))\n    mse = np.round(((np.array(predictions) - np.array(labels)) ** 2).mean(), 4)\n    return (predictions.tolist(), mse.tolist())\nfeatures = [[0.5, 0.3], [0.8, 0.2], [0.1, 0.5]]\nweights = [0.1, 0.2]\nbias = 0.3"}
{"task_id": 24, "completion_id": 5, "solution": "import numpy as np\nimport math\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    This function simulates a basic neuron with a sigmoid activation function, applied for binary classification.\n    It computes the predicted probabilities and calculates the mean squared error (MSE) between these and the true labels.\n    \n    :param features: list of feature vectors\n    :param labels: list of true binary labels (0 or 1)\n    :param weights: list of weights, one for each feature\n    :param bias: bias value used in the computation of the neuron's output \n    :return: a tuple with a list of predicted probabilities and the MSE\n    \"\"\"\n\n    def sigmoid(x: float) -> float:\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * f for (w, f) in zip(weights, feature_vector))) + bias\n        output = sigmoid(weighted_sum)\n        predictions.append(round(output, 4))\n    squared_errors = [(p - l) ** 2 for (p, l) in zip(predictions, labels)]\n    mse = np.mean(squared_errors)\n    return (predictions, round(mse, 4))"}
{"task_id": 24, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    sigmoid_val = 1 / (1 + math.exp(-x))\n    return round(sigmoid_val, 4)\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"Simulates a single neuron with a sigmoid activation function for binary classification.\"\"\"\n    predictions = []\n    squared_errors = 0\n    for (i, feature_vector) in enumerate(features):\n        linear_output = sum((weight * feature for (weight, feature) in zip(weights, feature_vector))) + bias\n        predicted_prob = sigmoid(linear_output)\n        predictions.append(predicted_prob)\n        squared_errors += (predicted_prob - labels[i]) ** 2\n    mse = squared_errors / len(labels)\n    return (np.array(predictions).tolist(), round(mse, 4))\nfeatures = [[2.0, 1.5], [1.0, 2.0], [2.0, 2.0]]\nlabels = [1, 0, 1]\nweights = [0.4, 0.6]\nbias = 0.5"}
{"task_id": 24, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Implements a single neuron model with a sigmoid activation function.\n    \n    :param features: List of feature vectors for the examples\n    :param labels: True binary labels for the examples\n    :param weights: Weights associated with each feature\n    :param bias: Bias value used in the model\n    :return: A tuple of (predicted probabilities, mean squared error)\n    \"\"\"\n    predictions = []\n    for feature in features:\n        weighted_sum = sum((weight * feat for (weight, feat) in zip(weights, feature))) + bias\n        prediction = 1 / (1 + math.exp(-weighted_sum))\n        predictions.append(prediction)\n    mse = round(sum(((pred - label) ** 2 for (pred, label) in zip(predictions, labels))) / len(labels), 4)\n    return (predictions, mse)\nfeatures = [[1.5, 2.0], [2.5, 3.5], [3.5, 4.5]]\nlabels = [1, 0, 1]\nweights = [0.4, -0.3]\nbias = 0.6"}
{"task_id": 24, "completion_id": 8, "solution": "import numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        \"\"\"Compute sigmoid activation value.\"\"\"\n        return 1 / (1 + np.exp(-x))\n    features_np = np.array(features)\n    labels_np = np.array(labels)\n    weights_np = np.array(weights)\n    z = np.dot(features_np, weights_np) + bias\n    predictions = sigmoid(z)\n    predictions_rounded = np.round(predictions, 4).tolist()\n    mse = np.round(np.mean((predictions - labels_np) ** 2), 4)\n    return (predictions_rounded, mse)\nfeatures = [[0.5, 0.6], [0.7, 0.3], [-0.4, 0.2]]\nlabels = [1, 0, 1]\nweights = [0.8, -0.5]\nbias = 0.2"}
{"task_id": 24, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef mean_squared_error(predictions: list[float], true_labels: list[int]) -> float:\n    \"\"\"Calculate the mean squared error between predictions and true labels.\"\"\"\n    error = np.array(predictions) - np.array(true_labels)\n    mse = (error ** 2).mean()\n    return round(mse, 4)\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of feature vectors.\n    :param labels: The true binary labels for the classification.\n    :param weights: Weights for each feature.\n    :param bias: Bias for the model.\n    :return: A tuple containing the predicted probabilities and the mean squared error.\n    \"\"\"\n    predictions = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(round(prediction, 4))\n    mse = mean_squared_error(predictions, labels)\n    return (predictions, mse)\nfeatures = [[0.5, 1.0], [1.0, 2.0], [1.5, 1.5]]\nlabels = [0, 1, 1]\nweights = [0.4, 0.7]\nbias = 0.3"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    \"\"\"Derivative of sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Calculates Mean Squared Error.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        error = labels - predictions\n        mse = mean_squared_error(labels, predictions)\n        mse_list.append(np.round(mse, 4))\n        gradients = error * sigmoid_derivative(predictions)\n        weights += learning_rate * np.dot(features.T, gradients)\n        bias += learning_rate * np.sum(gradients)\n    return (np.round(weights, 4).tolist(), np.round(bias, 4), mse_list)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_bias = 0\nlearning_rate = 0.1\nepochs = 100"}
{"task_id": 25, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid activation function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate the Mean Squared Error between true and predicted values.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        predictions = np.dot(features, weights) + bias\n        activations = sigmoid(predictions)\n        error = activations - labels\n        mse = mean_squared_error(labels, activations)\n        mse_values.append(round(mse, 4))\n        gradient_error = error * sigmoid_derivative(predictions)\n        gradient_weights = np.dot(features.T, gradient_error)\n        gradient_bias = np.sum(gradient_error)\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    return (weights.tolist(), round(bias, 4), mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([[0], [1], [1], [0]])\ninitial_weights = np.array([0.5, 0.5])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 25, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean squared error loss.\"\"\"\n    return np.square(y_true - y_pred).mean()\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list):\n    weights = initial_weights\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        inputs = np.dot(features, weights) + bias\n        outputs = sigmoid(inputs)\n        loss = mse_loss(labels, outputs)\n        mse_list.append(round(loss, 4))\n        grad_weights = -2 * np.dot(features.T, (labels - outputs) * sigmoid_derivative(inputs)) / labels.size\n        grad_bias = -2 * np.mean((labels - outputs) * sigmoid_derivative(inputs))\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n    return (weights.tolist(), round(bias, 4), mse_list)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 100"}
{"task_id": 25, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Computes the sigmoid activation of the given input.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Computes the derivative of the sigmoid function with respect to input x.\"\"\"\n    sig_x = sigmoid(x)\n    return sig_x * (1 - sig_x)\ndef mse_loss(predictions: np.ndarray, true_labels: np.ndarray) -> float:\n    \"\"\"Computes the Mean Squared Error loss.\"\"\"\n    return np.mean((predictions - true_labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_losses = []\n    for epoch in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        error = predictions - labels\n        dLoss_dPrediction = 2 * error\n        dPrediction_dLinear = sigmoid_derivative(linear_output)\n        gradient_wrt_weight = np.dot(features.T, dLoss_dPrediction * dPrediction_dLinear)\n        gradient_wrt_bias = np.sum(dLoss_dPrediction * dPrediction_dLinear)\n        weights -= learning_rate * gradient_wrt_weight\n        bias -= learning_rate * gradient_wrt_bias\n        mse = mse_loss(predictions, labels)\n        mse_losses.append(round(mse, 4))\n    return (weights.tolist(), round(bias, 4), mse_losses)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 25, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mse_loss(predictions: np.ndarray, true_labels: np.ndarray) -> float:\n    return np.mean((predictions - true_labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = np.copy(initial_weights)\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse_value = mse_loss(predictions, labels)\n        mse_values.append(round(mse_value, 4))\n        gradients = 2 * (predictions - labels) * sigmoid_derivative(weighted_sum)\n        weight_gradients = np.dot(features.T, gradients)\n        bias_gradients = np.sum(gradients)\n        weights -= learning_rate * weight_gradients\n        bias -= learning_rate * bias_gradients\n    return (weights.tolist(), round(bias, 4), mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 0, 0, 1])\nweights = np.array([0.5, -0.5])\nbias = 0.0\nlearning_rate = 0.1\nepochs = 10000"}
{"task_id": 25, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef mse_loss(predictions, labels):\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_each_epoch = []\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        loss = mse_loss(predictions, labels)\n        mse_each_epoch.append(round(loss, 4))\n        gradients = 2 * (predictions - labels) * predictions * (1 - predictions)\n        d_weights = np.dot(features.T, gradients)\n        d_bias = np.sum(gradients)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights.tolist(), round(bias, 4), mse_each_epoch)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 10000"}
{"task_id": 25, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"Calculate the Mean Squared Error.\"\"\"\n    return ((y_true - y_pred) ** 2).mean()\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single neuron using sigmoid activation and backpropagation.\n    \n    Args:\n        features (np.ndarray): A 2D array of input features.\n        labels (np.ndarray): A 1D array of true binary labels.\n        initial_weights (np.ndarray): Initial weights of the neuron.\n        initial_bias (float): Initial bias of the neuron.\n        learning_rate (float): Learning rate for the gradient descent.\n        epochs (int): Number of epochs to train.\n        \n    Returns:\n        (np.ndarray, float, list[float]): Updated weights, bias, and list of MSE values per epoch.\n    \"\"\"\n    weights = initial_weights\n    bias = initial_bias\n    mse_history = []\n    for _ in range(epochs):\n        linear_output = features @ weights + bias\n        predictions = sigmoid(linear_output)\n        mse = mean_squared_error(labels, predictions)\n        mse_history.append(round(mse, 4))\n        error = predictions - labels\n        gradient_weights = error * sigmoid_derivative(linear_output) @ features\n        gradient_bias = (error * sigmoid_derivative(linear_output)).mean()\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    return (weights, bias, mse_history)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 0, 0, 1])\ninitial_weights = np.array([0.1, -0.1])\ninitial_bias = 0.0\nlearning_rate = 0.5\nepochs = 15000"}
{"task_id": 25, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_history = []\n    for _ in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        mse = mse_loss(labels, predictions)\n        mse_history.append(round(mse, 4))\n        gradient = predictions - labels\n        weight_gradient = np.dot(features.T, gradient)\n        bias_gradient = np.sum(gradient, axis=0)\n        weights -= learning_rate * weight_gradient\n        bias -= learning_rate * bias_gradient\n    return (weights.tolist(), round(bias, 4), mse_history)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 25, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Derivative of the sigmoid activation function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Mean squared error loss function.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_history = []\n    for epoch in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_pred = sigmoid(z)\n        error = y_pred - labels\n        mse = mean_squared_error(labels, y_pred)\n        mse_history.append(round(mse, 4))\n        der_sigmoid = sigmoid_derivative(z)\n        der_loss = 2 * error\n        der_z = error * der_sigmoid\n        d_weights = np.dot(features.T, der_z)\n        d_bias = np.sum(der_z)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    return (weights.tolist(), round(bias, 4), mse_history)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 25, "completion_id": 9, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (list, float, list):\n    weights = initial_weights\n    bias = initial_bias\n    mse_history = []\n    for epoch in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        mse = mean_squared_error(labels, predictions)\n        mse_history.append(round(mse, 4))\n        error = predictions - labels\n        derivative = sigmoid_derivative(linear_output)\n        gradient_weights = np.dot(features.T, error * derivative)\n        gradient_bias = np.sum(error * derivative, axis=0)\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    weights_list = weights.tolist()\n    bias_rounded = round(bias, 4)\n    mse_history = [round(mse, 4) for mse in mse_history]\n    return (weights_list, bias_rounded, mse_history)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()"}
{"task_id": 26, "completion_id": 1, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 2, "solution": "class Value:\n\n    def __init__(self, data, _prev=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_prev)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out"}
{"task_id": 26, "completion_id": 3, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 4, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 5, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 6, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n            node._prev = set()"}
{"task_id": 26, "completion_id": 7, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 8, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        if not isinstance(other, Value):\n            other = Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 26, "completion_id": 9, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C in R^3.\n    \n    Parameters:\n    B (list of list of int): The basis B represented as a list of three vectors.\n    C (list of list of int): The basis C represented as a list of three vectors.\n    \n    Returns:\n    list of list of float: The transformation matrix from basis B to C, rounded to 4 decimal places.\n    \"\"\"\n    B = np.array(B)\n    C = np.array(C)\n    P = np.linalg.inv(C).dot(B)\n    P = np.round(P, 4).tolist()\n    return P"}
{"task_id": 27, "completion_id": 1, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of lists of int): Basis vectors forming the matrix B.\n    C (list of lists of int): Basis vectors forming the matrix C.\n    \n    Returns:\n    list of lists of float: The transformation matrix P from B to C, with elements rounded to 4 decimal places.\n    \"\"\"\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    C_inv = np.linalg.inv(C_matrix)\n    P = np.dot(C_inv, B_matrix)\n    P_rounded = np.round(P, decimals=4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 27, "completion_id": 2, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    B (list of lists of int): Basis vectors for B as column vectors.\n    C (list of lists of int): Basis vectors for C as column vectors.\n    \n    Returns:\n    list of lists of float: The transformation matrix P as a list of lists.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    C_inv = np.linalg.inv(C_np)\n    P = np.dot(C_inv, B_np)\n    P_rounded = np.around(P, decimals=4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 1, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 3, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C.\n    \n    Parameters:\n    - B: A list of list of integers representing the basis B vectors for R^3.\n    - C: A list of list of integers representing the basis C vectors for R^3.\n    \n    Returns:\n    - A 3x3 matrix as a list of lists of floats, representing the transformation matrix P.\n    \"\"\"\n    B_np = np.array(B).T\n    C_np = np.array(C).T\n    P = np.linalg.solve(B_np, C_np)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded"}
{"task_id": 27, "completion_id": 4, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C for R^3.\n    \n    Parameters:\n    B (list of lists): Basis B as a list of 3 vectors (each a list of 3 ints).\n    C (list of lists): Basis C as a list of 3 vectors (each a list of 3 ints).\n    \n    Returns:\n    list of lists: The transformation matrix as a list of lists of floats.\n    \"\"\"\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    return np.round(P, decimals=4).tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 27, "completion_id": 5, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to C in R^3.\n    \n    Parameters:\n    B (list of lists of ints): Basis vectors in basis B.\n    C (list of lists of ints): Basis vectors in basis C.\n    \n    Returns:\n    list of lists of floats: The transformation matrix P rounded to 4 decimal places.\n    \"\"\"\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    C_inv = np.linalg.inv(C_matrix)\n    P = np.dot(C_inv, B_matrix)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded"}
{"task_id": 27, "completion_id": 6, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to C.\n    Args:\n    - B: A list of list of integers representing the basis vectors in B.\n    - C: A list of list of integers representing the basis vectors in C.\n    \n    Returns:\n    - A list of list of floats representing the transformation matrix from B to C.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    C_inv = np.linalg.inv(C_np)\n    P = np.dot(C_inv, B_np)\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 7, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Given two bases B and C in R^3, compute the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of lists): Basis vectors in B as columns.\n    C (list of lists): Basis vectors in C as columns.\n    \n    Returns:\n    list of lists: Transformation matrix P such that [v]_C = P [v]_B, rounded to 4 decimal places.\n    \"\"\"\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    C_inv = np.linalg.inv(C_matrix)\n    P = np.dot(B_matrix, C_inv)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded"}
{"task_id": 27, "completion_id": 8, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    - B: A list of lists, each list representing a vector in the original basis B.\n    - C: A list of lists, each list representing a vector in the target basis C.\n    \n    Returns:\n    - A 2D list representing the transformation matrix P, rounded to 4 decimal places.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    inverse_B = np.linalg.inv(B_np)\n    transformation_matrix = C_np @ inverse_B\n    rounded_matrix = np.round(transformation_matrix, decimals=4)\n    return rounded_matrix.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 27, "completion_id": 9, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C in R^3.\n    \n    Parameters:\n    - B: list[list[int]], a 3x3 matrix, its column vectors represent basis B,\n         i.e., B = [[b11, b12, b13], [b21, b22, b23], [b31, b32, b33]]\n    - C: list[list[int]], a 3x3 matrix, its column vectors represent basis C,\n         i.e., C = [[c11, c12, c13], [c21, c22, c23], [c31, c32, c33]]\n         \n    Returns:\n    - P: list[list[float]], a 3x3 transformation matrix, rounded to 4 decimals.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    P_np = np.linalg.solve(B_np.T, C_np.T)\n    P = np.round(P_np, 4).tolist()\n    return P"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.diag(np.sqrt(eigenvalues))\n    V = eigenvectors\n    U = np.zeros(A.shape)\n    for i in range(A.shape[1]):\n        if eigenvalues[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / np.sqrt(eigenvalues[i])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[3, 1], [0, 2]])"}
{"task_id": 28, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Perform Singular Value Decomposition on a 2x2 matrix A and return U, S, V.\n    A = U * S * V^T, where U and V are orthogonal matrices and S is a diagonal matrix.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(A.shape[1]):\n        u_i = np.dot(A, V[:, i]) / singular_values[i]\n        U[:, i] = u_i\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = A.T @ A\n    (eigenvalues, V) = np.linalg.eigh(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, i] = A @ V[:, i] / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[2, 3], [4, 5]])"}
{"task_id": 28, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the SVD of a 2x2 matrix A using eigenvalues and eigenvectors.\n    \n    Args:\n    A (np.ndarray): 2x2 numpy array\n    \n    Returns:\n    tuple: U, S, V such that A = U * S * V.T where U and V are orthogonal matrices and S is a diagonal matrix.\n    \"\"\"\n    AT_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(AT_A)\n    sort_indices = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[sort_indices]\n    eigenvectors = eigenvectors[:, sort_indices]\n    S = np.sqrt(eigenvalues)\n    S = np.diag(S)\n    V = eigenvectors.T\n    U = []\n    for i in range(2):\n        if S[i, i] != 0:\n            U.append(A @ V[i] / S[i, i])\n        else:\n            U.append(np.cross(U[0], np.array([0, 0, 1])[:2]))\n    U = np.array(U).T\n    return (np.round(U, decimals=4).tolist(), np.round(S, decimals=4).tolist(), np.round(V, decimals=4).tolist())\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 28, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    sort_indices = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[sort_indices]\n    eigenvectors = eigenvectors[:, sort_indices]\n    S = np.diag(np.sqrt(eigenvalues))\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(2):\n        if eigenvalues[i] > 1e-14:\n            u = A @ V[:, i]\n            u = u / np.linalg.norm(u)\n            U[:, i] = u\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[2, 0], [0, 3]])"}
{"task_id": 28, "completion_id": 5, "solution": "import numpy as np"}
{"task_id": 28, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Perform Singular Value Decomposition on a 2x2 matrix A and return U, S, V\n    such that A = U * S * V.T, using eigen decomposition method.\n    \"\"\"\n    A_T = A.T\n    A_T_A = np.dot(A_T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(2):\n        if singular_values[i] > 1e-10:\n            U[:, [i]] = np.dot(A, V[:, [i]]) / singular_values[i]\n    U_rounded = np.round(U, decimals=4).tolist()\n    S_rounded = np.round(S, decimals=4).tolist()\n    V_rounded = np.round(V, decimals=4).tolist()\n    return (U_rounded, S_rounded, V_rounded)\nA = np.array([[3, 9], [9, 3]])"}
{"task_id": 28, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using eigenvalue decomposition.\n    \n    Parameters:\n    A (np.ndarray): 2x2 matrix for which to compute the SVD.\n    \n    Returns:\n    tuple: (U, S, V) where U and V are orthogonal matrices and S is a diagonal matrix with singular values.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    V1 = V[:, 0]\n    V2 = V[:, 1]\n    U1 = A @ V1 / singular_values[0]\n    U2 = A @ V2 / singular_values[1]\n    U = np.column_stack((U1, U2))\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[3, 1], [1, 3]])"}
{"task_id": 28, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Perform SVD on a 2x2 matrix A using eigenvalues and eigenvectors.\n    Returns (U, S, V) such that A = U * S * V.T, where S is a diagonal matrix,\n    and U and V are orthogonal matrices.\n    \"\"\"\n    A_AT = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A_AT)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    indices = eigenvalues.argsort()[::-1]\n    S = S[indices]\n    V = eigenvectors[:, indices]\n    U = np.zeros_like(A)\n    for i in range(2):\n        U[:, i] = np.dot(A, V[:, i]) / singular_values[indices[i]]\n    S = np.diag(singular_values[indices])\n    return (np.around(U, decimals=4).tolist(), np.around(S, decimals=4).tolist(), np.around(V, decimals=4).tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition of a 2x2 matrix A.\n    Returns U, S, V such that A \u2248 U * np.diag(S) * V.T.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    sorted_indices = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    V = eigenvectors\n    S = np.diag(singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n        elif i == 0:\n            U[:, i] = np.array([1, 0]) if V[1, 0] != 0 else np.array([0, 1])\n        else:\n            U[:, i] = np.cross(U[:, 0])\n    U = U / np.linalg.norm(U, axis=0)\n    return (U.tolist(), np.around(S, decimals=4).tolist(), V.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    num_samples = X.shape[0]\n    shuffled_indices = np.random.permutation(num_samples)\n    shuffled_X = X[shuffled_indices]\n    shuffled_y = y[shuffled_indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([[1], [0], [1]])"}
{"task_id": 29, "completion_id": 1, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the rows of two numpy arrays X and y while maintaining their correspondence.\n    \n    :param X: numpy array of features\n    :param y: numpy array of labels\n    :param seed: Optional seed for reproducibility\n    :return: shuffled X and y as python lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    (X_shuffled, y_shuffled) = (X[indices], y[indices])\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\ny = np.array([0, 1, 2])"}
{"task_id": 29, "completion_id": 2, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the order of elements in the datasets X and y while maintaining their correspondence.\n    \n    Parameters:\n    X (np.ndarray): A numpy array of shape (n_samples, n_features) representing the samples.\n    y (np.ndarray): A numpy array of shape (n_samples,) representing the labels for the samples.\n    seed (int, optional): A seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    tuple: Two lists containing the shuffled numpy arrays X and y.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError('Inputs X and y must be numpy arrays.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('Inputs X and y must have the same number of samples.')\n    if seed is not None:\n        np.random.seed(seed)\n    p = np.random.permutation(X.shape[0])\n    shuffled_X = X[p]\n    shuffled_y = y[p]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 29, "completion_id": 3, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle X and y numpy arrays in unison, maintaining the correspondence between samples and labels.\n\n    Parameters:\n    - X: numpy.ndarray, feature dataset.\n    - y: numpy.ndarray, labels for the dataset.\n    - seed: int, optional, random seed for reproducibility.\n\n    Returns:\n    - Tuple of shuffled numpy arrays as lists.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('Features (X) and labels (y) must have the same length.')\n    np.random.seed(seed)\n    permutation = np.random.permutation(X.shape[0])\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 4, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the rows of two numpy arrays, X and y, maintaining the correspondence between the arrays.\n    \n    :param X: Numpy array of features.\n    :param y: Numpy array of labels.\n    :param seed: Optional seed for the random number generator to maintain reproducibility.\n    :return: Two numpy arrays (X and y) with the rows shuffled in the same order.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])"}
{"task_id": 29, "completion_id": 5, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the rows of two numpy arrays, X and y, maintaining their correspondence.\n    \n    Parameters:\n    - X: numpy array, the feature matrix to be shuffled.\n    - y: numpy array, the target labels to be shuffled.\n    - seed: int, optional random seed integer for reproducibility.\n    \n    Returns:\n    - X_shuffled: list, the shuffled feature matrix.\n    - y_shuffled: list, the shuffled target labels.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same length.'\n    p = np.random.permutation(len(X))\n    X_shuffled = X[p]\n    y_shuffled = y[p]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[10, 20], [30, 40], [50, 60]])\ny = np.array([1, 0, 1])"}
{"task_id": 29, "completion_id": 6, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"Randomly shuffles the rows of two numpy arrays, X and y, in the same order.\n    \n    Args:\n        X (numpy.ndarray): The input features array.\n        y (numpy.ndarray): The output labels array.\n        seed (int, optional): The seed for random number generation. Defaults to None.\n    \n    Returns:\n        tuple: Two lists representing the shuffled versions of X and y.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y)\n    p = np.random.permutation(len(X))\n    return (X[p].tolist(), y[p].tolist())"}
{"task_id": 29, "completion_id": 7, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays, X and y, with the same random order.\n    This is useful for shuffling samples and labels in machine learning datasets.\n    \n    Arguments:\n    X -- A numpy array of samples\n    y -- A numpy array of labels corresponding to samples in X\n    seed -- An optional integer for the random number generator seed for reproducibility\n    \n    Returns:\n    X_shuffled -- A numpy array of the samples in X after shuffling\n    y_shuffled -- A numpy array of the labels in y after shuffling\n    \"\"\"\n    np.random.seed(seed)\n    assert len(X) == len(y)\n    p = np.random.permutation(len(X))\n    X_shuffled = X[p]\n    y_shuffled = y[p]\n    if len(X_shuffled.shape) == 1:\n        X_shuffled = X_shuffled.reshape(-1, 1)\n    if len(y_shuffled.shape) == 1:\n        y_shuffled = y_shuffled.reshape(-1, 1)\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[_] for _ in range(10)])\ny = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"}
{"task_id": 29, "completion_id": 8, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the elements in the provided numpy arrays X and y while maintaining\n    their correspondence, and returns the shuffled arrays as lists.\n    \n    Args:\n    X: numpy.ndarray - The feature matrix.\n    y: numpy.ndarray - The target vector.\n    seed: int - An optional random seed for reproducibility.\n    \n    Returns:\n    list_X, list_y: The shuffled arrays X and y as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    (X, y) = (np.asarray(X), np.asarray(y))\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n    perm_indices = np.random.permutation(len(X))\n    shuffled_X = X[perm_indices]\n    shuffled_y = y[perm_indices]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 29, "completion_id": 9, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    :param X: numpy array of features\n    :param y: numpy array of labels\n    :param seed: Optional seed for the random number generator for reproducibility\n    :return: A tuple of (X, y) shuffled numpy arrays, converted to lists\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same length'\n    p = np.random.permutation(len(X))\n    X_shuffled = X[p]\n    y_shuffled = y[p]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 0])"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates a batch iterator that yields tuples of (batch_x, batch_y) if y is not None,\n    or just batch_x if y is None.\n    \n    Parameters:\n    X: numpy.ndarray\n        The input feature data.\n    y: numpy.ndarray, optional\n        The target or label data.\n    batch_size: int, optional\n        The size of each batch.\n        \n    Yields:\n    tuple of (list, list) or list\n        If y is not None, yields (batch_x, batch_y) where both are lists.\n        If y is None, yields batch_x as a list.\n    \"\"\"\n    num_samples = X.shape[0]\n    for start_index in range(0, num_samples, batch_size):\n        end_index = min(start_index + batch_size, num_samples)\n        batch_x = X[start_index:end_index].tolist()\n        if y is not None:\n            batch_y = y[start_index:end_index].tolist()\n            yield (batch_x, batch_y)\n        else:\n            yield batch_x"}
{"task_id": 30, "completion_id": 1, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator function that yields batches of X and optionally y.\n    \n    Args:\n    X (np.array): Numpy array of features.\n    y (np.array, optional): Numpy array of labels. Defaults to None.\n    batch_size (int, optional): Size of each batch. Defaults to 64.\n    \n    Yields:\n    tuple or list: If y is None, a list of batch_size elements from X is yielded.\n                   Otherwise, a tuple of two lists ((X_batch, y_batch)) is yielded.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_indices = indices[start:end]\n        X_batch = X[batch_indices].tolist()\n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\ny = np.array([i % 2 for i in range(100)])"}
{"task_id": 30, "completion_id": 2, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from numpy arrays X and y (if provided).\n    \n    Parameters:\n    - X (numpy.ndarray): Input features array.\n    - y (numpy.ndarray, optional): Target labels array. If provided, it must have the same length as X.\n    - batch_size (int): Number of samples per batch. Should be an integer greater than 0.\n    \n    Yields:\n    - tuple: Tuple (X_batch, y_batch) if y is not None, otherwise X_batch.\n      The batch lists are derived using numpy's tolist() method.\n    \"\"\"\n    n_samples = X.shape[0]\n    if y is not None and X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same length')\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i + batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i + batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch\ny = np.array([i % 2 for i in range(100)])"}
{"task_id": 30, "completion_id": 3, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates and returns a batch iterator yielding slices of the provided data.\n    \n    :param X: The numpy array of features.\n    :param y: The optional numpy array of labels.\n    :param batch_size: The size of each batch to yield.\n    :yield: Tuples of (batch_X, batch_y) or just batch_X if y is None.\n    \"\"\"\n    number_of_samples = X.shape[0]\n    for i in range(0, number_of_samples, batch_size):\n        batch_X = X[i:i + batch_size].tolist()\n        if y is not None:\n            batch_y = y[i:i + batch_size].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 30, "completion_id": 4, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterable that returns batches of X and optionally y.\n    \n    :param X: Numpy array for features\n    :param y: Numpy array for labels (optional)\n    :param batch_size: Number of samples per batch\n    :yield: Batch of X and y (if provided) as lists\n    \"\"\"\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for i in range(0, len(indices), batch_size):\n        batch_indices = indices[i:i + batch_size]\n        batch_X = X[batch_indices].tolist()\n        if y is not None:\n            batch_y = y[batch_indices].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 30, "completion_id": 5, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Returns a generator that yields batches of data of size 'batch_size'.\n    \n    If y is not None, returns tuples of batches: (batch of X, batch of y).\n    If y is None, returns batches of X.\n    \n    Args:\n    - X: numpy.array, the input dataset.\n    - y: numpy.array, the output dataset. Should be the same length as X. Optional.\n    - batch_size: int, size of the batch.\n    \n    Yields:\n    - tuple: (list of batch of X, list of batch of y) if y is not None.\n    - list of batch of X if y is None.\n    \"\"\"\n    if y is not None and len(X) != len(y):\n        raise ValueError('X and y must have the same number of elements.')\n    total_samples = len(X)\n    for start_idx in range(0, total_samples, batch_size):\n        end_idx = min(start_idx + batch_size, total_samples)\n        batch_X = X[start_idx:end_idx].tolist()\n        if y is not None:\n            batch_y = y[start_idx:end_idx].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\nX = np.array([i for i in range(10)])\ny = np.array([i * 2 for i in range(10)])"}
{"task_id": 30, "completion_id": 6, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterable function that yields batches of numpy arrays X and optionally y.\n\n    Args:\n    - X (numpy.ndarray): Input data.\n    - y (numpy.ndarray, optional): Labels for input data. Defaults to None.\n    - batch_size (int, optional): Size of each batch. Defaults to 64.\n\n    Yields:\n    - list: Batch of X if y is not provided.\n    - tuple: Tuple of batches (X, y) if y is provided.\n    \"\"\"\n    assert isinstance(X, np.ndarray), 'X must be a numpy array.'\n    if y is not None:\n        assert isinstance(y, np.ndarray), 'y must be a numpy array.'\n        assert X.shape[0] == y.shape[0], 'First dimension of X and y must match.'\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_X = X[start:end].tolist()\n        if y is not None:\n            batch_y = y[start:end].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 30, "completion_id": 7, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterable function that generates batches from a numpy array X and an optional numpy array y.\n    \n    :param X: Input data as a numpy array.\n    :param y: Target data as a numpy array, optional.\n    :param batch_size: Size of each batch, default is 64.\n    :return: Yields one batch at a time, as a tuple of lists (batch_X, batch_y) or a single list batch_X.\n    \"\"\"\n    assert type(X) == np.ndarray, 'X must be a numpy array.'\n    length = X.shape[0]\n    num_batches = int(np.ceil(length / batch_size))\n    for i in range(num_batches):\n        batch_start = i * batch_size\n        batch_end = min(batch_start + batch_size, length)\n        batch_X = X[batch_start:batch_end].tolist()\n        if y is not None:\n            assert type(y) == np.ndarray, 'y must be a numpy array.'\n            assert X.shape[0] == y.shape[0], 'X and y must have the same length.'\n            batch_y = y[batch_start:batch_end].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\nX = np.array([i for i in range(200)])\ny = np.array([i * 2 for i in range(200)])"}
{"task_id": 30, "completion_id": 8, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of X, and optionally y with a specified batch size.\n    \n    :param X: Numpy array of features.\n    :param y: Numpy array of labels (optional).\n    :param batch_size: Number of samples per batch.\n    :yield: Batches of X or (X, y) as lists.\n    \"\"\"\n    number_of_samples = X.shape[0]\n    number_of_batches = int(np.ceil(number_of_samples / batch_size))\n    for batch in range(number_of_batches):\n        start_index = batch * batch_size\n        end_index = start_index + batch_size\n        if y is not None:\n            yield (X[start_index:end_index].tolist(), y[start_index:end_index].tolist())\n        else:\n            yield X[start_index:end_index].tolist()\ny = np.array([0, 1, 0, 1, 0])"}
{"task_id": 30, "completion_id": 9, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of arrays X and optionally y.\n\n    Parameters:\n    X (np.ndarray): The first numpy array to be batched.\n    y (np.ndarray, optional): The second numpy array to be batched.\n    batch_size (int): The size of each batch.\n\n    Yields:\n    tuple or list: A tuple (batch_X, batch_y) if y is provided, else a list batch_X.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_X = X[i:i + batch_size].tolist()\n        if y is not None:\n            batch_y = y[i:i + batch_size].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\ny = np.array([1.5, 2.5, 3.5, 4.5, 5.5])"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset into two subsets based on whether a sample's \n    feature value is greater than or equal to the threshold.\n    \n    Parameters:\n    - X: numpy array representing the dataset.\n    - feature_i: integer representing the index of the feature to divide by.\n    - threshold: float representing the threshold for division.\n    \n    Returns:\n    - Two numpy arrays: left subset (values below threshold) and right subset (values above threshold).\n    \"\"\"\n    left_indices = X[:, feature_i] < threshold\n    right_indices = X[:, feature_i] >= threshold\n    left_subset = X[left_indices].tolist()\n    right_subset = X[right_indices].tolist()\n    return (left_subset, right_subset)"}
{"task_id": 31, "completion_id": 1, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset X into two subsets based on whether the value of the specified\n    feature is greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The input dataset.\n    feature_i (int): The index of the feature to split the dataset by.\n    threshold (float): The threshold value for splitting.\n\n    Returns:\n    tuple: Two lists containing the indices of the elements that satisfy the condition\n           and those that do not, respectively.\n    \"\"\"\n    split_indices = X[:, feature_i] >= threshold\n    true_indices = X[split_indices]\n    false_indices = X[~split_indices]\n    return (true_indices.tolist(), false_indices.tolist())"}
{"task_id": 31, "completion_id": 2, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide dataset based on whether values of feature_i are greater than or equal to threshold.\n\n    :param X: numpy array of shape (n_samples, n_features), the dataset to be divided.\n    :param feature_i: int, the index of the feature to divide the dataset on.\n    :param threshold: float or int, the threshold to compare the feature values against.\n    :return: tuple of two lists, (subset_on, subset_off), \n             the first subset contains elements where X[:, feature_i] >= threshold,\n             the second subset contains elements where X[:, feature_i] < threshold.\n    \"\"\"\n    condition = X[:, feature_i] >= threshold\n    subset_on = X[condition].tolist()\n    subset_off = X[~condition].tolist()\n    return (subset_on, subset_off)"}
{"task_id": 31, "completion_id": 3, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides data X into two subsets based on whether values\n    in specified feature column (feature_i) are greater than or equal to\n    or less than the threshold.\n    \n    Parameters:\n    X (np.array): Numpy array representing the dataset.\n    feature_i (int): Index of the feature to check against the threshold.\n    threshold (float): The threshold value to divide the dataset.\n    \n    Returns:\n    tuple: Containing the left and right splits.\n    \"\"\"\n    left_split = X[X[:, feature_i] < threshold]\n    right_split = X[X[:, feature_i] >= threshold]\n    return (left_split.tolist(), right_split.tolist())"}
{"task_id": 31, "completion_id": 4, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset based on whether the value of the specified feature is\n    greater than or equal to the given threshold.\n    \n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to check the threshold against.\n    threshold (float): The value to compare the feature values against.\n    \n    Returns:\n    list: Two subsets of the dataset split based on the condition.\n    \"\"\"\n    X[:, feature_i] = X[:, feature_i].astype(float)\n    left_split = X[X[:, feature_i] < threshold]\n    right_split = X[X[:, feature_i] >= threshold]\n    return [left_split.tolist(), right_split.tolist()]"}
{"task_id": 31, "completion_id": 5, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides dataset based on whether values of a specified feature are greater than or equal to a threshold.\n    \n    Parameters:\n    X (np.array): Dataset.\n    feature_i (int): Index of the feature to be checked against the threshold.\n    threshold (float): The threshold value for dividing the dataset.\n    \n    Returns:\n    tuple: Two lists representing the subsets that meet and do not meet the threshold condition.\n    \"\"\"\n    condition = X[:, feature_i] >= threshold\n    meet_condition = np.compress(condition, X, axis=0).tolist()\n    not_meet_condition = np.compress(np.logical_not(condition), X, axis=0).tolist()\n    return (meet_condition, not_meet_condition)"}
{"task_id": 31, "completion_id": 6, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X based on whether the value at the feature_i is greater than\n    or equal to the threshold. Returns two datasets: one that meets the condition and\n    another that does not.\n    \n    :param X: numpy.ndarray, dataset\n    :param feature_i: int, index of feature to divide on\n    :param threshold: float, threshold value to compare feature values against\n    :return: (list, list), two datasets based on the condition (meeting, not meeting)\n    \"\"\"\n    assert len(X) > 0, 'The dataset X is empty.'\n    meet_condition = []\n    not_meet_condition = []\n    for sample in X:\n        if sample[feature_i] >= threshold:\n            meet_condition.append(sample.tolist())\n        else:\n            not_meet_condition.append(sample.tolist())\n    return (meet_condition, not_meet_condition)"}
{"task_id": 31, "completion_id": 7, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides dataset based on whether values of a specified feature meet a given threshold.\n    \n    :param X: Numpy array representing the dataset.\n    :param feature_i: Integer representing the feature index in the dataset to compare against the threshold.\n    :param threshold: Value to compare the feature's value against.\n    :return: Two python lists representing the subsets of the dataset which meet and do not meet the condition.\n    \"\"\"\n    feature_values = X[:, feature_i]\n    condition = feature_values >= threshold\n    meet_condition = X[condition].tolist()\n    not_meet_condition = X[~condition].tolist()\n    return (meet_condition, not_meet_condition)"}
{"task_id": 31, "completion_id": 8, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset X on a given feature and threshold.\n\n    :param X: numpy.ndarray, the dataset to be split.\n    :param feature_i: int, the index of the feature on which to split the dataset.\n    :param threshold: float, the threshold value for splitting.\n    :return: (numpy.ndarray, numpy.ndarray), the two subsets created by the split.\n    \"\"\"\n    split_func = lambda sample: sample[feature_i] >= threshold\n    X_above = np.array([sample for sample in X if split_func(sample)])\n    X_below = np.array([sample for sample in X if not split_func(sample)])\n    return (X_above.tolist(), X_below.tolist())"}
{"task_id": 31, "completion_id": 9, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset based on whether the value in the specified feature column\n    is greater than or equal to the provided threshold.\n    \n    :param X: numpy.ndarray, the dataset to be divided\n    :param feature_i: int, index of the feature to divide on\n    :param threshold: float, the value to compare feature values to\n    :return: tuple, two lists representing the subsets of the dataset.\n    \"\"\"\n    split_function = np.greater_equal if threshold is not None else np.ones_like\n    split = split_function(X[:, feature_i], threshold)\n    subset1 = X[split].tolist()\n    subset2 = X[~split].tolist()\n    return (subset1, subset2)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement"}
{"task_id": 32, "completion_id": 1, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial features for a given dataset up to the specified degree.\n    \n    Parameters:\n    X (2D numpy array): Input dataset.\n    degree (int): The degree up to which to generate polynomial features.\n    \n    Returns:\n    list: A list of polynomial features generated from the input dataset.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    indices = np.arange(n_features)\n    features = [combinations_with_replacement(indices, d) for d in range(0, degree + 1)]\n    features = [item for sublist in features for item in sublist]\n    poly_features = np.empty((n_samples, 0))\n    for feature in features:\n        feature_product = np.prod([np.power(X[:, i], feature.count(i)) for i in range(n_features)], axis=0)[:, np.newaxis]\n        poly_features = np.hstack((poly_features, feature_product))\n    return poly_features.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\ndegree = 2"}
{"task_id": 32, "completion_id": 2, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates a new array containing the polynomial features of the given input array.\n    \n    Parameters:\n    X (numpy.ndarray): The input 2D numpy array.\n    degree (int): The maximum degree of the polynomial features.\n    \n    Returns:\n    list: A list containing the polynomial features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    features_indices = list(range(n_features))\n    combs = [combinations_with_replacement(features_indices, d) for d in range(0, degree + 1)]\n    combs = [item for sublist in combs for item in sublist]\n    poly_features = np.empty((n_samples, 0), dtype=np.float)\n    for c in combs:\n        feature = X[:, 0] ** c[0]\n        for i in range(1, len(c)):\n            feature = feature * X[:, i] ** c[i]\n        poly_features = np.hstack((poly_features, feature[:, np.newaxis]))\n    return poly_features.tolist()\nX = np.array([[2, 3], [3, 4], [1, 5]])\ndegree = 2"}
{"task_id": 32, "completion_id": 3, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Expand the input matrix X to include polynomial features up to the specified degree.\n    \n    Parameters:\n    X : 2D numpy array\n        The input features.\n    degree : int\n        The degree of the polynomial features.\n    \n    Returns:\n    expanded_X : list\n        A list of lists of the expanded features in the form of [1, x1, x2, x1^2, x1*x2, x2^2, ...] for degree=2.\n    \"\"\"\n    (n_samples, n_features) = np.shape(X)\n    combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combs = [item for sublist in combs for item in sublist]\n    num_feats = len(combs)\n    expanded_X = np.empty((n_samples, num_feats))\n    for (i, index_combs) in enumerate(combs):\n        expanded_X[:, i] = np.prod(X[:, index_combs], axis=1)\n    return expanded_X.tolist()"}
{"task_id": 32, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features up to the specified degree.\n    \n    Parameters:\n    - X: A 2D numpy array of shape (n_samples, n_features)\n    - degree: An integer representing the degree of the polynomial features.\n    \n    Returns:\n    - A 2D numpy array containing the new polynomial feature matrix.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    indices = list(combinations_with_replacement(range(n_features), degree))\n    n_output_features = len(indices)\n    P = np.empty((n_samples, n_output_features))\n    for (i, index) in enumerate(indices):\n        P[:, i] = np.prod([X[:, j] ** index.count(j) for j in range(n_features)], axis=0)\n    return P.tolist()\nX = np.array([[2, 3], [3, 5]])\ndegree = 2"}
{"task_id": 32, "completion_id": 5, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    Parameters:\n    X : np.ndarray\n        2D numpy array to which to apply the polynomial generator.\n    degree : int\n        Degree of the polynomial features.\n    \n    Returns:\n    np.ndarray\n        2D numpy array containing the polynomial and interaction features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, None]\n    (n_samples, n_features) = X.shape\n    polynomial_features = [np.prod(X[:, exponents], axis=1) for exponents in combinations_with_replacement(range(n_features), degree)]\n    return np.hstack(polynomial_features).tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 6, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    :param X: numpy array of shape [n_samples, n_features]\n    :param degree: Degree of the polynomial features.\n    :return: The matrix of features, where polynomial features are generated.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    powers = list(combinations_with_replacement(range(n_features + 1), degree))\n    n_powers = len(powers)\n    Y = np.ones((n_samples, n_powers))\n    for i in range(n_samples):\n        for (j, power) in enumerate(powers):\n            Y[i, j] = np.prod(np.power(X[i], power))\n    return Y.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 7, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    Parameters:\n    - X: numpy.ndarray, the input data.\n    - degree: int, the maximum degree of the polynomial features.\n    \n    Returns:\n    - poly_features: list, the polynomial features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    polynomial_indices = []\n    for d in range(degree + 1):\n        for index_comb in combinations_with_replacement(range(n_features), d):\n            if len(index_comb) > 0:\n                max_power_indices = list(index_comb)\n                polynomial_indices.append(max_power_indices)\n    poly_features = np.empty((n_samples, len(polynomial_indices)))\n    for (i, index) in enumerate(polynomial_indices):\n        poly_features[:, i] = np.prod(X[:, index], axis=1)\n    return poly_features.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 32, "completion_id": 8, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial and interaction features up to the specified degree.\n    \n    Parameters:\n    - X: 2D numpy array, the input data.\n    - degree: int, the highest degree of the polynomial features.\n    \n    Returns:\n    - numpy array, transformed array containing the polynomial features.\n    \"\"\"\n    (n_samples, n_features) = np.shape(X)\n    combinations = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combinations = [item for sublist in combinations for item in sublist]\n    n_output_features = len(combinations)\n    poly_array = np.empty((n_samples, n_output_features))\n    for (col, row) in enumerate(combinations):\n        poly_array[:, col] = np.prod(X[:, row], axis=1)\n    return poly_array.tolist()"}
{"task_id": 32, "completion_id": 9, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generates polynomial and interaction features up to the specified degree.\n    \n    Parameters:\n    X (2D numpy array): The input data.\n    degree (int): The degree of the polynomial features.\n    \n    Returns:\n    numpy array: The output array with generated polynomial features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    indices = np.arange(n_features)\n    new_features = [combinations_with_replacement(indices, i) for i in range(0, degree + 1)]\n    new_features = [item for sublist in new_features for item in sublist]\n    n_outcomes = len(new_features)\n    FX = np.empty((n_samples, n_outcomes))\n    for (i, feature) in enumerate(new_features):\n        FX[:, i] = np.prod(X[:, feature], axis=1)\n    return FX.tolist()\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of n_subsets random subsets of the input dataset (X, y).\n    Parameters:\n    - X: 2D numpy array, features of the dataset.\n    - y: 1D numpy array, labels of the dataset.\n    - n_subsets: integer, number of subsets to generate.\n    - replacements: boolean, whether to use replacement or not while sampling.\n    - seed: integer, random seed for reproducibility.\n    \n    Returns:\n    - A list of n_subsets tuples, each containing (X_subset, y_subset) as its elements.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([0, 1, 0, 1])\nn_subsets = 3"}
{"task_id": 33, "completion_id": 1, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    if replacements:\n        for _ in range(n_subsets):\n            indices = np.random.randint(0, len(y), size=len(y))\n            X_subset = X[indices]\n            y_subset = y[indices]\n            subsets.append((X_subset.tolist(), y_subset.tolist()))\n    else:\n        for _ in range(n_subsets):\n            (X_shuffled, y_shuffled) = shuffle_without_replacement(X, y)\n            subsets.append((X_shuffled.tolist(), y_shuffled.tolist()))\n    return subsets\ndef shuffle_without_replacement(X, y):\n    permutation = np.random.permutation(len(y))\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    return (X_shuffled, y_shuffled)"}
{"task_id": 33, "completion_id": 2, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Function to get a list of n_subsets random subsets of a dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D array of the features.\n    y (np.ndarray): 1D array of the labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to select subsets with or without replacements.\n    seed (int): Seed for reproducible random selections.\n    \n    Returns:\n    list: A list of n_subsets random subsets. Each subset is a tuple (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed=seed)\n    (rows, _) = X.shape\n    all_indices = np.arange(rows)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            selected_indices = np.random.choice(all_indices, rows, replace=True)\n        else:\n            selected_indices = np.random.choice(all_indices, size=rows, replace=False)\n        (X_subset, y_subset) = (X[selected_indices], y[selected_indices])\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.random.rand(100, 5)\ny = np.random.randint(2, size=100)"}
{"task_id": 33, "completion_id": 3, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([1, 2, 3, 4, 5])\nn_subsets = 3"}
{"task_id": 33, "completion_id": 4, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    X (np.array): 2D numpy array representing the features of the dataset.\n    y (np.array): 1D numpy array representing the labels of the dataset.\n    n_subsets (int): The number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements. Default is True.\n    seed (int): Seed for the random number generator. Default is 42.\n    \n    Returns:\n    list: A list of n_subsets random subsets of the dataset. Each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    subset_size = n_samples // n_subsets if not replacements else n_samples\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=subset_size, replace=replacements)\n        (X_subset, y_subset) = (X[indices], y[indices])\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([0, 1, 0, 1, 0])"}
{"task_id": 33, "completion_id": 5, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n        else:\n            indices = np.random.choice(X.shape[0], X.shape[0], replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 6, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same number of samples'\n    n_samples = len(X)\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 7, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of random subsets of the dataset.\n\n    Parameters:\n    X (numpy.ndarray): 2D numpy array of features.\n    y (numpy.ndarray): 1D numpy array of labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Seed value for the random number generator for reproducibility.\n\n    Returns:\n    list: A list of tuples, where each tuple contains a subset of X and y.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        (X_subset, y_subset) = (X[indices], y[indices])\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.random.rand(100, 2)\ny = np.random.randint(0, 2, 100)\nn_subsets = 5"}
{"task_id": 33, "completion_id": 8, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of n_subsets random subsets of the dataset.\n    \n    Parameters:\n    X: 2D numpy array, the training data.\n    y: 1D numpy array, the target data.\n    n_subsets: The number of subsets to create.\n    replacements: Whether to create subsets with or without replacements.\n    seed: Seed for the random number generator for reproducibility.\n    \n    Returns:\n    A list of n_subsets random subsets of the dataset.\n    Each subset is a tuple of (X_subset, y_subset) lists.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    indices = np.arange(X.shape[0])\n    subset_size = X.shape[0] // n_subsets if not replacements else X.shape[0]\n    for _ in range(n_subsets):\n        if not replacements:\n            chosen_indices = np.random.choice(indices, size=subset_size, replace=False)\n        else:\n            chosen_indices = np.random.choice(indices, size=subset_size, replace=True)\n        X_subset = X[chosen_indices]\n        y_subset = y[chosen_indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([1, 2, 3, 4, 5])\nn_subsets = 3"}
{"task_id": 33, "completion_id": 9, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of the dataset (X, y).\n    \n    Args:\n    - X: 2D numpy array, the feature matrix.\n    - y: 1D numpy array, the labels.\n    - n_subsets: int, the number of subsets to generate.\n    - replacements: bool, True for sampling with replacement, False otherwise.\n    - seed: int, seed for the random number generator.\n\n    Returns:\n    - list of tuples (X_subset, y_subset) where X_subset and y_subset are python lists.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Converts a 1D numpy array of categorical values into a 2D one-hot encoded representation.\n    \n    Parameters:\n    x (np.array): A 1D numpy array of categorical values.\n    n_col (int, optional): The number of columns for the one-hot encoded array. If None, will be determined from the input values.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = int(x.max()) + 1\n    one_hot = np.zeros((x.size, n_col))\n    one_hot[np.arange(x.size), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 1, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on the input 1D numpy array of integer values.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): The number of columns for the one-hot encoded array. If None, determined from input.\n    \n    Returns:\n    list: A list after reshaping by using numpy's tolist() method.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.size, n_col))\n    one_hot[np.arange(x.size), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 2, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Performs one-hot encoding for a given 1D numpy array of integer values.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values to be encoded.\n    n_col (int, optional): The number of columns for the one-hot encoded array.\n                           If not given, it will be inferred from the input array.\n                           \n    Returns:\n    list: The one-hot encoded data as a list of lists (rows of the one-hot encoded matrix).\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_matrix = np.eye(n_col)[x]\n    return one_hot_matrix.tolist()\nx = np.array([2, 3, 0, 1, 4])"}
{"task_id": 34, "completion_id": 3, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers to a one-hot encoded 2D numpy array.\n    \n    Parameters:\n    x (numpy.array): 1D numpy array of integers representing nominal values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. \n        If not provided, it will be determined from the input array.\n    \n    Returns:\n    list: A 2D list representation of one-hot encoded values.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 4, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values in the input 1D numpy array.\n    \n    Parameters:\n    - x: A 1D numpy array of integer values representing the nominal values.\n    - n_col: An optional integer representing the number of columns in the \n             one-hot encoded array. If not provided, it is determined from the input array.\n             \n    Returns:\n    - A list representing the one-hot encoded values of the input array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 5, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (np.array): 1D numpy array of integers.\n    n_col (int, optional): The number of columns for the one-hot encoded array. \n        If not provided, it is determined from the input array.\n    \n    Returns:\n    list: A list of lists containing the one-hot encoded values.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([1, 0, 3, 2])"}
{"task_id": 34, "completion_id": 6, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers to a one-hot encoded matrix.\n    \n    Args:\n    x : numpy.ndarray\n        1D input numpy array containing integer values.\n    n_col : int, optional\n        Number of columns for the one-hot encoded output. Defaults to None, \n        in which case, the number of columns is determined by the largest\n        integer value in x plus one.\n        \n    Returns:\n    list\n        A list of lists representing the one-hot encoded matrix.\n    \"\"\"\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot_encoded_matrix = np.eye(n_col)[x]\n    return one_hot_encoded_matrix.tolist()"}
{"task_id": 34, "completion_id": 7, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Converts a 1D numpy array of integers into a one-hot encoded matrix.\n    \n    Parameters:\n    - x: A 1D numpy array of integers.\n    - n_col: An optional integer representing the number of columns (categories) in the one-hot encoded matrix.\n             If not provided, it will be determined from the input array maximum value.\n    \n    Returns:\n    - A list containing the one-hot encoded representation of the input array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_matrix = np.eye(n_col)[x]\n    return one_hot_matrix.tolist()\nx = np.array([0, 2, 1, 2])"}
{"task_id": 34, "completion_id": 8, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on the input array x with optional n_col for the number of columns.\n    \n    Parameters:\n    x (numpy.ndarray): Input 1D numpy array of integers.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be determined from the input array.\n    \n    Returns:\n    list: A list of lists representation of the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_array = np.eye(n_col)[x]\n    return one_hot_array.tolist()"}
{"task_id": 34, "completion_id": 9, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (numpy.ndarray): 1D array of integer values to be encoded.\n    n_col (int, optional): Number of columns for the one-hot encoded array. Defaults to None.\n    \n    Returns:\n    list: Python list of one-hot encoded values.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([0, 1, 3, 2, 1])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array x into a diagonal matrix.\n    \n    Args:\n    x (np.array): A 1D numpy array.\n    \n    Returns:\n    A 2D numpy array representing the diagonal matrix, returned as a python list.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 1, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Given a 1D numpy array, convert it into a diagonal matrix and return it as a 2D list.\n    \n    Parameters:\n    x (numpy.ndarray): A 1D numpy array.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    diagonal_matrix_list = diagonal_matrix.tolist()\n    return diagonal_matrix_list\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 2, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    Args:\n    x (numpy.ndarray): A 1D numpy array representing a vector.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    n = x.size\n    diagonal_matrix = np.zeros((n, n))\n    np.fill_diagonal(diagonal_matrix, x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 3, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    :param x: 1D numpy array\n    :return: 2D numpy array representing the diagonal matrix as a list\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError('Input should be a 1D numpy array.')\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 35, "completion_id": 4, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix and returns it as a list of lists.\n    \n    Parameters:\n    x (np.array): A 1D numpy array representing the diagonal elements.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.zeros((len(x), len(x)))\n    np.fill_diagonal(diagonal_matrix, x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 5, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x: 1D numpy array\n    \n    Returns:\n    - 2D numpy array representing the diagonal matrix of x, returned as a python list.\n    \"\"\"\n    diag_matrix = np.zeros((len(x), len(x)))\n    np.fill_diagonal(diag_matrix, x)\n    return diag_matrix.tolist()\nx = np.array([1, 2, 3, 4])"}
{"task_id": 35, "completion_id": 6, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): Input 1D numpy array\n    \n    Returns:\n    list: A 2D numpy array as a list of lists (matrix represented as list), \n          where the input array forms the diagonal.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 7, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x (numpy.ndarray): 1D numpy array\n    \n    Returns:\n    - list: List form of the diagonal matrix\n    \"\"\"\n    x = np.array(x)\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 8, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix and return as a list.\n    \n    Parameters:\n    x (np.array): 1D numpy array\n    \n    Returns:\n    list: 2D list representing the diagonal matrix\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError('Input must be a 1D numpy array')\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 35, "completion_id": 9, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Takes a 1D numpy array and converts it into a diagonal matrix.\n    \n    Args:\n    x (numpy.ndarray): 1D numpy array to be converted into a diagonal matrix.\n    \n    Returns:\n    list: A 2D list representation of the diagonal matrix.\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError('Input must be a 1D numpy array.')\n    size = x.shape[0]\n    diagonal_matrix = np.zeros((size, size))\n    np.fill_diagonal(diagonal_matrix, x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): 1D array containing true labels.\n    y_pred (np.array): 1D array containing predicted labels.\n    \n    Returns:\n    float: The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be equal.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 1, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates the accuracy score given the true and predicted labels.\n    \n    :param y_true: 1D numpy array containing true labels.\n    :param y_pred: 1D numpy array containing predicted labels.\n    :return: Float accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Input values must be numpy arrays')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 1, 0, 0, 1])\ny_pred = np.array([0, 0, 1, 0, 1, 1])"}
{"task_id": 36, "completion_id": 2, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of the model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score as a float, rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 3, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculates the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): 1D numpy array containing true labels.\n    y_pred (np.array): 1D numpy array containing predicted labels.\n    \n    Returns:\n    float: Accuracy score as a float rounded to the nearest 4th decimal.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 2, 2, 2])\ny_pred = np.array([0, 0, 2, 2, 1])"}
{"task_id": 36, "completion_id": 4, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): 1D numpy array containing true labels.\n    y_pred (np.array): 1D numpy array containing predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Length of y_true and y_pred must be the same')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 36, "completion_id": 5, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): The true labels.\n    y_pred (np.array): The predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('y_true and y_pred must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 6, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score, rounded to the nearest 4th decimal.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must be of the same length.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 1, 1, 0])"}
{"task_id": 36, "completion_id": 7, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 1, 0, 1, 0, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 0, 1, 1, 1])"}
{"task_id": 36, "completion_id": 8, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): 1D array of true labels.\n    y_pred (np.array): 1D array of predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\ny_true = np.array([1, 0, 1, 1, 0, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1])"}
{"task_id": 36, "completion_id": 9, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n\n    Parameters:\n    - y_true: 1D numpy array of true labels.\n    - y_pred: 1D numpy array of predicted labels.\n\n    Returns:\n    - float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have the same length')\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for the input data.\n    \n    Parameters:\n    X (2D numpy array): The first data set.\n    Y (2D numpy array, optional): The second data set. If not provided, X will be used.\n    \n    Returns:\n    A 2D numpy array representing the correlation matrix as a python list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    std_outer_product = np.outer(std_X, std_Y)\n    std_outer_product[std_outer_product == 0] = np.nan\n    correlation_matrix = cov_matrix / std_outer_product\n    correlation_matrix = np.nan_to_num(correlation_matrix)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    return correlation_matrix_rounded.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 37, "completion_id": 1, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (2D numpy array): The dataset to calculate the correlation for.\n    Y (2D numpy array, optional): An optional second dataset to calculate the correlation against X. Defaults to None.\n    \n    Returns:\n    2D numpy array: The correlation matrix as a 2D numpy array, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    std_dev_X = np.std(X, axis=0, keepdims=True).T\n    std_dev_Y = np.std(Y, axis=0, keepdims=True)\n    correlation_matrix = np.divide(cov_matrix, np.dot(std_dev_X, std_dev_Y))\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    return correlation_matrix_rounded.tolist()"}
{"task_id": 37, "completion_id": 2, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between the columns of the 2D numpy array X.\n    Optionally calculates the correlation matrix between the columns of X and Y.\n    \n    Parameters:\n    X (2D numpy array): First dataset.\n    Y (2D numpy array, optional): Second dataset. If not provided, computes correlation matrix of X with itself.\n    \n    Returns:\n    2D numpy array: Correlation matrix, rounded to 4 decimal places, converted to a python list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    mean_X = np.mean(X, axis=0)\n    mean_Y = np.mean(Y, axis=0)\n    X_zero_mean = X - mean_X\n    Y_zero_mean = Y - mean_Y\n    cov_matrix = np.dot(X_zero_mean.T, Y_zero_mean) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0, ddof=1)\n    std_Y = np.std(Y, axis=0, ddof=1)\n    std_outer_product = np.outer(std_X, std_Y)\n    std_outer_product[std_outer_product == 0] = np.nan\n    correlation_matrix = cov_matrix / std_outer_product\n    correlation_matrix = np.round(correlation_matrix, 4).tolist()\n    return correlation_matrix\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 3, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a dataset. If the second dataset Y is not provided,\n    the correlation matrix for X with itself is calculated.\n    \n    Parameters:\n    - X: A 2D numpy array.\n    - Y: An optional 2D numpy array. If None, Y defaults to X.\n    \n    Returns:\n    - A 2D numpy array representing the correlation matrix between X and Y, as a Python list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    X_std = np.linalg.norm(X_centered, axis=0)\n    Y_std = np.linalg.norm(Y_centered, axis=0)\n    X_std[X_std == 0] = 1e-10\n    Y_std[Y_std == 0] = 1e-10\n    norms_matrix = np.outer(X_std, Y_std)\n    corr_matrix = cov_matrix / norms_matrix\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 4, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a dataset.\n    \n    Parameters:\n    X (2D numpy array): The first dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the function calculates the correlation matrix of X with itself.\n    \n    Returns:\n    2D numpy array: The correlation matrix of X with Y, or of X with itself if Y is not provided. Results are rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered) / (X_centered.shape[0] - 1)\n    X_std = np.std(X, axis=0, ddof=1)\n    Y_std = np.std(Y, axis=0, ddof=1)\n    std_outer_product = np.outer(X_std, Y_std)\n    correlation_matrix = cov_matrix / std_outer_product\n    return np.round(correlation_matrix, decimals=4).tolist()\nX = np.random.rand(5, 3)\nY = np.random.rand(5, 3)"}
{"task_id": 37, "completion_id": 5, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for the given numpy array(s).\n    If only one array is provided, it calculates the correlation matrix of the array with itself.\n    \n    Parameters:\n    X (numpy.ndarray): 2D numpy array.\n    Y (numpy.ndarray, optional): 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: The correlation matrix as a list (python list).\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    X_scaled = X_centered / np.std(X_centered, axis=0)\n    Y_scaled = Y_centered / np.std(Y_centered, axis=0)\n    correlation_matrix = np.dot(X_scaled.T, Y_scaled) / (X.shape[0] - 1)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[3, 2, 1], [6, 5, 4], [9, 8, 7]])"}
{"task_id": 37, "completion_id": 6, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X : 2D numpy array\n        The first dataset.\n    Y : 2D numpy array, optional\n        The second dataset. If not provided, it defaults to X.\n    \n    Returns:\n    corr_matrix : 2D numpy array\n        The correlation matrix between X and Y, or between X and itself.\n    \"\"\"\n    if Y is None:\n        Y = X\n    mean_X = np.mean(X, axis=0)\n    mean_Y = np.mean(Y, axis=0)\n    numerator = np.dot((X - mean_X).T, Y - mean_Y)\n    denominator_X = np.sqrt(np.sum((X - mean_X) ** 2, axis=0))\n    denominator_Y = np.sqrt(np.sum((Y - mean_Y) ** 2, axis=0))\n    corr_matrix = np.divide(numerator, np.outer(denominator_X, denominator_Y + 1e-08))\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 37, "completion_id": 7, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for the given dataset.\n    \n    Args:\n    X: A 2D numpy array.\n    Y: An optional 2D numpy array. If not provided, Y is set equal to X.\n    \n    Returns:\n    A 2D numpy array (correlation matrix) as a python list, rounded to 4 decimal places.\n    \"\"\"\n    assert X.ndim == 2, 'X should be a 2D numpy array'\n    if Y is None:\n        Y = X\n    else:\n        assert Y.ndim == 2, 'Y should be a 2D numpy array'\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    X_std = np.std(X_centered, axis=0, ddof=1)\n    Y_std = np.std(Y_centered, axis=0, ddof=1)\n    std_matrix = np.outer(X_std, Y_std)\n    corr_matrix = np.divide(cov_matrix, std_matrix, out=np.zeros_like(cov_matrix), where=std_matrix != 0)\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])"}
{"task_id": 37, "completion_id": 8, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (2D numpy array): First dataset.\n    Y (2D numpy array, optional): Second dataset. Defaults to None.\n    \n    Returns:\n    list: Correlation matrix rounded to 4 decimal places as a list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X = np.array(X)\n    Y = np.array(Y)\n    mean_X = np.mean(X, axis=0)\n    mean_Y = np.mean(Y, axis=0)\n    numerator = np.mean((X - mean_X) * (Y - mean_Y), axis=0)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    correlation_matrix = np.divide(numerator, std_X * std_Y)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4).tolist()\n    return correlation_matrix_rounded\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])"}
{"task_id": 37, "completion_id": 9, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array, the dataset.\n    - Y: 2D numpy array, optional dataset. If not provided, assume X.\n    \n    Returns:\n    - corr_matrix: a 2D numpy array representing the correlation matrix between X and Y.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    std_devs_X = np.std(X, axis=0, ddof=1)\n    std_devs_Y = np.std(Y, axis=0, ddof=1)\n    outer_product = np.outer(std_devs_X, std_devs_Y)\n    corr_matrix = np.nan_to_num(cov_matrix / outer_product)\n    corr_matrix = np.round(corr_matrix, 4)\n    return corr_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fits an AdaBoost classifier to the provided dataset.\n    \n    Args:\n    X : np.ndarray\n        The training data set of shape (n_samples, n_features).\n    y : np.ndarray\n        The training labels of shape (n_samples,).\n    n_clf : int\n        The number of classifiers to use in the boosting algorithm.\n        \n    Returns:\n    list\n        A list of dictionaries, each containing information about a single classifier\n        learned during the boosting process, including 'feature_idx', 'threshold', \n        'alpha', and 'sign'.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_feature_idx = None\n        best_threshold = None\n        best_sign = None\n        best_prediction = None\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            thresholds = np.unique(feature_values)\n            for threshold in thresholds:\n                for sign in [-1, 1]:\n                    y_pred = sign * (feature_values <= threshold)\n                    error = np.sum(sample_weights[y != y_pred])\n                    if error < min_error:\n                        min_error = error\n                        best_feature_idx = feature_idx\n                        best_threshold = threshold\n                        best_sign = sign\n                        best_prediction = y_pred\n        alpha = 0.5 * math.log((1 - min_error) / min_error)\n        sample_weights = sample_weights * np.exp(-alpha * y * best_prediction)\n        sample_weights = sample_weights / np.sum(sample_weights)\n        classifiers.append({'feature_idx': int(best_feature_idx), 'threshold': round(float(best_threshold), 4), 'alpha': round(float(alpha), 4), 'sign': int(best_sign)})\n    return classifiers"}
{"task_id": 38, "completion_id": 1, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = +np.inf\n        best_clf = (None, None, None, None)\n        for feature_idx in range(n_features):\n            column = X[:, feature_idx]\n            thresholds = np.unique(column)\n            for threshold in thresholds:\n                pred = np.where(column <= threshold, -1, 1)\n                err = np.sum(weights * (pred != y))\n                if err < best_error:\n                    best_error = err\n                    best_clf = (feature_idx, threshold, 1 if err < 0.5 else -1, np.log((1 - err) / err))\n        (idx, thrsh, sign, weight_clf) = best_clf\n        classifiers.append((idx, thrsh, sign, weight_clf))\n        pred = np.where(X[:, idx] <= thrsh, -1, 1) * sign\n        Z = np.sum(weights * np.exp(-sign * y * weight_clf))\n        weights *= np.exp(-sign * y * pred * weight_clf) / Z\n        if best_error < 1e-10:\n            break\n    classifiers = [(idx, np.round(thrsh, 4), sign, np.round(weight_clf, 4)) for (idx, thrsh, sign, weight_clf) in classifiers]\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]])\ny = np.array([1, 1, 1, 1, -1, -1, -1, -1])\nn_clf = 2\nclassifiers = adaboost_fit(X, y, n_clf)"}
{"task_id": 38, "completion_id": 2, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    best_classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_classifier = None\n        for feature in range(n_features):\n            thresholds = sorted(set(X[:, feature]))\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    predicted = np.where(X[:, feature] >= threshold, polarity, -polarity)\n                    error = np.sum(sample_weights * (predicted != y))\n                    if polarity == -1:\n                        error = 1 - error\n                    if error < best_error:\n                        best_error = error\n                        best_classifier = {'feature': feature, 'threshold': threshold, 'polarity': polarity, 'error': error}\n        if best_error == 0:\n            break\n        alpha = 0.5 * np.log((1 - best_error) / best_error)\n        best_classifier['alpha'] = alpha\n        best_classifiers.append(best_classifier)\n        predicted = np.where(X[:, best_classifier['feature']] >= best_classifier['threshold'], best_classifier['polarity'], -best_classifier['polarity'])\n        sample_weights *= np.exp(-alpha * predicted * y)\n        sample_weights /= np.sum(sample_weights)\n    return best_classifiers"}
{"task_id": 38, "completion_id": 3, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    classifiers = []\n    weights = np.full(n_samples, 1 / n_samples)\n    for _ in range(n_clf):\n        best = {'error': 1, 'clf': None}\n        for feature_idx in range(n_features):\n            feature_values = np.unique(X[:, feature_idx])\n            thresholds = (feature_values[:-1] + feature_values[1:]) / 2\n            for threshold in thresholds:\n                predict = np.where(X[:, feature_idx] > threshold, 1, -1)\n                error = sum(weights[y != predict])\n                if error < best['error']:\n                    best['error'] = error\n                    best['clf'] = {'feature': feature_idx, 'threshold': threshold, 'prediction': np.sign(sum(weights * y * (X[:, feature_idx] > threshold)))}\n        clf = best['clf']\n        weighted_error = best['error']\n        alpha = 0.5 * np.log((1 - weighted_error) / weighted_error)\n        predictors = np.where(X[:, clf['feature']] > clf['threshold'], clf['prediction'], -clf['prediction'])\n        weights *= np.exp(-alpha * y * predictors)\n        weights /= np.sum(weights)\n        clf['alpha'] = alpha\n        classifiers.append(clf)\n    return classifiers\nX = np.random.randn(100, 3)\ny = np.random.choice([-1, 1], 100)\nn_clf = 3"}
{"task_id": 38, "completion_id": 4, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = np.inf\n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            thresholds = np.unique(feature_values)\n            for threshold in thresholds:\n                pred = np.where(feature_values > threshold, 1, -1)\n                error = np.sum(weights * (pred != y))\n                if error > 0.5:\n                    error = 1 - error\n                    pred = np.where(feature_values > threshold, -1, 1)\n                if error < best_error:\n                    best_error = error\n                    best_pred = pred\n                    best_feature = feature\n                    best_threshold = threshold\n        beta = error / (1 - error)\n        alpha = np.log(1 / beta)\n        weights *= np.where(best_pred == y, 1, beta)\n        weights /= np.sum(weights)\n        classifiers.append({'feat': best_feature, 'thresh': best_threshold, 'alpha': round(float(alpha), 4)})\n    return classifiers\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\ny = np.array([-1, -1, -1, 1], dtype=np.float32)\nn_clf = 2"}
{"task_id": 38, "completion_id": 5, "solution": "import numpy as np\ndef calculate_error(targets, predictions, weights):\n    \"\"\"Calculate the weighted error.\"\"\"\n    return np.sum(weights * np.not_equal(targets, predictions).astype(int)) / np.sum(weights)\ndef stump_predict(X, feature, threshold):\n    \"\"\"Predict labels based on the feature and threshold.\"\"\"\n    return np.where(X[:, feature] > threshold, 1, -1)\ndef stump_fit(X, y, weights):\n    \"\"\"Fit a weak classifier (stump) to the data.\"\"\"\n    (best_feature, best_threshold, best_error) = (None, None, float('inf'))\n    for feature in range(X.shape[1]):\n        thresholds = np.unique(X[:, feature])\n        for threshold in thresholds:\n            predictions = stump_predict(X, feature, threshold)\n            error = calculate_error(y, predictions, weights)\n            if error < best_error:\n                (best_feature, best_threshold, best_error) = (feature, threshold, error)\n    return (best_feature, best_threshold, best_error)\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"Adaboost algorithm.\"\"\"\n    n_samples = X.shape[0]\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        (feature, threshold, error) = stump_fit(X, y, weights)\n        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n        prediction = stump_predict(X, feature, threshold)\n        weights *= np.exp(-alpha * y * prediction)\n        classifiers.append((feature, threshold, alpha))\n    return classifiers\nX = np.array([[0, 0], [1, 1], [2, 0], [3, 1], [4, 0]])\ny = np.array([-1, -1, 1, 1, 1])\nn_clf = 3"}
{"task_id": 38, "completion_id": 6, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1.0 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        (best_error, best_clf) = (math.inf, None)\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                predictions = np.where(X[:, feature] <= threshold, -1, 1)\n                error = np.sum(weights * (predictions != y))\n                if error > 0.5:\n                    predictions = -predictions\n                    error = 1 - error\n                if error < best_error:\n                    best_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'polarity': np.sign(predictions[0]), 'alpha': 0.5 * np.log((1 - error) / max(error, 1e-10))}\n        incorrect = y != (X[:, best_clf['feature']] <= best_clf['threshold']) * np.sign(best_clf['polarity'])\n        Z = 2 * np.sqrt(best_clf['alpha'] * (1 - best_clf['alpha']))\n        weights = weights / Z * np.exp(-best_clf['alpha'] * incorrect)\n        classifiers.append(best_clf)\n    for clf in classifiers:\n        clf['alpha'] = round(clf['alpha'], 4)\n    return classifiers\nX = np.array([[0], [1], [2], [3]])\ny = np.array([1, 1, -1, -1])\nn_clf = 2"}
{"task_id": 38, "completion_id": 7, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the AdaBoost fit method to generate weak classifiers.\n    \n    Parameters:\n    -----------\n    X : numpy.ndarray\n        2D numpy array representing the dataset of shape (n_samples, n_features).\n    y : numpy.ndarray\n        1D numpy array representing the labels of shape (n_samples,).\n    n_clf : int\n        Number of weak classifiers.\n    \n    Returns:\n    --------\n    list :\n        A list of dictionaries containing the parameters of each weak classifier:\n        [feature_index, threshold, polarity, alpha].\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            thresholds = np.unique(X[:, feature_i])\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    pred = np.where(y * polarity > 0, 1, -1)\n                    pred[y * polarity < 0] = -1\n                    pred[X[:, feature_i] < threshold] *= -1\n                    weighted_error = sum(weights[y != pred])\n                    if weighted_error < min_error:\n                        min_error = weighted_error\n                        clf_params = {'feature_idx': feature_i, 'threshold': threshold, 'polarity': polarity, 'weight': weighted_error}\n        error = clf_params['weight']\n        alpha = 0.5 * np.log((1.0 - error) / max(error, 1e-10))\n        y_pred = np.array([1 if X[i, clf_params['feature_idx']] > clf_params['threshold'] else -1 for i in range(n_samples)])\n        y_pred = y_pred * clf_params['polarity']\n        weights *= np.exp(-alpha * y * y_pred)\n        weights /= weights.sum()\n        clf_params['alpha'] = round(alpha, 4)\n        classifiers.append(clf_params)\n    return classifiers\nX = np.random.rand(100, 2)\ny = np.random.choice([-1, 1], 100)\nn_clf = 10"}
{"task_id": 38, "completion_id": 8, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement AdaBoost classifier training.\n    \n    Parameters:\n    X (np.ndarray): Training vectors of shape (n_samples, n_features).\n    y (np.ndarray): Target values of shape (n_samples).\n    n_clf (int): Number of weak classifiers in the boost ensemble.\n    \n    Returns:\n    classifiers (list): Each element is a dictionary with 'dim', 'polarity',\n                        'threshold', and 'alpha' of the weak classifier.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = np.inf\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                for polarity in [-1, 1]:\n                    prediction = np.where(polarity * X[:, feature] > polarity * threshold, 1, -1)\n                    error = np.sum(weights[y != prediction])\n                    if error < best_error:\n                        best_error = error\n                        best_feature = feature\n                        best_threshold = threshold\n                        best_polarity = polarity\n                        best_prediction = prediction.copy()\n        alpha = 0.5 * np.log((1 - best_error) / best_error)\n        weights = weights * np.exp(-alpha * y * best_prediction)\n        weights = weights / np.sum(weights)\n        classifiers.append({'dim': best_feature, 'polarity': best_polarity, 'threshold': round(best_threshold, 4), 'alpha': round(alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 9, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        feature_idx = None\n        threshold = None\n        error = float('inf')\n        pred_sign = None\n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            thresholds = np.unique(feature_values)\n            for t in thresholds:\n                pred = np.where(feature_values < t, -1, 1)\n                misclassified = pred != y\n                err = np.sum(sample_weights[misclassified])\n                if err < error:\n                    error = err\n                    feature_idx = feature\n                    threshold = t\n                    pred_sign = pred\n        alpha = 0.5 * np.log((1 - error) / error)\n        classifiers.append({'feature': feature_idx, 'threshold': threshold, 'alpha': np.round(alpha, 4), 'pred_sign': pred_sign})\n        sample_weights *= np.exp(-alpha * y * pred_sign)\n        sample_weights /= np.sum(sample_weights)\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of each element in the input list of scores.\n    The log softmax function is defined as:\n    log_softmax(x_i) = x_i - log(sum(exp(x_j) for all j))\n    :param scores: A list of floating-point numbers representing scores to compute the log-softmax.\n    :return: A list of floats representing the log-softmax values for each input score.\n    \"\"\"\n    scores = np.array(scores)\n    scores -= np.max(scores)\n    exp_scores = np.exp(scores)\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return np.round(log_softmax_scores, decimals=4).tolist()"}
{"task_id": 39, "completion_id": 1, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    Args:\n    - scores (list): A list of numbers representing scores.\n    \n    Returns:\n    - list: A list of numbers representing the log-softmax of the input scores, rounded to 4 decimal places.\n    \"\"\"\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    normalized_scores = scores_array - max_score\n    exp_scores = np.exp(normalized_scores)\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=0)\n    log_scores = np.log(softmax_scores)\n    rounded_log_scores = np.round(log_scores, 4)\n    return rounded_log_scores.tolist()"}
{"task_id": 39, "completion_id": 2, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of the given scores.\n    \n    The log-softmax function is computed in two steps:\n    1. Compute the softmax which is np.exp(score) / sum(np.exp(scores)).\n    2. Take the natural log of the softmax values.\n    To ensure numerical stability, we subtract the max score from all scores before exponentiating.\n    \n    Parameters:\n    scores (list): A list of scores for each class.\n    \n    Returns:\n    list: The log-softmax of the input scores rounded to 4 decimal places.\n    \"\"\"\n    scores = np.asarray(scores)\n    max_score = np.max(scores, axis=-1, keepdims=True)\n    exp_scores = np.exp(scores - max_score)\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    log_softmax_scores = np.log(softmax_scores)\n    return [round(x, 4) for x in log_softmax_scores.tolist()]"}
{"task_id": 39, "completion_id": 3, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores. The log-softmax is computed\n    to maintain numerical stability when converting raw scores to probabilities.\n    \n    :param scores: List of scores.\n    :return: List of log-softmax values for each score, rounded to 4 decimal places.\n    \"\"\"\n    scores_np = np.array(scores)\n    stable_scores = scores_np - np.max(scores_np)\n    softmax_values = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax_values)\n    rounded_values = np.round(log_softmax_values, 4)\n    return rounded_values.tolist()\nscores = [1, 2, 3, 4, 5]"}
{"task_id": 39, "completion_id": 4, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    Args:\n    - scores (list): A list of scores.\n    \n    Returns:\n    - list: The log-softmax of the list of scores, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    softmax_values = np.exp(scores - np.max(scores)) / np.sum(np.exp(scores - np.max(scores)))\n    log_softmax_values = np.log(softmax_values)\n    return [round(float(x), 4) for x in log_softmax_values.tolist()]"}
{"task_id": 39, "completion_id": 5, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    Parameters:\n    - scores (list): A list of scores (floats).\n    \n    Returns:\n    list: The log-softmax of the input scores rounded to 4 decimal places.\n    \"\"\"\n    scores_array = np.array(scores)\n    scores_array -= np.max(scores_array)\n    exp_scores = np.exp(scores_array)\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return [round(x, 4) for x in log_softmax_scores.tolist()]\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 39, "completion_id": 6, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Calculates the log-softmax of a list of scores.\n    \n    The log-softmax of a score s_i is defined as:\n    log_softmax(s_i) = log( exp(s_i) / (sum of exp of all scores) )\n    \n    Args:\n    scores (list): A list of scores for which the log-softmax is to be computed.\n    \n    Returns:\n    list: A list containing the log-softmax values for the input scores, rounded to 4 decimal places.\n    \"\"\"\n    scores_arr = np.array(scores)\n    log_softmax_values = scores_arr - np.log(np.sum(np.exp(scores_arr)))\n    return [round(x, 4) for x in log_softmax_values.tolist()]"}
{"task_id": 39, "completion_id": 7, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of the given scores.\n    \n    :param scores: A list of scores\n    :return: A list of the log-softmax values, rounded to 4 decimal places\n    \"\"\"\n    scores_array = np.array(scores)\n    log_softmax_values = scores_array - np.max(scores_array)\n    log_softmax_values = log_softmax_values - np.log(np.sum(np.exp(log_softmax_values)))\n    return np.round(log_softmax_values, decimals=4).tolist()"}
{"task_id": 39, "completion_id": 8, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The function takes a list of numerical scores, computes the softmax\n    to convert scores into probabilities, then takes the natural log\n    of these probabilities to output their log-softmax values, ensuring\n    numerical stability.\n    \n    Args:\n        scores (list): A list of numerical scores.\n        \n    Returns:\n        list: A list of the same length as the input, containing the log-softmax\n              values of the input scores, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    numerator = np.exp(scores - max_score)\n    denominator = np.sum(numerator)\n    softmax = numerator / denominator\n    log_softmax_values = np.log(softmax)\n    log_softmax_rounded = np.round(log_softmax_values, 4)\n    return log_softmax_rounded.tolist()"}
{"task_id": 39, "completion_id": 9, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Calculates the log-softmax of a list of scores.\n    The log-softmax is computed as log softmax = score - log(sum(exp(scores))),\n\n    where softmax is a function that rescales elements to the range (0, 1) and \n    ensures the elements sum up to 1, thus forming a probability distribution.\n\n    Args:\n    scores: A list of real numbers representing the raw predicted scores from a model.\n\n    Returns:\n    A list of numbers representing the log-softmax of the input scores, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    stable_scores = scores - max_score\n    exp_scores = np.exp(stable_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    softmax_scores = exp_scores / sum_exp_scores\n    log_softmax_scores = np.log(softmax_scores)\n    return np.round(log_softmax_scores, decimals=4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.optimizer_W.update(self.W, grad_w)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        return accum_grad.dot(W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 1, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optim = None\n        self.w0_optim = None\n\n    def initialize(self, optimizer):\n        input_shape = self.input_shape\n        limit = 1 / math.sqrt(input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_optim = copy.deepcopy(optimizer)\n        self.w0_optim = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = self.layer_input.T.dot(accum_grad)\n        self.W = self.W_optim.update(self.W, W_grad)\n        self.w0 = self.w0_optim.update(self.w0, w0_grad)\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 2, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.b_w = None\n        self.b_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.b_w = copy.deepcopy(optimizer)\n        self.b_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        accum_grad = np.dot(accum_grad, W.T)\n        if self.trainable:\n            grad_wrt_weight = np.dot(self.layer_input.T, accum_grad)\n            grad_wrt_bias = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.b_w.update(self.W, grad_wrt_weight)\n            self.w0 = self.b_w0.update(self.w0, grad_wrt_bias)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 3, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0]) if self.input_shape else 1\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = Optimizer()\n        self.optimizer_w0 = Optimizer()\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w_grad = np.dot(self.layer_input.T, accum_grad)\n        self.optimizer_W.update(self.W, w_grad)\n        self.optimizer_w0.update(self.w0, np.sum(accum_grad, axis=0, keepdims=True))\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\nclass Optimizer:\n\n    def update(self, param, grad):\n        pass"}
{"task_id": 40, "completion_id": 4, "solution": "import numpy as np\nimport math\nclass Optimizer:\n\n    def __init__(self):\n        self.update_rule = lambda x, dx: x - dx\n\n    def update(self, params, grad):\n        return self.update_rule(params, grad)\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.n_units = n_units\n        self.input_shape = input_shape\n        self.layer_input = None\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = Optimizer()\n        self.w0_optimizer = Optimizer()\n\n    def initialize(self):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.W_optimizer = Optimizer()\n        self.w0_optimizer = Optimizer()\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        wgrad = self.layer_input.T.dot(accum_grad)\n        bgrad = np.sum(accum_grad, axis=0)\n        if self.trainable:\n            self.W = self.W_optimizer.update(self.W, wgrad)\n            self.w0 = self.w0_optimizer.update(self.w0, bgrad)\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 5, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Optimizer(object):\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, W, grad):\n        return W - self.learning_rate * grad\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = Optimizer()\n\n    def initialize(self):\n        input_shape = self.input_shape\n        lim = 1 / math.sqrt(input_shape[0])\n        self.W = np.random.uniform(-lim, lim, (input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(self.optimizer)\n        self.w0_opt = copy.deepcopy(self.optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X @ self.W + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        grad_w = self.layer_input.T @ accum_grad\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        accum_grad = accum_grad @ W.T\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 6, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.sum(self.W.size) + np.sum(self.w0.size)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        grad_w = self.layer_input.T.dot(accum_grad)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, w0)\n        accum_grad = accum_grad.dot(self.W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 7, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        super(Dense, self).__init__()\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer=SGD()):\n        input_size = np.prod(self.input_shape)\n        limit = 1 / math.sqrt(input_size)\n        self.W = np.random.uniform(-limit, limit, size=(input_size, self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.input_shape) * self.n_units + self.n_units\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        X = self.layer_input\n        W = self.W\n        if X.ndim > 2:\n            X = X.reshape(X.shape[0], -1)\n        if accum_grad.ndim > 2:\n            accum_grad = accum_grad.reshape(accum_grad.shape[0], -1)\n        W_grad = np.dot(X.T, accum_grad)\n        self.w0 = self.w0_opt.update(self.w0, w0_grad)\n        self.W = self.W_opt.update(self.W, W_grad)\n        return np.dot(accum_grad, W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\nclass SGD:\n\n    def __init__(self, learning_rate=0.01, momentum=0.9):\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n\n    def update(self, param, grad):\n        if self.velocity is None:\n            self.velocity = np.zeros_like(param)\n        self.velocity = self.momentum * self.velocity - self.learning_rate * grad\n        return param + self.velocity"}
{"task_id": 40, "completion_id": 8, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.input_shape = input_shape\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizerW = copy.copy(optimizer)\n        self.optimizerw0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.sum(self.W.size) + np.sum(self.w0.size)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X @ self.W + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T @ accum_grad\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.optimizerW.update(self.W, grad_w)\n            self.optimizerw0.update(self.w0, grad_w0)\n            self.W = self.optimizerW.current_parameters\n            self.w0 = self.optimizerw0.current_parameters\n        accum_grad = accum_grad @ W.T\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)\nclass SimpleSGD:\n\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n        self.current_parameters = None\n\n    def update(self, parameters, gradient):\n        self.current_parameters = parameters - self.learning_rate * gradient\ninput_shape = (32, 100)\nn_units = 128\noptimizer = SimpleSGD(0.01)\naccum_grad = np.random.rand(*forward_output.shape)"}
{"task_id": 40, "completion_id": 9, "solution": "import numpy as np\nimport math\nclass Layer:\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X @ self.W + self.w0\n\n    def backward_pass(self, accum_grad):\n        weights = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T @ accum_grad\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        accum_grad = accum_grad @ weights.T\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    Applies a 2D convolution on the input matrix with the provided kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix for the convolution.\n    kernel (np.ndarray): The convolution kernel.\n    padding (int): The padding size to be applied to the input matrix.\n    stride (int): The stride length for convolution.\n    \n    Returns:\n    list: The result of the convolution operation after reshaping to match output requirements.\n    \"\"\"\n    padded_input = np.pad(input_matrix, padding, 'constant')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            row = i * stride\n            col = j * stride\n            window = padded_input[row:row + kernel_height, col:col + kernel_width]\n            scaling = (window * kernel).sum()\n            output_matrix[i, j] = round(scaling, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 1, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Performs a 2D convolution on the input_matrix using the provided kernel and stride.\n    \n    :param input_matrix: 2D numpy array of the input matrix.\n    :param kernel: 2D numpy array of the convolutional kernel.\n    :param padding: Integer value indicating padding size.\n    :param stride: Integer value indicating the stride size.\n    :return: A list of the output matrix after applying the convolution.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    padded_input_matrix = np.pad(input_matrix, padding, mode='constant')\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for y in range(0, output_height * stride, stride):\n        for x in range(0, output_width * stride, stride):\n            region = padded_input_matrix[y:y + kernel_height, x:x + kernel_width]\n            output_matrix[y // stride, x // stride] = np.sum(region * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 2, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    A simple 2D convolutional layer function.\n    \n    Parameters:\n    - input_matrix: A 2D numpy.ndarray representing the input matrix.\n    - kernel: A 2D numpy.ndarray representing the convolutional kernel.\n    - padding: An integer value indicating the padding size.\n    - stride: An integer value indicating the stride size.\n\n    Returns:\n    - output_matrix: A list resulting from applying the 2D convolution on the input matrix with the specified kernel, padding, and stride.\n    \"\"\"\n    input_matrix = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    (in_rows, in_cols) = input_matrix.shape\n    (kern_rows, kern_cols) = kernel.shape\n    out_rows = (in_rows - kern_rows) // stride + 1\n    out_cols = (in_cols - kern_cols) // stride + 1\n    output_matrix = np.zeros((out_rows, out_cols))\n    for row in range(0, out_rows, stride):\n        for col in range(0, out_cols, stride):\n            patch = input_matrix[row:row + kern_rows, col:col + kern_cols]\n            output_matrix[row, col] = np.sum(patch * kernel)\n    return np.around(output_matrix, decimals=4).tolist()"}
{"task_id": 41, "completion_id": 3, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Apply a 2D convolution over an input matrix with a specified kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (ndarray): Input matrix over which the convolution is performed.\n    kernel (ndarray): Convolutional kernel.\n    padding (int): Size of padding to add to the input matrix.\n    stride (int): Step size to move the kernel over the input.\n    \n    Returns:\n    list: Result of the 2D convolution as a python list.\n    \"\"\"\n    input_padded = np.pad(input_matrix, ((padding, padding), (padding, padding)), 'constant', constant_values=0)\n    input_height = input_padded.shape[0]\n    input_width = input_padded.shape[1]\n    kernel_height = kernel.shape[0]\n    kernel_width = kernel.shape[1]\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    result = np.zeros((output_height, output_width))\n    for row in range(0, output_height):\n        for col in range(0, output_width):\n            row_start = row * stride\n            row_end = row_start + kernel_height\n            col_start = col * stride\n            col_end = col_start + kernel_width\n            current_region = input_padded[row_start:row_end, col_start:col_end]\n            conv_sum = np.sum(current_region * kernel)\n            result[row, col] = round(conv_sum, 4)\n    return result.tolist()\ninput_matrix = np.array([[1, 2, 1], [0, 1, 0], [2, 1, 0]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 4, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"Perform 2D convolution with numpy arrays.\"\"\"\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for y in range(0, output_height):\n        for x in range(0, output_width):\n            y_start = y * stride\n            y_end = y_start + kernel_height\n            x_start = x * stride\n            x_end = x_start + kernel_width\n            output_matrix[y, x] = np.sum(padded_input[y_start:y_end, x_start:x_end] * kernel)\n    rounded_output = np.round(output_matrix, decimals=4)\n    return rounded_output.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1"}
{"task_id": 41, "completion_id": 5, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    if not input_matrix.shape or not kernel.shape:\n        raise ValueError('Input_matrix and kernel must have valid dimensions.')\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    input_padded = np.pad(input_matrix, padding, mode='constant')\n    output_height = int((input_height - kernel_height + 2 * padding) / stride) + 1\n    output_width = int((input_width - kernel_width + 2 * padding) / stride) + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            row_start = i * stride\n            row_end = row_start + kernel_height\n            col_start = j * stride\n            col_end = col_start + kernel_width\n            current_slice = input_padded[row_start:row_end, col_start:col_end]\n            output_matrix[i, j] = np.sum(current_slice * kernel)\n    output_matrix_rounded = np.round(output_matrix, 4)\n    return output_matrix_rounded.tolist()\ninput_matrix = np.array([[1.0, 2.0], [3.0, 4.0]])\nkernel = np.array([[1.0, 1.0], [-1.0, -1.0]])\npadding = 0\nstride = 1"}
{"task_id": 41, "completion_id": 6, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution on the input_matrix using the provided kernel, padding and stride. \n    \n    Parameters:\n    input_matrix (np.ndarray): 2D array representing the data to apply the convolution on.\n    kernel (np.ndarray): 2D array representing the convolutional kernel.\n    padding (int): Number of zero-valued layers added to the edges of the input_matrix.\n    stride (int): The stride or step size for the convolution operation.\n    \n    Returns:\n    np.ndarray: The output of the convolution (as a list after reshaping).\n    \"\"\"\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (kernel_height, kernel_width) = kernel.shape\n    (input_height, input_width) = padded_input.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output = []\n    for i in range(0, input_height - kernel_height + 1, stride):\n        row_output = []\n        for j in range(0, input_width - kernel_width + 1, stride):\n            window = padded_input[i:i + kernel_height, j:j + kernel_width]\n            conv_sum = round(np.sum(window * kernel), 4)\n            row_output.append(conv_sum)\n        output.append(row_output)\n    return np.array(output).tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 41, "completion_id": 7, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    This function applies a 2D convolution on the input_matrix with the given kernel,\n    padding, and stride. The output is rounded to the nearest 4th decimal and\n    converted to a list.\n    \n    :param input_matrix: A 2D numpy array on which the convolution is to be performed.\n    :param kernel: A 2D numpy array representing the convolutional kernel.\n    :param padding: An integer indicating the number of rows/columns of zeros to be\n                    added to the input_matrix's perimeter.\n    :param stride: An integer indicating the step size for the kernel movement.\n    :return: A list with the output of the convolution operation.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    input_matrix_padded = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for h in range(0, output_height, stride):\n        for w in range(0, output_width, stride):\n            h_start = h\n            h_end = h_start + kernel_height\n            w_start = w\n            w_end = w_start + kernel_width\n            receptive_field = input_matrix_padded[h_start:h_end, w_start:w_end]\n            output_matrix[h, w] = np.round(np.sum(receptive_field * kernel), decimals=4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[3, 2, 5], [1, 6, 4], [2, 4, 3]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1"}
{"task_id": 41, "completion_id": 8, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution given an input matrix, kernel, padding, and stride.\n    Convolves the input_matrix with the kernel using the specified padding and stride.\n    Round the results to 4 decimal places and return them.\n\n    :param input_matrix: 2D numpy array\n    :param kernel: 2D numpy array\n    :param padding: int, number of zeros to add to the border of the input\n    :param stride: int, size of the stride for the kernel\n    :return: 2D numpy array as a list, post-convolution with the given parameters\n    \"\"\"\n    padded_input = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    (input_h, input_w) = padded_input.shape\n    (kernel_h, kernel_w) = kernel.shape\n    output_h = (input_h - kernel_h) // stride + 1\n    output_w = (input_w - kernel_w) // stride + 1\n    output_matrix = np.zeros((output_h, output_w))\n    for i in range(0, input_h - kernel_h + 1, stride):\n        for j in range(0, input_w - kernel_w + 1, stride):\n            conv_region = padded_input[i:i + kernel_h, j:j + kernel_w]\n            output_matrix[i // stride, j // stride] = np.sum(kernel * conv_region)\n    output_matrix_rounded = np.round(output_matrix, decimals=4)\n    return output_matrix_rounded.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)\nkernel = np.array([[1, -1], [-1, 1]], dtype=float)"}
{"task_id": 41, "completion_id": 9, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"Applies a 2D convolution operation on the input_matrix using the specified kernel, padding and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix for the convolution.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): Number of padding layers added to the input matrix.\n    stride (int): The stride for moving the convolution kernel over the input.\n    \n    Returns:\n    list: The result of the convolution operation as a list.\n    \"\"\"\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = int((input_height + 2 * padding - kernel_height) / stride) + 1\n    output_width = int((input_width + 2 * padding - kernel_width) / stride) + 1\n    output_matrix = np.zeros((output_height, output_width))\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    for h in range(output_height):\n        for w in range(output_width):\n            h_start = h * stride\n            h_end = h_start + kernel_width\n            w_start = w * stride\n            w_end = w_start + kernel_height\n            output_matrix[h, w] = np.sum(padded_input[h_start:h_end, w_start:w_end] * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function applied on the input.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 1, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function.\n\n    Args:\n    z (float): A single floating-point number.\n\n    Returns:\n    float: The input z if it's greater than 0, otherwise 0.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 2, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Apply the Rectified Linear Unit (ReLU) activation function to the input value.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The result of applying the ReLU function to the input value.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 3, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n\n    Parameters:\n    z (float): A float number to which the ReLU function is applied.\n\n    Returns:\n    float: The result of applying the ReLU function to z.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 4, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) activation function to a single input.\n    \n    The ReLU function is defined as 0 for input less than 0, and the input itself for input greater or equal to 0.\n    \n    Parameters:\n    z (float): A single float value input to the ReLU function.\n    \n    Returns:\n    float: The result of applying the ReLU function to the input.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 5, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 6, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The result after applying the ReLU function.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 7, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The rectified value, which is the maximum of 0 and the input z.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 8, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A single float input to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 42, "completion_id": 9, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the ReLU activation function to the input value.\n    \n    Parameters:\n    z (float): The input value to apply the ReLU function on.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given input matrix X, coefficient vector w,\n    true labels y_true, and regularization parameter alpha.\n\n    Parameters:\n    - X: np.ndarray, feature matrix.\n    - w: np.ndarray, vector of coefficients.\n    - y_true: np.ndarray, true labels.\n    - alpha: float, regularization parameter.\n\n    Returns:\n    - float: Ridge Regression loss.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    y_pred = X @ w\n    mse = np.sum((y_pred - y_true) ** 2) / n_samples\n    regularization = alpha / (2 * n_samples) * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 1, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix.\n    w (np.ndarray): Coefficients.\n    y_true (np.ndarray): True labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: Ridge loss.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 2, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the loss for Ridge Regression.\n\n    Parameters:\n    - X: np.ndarray, Feature matrix of shape (n_samples, n_features)\n    - w: np.ndarray, Coefficients of shape (n_features,)\n    - y_true: np.ndarray, True labels of shape (n_samples,)\n    - alpha: float, Regularization parameter\n\n    Returns:\n    - float, The computed Ridge loss\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.dot(w, w)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 3, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge loss function which includes MSE and a regularization term.\n\n    Parameters:\n    - X (np.ndarray): 2D numpy array representing the feature matrix.\n    - w (np.ndarray): 1D numpy array representing the coefficients.\n    - y_true (np.ndarray): 1D numpy array representing the true labels.\n    - alpha (float): Regularization parameter.\n\n    Returns:\n    float: The computed Ridge loss.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg = alpha * np.sum(w ** 2)\n    total_loss = mse / 2 + reg / 2\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 4, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Loss for a given set of parameters.\n    \n    Ridge Loss is defined as:\n    Ridge Loss = MSE + alpha * (L2 Norm of w)\n    where,\n    MSE = Mean Squared Error\n    L2 Norm of w = sum of the squares of the coefficients\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The computed Ridge Loss rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_norm = np.sum(w ** 2)\n    ridge_loss = mse + alpha * l2_norm\n    return round(ridge_loss, 4)"}
{"task_id": 43, "completion_id": 5, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given the feature matrix X, coefficients w, true labels y_true,\n    and regularization parameter alpha. The loss is the sum of MSE and regularization term.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array, feature matrix.\n    w (np.ndarray): 1D numpy array, coefficients.\n    y_true (np.ndarray): 1D numpy array, true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: Ridge Regression loss.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 6, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss, which is the sum of the Mean Squared Error (MSE)\n    and a regularization term proportional to the square of the L2 norm of the coefficients.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix.\n    w (np.ndarray): The coefficients for the feature matrix.\n    y_true (np.ndarray): The true labels.\n    alpha (float): The regularization parameter.\n    \n    Returns:\n    float: The Ridge Regression loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_reg = alpha * np.sum(w ** 2)\n    loss = mse + l2_reg\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 7, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute Ridge Regression loss.\n    \n    Parameters:\n    - X: np.ndarray, shape (n_samples, n_features), feature matrix.\n    - w: np.ndarray, shape (n_features,), coefficients.\n    - y_true: np.ndarray, shape (n_samples,), true labels.\n    - alpha: float, regularization parameter.\n    \n    Returns:\n    - loss: float, Ridge Regression loss.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_reg = np.sum(w ** 2)\n    loss = mse + alpha * l2_reg\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 8, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculates the Ridge Regression loss for the given inputs.\n    \n    Parameters:\n    X: np.ndarray - 2D numpy array of feature variables.\n    w: np.ndarray - 1D numpy array of coefficients.\n    y_true: np.ndarray - 1D numpy array of the real values.\n    alpha: float - Regularization parameter.\n    \n    Returns:\n    float - The Ridge loss, which is the sum of the Mean Squared Error (MSE) and the L2 regularization term.\n    \"\"\"\n    y_pred = X.dot(w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * w.T.dot(w)\n    ridge_reg_loss = mse + reg_term\n    return round(ridge_reg_loss, 4)"}
{"task_id": 43, "completion_id": 9, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n\n    Parameters:\n    - X : np.ndarray, feature matrix\n    - w : np.ndarray, coefficient vector\n    - y_true : np.ndarray, true labels\n    - alpha : float, regularization parameter\n\n    Returns:\n    - float, the Ridge Regression loss\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\nw = np.array([1.0, 2.0])\ny_true = np.array([6, 8, 9, 10])\nalpha = 0.1"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the Leaky ReLU function.\n    alpha (float): The slope for the negative part of the function. Defaults to 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 1, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky ReLU activation function.\n    \n    Args:\n        z (float): The input to the activation function.\n        alpha (float): The slope for the Leaky ReLU function for z < 0. Defaults to 0.01.\n    \n    Returns:\n        float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 2, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) function defined as:\n    f(z) = z if z > 0 else alpha * z\n    \n    Parameters:\n    z (float): The input value to the Leaky ReLU function.\n    alpha (float): The slope of the function for z < 0. Default is 0.01.\n    \n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 3, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for the negative input range. Default is 0.01.\n    \n    Returns:\n    float|int: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 4, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Args:\n    z (float): The input value.\n    alpha (float): The slope for the negative input range. Default is 0.01.\n    \n    Returns:\n    float: The Leaky ReLU value.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 5, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope of the function for z < 0. Defaults to 0.01.\n    \n    Returns:\n    float: The Leaky ReLU of z.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 6, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): Input value.\n    alpha (float): Slope of the activation for input < 0. Defaults to 0.01.\n    \n    Returns:\n    float: Output of the Leaky ReLU activation function for the input value.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 7, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the function.\n    alpha (float): The leakiness of the activation, i.e., the slope for negative input values.\n    \n    Returns:\n    float: The output of the Leaky ReLU function for input z.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 44, "completion_id": 8, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value to the function.\n    - alpha (float): The slope of the function for input less than zero.\n    \n    Returns:\n    - float: The output of the Leaky ReLU function for input z.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 44, "completion_id": 9, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input to the leaky ReLU function.\n    - alpha (float): The slope for the leak, applied to negative inputs. Defaults to 0.01.\n    \n    Returns:\n    - float: The output of the leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    The linear kernel is defined as the dot product of the two vectors.\n    \n    :param x1: A numpy array of the first input vector\n    :param x2: A numpy array of the second input vector\n    :return: The dot product of x1 and x2\n    \"\"\"\n    if not (isinstance(x1, np.ndarray) and isinstance(x2, np.ndarray)):\n        raise TypeError('Both x1 and x2 inputs must be numpy arrays.')\n    if x1.shape[0] != x2.shape[0]:\n        raise ValueError('The input vectors must have the same length.')\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 1, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product (inner product) of two vectors.\n    \n    Parameters:\n    x1 (np.ndarray): A vector of shape (n_features,).\n    x2 (np.ndarray): A vector of shape (n_features,).\n    \n    Returns:\n    float: The linear kernel value between x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 2, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Args:\n        x1 (np.array): The first input vector.\n        x2 (np.array): The second input vector.\n        \n    Returns:\n        float: The linear kernel computed as the dot product of x1 and x2.\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    linear_kernel = np.dot(x1, x2)\n    return linear_kernel\nx1 = [1, 2, 3]\nx2 = [4, 5, 6]"}
{"task_id": 45, "completion_id": 3, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    - x1: A numpy array of shape (n_features, ) representing the first vector.\n    - x2: A numpy array of shape (n_features, ) representing the second vector.\n    \n    Returns:\n    - The linear kernel value as a float, which is the dot product of x1 and x2.\n    \n    Raises:\n    - ValueError: If x1 and x2 have different dimensions.\n    \"\"\"\n    if x1.shape != x2.shape:\n        raise ValueError('x1 and x2 must have the same length')\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 4, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors.\n    \n    The linear kernel is defined as the dot product of the two vectors.\n    \n    Parameters:\n    - x1: numpy array, first input vector.\n    - x2: numpy array, second input vector.\n    \n    Returns:\n    - The dot product of x1 and x2, a scalar value.\n    \"\"\"\n    if not isinstance(x1, np.ndarray):\n        x1 = np.array(x1)\n    if not isinstance(x2, np.ndarray):\n        x2 = np.array(x2)\n    if x1.shape != x2.shape:\n        raise ValueError('Input vectors must be of the same dimension.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 5, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product between the two vectors.\n    \n    Parameters:\n    x1 (numpy.ndarray): First input vector.\n    x2 (numpy.ndarray): Second input vector.\n    \n    Returns:\n    int: The computed linear kernel value.\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    kernel_val = np.dot(x1, x2)\n    return kernel_val\nx1 = np.array([1, 2, 3])\nx2 = np.array([3, 2, 1])"}
{"task_id": 45, "completion_id": 6, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Args:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value which is the dot product of x1 and x2.\n    \n    Note:\n    The length of vectors x1 and x2 must be equal.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError('Input vectors must have the same length.')\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 7, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (numpy array): Input vector 1.\n    x2 (numpy array): Input vector 2.\n    \n    Returns:\n    float: The computed linear kernel (dot product of x1 and x2).\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 8, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between vectors x1 and x2.\n    \n    Args:\n        x1 (np.ndarray): The first input vector.\n        x2 (np.ndarray): The second input vector.\n    \n    Returns:\n        float: The result of the linear kernel computation.\n    \n    Raises:\n        ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if x1.shape[0] != x2.shape[0]:\n        raise ValueError('Input vectors must be of the same length.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 45, "completion_id": 9, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): The first input vector.\n    x2 (np.ndarray): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \n    Raises:\n    ValueError: If x1 and x2 are not of the same length.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError('The input vectors must be of the same length.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision given true and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): Array of true binary labels.\n    y_pred (numpy.ndarray): Array of predicted binary labels.\n    \n    Returns:\n    float: Precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    unique_true = np.unique(y_true)\n    unique_pred = np.unique(y_pred)\n    if not (set(unique_true).issubset({0, 1}) and set(unique_pred).issubset({0, 1})):\n        raise ValueError('Labels should be binary, found unique values in y_true: {} and in y_pred: {}'.format(unique_true, unique_pred))\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 1.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 1, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision of binary classification.\n\n    Precision is the ratio of correctly predicted positive observations to the total predicted positives.\n    :param y_true: Numpy array of true binary labels.\n    :param y_pred: Numpy array of predicted binary labels.\n    :return: Precision as a float.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    precision_score = true_positives / predicted_positives\n    return precision_score\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 1])"}
{"task_id": 46, "completion_id": 2, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric from the given true and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    positive_predictions = np.sum(y_pred == 1)\n    if positive_predictions == 0:\n        return 1.0\n    precision_value = true_positives / positive_predictions\n    return precision_value"}
{"task_id": 46, "completion_id": 3, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels (0 or 1).\n    y_pred (numpy.ndarray): An array of predicted binary labels (0 or 1).\n    \n    Returns:\n    float: The precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The true and predicted labels must have the same shape.')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    precision_value = true_positives / predicted_positives\n    return precision_value\ny_true = np.array([1, 1, 0, 0, 1, 1])\ny_pred = np.array([1, 0, 0, 1, 1, 1])"}
{"task_id": 46, "completion_id": 4, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision score between two binary label arrays.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: Precision score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The true and predicted label arrays must be of the same shape.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    if true_positives + false_positives == 0:\n        return 0.0\n    prec = true_positives / (true_positives + false_positives)\n    return prec"}
{"task_id": 46, "completion_id": 5, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision of binary predictions.\n    \n    :param y_true: numpy array of true binary labels.\n    :param y_pred: numpy array of predicted binary labels.\n    :return: Precision value as a float.\n    \n    Precision = TP / (TP + FP)\n    where TP is the number of true positives,\n    and FP is the number of false positives.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('y_true and y_pred must be numpy arrays')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    if true_positives + false_positives == 0:\n        return 1.0\n    else:\n        return true_positives / (true_positives + false_positives)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 46, "completion_id": 6, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given the true binary labels and the predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): True binary labels.\n    y_pred (numpy.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: Precision of the predictions.\n    \"\"\"\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError('The length of y_true and y_pred should be equal.')\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    if true_positives + false_positives == 0:\n        return 0.0\n    else:\n        return true_positives / (true_positives + false_positives)\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 0])"}
{"task_id": 46, "completion_id": 7, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision of a binary classification model.\n    \n    Parameters:\n    y_true (np.array): The true binary labels.\n    y_pred (np.array): The predicted labels by the model.\n    \n    Returns:\n    float: The precision metric, which is the ratio of true positive predictions to the total positive predictions (true positives + false positives). If there are no positive predictions, returns 0.\n    \"\"\"\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    pred_positives = np.sum(y_pred == 1)\n    if pred_positives == 0:\n        return 0.0\n    precision_score = true_positives / pred_positives\n    return precision_score"}
{"task_id": 46, "completion_id": 8, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision, which is the ratio of true positive predictions to the total positive predictions (including false positives).\n    \n    :param y_true: np.array of true binary labels\n    :param y_pred: np.array of predicted binary labels\n    :return: Precision value\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    if true_positives + false_positives == 0:\n        return 0\n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 1])"}
{"task_id": 46, "completion_id": 9, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Args:\n    y_true: numpy array of true binary labels.\n    y_pred: numpy array of predicted binary labels.\n\n    Returns:\n    float: Precision value. If there are no positive predictions,\n           precision is considered as 0.\n    \"\"\"\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    all_positives = np.sum(y_pred == 1)\n    if all_positives == 0:\n        return 0.0\n    precision_value = true_positives / all_positives\n    return precision_value"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform Gradient Descent using Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    - X: numpy.ndarray, input features.\n    - y: numpy.ndarray, target values.\n    - weights: numpy.ndarray, initial weights.\n    - learning_rate: float, learning rate for the gradient descent step.\n    - n_iterations: int, number of iterations for the gradient descent.\n    - batch_size: int, size of the mini-batch (used for method='minibatch').\n    - method: str, specifies the GD variant: 'batch', 'sgd', or 'minibatch'.\n    \n    Returns:\n    - numpy.ndarray, final weights after GD.\n    \"\"\"\n    m = len(y)\n    if method == 'sgd':\n        batch_size = 1\n    elif method == 'batch':\n        batch_size = m\n    for i in range(n_iterations):\n        indices = np.random.permutation(m)\n        X_shuffled = X[indices]\n        y_shuffled = y[indices]\n        for batch_start in range(0, m, batch_size):\n            batch_end = min(batch_start + batch_size, m)\n            X_batch = X_shuffled[batch_start:batch_end]\n            y_batch = y_shuffled[batch_start:batch_end]\n            predictions = np.dot(X_batch, weights)\n            error = predictions - y_batch\n            gradient = 2 / batch_size * np.dot(X_batch.T, error)\n            weights -= learning_rate * gradient\n    return np.round(weights, decimals=4).tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([3, 7, 11])\nweights = np.array([0.5, 0.5])\nlearning_rate = 0.1\nn_iterations = 100"}
{"task_id": 47, "completion_id": 1, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements three variants of gradient descent: Stochastic (SGD), Batch, and Mini-Batch.\n    Args:\n        X (numpy.ndarray): The training data points.\n        y (numpy.ndarray): The target values.\n        weights (numpy.ndarray): The initial weights.\n        learning_rate (float): The learning rate for updates.\n        n_iterations (int): The number of iterations to perform.\n        batch_size (int): Size of the batch for mini-batch gradient descent. Defaults to 1 for SGD.\n        method (str): 'batch', 'sgd', or 'mini-batch'. Defaults to 'batch'.\n    Returns:\n        list: The updated weights after performing gradient descent with 4 decimal precision.\n    \"\"\"\n    num_samples = X.shape[0]\n\n    def update_weights(indices):\n        predictions = X[indices].dot(weights)\n        errors = predictions - y[indices]\n        gradients = X[indices].T.dot(errors) / len(indices)\n        return weights - learning_rate * gradients\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            weights = update_weights(np.arange(num_samples))\n        elif method == 'sgd':\n            index = np.random.randint(num_samples)\n            weights = update_weights([index])\n        elif method == 'mini-batch':\n            indices = np.random.choice(num_samples, batch_size, replace=False)\n            weights = update_weights(indices)\n    return np.round(weights, decimals=4).tolist()\ny = np.array([3, 4, 5])\nweights = np.array([0.1, 0.2])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2"}
{"task_id": 47, "completion_id": 2, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using specified method and return final weights.\n    :param X: numpy array of shape (n_samples, n_features) containing the features\n    :param y: numpy array of shape (n_samples,) containing the target values\n    :param weights: initial weights array of shape (n_features,)\n    :param learning_rate: learning rate of the gradient descent\n    :param n_iterations: number of iterations for the gradient descent\n    :param batch_size: size of the batch for mini-batch method, default 1 which corresponds to SGD\n    :param method: one of 'sgd', 'batch', 'minibatch' specifying the gradient descent variant to be used\n    :return: list of final weights\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'sgd':\n            idx = np.random.randint(n_samples)\n            x_i = X[idx:idx + 1]\n            y_i = y[idx:idx + 1]\n        elif method == 'minibatch':\n            idxs = np.random.choice(n_samples, batch_size, replace=False)\n            x_i = X[idxs]\n            y_i = y[idxs]\n        else:\n            x_i = X\n            y_i = y\n        y_pred = np.dot(x_i, weights)\n        error = y_pred - y_i\n        gradient = np.dot(x_i.T, error) / len(x_i)\n        weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()\nX = np.random.rand(100, 3)\ny = np.random.rand(100)\nweights = np.zeros(3)\nlearning_rate = 0.1\nn_iterations = 1000\nbatch_size = 5"}
{"task_id": 47, "completion_id": 3, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using different methods: 'batch', 'stochastic', or 'mini-batch'.\n    :param X: numpy array of shape [n_samples, n_features], input features\n    :param y: numpy array of shape [n_samples,], true labels\n    :param weights: numpy array of shape [n_features,], model weights\n    :param learning_rate: float, learning rate for gradient descent\n    :param n_iterations: int, number of iterations to run gradient descent\n    :param batch_size: int, size of batches for mini-batch gradient descent\n    :param method: str, specifies the method to use: 'batch', 'stochastic', or 'mini-batch'\n    :return: list, updated weights after performing gradient descent\n    \"\"\"\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                diff = np.dot(X[i], weights) - y[i]\n                gradient = 2 * X[i] * diff\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                end = min(i + batch_size, n_samples)\n                X_batch = X[i:end]\n                y_batch = y[i:end]\n                predictions = np.dot(X_batch, weights)\n                errors = predictions - y_batch\n                gradient = 2 * np.dot(X_batch.T, errors) / len(X_batch)\n                weights = weights - learning_rate * gradient\n        elif method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 * np.dot(X.T, errors) / n_samples\n            weights = weights - learning_rate * gradient\n    return weights.round(4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.array([6, 8, 9, 11])\nweights = np.array([0.5, -1.0])\nlearning_rate = 0.01\nn_iterations = 1000"}
{"task_id": 47, "completion_id": 4, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements gradient descent variants: 'batch', 'stochastic', 'mini-batch', using MSE loss.\n\n    Parameters:\n    X (np.array): The input features matrix.\n    y (np.array): The true values vector.\n    weights (np.array): Weights to be optimized.\n    learning_rate (float): Learning rate for the weight updates.\n    n_iterations (int): Number of iterations to run the gradient descent.\n    batch_size (int): Size of the mini-batch (only relevant when method is 'mini-batch').\n    method (str): Method type. Can be 'batch', 'stochastic' or 'mini-batch'.\n\n    Returns:\n    list: Optimized weights as a list.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for i in range(n_iterations):\n        if method == 'stochastic':\n            for j in range(n_samples):\n                index = np.random.randint(n_samples)\n                Xj = X[index:index + 1]\n                yj = y[index:index + 1]\n                prediction = np.dot(Xj, weights)\n                error = prediction - yj\n                gradient = 2 * Xj.T.dot(error) / 1\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for j in range(0, n_samples, batch_size):\n                Xb = X[j:j + batch_size]\n                yb = y[j:j + batch_size]\n                prediction = np.dot(Xb, weights)\n                error = prediction - yb\n                gradient = 2 * Xb.T.dot(error) / len(Xb)\n                weights -= learning_rate * gradient\n        elif method == 'batch':\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * X.T.dot(error) / n_samples\n            weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.01\nn_iterations = 1000"}
{"task_id": 47, "completion_id": 5, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    (n_samples, n_features) = X.shape\n    weights = np.array(weights).reshape(n_features, 1)\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                x_i = X[i:i + 1]\n                y_i = y[i:i + 1]\n                prediction = np.dot(x_i, weights)\n                error = prediction - y_i\n                gradient = 2 * np.dot(x_i.T, error)\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                x_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                prediction = np.dot(x_batch, weights)\n                error = prediction - y_batch\n                gradient = 2 * np.dot(x_batch.T, error) / batch_size\n                weights -= learning_rate * gradient\n        elif method == 'batch':\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * np.dot(X.T, error) / n_samples\n            weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"method parameter should be one of 'stochastic', 'mini-batch', or 'batch'\")\n    return np.round(weights, 4).tolist()\nn_samples = 10\nn_features = 3\nX = np.random.rand(n_samples, n_features)\ny = np.random.rand(n_samples, 1)\nweights = [0] * n_features\nlearning_rate = 0.01\nn_iterations = 1000"}
{"task_id": 47, "completion_id": 6, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent based on the specified method (Stochastic, Batch, Mini-batch).\n    \n    Parameters:\n    X: Feature matrix of shape (n_samples, n_features).\n    y: Target vector of shape (n_samples,).\n    weights: Initial weights vector of shape (n_features,).\n    learning_rate: The step size at each iteration while moving toward a minimum of a loss function.\n    n_iterations: Number of iterations for optimizing the weights.\n    batch_size: The number of samples to work through before updating the internal model parameters.\n    method: The variant of gradient descent to perform ('sgd', 'batch', 'mini-batch').\n    \n    Returns:\n    weights: The learned weights to predict the output.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for iteration in range(n_iterations):\n        if method == 'sgd':\n            for index in range(n_samples):\n                weight_update = learning_rate * np.dot(np.dot(X[index], weights) - y[index], X[index])\n                weights -= weight_update\n        elif method == 'batch':\n            predictions = np.dot(X, weights)\n            error = predictions - y\n            gradient = np.dot(X.T, error) / n_samples\n            weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for index in range(0, n_samples, batch_size):\n                end_index = min(index + batch_size, n_samples)\n                X_batch = X[index:end_index]\n                y_batch = y[index:end_index]\n                predictions = np.dot(X_batch, weights)\n                error = predictions - y_batch\n                gradient = np.dot(X_batch.T, error) / len(X_batch)\n                weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([2, 4, 6, 8])\nweights = np.array([0.1, 0.1])\nlearning_rate = 0.001\nn_iterations = 1000\nbatch_size = 2"}
{"task_id": 47, "completion_id": 7, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform one of three methods of gradient descent: Stochastic, Batch, and Mini-Batch.\n    :param X: numpy.ndarray, feature matrix\n    :param y: numpy.ndarray, target vector\n    :param weights: numpy.ndarray, initial weights\n    :param learning_rate: float, learning rate\n    :param n_iterations: int, number of iterations\n    :param batch_size: int, batch size for SGD and Mini-batch\n    :param method: str, method of GD: 'stochastic', 'batch', 'mini-batch'\n    :return: numpy.ndarray, updated weights\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if method == 'stochastic':\n        batch_size = 1\n    elif method == 'batch':\n        batch_size = n_samples\n    for _ in range(n_iterations):\n        indices = np.random.permutation(n_samples)\n        X = X[indices]\n        y = y[indices]\n        for i in range(0, n_samples, batch_size):\n            X_batch = X[i:i + batch_size]\n            y_batch = y[i:i + batch_size]\n            predictions = np.dot(X_batch, weights)\n            errors = predictions - y_batch\n            gradient = 2 / batch_size * np.dot(X_batch.T, errors)\n            weights -= learning_rate * gradient\n    return np.round(weights, decimals=4).tolist()"}
{"task_id": 47, "completion_id": 8, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization according to the specified method.\n    \n    :param np.ndarray X: The feature matrix.\n    :param np.ndarray y: The target vector.\n    :param np.ndarray weights: The initial weights vector.\n    :param float learning_rate: The learning rate for the gradient descent.\n    :param int n_iterations: The number of iterations to train.\n    :param int batch_size: The size of the batch for mini-batch gradient descent.\n    :param str method: The method of gradient descent to use: 'batch', 'stochastic', or 'mini-batch'.\n    \n    :return: Updated weights after performing gradient descent.\n    :rtype: list\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                gradient = 2 * X[i] * (X[i].dot(weights) - y[i])\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                batch_X = X[i:i + batch_size]\n                batch_y = y[i:i + batch_size]\n                gradient = 2 * batch_X.T.dot(batch_X.dot(weights) - batch_y) / len(batch_X)\n                weights = weights - learning_rate * gradient\n        else:\n            gradient = 2 * X.T.dot(X.dot(weights) - y) / n_samples\n            weights = weights - learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 9, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization with Mean Squared Error (MSE) loss function.\n\n    :param X: numpy.ndarray, feature matrix.\n    :param y: numpy.ndarray, target vector.\n    :param weights: numpy.ndarray, initial weights.\n    :param learning_rate: float, learning rate for the gradient descent.\n    :param n_iterations: int, number of iterations to perform.\n    :param batch_size: int, size of the batch used for gradient calculation.\n    :param method: str, specifies which variant of gradient descent to perform. Can be 'batch', 'stochastic' or 'mini-batch'.\n    :return: list, learned weights after performing gradient descent.\n    \"\"\"\n    m = len(y)\n    n_features = X.shape[1]\n    np.random.seed(42)\n    if method == 'batch':\n        batch_size = m\n    elif method == 'stochastic':\n        batch_size = 1\n    for _ in range(n_iterations):\n        for i in range(0, m, batch_size):\n            end = min(i + batch_size, m)\n            batch_X = X[i:end]\n            batch_y = y[i:end]\n            predictions = np.dot(batch_X, weights)\n            errors = predictions - batch_y\n            gradient = 2 / batch_size * np.dot(batch_X.T, errors)\n            weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a matrix to its Reduced Row Echelon Form (RREF).\n\n    Args:\n    matrix: A list of lists where each inner list represents a row of the matrix.\n    \n    Returns:\n    A list of lists representing the RREF of the matrix.\n    \"\"\"\n    m = np.array(matrix, dtype=np.float64)\n    (rows, cols) = m.shape\n    row_num = 0\n    for col_num in range(cols):\n        if row_num >= rows:\n            break\n        max_row = np.argmax(abs(m[row_num:, col_num])) + row_num\n        m[[row_num, max_row]] = m[[max_row, row_num]]\n        pivot = m[row_num, col_num]\n        if pivot == 0:\n            continue\n        m[row_num] = m[row_num] / pivot\n        for b in range(rows):\n            if b == row_num:\n                continue\n            m[b] = m[b] - m[row_num] * m[b, col_num]\n        row_num += 1\n    return m.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 48, "completion_id": 1, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list of lists: The Reduced Row Echelon Form of the matrix.\n    \"\"\"\n    matrix = np.array(matrix, dtype=np.float64)\n    (row, col) = matrix.shape\n    lead = 0\n    for r in range(row):\n        if col <= lead:\n            break\n        i = r\n        while matrix[i, lead] == 0:\n            i += 1\n            if row == i:\n                i = r\n                lead += 1\n                if col == lead:\n                    break\n        matrix[[i, r]] = matrix[[r, i]]\n        pivot_val = matrix[r, lead]\n        matrix[r] = matrix[r] / pivot_val\n        for j in range(row):\n            if j != r:\n                matrix[j] -= matrix[j, lead] * matrix[r]\n        lead += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 2, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Function to convert a matrix to Reduced Row Echelon Form (RREF).\n    \n    Args:\n    - matrix: numpy array representing the matrix to be transformed.\n    \n    Returns:\n    - The transformed matrix in RREF as a list.\n    \"\"\"\n    mutable_matrix = np.copy(matrix)\n    (rows, cols) = mutable_matrix.shape\n    row_index = 0\n    for col_index in range(cols):\n        selected_row = np.argmax(np.abs(mutable_matrix[row_index:rows, col_index])) + row_index\n        if mutable_matrix[selected_row, col_index] == 0:\n            continue\n        mutable_matrix[[row_index, selected_row]] = mutable_matrix[[selected_row, row_index]]\n        mutable_matrix[row_index] /= mutable_matrix[row_index, col_index]\n        for other_row in range(rows):\n            if other_row != row_index:\n                mutable_matrix[other_row] -= mutable_matrix[other_row, col_index] * mutable_matrix[row_index]\n        row_index += 1\n        if row_index == rows:\n            break\n    return mutable_matrix.tolist()"}
{"task_id": 48, "completion_id": 3, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix into its Reduced Row Echelon Form (RREF).\n\n    Parameters:\n    matrix (list of lists): The matrix to be converted into RREF.\n\n    Returns:\n    list: The matrix in RREF form.\n    \"\"\"\n    A = np.array(matrix).astype(float)\n    (num_rows, num_cols) = A.shape\n    (row, col) = (0, 0)\n    while row < num_rows and col < num_cols:\n        max_row = np.argmax(abs(A[row:, col])) + row\n        if A[max_row, col] == 0:\n            col += 1\n        else:\n            A[[row, max_row]] = A[[max_row, row]]\n            pivot = A[row, col]\n            A[row] /= pivot\n            for i in range(num_rows):\n                if i != row:\n                    A[i] -= A[i, col] * A[row]\n            row += 1\n            col += 1\n    RREF_result = [row for row in A.tolist() if not all([abs(x) < 1e-10 for x in row])]\n    return RREF_result"}
{"task_id": 48, "completion_id": 4, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    This function computes the Reduced Row Echelon Form (RREF) of a given matrix.\n    :param matrix: Input matrix as a list of lists\n    :return: The RREF of the input matrix as a list of lists\n    \"\"\"\n    m = np.array(matrix, dtype=float)\n    (rows, cols) = m.shape\n    row_idx = 0\n    for col_idx in range(cols):\n        if row_idx < rows:\n            max_elem = abs(m[row_idx, col_idx])\n            max_row = row_idx\n            for r in range(row_idx + 1, rows):\n                if abs(m[r, col_idx]) > max_elem:\n                    max_elem = abs(m[r, col_idx])\n                    max_row = r\n            if max_row != row_idx:\n                m[[row_idx, max_row]] = m[[max_row, row_idx]]\n            if m[row_idx, col_idx] == 0:\n                continue\n            m[row_idx] = m[row_idx] / m[row_idx, col_idx]\n            for r in range(rows):\n                if r != row_idx:\n                    m[r] = m[r] - m[r, col_idx] * m[row_idx]\n            row_idx += 1\n    return m.tolist()"}
{"task_id": 48, "completion_id": 5, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Calculate the Reduced Row Echelon Form (RREF) of a given matrix.\n    :param matrix: A 2D list representing the input matrix.\n    :return: A 2D list representing the matrix in its RREF.\n    \"\"\"\n    m = np.array(matrix)\n    (rows, cols) = m.shape\n    row_num = 0\n    for col_num in range(cols):\n        if row_num >= rows:\n            break\n        max_row = row_num\n        for r in range(row_num, rows):\n            if abs(m[r, col_num]) > abs(m[max_row, col_num]):\n                max_row = r\n        if m[max_row, col_num] == 0:\n            continue\n        m[[row_num, max_row]] = m[[max_row, row_num]]\n        pivot = m[row_num, col_num]\n        m[row_num] = m[row_num] / pivot\n        for r in range(rows):\n            if r == row_num:\n                continue\n            if m[r, col_num] != 0:\n                m[r] -= m[r, col_num] / m[row_num, col_num] * m[row_num]\n        row_num += 1\n    return m.tolist()\nmatrix = [[1, 2, -1, -4], [-2, -3, 4, 4], [-1, -2, 2, 2]]"}
{"task_id": 48, "completion_id": 6, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Takes a matrix and returns its Reduced Row Echelon Form.\n    \n    Args:\n    - matrix (list of lists): A 2D list representing the input matrix.\n    \n    Returns:\n    - list of lists: The RREF of the input matrix.\n    \"\"\"\n    mat = np.array(matrix, dtype=np.float64)\n    (numRows, numCols) = mat.shape\n    lead = 0\n    rowCount = numRows\n    for r in range(rowCount):\n        if numCols <= lead:\n            return mat.tolist()\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if numRows == i:\n                i = r\n                lead += 1\n                if numCols == lead:\n                    return mat.tolist()\n        mat[[i, r]] = mat[[r, i]]\n        lv = mat[r, lead]\n        mat[r] = mat[r] / lv\n        for i in range(rowCount):\n            if i != r:\n                lv = mat[i, lead]\n                mat[i] = mat[i] - lv * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 48, "completion_id": 7, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): The matrix to be converted into RREF.\n    \n    Returns:\n    list of lists: The matrix in its RREF form.\n    \"\"\"\n    mat = np.array(matrix, dtype=np.float64)\n    (rows, cols) = mat.shape\n    lead = 0\n    rowCount = rows\n    for r in range(rowCount):\n        if cols <= lead:\n            return mat.tolist()\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if rows == i:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        mat[[i, r]] = mat[[r, i]]\n        mat[r] = mat[r] / mat[r, lead]\n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 48, "completion_id": 8, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Computes the Reduced Row Echelon Form (RREF) of a given matrix.\n    \n    Arguments:\n    matrix -- a nxm numpy matrix of numbers\n    \n    Returns:\n    rref_matrix -- the nxm matrix in RREF as python list\n    \"\"\"\n    if not isinstance(matrix, np.ndarray):\n        matrix = np.array(matrix)\n    (rows, cols) = matrix.shape\n    row = 0\n    for col in range(cols):\n        if row >= rows:\n            break\n        max_row = row\n        for r in range(row + 1, rows):\n            if abs(matrix[r, col]) > abs(matrix[max_row, col]):\n                max_row = r\n        if matrix[max_row, col] == 0:\n            continue\n        matrix[[row, max_row]] = matrix[[max_row, row]]\n        pivot = matrix[row, col]\n        matrix[row] /= pivot\n        for r in range(rows):\n            if r != row:\n                factor = matrix[r, col]\n                matrix[r] -= factor * matrix[row]\n        row += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 9, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists of int/float): The matrix to convert.\n    \n    Returns:\n    list of lists of int/float: The matrix in RREF.\n    \"\"\"\n    mat = np.array(matrix, dtype=np.float64)\n    (row, col) = mat.shape\n    lead = 0\n    for r in range(row):\n        if lead >= col:\n            break\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if i == row:\n                i = r\n                lead += 1\n                if lead == col:\n                    break\n        if i != r:\n            mat[[i, r]] = mat[[r, i]]\n        pivot = mat[r, lead]\n        mat[r, :] /= pivot\n        for i in range(row):\n            if i != r:\n                mat[i, :] -= mat[i, lead] * mat[r, :]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm implementation.\n    \n    Parameters:\n    f : function\n        The objective function to be optimized.\n    grad : function\n        A function that computes the gradient of f.\n    x0 : numpy array\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size. Default is 0.001.\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates. Default is 0.9.\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates. Default is 0.999.\n    epsilon : float, optional\n        A small constant for numerical stability. Default is 1e-8.\n    num_iterations : int, optional\n        Number of iterations to run the optimizer. Default is 1000.\n\n    Returns:\n    x : list\n        Optimized parameters.\n    \"\"\"\n    m_t = 0\n    v_t = 0\n    t = 0\n    x = x0\n    for t in range(1, num_iterations + 1):\n        g_t = grad(x)\n        m_t = beta1 * m_t + (1 - beta1) * g_t\n        v_t = beta2 * v_t + (1 - beta2) * g_t ** 2\n        m_cap = m_t / (1 - np.power(beta1, t))\n        v_cap = v_t / (1 - np.power(beta2, t))\n        x -= learning_rate * m_cap / (np.sqrt(v_cap) + epsilon)\n    return np.round(x, decimals=4).tolist()"}
{"task_id": 49, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    (m, v) = (0, 0)\n    t = 0\n    x = np.array(x0, dtype=np.float32)\n    for t in range(1, num_iterations + 1):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - np.power(beta1, t))\n        v_hat = v / (1 - np.power(beta2, t))\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [2.0, 2.0]"}
{"task_id": 49, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Optimizes the parameters of the objective function using the Adam optimization algorithm.\n    \n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (numpy.ndarray): Initial parameter values.\n    learning_rate (float): The step size. Default: 0.001.\n    beta1 (float): Exponential decay rate for the first moment estimates. Default: 0.9.\n    beta2 (float): Exponential decay rate for the second moment estimates. Default: 0.999.\n    epsilon (float): A small constant for numerical stability. Default: 1e-8.\n    num_iterations (int): Number of iterations to run the optimizer. Default: 1000.\n    \n    Returns:\n    list: Optimized parameters rounded to the nearest 4th decimal.\n    \"\"\"\n    m_t = np.zeros_like(x0)\n    v_t = np.zeros_like(x0)\n    t = 0\n    x = np.array(x0)\n    for _ in range(num_iterations):\n        t += 1\n        g_t = grad(x)\n        m_t = beta1 * m_t + (1 - beta1) * g_t\n        v_t = beta2 * v_t + (1 - beta2) * g_t ** 2\n        m_t_hat = m_t / (1 - beta1 ** t)\n        v_t_hat = v_t / (1 - beta2 ** t)\n        x -= learning_rate * m_t_hat / (np.sqrt(v_t_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = np.array([-2.0, 2.0])"}
{"task_id": 49, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Optimize the given function using the Adam optimization algorithm.\n    \n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float): The learning rate. Default is 0.001.\n    beta1 (float): Exponential decay rate for the first moment estimates. Default is 0.9.\n    beta2 (float): Exponential decay rate for the second moment estimates. Default is 0.999.\n    epsilon (float): A small constant for numerical stability. Default is 1e-8.\n    num_iterations (int): Number of iterations to run the optimizer. Default is 1000.\n    \n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0, dtype=np.float64)\n    v = np.zeros_like(x0, dtype=np.float64)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0, decimals=4).tolist()"}
{"task_id": 49, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm to find and return the optimal value of x.\n    \n    Parameters:\n    - f: Objective function to be minimized.\n    - grad: Function that computes the gradient of f.\n    - x0: Initial estimates of parameters.\n    - learning_rate: The step size.\n    - beta1: Exponential decay rate for the first moment estimates.\n    - beta2: Exponential decay rate for the second moment estimates.\n    - epsilon: A small constant for numerical stability.\n    - num_iterations: Number of iterations to run the optimizer.\n\n    Returns:\n    - The optimized parameters as a list rounded to the nearest 4th decimal.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0, dtype=np.float)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    optimized_x = np.round(x, 4).tolist()\n    return optimized_x\nx0 = [1.5, -0.5, 1.0]"}
{"task_id": 49, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Optimizes the parameters of a given function using the Adam algorithm.\n    \n    Args:\n    f: The objective function to be optimized.\n    grad: A function that computes the gradient of f.\n    x0: Initial parameter values.\n    learning_rate: The step size.\n    beta1: Exponential decay rate for the first moment estimates.\n    beta2: Exponential decay rate for the second moment estimates.\n    epsilon: A small constant for numerical stability.\n    num_iterations: Number of iterations to run the optimizer.\n    \n    Returns:\n    Optimized parameters as a list.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\nx0 = [1.0, 1.0]"}
{"task_id": 49, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = 0\n    v = 0\n    x = np.array(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, decimals=4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = [2, 2]"}
{"task_id": 49, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implement the Adam optimization algorithm to find the optimized parameters for a given function.\n\n    :param f: Objective function\n    :param grad: Function to compute the gradient of f\n    :param x0: Initial parameter values\n    :param learning_rate: Learning rate\n    :param beta1: Exponential decay rate for the first moment estimates\n    :param beta2: Exponential decay rate for the second moment estimates\n    :param epsilon: Small constant to avoid division by zero\n    :param num_iterations: Number of iterations to run the optimizer\n    :return: Optimized parameters\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = x0.copy()\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, decimals=4).tolist()\nx0 = np.array([1.5, 1.5])"}
{"task_id": 49, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implementation of the Adam optimization algorithm.\n    \n    :param f: Objective function to be optimized\n    :param grad: Function that computes the gradient of f\n    :param x0: Initial parameter values\n    :param learning_rate: The step size\n    :param beta1: Exponential decay rate for the first moment estimates\n    :param beta2: Exponential decay rate for the second moment estimates\n    :param epsilon: A small constant for numerical stability\n    :param num_iterations: Number of iterations to run the optimizer\n    :return: Optimized parameters\n    \"\"\"\n    t = 0\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - np.power(beta1, t))\n        v_hat = v / (1 - np.power(beta2, t))\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0, 4).tolist()\nx0 = np.array([1.0, 1.0])"}
{"task_id": 49, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0, dtype=np.float64)\n    v = np.zeros_like(x0, dtype=np.float64)\n    m_corrected = np.zeros_like(x0, dtype=np.float64)\n    v_corrected = np.zeros_like(x0, dtype=np.float64)\n    t = 0\n    dim = len(x0)\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_corrected = m / (1 - beta1 ** t)\n        v_corrected = v / (1 - beta2 ** t)\n        x0 = x0 - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return np.around(x0, decimals=4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    for _ in range(max_iter):\n        y_pred = X.dot(w) + b\n        loss = y_pred - y\n        error = np.mean(loss ** 2) / 2\n        dw = np.zeros(n)\n        for j in range(n):\n            dw[j] = 1 / m * X[:, j].dot(loss) + alpha * np.sign(w[j])\n        db = 1 / m * np.sum(loss)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.all(np.abs(dw) < tol) and np.abs(db) < tol:\n            break\n    w_rounded = [round(val, 4) for val in w.tolist()]\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 50, "completion_id": 1, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    prev_loss = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        error = y_pred - y\n        w_gradient = 1 / m * (np.dot(X.T, error) + alpha * np.sign(w))\n        b_gradient = 1 / m * np.sum(error)\n        w -= learning_rate * w_gradient\n        b -= learning_rate * b_gradient\n        loss = 1 / (2 * m) * np.sum(error ** 2) + alpha / m * np.sum(np.abs(w))\n        if np.abs(prev_loss - loss) < tol:\n            break\n        prev_loss = loss\n    w = np.round(w, 4)\n    b = np.round(b, 4)\n    return (w.tolist(), b)"}
{"task_id": 50, "completion_id": 2, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent.\n    \n    Parameters:\n    - X: Features matrix (n_samples, n_features)\n    - y: Target values (n_samples,)\n    - alpha: Regularization strength\n    - learning_rate: Learning rate for the gradient descent\n    - max_iter: Maximum number of iterations for gradient descent\n    - tol: Tolerance for stopping criteria\n    \n    Returns:\n    - Tuple (w, b): Coefficients and bias trained using Lasso Regression.\n    \"\"\"\n    m = len(X)\n    n = X.shape[1]\n    w = np.zeros(n)\n    b = 0\n    prev_cost = np.inf\n    for i in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        grad_w = -(2 / m) * np.dot(X.T, y - y_pred) + alpha * np.sign(w)\n        grad_b = -(2 / m) * np.sum(y - y_pred)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n        w = np.sign(w) * np.maximum(np.abs(w) - learning_rate * alpha, 0)\n        cost = 1 / (2 * m) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(w))\n        if np.abs(prev_cost - cost) <= tol:\n            break\n        prev_cost = cost\n    return (w.tolist(), round(b, 4))\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([2, 2.3, 3.5, 4])"}
{"task_id": 50, "completion_id": 3, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 Regularization.\n    \n    Parameters:\n        X (np.array): The features matrix.\n        y (np.array): The target vector.\n        alpha (float): The regularization parameter.\n        learning_rate (float): The learning rate for gradient descent.\n        max_iter (int): The maximum number of iterations.\n        tol (float): The tolerance for stopping criteria.\n        \n    Returns:\n        tuple: A tuple containing the final weights vector and the bias term.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_loss = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        loss = np.mean((y_pred - y) ** 2)\n        if abs(prev_loss - loss) <= tol:\n            break\n        prev_loss = loss\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred)\n        dw += alpha * np.sign(weights)\n        db = -2 / n_samples * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (np.round(weights, 4).tolist(), np.round(bias, 4).tolist())"}
{"task_id": 50, "completion_id": 4, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_loss = 0\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        loss = 1 / (2 * n_samples) * np.dot(error.T, error) + alpha / n_samples * np.sum(np.abs(weights))\n        dJ_dW = -(1 / n_samples) * (X.T @ (y - y_pred)) + alpha / n_samples * np.sign(weights)\n        dJ_db = -(1 / n_samples) * np.sum(y - y_pred)\n        weights -= learning_rate * dJ_dW\n        bias -= learning_rate * dJ_db\n        if abs(loss - prev_loss) < tol:\n            break\n        prev_loss = loss\n    weights = np.round(weights, 4).tolist()\n    bias = np.round(bias, 4)\n    return (weights, bias)"}
{"task_id": 50, "completion_id": 5, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 Regularization.\n    \n    Parameters:\n    X (np.array): Input features matrix.\n    y (np.array): Target values.\n    alpha (float): Regularization parameter for L1 penalty.\n    learning_rate (float): Learning rate for the optimization process.\n    max_iter (int): Maximum number of iterations for the gradient descent.\n    tol (float): Tolerance for convergence criteria.\n    \n    Returns:\n    tuple: (weights, bias), where both are rounded to the 4th decimal.\n    \"\"\"\n    (m, n) = X.shape\n    X_b = np.c_[np.ones((m, 1)), X]\n    weights = np.zeros(n + 1)\n    prev_loss = None\n    for _ in range(max_iter):\n        predictions = X_b.dot(weights)\n        error = predictions - y\n        gradient = (X_b.T.dot(error) + alpha * np.sign(weights)) / m\n        weights -= learning_rate * gradient\n        current_loss = np.sqrt(((predictions - y) ** 2).mean())\n        if prev_loss is not None and prev_loss - current_loss < tol:\n            break\n        prev_loss = current_loss\n    b = np.round(weights[0], 4)\n    w = np.round(weights[1:], 4).tolist()\n    return (w, b)"}
{"task_id": 50, "completion_id": 6, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using gradient descent with L1 regularization.\n    \n    Parameters:\n    -----------\n    X : np.array\n        Feature matrix.\n    y : np.array\n        Target vector.\n    alpha : float\n        Regularization strength.\n    learning_rate : float\n        Gradient descent step size.\n    max_iter : int\n        Maximum number of iterations.\n    tol : float\n        Tolerance for stopping condition.\n        \n    Returns:\n    --------\n    tuple\n        Returns a tuple (w, b) where w are the learned weights and b is the bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    prev_cost = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = (-2 * np.dot(X.T, y - y_pred) + alpha * np.sign(w)) / n_samples\n        db = -2 * np.sum(y - y_pred) / n_samples\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        current_cost = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha / (2 * n_samples) * np.sum(np.abs(w))\n        if abs(prev_cost - current_cost) <= tol:\n            break\n        prev_cost = current_cost\n    return (np.round(w, 4).tolist(), np.round(b, 4).item())"}
{"task_id": 50, "completion_id": 7, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        grad_weights = 1 / m * X.T.dot(error) + alpha * np.sign(weights)\n        grad_bias = 1 / m * np.sum(error)\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n        if np.linalg.norm(grad_weights) < tol:\n            break\n    weights_list = weights.round(4).tolist()\n    bias = round(bias, 4)\n    return (weights_list, bias)\nX = np.array([[0.5, 1.0], [1.0, 2.0], [1.5, 3.0]])\ny = np.array([2.5, 4.0, 5.5])"}
{"task_id": 50, "completion_id": 8, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using gradient descent.\n    \n    :param X: np.array, features matrix (n_samples, n_features)\n    :param y: np.array, target values (n_samples,)\n    :param alpha: float, regularization parameter\n    :param learning_rate: float, update step size\n    :param max_iter: int, maximum number of iterations\n    :param tol: float, tolerance for stopping criteria\n    :return: tuple, (weights, bias) of the best model\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -(1 / n_samples) * np.dot(X.T, y - y_pred) + np.sign(weights) * alpha\n        db = -(1 / n_samples) * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        if np.linalg.norm(learning_rate * dw) < tol and np.abs(learning_rate * db) < tol:\n            break\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights.tolist(), [bias])"}
{"task_id": 50, "completion_id": 9, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = float('inf')\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        loss = y_pred - y\n        cost = 1 / (2 * n_samples) * np.sum(loss ** 2) + alpha * np.linalg.norm(weights, 1)\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n        gradients = -(1 / n_samples) * (X.T.dot(loss) - alpha * np.sign(weights))\n        weights -= learning_rate * gradients\n        bias -= learning_rate * -(1 / n_samples) * np.sum(loss)\n    return ([round(w, 4) for w in weights.tolist()], round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is defined as the minimum number of insertions, deletions,\n    substitutions, and transpositions of adjacent characters needed to transform\n    one string into the other, with each operation costing 1 unit.\n    \n    Args:\n    source: The source string.\n    target: The target string.\n    \n    Returns:\n    The OSA distance between the source and target strings.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    distance_matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        distance_matrix[i][0] = i\n    for j in range(len_target + 1):\n        distance_matrix[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            distance_matrix[i][j] = min(distance_matrix[i - 1][j] + 1, distance_matrix[i][j - 1] + 1, distance_matrix[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                distance_matrix[i][j] = min(distance_matrix[i][j], distance_matrix[i - 2][j - 2] + 1)\n    return distance_matrix[len_source][len_target]"}
{"task_id": 51, "completion_id": 1, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    \n    :param source: str, the source string.\n    :param target: str, the target string.\n    :return: int, the OSA distance.\n    \"\"\"\n    (len_source, len_target) = (len(source), len(target))\n    matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        matrix[i][0] = i\n    for j in range(len_target + 1):\n        matrix[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                matrix[i][j] = min(matrix[i][j], matrix[i - 2][j - 2] + cost)\n    return matrix[len_source][len_target]"}
{"task_id": 51, "completion_id": 2, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        matrix[i, 0] = i\n    for j in range(len_target + 1):\n        matrix[0, j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            substitution_cost = 0 if source[i - 1] == target[j - 1] else 1\n            matrix[i, j] = min(matrix[i - 1, j] + 1, matrix[i, j - 1] + 1, matrix[i - 1, j - 1] + substitution_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                matrix[i, j] = min(matrix[i, j], matrix[i - 2, j - 2] + 1)\n    return matrix[len_source, len_target]"}
{"task_id": 51, "completion_id": 3, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The OSA distance between the source and target strings.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 4, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the minimum number of edits to transform one string into another.\n    The allowed operations are insertion, deletion, substitution, and transposition.\n    \n    :param source: The source string to be transformed.\n    :param target: The target string to transform into.\n    :return: The minimum number of edits required.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + 1)\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len_source][len_target]"}
{"task_id": 51, "completion_id": 5, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            mismatch_cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + mismatch_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 6, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment distance between two strings.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1))\n    for i in range(len_source + 1):\n        for j in range(len_target + 1):\n            if i == 0:\n                dp[i][j] = j\n            elif j == 0:\n                dp[i][j] = i\n            elif source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    dp[i][j] = min(dp[i][j], 1 + dp[i - 2][j - 2])\n    return int(dp[len_source][len_target])\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 51, "completion_id": 7, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n        source (str): The source string.\n        target (str): The target string.\n        \n    Returns:\n        int: The OSA distance between the source and target strings.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return dp[m, n]"}
{"task_id": 51, "completion_id": 8, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of one-character edit operations\n    (insert, delete, substitute, transpose) required to change one string into the other.\n    \n    Args:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The OSA distance between the source and target strings.\n    \"\"\"\n    (n, m) = (len(source), len(target))\n    dp = np.zeros((n + 1, m + 1), dtype=int)\n    for i in range(n + 1):\n        dp[i][0] = i\n    for j in range(m + 1):\n        dp[0][j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[n][m]"}
{"task_id": 51, "completion_id": 9, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of edits (insertion, deletion, substitution, or transposition of two adjacent characters)\n    required to transform one string into the other.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        matrix[i, 0] = i\n    for j in range(len_target + 1):\n        matrix[0, j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            matrix[i, j] = min(matrix[i - 1, j] + 1, matrix[i, j - 1] + 1, matrix[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                matrix[i, j] = min(matrix[i, j], matrix[i - 2, j - 2] + cost)\n    return matrix[len_source, len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall for the given true and predicted binary labels.\n    \n    :param y_true: List[int], true binary labels\n    :param y_pred: List[int], predicted binary labels\n    :return: float, recall rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    if true_positives + false_negatives == 0:\n        return 0.0\n    recall_value = true_positives / (true_positives + false_negatives)\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 1, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: list of true binary labels (0 or 1).\n    - y_pred: list of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value rounded to three decimal places. If the denominator (TP + FN) is zero, recall is 0.0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 2, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification problem.\n    \n    Parameters:\n    - y_true: List[int], true binary labels (0 or 1) for the dataset.\n    - y_pred: List[int], predicted binary labels (0 or 1) from the model.\n    \n    Returns:\n    - float: The recall value rounded to three decimal places. If the denominator is zero, returns 0.0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n        return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 3, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate recall metric for binary classification.\n    \n    Parameters:\n    y_true (list): List of true binary labels.\n    y_pred (list): List of predicted binary labels.\n    \n    Returns:\n    float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    recall = tp / (tp + fn)\n    return round(recall, 3)\ny_true = [0, 1, 0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 1, 0, 1, 0, 0, 1]"}
{"task_id": 52, "completion_id": 4, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true: List[int], the true binary labels.\n    - y_pred: List[int], the predicted binary labels.\n\n    Returns:\n    - float, the recall score rounded to three decimal places.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    if true_positives + false_negatives == 0:\n        return 0.0\n    recall_value = true_positives / (true_positives + false_negatives)\n    return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]"}
{"task_id": 52, "completion_id": 5, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification model.\n    \n    Parameters:\n    y_true (list of int): True binary labels (0 or 1) for the dataset.\n    y_pred (list of int): Predicted binary labels (0 or 1) from the model.\n    \n    Returns:\n    float: Recall metric rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n    return round(recall_value, 3)\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 1, 0, 0, 1]"}
{"task_id": 52, "completion_id": 6, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: A list of true binary labels (0 or 1).\n    - y_pred: A list of predicted binary labels (0 or 1).\n    \n    Returns:\n    - Recall value as a float rounded to three decimal places. Returns 0.0 if TP+FN=0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n        return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 52, "completion_id": 7, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall for a binary classification problem.\n    \n    :param y_true: list of true binary labels (0 or 1)\n    :param y_pred: list of predicted binary labels (0 or 1)\n    :return: Recall value rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n    if TP + FN == 0:\n        return 0.0\n    recall = TP / (TP + FN)\n    return round(recall, 3)\ny_true = [1, 0, 1, 1, 0, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 1]"}
{"task_id": 52, "completion_id": 8, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification problem.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1)\n    - y_pred: List of predicted binary labels (0 or 1)\n    \n    Returns:\n    - recall_value: The recall metric rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        recall_value = 0.0\n    else:\n        recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 9, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculates the recall metric for binary classification.\n    \n    Args:\n    y_true (list of int): The true binary labels.\n    y_pred (list of int): The predicted binary labels.\n    \n    Returns:\n    float: The recall value rounded to three decimal places, or 0.0 if TP+FN is 0.\n    \"\"\"\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_negatives = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    recall_value = true_positives / denominator\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Args:\n        X: Input tensor of shape (sequence_length, d_model) where sequence_length is the length of the sequence and d_model is the dimensionality of the model.\n        W_q: Query weight matrix of shape (d_model, d_k) where d_k is the dimensionality of the queries.\n        W_k: Key weight matrix of shape (d_model, d_k).\n        W_v: Value weight matrix of shape (d_model, d_v) where d_v is the dimensionality of the values.\n        \n    Returns:\n        The output tensor of shape (sequence_length, d_v) after applying self-attention.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    attention_scores = Q @ K.T / np.sqrt(Q.shape[1])\n    attention_probs = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = attention_probs @ V\n    output = np.round(output, 4)\n    return output.tolist()\nsequence_length = 5\nd_model = 10\nd_k = 5\nd_v = 4\nX = np.random.rand(sequence_length, d_model)\nW_q = np.random.rand(d_model, d_k)\nW_k = np.random.rand(d_model, d_k)\nW_v = np.random.rand(d_model, d_v)\noutput = self_attention(X, W_q, W_k, W_v)"}
{"task_id": 53, "completion_id": 1, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input sequence as a tensor of shape (seq_len, d_model).\n    W_q (numpy.ndarray): Query weight matrix of shape (d_model, d_key).\n    W_k (numpy.ndarray): Key weight matrix of shape (d_model, d_key).\n    W_v (numpy.ndarray): Value weight matrix of shape (d_model, d_value).\n    \n    Returns:\n    numpy.ndarray: Self-attention output as a tensor of shape (seq_len, d_value).\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    attention_scores = Q @ K.T\n    scaled_attention_scores = attention_scores / np.sqrt(K.shape[1])\n    attention_weights = np.softmax(scaled_attention_scores, axis=-1)\n    attention_output = attention_weights @ V\n    attention_output = np.round(attention_output, 4)\n    return attention_output.tolist()\nseq_len = 3\nd_model = 50\nd_key = 10\nd_value = 10\nX = np.random.randn(seq_len, d_model)\nW_q = np.random.randn(d_model, d_key)\nW_k = np.random.randn(d_model, d_key)\nW_v = np.random.randn(d_model, d_value)"}
{"task_id": 53, "completion_id": 2, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input tensor of shape (batch_size, seq_length, dim_input).\n    W_q (numpy.ndarray): Query weight matrix of shape (dim_input, dim_key).\n    W_k (numpy.ndarray): Key weight matrix of shape (dim_input, dim_key).\n    W_v (numpy.ndarray): Value weight matrix of shape (dim_input, dim_value).\n\n    Returns:\n    numpy.ndarray: The output of self-attention mechanism, rounded to 4 decimal places.\n    \"\"\"\n    Q = np.tensordot(X, W_q, axes=([2], [0]))\n    K = np.tensordot(X, W_k, axes=([2], [0]))\n    V = np.tensordot(X, W_v, axes=([2], [0]))\n    att_scores = np.tensordot(Q, np.transpose(K, (0, 2, 1)), axes=([2], [1])) / np.sqrt(K.shape[-1])\n    att_weights = np.exp(att_scores)\n    att_weights = att_weights / np.sum(att_weights, axis=-1, keepdims=True)\n    output = np.tensordot(att_weights, V, axes=([2], [1]))\n    return np.round(output, 4).tolist()\nX = np.random.rand(1, 3, 2)\nW_q = np.random.rand(2, 2)\nW_k = np.random.rand(2, 2)\nW_v = np.random.rand(2, 3)\noutput = self_attention(X, W_q, W_k, W_v)"}
{"task_id": 53, "completion_id": 3, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement self-attention mechanism given the input sequence and the weight matrices.\n    \n    Parameters:\n    X (numpy.ndarray): Input tensor of shape (seq_len, d_model).\n    W_q (numpy.ndarray): Weight matrix for query of shape (d_model, d_k).\n    W_k (numpy.ndarray): Weight matrix for key of shape (d_model, d_k).\n    W_v (numpy.ndarray): Weight matrix for value of shape (d_model, d_v).\n\n    Returns:\n    list: A list of the self-attention outputs for each token in the sequence, rounded to the nearest 4th decimal.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attn_scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    attn_scores = np.exp(attn_scores)\n    sum_attn_scores = np.sum(attn_scores, axis=-1, keepdims=True)\n    attn_weights = attn_scores / sum_attn_scores\n    output = np.dot(attn_weights, V)\n    return np.round(output, 4).tolist()"}
{"task_id": 53, "completion_id": 4, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implementation of the self-attention mechanism.\n    \n    Parameters:\n    X: Input tensor of shape (batch_size, seq_length, embedding_dim)\n    W_q: Query weight tensor of shape (embedding_dim, embedding_dim)\n    W_k: Key weight tensor of shape (embedding_dim, embedding_dim)\n    W_v: Value weight tensor of shape (embedding_dim, embedding_dim)\n    \n    Returns:\n    Output tensor of shape (batch_size, seq_length, embedding_dim)\n    \"\"\"\n    (batch_size, seq_length, embedding_dim) = X.shape\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(embedding_dim)\n    attention_scores = np.exp(attention_scores)\n    attention_scores /= attention_scores.sum(axis=-1, keepdims=True)\n    output = np.matmul(attention_scores, V)\n    output = np.round(output, 4)\n    return output.tolist()\nbatch_size = 1\nseq_length = 3\nembedding_dim = 2\nX = np.array([[[1, 2], [3, 4], [5, 6]]])\nW_q = np.array([[0.5, 1.5], [-1.0, 2.0]])\nW_k = np.array([[2.0, -1.0], [1.5, 0.5]])\nW_v = np.array([[-1.0, 2.0], [1.0, 1.0]])"}
{"task_id": 53, "completion_id": 5, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism for a given input tensor and weight matrices.\n    \n    Args:\n    X: Input tensor of shape (N, seq_len, d_model), where N is the batch size, seq_len is the sequence length, and d_model is the dimension of the model.\n    W_q: Weight matrix for query computation, of shape (d_model, d_k).\n    W_k: Weight matrix for key computation, of shape (d_model, d_k).\n    W_v: Weight matrix for value computation, of shape (d_model, d_v).\n    \n    Returns:\n    Output tensor after applying self-attention, of shape (N, seq_len, d_v).\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = W_q.shape[1]\n    attention_scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)\n    attention_scores = np.where(np.isnan(attention_scores), 0, attention_scores)\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    attention_output = attention_weights @ V\n    return np.round(attention_output, decimals=4).tolist()\nX = np.random.rand(1, 3, 4)\nW_q = np.random.rand(4, 4)\nW_k = np.random.rand(4, 4)\nW_v = np.random.rand(4, 4)"}
{"task_id": 53, "completion_id": 6, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implementation of the self-attention mechanism.\n\n    Parameters:\n    X (numpy.ndarray): Input sequence of shape (seq_len, d_model), where seq_len is the length of the sequence\n                       and d_model is the dimension of the model (input dimension).\n    W_q (numpy.ndarray): Query weight matrix.\n    W_k (numpy.ndarray): Key weight matrix.\n    W_v (numpy.ndarray): Value weight matrix.\n\n    Returns:\n    numpy.ndarray: The self-attention output as a (seq_len, d_model) numpy array.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_weights = np.dot(Q, K.T)\n    scaled_attention_weights = attention_weights / np.sqrt(K.shape[1])\n    softmax_attention = np.exp(scaled_attention_weights) / np.sum(np.exp(scaled_attention_weights), axis=-1, keepdims=True)\n    output = np.dot(softmax_attention, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 53, "completion_id": 7, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    - X: (S, D) numpy array, the input sequence of shape (sequence length, feature dimension).\n    - W_q, W_k, W_v: (D, D_a) numpy arrays, learnable parameters for query, key, and value transformations,\n                     where D_a is the attention dimension.\n    \n    Returns:\n    - output: (S, D_a) numpy array, the output of self-attention mechanism rounded to the 4th decimal.\n    \"\"\"\n    Q = np.matmul(X, W_q)\n    K = np.matmul(X, W_k)\n    V = np.matmul(X, W_v)\n    d_k = np.sqrt(K.shape[1])\n    attention_scores = np.matmul(Q, K.T) / d_k\n    attention_probs = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_probs, V)\n    output = np.round(output, decimals=4)\n    return output.tolist()\nX = np.array([[1, 0], [0, 1], [1, 1]])\nW_q = np.array([[1, 1], [0, 1]])\nW_k = np.array([[1, 0], [1, 1]])\nW_v = np.array([[0, 1], [1, 0]])\noutput = self_attention(X, W_q, W_k, W_v)"}
{"task_id": 53, "completion_id": 8, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input sequence tensor of shape (seq_length, embedding_dim).\n    W_q (numpy.ndarray): Query weight matrix of shape (embedding_dim, embedding_dim).\n    W_k (numpy.ndarray): Key weight matrix of shape (embedding_dim, embedding_dim).\n    W_v (numpy.ndarray): Value weight matrix of shape (embedding_dim, embedding_dim).\n    \n    Returns:\n    numpy.ndarray: The output of self-attention mechanism after applying softmax on attention scores\n                   and using it to weight the values. It should be a numpy array with the same shape as X,\n                   rounded to the nearest 4th decimal and converted to a python list.\n    \"\"\"\n    (seq_length, embedding_dim) = X.shape\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    attention_scores = np.dot(Q, K.T) / np.sqrt(embedding_dim)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output = np.round(output, 4)\n    return output.tolist()\nX = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\nW_q = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nW_k = np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\nW_v = np.array([[1, 2, 3], [3, 2, 1], [2, 3, 2]])"}
{"task_id": 53, "completion_id": 9, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Applies the self-attention mechanism to the input sequence.\n    \n    Parameters:\n        X (numpy array): The input sequence of shape (sequence_length, embedding_dimension).\n        W_q (numpy array): The weights for generating the queries, shape (embedding_dimension, dimension).\n        W_k (numpy array): The weights for generating the keys, shape (embedding_dimension, dimension).\n        W_v (numpy array): The weights for generating the values, shape (embedding_dimension, dimension).\n    \n    Returns:\n        numpy array: The output sequence after applying self-attention.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    attention_scores = Q @ K.T / np.sqrt(W_q.shape[1])\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    context_vector = attention_weights @ V\n    context_vector = np.round(context_vector, 4)\n    context_vector = context_vector.tolist()\n    return context_vector\nX = np.random.rand(5, 10)\nW_q = np.random.rand(10, 5)\nW_k = np.random.rand(10, 5)\nW_v = np.random.rand(10, 5)"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Perform forward pass for a simple RNN cell.\n    \n    Parameters:\n    - input_sequence: A list of input vectors for each time step.\n    - initial_hidden_state: Initial hidden state vector.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to 4 decimal places.\n    \"\"\"\n    H = np.array(initial_hidden_state)\n    inputs = np.array(input_sequence)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in inputs:\n        H = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, H) + b)\n    final_hidden_state = np.round(H, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4]]\ninitial_hidden_state = [0.5, 0.6]\nWx = [[0.7, 0.8], [0.9, 1.0]]\nWh = [[1.1, 1.2], [1.3, 1.4]]\nb = [1.5, 1.6]"}
{"task_id": 54, "completion_id": 1, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass for a simple RNN cell.\n    \n    Args:\n    - input_sequence (list of list of float): A sequence of input vectors.\n    - initial_hidden_state (list of float): The initial hidden state.\n    - Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    - Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    - b (list of float): Bias vector.\n    \n    Returns:\n    - list of float: The final hidden state after processing the input sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        h = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h) + b)\n    final_hidden_state = np.round(h, 4).tolist()\n    return final_hidden_state\ninput_sequence = [[2, 3], [4, 5], [6, 7]]\ninitial_hidden_state = [0, 0]\nWx = [[0.3, 0.1], [0.2, 0.4]]\nWh = [[0.2, 0.1], [0.4, 0.3]]\nb = [0.1, 0.2]"}
{"task_id": 54, "completion_id": 2, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(x.dot(Wx) + h.dot(Wh) + b)\n    h_rounded = np.around(h, decimals=4)\n    return h_rounded.tolist()"}
{"task_id": 54, "completion_id": 3, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to process a sequence of input vectors and produce the final hidden state.\n    \n    Parameters:\n    - input_sequence: A list of input vectors (each is a list of floats) to the RNN.\n    - initial_hidden_state: The initial hidden state of the RNN (list of floats).\n    - Wx: Weight matrix for input-to-hidden connections (list of lists of floats).\n    - Wh: Weight matrix for hidden-to-hidden connections (list of lists of floats).\n    - b: Bias vector (list of floats).\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to the nearest 4th decimal.\n    \"\"\"\n    hidden_state = np.array(initial_hidden_state, dtype=float)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        hidden_state = np.tanh(np.dot(x_t, Wx) + np.dot(hidden_state, Wh) + np.array(b))\n    final_hidden_state = np.round(hidden_state, decimals=4).tolist()\n    return final_hidden_state"}
{"task_id": 54, "completion_id": 4, "solution": "import numpy as np\ndef tanh(x):\n    \"\"\"Tanh activation function\"\"\"\n    return np.tanh(x)\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    RNN forward pass.\n    \n    Parameters:\n    input_sequence (list of list of float): Sequence of input vectors.\n    initial_hidden_state (list of float): The initial hidden state, h0.\n    Wx (list of list of float): Weight matrix of input-to-hidden connections.\n    Wh (list of list of float): Weight matrix of hidden-to-hidden connections.\n    b (list of float): Bias vector.\n    \n    Returns:\n    list of float: The final hidden state of the RNN, after the last input, rounded to 4th decimal.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_dim = initial_hidden_state.shape[0]\n    h = initial_hidden_state\n    for x_t in input_sequence:\n        h = tanh(np.dot(Wx, x_t) + np.dot(Wh, h) + b)\n    return np.round(h, decimals=4).tolist()"}
{"task_id": 54, "completion_id": 5, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to process a sequence of input vectors and produce the final hidden state.\n    \n    Parameters:\n    - input_sequence: A list of input vectors.\n    - initial_hidden_state: The initial hidden state of the RNN.\n    - Wx: The weight matrix for input-to-hidden connections.\n    - Wh: The weight matrix for hidden-to-hidden connections.\n    - b: The bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire input sequence.\n    \"\"\"\n    x = np.array(input_sequence)\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for i in x:\n        h = np.tanh(np.dot(Wx, i) + np.dot(Wh, h) + b)\n    return np.round(h, decimals=4).tolist()\ninput_sequence = [[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]]\ninitial_hidden_state = [0.1, 0.1]\nWx = [[0.2, 0.3], [0.3, 0.4]]\nWh = [[0.1, 0.2], [0.2, 0.3]]\nb = [0.1, 0.1]"}
{"task_id": 54, "completion_id": 6, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass for a simple RNN cell.\n    \n    Parameters:\n    - input_sequence (list of list of floats): Sequence of input vectors.\n    - initial_hidden_state (list of floats): The initial hidden state h_0.\n    - Wx (list of list of floats): Weight matrix for input to hidden state.\n    - Wh (list of list of floats): Weight matrix for hidden to hidden state.\n    - b (list of floats): The bias vector.\n    \n    Returns:\n    - list of floats: The final hidden state after processing the entire sequence.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for input_t in input_sequence:\n        hidden_state = np.tanh(np.dot(input_t, Wx) + np.dot(hidden_state, Wh) + b)\n    final_hidden_state = np.round(hidden_state, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[1, 2], [3, 4], [5, 6]]\ninitial_hidden_state = [0, 0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, -0.1]"}
{"task_id": 54, "completion_id": 7, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements an RNN cell. It takes a sequence of input vectors, the initial hidden state,\n    weight matrices for input-to-hidden and hidden-to-hidden connections, and a bias vector. It returns\n    the final hidden state after processing the entire input sequence, rounded to 4 decimal places.\n    \n    :param input_sequence: List of input vectors for each time step.\n    :param initial_hidden_state: Initial hidden state.\n    :param Wx: Input-to-hidden weight matrix.\n    :param Wh: Hidden-to-hidden weight matrix.\n    :param b: Bias vector.\n    :return: Final hidden state as a list.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for t in range(len(input_sequence)):\n        hidden_state = np.tanh(np.dot(input_sequence[t], Wx) + np.dot(hidden_state, Wh) + b)\n    return np.round(hidden_state, 4).tolist()"}
{"task_id": 54, "completion_id": 8, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell which uses tanh as the activation function for the hidden state.\n    \n    :param input_sequence: A list of input vectors each of which is a list of floats.\n    :param initial_hidden_state: The initial hidden state as a list of floats.\n    :param Wx: Input-to-hidden connections weight matrix as a list of lists.\n    :param Wh: Hidden-to-hidden connections weight matrix as a list of lists.\n    :param b: Bias vector as a list of floats.\n    :return: Final hidden state after processing the sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence_array = np.array(input_sequence)\n    H = np.array(initial_hidden_state)\n    Wx_array = np.array(Wx)\n    Wh_array = np.array(Wh)\n    b_array = np.array(b)\n    for t in range(input_sequence_array.shape[0]):\n        H = np.tanh(np.dot(Wx_array, input_sequence_array[t]) + np.dot(Wh_array, H) + b_array)\n    return np.round(H, 4).tolist()"}
{"task_id": 54, "completion_id": 9, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    This function implements the forward pass of a simple RNN cell. It iterates over the input sequence,\n    updating the hidden state at each step using tanh activation before producing the final hidden state.\n    \n    Parameters:\n    - input_sequence: list of input vectors for each time step\n    - initial_hidden_state: the initial hidden state of the RNN\n    - Wx: weight matrix for input-to-hidden connections\n    - Wh: weight matrix for hidden-to-hidden connections\n    - b: bias vector applied at each hidden state update\n    \n    Returns:\n    - The final hidden state as a list of floats, rounded to the nearest 4th decimal\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x_t in input_sequence:\n        h = np.tanh(Wx @ x_t + Wh @ h + b)\n    h = np.around(h, 4).tolist()\n    return h\ninput_sequence = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.4, 0.3], [0.2, 0.1]]\nb = [0.1, -0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2D translation to a list of points.\n    \n    :param points: List of [x, y] coordinate pairs.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of points.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_homogeneous = np.hstack((points_array, ones))\n    translated_points = np.dot(translation_matrix, points_homogeneous.T).T\n    translated_points_final = translated_points[:, :-1].tolist()\n    return translated_points_final\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3"}
{"task_id": 55, "completion_id": 1, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by given distances along the x and y axes.\n    \n    :param points: List of [x, y] coordinates\n    :param tx: Translation distance along the x-axis\n    :param ty: Translation distance along the y-axis\n    :return: Translated list of points as a list of [x, y] coordinates\n    \"\"\"\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_np = np.array(points)\n    ones = np.ones((points_np.shape[0], 1))\n    points_homo = np.hstack((points_np, ones))\n    translated_homo = np.dot(points_homo, translation_matrix.T)\n    translated_points = translated_homo[:, :2]\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 2, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by tx in the x-direction and ty in the y-direction.\n    \n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation distance in the x-direction.\n    - ty: Translation distance in the y-direction.\n    \n    Returns:\n    - List of translated [x, y] coordinates.\n    \"\"\"\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_matrix = np.array(points)\n    points_matrix = np.hstack([points_matrix, np.ones((points_matrix.shape[0], 1))])\n    translated_points = np.dot(points_matrix, translation_matrix.T)\n    translated_points = translated_points[:, :-1].tolist()\n    return translated_points"}
{"task_id": 55, "completion_id": 3, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Apply a 2D translation matrix to a set of points.\n    \n    Parameters:\n    points (list of lists): A list of [x, y] coordinates.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: A new list of points after applying the translation matrix.\n    \"\"\"\n    points_arr = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones_column = np.ones(points_arr.shape[0]).reshape(-1, 1)\n    points_arr_homogeneous = np.hstack((points_arr, ones_column))\n    translated_arr = np.dot(translation_matrix, points_arr_homogeneous.T).T\n    translated_points = translated_arr[:, :-1].tolist()\n    return translated_points\ntest_points = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 2\ntranslated_points = translate_object(test_points, tx, ty)"}
{"task_id": 55, "completion_id": 4, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of points.\n    :param points: List of [x, y] points.\n    :param tx: Translation distance in the x-direction.\n    :param ty: Translation distance in the y-direction.\n    :return: List of translated [x, y] points.\n    \"\"\"\n    points_np = np.array(points + [1] * len(points))\n    points_np = points_np.reshape(-1, 3)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points_np = np.dot(points_np, translation_matrix.T)\n    translated_points = translated_points_np[:, :2].tolist()\n    return translated_points"}
{"task_id": 55, "completion_id": 5, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by a specified translation matrix.\n    \n    Parameters:\n    points (list of lists): List of [x, y] coordinates.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: Translated list of points.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones_row = np.ones(len(points))\n    homogeneous_points = np.vstack((points_array.T, ones_row))\n    translated_points = np.dot(translation_matrix, homogeneous_points)\n    return translated_points[:2].T.tolist()"}
{"task_id": 55, "completion_id": 6, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of 2D points by specified distances in x and y directions.\n    \n    Parameters:\n    points (list of lists): Each sublist contains [x, y] coordinates of a point.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: Translated points as a list of [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_homogeneous = np.hstack((points_array, ones))\n    translated_matrix = np.dot(points_homogeneous, translation_matrix.T)\n    translated_points = translated_matrix[:, :2] / translated_matrix[:, 2, np.newaxis]\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 7, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by specified distances in the x and y directions.\n    \n    :param points: A list of [x, y] points to translate.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: A list of translated points.\n    \"\"\"\n    points_np = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_np.shape[0], 1))\n    points_with_ones = np.hstack((points_np, ones))\n    translated_points = np.dot(points_with_ones, translation_matrix.T)\n    return translated_points[:, :-1].tolist()\npoints = [[1, 2], [4, 5], [7, 8]]\ntx = 3\nty = 4\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 55, "completion_id": 8, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of points by tx and ty in the x and y direction respectively.\n    \n    Parameters:\n    points (list of list of float): List of [x, y] coordinates.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: Translated points as a list of [x, y] coordinates.\n    \"\"\"\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    points_homo = [point + [1] for point in points]\n    points_homo = np.array(points_homo).T\n    translated_points = np.dot(translation_matrix, points_homo)\n    translated_points_cart = translated_points[:2].T.tolist()\n    return translated_points_cart"}
{"task_id": 55, "completion_id": 9, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of 2D points by a given translation vector (tx, ty).\n    \n    Parameters:\n    points (list of lists): List of [x, y] points.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: The list of translated points.\n    \"\"\"\n    points_array = np.array(points)\n    translation_vector = np.array([tx, ty])\n    translated_points_array = points_array + translation_vector\n    return translated_points_array.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n    - mu_p: Mean of distribution P\n    - sigma_p: Standard deviation of distribution P\n    - mu_q: Mean of distribution Q\n    - sigma_q: Standard deviation of distribution Q\n\n    Returns:\n    - kl_div: KL divergence between distributions P and Q\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2"}
{"task_id": 56, "completion_id": 1, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two univariate normal distributions.\n    \n    Parameters:\n    - mu_p: mean of distribution P\n    - sigma_p: standard deviation of distribution P\n    - mu_q: mean of distribution Q\n    - sigma_q: standard deviation of distribution Q\n    \n    Returns:\n    - The KL divergence between the two distributions as a float.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive')\n    ratio = sigma_p / sigma_q\n    diff_means_sq = (mu_q - mu_p) ** 2\n    kl_div = np.log(ratio) + (sigma_p ** 2 + diff_means_sq) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 56, "completion_id": 2, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p, sigma_p: Mean and standard deviation of the first normal distribution.\n    - mu_q, sigma_q: Mean and standard deviation of the second normal distribution.\n    \n    Returns:\n    - The KL divergence as a float.\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_div = np.log(sigma_q / sigma_p) + (var_p + (mu_p - mu_q) ** 2) / (2 * var_q) - 0.5\n    return kl_div"}
{"task_id": 56, "completion_id": 3, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Args:\n    - mu_p: Mean of the first distribution (P).\n    - sigma_p: Standard deviation of the first distribution (P).\n    - mu_q: Mean of the second distribution (Q).\n    - sigma_q: Standard deviation of the second distribution (Q).\n    \n    Returns:\n    - float: KL divergence between P and Q.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = term1 + term2 - 0.5\n    return kl_div\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2"}
{"task_id": 56, "completion_id": 4, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n    - mu_p: Mean of distribution P.\n    - sigma_p: Standard deviation of distribution P.\n    - mu_q: Mean of distribution Q.\n    - sigma_q: Standard deviation of distribution Q.\n\n    Returns:\n    - float: KL divergence between the two distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Variance must be positive')\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    return term1 + term2 + term3"}
{"task_id": 56, "completion_id": 5, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Arguments:\n    mu_p -- Mean of the first distribution (P)\n    sigma_p -- Standard deviation of the first distribution (P)\n    mu_q -- Mean of the second distribution (Q)\n    sigma_q -- Standard deviation of the second distribution (Q)\n    \n    Returns:\n    float -- KL divergence between the two distributions\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive numbers.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nmu_p = 1\nsigma_p = 2\nmu_q = 2\nsigma_q = 3"}
{"task_id": 56, "completion_id": 6, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p (float): mean of the first normal distribution P\n    - sigma_p (float): standard deviation of the first normal distribution P\n    - mu_q (float): mean of the second normal distribution Q\n    - sigma_q (float): standard deviation of the second normal distribution Q\n    \n    Returns:\n    - kl_div (float): KL divergence between distribution P and Q\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Sigma must be positive')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)"}
{"task_id": 56, "completion_id": 7, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Each normal distribution is defined by its mean (mu) and \n    standard deviation (sigma), and the KL divergence is given\n    by: 0.5 * (log(sigma_q^2 / sigma_p^2) + (sigma_p^2 + (mu_p - mu_q)^2) / sigma_q^2 - 1)\n    \n    Parameters:\n    - mu_p (float): Mean of distribution P.\n    - sigma_p (float): Standard deviation of distribution P, must be > 0.\n    - mu_q (float): Mean of distribution Q.\n    - sigma_q (float): Standard deviation of distribution Q, must be > 0.\n    \n    Returns:\n    - float: The KL divergence between the two distributions.\n    \"\"\"\n    var_p = sigma_p ** 2\n    var_q = sigma_q ** 2\n    kl_div = 0.5 * (np.log(var_q / var_p) + (var_p + (mu_p - mu_q) ** 2) / var_q - 1)\n    return kl_div"}
{"task_id": 56, "completion_id": 8, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n    \n    Returns:\n    float: The KL divergence between distributions P and Q.\n    \"\"\"\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)"}
{"task_id": 56, "completion_id": 9, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first distribution P\n    sigma_p (float): Standard deviation of the first distribution P\n    mu_q (float): Mean of the second distribution Q\n    sigma_q (float): Standard deviation of the second distribution Q\n    \n    Returns:\n    float: The KL divergence between distribution P and Q\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviation must be positive')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel iterative method to solve a system of linear equations Ax = b.\n    \n    Parameters:\n    A (numpy.ndarray): The coefficient matrix, assumed to be square.\n    b (numpy.ndarray): The constant terms' column matrix.\n    n (int): The number of iterations to perform.\n    x_ini (numpy.ndarray, optional): An initial guess for the solution vector. If not provided, uses a vector of zeros.\n    \n    Returns:\n    list: The approximated solution vector as a list, rounded to the 4th decimal place.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(size):\n            d = x[i]\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n            d = x[i]\n    x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([24, -13, -9, 8])\nn = 20\nx = gauss_seidel(A, b, n)"}
{"task_id": 57, "completion_id": 1, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the linear equation system Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n        A (numpy.ndarray): A square coefficient matrix.\n        b (numpy.ndarray): The right-hand side vector.\n        n (int): The number of iterations.\n        x_ini (numpy.ndarray, optional): Initial guess for the solution vector. Defaults to None.\n        \n    Returns:\n        list: The approximated solution vector after n iterations as a list with elements rounded to the nearest 4th decimal.\n    \"\"\"\n    (rows, cols) = A.shape\n    if rows != cols:\n        raise ValueError('Matrix A must be a square matrix.')\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.double)\n    else:\n        x = x_ini.copy()\n    for k in range(n):\n        x_new = x.copy()\n        for i in range(rows):\n            s1 = A[i, :i].dot(x_new[:i])\n            s2 = A[i, i + 1:].dot(x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, decimals=4).tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]], dtype=float)\nb = np.array([1, 1, 1, 1], dtype=float)\nn = 25\nx = gauss_seidel(A, b, n)"}
{"task_id": 57, "completion_id": 2, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the given system of linear equations using the Gauss-Seidel method.\n    \n    :param A: np.array, a square matrix of coefficients\n    :param b: np.array, the right-hand side vector\n    :param n: int, the number of iterations\n    :param x_ini: np.array, the initial guess for the solution vector (optional)\n    :return: list, the solution vector after specified number of iterations\n    \"\"\"\n    (m, _) = A.shape\n    if x_ini is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x_ini)\n    for _ in range(n):\n        for i in range(m):\n            s = 0.0\n            for j in range(m):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[10, -1, 2], [-1, 10, -1], [2, -1, 10]])\nb = np.array([9, 10, 11])"}
{"task_id": 57, "completion_id": 3, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel iterative method for solving a linear system Ax = b.\n    \n    Parameters:\n    A (np.array): Coefficient matrix, assumed to be square.\n    b (np.array): Constant vector.\n    n (int): Number of iterations.\n    x_ini (np.array, optional): Initial guess vector. If not provided, defaults to a vector of zeros.\n    \n    Returns:\n    list: Approximated solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini.copy()\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([11, 16, -8, 23])"}
{"task_id": 57, "completion_id": 4, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implement the Gauss-Seidel method for solving linear equations Ax = b.\n    \n    Parameters:\n    - A: A square matrix of coefficients.\n    - b: The right-hand side vector.\n    - n: Number of iterations.\n    - x_ini: Optional; initial guess for x (default is a vector of zeros).\n    \n    Returns:\n    - x: Approximated solution vector after n iterations.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s1 = sum((A[i][j] * x[j] for j in range(i)))\n            s2 = sum((A[i][j] * x[j] for j in range(i + 1, A.shape[1])))\n            x[i] = (b[i] - s1 - s2) / A[i][i]\n    return np.round(x, 4).tolist()\nA = [[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]]\nb = [2, 5, 7, 13]\nn = 20\nx = gauss_seidel(A, b, n)"}
{"task_id": 57, "completion_id": 5, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a system of linear equations using the Gauss-Seidel method.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix (n x n).\n    b (numpy.ndarray): Right-hand side vector (n x 1).\n    n (int): Number of iterations.\n    x_ini (numpy.ndarray, optional): Initial guess for solution vector (n x 1). Defaults to None (vector of zeros).\n    \n    Returns:\n    list: The solution vector x after given iterations.\n    \"\"\"\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini.copy()\n    for _ in range(n):\n        for i in range(len(A)):\n            s1 = sum((A[i, j] * x[j] for j in range(i)))\n            s2 = sum((A[i, j] * x[j] for j in range(i + 1, len(A))))\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[4, 1, 0], [1, 5, 2], [0, 2, 6]])\nb = np.array([1, 2, 3])\nn = 100\nx_ini = np.array([1, 1, 1])"}
{"task_id": 57, "completion_id": 6, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A)\n    b = np.array(b)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini)\n    num_eq = A.shape[0]\n    for _ in range(n):\n        for i in range(num_eq):\n            s1 = sum((A[i][j] * x[j] for j in range(i)))\n            s2 = sum((A[i][j] * x[j] for j in range(i + 1, num_eq)))\n            x[i] = (b[i] - s1 - s2) / A[i][i]\n    return np.round(x, decimals=4).tolist()"}
{"task_id": 57, "completion_id": 7, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a system of linear equations using the Gauss-Seidel Iteration Method.\n    \n    :param A: numpy array, Coefficient matrix\n    :param b: numpy array, Right-hand side vector\n    :param n: int, Number of iterations\n    :param x_ini: numpy array, Initial guess vector. If not provided, initializes with zeros.\n    :return: numpy array, Approximate solution vector after n iterations\n    \"\"\"\n    (num_rows, num_cols) = np.shape(A)\n    assert num_rows == num_cols, 'Matrix A must be square.'\n    assert len(b) == num_rows, 'A and b dimensions are incompatible.'\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.double)\n    else:\n        x = np.array(x_ini, dtype=np.double)\n    for i in range(n):\n        for j in range(num_rows):\n            x[j] = (b[j] - np.dot(A[j, :j], x[:j]) - np.dot(A[j, j + 1:], x[j + 1:])) / A[j, j]\n        x = np.around(x, decimals=4)\n    return x.tolist()\nA = np.array([[3.0, 1.0, 2.0], [3.0, 5.0, 2.0], [6.0, 6.0, 8.0]])\nb = np.array([1.0, -2.0, 5.0])\nx = gauss_seidel(A, b, n=25)"}
{"task_id": 57, "completion_id": 8, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a linear system of equations using the Gauss-Seidel iterative method.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix, must be a square matrix.\n    b (numpy.ndarray): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (numpy.ndarray, optional): Initial guess for the solution vector. Defaults to a zero vector.\n    \n    Returns:\n    list: List containing the solution vector rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Coefficient matrix must be square.')\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=np.float64)\n    for i in range(n):\n        for j in range(A.shape[0]):\n            temp_x = x[j]\n            for k in range(A.shape[1]):\n                if k != j:\n                    temp_x -= A[j][k] * x[k]\n            x[j] = (temp_x + b[j]) / A[j][j]\n    return np.round(x, 4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 5\nx_ini = np.array([0.0, 0.0, 0.0, 0.0])"}
{"task_id": 57, "completion_id": 9, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a given square system of linear equations using the Gauss-Seidel method.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of the linear system.\n    b (np.array): The right-hand side vector of the linear system.\n    n (int): Number of iterations.\n    x_ini (np.array, optional): Initial guess for the solution vector. Defaults to None.\n    \n    Returns:\n    list: The approximated solution vector to the system after n iterations.\n    \"\"\"\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini.astype(float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = b[i]\n            for j in range(A.shape[1]):\n                if j != i:\n                    x[i] -= A[i, j] * x[j]\n            x[i] /= A[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([0, 5, -2, 7])\nn = 20\nx_ini = np.array([1, 1, 1, 1])"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian elimination with partial pivoting on matrix A and vector b to solve Ax = b.\n    \n    Args:\n    A (np.array): The coefficient matrix.\n    b (np.array): The constant vector.\n    \n    Returns:\n    list: Solution vector x as a list of values rounded to 4 decimal places.\n    \"\"\"\n    n = len(b)\n    Ab = np.column_stack((A, b))\n    for i in range(n):\n        max_row = max(range(i, n), key=lambda r: abs(Ab[r, i]))\n        if i != max_row:\n            Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - Ab[i, :-1] @ x) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[3, 2, -4], [2, 3, 3], [5, -3, 1]])\nb = np.array([3, 15, 14])"}
{"task_id": 58, "completion_id": 1, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Gaussian Elimination with partial pivoting to solve the system Ax = b.\n    \n    Parameters:\n    A (list of list of float): Coefficient matrix of the system.\n    b (list of float): Constants list on the right-hand side of the system.\n    \n    Returns:\n    list of float: The solution vector x.\n    \"\"\"\n    n = len(A)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        pivot_row = np.abs(Ab[i:, i]).argmax() + i\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[1, 3, 2], [4, 4, -3], [2, -3, 1]], dtype=float)"}
{"task_id": 58, "completion_id": 2, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): n x n matrix of coefficients.\n    b (np.array): n x 1 matrix of constants.\n    \n    Returns:\n    list: Solution vector x as a list of numbers (rounded to 4 decimal places).\n    \"\"\"\n    n = len(A)\n    A_aug = np.concatenate((A, b.reshape(n, 1)), axis=1)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A_aug[i:, i]))\n        A_aug[[i, max_row]] = A_aug[[max_row, i]]\n        for j in range(i + 1, n):\n            C = A_aug[j, i] / A_aug[i, i]\n            A_aug[j, i:] = A_aug[j, i:] - C * A_aug[i, i:]\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (A_aug[i, -1] - np.dot(A_aug[i, :-1], x)) / A_aug[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[3, 2, -4], [2, 3, 3], [5, -3, 1]])\nb = np.array([3, 15, 14])"}
{"task_id": 58, "completion_id": 3, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear equation system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix of the system.\n        b (np.ndarray): Column vector of the constants.\n    \n    Returns:\n        list: The solution vector x represented as a list.\n    \"\"\"\n    n = len(A)\n    m = len(b[0])\n    Ab = np.hstack([A, b])\n    for i in range(n):\n        pivot_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros((n, m))\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x[:, 0])) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[2, 1, 1], [1, 3, 2], [1, 0, 0]], dtype=float)\nb = np.array([[4], [5], [6]], dtype=float)"}
{"task_id": 58, "completion_id": 4, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian elimination with partial pivoting to solve the system Ax = b.\n    \n    Parameters:\n        A (numpy.ndarray): Coefficient matrix.\n        b (numpy.ndarray): Vector on the right side of the equation.\n    \n    Returns:\n        list: Solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    n = A.shape[0]\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for j in range(n):\n        pivot_row = np.argmax(np.abs(Ab[j:, j])) + j\n        Ab[[j, pivot_row]] = Ab[[pivot_row, j]]\n        for i in range(j + 1, n):\n            factor = Ab[i, j] / Ab[j, j]\n            Ab[i, j:] -= factor * Ab[j, j:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[1, 2, 3], [2, 5, 2], [2, 3, 8]], dtype=float)"}
{"task_id": 58, "completion_id": 5, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"Solve the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Args:\n    A: numpy.ndarray, the coefficient matrix of the system of equation.\n    b: numpy.ndarray, the constant terms of the system of equation.\n    \n    Returns:\n    A list of floats, the solution vector x for the system of linear equations.\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros_like(b)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-2], x[i + 1:])) / Ab[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 6, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear equation system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): The coefficient matrix.\n    b (np.array): The constant term vector.\n    \n    Returns:\n    list: Solution vector as a list.\n    \"\"\"\n    n = A.shape[0]\n    assert A.shape[1] == n, 'A must be a square matrix'\n    assert A.shape[0] == len(b), 'Length of b must equal the number of rows in A'\n    Ab = np.concatenate((A, b.reshape(n, 1)), axis=1)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3, 2, -4], [2, 3, 3], [5, -3, 1]])\nb = np.array([3, 15, 14])"}
{"task_id": 58, "completion_id": 7, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of the system.\n    b (np.array): Column vector of right-hand side constants.\n    \n    Returns:\n    list: Solution vector x with elements rounded to the 4th decimal place.\n    \"\"\"\n    n = len(b)\n    Ab = np.concatenate([A, b.reshape(n, 1)], axis=1)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        pivot = Ab[i, i]\n        for row in range(i + 1, n):\n            factor = Ab[row, i] / pivot\n            Ab[row, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[4, -1, 1], [4, -8, 1], [-2, 1, 5]])\nb = np.array([7, -21, 15])"}
{"task_id": 58, "completion_id": 8, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Perform Gaussian Elimination with partial pivoting to solve the system Ax = b.\n    \n    Parameters:\n    A (np.array): A square matrix (n x n).\n    b (np.array): A column vector (n x 1).\n    \n    Returns:\n    list: The solution vector (x) as a list.\n    \"\"\"\n    n = len(A)\n    A = A.astype(float)\n    b = b.astype(float)\n    Ab = np.hstack([A, b.reshape(n, 1)])\n    for i in range(n):\n        swap_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, swap_row]] = Ab[[swap_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j] -= factor * Ab[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return [round(num, 4) for num in x.tolist()]\nA = np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]])\nb = np.array([1, 2, 3])"}
{"task_id": 58, "completion_id": 9, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Constant vector.\n    \n    Returns:\n    list: Solution vector x rounded to 4 decimal places.\n    \"\"\"\n    n = A.shape[0]\n    Aug = np.concatenate([A, b.reshape(-1, 1)], axis=1)\n    for i in range(n):\n        max_element = abs(Aug[i, i])\n        max_row = i\n        for k in range(i + 1, n):\n            if abs(Aug[k, i]) > max_element:\n                max_element = abs(Aug[k, i])\n                max_row = k\n        Aug[[i, max_row]] = Aug[[max_row, i]]\n        for k in range(i + 1, n):\n            c = -Aug[k, i] / Aug[i, i]\n            Aug[k, i:] = Aug[k, i:] + c * Aug[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Aug[i, -1] - np.dot(Aug[i, :-1], x)) / Aug[i, i]\n    x_rounded = np.round(x, 4)\n    return x_rounded.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n\n        :param x: Input sequence (list of vectors).\n        :param initial_hidden_state: Initial hidden state.\n        :param initial_cell_state: Initial cell state.\n        :return: (list of hidden states, final hidden state, final cell state)\n        \"\"\"\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        hidden_states = []\n        for input_t in x:\n            concat_state = np.concatenate((h_t, input_t))\n            f_t = self.sigmoid(np.dot(self.Wf, concat_state) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat_state) + self.bi)\n            c_tilda = self.tanh(np.dot(self.Wc, concat_state) + self.bc)\n            c_t = f_t * c_t + i_t * c_tilda\n            o_t = self.sigmoid(np.dot(self.Wo, concat_state) + self.bo)\n            h_t = o_t * self.tanh(c_t)\n            hidden_states.append(h_t)\n        final_hidden_state = hidden_states[-1]\n        final_cell_state = c_t\n        final_hidden_state = np.round(final_hidden_state, 4).tolist()\n        final_cell_state = np.round(final_cell_state, 4).tolist()\n        return ([h_t.tolist() for h_t in hidden_states], final_hidden_state, final_cell_state)"}
{"task_id": 59, "completion_id": 1, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_seq = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(T):\n            xt = x[t].reshape(-1, 1)\n            combined = np.vstack([hidden_state, xt])\n            ft = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            candidate = self.tanh(np.dot(self.Wc, combined) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            cell_state = ft * cell_state + it * candidate\n            hidden_state = ot * self.tanh(cell_state)\n            hidden_seq.append(hidden_state)\n        hidden_seq = np.concatenate(hidden_seq, axis=-1).round(4).tolist()\n        final_hidden_state = hidden_state.round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n        return (hidden_seq, final_hidden_state, final_cell_state)\ninput_size = 5\nhidden_size = 4\nx = [np.random.randn(input_size) for _ in range(10)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 59, "completion_id": 2, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        if x.ndim == 2:\n            x = x[None, :, :]\n        assert x.shape[2] == self.input_size, 'Input size does not match the specified input size'\n        h = initial_hidden_state\n        c = initial_cell_state\n        (num_samples, seq_length, _) = x.shape\n        h_states = []\n        for t in range(seq_length):\n            xt = x[:, t, :]\n            xt = xt.transpose(1, 2, 0).reshape(-1, num_samples)\n            h = h.reshape(-1, num_samples)\n            c = c.reshape(-1, num_samples)\n            z = np.concatenate((h, xt), axis=0)\n            f_t = self.sigmoid(np.dot(self.Wf, z) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, z) + self.bi)\n            c_bar_t = self.tanh(np.dot(self.Wc, z) + self.bc)\n            c = f_t * c + i_t * c_bar_t\n            o_t = self.sigmoid(np.dot(self.Wo, z) + self.bo)\n            h = o_t * self.tanh(c)\n            h_states.append(h)\n        final_hidden_state = np.round(h, decimals=4).tolist()\n        final_cell_state = np.round(c, decimals=4).tolist()\n        hidden_states = [np.round(h, decimals=4).tolist() for h in h_states]\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_size = 2\nhidden_size = 3\nseq_length = 2\nx = np.random.randn(seq_length, input_size)[None, :, :]"}
{"task_id": 59, "completion_id": 3, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        h = initial_hidden_state\n        c = initial_cell_state\n        hidden_states = []\n        hidden_state_list = []\n        for t in range(T):\n            combined_input = np.concatenate((x[t], h), axis=0)[:, np.newaxis]\n            f = self.sigmoid(self.Wf @ combined_input + self.bf)\n            i = self.sigmoid(self.Wi @ combined_input + self.bi)\n            o = self.sigmoid(self.Wo @ combined_input + self.bo)\n            c_tilda = self.tanh(self.Wc @ combined_input + self.bc)\n            c = f * c + i * c_tilda\n            h = o * self.tanh(c)\n            hidden_state_list.append(h.flatten())\n            hidden_states.append(h)\n        final_hidden_state = hidden_state_list[-1]\n        final_cell_state = c.flatten()\n        return [np.array(hidden_state_list).round(4).tolist(), final_hidden_state.round(4).tolist(), final_cell_state.round(4).tolist()]\nx = [np.array([0, 0]), np.array([1, 0]), np.array([0, 1]), np.array([1, 1])]\ninitial_hidden_state = np.array([0, 0, 0])\ninitial_cell_state = np.array([0, 0, 0])"}
{"task_id": 59, "completion_id": 4, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        Args:\n            x: A list of numpy arrays, each representing an input vector for each time step.\n            initial_hidden_state: The initial hidden state as a numpy array.\n            initial_cell_state: The initial cell state as a numpy array.\n        Returns:\n            hidden_states: A list of hidden states at each time step.\n            final_hidden_state: The final hidden state after the last time step.\n            final_cell_state: The final cell state after the last time step.\n        \"\"\"\n        hidden_states = []\n        hs = initial_hidden_state\n        cs = initial_cell_state\n        for t in range(len(x)):\n            concat = np.concatenate((x[t], hs), axis=0).reshape(-1, 1)\n            f_t = self.sigmoid(self.Wf @ concat + self.bf)\n            i_t = self.sigmoid(self.Wi @ concat + self.bi)\n            c_tilda = self.tanh(self.Wc @ concat + self.bc)\n            o_t = self.sigmoid(self.Wo @ concat + self.bo)\n            cs = f_t * cs + i_t * c_tilda\n            hs = o_t * self.tanh(cs)\n            hidden_states.append(np.round(hs.flatten(), decimals=4))\n        final_hidden_state = np.round(hs.flatten(), decimals=4)\n        final_cell_state = np.round(cs.flatten(), decimals=4)\n        return (hidden_states, final_hidden_state.tolist(), final_cell_state.tolist())\ninput_size = 3\nhidden_size = 5\nx = [np.random.randn(input_size, 1) for _ in range(3)]\ninitial_hidden_state = np.random.randn(hidden_size, 1)\ninitial_cell_state = np.random.randn(hidden_size, 1)"}
{"task_id": 59, "completion_id": 5, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def _tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        seq_length = len(x)\n        h = initial_hidden_state\n        c = initial_cell_state\n        h_list = []\n        for t in range(seq_length):\n            xt = x[t].reshape(-1, 1)\n            ht = h.reshape(-1, 1)\n            xt_and_ht = np.vstack((xt, ht))\n            ft = self._sigmoid(self.Wf.dot(xt_and_ht) + self.bf)\n            it = self._sigmoid(self.Wi.dot(xt_and_ht) + self.bi)\n            cct = self._tanh(self.Wc.dot(xt_and_ht) + self.bc)\n            ot = self._sigmoid(self.Wo.dot(xt_and_ht) + self.bo)\n            c = ft * c + it * cct\n            h = ot * self._tanh(c)\n            h_list.append(np.round(h.flatten(), 4).tolist())\n        final_hidden_state = np.round(h.flatten(), 4).tolist()\n        final_cell_state = np.round(c.flatten(), 4).tolist()\n        return (h_list, final_hidden_state, final_cell_state)\nx = [np.array([1.0, 2.0]), np.array([2.0, 3.0]), np.array([3.0, 4.0])]\ninitial_hidden_state = np.array([0.1, 0.2, 0.3])\ninitial_cell_state = np.array([0.01, 0.02, 0.03])"}
{"task_id": 59, "completion_id": 6, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, tensor):\n        return 1 / (1 + np.exp(-tensor))\n\n    def tanh(self, tensor):\n        return np.tanh(tensor)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        hidden_states = []\n        cell_states = [initial_cell_state]\n        hidden_state_t = initial_hidden_state\n        cell_state_t = initial_cell_state\n        for t in range(len(x)):\n            input_t = np.concatenate((x[t], hidden_state_t), axis=0)\n            forget_gate = self.sigmoid(np.dot(self.Wf, input_t) + self.bf)\n            input_gate = self.sigmoid(np.dot(self.Wi, input_t) + self.bi)\n            candidate_cell_state = self.tanh(np.dot(self.Wc, input_t) + self.bc)\n            cell_state_t = forget_gate * cell_state_t + input_gate * candidate_cell_state\n            cell_states.append(cell_state_t)\n            output_gate = self.sigmoid(np.dot(self.Wo, input_t) + self.bo)\n            hidden_state_t = output_gate * self.tanh(cell_state_t)\n            hidden_states.append(hidden_state_t)\n        final_hidden_state = hidden_state_t\n        final_cell_state = cell_state_t\n        hidden_states_rounded = [h.round(4).tolist() for h in hidden_states]\n        final_hidden_state = final_hidden_state.round(4).tolist()\n        final_cell_state = final_cell_state.round(4).tolist()\n        return (hidden_states_rounded, final_hidden_state, final_cell_state)\nx = [np.array([[i], [i + 1], [i + 2]]) for i in range(4)]\ninitial_hidden_state = np.zeros((2, 1))\ninitial_cell_state = np.zeros((2, 1))"}
{"task_id": 59, "completion_id": 7, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        hidden_states = []\n        for t in range(x.shape[1]):\n            input_and_hidden = np.concatenate((x[:, t:t + 1], hidden_state), axis=0)\n            forget_gate = self.sigmoid(self.Wf @ input_and_hidden + self.bf)\n            input_gate = self.sigmoid(self.Wi @ input_and_hidden + self.bi)\n            candidate_cell_state = self.tanh(self.Wc @ input_and_hidden + self.bc)\n            output_gate = self.sigmoid(self.Wo @ input_and_hidden + self.bo)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n        hidden_states_list = np.hstack(hidden_states).tolist()\n        final_hidden_state_list = np.round(hidden_state, 4).tolist()\n        final_cell_state_list = np.round(cell_state, 4).tolist()\n        return (hidden_states_list, final_hidden_state_list, final_cell_state_list)\ninput_size = 3\nhidden_size = 2\nx = np.random.randn(input_size, 4)\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 59, "completion_id": 8, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_sequence = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for t in range(x.shape[1]):\n            concat_vector = np.concatenate((hidden_state, x[:, t:t + 1]), axis=0)\n            ft = self.sigmoid(self.Wf @ concat_vector + self.bf)\n            it = self.sigmoid(self.Wi @ concat_vector + self.bi)\n            gt = self.tanh(self.Wc @ concat_vector + self.bc)\n            ot = self.sigmoid(self.Wo @ concat_vector + self.bo)\n            cell_state = ft * cell_state + it * gt\n            hidden_state = ot * self.tanh(cell_state)\n            hidden_sequence.append(hidden_state)\n        hidden_sequence = np.concatenate(hidden_sequence, axis=1)\n        final_hidden_state = hidden_sequence[:, -1:]\n        final_cell_state = cell_state\n        hidden_sequence = np.round(hidden_sequence, 4).tolist()\n        final_hidden_state = np.round(final_hidden_state, 4).tolist()\n        final_cell_state = np.round(final_cell_state, 4).tolist()\n        return (hidden_sequence, final_hidden_state, final_cell_state)\nx = np.random.rand(3, 5)\ninitial_hidden_state = np.zeros((2, 1))\ninitial_cell_state = np.zeros((2, 1))"}
{"task_id": 59, "completion_id": 9, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_seq = []\n        hidden_state = initial_hidden_state\n        cell_state = initial_cell_state\n        for time_step in x:\n            combined = np.vstack((time_step, hidden_state))\n            forget_gate = self.sigmoid(self.Wf @ combined + self.bf)\n            input_gate = self.sigmoid(self.Wi @ combined + self.bi)\n            output_gate = self.sigmoid(self.Wo @ combined + self.bo)\n            candidate_cell_state = self.tanh(self.Wc @ combined + self.bc)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_seq.append(np.round(hidden_state, decimals=4).tolist())\n        return (hidden_seq, np.round(hidden_state, decimals=4).tolist(), np.round(cell_state, decimals=4).tolist())\nx = [np.array([0.1, 0.2, 0.3]).reshape(-1, 1)] * 5\ninitial_hidden_state = np.zeros((2, 1))\ninitial_cell_state = np.zeros((2, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for each query word in each document.\n    \n    Parameters:\n    - corpus: A list of documents, where each document is a list of words.\n    - query: A list of words for which you want to compute the TF-IDF scores.\n    \n    Returns:\n    - A list of lists containing the TF-IDF scores for the query words in each document, rounded to 4 decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus is empty.')\n    N = len(corpus)\n    df = {term: 0 for term in query}\n    for term in query:\n        for doc in corpus:\n            if term in doc:\n                df[term] += 1\n    tf_idfs = []\n    for doc in corpus:\n        doc_words_count = len(doc)\n        tf = {term: doc.count(term) / doc_words_count for term in query}\n        tf_idf_scores = []\n        for term in query:\n            idf = log((N + 1) / (df[term] + 1)) + 1\n            tf_idf = tf[term] * idf\n            tf_idf_scores.append(round(tf_idf, 4))\n        tf_idfs.append(tf_idf_scores)\n    return tf_idfs\ncorpus = [['hello world', 'world'], ['hello universe', 'universe']]\nquery = ['hello', 'world', 'universe']"}
{"task_id": 60, "completion_id": 1, "solution": "import numpy as np\nfrom math import log\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    N = len(corpus)\n    idf = {}\n    for word in query:\n        df = sum((word in document for document in corpus))\n        idf[word] = log((N + 1) / (df + 1)) + 1\n    tf_idf_scores = []\n    for document in corpus:\n        tf_idf_document = []\n        for word in query:\n            if word not in document:\n                tf_idf_document.append(0.0)\n                continue\n            tf = document.count(word) / len(document)\n            idf_word = idf.get(word, 0.0)\n            tf_idf = tf * idf_word\n            tf_idf_document.append(round(tf_idf, 4))\n        tf_idf_scores.append(tf_idf_document)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['hello', 'world', 'hello'], ['hello', 'python'], ['python', 'programming'], []]\nquery = ['hello', 'python', 'programming', 'world']"}
{"task_id": 60, "completion_id": 2, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    tf_matrix = []\n    for doc in corpus:\n        tf_doc = {}\n        for word in doc:\n            tf_doc[word] = tf_doc.get(word, 0) + 1\n        for word in query:\n            tf_doc[word] = tf_doc.get(word, 0) / len(doc)\n        tf_matrix.append([tf_doc.get(word, 0.0) for word in query])\n    df_dict = {}\n    for doc in corpus:\n        for word in set(doc):\n            df_dict[word] = df_dict.get(word, 0) + 1\n    idf_dict = {}\n    num_docs = len(corpus)\n    for word in query:\n        df = df_dict.get(word, 0) + 1\n        idf_dict[word] = math.log(num_docs / df) + 1\n    tf_idf_matrix = []\n    for tf_doc in tf_matrix:\n        tf_idf_doc = [tf * idf_dict.get(word) for (tf, word) in zip(tf_doc, query)]\n        tf_idf_matrix.append([round(score, 4) for score in tf_idf_doc])\n    return tf_idf_matrix\ncorpus = [['hello', 'world'], ['python', 'programming'], ['machine', 'learning']]\nquery = ['python', 'world']"}
{"task_id": 60, "completion_id": 3, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty.')\n    df = {}\n    for doc in corpus:\n        for term in set(doc):\n            if term in df:\n                df[term] += 1\n            else:\n                df[term] = 1\n    N = len(corpus)\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term)\n            idf = math.log((N + 1) / (df.get(term, 0) + 1))\n            tf_idf = tf * idf\n            doc_tf_idf.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_tf_idf)\n    return tf_idf_scores"}
{"task_id": 60, "completion_id": 4, "solution": "import numpy as np\nimport math\nfrom typing import List\ndef compute_tf_idf(corpus: List[List[str]], query: List[str]) -> List[List[float]]:\n    if not corpus:\n        return []\n\n    def tf(term, document):\n        return document.count(term) / len(document)\n\n    def idf(term, all_docs):\n        df = sum((int(term in doc) for doc in all_docs))\n        return math.log((len(all_docs) + 1) / (df + 1)) + 1\n\n    def tf_idf(term, corpus, document):\n        return tf(term, document) * idf(term, corpus)\n    results = []\n    for doc in corpus:\n        doc_results = [round(tf_idf(term, corpus, doc), 4) for term in query]\n        results.append(doc_results)\n    return np.array(results).tolist()\ncorpus = [['this', 'is', 'the', 'first', 'document'], ['this', 'document', 'is', 'the', 'second', 'document'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'fourth', 'one']]\nquery = ['document', 'is', 'third']"}
{"task_id": 60, "completion_id": 5, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for a given query against a corpus.\n    \n    :param corpus: List of documents, where each document is a list of words.\n    :param query: List of words for which TF-IDF scores are to be computed.\n    :return: List of lists containing the TF-IDF scores for the query words in each document.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Empty corpus, unable to compute TF-IDF scores.')\n    tf_matrix = []\n    df_dict = {term: 0 for term in query}\n    for doc in corpus:\n        freq = {term: doc.count(term) / len(doc) for term in query}\n        tf_matrix.append(freq)\n        for term in set(doc):\n            if term in df_dict:\n                df_dict[term] += 1\n    idf_dict = {term: math.log((1 + len(corpus)) / (1 + df)) for (term, df) in df_dict.items()}\n    tf_idf_scores = []\n    for doc_tf in tf_matrix:\n        doc_scores = [round(doc_tf[term] * idf_dict[term], 4) if term in doc_tf else 0.0 for term in query]\n        tf_idf_scores.append(doc_scores)\n    return tf_idf_scores\ncorpus = [['data', 'mining', 'data', 'exploration'], ['machine', 'learning', 'data', 'algorithms'], ['data', 'science', 'data', 'management']]\nquery = ['data', 'machine', 'mining']"}
{"task_id": 60, "completion_id": 6, "solution": "import numpy as np\nimport math\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    df = Counter()\n    for doc in corpus:\n        unique_words = set(doc)\n        for word in unique_words:\n            if word in query:\n                df[word] += 1\n    idf = {word: math.log((1 + len(corpus)) / (1 + df[word])) if df[word] else 0 for word in query}\n    tf_idf_results = []\n    for doc in corpus:\n        tf = Counter(doc)\n        doc_scores = [round(tf[word] * idf[word], 4) if word in tf else 0.0 for word in query]\n        tf_idf_results.append(doc_scores)\n    return np.array(tf_idf_results).tolist()\ncorpus = [['the', 'cat', 'is', 'on', 'the', 'roof'], ['the', 'dog', 'is', 'under', 'the', 'roof']]\nquery = ['the', 'cat', 'dog', 'roof']"}
{"task_id": 60, "completion_id": 7, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty!')\n\n    def term_frequency(term, document):\n        return document.count(term) / float(len(document))\n\n    def inverse_document_frequency(term, corpus):\n        num_documents_with_term = sum((1 for document in corpus if term in document))\n        if num_documents_with_term > 0:\n            return np.log10((1 + len(corpus)) / (1 + num_documents_with_term))\n        else:\n            return np.log10(1 + len(corpus))\n    tf_idf_scores = []\n    for document in corpus:\n        document_scores = []\n        for term in query:\n            tf = term_frequency(term, document)\n            idf = inverse_document_frequency(term, corpus)\n            document_scores.append(tf * idf)\n        tf_idf_scores.append(document_scores)\n    tf_idf_scores = np.around(tf_idf_scores, decimals=4).tolist()\n    return tf_idf_scores\ncorpus = [['python', 'programming', 'is', 'fun'], ['i', 'love', 'python'], ['programming', 'in', 'python', 'is', 'awesome'], []]\nquery = ['python', 'love', 'programming', 'fun']"}
{"task_id": 60, "completion_id": 8, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores for a query against a given corpus of documents.\n    \n    :param corpus: list of documents, where each document is a list of words.\n    :param query: list of words for which to compute the TF-IDF scores.\n    :return: list of lists containing the TF-IDF scores for query words in each document.\n    \"\"\"\n    if len(corpus) == 0:\n        raise ValueError('Corpus must contain at least one document.')\n\n    def compute_tf(document, word):\n        return sum((1 for w in document if w == word)) / len(document)\n\n    def compute_idf(corpus, word):\n        return math.log((1 + len(corpus)) / (1 + sum((1 for doc in corpus if word in doc))))\n    tf_idf_scores = [[] for _ in range(len(corpus))]\n    for (doc_idx, document) in enumerate(corpus):\n        for word in query:\n            tf = compute_tf(document, word)\n            idf = compute_idf(corpus, word)\n            tf_idf_scores[doc_idx].append(tf * idf)\n    tf_idf_scores_rounded = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores_rounded"}
{"task_id": 60, "completion_id": 9, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    num_docs = len(corpus)\n    query_words = set(query)\n    document_freqs = {word: 1 for word in query_words}\n    for doc in corpus:\n        appeared = set()\n        for word in doc:\n            if word in query_words and word not in appeared:\n                document_freqs[word] += 1\n                appeared.add(word)\n    idf_scores = {word: math.log((num_docs + 1) / df) for (word, df) in document_freqs.items()}\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_tf_idf = []\n        word_counts = {word: 0 for word in query_words}\n        for word in doc:\n            if word in query_words:\n                word_counts[word] += 1\n        doc_word_count = sum(word_counts.values()) + 1\n        for q_word in query:\n            tf = word_counts[q_word] / doc_word_count\n            doc_tf_idf.append(round(tf * idf_scores[q_word], 4))\n        tf_idf_scores.append(doc_tf_idf)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['hello', 'world', 'python', 'programming'], ['hello', 'java', 'programming', 'world'], ['python', 'code']]\nquery = ['hello', 'python', 'code']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_beta_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if precision + recall > 0 else 0\n    return round(f_beta_score, 3)\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 0, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 1, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision == 0 and recall == 0:\n        f_beta = 0.0\n    else:\n        f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 61, "completion_id": 2, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    beta_squared = beta ** 2\n    numerator = (1 + beta_squared) * (precision * recall)\n    denominator = beta_squared * precision + recall\n    f_score = numerator / denominator if denominator > 0 else 0\n    return round(f_score, 3)\ny_true = np.array([1, 1, 0, 1, 0, 0, 0, 1, 1, 1])\ny_pred = np.array([1, 0, 0, 1, 1, 0, 0, 0, 1, 0])\nbeta = 2"}
{"task_id": 61, "completion_id": 3, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    beta_squared = beta ** 2\n    f_beta_score = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall) if precision + recall > 0 else 0\n    return round(f_beta_score, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 4, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    if precision == 0 or recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\ny_true = np.random.randint(2, size=50)\ny_pred = np.random.randint(2, size=50)"}
{"task_id": 61, "completion_id": 5, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    beta2 = beta ** 2\n    f_beta_score = (1 + beta2) * precision * recall / (beta2 * precision + recall) if precision > 0 or recall > 0 else 0\n    return round(f_beta_score, 3)\ny_true = np.array([0, 1, 1, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nbeta = 2.0"}
{"task_id": 61, "completion_id": 6, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    epsilon = 1e-10\n    precision = precision if precision != 0 else epsilon\n    recall = recall if recall != 0 else epsilon\n    f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return np.round(f_beta, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 7, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_beta = 0\n    else:\n        f_beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 8, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    if precision == 0 and recall == 0:\n        fScore = 0.0\n    else:\n        fScore = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(fScore, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 61, "completion_id": 9, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FP = np.sum((y_pred == 1) & (y_true == 0))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    if precision == 0 and recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)\ny_true = np.array([1, 0, 0, 1, 1, 0, 1, 0, 1, 0])\ny_pred = np.array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0])\nbeta = 1"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, z):\n        \"\"\"\n        Sigmoid activation function.\n        \"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(x)\n        hidden_state = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for t in range(T):\n            input_t = np.array(x[t]).reshape(-1, 1)\n            hidden_input = np.dot(self.W_xh, input_t) + np.dot(self.W_hh, hidden_state) + self.b_h\n            hidden_state = self.sigmoid(hidden_input)\n            output = np.dot(self.W_hy, hidden_state) + self.b_y\n            outputs.append(output)\n            last_inputs.append(hidden_input)\n            last_hiddens.append(hidden_state)\n        return (np.array(outputs), np.array(last_inputs), np.array(last_hiddens))\n\n    def backward(self, x, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \"\"\"\n        T = len(x)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dhprev = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(T)):\n            input_t = np.array(x[t]).reshape(-1, 1)\n            dy = outputs[t] - np.array(expected_output[t]).reshape(-1, 1)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dhprev\n            dh_raw = dh * (last_hiddens[t] * (1 - last_hiddens[t]))\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else 0\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, input_t.T)\n            dhprev = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def mse_loss(self, expected_output, outputs):\n        \"\"\"\n        Mean Squared Error Loss Function.\n        \"\"\"\n        loss = 0\n        for t in range(len(expected_output)):\n            y = np.array(expected_output[t]).reshape(-1, 1)\n            output = outputs[t]\n            loss += 0.5 * np.sum((output - y) ** 2)\n        return loss"}
{"task_id": 62, "completion_id": 1, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hiddens = np.zeros((self.hidden_size, T + 1))\n        outputs = np.zeros((self.hidden_size, T))\n        last_inputs = np.zeros((self.hidden_size, T))\n        hiddens[:, 0] = np.zeros(self.hidden_size)\n        for t in range(T):\n            input_t = input_sequence[t].reshape(-1, 1)\n            hiddens[:, t + 1] = np.tanh(np.dot(self.W_xh, input_t) + np.dot(self.W_hh, hiddens[:, t].reshape(-1, 1)) + self.b_h).reshape(self.hidden_size)\n            output_t = np.dot(self.W_hy, hiddens[:, t + 1].reshape(-1, 1)) + self.b_y\n            outputs[:, t] = output_t.reshape(self.hidden_size)\n            last_inputs[:, t] = np.tanh(np.dot(self.W_xh, input_t) + np.dot(self.W_hh, hiddens[:, t].reshape(-1, 1)) + self.b_h).reshape(self.hidden_size)\n        return (outputs, last_inputs, hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_state, learning_rate):\n        \"\"\"\n        Backward pass through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        W_xh_gradient = np.zeros_like(self.W_xh)\n        W_hh_gradient = np.zeros_like(self.W_hh)\n        W_hy_gradient = np.zeros_like(self.W_hy)\n        b_h_gradient = np.zeros_like(self.b_h)\n        b_y_gradient = np.zeros_like(self.b_y)\n        for t in reversed(range(T)):\n            input_t = input_sequence[t].reshape(-1, 1)\n            output_t = outputs[:, t].reshape(-1, 1)\n            hidden_t = hidden_state[:, t + 1].reshape(-1, 1)\n            dy = output_t - expected_output[t].reshape(-1, 1)\n            dh = np.dot(W_hy_gradient.T, dy) * (1 - last_inputs[:, t].reshape(-1, 1) ** 2)\n            W_hy_gradient += np.dot(dy, hidden_t.T)\n            b_y_gradient += dy\n            for ts in reversed(range(t + 1)):\n                hidden_state_t = hidden_state[:, ts].reshape(-1, 1)\n                W_xh_gradient += np.dot(dh, input_t.T)\n                W_hh_gradient += np.dot(dh, hidden_state_t.T)\n                b_h_gradient += dh\n                dh = np.dot(W_hh.T, dh) * (1 - hidden_state_t ** 2)\n                input_t = input_sequence[ts].reshape(-1, 1)\n        self.W_xh -= learning_rate * W_xh_gradient\n        self.W_hh -= learning_rate * W_hh_gradient\n        self.W_hy -= learning_rate * W_hy_gradient\n        self.b_h -= learning_rate * b_h_gradient\n        self.b_y -= learning_rate * b_y_gradient\ninput_size = 3\nhidden_size = 2\noutput_size = 3\ninput_sequence = [np.array([1, 2, 3]), np.array([1, 2, 3]), np.array([1, 2, 3])]"}
{"task_id": 62, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef dsigmoid(x):\n    return x * (1 - x)\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        T = len(x)\n        hprev = np.zeros((self.hidden_size, 1))\n        for t in range(T):\n            input_t = np.array(x[t]).reshape(-1, 1)\n            z_h = self.W_xh @ input_t + self.W_hh @ hprev + self.b_h\n            h = sigmoid(z_h)\n            z_y = self.W_hy @ h + self.b_y\n            y = sigmoid(z_y)\n            outputs.append(y)\n            last_inputs.append(input_t)\n            last_hiddens.append(h)\n            hprev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, x, y, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(x)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dhprev = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(T)):\n            input_t = last_inputs[t]\n            h_t = last_hiddens[t]\n            output_t = outputs[t]\n            dy = (output_t - y[t].reshape(-1, 1)) * dsigmoid(output_t)\n            dW_hy += dy @ h_t.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dhprev\n            dh_raw = dh * dsigmoid(h_t)\n            dW_xh += dh_raw @ input_t.T\n            dW_hh += dh_raw @ last_hiddens[t - 1].T if t > 0 else np.zeros_like(dW_hh)\n            db_h += dh_raw\n            dhprev = self.W_hh.T @ dh_raw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, xs, ys, learning_rate=0.1):\n        (outputs, last_inputs, last_hiddens) = self.forward(xs)\n        self.backward(xs, ys, outputs, last_inputs, last_hiddens, learning_rate)\n        return self.loss(ys, outputs)\n\n    def predict(self, xs):\n        (outputs, _, _) = self.forward(xs)\n        return [output.argmax() for output in outputs]\n\n    def loss(self, ys, outputs):\n        loss = 0.0\n        for (i, output) in enumerate(outputs):\n            y_true = ys[i].reshape(-1, 1)\n            loss += 0.5 * np.mean((y_true - output) ** 2)\n        return loss"}
{"task_id": 62, "completion_id": 3, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n        self.hidden = np.zeros((hidden_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hiddens = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        last_inputs = []\n        for (i, x_t) in enumerate(x):\n            x_t = np.array(x_t).reshape(-1, 1)\n            last_inputs.append(x_t)\n            hidden = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, hiddens[i]) + self.b_h)\n            hiddens.append(hidden)\n            output = np.dot(self.W_hy, hidden) + self.b_y\n            outputs.append(output)\n        return (outputs, last_inputs, hiddens)\n\n    def backward(self, x, y, outputs, last_inputs, hiddens, learning_rate=0.01):\n        \"\"\"\n        Perform backward propagation through time (BPTT) and update the gradients.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dhnext = np.zeros_like(hiddens[0])\n        loss = 0\n        for t in reversed(range(len(x))):\n            y_t = np.array(y[t]).reshape(-1, 1)\n            loss += 0.5 * (outputs[t] - y_t) ** 2\n            d_output = outputs[t] - y_t\n            dh = np.dot(self.W_hh.T, dhnext) + np.dot(self.W_hy.T, d_output)\n            dW_hy += np.dot(d_output, hiddens[t + 1].T)\n            db_y += d_output\n            dhraw = (1 - hiddens[t + 1] ** 2) * dh\n            dW_xh += np.dot(dhraw, last_inputs[t].T)\n            dW_hh += np.dot(dhraw, hiddens[t].T)\n            db_h += dhraw\n            dhnext = np.dot(self.W_hh.T, dhraw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 62, "completion_id": 4, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hidden_states = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        timesteps = len(input_sequence)\n        for t in range(timesteps):\n            current_input = input_sequence[t].reshape(-1, 1)\n            ht = np.tanh(self.W_hh @ hidden_states[t] + self.W_xh @ current_input + self.b_h)\n            yt = self.W_hy @ ht + self.b_y\n            hidden_states.append(ht)\n            outputs.append(yt)\n        return (outputs, hidden_states[-1], hidden_states[:-1])\n\n    def backward(self, input_sequence, expected_output, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected output.\n        \"\"\"\n        dW_xh = np.zeros(self.W_xh.shape)\n        dW_hh = np.zeros(self.W_hh.shape)\n        dW_hy = np.zeros(self.W_hy.shape)\n        db_h = np.zeros(self.b_h.shape)\n        db_y = np.zeros(self.b_y.shape)\n        (outputs, last_hidden, hidden_states) = self.forward(input_sequence)\n        temp_loss = 0\n        for t in reversed(range(len(input_sequence))):\n            current_input = input_sequence[t].reshape(-1, 1)\n            current_output = outputs[t]\n            delta_y = current_output - expected_output[t].reshape(-1, 1)\n            dW_hy += delta_y @ hidden_states[t].T\n            db_y += delta_y\n            dhnext = self.W_hy.T @ delta_y\n            for bptt_step in reversed(range(t + 1)):\n                current_input = input_sequence[bptt_step].reshape(-1, 1)\n                ht = hidden_states[bptt_step]\n                dhraw = (1 - ht ** 2) * (dhnext + dhraw if bptt_step != t else dhnext)\n                dW_xh += dhraw @ current_input.T\n                dW_hh += dhraw @ hidden_states[bptt_step - 1].T\n                db_h += dhraw\n                dhnext = self.W_hh.T @ dhraw\n            dhraw = None\n            temp_loss += 0.5 * np.sum(delta_y ** 2)\n        for (param, dparam) in zip([self.W_hy, self.W_hh, self.W_xh, self.b_h, self.b_y], [dW_hy, dW_hh, dW_xh, db_h, db_y]):\n            param -= learning_rate * dparam\n        return temp_loss\ninput_size = 2\nhidden_size = 3\noutput_size = 2\ninput_sequence = np.array([[1, 1], [2, 2], [3, 3]])"}
{"task_id": 62, "completion_id": 5, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        input_sequence: list of np.array. Each array is an input vector.\n        returns: last_inputs, last_hiddens, outputs\n        \"\"\"\n        T = len(input_sequence)\n        hidden_state = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for t in range(T):\n            x_t = input_sequence[t].reshape(-1, 1)\n            hidden_state = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, hidden_state) + self.b_h)\n            output = np.dot(self.W_hy, hidden_state) + self.b_y\n            last_inputs.append(np.concatenate((x_t.flatten(), hidden_state.flatten())))\n            last_hiddens.append(hidden_state)\n            outputs.append(output)\n        return (last_inputs, last_hiddens, outputs)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Backward pass through the RNN using backpropagation through time.\n        input_sequence: list of np.array\n        expected_output: list of np.array\n        outputs: list of np.array\n        last_inputs: list of np.array, from forward pass\n        last_hiddens: list of np.array, from forward pass\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(T)):\n            error = outputs[t] - expected_output[t].reshape(-1, 1)\n            dW_hy += np.dot(error, last_hiddens[t].T)\n            db_y += error\n            dh += np.dot(self.W_hy.T, error)\n            dtanh = (1 - np.power(last_hiddens[t], 2)) * dh\n            dW_hh += np.dot(dtanh, last_hiddens[t - 1].T) if t != 0 else 0\n            dh = np.dot(self.W_hh.T, dtanh)\n            dW_xh += np.dot(dtanh, input_sequence[t].reshape(-1, 1).T)\n            db_h += dtanh\n            self.W_hy -= learning_rate * dW_hy\n            self.W_hh -= learning_rate * dW_hh\n            self.W_xh -= learning_rate * dW_xh\n            self.b_y -= learning_rate * db_y\n            self.b_h -= learning_rate * db_h\n            dW_hy *= 0\n            dW_hh *= 0\n            dW_xh *= 0\n            db_y *= 0\n            db_h *= 0"}
{"task_id": 62, "completion_id": 6, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def _tanh(self, x):\n        return np.tanh(x)\n\n    def _tanh_derivative(self, x):\n        return 1 - np.power(np.tanh(x), 2)\n\n    def _affine_forward(self, w, x, b):\n        return np.dot(w, x) + b\n\n    def _affine_backward(self, dz, w):\n        grads = (dw, dx, db) = (np.dot(dz, w.T), np.dot(w.T, dz), dz)\n        return grads\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns: outputs, last_inputs, last_hiddens (for each time step)\n        \"\"\"\n        (hs, ys, xs) = ({}, {}, {})\n        hs[-1] = np.zeros((self.hidden_size, 1))\n        T = len(input_sequence)\n        outputs = []\n        for t in range(T):\n            hs[t] = self._tanh(self._affine_forward(self.W_hh, hs[t - 1], self.b_h) + self._affine_forward(self.W_xh, input_sequence[t][np.newaxis].T, self.b_h))\n            ys[t] = self._affine_forward(self.W_hy, hs[t], self.b_y)\n            outputs.append(ys[t])\n        return (outputs, xs, hs)\n\n    def backwards(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        Adjusts the weights based on the loss.\n        \"\"\"\n        gradients = {}\n        for weight in ['W_hh', 'W_xh', 'W_hy', 'b_h', 'b_y']:\n            gradients[weight] = np.zeros_like(getattr(self, weight))\n        hs = hidden_states\n        xs = {t: seq[np.newaxis].T for (t, seq) in enumerate(input_sequence)}\n        T = len(input_sequence)\n        losses = []\n        for t in reversed(range(T)):\n            error = ys[t] - expected_output[t][np.newaxis].T\n            dh = np.dot(self.W_hy.T, error) + gradients['dh']\n            gradients['W_hy'] += np.dot(error, hs[t].T)\n            gradients['b_y'] += error\n            dhidden = (1 - hs[t] ** 2) * dh\n            gradients['W_hh'] += np.dot(dhidden, hs[t - 1].T)\n            gradients['W_xh'] += np.dot(dhidden, xs[t].T)\n            gradients['b_h'] += dhidden\n            losses.append(0.5 * np.sum(error ** 2))\n        for weight in ['W_hh', 'W_xh', 'W_hy', 'b_h', 'b_y']:\n            setattr(self, weight, getattr(self, weight) - learning_rate * gradients[weight])\n        return np.sum(losses)\nx = [0, 1, 0, 1]"}
{"task_id": 62, "completion_id": 7, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_derivative(self, z):\n        return z * (1 - z)\n\n    def forward(self, input_sequence):\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        inputs = []\n        hiddens = [h_prev]\n        for i in input_sequence:\n            x = np.array([[i]])\n            z_h = np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h\n            h = self.sigmoid(z_h)\n            z_y = np.dot(self.W_hy, h) + self.b_y\n            y = self.sigmoid(z_y)\n            h_prev = h\n            outputs.append(y)\n            inputs.append(x)\n            hiddens.append(h)\n        return (outputs[-1], inputs, hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, inputs, hiddens, learning_rate):\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dnext_h = np.zeros_like(hiddens[0])\n        loss = 1 / 2 * np.sum(np.square(expected_output - outputs[-1]))\n        delta_y = -(expected_output - outputs[-1])\n        dW_hy += np.dot(delta_y, hiddens[-1].T)\n        db_y += delta_y\n        delta_h = np.zeros_like(hiddens[0])\n        for t in reversed(range(len(input_sequence))):\n            delta_h = np.dot(self.W_hy.T, delta_y) + dnext_h\n            delta_h_raw = delta_h * self.sigmoid_derivative(hiddens[t + 1])\n            dW_hh += np.dot(delta_h_raw, hiddens[t].T)\n            dW_xh += np.dot(delta_h_raw, inputs[t].T)\n            db_h += delta_h_raw\n            dnext_h = np.dot(self.W_hh.T, delta_h_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -1, 1, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\ninput_size = 3\nhidden_size = 4\noutput_size = 1\ninput_sequence = np.array([[0.2, 0.4, 0.6], [0.4, 0.6, 0.8], [0.6, 0.8, 1.0]])\nexpected_output = np.array([[1.0]])\nlearning_rate = 0.1\nloss = rnn.backward(input_sequence, expected_output, outputs, inputs, hiddens, learning_rate)"}
{"task_id": 62, "completion_id": 8, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def _forward_step(self, x_t, h_t_minus_1):\n        \"\"\"\n        Single step of the forward pass in the RNN.\n        :param x_t: input at time step t\n        :param h_t_minus_1: hidden state from the previous time step\n        :return: hidden state and output at this time step\n        \"\"\"\n        h_t = np.tanh(self.W_xh @ x_t + self.W_hh @ h_t_minus_1 + self.b_h)\n        y_t = self.W_hy @ h_t + self.b_y\n        return (h_t, y_t)\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs, and the hidden states.\n        \n        :param input_sequence: A list of input vectors for the forward pass.\n        :return: last_inputs, last_hiddens, and outputs\n        \"\"\"\n        T = len(input_sequence)\n        last_inputs = [None] * T\n        last_hiddens = np.zeros((self.hidden_size, 1))\n        outputs = []\n        for t in range(T):\n            x_t = input_sequence[t].reshape(-1, 1)\n            (last_hiddens, y_t) = self._forward_step(x_t, last_hiddens)\n            last_inputs[t] = x_t\n            outputs.append(y_t)\n        return (last_inputs, last_hiddens, np.array(outputs))\n\n    def _backward_step(self, d_h_T, x_t, h_t, h_t_minus_1):\n        \"\"\"\n        Single step of the backward pass in the RNN.\n        :param d_h_T: Gradient on hidden state at time t\n        :param x_t: input at time step t\n        :param h_t: hidden state at time step t\n        :param h_t_minus_1: hidden state from the previous time step\n        :return: gradients for W_xh, W_hh, b_h, and gradient of hidden state to be passed back in time\n        \"\"\"\n        dhrec = (1 - h_t ** 2) * d_h_T\n        dW_xh = dhrec @ x_t.T\n        dW_hh = dhrec @ h_t_minus_1.T\n        db_h = dhrec\n        dx_t = self.W_xh.T @ dhrec\n        d_h_t_minus_1 = self.W_hh.T @ dhrec\n        return (dW_xh, dW_hh, db_h, dx_t, d_h_t_minus_1)\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        :param input_sequence: A list of input vectors.\n        :param expected_output: Expected output for each input in the sequence.\n        :param outputs: Outputs produced by the forward pass.\n        :param learning_rate: Learning rate for updating the weights and biases.\n        :return: Updated weights and biases\n        \"\"\"\n        T = len(input_sequence)\n        h_t_minus_1 = np.zeros_like(self.b_h)\n        W_xh_grad = np.zeros_like(self.W_xh)\n        W_hh_grad = np.zeros_like(self.W_hh)\n        W_hy_grad = np.zeros_like(self.W_hy)\n        b_h_grad = np.zeros_like(self.b_h)\n        b_y_grad = np.zeros_like(self.b_y)\n        for t in reversed(range(T)):\n            x_t = input_sequence[t].reshape(-1, 1)\n            y_t = outputs[t]\n            d_y = y_t - expected_output[t].reshape(-1, 1)\n            d_h = self.W_hy.T @ d_y\n            W_hy_grad += d_y @ y_t.T\n            b_y_grad += d_y\n            for bptt_step in reversed(range(t + 1)):\n                x_t_bptt = input_sequence[bptt_step].reshape(-1, 1)\n                h_t_bptt = outputs[bptt_step]\n                if bptt_step == t:\n                    d_h_t = d_h\n                else:\n                    (h_t1, _) = self._forward_step(x_t_bptt, h_t_bptt)\n                    d_h_t = self.W_hh.T @ d_h_t1 + np.tanh(x_t_bptt @ self.W_xh + h_t1 @ self.W_hh + self.b_h)\n                (dW_xh, dW_hh, db_h, _, d_h_t1) = self._backward_step(d_h_t, x_t_bptt, h_t_bptt, h_t_minus_1)\n                W_xh_grad += dW_xh\n                W_hh_grad += dW_hh\n                b_h_grad += db_h\n            h_t_minus_1 = np.zeros_like(h_t_bptt)\n        self.W_xh -= learning_rate * W_xh_grad\n        self.W_hh -= learning_rate * W_hh_grad\n        self.W_hy -= learning_rate * W_hy_grad\n        self.b_h -= learning_rate * b_h_grad\n        self.b_y -= learning_rate * b_y_grad\n        return (self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y)"}
{"task_id": 62, "completion_id": 9, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        outputs = np.zeros((T, self.hidden_size))\n        last_inputs = {}\n        last_hiddens = {}\n        hidden = np.zeros((self.hidden_size, 1))\n        for t in range(T):\n            x = np.expand_dims(input_sequence[t], axis=-1)\n            hidden = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, hidden) + self.b_h)\n            output = np.dot(self.W_hy, hidden) + self.b_y\n            outputs[t, :] = output.T\n            last_inputs[t] = x\n            last_hiddens[t] = hidden\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        delta_h = np.zeros_like(last_hiddens[0])\n        loss = 0.0\n        for t in reversed(range(T)):\n            delta_y = outputs[t].T - expected_output[t]\n            loss += np.sum(np.square(delta_y) * 0.5)\n            db_y += delta_y\n            dW_hy += np.dot(delta_y, last_hiddens[t].T)\n            delta_h += np.dot(self.W_hy.T, delta_y)\n            delta_h_tanh = delta_h * (1 - last_hiddens[t] ** 2)\n            db_h += delta_h_tanh\n            dW_hh += np.dot(delta_h_tanh, last_hiddens[t - 1].T if t > 0 else np.zeros_like(last_hiddens[0]).T)\n            dW_xh += np.dot(delta_h_tanh, last_inputs[t].T)\n            delta_h = np.dot(self.W_hh.T, delta_h_tanh)\n        m = len(input_sequence)\n        self.W_xh -= learning_rate * dW_xh / m\n        self.W_hh -= learning_rate * dW_hh / m\n        self.W_hy -= learning_rate * dW_hy / m\n        self.b_h -= learning_rate * db_h / m\n        self.b_y -= learning_rate * db_y / m\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r.T, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p.T, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r.T, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x.tolist(), 8)\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\nx = conjugate_gradient(A, b, n)"}
{"task_id": 63, "completion_id": 1, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = b - np.dot(A, x0)\n    p = r\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        if np.linalg.norm(r_new, 2) < tol:\n            break\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        p = r_new + beta * p\n        r = r_new\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 2, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    x = np.zeros_like(b) if x0 is None else x0.copy()\n    r = b - np.dot(A, x)\n    p = r\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x += alpha * p\n        r_new = r - alpha * Ap\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        if np.linalg.norm(r_new) < tol:\n            break\n        p = r_new + beta * p\n        r = r_new\n    return np.round(x, decimals=8).tolist()"}
{"task_id": 63, "completion_id": 3, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r += alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100"}
{"task_id": 63, "completion_id": 4, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b, dtype=np.double)\n    r = np.dot(A, x0) - b\n    p = -r\n    rk_norm2 = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rk_norm2 / np.dot(p, Ap)\n        x0 += alpha * p\n        r += alpha * Ap\n        rk_new_norm2 = np.dot(r, r)\n        beta = rk_new_norm2 / rk_norm2\n        p = beta * p - r\n        rk_norm2 = rk_new_norm2\n        if np.sqrt(rk_norm2) < tol:\n            break\n    return np.round(x0.tolist(), 8)"}
{"task_id": 63, "completion_id": 5, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0.copy()\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r += alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 63, "completion_id": 6, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r.T, r)\n    for iteration in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p.T, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r.T, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x.tolist(), 8)\nA = np.array([[4.0, 1.0], [1.0, 3.0]])\nb = np.array([1.0, 2.0])\nn = 1000\nx0 = np.array([0.0, 0.0])"}
{"task_id": 63, "completion_id": 7, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return [round(val, 8) for val in x.tolist()]\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])"}
{"task_id": 63, "completion_id": 8, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    r = np.dot(A, x0) - b\n    p = -r\n    rsold = np.dot(r, r)\n    x = x0.copy()\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r += alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, decimals=8).tolist()"}
{"task_id": 63, "completion_id": 9, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    class_counts = np.unique(y, return_counts=True)[1]\n    probabilities = class_counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return np.round(gini, 3)"}
{"task_id": 64, "completion_id": 1, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    n_samples = len(y)\n    (unique_classes, class_counts) = np.unique(y, return_counts=True)\n    gini = 1.0 - sum(((count / n_samples) ** 2 for count in class_counts))\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 2, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / np.sum(counts)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 3, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    labels = np.array(y)\n    (unique_labels, counts) = np.unique(labels, return_counts=True)\n    probabilities = counts / len(labels)\n    impurity = 1 - np.sum(probabilities ** 2)\n    return round(impurity, 3)"}
{"task_id": 64, "completion_id": 4, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 5, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    counts = np.bincount(y)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return np.round(gini, 3)"}
{"task_id": 64, "completion_id": 6, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n    \n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    y = np.array(y)\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / float(len(y))\n    gini = 1.0 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 7, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (classes, counts) = np.unique(y, return_counts=True)\n    gini = 1.0\n    for count in counts:\n        probability = count / len(y)\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 8, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (unique_y, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 9, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n    \n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = np.sum(probabilities * (1 - probabilities))\n    return round(gini, 3)\ny = [1, 2, 1, 0, 2, 2, 1, 0, 0, 0]"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers[:-1])"}
{"task_id": 65, "completion_id": 1, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_elements = [(col, value) for (col, value) in enumerate(row) if value != 0]\n        for (col, value) in non_zero_elements:\n            values.append(value)\n            column_indices.append(col)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)\ndense_matrix = [[1, 0, 2, 0], [0, 3, 0, 4], [5, 0, 0, 0], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 2, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        non_zero_elements = [(col, val) for (col, val) in enumerate(row) if val != 0]\n        for (col, val) in non_zero_elements:\n            values.append(val)\n            column_indices.append(col)\n        row_pointer.append(row_pointer[-1] + len(non_zero_elements))\n    return (values, column_indices, row_pointer)"}
{"task_id": 65, "completion_id": 3, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    import numpy as np\n    if not dense_matrix or not dense_matrix[0]:\n        raise ValueError('Matrix cannot be empty')\n    row_pointers = [0]\n    column_indices = []\n    values = []\n    for row in dense_matrix:\n        non_zero_elements = [(value, idx) for (idx, value) in enumerate(row) if value != 0]\n        for (value, idx) in non_zero_elements:\n            values.append(value)\n            column_indices.append(idx)\n        row_pointers.append(len(values))\n    for i in range(1, len(row_pointers)):\n        row_pointers[i] += row_pointers[i - 1]\n    return (np.array(values), np.array(column_indices), np.array(row_pointers))"}
{"task_id": 65, "completion_id": 4, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    from collections import defaultdict\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    num_rows = len(dense_matrix)\n    for (row_idx, row) in enumerate(dense_matrix):\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n        row_pointer.append(row_pointer[-1] + len([e for e in row if e != 0]))\n    return (values, column_indices, row_pointer[:-1])\ndense_matrix = [[1, 0, 2, 0], [0, 3, 0, 0], [4, 0, 5, 6], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 5, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for (i, row) in enumerate(dense_matrix):\n        for (j, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(j)\n        row_pointers.append(len(values))\n    if len(row_pointers) <= len(dense_matrix):\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers[:-1])\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0], [0, 0, 0, 0]]"}
{"task_id": 65, "completion_id": 6, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n    \n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_elements = [(column, value) for (column, value) in enumerate(row) if value != 0]\n        for (column, value) in non_zero_elements:\n            values.append(value)\n            column_indices.append(column)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 7, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n        row_pointer.append(len(values))\n    return (values, col_indices, row_pointer[:-1])"}
{"task_id": 65, "completion_id": 8, "solution": "import numpy as np\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    np_matrix = np.array(dense_matrix)\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in np_matrix:\n        nz_mask = row != 0\n        values += row[nz_mask].tolist()\n        col_indices += np.where(nz_mask)[0].tolist()\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)"}
{"task_id": 65, "completion_id": 9, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        return ([], [], [])\n    num_cols = len(dense_matrix[0])\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_idx, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, column_indices, row_pointers)\ndense_matrix = [[1, 0, 0, 2], [0, 3, 0, 0], [4, 0, 5, 0], [0, 0, 0, 6]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    norm_L = np.dot(L, L)\n    projection = dot_product / norm_L * L\n    return [round(x, 3) for x in projection]"}
{"task_id": 66, "completion_id": 1, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v_np = np.array(v)\n    L_np = np.array(L)\n    scalar_projection = np.dot(v_np, L_np) / np.linalg.norm(L_np)\n    proj_vector = (scalar_projection * L_np / np.linalg.norm(L_np)).tolist()\n    return [round(x, 3) for x in proj_vector]"}
{"task_id": 66, "completion_id": 2, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v_array = np.array(v)\n    L_array = np.array(L)\n    dot_v_L = np.dot(v_array, L_array)\n    dot_L_L = np.dot(L_array, L_array)\n    if dot_L_L == 0:\n        raise ValueError('The vector L cannot be a zero vector.')\n    scale_factor = dot_v_L / dot_L_L\n    projection_array = scale_factor * L_array\n    projection_list = [round(x, 3) for x in projection_array.tolist()]\n    return projection_list"}
{"task_id": 66, "completion_id": 3, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    proj_v = (np.dot(v, L) / np.dot(L, L) * L).round(3)\n    return proj_v.tolist()"}
{"task_id": 66, "completion_id": 4, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    squared_norm_L = np.dot(L, L)\n    scalar_projection = dot_product / squared_norm_L\n    vector_projection = scalar_projection * L\n    return [round(x, 3) for x in vector_projection]"}
{"task_id": 66, "completion_id": 5, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    proj = np.dot(v, L) / np.linalg.norm(L) ** 2 * L\n    return [float(f'{p:.3f}') for p in proj]"}
{"task_id": 66, "completion_id": 6, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    vector_projection = scalar_projection * L\n    projection_list = vector_projection.tolist()\n    rounded_projection = [round(num, 3) for num in projection_list]\n    return rounded_projection\nv = [3, 4, 5]\nL = [1, 0, 0]"}
{"task_id": 66, "completion_id": 7, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v_arr = np.array(v)\n    L_arr = np.array(L)\n    proj = np.dot(v_arr, L_arr) / np.dot(L_arr, L_arr) * L_arr\n    proj_rounded = [round(x, 3) for x in proj]\n    return proj_rounded\nv = [1, 2, 3]\nL = [2, -1, 1]"}
{"task_id": 66, "completion_id": 8, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product = np.dot(v, L)\n    projection_length = dot_product / np.dot(L, L)\n    projection = projection_length * L\n    projection_rounded = [round(x, 3) for x in projection]\n    return projection_rounded\nv = [3, 4, 0]\nL = [1, 1, 0]"}
{"task_id": 66, "completion_id": 9, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v, dtype=float)\n    L = np.array(L, dtype=float)\n    projection = (np.dot(v, L) / np.dot(L, L) * L).round(3)\n    return projection.tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    col_ptr = [0]\n    row_indices = []\n    values = []\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 67, "completion_id": 1, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[1, 0, 0], [0, 5, 0], [2, 0, 3]]"}
{"task_id": 67, "completion_id": 2, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    import numpy as np\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if dense_matrix else 0\n    for c in range(num_cols):\n        for r in range(num_rows):\n            val = dense_matrix[r][c]\n            if val != 0:\n                values.append(val)\n                row_indices.append(r)\n        col_ptr.append(len(values))\n    return (np.array(values), np.array(row_indices), np.array(col_ptr))\ndense_matrix = [[0, 3, 0], [4, 0, 0], [0, 0, 5], [0, 2, 6]]"}
{"task_id": 67, "completion_id": 3, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[1, 0, 0], [0, 2, 0], [0, 0, 3], [4, 0, 0]]"}
{"task_id": 67, "completion_id": 4, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\ndense_matrix = [[1, 0, 0, 2], [0, 3, 0, 0], [4, 0, 0, 5], [0, 6, 0, 0]]"}
{"task_id": 67, "completion_id": 5, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        raise ValueError('Dense matrix cannot be empty')\n    num_cols = len(dense_matrix[0])\n    col_ptr = [0] * (num_cols + 1)\n    values_temp = []\n    row_indices_temp = []\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values_temp.append(dense_matrix[row][col])\n                row_indices_temp.append(row)\n        col_ptr[col + 1] = col_ptr[col] + len(values_temp)\n    if col_ptr[-1] != len(values_temp):\n        col_ptr[-1] = len(values_temp)\n    return (values_temp, row_indices_temp, col_ptr)\ndense_matrix = [[0, 2, 0], [3, 0, 0], [4, 5, 6], [0, 0, 7]]"}
{"task_id": 67, "completion_id": 6, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n    \n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        raise ValueError('Matrix cannot be empty.')\n    num_cols = len(dense_matrix[0])\n    if num_cols == 0:\n        raise ValueError('Matrix cannot have empty rows.')\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 4], [0, 5, 0, 0]]"}
{"task_id": 67, "completion_id": 7, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    col_ptr = [0]\n    values = []\n    row_indices = []\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[0, 1, 0], [2, 0, 0], [0, 3, 0], [0, 0, 4]]"}
{"task_id": 67, "completion_id": 8, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        return ([], [], [0])\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for c in range(num_cols):\n        for r in range(num_rows):\n            val = dense_matrix[r][c]\n            if val != 0:\n                values.append(val)\n                row_indices.append(r)\n        col_ptr.append(len(row_indices))\n    return (values, row_indices, col_ptr)"}
{"task_id": 67, "completion_id": 9, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    col_pointer = [0]\n    values = []\n    row_indices = []\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_pointer.append(len(values))\n    assert col_pointer[-1] == len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 1, 0, 7], [0, 0, 0, 0, 0], [0, 8, 0, 9, 0], [0, 0, 0, 0, 3], [2, 0, 0, 0, 0], [0, 0, 5, 0, 4]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    This function calculates the column space (image) of matrix A.\n    It returns the basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix A and correspond to the independent columns.\n    \n    :param A: Input matrix as a numpy array\n    :return: Basis vectors of the column space of A as a list.\n    \"\"\"\n    (A_echelon_form, pivots) = row_echelon_form(A)\n    pivot_columns = []\n    for (i, col) in enumerate(A_echelon_form.T):\n        if any((col[j] != 0 for j in range(len(col)))):\n            pivot_columns.append(i)\n    basis = A[:, pivot_columns]\n    return np.round(basis, 8).tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    This function transforms the input matrix into its Row Echelon Form and returns the form and pivot indices.\n    \n    :param A: Input matrix as a numpy array\n    :return: Row echelon form and pivot indices of the matrix A\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    (i, j) = (0, 0)\n    pivots = []\n    while i < m and j < n:\n        max_row = max(range(i, m), key=lambda x: abs(A[x, j]))\n        if A[max_row, j] == 0:\n            j += 1\n        else:\n            A[[i, max_row]] = A[[max_row, i]]\n            pivots.append(j)\n            for k in range(i + 1, m):\n                c = A[k, j] / A[i, j]\n                A[k, j:] -= c * A[i, j:]\n            i += 1\n            j += 1\n    return (A, pivots)\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)"}
{"task_id": 68, "completion_id": 1, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A using row echelon form.\n    Returns a list of basis vectors (to list) representing the column space of A, with values rounded to 8 decimals.\n    \"\"\"\n    A = np.array(A)\n    row_echelon = np.linalg.qr(A.T)[1].T\n    pivot_indices = []\n    for i in range(min(row_echelon.shape)):\n        if not np.allclose(row_echelon[i, i:], 0):\n            pivot_indices.append(i)\n    basis_vectors = A[:, pivot_indices]\n    return np.round(basis_vectors, 8).tolist()\nA = [[2, 4, 1], [3, 6, 2], [1, 2, 0]]"}
{"task_id": 68, "completion_id": 2, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of a given matrix A.\n    \n    Parameters:\n    A (list of list of float): The input matrix.\n    \n    Returns:\n    list of list of float: The basis vectors that span the column space of A.\n    \"\"\"\n    A = np.array(A)\n    (A_row_echelon, pivot_cols) = row_echelon_form(A)\n    basis = A[:, pivot_cols]\n    basis = np.round(basis, 8)\n    return basis.tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    Transforms the matrix A to row echelon form and returns pivot columns.\n    \n    Parameters:\n    A (np.ndarray): The input matrix.\n    \n    Returns:\n    np.ndarray: The row echelon form of the matrix.\n    np.ndarray: The indices of the pivot columns.\n    \"\"\"\n    A = A.astype(float)\n    (m, n) = A.shape\n    pivot_index = 0\n    for r in range(m):\n        if pivot_index >= n:\n            break\n        i = r\n        while A[i, pivot_index] == 0:\n            i += 1\n            if i == m:\n                i = r\n                pivot_index += 1\n                if pivot_index == n:\n                    return (A, np.array(range(pivot_index)))\n        A[[i, r]] = A[[r, i]]\n        pivot_value = A[r, pivot_index]\n        A[r] /= pivot_value\n        for i in range(r + 1, m):\n            A[i] -= A[r] * A[i, pivot_index]\n        pivot_index += 1\n    return (A, np.arange(pivot_index))\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 68, "completion_id": 3, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Find the image of matrix A using its row echelon form.\n    Returns a list of basis vectors that span the column space of A.\n    \"\"\"\n\n    def row_echelon_form(A):\n        A = np.copy(A)\n        (rows, cols) = A.shape\n        i = 0\n        for j in range(cols):\n            max_row = np.argmax(abs(A[i:rows, j])) + i\n            A[[i, max_row]] = A[[max_row, i]]\n            pivot = A[i, j]\n            if pivot != 0:\n                A[i] = A[i] / pivot\n                A[i + 1:rows] = A[i + 1:rows] - A[i + 1:rows, j:j + 1] * A[i]\n            i += 1\n            if i == rows:\n                break\n        return A\n    ref = row_echelon_form(A)\n    (num_rows, num_cols) = ref.shape\n    pivots = []\n    for col in range(num_cols):\n        if np.argmax(ref[:, col]) == len(pivots):\n            pivots.append(col)\n    basis = A[:, pivots]\n    basis_rounded = np.round(basis, 8)\n    return basis_rounded.tolist()\nA = np.array([[1, 2, 3, 1], [4, 5, 6, 4], [7, 8, 9, 7], [10, 11, 12, 10]])"}
{"task_id": 68, "completion_id": 4, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of matrix A by finding independent columns.\n    \n    Parameters:\n    A (list of list of float): The input matrix\n    \n    Returns:\n    list of list of float: Basis vectors that span the column space of A\n    \"\"\"\n    A = np.array(A)\n    A_ref = row_echelon_form(A)\n    pivot_columns = set()\n    for i in range(A_ref.shape[0]):\n        for j in range(A_ref.shape[1]):\n            if A_ref[i, j] != 0:\n                pivot_columns.add(j)\n                break\n    basis_vectors = A[:, list(pivot_columns)]\n    basis_vectors = np.round(basis_vectors, 8)\n    return basis_vectors.tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    Convert matrix A to row echelon form.\n    \n    Parameters:\n    A (numpy array): The input matrix\n    \n    Returns:\n    numpy array: Row echelon form of A\n    \"\"\"\n    A = A.copy()\n    (rows, cols) = A.shape\n    r = 0\n    for c in range(cols):\n        pivot_row = np.argmax(np.abs(A[r:rows, c])) + r\n        if A[pivot_row, c] == 0:\n            continue\n        A[[r, pivot_row]] = A[[pivot_row, r]]\n        for below_row in range(r + 1, rows):\n            factor = A[below_row, c] / A[r, c]\n            A[below_row] -= factor * A[r]\n        r += 1\n        if r == rows:\n            break\n    return A\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 68, "completion_id": 5, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Finds the basis vectors that span the column space of matrix A.\n    \n    Parameters:\n    A (np.array): A matrix given as a numpy array.\n    \n    Returns:\n    list: The basis vectors (columns) of the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A)\n    (row_echelon_form, pivots) = _row_echelon_form(A)\n    independent_columns = [np.argmax(abs(row_echelon_form[:, i:]), axis=0) + i for i in pivots]\n    basis_vectors = A[:, independent_columns]\n    return np.round(basis_vectors, 8).tolist()\ndef _row_echelon_form(A):\n    \"\"\"\n    Helper function to calculate the row echelon form of matrix A.\n    Returns the row echelon form and the indices of pivot columns.\n    \"\"\"\n    A = A.copy()\n    (m, n) = A.shape\n    pivots = []\n    for col in range(n):\n        for row in range(col, m):\n            if A[row, col] != 0:\n                A[[row, col]] = A[[col, row]]\n                pivots.append(col)\n                break\n        else:\n            continue\n        for row in range(col + 1, m):\n            factor = A[row, col] / A[col, col]\n            A[row, col:] -= factor * A[col, col:]\n    return (A, pivots)\nA = [[1, 2, 3], [2, 3, 4], [3, 4, 5]]"}
{"task_id": 68, "completion_id": 6, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Finds the column space (image) of matrix A by identifying independent columns.\n    Returns the basis vectors that span the column space of A.\n    \n    :param A: A numpy matrix.\n    :return: Basis vectors of the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    A_echelon_form = row_echelon_form(A)\n    pivot_indices = []\n    for i in range(A_echelon_form.shape[0]):\n        for j in range(A_echelon_form.shape[1]):\n            if A_echelon_form[i, j] != 0:\n                pivot_indices.append(j)\n                break\n    basis_vectors = A[:, pivot_indices]\n    return np.round(basis_vectors, decimals=8).tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    Converts a matrix to its row echelon form.\n    \n    :param A: A numpy matrix.\n    :return: The row echelon form of the matrix A.\n    \"\"\"\n    A = np.array(A)\n    (row, col) = A.shape\n    lead = 0\n    for r in range(row):\n        if col <= lead:\n            return A\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if row == i:\n                i = r\n                lead += 1\n                if col == lead:\n                    return A\n        A[[i, r]] = A[[r, i]]\n        A[r] = A[r] / A[r, lead]\n        for i in range(row):\n            if i != r:\n                A[i] = A[i] - A[i, lead] * A[r]\n        lead += 1\n    return A\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 68, "completion_id": 7, "solution": ""}
{"task_id": 68, "completion_id": 8, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of a given matrix A.\n    The function returns a list of basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix and correspond to the independent columns.\n    \n    Parameters:\n    A (np.ndarray): A numpy array representing the input matrix.\n    \n    Returns:\n    list: A list of lists representing the basis vectors spanning the column space, rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A)\n    A_row_echelon = np.array(rref(A)[0])\n    (pivots, _) = np.where(A_row_echelon == 1)\n    basis_vectors = A[:, pivots]\n    basis_vectors = np.round(basis_vectors, 8)\n    return basis_vectors.tolist()\ndef rref(A):\n    \"\"\"\n    Performs row operations to convert matrix A to Reduced Row Echelon Form (RREF).\n    \"\"\"\n    A = np.array(A)\n    (numRows, numCols) = A.shape\n    (i, j) = (0, 0)\n    while i < numRows and j < numCols:\n        maxIdx = np.argmax(A[i:, j]) + i\n        A[[i, maxIdx]] = A[[maxIdx, i]]\n        pivot = A[i, j]\n        if pivot != 0:\n            A[i] /= pivot\n        for ii in range(0, numRows):\n            if ii != i:\n                A[ii] -= A[i] * A[ii, j]\n        i += 1\n        j += 1\n    return A\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 68, "completion_id": 9, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a matrix A by identifying\n    the independent columns that span the matrix's image.\n    \n    Parameters:\n    A (List[List[float]]): A 2D list representing the input matrix.\n    \n    Returns:\n    List[List[float]]: A 2D list that represents the basis vectors\n                       spanning the column space of A.\n    \"\"\"\n    matrix = np.array(A)\n    (row_echelon_form, pivots) = get_row_echelon_form(matrix)\n    independent_columns = matrix[:, pivots]\n    independent_columns_rounded = np.round(independent_columns, 8)\n    return independent_columns_rounded.tolist()\ndef get_row_echelon_form(matrix):\n    \"\"\"\n    Computes the row echelon form of a matrix and returns the pivot columns.\n    \n    Parameters:\n    matrix (np.ndarray): A NumPy array representing the input matrix.\n    \n    Returns:\n    np.ndarray, np.ndarray: The row echelon form of the matrix and the indices\n                            of the pivot columns.\n    \"\"\"\n    (rows, cols) = matrix.shape\n    row_echelon = matrix.copy()\n    pivot_col = 0\n    for row in range(rows):\n        if row_echelon[row, pivot_col] == 0:\n            for below_row in range(row + 1, rows):\n                if row_echelon[below_row, pivot_col] != 0:\n                    row_echelon[[row, below_row]] = row_echelon[[below_row, row]]\n                    break\n        if row_echelon[row, pivot_col] == 0:\n            continue\n        for below_row in range(row + 1, rows):\n            multiplier = row_echelon[below_row, pivot_col] / row_echelon[row, pivot_col]\n            row_echelon[below_row, :] -= multiplier * row_echelon[row, :]\n        pivot_col += 1\n        if pivot_col == cols:\n            break\n    pivots = []\n    for r in range(rows):\n        for c in range(cols):\n            if row_echelon[r, c] != 0 and all(row_echelon[r + 1:, c] == 0):\n                pivots.append(c)\n                break\n    return (row_echelon, np.array(pivots))\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Computes the R-squared value (coefficient of determination) given true and predicted y values.\n    \n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ssr = np.sum((y_true - y_pred) ** 2)\n    sst = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - ssr / sst\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 1, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for given true and predicted values.\n    \n    Parameters:\n    y_true (np.array): The true values of the target variable.\n    y_pred (np.array): The predicted values of the target variable.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_mean = np.mean(y_true)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_mean) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 2, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) between true and predicted values.\n    \n    Args:\n    y_true: Array of true values.\n    y_pred: Array of predicted values.\n    \n    Returns:\n    R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 69, "completion_id": 3, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) value given arrays of true and predicted values.\n    \n    Parameters:\n    y_true (np.array): Array of true values.\n    y_pred (np.array): Array of predicted values.\n    \n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    mean_y_true = np.mean(y_true)\n    tss = np.sum((y_true - mean_y_true) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    rsquared = 1 - rss / tss\n    return round(rsquared, 3)"}
{"task_id": 69, "completion_id": 4, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) for the regression model.\n    \n    Parameters:\n    y_true : np.ndarray\n        An array of the true target values.\n    y_pred : np.ndarray\n        An array of the predicted target values.\n        \n    Returns:\n    float\n        The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return np.round(r2, 3)\ny_true = [1, 2, 3, 4, 5]\ny_pred = [1.1, 1.9, 3.1, 3.9, 4.8]"}
{"task_id": 69, "completion_id": 5, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) value given true and predicted values.\n    \n    Parameters:\n    y_true (numpy array): Array of true/regressed values.\n    y_pred (numpy array): Array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 69, "completion_id": 6, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression model.\n    \n    Parameters:\n    - y_true: numpy array of true values\n    - y_pred: numpy array of predicted values\n    \n    Returns:\n    - R^2 (R-squared) value, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TSS = np.sum((y_true - np.mean(y_true)) ** 2)\n    RSS = np.sum((y_true - y_pred) ** 2)\n    R_squared = 1 - RSS / TSS\n    return round(R_squared, 3)\ny_true = [1, 2, 3, 4, 5]\ny_pred = [2, 3, 4, 5, 6]"}
{"task_id": 69, "completion_id": 7, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value from the given true and predicted values.\n\n    :param y_true: An array of the true values.\n    :param y_pred: An array of the predicted values.\n    :return: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 8, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculates the R-squared value given the true and predicted values.\n\n    Parameters:\n    y_true (numpy array): The true values.\n    y_pred (numpy array): The predicted values.\n\n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 9, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value given the true and predicted values.\n    \n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ssres = np.sum((y_true - y_pred) ** 2)\n    sstot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r2 = 1 - ssres / sstot\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    ref_row_len = len(img[0])\n    for row in img:\n        if len(row) != ref_row_len:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    avg_brightness = round(total_brightness / total_pixels, 2)\n    return avg_brightness"}
{"task_id": 70, "completion_id": 1, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented as a 2D list.\n    \n    :param img: A 2D list where each element is a pixel value [0, 255].\n    :return: Average brightness as a float rounded to 2 decimal places, or -1 for invalid input.\n    \"\"\"\n    if not img or not all((len(row) == len(img[0]) for row in img)):\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 2, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n    \n    :param img: A 2D list representing the image where each element is a pixel value from 0 to 255.\n    :return: The average brightness of the image rounded to 2 decimal places, or -1 if any edge case occurs.\n    \"\"\"\n    if not img or not all((row for row in img)):\n        return -1\n    total_pixels = 0\n    sum_brightness = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            sum_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = sum_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 3, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image.\n    \n    :param img: A 2D list (matrix) representing the grayscale image.\n    :return: The average brightness of the image rounded to two decimal places as a float,\n             -1 for invalid images.\n    \"\"\"\n    if not img or not all((isinstance(row, list) for row in img)):\n        return -1\n    total_brightness = 0\n    pixels = 0\n    first_row_length = len(img[0])\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixels += 1\n    if pixels == 0:\n        return -1\n    return round(total_brightness / pixels, 2)"}
{"task_id": 70, "completion_id": 4, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n    \n    :param img: A 2D list representing the image, where each pixel's value ranges from 0 (black) to 255 (white).\n    :return: The average brightness of the image rounded to two decimal places, or -1 if the image is invalid.\n    \"\"\"\n    if not img or not all(img[0]):\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    first_row_length = len(img[0])\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 5, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented as a 2D list.\n    \n    Parameters:\n    img (list of list of int): A 2D list where each element represents a pixel value (0-255).\n    \n    Returns:\n    float: Average brightness of the image rounded to two decimal places or -1 if invalid.\n    \"\"\"\n    if not img or not img[0]:\n        return -1\n    total_pixels = 0\n    total_brightness = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    avg_brightness = total_brightness / total_pixels\n    return round(avg_brightness, 2)"}
{"task_id": 70, "completion_id": 6, "solution": "def calculate_brightness(img):\n    \"\"\"\n    This function takes a 2D matrix representing a grayscale image and calculates\n    the average brightness rounded to two decimal places. It returns -1 for invalid\n    input like non-square matrices, empty images, and out-of-range pixel values.\n    \n    :param img: A 2D matrix representing the image.\n    :return: Average brightness of the image or -1 for invalid cases.\n    \"\"\"\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    return round(total_brightness / pixel_count, 2)\nimg = [[128, 128, 128], [128, 128, 128], [128, 128, 128]]"}
{"task_id": 70, "completion_id": 7, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D list.\n    The function returns the average brightness rounded to two decimal places.\n    For invalid images, it returns -1.\n    \"\"\"\n    if not img or not all((isinstance(row, list) and row for row in img)):\n        return -1\n    total_sum = 0\n    pixel_count = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_sum += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    average_brightness = round(total_sum / pixel_count, 2)\n    return average_brightness"}
{"task_id": 70, "completion_id": 8, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    reference_row_length = len(img[0])\n    for row in img:\n        if len(row) != reference_row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 9, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): The image matrix, where each element is a pixel value [0, 255].\n    \n    Returns:\n    float: Average brightness rounded to two decimal places, or -1 if the image is invalid.\n    \"\"\"\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    return round(total_brightness / num_pixels, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n    \n    Parameters:\n    y_true (np.array): Actual values array.\n    y_pred (np.array): Predicted values array.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have mismatched shapes or are empty.\n    TypeError: If inputs are not numpy arrays.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])"}
{"task_id": 71, "completion_id": 1, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true and predicted values.\n    \n    Parameters:\n    y_true (list or np.ndarray): The true values.\n    y_pred (list or np.ndarray): The predicted values.\n\n    Returns:\n    float: The calculated RMSE rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the shapes of y_true and y_pred do not match or if the arrays are empty.\n    TypeError: If input types are not arrays or lists.\n    \"\"\"\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise TypeError('Input types must be either lists or np.ndarrays.')\n    (y_true, y_pred) = (np.array(y_true), np.array(y_pred))\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Shape mismatch. y_true and y_pred must have the same shape.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse_value = np.sqrt(mse)\n    return np.round(rmse_value, decimals=3)"}
{"task_id": 71, "completion_id": 2, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true and predicted values.\n    \n    :param y_true: numpy array of true target values.\n    :param y_pred: numpy array of predicted target values.\n    :return: RMSE value rounded to three decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Dimensions of actual and predicted arrays do not match.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays are empty.')\n    current_rmse = np.sqrt(np.mean(np.square(y_true - y_pred)))\n    return round(current_rmse, 3)"}
{"task_id": 71, "completion_id": 3, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) for the input arrays.\n    \n    :param y_true: np.array, the true values.\n    :param y_pred: np.array, the predicted values.\n    :return: float, the RMSE rounded to three decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Both y_true and y_pred must be of type numpy.ndarray')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Shapes of y_true and y_pred do not match')\n    n = len(y_true)\n    squared_errors = (y_true - y_pred) ** 2\n    mse = np.sum(squared_errors) / n\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 4, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true and predicted values.\n    \n    Parameters:\n    - y_true (np.ndarray): The true values.\n    - y_pred (np.ndarray): The predicted values.\n    \n    Returns:\n    - float: The RMSE value rounded to three decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Input types must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of the true and predicted arrays must match.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 5, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the arrays of actual values and predicted values.\n    \n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n    \n    Returns:\n    float: RMSE rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have mismatched shapes or are empty.\n    TypeError: If input is not array-like.\n    \"\"\"\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise TypeError('Input values must be array-like (list or numpy array).')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape == (0,) or y_pred.shape == (0,):\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same length.')\n    error = y_true - y_pred\n    rmse_value = np.sqrt(np.mean(error ** 2))\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 6, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Input should be of valid numpy array type!')\n    if y_true.shape[0] == 0 or y_pred.shape[0] == 0:\n        raise ValueError('Input arrays cannot be empty!')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Mismatched shape of actual and predicted values.')\n    error = y_true - y_pred\n    rmse_value = np.sqrt(np.mean(np.square(error)))\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 7, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): The actual values.\n    y_pred (np.ndarray): The predicted values.\n    \n    Returns:\n    float: The RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have different lengths, or if either of them is empty.\n    TypeError: If inputs are not numpy arrays or lists.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Both y_true and y_pred must be of type np.ndarray')\n    if y_true.shape[0] == 0 or y_pred.shape[0] == 0:\n        raise ValueError('Input arrays cannot be empty')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same length')\n    squared_diff = (y_true - y_pred) ** 2\n    mean_squared_diff = np.mean(squared_diff)\n    rmse_val = np.sqrt(mean_squared_diff)\n    return round(rmse_val, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 71, "completion_id": 8, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual and predicted values.\n    :param y_true: np.ndarray, the actual target values.\n    :param y_pred: np.ndarray, the predicted target values.\n    :return: RMSE value rounded to three decimal places as a float, or None if an error occurs.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        print('One or both inputs are not numpy arrays.')\n        return None\n    if y_true.shape != y_pred.shape:\n        print('Input arrays must have the same shape.')\n        return None\n    if len(y_true) == 0 or len(y_pred) == 0:\n        print('Input arrays cannot be empty.')\n        return None\n    try:\n        mse = np.mean(np.square(np.subtract(y_true, y_pred)))\n        rmse_value = np.sqrt(mse)\n        return round(rmse_value, 3)\n    except Exception as e:\n        print(f'An error occurred: {e}')\n        return None\ny_true = np.array([3.0, -0.5, 2.0, 7.0])\ny_pred = np.array([2.5, 0.0, 2.0, 8.0])"}
{"task_id": 71, "completion_id": 9, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays.\n    \n    Parameters:\n    - y_true: An array of true values.\n    - y_pred: An array of predicted values.\n    \n    Returns:\n    - The RMSE value as a float rounded to three decimal places.\n    \n    Raises:\n    - ValueError: If the shapes of y_true and y_pred do not match.\n    - ValueError: If either y_true or y_pred is not an array or is empty.\n    - TypeError: If the elements of y_true or y_pred are not all numeric.\n    \"\"\"\n    if not (isinstance(y_true, (list, np.ndarray)) and isinstance(y_pred, (list, np.ndarray))):\n        raise TypeError('Both y_true and y_pred should be list or numpy array.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred do not have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty or null.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])"}
{"task_id": 72, "completion_id": 1, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n\n    Returns:\n    float: The Jaccard Index, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return round(0.0, 3)\n    else:\n        return round(intersection / union, 3)"}
{"task_id": 72, "completion_id": 2, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (np.ndarray): A 1D binary array representing true labels.\n    y_pred (np.ndarray): A 1D binary array representing predicted labels.\n\n    Returns:\n    float: The Jaccard Index as a float rounded to three decimal places.\n    \"\"\"\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 3, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    - y_true: A numpy array of integers representing the true binary labels.\n    - y_pred: A numpy array of integers representing the predicted binary labels.\n    \n    Returns:\n    - A float representing the Jaccard Index, rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 4, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary sets.\n    \n    Parameters:\n    - y_true: np.array of true binary labels.\n    - y_pred: np.array of predicted binary labels.\n    \n    Returns:\n    - Jaccard Index as a float value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.bitwise_and(y_true, y_pred))\n    union = np.sum(np.bitwise_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])"}
{"task_id": 72, "completion_id": 5, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for two binary sets y_true and y_pred.\n    \n    Parameters:\n    - y_true: list of actual binary labels\n    - y_pred: list of predicted binary labels\n    \n    Returns:\n    - float: The Jaccard Index rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 6, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Compute the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    - y_true: A numpy array of true binary labels.\n    - y_pred: A numpy array of predicted binary labels.\n    \n    Returns:\n    The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        if np.sum(y_true) == 0 and np.sum(y_pred) == 0:\n            return 1.0\n        else:\n            return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])"}
{"task_id": 72, "completion_id": 7, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index, a measure of similarity between the true binary\n    labels (y_true) and predicted binary labels (y_pred).\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 8, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays y_true and y_pred.\n    \n    Parameters:\n    - y_true (list or np.ndarray): True binary labels.\n    - y_pred (list or np.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: The Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    else:\n        jaccard = intersection / union\n        return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 9, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    y_true (list or np.array): True binary labels.\n    y_pred (list or np.array): Predicted binary labels.\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A binary array representing the true labels.\n    - y_pred: A binary array representing the predicted labels.\n    \n    Returns:\n    - The Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        score = 1.0\n    else:\n        score = 2 * true_positives / (sum_true + sum_pred)\n    return round(score, 3)"}
{"task_id": 73, "completion_id": 1, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    size_y_true = np.sum(y_true)\n    size_y_pred = np.sum(y_pred)\n    if size_y_true + size_y_pred == 0:\n        return 0.0\n    dice_score = 2.0 * intersection / (size_y_true + size_y_pred)\n    return round(dice_score, 3)\ny_true = np.array([1, 0, 1, 1, 0])\ny_pred = np.array([1, 1, 1, 0, 0])"}
{"task_id": 73, "completion_id": 2, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A numpy array of true binary labels.\n    - y_pred: A numpy array of predicted binary labels.\n    \n    Returns:\n    - The Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    total = np.sum(y_true) + np.sum(y_pred)\n    if total == 0:\n        return 1.0\n    dice = 2.0 * intersection / total\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 3, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification given true and predicted labels.\n\n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray):\n        y_true = np.array(y_true)\n    if not isinstance(y_pred, np.ndarray):\n        y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true does not match y_pred.')\n    if y_true.size == 0 or y_pred.size == 0:\n        return 0.0\n    intersection = np.sum(y_true * y_pred)\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    if sum_true_pred == 0:\n        return 1.0\n    dice = 2 * intersection / sum_true_pred\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 4, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: np.array, the true binary labels.\n    - y_pred: np.array, the predicted binary labels.\n    \n    Returns:\n    - float: The Dice Score as a float value rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum(y_true * y_pred)\n    sum_true_positives = np.sum(y_true) + np.sum(y_pred)\n    if sum_true_positives == 0:\n        return 1.0\n    dice = 2 * true_positives / sum_true_positives\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 5, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (list or np.ndarray): True binary labels.\n    y_pred (list or np.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.sum() == 0 and y_pred.sum() == 0:\n        return float('nan')\n    intersection = np.sum(y_true * y_pred)\n    num_true = np.sum(y_true)\n    num_pred = np.sum(y_pred)\n    score = 2 * intersection / (num_true + num_pred)\n    return round(score, 3)"}
{"task_id": 73, "completion_id": 6, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels as a numpy array.\n    y_pred (np.ndarray): Predicted binary labels as a numpy array.\n\n    Returns:\n    float: The Dice Score as a float, rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=bool)\n    y_pred = np.asarray(y_pred, dtype=bool)\n    overlap = np.logical_and(y_true, y_pred)\n    true_positives = np.sum(overlap)\n    true_pos = np.sum(y_true)\n    pred_pos = np.sum(y_pred)\n    if true_pos == 0 or pred_pos == 0:\n        return round(0.0, 3)\n    score = 2 * true_positives / (true_pos + pred_pos)\n    return round(score, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 73, "completion_id": 7, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Dice Score for binary classification, which measures the\n    similarity between two sets. Handle edge cases where the sum of either\n    true or predicted positives is zero.\n    \n    Parameters:\n    y_true (np.array): Array of true binary labels.\n    y_pred (np.array): Array of predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum(y_true)\n    pred_positives = np.sum(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    if true_positives == 0 and pred_positives == 0:\n        return 1.0\n    dice = 2.0 * intersection / (true_positives + pred_positives)\n    return round(dice, 3)\ny_true = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]\ny_pred = [1, 0, 0, 0, 1, 1, 1, 1, 1, 0]"}
{"task_id": 73, "completion_id": 8, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Compute the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.array): Binary array representing the true labels.\n    y_pred (np.array): Binary array representing the predicted labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    if np.sum(y_true) == 0 and np.sum(y_pred) == 0:\n        return 1.0\n    dice = 2.0 * intersection / (np.sum(y_true) + np.sum(y_pred))\n    return round(dice, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 73, "completion_id": 9, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A binary np.ndarray representing the true labels.\n    - y_pred: A binary np.ndarray representing the predicted labels.\n    \n    Returns:\n    - A float value representing the Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum(y_true * y_pred)\n    if np.sum(y_true) == 0 and np.sum(y_pred) == 0:\n        return 1.0\n    elif true_positives == 0:\n        return 0.0\n    dice = 2 * true_positives / (np.sum(y_true) + np.sum(y_pred))\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef random_hypervector(dim, seed=None):\n    \"\"\"\n    Helper function to generate a random hypervector of a given dimensionality.\n    :param dim: The dimensionality of the hypervector.\n    :param seed: The random seed to ensure reproducibility.\n    :return: A random hypervector of dimensionality 'dim'.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using HDC.\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n    :param row: A dictionary representing a dataset row.\n    :param dim: The dimensionality of the hypervectors.\n    :param random_seeds: A dictionary to ensure reproducibility of hypervectors.\n    :return: A composite hypervector representing the entire row.\n    \"\"\"\n    hv_accumulator = np.zeros(dim)\n    for (feature_name, value) in row.items():\n        feature_hv = random_hypervector(dim, hash(feature_name))\n        value_hv = random_hypervector(dim, random_seeds[feature_name])\n        bound_hv = feature_hv * value_hv\n        hv_accumulator += bound_hv\n    return hv_accumulator.tolist()"}
{"task_id": 74, "completion_id": 1, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def hv(name, seed, dim):\n        np.random.seed(seed)\n        return np.where(np.random.rand(dim) > 0.5, 1, -1)\n\n    def bind(a, b):\n        return np.multiply(a, b) % 2 * 2 - 1\n\n    def bundle(hvs):\n        return np.sign(np.sum(hvs, axis=0))\n    feature_hvs = []\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        feature_name_hv = hv(feature, seed, dim)\n        feature_value_hv = hv(str(value), seed, dim)\n        feature_hv = bind(feature_name_hv, feature_value_hv)\n        feature_hvs.append(feature_hv)\n    composite_hv = bundle(feature_hvs)\n    return composite_hv.tolist()\ndim = 10\nrow = {'age': 25, 'gender': 'M', 'income': 50000}\nrandom_seeds = {'age': 42, 'gender': 24, 'income': 1234}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 2, "solution": "import numpy as np\ndef create_hv(seed, dim, is_random):\n    \"\"\"\n    Helper function to generate hypervector.\n    \n    :param seed: seed value for generating specific hypervector\n    :param dim: dimensionality of the hypervector\n    :param is_random: flag to determine if hypervector has random +/-1 elements\n    :return: hypervector (np.array of size dim, elements are either 1 or -1)\n    \"\"\"\n    np.random.seed(seed)\n    hv = np.random.choice([-1, 1], size=dim) if is_random else np.zeros(dim, dtype=int)\n    return hv\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate composite hypervector for a dataset row using HDC.\n    \n    :param row: A dictionary representing a dataset row\n    :param dim: Dimensionality of the hypervectors\n    :param random_seeds: A dictionary where keys are feature names and values are seeds\n    :return: A composite hypervector representing the entire row as a list\n    \"\"\"\n    composite_hv = np.zeros(dim, dtype=int)\n    for (feature, value) in row.items():\n        feature_hv = create_hv(hash(feature), dim, is_random=True)\n        value_hv = create_hv(random_seeds[feature], dim, is_random=True)\n        feature_value_hv = np.multiply(feature_hv, value_hv)\n        composite_hv = np.add(composite_hv, feature_value_hv)\n    return composite_hv.tolist()\ndim = 10"}
{"task_id": 74, "completion_id": 3, "solution": "import numpy as np\ndef create_hypervector(dim, seed=None):\n    \"\"\"Creates a random binary hypervector of a given dimensionality.\"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Performs binding between two hypervectors by element-wise multiplication and normalization.\"\"\"\n    return hv1 * hv2\ndef bundle_hypervectors(*hvs):\n    \"\"\"Bundles multiple hypervectors by summing them up and normalizing.\"\"\"\n    return np.sign(sum(hvs))\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generates the composite hypervector for a given dataset row.\"\"\"\n    hypervector_components = []\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        feature_hv = create_hypervector(dim, seed)\n        value_hv = create_hypervector(dim, seed * int(value))\n        feature_value_hv = bind_hypervectors(feature_hv, value_hv)\n        hypervector_components.append(feature_value_hv)\n    row_hv = bundle_hypervectors(*hypervector_components)\n    return row_hv.tolist()\nrandom_seeds = {'temperature': 111, 'humidity': 222, 'wind_speed': 333}\ndim = 10000"}
{"task_id": 74, "completion_id": 4, "solution": "def create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row.\n    \n    Parameters:\n    - row: A dictionary with feature names and their values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary with feature names as keys and seeds for reproducibility.\n    \n    Returns:\n    - A composite hypervector representing the entire row.\n    \"\"\"\n    row_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_hv = create_hypervector(dim, random_seeds[feature])\n        value_hv = create_hypervector(dim, random_seeds[feature])\n        feature_value_hv = create_binding(feature_hv, value_hv)\n        row_hv = np.add(row_hv, feature_value_hv) % 2\n    return row_hv.tolist()\ndim = 100\nrandom_seeds = {'feature1': 1001, 'feature2': 1002, 'feature3': 1003}"}
{"task_id": 74, "completion_id": 5, "solution": "import numpy as np\ndef binary_hypervector(dim, seed):\n    \"\"\"\n    Generate a binary hypervector of a specified dimension with a given seed for reproducibility.\n    \"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"\n    Bind two hypervectors using element-wise multiplication followed by normalization.\n    \"\"\"\n    return hv1 * hv2\ndef bundle_hypervectors(*hv_list):\n    \"\"\"\n    Bundles multiple hypervectors using element-wise summation followed by normalization.\n    \"\"\"\n    bundle = np.sum(hv_list, axis=0)\n    return bundle / np.linalg.norm(bundle)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector representing the entire row from given feature-value pairs.\n    \"\"\"\n    feature_hvs = []\n    for (feature, value) in row.items():\n        feature_hv = binary_hypervector(dim, seed=random_seeds[feature])\n        value_seed = hash((feature, value))\n        value_hv = binary_hypervector(dim, seed=value_seed)\n        bound_hv = bind_hypervectors(feature_hv, value_hv)\n        feature_hvs.append(bound_hv)\n    row_hv = bundle_hypervectors(*feature_hvs)\n    return row_hv.tolist()"}
{"task_id": 74, "completion_id": 6, "solution": "import numpy as np\ndef create_hypervector(seed, dim):\n    \"\"\"\n    Generates hypervector with given seed and dimension\n    \"\"\"\n    np.random.seed(seed)\n    hv = np.where(np.random.rand(dim) > 0.5, 1, -1)\n    return hv\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"\n    Binds two hypervectors using element-wise multiplication\n    \"\"\"\n    return hv1 * hv2\ndef bundle_hypervectors(*hvs):\n    \"\"\"\n    Bundles multiple hypervectors by summing them and normalizing\n    \"\"\"\n    bundled_hv = np.sum(hvs, axis=0)\n    return np.where(bundled_hv > 0, 1, -1)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generates a composite hypervector for a dataset row using HDC principles.\n\n    Each feature in the row is represented by a hypervector, which is a binding\n    of feature name and value hypervectors. All these hypervectors are then bundled to\n    represent the entire row.\n\n    Args:\n        row (dict): A dictionary of feature name-value pairs for a dataset row.\n        dim (int): The dimensionality of the hypervectors.\n        random_seeds (dict): A dictionary with feature names as keys and numeric seeds as values.\n\n    Returns:\n        list: A composite hypervector as a list representing the dataset row.\n    \"\"\"\n    hypervectors = []\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        feature_hv = create_hypervector(seed, dim)\n        value_hv = create_hypervector(int(value * 100 + seed), dim)\n        bound_hv = bind_hypervectors(feature_hv, value_hv)\n        hypervectors.append(bound_hv)\n    composite_hv = bundle_hypervectors(*hypervectors)\n    return composite_hv.tolist()\nrow = {'feature1': 3.4, 'feature2': 1.2, 'feature3': 2.9}\ndim = 32\nrandom_seeds = {'feature1': 10, 'feature2': 20, 'feature3': 30}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 7, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hypervector(seed, dim):\n        \"\"\"Generate a hypervector of dimension `dim` using `seed` for reproducibility.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n\n    def bind_hypervectors(hv1, hv2):\n        \"\"\"Element-wise multiplication of two hypervectors.\"\"\"\n        return hv1 * hv2\n\n    def bundle_hypervectors(hvs):\n        \"\"\"Bundling of a list of hypervectors.\"\"\"\n        return np.sum(hvs, axis=0) / len(hvs)\n    feature_hvs = []\n    for (feature, value) in row.items():\n        if feature in random_seeds:\n            name_hv = generate_hypervector(random_seeds[feature], dim)\n            value_hv = generate_hypervector(random_seeds[feature], dim)\n            feature_hv = bind_hypervectors(name_hv, value_hv)\n            feature_hvs.append(feature_hv)\n    row_hv = bundle_hypervectors(feature_hvs)\n    return row_hv.tolist()"}
{"task_id": 74, "completion_id": 8, "solution": "import numpy as np\ndef create_hypervector(name, value, dim, seed):\n    \"\"\"Create a hypervector for a feature by XORing the hypervector for the feature name with the value.\n    \n    Args:\n    - name (str): The feature name.\n    - value (str): The feature value.\n    - dim (int): The dimensionality of the hypervector.\n    - seed (int): The seed to use for generating the hypervector, ensuring reproducibility.\n    \n    Returns:\n    - np.array: A binary hypervector.\n    \"\"\"\n    np.random.seed(seed)\n    hv_name = np.random.choice([0, 1], size=dim)\n    hv_value = np.random.choice([0, 1], size=dim)\n    hv_feature = np.bitwise_xor(hv_name, hv_value)\n    return hv_feature\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Args:\n    - row (dict): A dictionary representing a dataset row, where keys are feature names and values are corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure hypervector reproducibility.\n    \n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature)\n        if seed is not None:\n            hv_feature = create_hypervector(feature, str(value), dim, seed)\n            composite_hv = np.bitwise_xor(composite_hv, hv_feature)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 9, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row.\n    \n    :param row: A dictionary where keys are feature names and values are their corresponding values.\n    :param dim: The dimensionality of the hypervectors.\n    :param random_seeds: A dictionary for seeds to ensure reproducibility of hypervectors.\n    :return: A composite hypervector (as a list) representing the entire row.\n    \"\"\"\n\n    def generate_hv(seed, dim):\n        \"\"\"Generate a bipolar hypervector with a given seed.\"\"\"\n        rng = np.random.default_rng(seed=seed)\n        return rng.choice([-1, 1], size=dim)\n\n    def bind_hyps(hv1, hv2):\n        \"\"\"Element-wise multiplication of two hypervectors.\"\"\"\n        return hv1 * hv2\n\n    def bundle(features_hv):\n        \"\"\"Bundle a list of hypervectors.\"\"\"\n        return np.sign(np.sum(features_hv, axis=0))\n    features_hv = []\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        hv_feature = generate_hv(seed, dim)\n        hv_value = generate_hv(hash((feature, value)), dim)\n        bound_hv = bind_hyps(hv_feature, hv_value)\n        features_hv.append(bound_hv)\n    composite_hv = bundle(features_hv)\n    return composite_hv.tolist()\nrow = {'feature1': 42, 'feature2': False, 'feature3': 'alpha'}\ndim = 20\nrandom_seeds = {'feature1': 101, 'feature2': 202, 'feature3': 303}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    :param data: A list of lists where each inner list represents a pair [y_true, y_pred] for one observation.\n    :return: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    conf_matrix_counter = Counter()\n    for (y_true, y_pred) in data:\n        conf_matrix_counter[y_true, y_pred] += 1\n    confusion_matrix = [[0, 0], [0, 0]]\n    confusion_matrix[0][0] = conf_matrix_counter[0, 0]\n    confusion_matrix[0][1] = conf_matrix_counter[1, 0]\n    confusion_matrix[1][0] = conf_matrix_counter[0, 1]\n    confusion_matrix[1][1] = conf_matrix_counter[1, 1]\n    return confusion_matrix"}
{"task_id": 75, "completion_id": 1, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    \n    :param data: List of lists, where each inner list represents a pair [y_true, y_pred] for one observation.\n    :return: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    confusion_count = Counter(((y_true, y_pred) for (y_true, y_pred) in data))\n    matrix = [[0, 0], [0, 0]]\n    matrix[0][0] = confusion_count[0, 0]\n    matrix[0][1] = confusion_count[0, 1]\n    matrix[1][0] = confusion_count[1, 0]\n    matrix[1][1] = confusion_count[1, 1]\n    return matrix\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]]"}
{"task_id": 75, "completion_id": 2, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification from provided data.\n    Each inner list in data contains two elements, where the first one is the ground truth\n    and the second is the predicted value.\n\n    :param data: List of lists with [y_true, y_pred] pairs\n    :return: A 2x2 confusion matrix in list form\n    \"\"\"\n    cm = {('0', '0'): 0, ('0', '1'): 0, ('1', '0'): 0, ('1', '1'): 0}\n    counts = Counter((tuple(pair) for pair in data))\n    for (key, count) in counts.items():\n        cm[key] = count\n    result = [[cm['0', '0'], cm['0', '1']], [cm['1', '0'], cm['1', '1']]]\n    return result"}
{"task_id": 75, "completion_id": 3, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Calculate the confusion matrix for a binary classification based on the provided data.\n    \n    Parameters:\n    data (list of lists): A list where each element is a list of two elements representing [true label, predicted label].\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix [[TP, FP], [FN, TN]], where\n        TP (int): True Positives\n        FP (int): False Positives\n        FN (int): False Negatives\n        TN (int): True Negatives\n    \"\"\"\n    count = Counter()\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            count['TP'] += 1\n        elif y_true == 0 and y_pred == 1:\n            count['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            count['FN'] += 1\n        elif y_true == 0 and y_pred == 0:\n            count['TN'] += 1\n    TP = count['TP']\n    FP = count['FP']\n    FN = count['FN']\n    TN = count['TN']\n    return [[TP, FP], [FN, TN]]\ndata = [[1, 0], [0, 0], [1, 1], [0, 1], [1, 0], [0, 0]]"}
{"task_id": 75, "completion_id": 4, "solution": "from collections import Counter\nfrom typing import List\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    The confusion matrix is represented as a list of lists:\n    [[TN, FP],\n     [FN, TP]]\n    where TN = True Negatives, FP = False Positives, FN = False Negatives, and TP = True Positives.\n    \n    :param data: A list of lists, where each inner list contains a pair [y_true, y_pred]\n    :return: A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    counter = Counter()\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            counter['TN'] += 1\n        elif y_true == 0 and y_pred == 1:\n            counter['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            counter['FN'] += 1\n        elif y_true == 1 and y_pred == 1:\n            counter['TP'] += 1\n    matrix = [[counter['TN'], counter['FP']], [counter['FN'], counter['TP']]]\n    return matrix\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0]]"}
{"task_id": 75, "completion_id": 5, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for a binary classification problem.\n    \n    Parameters:\n    data (list of lists): A list containing [y_true, y_pred] pairs, each representing the actual label and the predicted label.\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix, in the form of [[TN, FP], [FN, TP]].\n    \"\"\"\n    cm = [[0, 0], [0, 0]]\n    counters = Counter([(a, b) for (a, b) in data])\n    cm[0][0] = counters.get((0, 0), 0)\n    cm[0][1] = counters.get((0, 1), 0)\n    cm[1][0] = counters.get((1, 0), 0)\n    cm[1][1] = counters.get((1, 1), 0)\n    return cm\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [1, 0]]"}
{"task_id": 75, "completion_id": 6, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification based on the input data.\n    The input should be a list of lists, where each sublist is a pair [y_true, y_pred].\n    The output is a 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    counters = Counter((tuple(point) for point in data))\n    tn = counters[0, 0] if (0, 0) in counters else 0\n    fp = counters[0, 1] if (0, 1) in counters else 0\n    fn = counters[1, 0] if (1, 0) in counters else 0\n    tp = counters[1, 1] if (1, 1) in counters else 0\n    matrix = [[tn, fp], [fn, tp]]\n    return matrix\ndata = [[1, 1], [1, 0], [0, 0], [1, 1], [1, 1], [1, 0], [0, 1]]"}
{"task_id": 75, "completion_id": 7, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification based on the input data.\n    \n    Parameters:\n    data (list of lists): A list where each element is a list of the form [y_true, y_pred].\n    \n    Returns:\n    list of list: A 2x2 confusion matrix of the form [[TN, FP], [FN, TP]].\n    \"\"\"\n    confusion_counts = Counter()\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            confusion_counts['TN'] += 1\n        elif y_true == 0 and y_pred == 1:\n            confusion_counts['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            confusion_counts['FN'] += 1\n        elif y_true == 1 and y_pred == 1:\n            confusion_counts['TP'] += 1\n    confusion_matrix = [[confusion_counts['TN'], confusion_counts['FP']], [confusion_counts['FN'], confusion_counts['TP']]]\n    return confusion_matrix"}
{"task_id": 75, "completion_id": 8, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n    \n    Parameters:\n    data (list of lists): Each inner list represents a pair [y_true, y_pred] for one observation.\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix [[TP, FP], [FN, TN]], where\n        TP: True Positives, FP: False Positives,\n        FN: False Negatives, TN: True Negatives.\n    \"\"\"\n    condition_counter = Counter()\n    for row in data:\n        (y_true, y_pred) = row\n        if y_true == 1 and y_pred == 1:\n            condition_counter['TP'] += 1\n        elif y_true == 0 and y_pred == 1:\n            condition_counter['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            condition_counter['FN'] += 1\n        elif y_true == 0 and y_pred == 0:\n            condition_counter['TN'] += 1\n    cm = [[condition_counter['TP'], condition_counter['FP']], [condition_counter['FN'], condition_counter['TN']]]\n    return cm"}
{"task_id": 75, "completion_id": 9, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Calculate a 2x2 confusion matrix for a binary classification problem.\n    \n    Parameters:\n    - data: List of lists with each inner list being of the form [y_true, y_pred].\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    conf_mat = [[0, 0], [0, 0]]\n    for row in data:\n        (y_true, y_pred) = row\n        if y_true == 1 and y_pred == 1:\n            conf_mat[1][1] += 1\n        elif y_true == 0 and y_pred == 0:\n            conf_mat[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            conf_mat[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            conf_mat[1][0] += 1\n    return conf_mat"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two numpy arrays.\n    \n    Parameters:\n    - v1: numpy array, first vector.\n    - v2: numpy array, second vector.\n    \n    Returns:\n    - A float, cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot be empty or have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    mag_v1 = np.linalg.norm(v1)\n    mag_v2 = np.linalg.norm(v2)\n    cos_sim = dot_product / (mag_v1 * mag_v2)\n    return round(cos_sim, 3)"}
{"task_id": 76, "completion_id": 1, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (numpy.ndarray): The first input vector.\n    v2 (numpy.ndarray): The second input vector.\n    \n    Returns:\n    float: The cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    if v1.shape[0] == 0 or v2.shape[0] == 0:\n        raise ValueError('Vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors must not have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 2, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n        v1 (np.array): First input vector.\n        v2 (np.array): Second input vector.\n        \n    Returns:\n        float: Cosine similarity rounded to three decimal places.\n    \n    Raises:\n        ValueError: If the input vectors are of different lengths or have zero magnitude.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)"}
{"task_id": 76, "completion_id": 3, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors v1 and v2.\n    \n    Parameters:\n        v1 (np.ndarray): First vector.\n        v2 (np.ndarray): Second vector.\n        \n    Returns:\n        float: The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    (v1, v2) = (np.array(v1), np.array(v2))\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 4, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors v1 and v2.\n    \n    :param v1: Numpy array representing the first vector.\n    :param v2: Numpy array representing the second vector.\n    :return: A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    if np.count_nonzero(v1) == 0 or np.count_nonzero(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 5, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors, v1 and v2.\n    \n    Parameters:\n    v1 (numpy.ndarray): Input vector.\n    v2 (numpy.ndarray): Input vector.\n    \n    Returns:\n    float: Cosine similarity with three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    if len(v1) == 0 or len(v2) == 0:\n        raise ValueError('Vectors must be non-empty.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Vectors must have non-zero magnitude.')\n    result = dot_product / (norm_v1 * norm_v2)\n    return round(result, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 6, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: Numpy array representing the first input vector.\n    - v2: Numpy array representing the second input vector.\n    \n    Returns:\n    - float: cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('The input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    cos_sim = round(cos_sim, 3)\n    return cos_sim\nv1 = np.array([5, 5, 5])\nv2 = np.array([2, 2, 2])"}
{"task_id": 76, "completion_id": 7, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: Numpy array, the first input vector.\n    - v2: Numpy array, the second input vector.\n    \n    Returns:\n    - float, the cosine similarity rounded to three decimal places.\n    \n    Raises:\n    - ValueError if vectors have zero magnitude or different sizes.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 8, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Vectors cannot have zero magnitude')\n    cosine_similarity_value = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_similarity_value, 3)"}
{"task_id": 76, "completion_id": 9, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculates the cosine similarity between two vectors v1 and v2.\n    \n    Parameters:\n        v1 (numpy.ndarray): The first input vector.\n        v2 (numpy.ndarray): The second input vector.\n        \n    Returns:\n        float: The cosine similarity, rounded to three decimal places.\n    \"\"\"\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    try:\n        similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    except ZeroDivisionError:\n        raise ValueError('Encountered division by zero when calculating cosine similarity.')\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom sklearn.metrics import f1_score\nfrom typing import List, Tuple, Any\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[Any, float, float, float, float]:\n    assert len(actual) == len(predicted), 'actual and predicted lists must have the same length'\n    assert all((x in [0, 1] for x in actual)), 'actual list can only contain 0s and 1s'\n    assert all((x in [0, 1] for x in predicted)), 'predicted list can only contain 0s and 1s'\n    TP = FP = TN = FN = 0\n    for (a, p) in zip(actual, predicted):\n        if a == p == 1:\n            TP += 1\n        if p == 1 and a != p:\n            FP += 1\n        if a == p == 0:\n            TN += 1\n        if p == 0 and a != p:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    accuracy = round((TP + TN) / len(actual), 3)\n    f1_score_val = round(f1_score(actual, predicted), 3)\n    specificity = round(TN / (TN + FP), 3)\n    if FP + TN == 0:\n        negative_predictive_value = 1.0\n    else:\n        negative_predictive_value = round(TN / (TN + FN), 3)\n    return (confusion_matrix, accuracy, f1_score_val, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 0, 0]"}
{"task_id": 77, "completion_id": 1, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Computes performance metrics for a binary classification model.\n    Returns a tuple containing:\n    - Confusion Matrix\n    - Accuracy\n    - F1 Score\n    - Specificity\n    - Negative Predictive Value\n    \"\"\"\n    tp = fp = tn = fn = 0\n    for (a, p) in zip(actual, predicted):\n        if a == p == 1:\n            tp += 1\n        elif p == 1 and a != p:\n            fp += 1\n        elif a == p == 0:\n            tn += 1\n        elif p == 0 and a != p:\n            fn += 1\n    accuracy = (tp + tn) / len(actual)\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    specificity = tn / (tn + fp) if tn + fp != 0 else 0\n    npv = tn / (tn + fn) if tn + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return (np.array([[tn, fp], [fn, tp]]), round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(npv, 3))\nactual = [1, 1, 0, 0, 1, 0, 0, 1, 1, 1]\npredicted = [1, 0, 1, 0, 1, 0, 1, 1, 1, 0]"}
{"task_id": 77, "completion_id": 2, "solution": "from collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    assert len(actual) == len(predicted), 'Input lists must have the same length.'\n    assert all((isinstance(x, int) and x in (0, 1) for x in actual)), 'Invalid actual values.'\n    assert all((isinstance(x, int) and x in (0, 1) for x in predicted)), 'Invalid predicted values.'\n    confusion_matrix = np.zeros((2, 2))\n    for (a, p) in zip(actual, predicted):\n        confusion_matrix[a, p] += 1\n    (TN, FP, FN, TP) = confusion_matrix.flatten()\n    accuracy = np.round((TP + TN) / (TP + TN + FP + FN), 3)\n    precision = TP / (TP + FP) if TP + FP != 0 else 0\n    recall = TP / (TP + FN) if TP + FN != 0 else 0\n    f1_score = np.round(2 * (precision * recall) / (precision + recall), 3) if precision + recall != 0 else 0\n    specificity = np.round(TN / (TN + FP), 3) if TN + FP != 0 else 0\n    negative_predictive_value = np.round(TN / (TN + FN), 3) if TN + FN != 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 0, 1, 1, 0, 1]\npredicted = [1, 0, 0, 1, 1, 1, 0, 1]"}
{"task_id": 77, "completion_id": 3, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of 'actual' and 'predicted' should be the same.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    confusion_matrix_result = confusion_matrix(actual, predicted)\n    accuracy = sum((a == p for (a, p) in zip(actual, predicted))) / len(actual)\n    f1_score_result = f1_score(actual, predicted)\n    (tn, fp, fn, tp) = confusion_matrix(actual, predicted).ravel()\n    specificity_result = tn / (tn + fp) if tn + fp != 0 else 0\n    npv_result = tn / (tn + fn) if tn + fn != 0 else 0\n    return (confusion_matrix_result, round(accuracy, 3), round(f1_score_result, 3), round(specificity_result, 3), round(npv_result, 3))"}
{"task_id": 77, "completion_id": 4, "solution": "from collections import Counter\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom typing import Tuple, List\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple:\n    \"\"\"\n    Calculates several performance metrics for a binary classification problem.\n    :param actual: A list of actual class labels.\n    :param predicted: A list of predicted class labels.\n    :return: A tuple containing confusion matrix, accuracy, F1 score, specificity,\n             and negative predictive value.\n    \"\"\"\n    assert len(actual) == len(predicted), 'Lists actual and predicted must have the same length'\n    assert all((x in [0, 1] for x in actual)), 'All elements in the actual list must be either 0 or 1'\n    assert all((x in [0, 1] for x in predicted)), 'All elements in the predicted list must be either 0 or 1'\n    cm = confusion_matrix(actual, predicted)\n    (tn, fp, fn, tp) = cm.ravel()\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    f1_score_result = round(f1_score(actual, predicted), 3)\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn != 0 else 0\n    return (cm, accuracy, f1_score_result, specificity, negative_predictive_value)\nactual = [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1]\npredicted = [0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1]"}
{"task_id": 77, "completion_id": 5, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates various performance metrics for a binary classification model.\n    :param actual: List of actual class labels.\n    :param predicted: List of predicted class labels.\n    :return: A tuple containing confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('Actual and predicted lists must have the same length.')\n    conf_items = Counter(zip(actual, predicted)).items()\n    tp = conf_items[1, 1] if (1, 1) in conf_items else 0\n    fp = conf_items[0, 1] if (0, 1) in conf_items else 0\n    tn = conf_items[0, 0] if (0, 0) in conf_items else 0\n    fn = conf_items[1, 0] if (1, 0) in conf_items else 0\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = round(tp / (tp + fp), 3)\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = round(tp / (tp + fn), 3)\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = round(2 * (precision * recall) / (precision + recall), 3)\n    if tn + fp == 0:\n        specificity = 0\n    else:\n        specificity = round(tn / (tn + fp), 3)\n    if tn + fn == 0:\n        npv = 0\n    else:\n        npv = round(tn / (tn + fn), 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, npv)"}
{"task_id": 77, "completion_id": 6, "solution": "from typing import List, Tuple\nfrom collections import Counter\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    conf_matrix = np.zeros((2, 2), dtype=int)\n    for (a, p) in zip(actual, predicted):\n        conf_matrix[a][p] += 1\n    tp = conf_matrix[1][1]\n    fp = conf_matrix[0][1]\n    tn = conf_matrix[0][0]\n    fn = conf_matrix[1][0]\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0, 3)\n    specificity = round(tn / (tn + fp) if tn + fp > 0 else 0, 3)\n    negative_predictive_value = round(tn / (tn + fn) if tn + fn > 0 else 0, 3)\n    return (conf_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]"}
{"task_id": 77, "completion_id": 7, "solution": "from typing import List, Tuple\nfrom collections import Counter\nfrom sklearn.metrics import f1_score, confusion_matrix\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    \"\"\"\n    Calculates and returns performance metrics for a binary classification problem.\n    \n    :param actual: A list of actual class labels (0 or 1).\n    :param predicted: A list of predicted class labels (0 or 1).\n    :return: A tuple containing the confusion matrix, accuracy, F1 score, specificity, and negative predictive value.\n    \"\"\"\n    (tn, fp, fn, tp) = confusion_matrix(actual, predicted).ravel()\n    confusion_matrix_result = np.array([[tp, fp], [fn, tn]])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    f1 = round(f1_score(actual, predicted), 3)\n    specificity = round(tn / (tn + fp), 3)\n    npv = round(tn / (tn + fn), 3)\n    return (confusion_matrix_result, accuracy, f1, specificity, npv)"}
{"task_id": 77, "completion_id": 8, "solution": "from collections import Counter\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert len(actual) == len(predicted), 'Lists must be of the same length'\n    for value in actual + predicted:\n        assert value in [0, 1], 'Values must be either 0 or 1'\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall != 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp != 0 else 0\n    npv = round(tn / (tn + fn), 3) if tn + fn != 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, npv)\nactual = [1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]"}
{"task_id": 77, "completion_id": 9, "solution": "from collections import Counter\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, f1_score\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    assert len(actual) == len(predicted), 'Actual and predicted lists must have the same length'\n    unique_values = set(actual + predicted)\n    assert unique_values <= {0, 1}, 'Lists can only contain 0s and 1s'\n    confusion_mat = confusion_matrix(actual, predicted)\n    (tn, fp, fn, tp) = confusion_mat.ravel()\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    f1_s = round(f1_score(actual, predicted), 3)\n    specificity = round(tn / (tn + fp), 3)\n    negative_predictive_value = round(tn / (tn + fn), 3)\n    return (confusion_mat, accuracy, f1_s, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    statistics = {'mean': np.round(data.mean(), decimals=4), 'median': np.round(np.median(data), decimals=4), 'mode': stats.mode(data)[0][0].item() if stats.mode(data)[1][0] > 0 else np.nan, 'variance': np.round(np.var(data, ddof=1), decimals=4), 'standard_deviation': np.round(np.std(data, ddof=1), decimals=4), '25th_percentile': np.round(np.percentile(data, 25), decimals=4), '50th_percentile': np.round(np.percentile(data, 50), decimals=4), '75th_percentile': np.round(np.percentile(data, 75), decimals=4), 'interquartile_range': np.round(np.percentile(data, 75) - np.percentile(data, 25), decimals=4)}\n    return statistics"}
{"task_id": 78, "completion_id": 1, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate and return various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    - data: A list or NumPy array of numerical values.\n    \n    Returns:\n    A dictionary with keys: 'mean', 'median', 'mode', 'variance', 'standard_deviation',\n    '25th_percentile', '50th_percentile', '75th_percentile', 'interquartile_range'.\n    All metrics should be rounded to four decimal places except where not applicable.\n    \"\"\"\n    data_array = np.array(data)\n    stats_dict = {'mean': np.round(np.mean(data_array), 4) if not np.isnan(np.mean(data_array)) else np.mean(data_array), 'median': np.round(np.median(data_array), 4), 'mode': stats.mode(data_array)[0][0] if len(stats.mode(data_array)[0]) == 1 else stats.mode(data_array)[0], 'variance': np.round(np.var(data_array, ddof=1), 4) if not np.isnan(np.var(data_array, ddof=1)) else np.var(data_array, ddof=1), 'standard_deviation': np.round(np.std(data_array, ddof=1), 4) if not np.isnan(np.std(data_array, ddof=1)) else np.std(data_array, ddof=1), '25th_percentile': np.round(np.percentile(data_array, 25), 4), '50th_percentile': np.round(np.percentile(data_array, 50), 4), '75th_percentile': np.round(np.percentile(data_array, 75), 4), 'interquartile_range': np.round(np.percentile(data_array, 75) - np.percentile(data_array, 25), 4)}\n    return stats_dict\ndata = [12.4, 23.5, 23.5, 34.6, 12.4, 23.5, 45.6, 56.7, 67.8, 78.9, 89.0, 78.9, 67.8]"}
{"task_id": 78, "completion_id": 2, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data) if not isinstance(data, np.ndarray) else data\n    metrics = {'mean': round(np.mean(data), 4), 'median': round(np.median(data), 4), 'mode': stats.mode(data).mode[0], 'variance': round(np.var(data, ddof=1), 4), 'standard_deviation': round(np.std(data, ddof=1), 4), '25th_percentile': round(np.percentile(data, 25), 4), '50th_percentile': round(np.percentile(data, 50), 4), '75th_percentile': round(np.percentile(data, 75), 4), 'interquartile_range': round(np.percentile(data, 75) - np.percentile(data, 25), 4)}\n    return metrics\ndata = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]"}
{"task_id": 78, "completion_id": 3, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): The input data for which the statistics will be calculated.\n\n    Returns:\n    dict: A dictionary containing calculated statistics with keys as specified.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data).round(4)\n    median = np.median(data)\n    mode = float(np.round(np.average([np.unique(data)[np.argmax(np.unique(data, return_counts=True)[1])]], decimals=4)))\n    variance = np.var(data, ddof=0).round(4)\n    standard_deviation = np.std(data, ddof=0).round(4)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = (percentile_75 - percentile_25).round(4)\n    stats = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}\n    return stats"}
{"task_id": 78, "completion_id": 4, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = round(np.mean(data), 4)\n    median = round(np.median(data), 4)\n    mode = stats.mode(data)[0][0]\n    variance = round(np.var(data, ddof=1), 4)\n    standard_deviation = round(np.std(data, ddof=1), 4)\n    percentile_25 = round(np.percentile(data, 25), 4)\n    percentile_50 = round(np.percentile(data, 50), 4)\n    percentile_75 = round(np.percentile(data, 75), 4)\n    iqr = round(percentile_75 - percentile_25, 4)\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}"}
{"task_id": 78, "completion_id": 5, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics metrics for a given dataset.\n\n    :param data: list or NumPy array of numerical values\n    :return: dictionary containing 'mean', 'median', 'mode', 'variance',\n             'standard_deviation', '25th_percentile', '50th_percentile',\n             '75th_percentile', and 'interquartile_range'\n    \"\"\"\n    data_np = np.array(data) if isinstance(data, list) else data\n    mean = np.mean(data_np)\n    median = np.median(data_np)\n    mode = stats.mode(data_np)[0][0]\n    variance = np.var(data_np)\n    std_deviation = np.std(data_np)\n    percentile_25 = np.percentile(data_np, 25)\n    percentile_50 = np.percentile(data_np, 50)\n    percentile_75 = np.percentile(data_np, 75)\n    iqr = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(std_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\n    return results\ndata = [70, 85, 30, 90, 75, 95, 85, 60, 94, 98, 79, 91, 80, 74, 88, 86, 95, 88, 66, 76]"}
{"task_id": 78, "completion_id": 6, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Computes various descriptive statistics for a given dataset.\n    \n    Parameters:\n    - data: A list or NumPy array of numerical values.\n    \n    Returns:\n    A dictionary containing the computed statistics.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data, ddof=1)\n    std_deviation = np.std(data, ddof=1)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    return {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\ndata = [2, 4, 4, 4, 5, 5, 7, 9]"}
{"task_id": 78, "completion_id": 7, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n\n    Parameters:\n    data (list or numpy.ndarray): A dataset of numerical values.\n\n    Returns:\n    dict: A dictionary containing 'mean', 'median', 'mode', 'variance',\n          'standard_deviation', '25th_percentile', '50th_percentile',\n          '75th_percentile', and 'interquartile_range'.\n          All metrics, except for 'median' and 'mode', are rounded to 4 decimal places.\n    \"\"\"\n    if isinstance(data, list):\n        data = np.array(data)\n    mean = round(np.mean(data), 4)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = round(np.var(data, ddof=1), 4)\n    standard_deviation = round(np.std(data, ddof=1), 4)\n    percentile_25 = round(np.percentile(data, 25), 4)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = round(np.percentile(data, 75), 4)\n    iqr = round(percentile_75 - percentile_25, 4)\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\ndata = [1, 2, 2, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9]"}
{"task_id": 78, "completion_id": 8, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate and return a dictionary of descriptive statistics for a dataset.\n\n    Parameters:\n    data (list or numpy.ndarray): The array of numerical values.\n\n    Returns:\n    dict: A dictionary containing various statistical metrics.\n    \"\"\"\n    stats_dict = {}\n    data = np.array(data)\n    stats_dict['mean'] = round(np.mean(data), 4)\n    stats_dict['median'] = round(np.median(data), 4)\n    stats_dict['mode'] = float(np.mode(data))\n    stats_dict['variance'] = round(np.var(data, ddof=1), 4)\n    stats_dict['standard_deviation'] = round(np.std(data, ddof=1), 4)\n    stats_dict['25th_percentile'] = round(np.percentile(data, 25), 4)\n    stats_dict['50th_percentile'] = round(np.percentile(data, 50), 4)\n    stats_dict['75th_percentile'] = round(np.percentile(data, 75), 4)\n    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n    stats_dict['interquartile_range'] = round(iqr, 4)\n    return stats_dict"}
{"task_id": 78, "completion_id": 9, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a dataset.\n\n    Parameters:\n    data (list or numpy.ndarray): A list or numpy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing various statistics of the dataset.\n    \"\"\"\n    if isinstance(data, list):\n        data = np.array(data)\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data, ddof=1).round(4)\n    standard_deviation = np.std(data, ddof=1).round(4)\n    percentile_25 = np.percentile(data, 25).round(4)\n    percentile_50 = np.percentile(data, 50).round(4)\n    percentile_75 = np.percentile(data, 75).round(4)\n    interquartile_range = (percentile_75 - percentile_25).round(4)\n    stats_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}\n    return stats_dict"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials rounded to five decimal places\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 1, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 3\np = 0.5"}
{"task_id": 79, "completion_id": 2, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 3, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 4, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combinations = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    probability = combinations * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 5, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    combination = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 6, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coefficient = math.comb(n, k)\n    probability = binomial_coefficient * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 7, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 8, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    coefficient = math.comb(n, k)\n    probability = coefficient * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 9, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)\nn = 10\nk = 3\np = 0.5"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value of the normal distribution at x, rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-0.5 * ((x - mean) / std_dev) ** 2)\n    divisor = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / divisor\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 1, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = -(math.pow(x - mean, 2) / (2 * math.pow(std_dev, 2)))\n    pdf_value = 1 / (math.sqrt(2 * math.pi) * std_dev) * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 2, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-0.5 * ((x - mean) / std_dev) ** 2)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\nx = 1.0\nmean = 0.0\nstd_dev = 1.0"}
{"task_id": 80, "completion_id": 3, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The value of the PDF rounded to 5 decimal places.\n    \"\"\"\n    denominator = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(math.pow(x - mean, 2) / (2 * math.pow(std_dev, 2)))\n    pdf_value = denominator * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 4, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 5, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    pdf_value = 1 / (math.sqrt(2 * math.pi) * std_dev) * exponent\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 6, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    divisor = std_dev * math.sqrt(2 * math.pi)\n    return round(exponent / divisor, 5)"}
{"task_id": 80, "completion_id": 7, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value of the normal distribution rounded to 5 decimal places.\n    \"\"\"\n    coeff = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    result = coeff * math.exp(exponent)\n    return round(result, 5)"}
{"task_id": 80, "completion_id": 8, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 9, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than 0')\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    pdf = 1 / (math.sqrt(2 * math.pi) * std_dev) * exponent\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    denominator = math.factorial(k)\n    numerator = math.pow(lam, k) * math.exp(-lam)\n    probability = numerator / denominator\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 1, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of occurrences using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events (float with 5 decimal places)\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 2, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences in a fixed interval using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 3, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    prob = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(prob, 5)\nlam = 3\nk = 2"}
{"task_id": 81, "completion_id": 4, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places.\n    \"\"\"\n    numerator = math.exp(-lam) * lam ** k\n    denominator = math.factorial(k)\n    probability = numerator / denominator\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 5, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences in a fixed interval using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    numerator = math.pow(lam, k) * math.exp(-lam)\n    denominator = math.factorial(k)\n    probability = numerator / denominator\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 6, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam < 0:\n        raise ValueError('k and lam must be non-negative')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 7, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if lam < 0 or k < 0:\n        raise ValueError('Both k and lambda should be non-negative.')\n    e = math.e\n    factorial = math.factorial(k)\n    probability = lam ** k * e ** (-lam) / factorial\n    return round(probability, 5)\nk = 3\nlam = 2"}
{"task_id": 81, "completion_id": 8, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    try:\n        probability = math.exp(-lam) * lam ** k / math.factorial(k)\n        return round(probability, 5)\n    except ValueError:\n        return None"}
{"task_id": 81, "completion_id": 9, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events (float rounded to 5 decimal places)\n    \"\"\"\n    if k < 0 or lam < 0:\n        raise ValueError('Both k and lambda must be non-negative numbers.')\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or len(img.shape) != 2:\n        raise ValueError('Input img should be a 2D numpy array representing a grayscale image')\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 82, "completion_id": 1, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    img_array = np.array(img)\n    assert img_array.ndim == 2, 'The provided image should be 2D.'\n    max_pixel_value = np.max(img_array)\n    min_pixel_value = np.min(img_array)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 82, "completion_id": 2, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image.\n    \n    Note:\n        Contrast is defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 82, "completion_id": 3, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image.\n    \"\"\"\n    if img.dtype != np.uint8 or len(img.shape) != 2:\n        raise ValueError('Input must be a 2D numpy array with uint8 datatype.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\nimg = np.array([[255, 200, 100], [50, 0, 150]], dtype=np.uint8)"}
{"task_id": 82, "completion_id": 4, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        float: The contrast of the image calculated as the difference between the maximum and minimum pixel values.\n    \n    Raises:\n        ValueError: If the input image is not a 2D numpy array.\n    \"\"\"\n    if not (isinstance(img, np.ndarray) and img.ndim == 2):\n        raise ValueError('Input image must be a 2D numpy array')\n    contrast = np.max(img) - np.min(img)\n    return contrast"}
{"task_id": 82, "completion_id": 5, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the grayscale image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    flat_img = img.flatten()\n    max_pixel = np.max(flat_img)\n    min_pixel = np.min(flat_img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 6, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input must be a 2D numpy array representing a grayscale image.')\n    min_val = np.min(img)\n    max_val = np.max(img)\n    contrast = max_val - min_val\n    return contrast\nimg = np.array([[10, 20, 30], [40, 50, 255], [5, 150, 200]])"}
{"task_id": 82, "completion_id": 7, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Contrast is defined as the difference between the maximum and minimum pixel values.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        \n    Returns:\n        float: Contrast of the image, which is the difference between the max and min pixel values.\n    \"\"\"\n    max_value = np.max(img)\n    min_value = np.min(img)\n    contrast = max_value - min_value\n    return contrast"}
{"task_id": 82, "completion_id": 8, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the grayscale image, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 82, "completion_id": 9, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, which is the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray):\n        raise TypeError('Input must be a numpy ndarray')\n    if img.ndim != 2:\n        raise ValueError('Input must be a 2D array representing a grayscale image')\n    max_val = np.max(img)\n    min_val = np.min(img)\n    contrast = max_val - min_val\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        int or float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 1, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 2, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        int or float: The dot product of the two vectors.\n    \"\"\"\n    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):\n        raise ValueError('Both inputs must be numpy arrays.')\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Input vectors must be of the same length.')\n    return np.dot(vec1, vec2)\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 83, "completion_id": 3, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if not (isinstance(vec1, np.ndarray) and isinstance(vec2, np.ndarray)):\n        raise ValueError('Both inputs must be numpy arrays.')\n    if not (vec1.ndim == 1 and vec2.ndim == 1):\n        raise ValueError('Both inputs must be 1D arrays.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 4, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both vectors should be 1D arrays.')\n    if len(vec1) != len(vec2):\n        raise ValueError('Both vectors should have the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 5, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        raise ValueError('The vectors must be of the same length.')\n    dot_product_result = np.dot(vec1, vec2)\n    return dot_product_result"}
{"task_id": 83, "completion_id": 6, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 7, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 83, "completion_id": 8, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 9, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must have the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    n = len(data)\n    transformed_data = [np.power(data, deg).tolist() for deg in range(1, degree + 1)]\n    result = list(map(list, zip(*transformed_data)))\n    rounded_result = [[round(val, 8) for val in inner_list] for inner_list in result]\n    return rounded_result\ndegree = 3"}
{"task_id": 84, "completion_id": 1, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: The transformed features of data points.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_data = []\n    for x in data_array:\n        feature_vector = [x ** d for d in range(1, degree + 1)]\n        transformed_data.append(feature_vector)\n    transformed_data_rounded = [np.round(fv, 8).tolist() for fv in transformed_data]\n    return transformed_data_rounded"}
{"task_id": 84, "completion_id": 2, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list containing the transformed features.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_points = np.array(data)[:, np.newaxis]\n    powers = np.arange(degree + 1, dtype=int)\n    transformed_data = (data_points ** powers).T\n    return [list(map(lambda x: round(x, 8), sublist)) for sublist in transformed_data]\ndata_points = [1.0, 2.5, 3.0]\ndegree = 2"}
{"task_id": 84, "completion_id": 3, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data).reshape(-1, 1)\n    poly_features = np.vander(data_array, degree + 1, increasing=True)\n    poly_features_list = [list(np.round(row, 8)) for row in poly_features]\n    return poly_features_list"}
{"task_id": 84, "completion_id": 4, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    X = np.array(data)\n    poly_features = []\n    for x in X:\n        features = [x ** i for i in range(1, degree + 1)]\n        poly_features.append(features)\n    return [list(map(lambda x: round(x, 8), feature_set)) for feature_set in poly_features]"}
{"task_id": 84, "completion_id": 5, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree <= 0:\n        return []\n    data_matrix = np.array(data).reshape(-1, 1)\n    transformed_data = []\n    for d in range(1, degree + 1):\n        transformed_data.append(data_matrix ** d)\n    transformed_data = np.concatenate(transformed_data, axis=1)\n    return np.around(transformed_data.tolist(), decimals=8)"}
{"task_id": 84, "completion_id": 6, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list: A nested list where each inner list represents the transformed polynomial features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for value in data:\n        new_features = [value ** i for i in range(1, degree + 1)]\n        transformed_data.append(new_features)\n    return np.round(transformed_data, 8).tolist()"}
{"task_id": 84, "completion_id": 7, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    transformed_data = np.ones(data.shape)\n    for pow in range(1, degree + 1):\n        transformed_data = np.column_stack((transformed_data, data ** pow))\n    transformed_data_list = [list(map(lambda x: round(x, 8), row)) for row in transformed_data]\n    return transformed_data_list"}
{"task_id": 84, "completion_id": 8, "solution": "import numpy as np\nfrom itertools import product\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    def generate_polynomial_row(input_row, deg):\n        \"\"\"Generate polynomial features for one row of data.\"\"\"\n        features = [input_row] * deg\n        poly_row = [1]\n        for i in range(1, deg + 1):\n            for combo in product(input_row, repeat=i):\n                feature_value = np.prod(combo)\n                poly_row.append(feature_value)\n        return poly_row\n    transformed_data = [generate_polynomial_row([x], degree) for x in data]\n    return np.round(transformed_data, 8).tolist()"}
{"task_id": 84, "completion_id": 9, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: The polynomial transformation of the input data.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    transformed_features = [data ** deg for deg in range(1, degree + 1)]\n    transformed_features = np.array(transformed_features).T.tolist()\n    transformed_features = [[round(val, 8) for val in row] for row in transformed_features]\n    return transformed_features"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encodings for a Transformer model.\n    \n    :param position: An integer representing the sequence length.\n    :param d_model: An integer representing the embedding dimensions.\n    :return: A list containing the positional encodings or -1 if inputs are invalid.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(d_model) / d_model))\n    pos = np.arange(position)[:, np.newaxis]\n    pe[:, 0::2] = np.sin(pos * div_term)\n    pe[:, 1::2] = np.cos(pos * div_term)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 1, "solution": ""}
{"task_id": 85, "completion_id": 2, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for each position up to `position`\n    in the sequence and for each dimension in `d_model`. The encoding uses\n    sine and cosine functions of different frequencies which makes it possible\n    for the model to learn about order of sequences.\n    \n    :param position: The length of the sequence up to which to calculate the positional encodings.\n    :param d_model: The dimensionality of the model.\n    :return: A list containing the positional encodings or -1 for invalid inputs.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(d_model):\n            angle = pos / np.power(10000, 2 * (i // 2) / np.float16(d_model))\n            if i % 2 == 0:\n                pe[pos, i] = np.sin(angle)\n            else:\n                pe[pos, i] = np.cos(angle)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 3, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a sequence of positions and a given model dimensionality.\n    \n    Args:\n        position (int): The length of the position dimension.\n        d_model (int): The model's dimensionality.\n        \n    Returns:\n        list: A list of positional encodings or -1 according to the given constraints.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n    pos = np.arange(position)[:, np.newaxis]\n    pe[:, 0::2] = np.sin(pos * div_term)\n    pe[:, 1::2] = np.cos(pos * div_term)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 4, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encodings for a given sequence length and model dimensionality.\n    If position is 0 or d_model is not greater than 0, returns -1.\n    \n    :param position: int - the length of the position encoding to generate\n    :param d_model: int - the model dimensionality for which to generate the positional encoding\n    :return: list - list representation of the positional encodings\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            denominator = np.power(10000, 2 * i / d_model)\n            pe[pos, i] = np.sin(pos / denominator)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / denominator)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 5, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding as specified for Transformers.\n    \n    Args:\n    position (int): The maximum length of the input sequence.\n    d_model (int): The dimensionality of the model.\n\n    Returns:\n    -1 for invalid input, otherwise returns the positional encoding list.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(d_model) / d_model))\n    pos = np.arange(position)[:, np.newaxis]\n    pe[:, 0::2] = np.sin(pos * div_term)\n    pe[:, 1::2] = np.cos(pos * div_term)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 6, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a sequence based on the Transformer architecture.\n    \n    Args:\n    - position (int): The length of the sequence.\n    - d_model (int): The dimensionality of the model.\n    \n    Returns:\n    - np.ndarray: A list representing the positional encodings for the sequence if valid input; otherwise, -1.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle_rate = pos / 10000 ** (2 * i / d_model)\n            pe[pos, i] = np.sin(angle_rate)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(angle_rate)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 7, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encodings for a given position and model dimensionality.\n    \n    Args:\n    position (int): Length of the sequence.\n    d_model (int): Model dimensionality.\n    \n    Returns:\n    list: List of positional encodings or -1 if inputs are invalid.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 8, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a given sequence length and model dimensionality.\n    \n    Args:\n    position: The length of the input sequence.\n    d_model: The model dimensionality.\n    \n    Returns:\n    - A list with positional encodings, if input is valid.\n    - -1 if position is 0 or d_model is less than or equal to 0.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    angular_freq = np.array([1 / 10000 ** (2 * (i // 2) / d_model) for i in range(d_model)])\n    positional_vector = np.array([pos for pos in range(position)])\n    positional_vector = positional_vector[:, np.newaxis]\n    angular_freq = angular_freq[np.newaxis, :]\n    pos_angle = positional_vector * angular_freq\n    pos_angle[:, 0::2] = np.sin(pos_angle[:, 0::2])\n    pos_angle[:, 1::2] = np.cos(pos_angle[:, 1::2])\n    return pos_angle.astype(np.float16).tolist()"}
{"task_id": 85, "completion_id": 9, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a given sequence length and model dimensionality.\n    \n    Args:\n    position (int): The length of the input sequence (sequence length).\n    d_model (int): Model dimensionality.\n    \n    Returns:\n    List of positional encodings or -1 if input parameters are invalid.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float16(d_model))\n            pe[pos, i] = np.sin(pos * angle_rates)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos * angle_rates)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    accuracy_diff = training_accuracy - test_accuracy\n    if accuracy_diff > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 1, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 2, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 3, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 4, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 5, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 6, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 7, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 8, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 9, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 87, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)\nparameter = np.array([0.5, -0.5])\ngrad = np.array([0.1, -0.05])\nm = np.array([0.06, -0.03])\nv = np.array([0.004, 0.0009])\nt = 2\nlearning_rate = 0.001"}
{"task_id": 87, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(updated_parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 87, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.power(grad, 2)\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = np.around(parameter, decimals=5).tolist()\n    m = np.around(m, decimals=5).tolist()\n    v = np.around(v, decimals=5).tolist()\n    return (parameter, m, v)\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1"}
{"task_id": 87, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 87, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(updated_parameter.tolist(), 5), np.round(m.tolist(), 5), np.round(v.tolist(), 5))\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.1, 0.1])\nm = np.array([0.06, 0.06, 0.06])\nv = np.array([0.0013, 0.0013, 0.0013])\nt = 2"}
{"task_id": 87, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist() if isinstance(parameter, np.ndarray) else round(parameter, 5)\n    updated_m = np.round(m, 5).tolist() if isinstance(m, np.ndarray) else round(m, 5)\n    updated_v = np.round(v, 5).tolist() if isinstance(v, np.ndarray) else round(v, 5)\n    return (updated_parameter, updated_m, updated_v)\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.3, 0.5, 0.8])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1"}
{"task_id": 87, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - np.power(beta1, t))\n    v_hat = v / (1 - np.power(beta2, t))\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n    return (updated_parameter.tolist(), updated_m.tolist(), updated_v.tolist())\nparameter = np.array([0.5, -0.5])\ngrad = np.array([0.1, -0.1])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1"}
{"task_id": 87, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    if not isinstance(parameter, np.ndarray):\n        parameter = np.array(parameter)\n        m = np.array(m)\n        v = np.array(v)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)\nparameter = np.array([2.0, 3.0])\ngrad = np.array([0.5, -0.5])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1"}
{"task_id": 87, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    if isinstance(updated_parameter, np.ndarray):\n        updated_parameter = updated_parameter.tolist()\n        m = m.tolist()\n        v = v.tolist()\n    else:\n        updated_parameter = round(updated_parameter, 5)\n        m = round(m, 5)\n        v = round(v, 5)\n    return (updated_parameter, m, v)\nparameter = np.array([1.0, 2.0])\ngrad = np.array([0.1, 0.2])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1\nlearning_rate = 0.001\nbeta1 = 0.9\nbeta2 = 0.999\nepsilon = 1e-08"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef random_softmax(x: np.ndarray) -> np.ndarray:\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    np.random.seed(42)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n\n    def get_pos_emb(pos: int) -> np.ndarray:\n        return params['wpe'][pos % hparams['n_ctx']]\n\n    def emb_sum(token_id: int, pos: int) -> np.ndarray:\n        return params['wte'][token_id] + get_pos_emb(pos)\n    token_ids = encoder.encode(prompt)\n    output_ids = token_ids.copy()\n    for _ in range(n_tokens_to_generate):\n        all_embs = np.array([emb_sum(tok_id, i) for (i, tok_id) in enumerate(output_ids[-hparams['n_ctx']:])])\n        next_id_probs = random_softmax(np.sum(all_embs, axis=0))\n        next_id = np.random.choice(range(params['wte'].shape[0]), p=next_id_probs)\n        output_ids.append(next_id)\n    return encoder.decode(output_ids)"}
{"task_id": 88, "completion_id": 1, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n\n    def layer_normalize(x, g, b, epsilon=1e-05):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        std = np.std(x, axis=-1, keepdims=True)\n        return g * (x - mean) / (std + epsilon) + b\n\n    def multihead_attention(q, k, v, n_head):\n        return np.einsum('bik,bjk->bij', q, k) / np.sqrt(q.shape[-1])\n\n    def feed_forward_network(x, weights):\n        return x @ weights\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = params['wte'][token_ids]\n    pos_embeddings = params['wpe'][:len(token_embeddings)]\n    x = token_embeddings + pos_embeddings\n    for _ in range(n_tokens_to_generate):\n        attended_x = multihead_attention(x, x, x, hparams['n_head'])\n        y = feed_forward_network(attended_x, params['wte'])\n        normalized_y = layer_normalize(y, params['ln_f']['g'], params['ln_f']['b'])\n        next_token_prediction = np.argmax(normalized_y[-1])\n        token_ids = np.append(token_ids, next_token_prediction)\n        x = np.append(x, [params['wte'][next_token_prediction]], axis=0)\n    return encoder.decode(token_ids.tolist())"}
{"task_id": 88, "completion_id": 2, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n            self.encoder_dict_inv = {v: k for (k, v) in self.encoder_dict.items()}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            return ' '.join([self.encoder_dict_inv.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    n_ctx = hparams['n_ctx']\n    token_ids = token_ids[-n_ctx:]\n    for _ in range(n_tokens_to_generate):\n        next_token = np.random.choice(list(encoder.encoder_dict.values()))\n        token_ids.append(next_token)\n    generated_text = encoder.decode(token_ids)\n    return generated_text"}
{"task_id": 88, "completion_id": 3, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': np.random.rand(3, 2, 2, 10), 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    pos_ids = np.arange(len(input_ids))\n    token_embed = params['wte'][input_ids]\n    pos_embed = params['wpe'][pos_ids]\n    x = token_embed + pos_embed\n    for block in params['blocks']:\n        attention_res = np.matmul(x, block[0])\n        ffn_in = gelu(attention_res @ block[1])\n        x = x + ffn_in\n    x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n    next_token_id = np.argmax(x[-1])\n    generated_tokens = [next_token_id]\n    for _ in range(n_tokens_to_generate - 1):\n        input_ids = [generated_tokens[-1]]\n        pos_ids = [len(input_ids) + len(prompt.split())]\n        token_embed = params['wte'][input_ids]\n        pos_embed = params['wpe'][pos_ids[0]]\n        x = token_embed.squeeze() + pos_embed\n        attention_res = np.matmul(x, params['blocks'][0][0])\n        ffn_in = gelu(attention_res @ params['blocks'][0][1])\n        x = x + ffn_in\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        next_token_id = np.argmax(x[-1])\n        generated_tokens.append(next_token_id)\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text"}
{"task_id": 88, "completion_id": 4, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(30, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n\n    def positional_encoding(seq_len):\n        pos = np.arange(seq_len)[:, np.newaxis]\n        return params['wpe'][pos]\n\n    def layer_norm(x, g, b, eps=1e-05):\n        mean = x.mean(-1, keepdims=True)\n        std = x.std(-1, keepdims=True, ddof=1)\n        return g * (x - mean) / (std + eps) + b\n\n    def multihead_attention(h, w, n_head):\n        c_attn = w['c_attn']\n        c_proj = w['c_proj']\n        (q, k, v) = np.split(c_attn @ h.T, 3)\n        q = q.reshape(n_head, -1, q.shape[-1] // n_head)\n        k = k.reshape(n_head, -1, k.shape[-1] // n_head)\n        v = v.reshape(n_head, -1, v.shape[-1] // n_head)\n        scores = np.matmul(q, k.T) / float(v.shape[-1]) ** 0.5\n        attn = np.matmul(np.softmax(scores, -1), v)\n        attn = attn.reshape(-1, attn.shape[-1])\n        return c_proj @ attn\n\n    def feed_forward_network(h, w):\n        fc = w['c_fc']\n        proj = w['c_proj']\n        return proj @ np.maximum(fc @ h, 0)\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    id_tokens = encoder.encode(prompt)[:hparams['n_ctx']]\n    x = params['wte'][id_tokens + [params['wte'].shape[0] - 1] * (hparams['n_ctx'] - len(id_tokens))]\n    x += positional_encoding(len(id_tokens))\n    for block in params['blocks']:\n        x_attn = multihead_attention(layer_norm(x, **block['ln_1']), block['attn'], hparams['n_head'])\n        x += x_attn\n        x_mlp = feed_forward_network(layer_norm(x_attn + x, **block['ln_2']), block['mlp'])\n        x += x_mlp\n    x = layer_norm(x, **params['ln_f'])\n    x = [np.argmax(x, -1)[-1]]\n    for _ in range(n_tokens_to_generate - 1):\n        x = np.concatenate([x, [np.argmax(x, -1)[-1]]], 0)\n    return encoder.decode(x[:n_tokens_to_generate])"}
{"task_id": 88, "completion_id": 5, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(30, 10)}, 'ffn': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}} for _ in range(12)], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n\n    def get_embeddings(input_ids, position_ids):\n        return params['wte'][input_ids] + params['wpe'][position_ids]\n\n    def attention(embeddings):\n        qkv = np.dot(embeddings, params['blocks'][0]['attn']['c_attn'])\n        (q, k, v) = np.split(qkv, 3, axis=1)\n        attention_scores = np.dot(q, k.T) / np.sqrt(k.shape[1])\n        attention_output = np.dot(attention_scores, v)\n        return np.dot(attention_output, params['blocks'][0]['attn']['c_proj'])\n\n    def feed_forward(attention_out):\n        ff_hidden = np.dot(attention_out, params['blocks'][0]['ffn']['c_fc'])\n        ff_hidden = np.maximum(ff_hidden, 0)\n        return np.dot(ff_hidden, params['blocks'][0]['ffn']['c_proj'])\n\n    def layer_normalization(x):\n        return x * params['ln_f']['g'] + params['ln_f']['b']\n    input_ids = encoder.encode(prompt)\n    current_length = len(input_ids)\n    for _ in range(n_tokens_to_generate):\n        embeddings = get_embeddings(input_ids[-hparams['n_ctx']:], range(len(input_ids[-hparams['n_ctx']:])))\n        outputs = attention(embeddings)\n        outputs = feed_forward(outputs)\n        outputs = layer_normalization(outputs)\n        token_probabilities = np.sum(outputs, axis=0)\n        new_token_id = np.random.choice(np.arange(params['wte'].shape[0]), p=token_probabilities / np.sum(token_probabilities))\n        input_ids.append(new_token_id)\n        if current_length >= hparams['n_ctx']:\n            input_ids.pop(0)\n            current_length -= 1\n        current_length += 1\n    return encoder.decode(input_ids)"}
{"task_id": 88, "completion_id": 6, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n            self.decoder_dict = {v: k for (k, v) in self.encoder_dict.items()}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, 0) for token in tokens]\n\n        def decode(self, token_ids: list):\n            return ' '.join([self.decoder_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef positional_encoding(pos: np.ndarray, d_model: int):\n    pe = np.zeros((pos.shape[0], d_model))\n    position = pos[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n    return pe\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    d_model = params['wte'].shape[-1]\n    for _ in range(n_tokens_to_generate):\n        x = params['wte'][token_ids[-hparams['n_ctx']:]]\n        pos_ids = np.arange(len(token_ids[-hparams['n_ctx']:] - 1), -1, -1)\n        pos_enc = positional_encoding(pos_ids, d_model)\n        x = x + pos_enc\n        next_token_id = np.random.choice(range(len(encoder.encoder_dict)))\n        token_ids.append(next_token_id)\n    return encoder.decode(token_ids)"}
{"task_id": 88, "completion_id": 7, "solution": "import numpy as np\nclass DummyBPE:\n\n    def __init__(self):\n        self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0, 'it': 3, 'is': 4}\n\n    def encode(self, text: str):\n        tokens = text.strip().split()\n        return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n    def decode(self, token_ids: list):\n        reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n        return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\ndef layer_norm(x, g, b, epsilon=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + epsilon) + b\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(10, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(30, 10), 'attn_dropout': 0.1, 'resid_dropout': 0.1}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10), 'resid_dropout': 0.1}} for _ in range(3)], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = params['wte'][token_ids]\n    pos_embeddings = params['wpe'][np.arange(len(token_ids))]\n    x = token_embeddings + pos_embeddings\n    for block in params['blocks']:\n        attn_out = np.dot(x, block['attn']['c_attn'])\n        attn_out = layer_norm(attn_out, block['attn']['c_proj'][0], block['attn']['c_proj'][1])\n        mlp_out = np.dot(attn_out, block['mlp']['c_fc'])\n        mlp_out = np.dot(mlp_out, block['mlp']['c_proj'])\n        mlp_out = layer_norm(mlp_out, params['ln_f']['g'], params['ln_f']['b'])\n        x = attn_out + mlp_out\n    predictions = np.dot(x, params['wte'].T)\n    predicted_token_ids = np.argmax(predictions, axis=-1)\n    generated_text = encoder.decode(predicted_token_ids[-n_tokens_to_generate:])\n    return generated_text\nprompt = 'hello world'"}
{"task_id": 88, "completion_id": 8, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(10, 10), 'attn_dropout': 0.1}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}} for _ in range(12)], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef positional_encoding(position: int, d_model: int):\n    encoding = np.zeros((position, d_model))\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            encoding[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            encoding[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return encoding\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.shape[-1]\n    scores = np.matmul(query, key.T) / np.sqrt(d_k)\n    if mask is not None:\n        scores += mask * -1000000000.0\n    p_attn = np.softmax(scores, axis=-1)\n    if dropout is not None:\n        p_attn = np.random.binomial(1, 1 - dropout, p_attn.shape) * p_attn / (1 - dropout)\n    return (np.matmul(p_attn, value), p_attn)\ndef mlp(x, c_fc, c_proj):\n    return np.matmul(np.maximum(0, np.matmul(x, c_fc)), c_proj)\ndef layer_norm(x, g, b, eps: float=1e-05):\n    mean = x.mean(-1, keepdims=True)\n    std = x.std(-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    d_model = params['wte'].shape[-1]\n    pos_encoding = positional_encoding(len(prompt) + n_tokens_to_generate, d_model)\n    input_embedding = params['wte'][token_ids]\n    position_id = len(token_ids)\n    input_embedding += pos_encoding[position_id].reshape(1, -1)\n    for _ in range(n_tokens_to_generate):\n        for block in params['blocks']:\n            (attn_output, _) = attention(input_embedding, input_embedding, input_embedding)\n            attn_output = layer_norm(input_embedding + attn_output, block['ln_1']['g'], block['ln_1']['b'])\n            mlp_output = mlp(attn_output, block['mlp']['c_fc'], block['mlp']['c_proj'])\n            input_embedding = layer_norm(attn_output + mlp_output, block['ln_2']['g'], block['ln_2']['b'])\n        generated_token = np.argmax(np.sum(input_embedding * params['ln_f']['g'][:, np.newaxis] + params['ln_f']['b'][:, np.newaxis], axis=0))\n        token_ids.append(generated_token)\n        input_embedding = np.vstack([input_embedding, params['wte'][generated_token] + pos_encoding[position_id].reshape(1, -1)])\n        position_id += 1\n    return encoder.decode(token_ids)"}
{"task_id": 88, "completion_id": 9, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'<EOS>': 0, 'hello': 1, 'world': 2, '<UNK>': 3}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(4, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\ndef attention(q, k, v, n_head):\n    qk = np.matmul(q, k.T)\n    softmax_scores = np.exp(qk) / np.sum(np.exp(qk), axis=-1, keepdims=True)\n    return np.matmul(softmax_scores, v)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    tokenized_input = encoder.encode(prompt)\n    h = np.sum([params['wte'][input_id] for input_id in tokenized_input], axis=0)\n    h += params['wpe'][0]\n    for _ in range(n_tokens_to_generate):\n        query = key = value = h\n        attn_out = attention(query[None, :], key[None, :], value[None, :], hparams['n_head'])\n        attn_out = np.sum(attn_out, axis=1)\n        h = layer_norm(attn_out, params['ln_f']['g'], params['ln_f']['b'])\n        predicted_token_id = np.argmax(h[-1])\n        tokenized_input.append(predicted_token_id)\n        h = layer_norm(attn_out + params['wte'][predicted_token_id], params['ln_f']['g'], params['ln_f']['b'])\n    return encoder.decode(tokenized_input)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    crystals = np.array(crystal_values)\n    embeddings = np.random.rand(n, dimension)\n    scores = np.dot(embeddings, embeddings.T)\n    attention_weights = softmax(scores)\n    weighted_embeddings = np.dot(attention_weights, embeddings)\n    final_pattern = [round(np.sum(row), 4) for row in weighted_embeddings]\n    return final_pattern"}
{"task_id": 89, "completion_id": 1, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum()\n    assert n > 0 and dimension > 0, 'The number of crystals and dimension must be positive numbers.'\n    assert len(crystal_values) == n, 'The number of crystal values must match the n provided.'\n    crystal_array = np.array(crystal_values).reshape(n, 1)\n    expanded_crystal_values = np.tile(crystal_array, dimension)\n    attention_scores = np.dot(expanded_crystal_values, expanded_crystal_values.T)\n    weighted_scores = np.apply_along_axis(softmax, 1, attention_scores)\n    final_patterns = np.sum(weighted_scores, axis=1)\n    return [round(pattern, 4) for pattern in final_patterns]\nn = 3\ncrystal_values = [1.0, 2.0, 3.0]\ndimension = 2"}
{"task_id": 89, "completion_id": 2, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystals = np.array(crystal_values).reshape(-1, 1) * np.ones(dimension).reshape(1, -1)\n    interaction_scores = np.dot(crystals, crystals.T)\n    enhanced_patterns = np.zeros_like(crystals)\n    for i in range(n):\n        attention_scores = softmax(interaction_scores[i])\n        enhanced_patterns[i] = np.dot(attention_scores, crystals)\n    return [round(x, 4) for x in enhanced_patterns.flatten()]\nn = 3"}
{"task_id": 89, "completion_id": 3, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    Calculates the enhanced pattern for each crystal using a simplified self-attention mechanism.\n    \n    Args:\n    - n (int): The number of crystals.\n    - crystal_values (list): A list of integers representing the values of each crystal.\n    - dimension (int): The dimension for the projection of crystal values.\n    \n    Returns:\n    - List[float]: A list of floats representing the weighted patterns for each crystal.\n    \"\"\"\n    crystals = np.array(crystal_values)\n    WQ = np.random.randn(dimension, dimension)\n    WK = np.random.randn(dimension, dimension)\n    WV = np.random.randn(dimension, dimension)\n    projected_crystals = np.repeat(crystals[:, np.newaxis], dimension, axis=1).astype(float)\n    Q = np.dot(projected_crystals, WQ)\n    K = np.dot(projected_crystals, WK)\n    V = np.dot(projected_crystals, WV)\n\n    def softmax(values):\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum(axis=0)\n    weighted_patterns = []\n    for i in range(n):\n        attention_scores = np.dot(Q[i], K.T)\n        attention_distribution = softmax(attention_scores)\n        weighted_value = np.dot(attention_distribution, V)\n        weighted_pattern = np.mean(weighted_value)\n        weighted_patterns.append(round(weighted_pattern, 4))\n    return weighted_patterns\nn = 4\ncrystal_values = [10, 20, 30, 40]\ndimension = 3"}
{"task_id": 89, "completion_id": 4, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    embeddings = np.zeros((n, dimension))\n    for i in range(n):\n        embeddings[i][crystal_values[i] - 1] = 1\n    scores = np.dot(embeddings, embeddings.T)\n    attention_scores = np.apply_along_axis(softmax, 1, scores)\n    final_patterns = np.dot(attention_scores, embeddings)\n    result = np.round(final_patterns.flatten(), 4)\n    return result.tolist()\nn = 3\ncrystal_values = [1, 2, 3]\ndimension = 3"}
{"task_id": 89, "completion_id": 5, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    crystals = np.random.rand(n, dimension)\n    scores = np.dot(crystals, crystals.T)\n    attended_patterns = []\n    for i in range(n):\n        attention_scores = softmax(scores[i])\n        final_pattern = np.sum(attention_scores * crystal_values)\n        attended_patterns.append(round(final_pattern, 4))\n    return attended_patterns\nn = 5\ncrystal_values = np.array([1, 2, 3, 4, 5])\ndimension = 3"}
{"task_id": 89, "completion_id": 6, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    if n == 0:\n        return []\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    embeddings = np.random.rand(n, dimension)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                attention_scores[i][j] = np.dot(embeddings[i], embeddings[j])\n        attention_scores[i] = softmax(attention_scores[i])\n    final_pattern = np.zeros(n)\n    for i in range(n):\n        for j in range(n):\n            final_pattern[i] += crystal_values[j] * attention_scores[i][j]\n    return [round(value, 4) for value in final_pattern]\nn = 3\ncrystal_values = [5.0, 2.0, 3.0]\ndimension = 4"}
{"task_id": 89, "completion_id": 7, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    value_matrix = np.random.randn(n, dimension) * 0.01\n    value_matrix[:, 0] = np.array(crystal_values)\n    dot_product_matrix = np.dot(value_matrix, value_matrix.T)\n    attention_scores = np.apply_along_axis(softmax, 1, dot_product_matrix)\n    weighted_values = np.dot(attention_scores, value_matrix)\n    output = weighted_values[:, 0].tolist()\n    output = [round(value, 4) for value in output]\n    return output\nn = 5\ncrystal_values = [1, 2, 3, 4, 5]\ndimension = 128"}
{"task_id": 89, "completion_id": 8, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n    \"\"\"\n    Implements a simplified self-attention mechanism to derive the final weighted pattern for each crystal.\n    \n    Args:\n        n (int): The number of crystals.\n        crystal_values (list): A list of floats representing the values of the crystals.\n        dimension (int): The dimension to use for the query and key vectors generation.\n        \n    Returns:\n        list: A list of floats representing the final weighted pattern for each crystal, rounded to the 4th decimal place.\n    \"\"\"\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    crystal_values_array = np.array(crystal_values).reshape(-1, 1)\n    weight_matrix = np.random.rand(dimension, 1)\n    queries = np.dot(crystal_values_array, weight_matrix.T)\n    keys = queries\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(queries[i], keys[j].T)\n    attention_scores = softmax(attention_scores)\n    values = np.dot(attention_scores, crystal_values_array)\n    final_patterns = [round(val[0], 4) for val in values]\n    return final_patterns\nn = 3\ndimension = 4"}
{"task_id": 89, "completion_id": 9, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n\n    def self_attention(q, k, v):\n        scores = np.dot(q, k.T) / np.sqrt(dimension)\n        weights = softmax(scores)\n        weighted_sum = np.dot(weights, v)\n        return weighted_sum\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n    Q = K = V = crystal_values\n    output = []\n    for i in range(n):\n        weighted_pattern = self_attention(Q[i:i + 1], K, V)\n        output.append(weighted_pattern.flatten())\n    output = np.concatenate(output).tolist()\n    return [round(x, 4) for x in output]\nn = 3\ncrystal_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for a given corpus and query.\n    \n    :param corpus: A list of documents (strings), each document is a string of space-separated words.\n    :param query: A string of space-separated terms that form the query.\n    :param k1: The k1 parameter controls the saturation of the term frequency.\n    :param b: The b parameter controls the effect of document length.\n    :return: A list of BM25 scores for each document in the corpus with respect to the query.\n    \"\"\"\n    corpus = [doc.lower().split() for doc in corpus]\n    query = query.lower().split()\n    doc_freqs = [Counter(doc) for doc in corpus]\n    avgdl = np.average([len(doc) for doc in corpus])\n    N = len(corpus)\n\n    def score_document(doc_idx):\n        score = 0.0\n        doc = corpus[doc_idx]\n        for term in set(query):\n            if term in doc:\n                idf = np.log((N - doc_freqs.count(term) + 0.5) / (doc_freqs.count(term) + 0.5))\n                tf = doc_freqs[doc_idx][term]\n                dl = len(doc)\n                numerator = idf * tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * dl / avgdl)\n                score += numerator / denominator\n        return score\n    scores = [score_document(i) for i in range(N)]\n    return [round(score, 3) for score in scores]\ncorpus = ['the sky is blue', 'the sun is bright', 'the sun in the sky is bright', 'we can see the shining sun', 'the quick brown fox jumps over the lazy dog']\nquery = 'the sun shining'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a list of documents given a query.\n    \n    :param corpus: A list of strings, each representing a document.\n    :param query: A string representing the query.\n    :param k1: Term frequency saturation parameter. Default is 1.5.\n    :param b: Document length normalization parameter. Default is 0.75.\n    :return: A list of BM25 scores for each document in the corpus.\n    \"\"\"\n    query_terms = query.split()\n    doc_terms = [doc.split() for doc in corpus]\n    term_freqs = [Counter(doc) for doc in doc_terms]\n    doc_counts = Counter((term for doc in doc_terms for term in set(doc)))\n    doc_lens = np.array([len(doc) for doc in doc_terms])\n    avg_len = doc_lens.mean()\n    scores = []\n    N = len(doc_terms)\n    for i in range(N):\n        score = 0\n        for term in query_terms:\n            if term in doc_terms[i]:\n                tf = term_freqs[i][term]\n                idf = np.log((N - doc_counts[term] + 0.5) / (doc_counts[term] + 0.5))\n                score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_lens[i] / avg_len))\n        scores.append(score)\n    return [round(score, 3) for score in scores]\ncorpus = ['the sky is blue', 'the sun is bright', 'the sun in the sky is bright', 'we can see the shining sun, the bright sun', 'the sun is very bright today']\nquery = 'sun bright'\nk1 = 1.5\nb = 0.75\nscores = calculate_bm25_scores(corpus, query, k1=1.5, b=0.75)"}
{"task_id": 90, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the given corpus considering a query.\n    :param corpus: List of documents as lists of words.\n    :param query: List of words in the query.\n    :param k1: Free smoothing parameter k1.\n    :param b: Free smoothing parameter b.\n    :return: A list of floats representing the BM25 score for each document.\n    \"\"\"\n    N = len(corpus)\n    avgdl = sum((len(c) for c in corpus)) / N\n    query_freq = Counter(query)\n    doc_freqs = {term: sum((term in c for c in corpus)) for term in query_freq}\n    idf = {term: np.log((N - df + 0.5) / (df + 0.5)) for (term, df) in doc_freqs.items()}\n    doc_scores = []\n    for doc in corpus:\n        dl = len(doc)\n        doc_freq = Counter(doc)\n        score = 0.0\n        for term in query_freq:\n            if term in doc_freq:\n                tf = doc_freq[term]\n                numerator = idf[term] * tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * dl / avgdl)\n                score += numerator / denominator\n        doc_scores.append(score)\n    return [round(score, 3) for score in doc_scores]\ncorpus = ['the cat in the hat'.split(), 'a cat a hat'.split(), 'the big red fox'.split()]\nquery = 'cat hat'.split()"}
{"task_id": 90, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates the BM25 scores for documents in a corpus given a query.\n    \n    :param corpus: A list of documents where each document is a list of tokens.\n    :param query: A list of query tokens.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: A list of BM25 scores, one for each document in the corpus.\n    \"\"\"\n    avgdl = sum([len(doc) for doc in corpus]) / len(corpus)\n    doc_scores = []\n    doc_freq = Counter()\n    for doc in corpus:\n        doc_freq.update(set(doc))\n    for doc in corpus:\n        score = 0.0\n        for term in query:\n            tf = doc.count(term)\n            idf = np.log((len(corpus) - doc_freq[term] + 0.5) / (doc_freq[term] + 0.5))\n            score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (len(doc) / avgdl)))\n        doc_scores.append(score)\n    return [round(score, 3) for score in doc_scores]\ncorpus = [['data', 'science', 'and', 'data', 'engineering'], ['data', 'science', 'is', 'interesting'], ['data', 'engineering', 'is', 'crucial', 'to', 'science']]\nquery = ['data', 'science']"}
{"task_id": 90, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    avgdl = sum(map(len, corpus)) / len(corpus)\n    N = len(corpus)\n    D = ' '.join(corpus)\n    query_terms = set(query)\n    doc_scores = []\n    for doc in corpus:\n        doc_score = 0\n        for term in query_terms:\n            tf = doc.count(term)\n            dl = len(doc)\n            Nt = D.count(term)\n            idf = np.log((N - Nt + 0.5) / (Nt + 0.5))\n            doc_score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avgdl))\n        doc_scores.append(round(doc_score, 3))\n    return doc_scores\ncorpus = ['the sky is blue', 'spring is beautiful', 'the sun is bright']\nquery = 'the sun'"}
{"task_id": 90, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for documents in a corpus given a query.\n    \n    :param corpus: A list of documents where each document is represented as a list of words.\n    :param query: A list of query terms (words).\n    :param k1: Term frequency saturation constant.\n    :param b: Document length normalization constant.\n    :return: A list of BM25 scores for each document in the corpus.\n    \"\"\"\n    query_set = set(query)\n    N = len(corpus)\n    idf_vals = {}\n    for term in query_set:\n        df = sum((1 for doc in corpus if term in doc))\n        idf_vals[term] = np.log((N - df + 0.5) / (df + 0.5) + 1)\n    avgdl = sum((len(doc) for doc in corpus)) / N\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query_set:\n            if term in doc:\n                tf = doc.count(term)\n                idf = idf_vals[term]\n                doc_len = len(doc)\n                score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / avgdl))\n        scores.append(np.round(score, 3))\n    return scores\ncorpus = [['this', 'is', 'the', 'first', 'document'], ['this', 'document', 'is', 'the', 'second', 'document'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'first', 'document']]\nquery = ['this', 'document', 'first']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in a corpus given a query.\n    \n    Parameters:\n    corpus (list): List of lists, where each sublist represents a document as a list of tokens.\n    query (list): List of query tokens.\n    k1 (float): BM25 k1 parameter for term frequency saturation effect.\n    b (float): BM25 b parameter for document length normalization effect.\n    \n    Returns:\n    list: BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n\n    def tf(document):\n        return Counter(document)\n    avg_doc_length = sum((len(doc) for doc in corpus)) / len(corpus)\n    document_lengths = np.array([len(doc) for doc in corpus])\n    N = len(corpus)\n    idfs = {}\n    for term in query:\n        df = sum((term in doc for doc in corpus))\n        idf = np.log((N - df + 0.5) / (df + 0.5))\n        idfs[term] = idf\n    scores = []\n    for doc in corpus:\n        score = 0\n        doc_tf = tf(doc)\n        for term in query:\n            if term in doc:\n                term_score = idfs[term] * (doc_tf[term] * (k1 + 1)) / (doc_tf[term] + k1 * (1 - b + b * len(doc) / avg_doc_length))\n                score += term_score\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['this', 'document', 'contains', 'specific', 'words'], ['this', 'other', 'document', 'has', 'other', 'words'], ['just', 'some', 'random', 'text', 'in', 'the', 'document']]\nquery = ['document', 'random']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculates BM25 scores for a given corpus and query.\n    \n    :param corpus: A list of strings, each string representing a document in the corpus.\n    :param query: A string representing the query for which the BM25 scores are to be calculated.\n    :param k1: The k1 parameter used in the BM25 formula, defaulting to 1.5.\n    :param b: The b parameter used in the BM25 formula, defaulting to 0.75.\n    :return: A list of BM25 scores, one for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    corpus_words = [doc.lower().split() for doc in corpus]\n    query_words = query.lower().split()\n    term_freqs = [Counter(doc) for doc in corpus_words]\n    term_doc_freq = Counter((word for doc in corpus_words for word in set(doc)))\n    n = len(corpus)\n    doc_lens = [len(doc) for doc in corpus_words]\n    avg_doc_len = np.mean(doc_lens)\n    doc_scores = []\n    for (i, doc) in enumerate(corpus_words):\n        score = 0\n        for term in query_words:\n            tf = term_freqs[i][term]\n            idf = np.log(1 + (n - term_doc_freq[term] + 0.5) / (term_doc_freq[term] + 0.5))\n            score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_lens[i] / avg_doc_len)))\n        doc_scores.append(round(score, 3))\n    return doc_scores"}
{"task_id": 90, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a corpus of documents against a given query.\n    \n    Parameters:\n    corpus (list of str): List of documents.\n    query (str): Query to search against the corpus.\n    k1 (float): Parameter for term frequency saturation, default 1.5.\n    b (float): Parameter for document length normalization, default 0.75.\n    \n    Returns:\n    list: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    query_words = set(query.split())\n    avg_doc_len = sum((len(d.split()) for d in corpus)) / len(corpus)\n    scores = []\n    for doc in corpus:\n        words = doc.split()\n        doc_len = len(words)\n        score = 0.0\n        for word in query_words:\n            if word in words:\n                tf = words.count(word)\n                idf = np.log(1 + (len(corpus) - sum((1 for d in corpus if word in d.split())) + 0.5) / (sum((1 for d in corpus if word in d.split())) + 0.5))\n                term_freq_part = tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_len / avg_doc_len))\n                score += idf * term_freq_part\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\nquery = 'first second document'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the given corpus based on the provided query.\n\n    Parameters:\n    corpus (list of str): The corpus of documents as a list of strings.\n    query (str): The search query as a single string.\n    k1 (float): The k1 parameter of BM25 formula, which controls term frequency saturation.\n    b (float): The b parameter of BM25 formula, which controls the document length normalization.\n\n    Returns:\n    list: A list of BM25 scores for each document in the corpus, rounded to three decimal places.\n    \"\"\"\n    avgdl = sum((len(doc.split()) for doc in corpus)) / len(corpus)\n    query_terms = query.split()\n    unique_terms = list(set(query_terms))\n    df = Counter((term for doc in corpus for term in set(doc.split())))\n    scores = np.zeros(len(corpus))\n    for (i, doc) in enumerate(corpus):\n        doc_length = len(doc)\n        for term in unique_terms:\n            tf = doc.split().count(term)\n            idf = np.log((len(corpus) - df[term] + 0.5) / (df[term] + 0.5))\n            scores[i] += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_length / avgdl))\n    return [round(score, 3) for score in scores]\ncorpus = ['the brown fox jumps over the lazy dog', 'quick brown fox jumps over the lazy dog', 'the quick brown dog jumps over the lazy fox']\nquery = 'fox dog'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (t, p) in zip(y_true, y_pred):\n        if t == 1 and p == 1:\n            tp += 1\n        elif t == 0 and p == 1:\n            fp += 1\n        elif t == 1 and p == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\ny_pred = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"}
{"task_id": 91, "completion_id": 1, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    true_set = set(y_true)\n    pred_set = set(y_pred)\n    tp = len(true_set.intersection(pred_set))\n    fp = len(pred_set.difference(true_set))\n    fn = len(true_set.difference(pred_set))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 91, "completion_id": 2, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((1 for i in range(len(y_true)) if y_true[i] == y_pred[i] == 1))\n    fp = sum((1 for i in range(len(y_true)) if y_pred[i] == 1 and y_true[i] != y_pred[i]))\n    fn = sum((1 for i in range(len(y_true)) if y_true[i] == 1 and y_true[i] != y_pred[i]))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 1, 0, 1, 0, 1, 0, 0, 0, 0]\ny_pred = [1, 1, 0, 0, 0, 1, 0, 1, 1, 0]"}
{"task_id": 91, "completion_id": 3, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = fp = fn = 0\n    for (t, p) in zip(y_true, y_pred):\n        if t == 1 and p == 1:\n            tp += 1\n        elif t == 0 and p == 1:\n            fp += 1\n        elif t == 1 and p == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 4, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Both y_true and y_pred must be non-empty lists.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have the same length.')\n    TP = 0\n    FP = 0\n    FN = 0\n    for i in range(len(y_true)):\n        if y_true[i] == 1:\n            if y_pred[i] == 1:\n                TP += 1\n            else:\n                FN += 1\n        elif y_pred[i] == 1:\n            FP += 1\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 5, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = fp = fn = 0\n    for (yt, yp) in zip(y_true, y_pred):\n        if yt == 1:\n            if yp == 1:\n                tp += 1\n            else:\n                fn += 1\n        elif yp == 1:\n            fp += 1\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 0, 1, 0, 1, 1, 0, 1]"}
{"task_id": 91, "completion_id": 6, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of true and predicted labels must be the same.')\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == pred:\n            if true == 1:\n                tp += 1\n        elif true == 1:\n            fn += 1\n        else:\n            fp += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall > 0:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1_score = 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 7, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum([1 for i in range(len(y_true)) if y_true[i] == y_pred[i] and y_pred[i] == 1])\n    fp = sum([1 for i in range(len(y_true)) if y_true[i] != y_pred[i] and y_pred[i] == 1])\n    fn = sum([1 for i in range(len(y_true)) if y_true[i] != y_pred[i] and y_true[i] == 1])\n    try:\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = 2 * precision * recall / (precision + recall)\n    except ZeroDivisionError:\n        return 0.0\n    return round(f1, 3)\ny_true = [1, 0, 1, 0, 0]\ny_pred = [0, 0, 0, 0, 0]"}
{"task_id": 91, "completion_id": 8, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for i in range(len(y_true)):\n        if y_true[i] == 1 and y_pred[i] == 1:\n            tp += 1\n        elif y_true[i] == 0 and y_pred[i] == 1:\n            fp += 1\n        elif y_true[i] == 1 and y_pred[i] == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 91, "completion_id": 9, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    from sklearn.metrics import f1_score\n    score = f1_score(y_true, y_pred)\n    return round(score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    days = np.array(range(1, 11)).reshape(-1, 1)\n    PI = 3.14159\n    fluctuation = 10 * np.sin(2 * PI * days.flatten() / 10)\n    detrended_data = consumption_data - fluctuation\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_prediction = model.predict(day_15)\n    day_15_fluctuation = 10 * np.sin(2 * PI * 15 / 10)\n    final_prediction = base_prediction + day_15_fluctuation\n    final_prediction_rounded = math.ceil(final_prediction[0])\n    final_prediction_with_margin = math.ceil(final_prediction_rounded * 1.05)\n    return final_prediction_with_margin\nconsumption_data = np.array([250, 265, 300, 315, 350, 365, 400, 415, 450, 465])"}
{"task_id": 92, "completion_id": 1, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.arange(1, 11)\n    fluctuation = 10 * np.sin(2 * PI * days / 10)\n    detrended_consumption = np.array(consumption_data) - fluctuation\n    days = days.reshape((-1, 1))\n    model = LinearRegression()\n    model.fit(days, detrended_consumption)\n    day_15 = np.array([[15]])\n    base_day_15_prediction = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_prediction = base_day_15_prediction + fluctuation_day_15\n    day_15_prediction_rounded = math.ceil(day_15_prediction)\n    day_15_prediction_with_margin = math.ceil(day_15_prediction_rounded * 1.05)\n    return day_15_prediction_with_margin"}
{"task_id": 92, "completion_id": 2, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    This function takes in a list of power consumption data for 10 days,\n    removes the daily fluctuation, performs linear regression on the detrended data,\n    predicts day 15's base consumption, adds back the fluctuation for day 15,\n    and finally adds 5% safety margin.\n    \n    :param consumption_data: List of floats representing daily power consumption.\n    :return: An integer representing predicted consumption for day 15 with safety margin.\n    \"\"\"\n    days = np.array(list(range(1, 11))).reshape(-1, 1)\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = np.array(consumption_data) - np.array(fluctuation)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_day_15 = model.predict(day_15)\n    day_15_fluct = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_value = base_day_15 + day_15_fluct\n    final_predicted = math.ceil(predicted_value * 1.05)\n    return final_predicted[0]\nconsumption_data = [102.3, 101.2, 104.5, 105.6, 109.1, 114.3, 118.1, 122, 125.3, 129]"}
{"task_id": 92, "completion_id": 3, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Forecast the power consumption for day 15 with a 5% safety margin.\n    \n    :param consumption_data: A list of floats representing daily power consumption for 10 days.\n    :return: An integer representing the power consumption on day 15 with a 5% safety margin.\n    \"\"\"\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * i / 10) for i in range(len(consumption_data))]\n    X = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15 = np.array([15]).reshape(-1, 1)\n    prediction_base = model.predict(day_15)[0]\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    prediction_with_fluctuation = prediction_base + day_15_fluctuation\n    final_prediction = int(np.ceil(prediction_with_fluctuation * 1.05))\n    return final_prediction\nconsumption_data = [50 + 10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]"}
{"task_id": 92, "completion_id": 4, "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    days = np.array([i + 1 for i in range(len(consumption_data))]).reshape((-1, 1))\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([15]).reshape((-1, 1))\n    base_consumption_day_15 = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    consumption_day_15_rounded = math.ceil(consumption_day_15)\n    consumption_day_15_safety = math.ceil(consumption_day_15_rounded * 1.05)\n    return consumption_day_15_safety\nconsumption_data = [120, 115, 130, 150, 170, 180, 190, 200, 210, 215]"}
{"task_id": 92, "completion_id": 5, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nimport numpy.linalg as la\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    fluctuations = [10 * math.sin(2 * PI * i / 10) for i in days]\n    detrended_data = [consumption_data[i] - fluctuations[i] for i in range(len(consumption_data))]\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([15]).reshape(-1, 1)\n    predicted_base = model.predict(day_15)\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_daily_consumption = predicted_base + fluctuation_day_15\n    final_prediction = math.ceil(predicted_daily_consumption[0] * 1.05)\n    return final_prediction"}
{"task_id": 92, "completion_id": 6, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = [consumption_data[i] - fluctuation[i] for i in range(10)]\n    X = np.array(range(1, 11)).reshape((-1, 1))\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15 = np.array([[15]])\n    base_day_15 = model.predict(day_15)\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_day_15 = base_day_15 + fluctuation_day_15\n    rounded_prediction = math.ceil(predicted_day_15[0] * 1.05)\n    return rounded_prediction\nconsumption_data = [102, 115, 130, 132, 150, 157, 175, 176, 188, 205]"}
{"task_id": 92, "completion_id": 7, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Adjusts the given daily power consumption dataset for a sinusoidal fluctuation,\n    fits a linear regression model to the adjusted data, predicts the 15th day's\n    consumption, and adds a 5% safety margin to the result.\n    \"\"\"\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    daily_fluctuation = np.array([10 * math.sin(2 * PI * i / 10) for i in range(1, len(consumption_data) + 1)])\n    detrended_data = np.array(consumption_data) - daily_fluctuation\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    future_day = np.array([15]).reshape(-1, 1)\n    base_consumption_day_15 = model.predict(future_day)\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    safety_margin = 0.05\n    final_consumption = math.ceil(predicted_consumption_day_15 * (1 + safety_margin))\n    return final_consumption[0]"}
{"task_id": 92, "completion_id": 8, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = np.array(range(1, 11)).reshape((-1, 1))\n    fluctuation = [10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]\n    detrended_data = np.array(consumption_data) - np.array(fluctuation)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    consumption_with_safety_margin = math.ceil(predicted_consumption_day_15 * 1.05)\n    return consumption_with_safety_margin\nconsumption_data = [104, 120, 142, 170, 200, 232, 264, 290, 320, 350]"}
{"task_id": 92, "completion_id": 9, "solution": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    fluctuations = [10 * math.sin(2 * math.pi * i / 10) for i in range(1, 11)]\n    detrended_data = [consumption_data[i] - fluctuations[i] for i in range(len(consumption_data))]\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array(15).reshape(-1, 1)\n    base_consumption = model.predict(day_15)\n    fluctuation_day_15 = 10 * math.sin(2 * math.pi * 15 / 10)\n    predicted_consumption = base_consumption + fluctuation_day_15\n    predicted_consumption_rounded = round(predicted_consumption[0])\n    final_consumption = int(np.ceil(predicted_consumption_rounded * 1.05))\n    return final_consumption\nconsumption_data = [102, 105, 108, 110, 112, 114, 116, 118, 120, 122]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_abs_error = np.mean(absolute_errors)\n    return np.round(mean_abs_error, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 93, "completion_id": 1, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 2, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n    \n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n    \n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 3, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of actual and predicted values arrays must be equal.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 93, "completion_id": 4, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 93, "completion_id": 5, "solution": "import numpy as np"}
{"task_id": 93, "completion_id": 6, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy.ndarray instances')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(errors)\n    return round(mae_value, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 93, "completion_id": 7, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred should be the same.')\n    mae = np.mean(np.abs(y_true - y_pred))\n    return round(mae, 3)"}
{"task_id": 93, "completion_id": 8, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mae = np.mean(absolute_errors)\n    return round(mae, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 93, "completion_id": 9, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred should have the same length')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matricies from input X and weight matricies.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs self-attention operation.\n    The function computes the dot product between Q and K, applies softmax,\n    then multiplies the resulting attention weights by V.\n    \"\"\"\n    sqrt_dk = np.sqrt(K.shape[-1])\n    attention_scores = np.matmul(Q, K.T) / sqrt_dk\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    return np.matmul(attention_weights, V)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements multi-head attention.\n    \"\"\"\n    (batch_size, seq_length, d_model) = X.shape\n    assert d_model % n_heads == 0, 'Model dimensions must be divisible by the number of heads'\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = np.reshape(Q, (batch_size, seq_length, n_heads, d_model // n_heads))\n    K_heads = np.reshape(K, (batch_size, seq_length, n_heads, d_model // n_heads))\n    V_heads = np.reshape(V, (batch_size, seq_length, n_heads, d_model // n_heads))\n    attention_heads = []\n    for i in range(n_heads):\n        Q_head = Q_heads[:, :, i, :]\n        K_head = K_heads[:, :, i, :]\n        V_head = V_heads[:, :, i, :]\n        attention_heads.append(self_attention(Q_head, K_head, V_head))\n    combined_attention = np.concatenate(attention_heads, axis=-1)\n    output = np.reshape(combined_attention, (batch_size, seq_length, -1))\n    return np.round(output, 4).tolist()\nX = np.random.randn(2, 3, 10)\nW_q = np.random.randn(10, 10)\nW_k = np.random.randn(10, 10)\nW_v = np.random.randn(10, 10)\nn_heads = 2"}
{"task_id": 94, "completion_id": 1, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices given the input sequence and weight matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes self-attention for the given Query, Key, and Value matrices.\n    \"\"\"\n    d_k = K.shape[-1]\n    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), axis=-1, arr=attention_scores)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n    X: Input sequence tensor of shape (batch_size, seq_length, embed_dim).\n    W_q, W_k, W_v: Weight matrices for the query, key, and value projections.\n    n_heads: Number of attention heads.\n    Returns the multi-head attention output as a reshaped list.\n    \"\"\"\n    (batch_size, seq_length, embed_dim) = X.shape\n    head_dim = embed_dim // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = np.split(Q, n_heads, -1)\n    K_heads = np.split(K, n_heads, -1)\n    V_heads = np.split(V, n_heads, -1)\n    attention_heads = [self_attention(Q, K, V) for (Q, K, V) in zip(Q_heads, K_heads, V_heads)]\n    concatenated_heads = np.concatenate(attention_heads, axis=-1)\n    concatenated_heads = np.round(concatenated_heads, decimals=4)\n    return concatenated_heads.tolist()\nX = np.random.rand(2, 3, 5)\nW_q = np.random.rand(5, 5)\nW_k = np.random.rand(5, 5)\nW_v = np.random.rand(5, 5)\nn_heads = 5"}
{"task_id": 94, "completion_id": 2, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = K.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    scores = np.exp(scores)\n    scores = scores / np.sum(scores, axis=-1, keepdims=True)\n    output = scores @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    d_model = W_q.shape[1]\n    W_q_heads = np.split(W_q, n_heads, axis=1)\n    W_k_heads = np.split(W_k, n_heads, axis=1)\n    W_v_heads = np.split(W_v, n_heads, axis=1)\n    heads_output = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        head_output = self_attention(Q, K, V)\n        heads_output.append(head_output)\n    concatenated_output = np.concatenate(heads_output, axis=-1)\n    concatenated_output = np.round(concatenated_output, decimals=4)\n    return concatenated_output.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nW_q = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6], [0.7, 0.8, 0.9]])\nW_v = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6], [0.7, 0.8, 0.9]])\nn_heads = 2\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 94, "completion_id": 3, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices for the self-attention mechanism.\n    \n    Args:\n    X: The input matrix of shape (batch_size, seq_length, d_model)\n    W_q: The weight matrix for the query of shape (d_model, d_model)\n    W_k: The weight matrix for the key of shape (d_model, d_model)\n    W_v: The weight matrix for the value of shape (d_model, d_model)\n    \n    Returns:\n    A tuple containing the query, key, value matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Applies the self-attention mechanism to the query, key, and value matrices.\n    \n    Args:\n    Q: Query matrix\n    K: Key matrix\n    V: Value matrix\n    \n    Returns:\n    Output of self-attention\n    \"\"\"\n    d_k = Q.shape[-1]\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(), axis=1, arr=attention_scores)\n    output = np.matmul(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n    \n    Args:\n    X: Input matrix\n    W_q: Query weight matrix\n    W_k: Key weight matrix\n    W_v: Value weight matrix\n    n_heads: Number of heads in the multi-head attention\n    \n    Returns:\n    List of output matrices after multi-head attention\n    \"\"\"\n    (batch_size, seq_length, _) = X.shape\n    d_model = W_q.shape[0]\n    d_k = d_model // n_heads\n    heads_output = []\n    for i in range(n_heads):\n        Q_i = X[:, :, i * d_k:(i + 1) * d_k]\n        K_i = X[:, :, i * d_k:(i + 1) * d_k]\n        V_i = X[:, :, i * d_k:(i + 1) * d_k]\n        (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n        output = self_attention(Q_i, K_i, V_i)\n        heads_output.append(output)\n    concatenated = np.concatenate(heads_output, axis=-1)\n    return np.round(concatenated, 4).tolist()\nX = np.random.randn(2, 8, 512)\nW_q = np.random.randn(512, 512)\nW_k = np.random.randn(512, 512)\nW_v = np.random.randn(512, 512)\nn_heads = 8\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 94, "completion_id": 4, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Calculate Query, Key, and Value matrices for multi-head attention.\n\n    :param X: Input data of shape (n_samples, input_dim)\n    :param W_q: Weight matrix for queries of shape (input_dim, num_heads * head_dim)\n    :param W_k: Weight matrix for keys of shape (input_dim, num_heads * head_dim)\n    :param W_v: Weight matrix for values of shape (input_dim, num_heads * head_dim)\n    :return: tuple (Q, K, V) where each is of shape (n_samples, num_heads * head_dim)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int, head_dim: int) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention for one head.\n\n    :param Q: Query matrix of shape (n_samples, head_dim)\n    :param K: Key matrix of shape (n_samples, head_dim)\n    :param V: Value matrix of shape (n_samples, head_dim)\n    :return: Attended values of shape (n_samples, head_dim)\n    \"\"\"\n    Q = Q.reshape(-1, n_heads, head_dim)\n    K = K.reshape(-1, n_heads, head_dim)\n    V = V.reshape(-1, n_heads, head_dim)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(head_dim)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=2, keepdims=True)\n    attended_values = np.dot(attention_weights, V)\n    return attended_values.reshape(-1, n_heads * head_dim)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head self-attention.\n\n    :param X: Input data of shape (n_samples, input_dim)\n    :param W_q: Query weights of shape (input_dim, num_heads * head_dim)\n    :param W_k: Key weights of shape (input_dim, num_heads * head_dim)\n    :param W_v: Value weights of shape (input_dim, num_heads * head_dim)\n    :param n_heads: Number of attention heads\n    :return: Result of multi-head attention as a python list\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    (_, total_head_dim) = Q.shape\n    head_dim = total_head_dim // n_heads\n    attended_values = self_attention(Q, K, V, n_heads, head_dim)\n    attended_values = np.round(attended_values, 4)\n    return attended_values.tolist()\nX = np.random.rand(n_samples, input_dim)\nW_q = np.random.rand(input_dim, n_heads * head_dim)\nW_k = np.random.rand(input_dim, n_heads * head_dim)\nW_v = np.random.rand(input_dim, n_heads * head_dim)"}
{"task_id": 94, "completion_id": 5, "solution": "import numpy as np\ndef split_heads(X: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Split the last dimension of the input tensor into (n_heads, dim_per_head) and transpose the result.\n    \"\"\"\n    (batch_size, seq_len, d_model) = X.shape\n    d_head = d_model // n_heads\n    return X.reshape(batch_size, seq_len, n_heads, d_head).transpose(0, 2, 1, 3)\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Queries, Keys, and Values from the input X and corresponding weight matrices.\n    \"\"\"\n    queries = X @ W_q\n    keys = X @ W_k\n    values = X @ W_v\n    return (queries, keys, values)\ndef scaled_dot_product_attention(queries: np.ndarray, keys: np.ndarray, values: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implement scaled dot-product attention for one head.\n    \"\"\"\n    d_k = keys.shape[-1]\n    attention_scores = np.matmul(queries, keys.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda m: np.exp(m) / np.sum(np.exp(m)), axis=-1, arr=attention_scores)\n    output = np.matmul(attention_weights, values)\n    return output\ndef self_attention(queries: np.ndarray, keys: np.ndarray, values: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Implement self-attention for multiple heads and concatenate the output.\n    \"\"\"\n    (batch_size, seq_len, d_model) = queries.shape\n    d_head = d_model // n_heads\n    queries_heads = split_heads(queries, n_heads)\n    keys_heads = split_heads(keys, n_heads)\n    values_heads = split_heads(values, n_heads)\n    attention_heads = [scaled_dot_product_attention(q, k, v) for (q, k, v) in zip(queries_heads, keys_heads, values_heads)]\n    concatenated = np.concatenate([head.transpose(0, 2, 1, 3) for head in attention_heads], axis=-1)\n    return concatenated.reshape(batch_size, seq_len, d_model)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement the multi-head attention mechanism with rounding to 4 decimals and return as a python list.\n\n    Parameters:\n    X (numpy.ndarray): Input array, expecting shape (batch_size, seq_len, d_model).\n    W_q (numpy.ndarray): Query weight matrix, expecting shape (d_model, d_model).\n    W_k (numpy.ndarray): Key weight matrix, expecting shape (d_model, d_model).\n    W_v (numpy.ndarray): Value weight matrix, expecting shape (d_model, d_model).\n    n_heads (int): Number of heads for the multi-head attention.\n\n    Returns:\n    list: The result of the multi-head attention mechanism as a python list with elements rounded to 4 decimal places.\n    \"\"\"\n    (queries, keys, values) = compute_qkv(X, W_q, W_k, W_v)\n    attention_out = self_attention(queries, keys, values, n_heads)\n    rounded_output = np.round(attention_out, 4)\n    return rounded_output.tolist()\nX = np.random.rand(batch_size, seq_len, d_model)\nW_q = np.random.rand(d_model, d_model)\nW_k = np.random.rand(d_model, d_model)\nW_v = np.random.rand(d_model, d_model)\nn_heads = 2"}
{"task_id": 94, "completion_id": 6, "solution": "import numpy as np\ndef compute_qkv(X, W_q, W_k, W_v, n_heads):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    head_dim = Q.shape[1] // n_heads\n    Q = np.array_split(Q, n_heads, axis=1)\n    K = np.array_split(K, n_heads, axis=1)\n    V = np.array_split(V, n_heads, axis=1)\n    return (Q, K, V, head_dim)\ndef scaled_dot_product_attention(Q, K, V, head_dim):\n    attention_scores = np.dot(Q, K.T) / np.sqrt(head_dim)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    context = np.dot(attention_weights, V)\n    return context\ndef self_attention(Q, K, V, head_dim):\n    return [scaled_dot_product_attention(q, k, v, head_dim) for (q, k, v) in zip(Q, K, V)]\ndef multi_head_attention(X, W_q, W_k, W_v, n_heads):\n    (Q, K, V, head_dim) = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    self_attention_outs = self_attention(Q, K, V, head_dim)\n    concatenated_output = np.concatenate(self_attention_outs, axis=1)\n    rounded_output = np.round(concatenated_output, 4)\n    return rounded_output.tolist()\nX = np.array([[1.0, 2.0], [3.0, 4.0]])\nW_q = np.array([[1.0, 0.0], [0.0, 1.0]])\nW_k = np.array([[1.0, 0.0], [0.0, 1.0]])\nW_v = np.array([[1.0, 0.0], [0.0, 1.0]])\nn_heads = 2"}
{"task_id": 94, "completion_id": 7, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the queries (Q), keys (K), and values (V) using the input data and weight matrices.\n    \n    :param X: The input data tensor of shape (batch_size, seq_len, embedding_dim)\n    :param W_q: The query weight matrix of shape (embedding_dim, d_q)\n    :param W_k: The key weight matrix of shape (embedding_dim, d_k)\n    :param W_v: The value weight matrix of shape (embedding_dim, d_v)\n    :return: (Q, K, V)\n    \"\"\"\n    q = X.dot(W_q)\n    k = X.dot(W_k)\n    v = X.dot(W_v)\n    return (q, k, v)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implement the multi-head attention mechanism.\n    \n    :param X: Input tensor of shape (batch_size, seq_len, embedding_dim)\n    :param W_q: Query weight matrix of shape (embedding_dim, n_heads * d_q)\n    :param W_k: Key weight matrix of shape (embedding_dim, n_heads * d_k)\n    :param W_v: Value weight matrix of shape (embedding_dim, n_heads * d_v)\n    :param n_heads: Number of attention heads\n    :return: Output tensor of shape (batch_size, seq_len, embedding_dim) as a list\n    \"\"\"\n    (q, k, v) = compute_qkv(X, W_q, W_k, W_v)\n    (batch_size, seq_len, _) = X.shape\n    (d_q, d_k, d_v) = (W_q.shape[1] // n_heads, W_k.shape[1] // n_heads, W_v.shape[1] // n_heads)\n    q = q.reshape(batch_size, seq_len, n_heads, d_q).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_heads, d_v).transpose(0, 2, 1, 3)\n    attn_out = self_attention(q, k, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n    attn_out = np.round(attn_out, 4)\n    return attn_out.tolist()\nd_q = d_k = d_v = 4\nn_heads = 2\nX = np.random.rand(batch_size, seq_len, embedding_dim)\nW_q = np.random.rand(embedding_dim, n_heads * d_q)\nW_k = np.random.rand(embedding_dim, n_heads * d_k)\nW_v = np.random.rand(embedding_dim, n_heads * d_v)"}
{"task_id": 94, "completion_id": 8, "solution": "import numpy as np\ndef split_heads(x: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"Split the last dimension into (num_heads, depth).\n    \n    Transpose the result such that the shape is (batch_size, num_heads, seq_length, depth)\n    \"\"\"\n    (batch_size, seq_length, depth) = x.shape\n    assert depth % n_heads == 0\n    depth_per_head = depth // n_heads\n    return x.reshape(batch_size, seq_length, n_heads, depth_per_head).transpose(0, 2, 1, 3)\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"Compute the Query, Key, and Value matrices for given inputs.\"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the scaled dot product attention mechanism.\"\"\"\n    matmul_qk = np.matmul(Q, K.transpose(0, 1, 3, 2))\n    dk = K.shape[-1]\n    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n    attention_weights = np.exp(scaled_attention_logits) / np.sum(np.exp(scaled_attention_logits), axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Implement multi-head self-attention.\"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = split_heads(Q, n_heads)\n    K_heads = split_heads(K, n_heads)\n    V_heads = split_heads(V, n_heads)\n    attention_output = scaled_dot_product_attention(Q_heads, K_heads, V_heads)\n    depth = Q.shape[-1]\n    attention_output = attention_output.transpose(0, 2, 1, 3).reshape(-1, n_heads, depth)\n    return np.round(attention_output, 4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6]])\nW_q = np.random.rand(3, 3)\nW_k = np.random.rand(3, 3)\nW_v = np.random.rand(3, 3)\nn_heads = 2"}
{"task_id": 94, "completion_id": 9, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Compute the query, key, and value matrices.\n\n    Parameters:\n    X: input tensor, of shape (batch_size, seq_len, embed_dim)\n    W_q, W_k, W_v: the weights for q, k, v, each of shape (embed_dim, embed_dim)\n    n_heads: the number of attention heads.\n\n    Returns:\n    A tuple of query, key, and value matrices, each of shape (batch_size, n_heads, seq_len, embed_dim/n_heads)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    (batch_size, seq_len, embed_dim) = X.shape\n    Q = Q.reshape(batch_size, seq_len, n_heads, -1)\n    K = K.reshape(batch_size, seq_len, n_heads, -1)\n    V = V.reshape(batch_size, seq_len, n_heads, -1)\n    Q = Q.transpose(0, 2, 1, 3)\n    K = K.transpose(0, 2, 1, 3)\n    V = V.transpose(0, 2, 1, 3)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention given the query, key, and value matrices.\n\n    Parameters:\n    Q, K, V: the query, key, and value matrices, each of shape (batch_size, n_heads, seq_len, embed_dim/n_heads)\n\n    Returns:\n    The result of self-attention, of shape (batch_size, n_heads, seq_len, embed_dim/n_heads)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    softmax_scores = np.apply_along_axis(lambda m: np.exp(m) / np.sum(np.exp(m), axis=-1, keepdims=True), -1, scores)\n    output = np.dot(softmax_scores, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention.\n\n    Parameters:\n    X: input tensor, of shape (batch_size, seq_len, embed_dim)\n    W_q, W_k, W_v: the weights for q, k, v, each of shape (embed_dim, embed_dim)\n    n_heads: the number of attention heads.\n\n    Returns:\n    The result of multi-head attention, as a python list.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v, n_heads)\n    attention_output = self_attention(Q, K, V)\n    attention_output = attention_output.transpose(0, 2, 1, 3)\n    (batch_size, seq_len, _, head_dim) = attention_output.shape\n    attention_output = attention_output.reshape(batch_size, seq_len, -1)\n    attention_output = np.round(attention_output, decimals=4)\n    return attention_output.tolist()\nX = np.random.rand(2, 4, 8)\nW_q = np.random.rand(8, 8)\nW_k = np.random.rand(8, 8)\nW_v = np.random.rand(8, 8)\nn_heads = 2"}
{"task_id": 95, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    xy = np.array([x, y]).T\n    contingency_table = np.zeros((2, 2), dtype=int)\n    for pair in xy:\n        contingency_table[pair[0], pair[1]] += 1\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    n = len(x)\n    phi = np.sqrt(chi2 / n)\n    return round(phi, 4)\nx = [0, 0, 1, 1]"}
{"task_id": 95, "completion_id": 1, "solution": "import numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    assert len(x) == len(y), 'The lists must be of the same length.'\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    n11 = np.sum((x_arr == 1) & (y_arr == 1))\n    n01 = np.sum((x_arr == 0) & (y_arr == 1))\n    n10 = np.sum((x_arr == 1) & (y_arr == 0))\n    n00 = np.sum((x_arr == 0) & (y_arr == 0))\n    n = len(x)\n    phi = (n11 * n00 - n10 * n01) / np.sqrt((n10 + n11) * (n00 + n01) * (n10 + n00) * (n11 + n01))\n    return round(float(phi), 4)\nx = [0, 0, 0, 1, 1]\ny = [1, 1, 0, 0, 0]"}
{"task_id": 95, "completion_id": 2, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    contingency_table = np.array([[np.sum((x == 0) & (y == 0)), np.sum((x == 0) & (y == 1))], [np.sum((x == 1) & (y == 0)), np.sum((x == 1) & (y == 1))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    phi_squared = chi2 / len(x)\n    return round(np.sqrt(phi_squared), 4)"}
{"task_id": 95, "completion_id": 3, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    contingency_table = np.array([[sum((np.array(x) == 0) & (np.array(y) == 0)), sum((np.array(x) == 0) & (np.array(y) == 1))], [sum((np.array(x) == 1) & (np.array(y) == 0)), sum((np.array(x) == 1) & (np.array(y) == 1))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    phi = np.sqrt(chi2 / len(x))\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 4, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    from collections import Counter\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    if not all((i == 0 or i == 1 for i in x + y)):\n        raise ValueError('All elements in the lists must be 0s or 1s.')\n    counts = Counter(zip(x, y))\n    n00 = counts[0, 0]\n    n01 = counts[0, 1]\n    n10 = counts[1, 0]\n    n11 = counts[1, 1]\n    n1p = n10 + n11\n    n0p = n00 + n01\n    n_p1 = n01 + n11\n    n_p0 = n00 + n10\n    numerator = n11 * n00 - n10 * n01\n    denominator = (n1p * n0p * n_p1 * n_p0) ** 0.5\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [0, 0, 1, 1, 0, 1]\ny = [1, 0, 0, 1, 0, 1]"}
{"task_id": 95, "completion_id": 5, "solution": "import numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('The lists must be of the same length.')\n    n = len(x)\n    if n == 0:\n        raise ValueError('The lists cannot be empty.')\n    a = sum((xv == 1 and yv == 1 for (xv, yv) in zip(x, y)))\n    b = sum((xv == 1 and yv == 0 for (xv, yv) in zip(x, y)))\n    c = sum((xv == 0 and yv == 1 for (xv, yv) in zip(x, y)))\n    d = sum((xv == 0 and yv == 0 for (xv, yv) in zip(x, y)))\n    numerator = a * d - b * c\n    denominator = np.sqrt((a + b) * (a + c) * (b + d) * (c + d))\n    if denominator == 0:\n        raise ValueError('Cannot calculate Phi coefficient due to division by zero.')\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\ny = [1, 0, 1, 1, 0, 0, 1, 1, 0, 1]"}
{"task_id": 95, "completion_id": 6, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n    \n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    contingency_table = np.array([[np.sum((x == 1) & (y == 1)), np.sum((x == 1) & (y == 0))], [np.sum((x == 0) & (y == 1)), np.sum((x == 0) & (y == 0))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    n = len(x)\n    phi = np.sqrt(chi2 / n)\n    return round(phi, 4)\nx = [1, 0, 1, 0, 1, 1, 0, 1, 0]\ny = [0, 1, 0, 1, 1, 0, 1, 1, 0]"}
{"task_id": 95, "completion_id": 7, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x).astype(bool)\n    y_array = np.array(y).astype(bool)\n    contingency_table = np.array([[np.sum((x_array == 0) & (y_array == 0)), np.sum((x_array == 0) & (y_array == 1))], [np.sum((x_array == 1) & (y_array == 0)), np.sum((x_array == 1) & (y_array == 1))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    phi = np.sqrt(chi2 / len(x))\n    return round(phi, 4)\nx = [0, 1, 1, 0, 1, 1, 0, 0, 1]"}
{"task_id": 95, "completion_id": 8, "solution": "import numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x)\n    y_array = np.array(y)\n    n = len(x)\n    assert len(y) == n, 'Both lists must have the same length'\n    n11 = np.sum(x_array * y_array)\n    n10 = np.sum(x_array * (1 - y_array))\n    n01 = np.sum((1 - x_array) * y_array)\n    n00 = np.sum((1 - x_array) * (1 - y_array))\n    numerator = n11 * n00 - n10 * n01\n    denominator = np.sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [0, 1, 1, 0, 0, 1]\ny = [1, 1, 0, 0, 1, 0]"}
{"task_id": 95, "completion_id": 9, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    (x_np, y_np) = (np.array(x), np.array(y))\n    contingency_matrix = np.array([[(~x_np & ~y_np).sum(), (~x_np & y_np).sum()], [(x_np & ~y_np).sum(), (x_np & y_np).sum()]])\n    (chi2, _, _, _) = chi2_contingency(contingency_matrix)\n    n = len(x)\n    phi = np.sqrt(chi2 / n)\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 1, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 2, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 3, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 4, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 5, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0\n    elif x >= 2.5:\n        return 1\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 6, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, (x + 1.0) / 2.0))"}
{"task_id": 96, "completion_id": 7, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    raw_value = x / 6 + 0.5\n    return max(0.0, min(1.0, raw_value))"}
{"task_id": 96, "completion_id": 8, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 9, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU (Exponential Linear Unit) activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 1, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 2, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value, rounded to 4th decimal place\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 3, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 4, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 5, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place.\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 6, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 7, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 8, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 9, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to the 4th decimal place\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n    \n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n    \n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    return alpha * x"}
{"task_id": 98, "completion_id": 1, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 2, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 98, "completion_id": 3, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 4, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 5, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 6, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0.0, x) + alpha * min(0.0, x)"}
{"task_id": 98, "completion_id": 7, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 8, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 98, "completion_id": 9, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 1, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    try:\n        if x > 100:\n            return x\n        elif x < -100:\n            return 0.0\n        else:\n            return math.log(1 + math.exp(x))\n    except OverflowError:\n        return float('inf')\n    return round(result, 4)"}
{"task_id": 99, "completion_id": 2, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n        \n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 3, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n    \n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 100:\n        return round(x, 4)\n    elif x <= -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(-abs(x))) + max(x, 0), 4)"}
{"task_id": 99, "completion_id": 4, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 5, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x >= 20:\n        result = x\n    elif x <= -20:\n        result = math.exp(x)\n    else:\n        result = math.log(1 + math.exp(x))\n    return round(result, 4)"}
{"task_id": 99, "completion_id": 6, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return x\n    elif x < -100:\n        return 0.0\n    else:\n        return math.log(1 + math.exp(x))"}
{"task_id": 99, "completion_id": 7, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 100:\n        result = x\n    else:\n        result = math.log(1 + math.exp(x))\n    return round(result, 4)"}
{"task_id": 99, "completion_id": 8, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < 0:\n        return math.log(1 + math.exp(x))\n    else:\n        return x + math.log(1 + math.exp(-x))"}
{"task_id": 99, "completion_id": 9, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return x\n    elif x < -100:\n        return 0\n    else:\n        return math.log(1 + math.exp(x))"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 1, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 2, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 3, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 4, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 5, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 6, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 7, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 8, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 9, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value, rounded to the nearest 6th decimal.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    objective_value = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.mean(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_objective_value = objective_value - beta * kl_divergence\n    return round(grpo_objective_value, 6)\nrhos = [1.2, 0.8, 1.0, 1.5, 0.5]\nA = [0.3, -0.1, 0.2, 0.4, -0.2]\npi_theta_old = [0.6, 0.4, 0.3, 0.5, 0.1]\npi_theta_ref = [0.5, 0.5, 0.35, 0.45, 0.05]"}
{"task_id": 101, "completion_id": 1, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n    \n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.maximum(np.minimum(rhos, 1 + epsilon), 1 - epsilon)\n    surr_1 = rhos * A\n    surr_2 = clipped_rhos * A\n    surr = np.minimum(surr_1, surr_2)\n    objective_value = np.mean(surr) - beta * np.mean(rhos * np.log(np.maximum(pi_theta_old / pi_theta_ref, 1e-08)))\n    return round(float(objective_value), 6)\nrhos = np.array([0.9, 1.1, 1.5, 1.4, 0.6])\nA = np.array([0.5, 1.2, -0.8, 2.0, 0.3])\npi_theta_old = np.array([0.5, 0.5, 0.5, 0.4, 0.6])\npi_theta_ref = np.array([0.6, 0.6, 0.4, 0.5, 0.7])\nepsilon = 0.2\nbeta = 0.01"}
{"task_id": 101, "completion_id": 2, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n    \n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.asarray(rhos)\n    A = np.asarray(A)\n    pi_theta_old = np.asarray(pi_theta_old)\n    pi_theta_ref = np.asarray(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    kl_penalty = np.sum(pi_theta_ref * (np.log(pi_theta_ref) - np.log(pi_theta_old)))\n    objective = np.mean(np.minimum(rhos, clipped_rhos) * A - beta * kl_penalty)\n    return round(objective, 6)\nrhos = [0.9, 1.2, 1.5]\nA = [0.3, 0.6, -0.1]\npi_theta_old = [0.3, 0.45, 0.22]\npi_theta_ref = [0.25, 0.5, 0.25]\nepsilon = 0.1\nbeta = 0.05"}
{"task_id": 101, "completion_id": 3, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_grpo = np.minimum(rhos * A, clipped_rhos * A)\n    objective = np.mean(clipped_grpo)\n    kl_div = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    objective -= beta * kl_div\n    return round(objective, 6)\nrhos = [1.2, 0.9, 1.1, 0.8]\nA = [0.2, -0.3, 0.1, -0.2]\npi_theta_old = [0.3, 0.4, 0.3, 0.2]\npi_theta_ref = [0.2, 0.5, 0.2, 0.1]"}
{"task_id": 101, "completion_id": 4, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n    \n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rhos_clipped = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    surrogate_loss = np.minimum(rhos * A, rhos_clipped * A)\n    kl_divergence = np.sum((rhos - 1) * pi_theta_old + pi_theta_old - pi_theta_ref)\n    penalty = beta * kl_divergence\n    grpo_obj = np.mean(surrogate_loss) - penalty\n    return round(grpo_obj, 6)"}
{"task_id": 101, "completion_id": 5, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    objective_clipped = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_result = objective_clipped - beta * kl_divergence\n    return round(grpo_result, 6)"}
{"task_id": 101, "completion_id": 6, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    assert len(rhos) == len(A) == len(pi_theta_old) == len(pi_theta_ref), 'All input lists must have the same length'\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    weighted_advantages = np.array([rho * a if rho < 1 - epsilon or rho > 1 + epsilon else clipped_rho * a for (rho, a, clipped_rho) in zip(rhos, A, clipped_rhos)])\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    obj_value = np.mean(weighted_advantages) - beta * kl_divergence\n    return round(obj_value, 6)\nrhos = [1.1, 0.95, 1.2, 0.8]\nA = [2, -1, 3, -2]\npi_theta_old = [0.5, 0.4, 0.3, 0.2]\npi_theta_ref = [0.6, 0.5, 0.4, 0.3]"}
{"task_id": 101, "completion_id": 7, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    rho_refs = pi_theta_ref / pi_theta_old\n    clipped_rho_refs = np.clip(rho_refs, 1 - epsilon, 1 + epsilon)\n    advantages = np.array(A)\n    grpo_estimate = np.mean([min(r * adv, clipped_r * adv) * (1 + beta * min(rho_ref * adv / (r * adv), clipped_rho_ref * adv / (clipped_r * adv))) for (r, adv, r_ref, clipped_r, clipped_r_ref) in zip(rhos, advantages, rho_refs, clipped_rhos, clipped_rho_refs)])\n    return round(grpo_estimate, 6)"}
{"task_id": 101, "completion_id": 8, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1.0 - epsilon, 1.0 + epsilon)\n    clipped_objective_values = np.minimum(rhos * A, clipped_rhos * A)\n    surrogate_objective = np.mean(clipped_objective_values)\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_objective_value = surrogate_objective - beta * kl_divergence\n    return round(grpo_objective_value, 6)\nrhos = [1.2, 0.8, 1.5, 0.5]\nA = [0.5, 0.3, -0.5, -0.2]\npi_theta_old = [0.1, 0.05, 0.08, 0.03]\npi_theta_ref = [0.2, 0.1, 0.15, 0.05]"}
{"task_id": 101, "completion_id": 9, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    omega = clipped_rhos / pi_theta_ref\n    kl_penalty = pi_theta_old * np.log(pi_theta_old / pi_theta_ref)\n    objective = np.mean(omega * A) - beta * np.mean(kl_penalty)\n    return round(float(objective), 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.2, -0.5, 0.3, -0.1]\npi_theta_old = [0.6, 0.4, 0.7, 0.3]\npi_theta_ref = [0.55, 0.35, 0.65, 0.25]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal.\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    result = x * sigmoid_x\n    return round(result, 4)"}
{"task_id": 102, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n    \n    Args:\n        x: Input value\n        \n    Returns:\n        The Swish activation value\n    \"\"\"\n    result = x * (1 / (1 + math.exp(-x)))\n    return round(result, 4)"}
{"task_id": 102, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))"}
{"task_id": 102, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))"}
{"task_id": 102, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    swish_value = x * sigmoid_x\n    return round(swish_value, 4)"}
{"task_id": 102, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))"}
{"task_id": 102, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x / (1 + math.exp(-x))"}
{"task_id": 102, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    beta = 1.0\n    swish_value = x * (1 / (1 + math.exp(-beta * x)))\n    return round(swish_value, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n    \n    Args:\n        x: Input value\n    \n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * (alpha * (math.exp(x) - 1))"}
{"task_id": 103, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_predictions = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_predictions))\n    binary_predictions = (probabilities > 0.5).astype(int).tolist()\n    return binary_predictions\nD = 10\nweights = np.random.randn(D)\nbias = np.random.randn()"}
{"task_id": 104, "completion_id": 1, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    logits = np.dot(X, weights) + bias\n    probabilities = sigmoid(logits)\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 2, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n    \n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n    \n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-logits))\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()\nX_example = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\nweights_example = np.array([0.1, 0.2])\nbias_example = 0.3\npredictions = predict_logistic(X_example, weights_example, bias_example)"}
{"task_id": 104, "completion_id": 3, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D).\n        weights: Model weights (shape: D).\n        bias: Model bias.\n\n    Returns:\n        Binary predictions (0 or 1).\n    \"\"\"\n    linear_model = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_model))\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Implement the sigmoid function.\n\n    Args:\n        z (np.ndarray): Input array or scalar.\n\n    Returns:\n        np.ndarray: Output after applying the sigmoid function.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = sigmoid(logits)\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid function value for a given input z.\n\n    SIGMOID(z) = 1 / (1 + e^(-z)), where e is the base of natural logarithm and z is the input.\n\n    Args:\n        z: The linear input of sigmoid function.\n\n    Returns:\n        The sigmoid function value of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-np.where(z > 50, 50, z)))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D).\n        weights: Model weights (shape: D).\n        bias: Model bias.\n\n    Returns:\n        Binary predictions (0 or 1) in the form of a python list.\n    \"\"\"\n    z = X.dot(weights) + bias\n    probabilities = sigmoid(z)\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the output of the sigmoid function.\n\n    Args:\n        z: A scalar or numpy array of any shape.\n\n    Returns:\n        The output of the sigmoid function, with the same shape as z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()\nX_example = np.array([[1, 2], [2, 3], [3, 4]])\nweights_example = np.array([0.5, -0.4])\nbias_example = -1.0\npredictions = predict_logistic(X_example, weights_example, bias_example)"}
{"task_id": 104, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid activation function.\n    The function will map any real value into another value between 0 and 1,\n    which is useful for transforming the output of a linear combination of features into probabilities.\n\n    Args:\n        z: Input parameter for sigmoid function.\n\n    Returns:\n        Array of values transformed into a probability.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\\x97 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) after transforming into probabilities using the sigmoid function.\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if prob > 0.5 else 0 for prob in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z.\n\n    Args:\n        z: A scalar or numpy array of any size.\n\n    Returns:\n        The sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = sigmoid(z)\n    predictions = [1 if prob > 0.5 else 0 for prob in probabilities]\n    return predictions"}
{"task_id": 104, "completion_id": 9, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    logits = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-logits))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z):\n    exps = np.exp(z - np.max(z))\n    return exps / np.sum(exps, axis=1, keepdims=True)\ndef cross_entropy(y_true, y_pred):\n    n_samples = y_true.shape[0]\n    loss = -np.sum(np.log(y_pred[range(n_samples), y_true])) / n_samples\n    return loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list, list]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = np.max(y) + 1\n    losses = []\n    B = np.zeros((n_features, n_classes))\n    for i in range(iterations):\n        probabilities = softmax(np.dot(X, B))\n        gradient = np.dot(X.T, probabilities - y_one_hot(y, n_classes)) / n_samples\n        B -= learning_rate * gradient\n        loss = cross_entropy(y, probabilities)\n        losses.append(round(loss, 4))\n    return (np.round(B, 4).tolist(), losses)\ndef y_one_hot(y, n_classes):\n    \"\"\"\n    Convert y into a one-hot encoded matrix.\n    \"\"\"\n    n_samples = y.size\n    y_one_hot = np.zeros((n_samples, n_classes))\n    y_one_hot[np.arange(n_samples), y] = 1\n    return y_one_hot\nX = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\ny = np.array([0, 2, 0, 2])\nlearning_rate = 0.1\niterations = 4000"}
{"task_id": 105, "completion_id": 1, "solution": "import numpy as np\ndef softmax(z):\n    \"\"\"Computes the softmax of the input.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / e_z.sum(axis=1, keepdims=True)\ndef cross_entropy(y_true, y_pred):\n    \"\"\"Computes the cross-entropy loss.\"\"\"\n    return -np.sum(y_true * np.log(y_pred + 1e-10)) / y_true.shape[0]\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list, list]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n    \n    Returns:\n        B : list of floats, CxM updated parameter vector rounded to 4 floating points\n        losses : list of floats, collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = np.max(y) + 1\n    B = np.zeros((n_features, n_classes))\n    losses = []\n    y_true = np.zeros((n_samples, n_classes))\n    y_true[np.arange(n_samples), y] = 1\n    for _ in range(iterations):\n        predicted = softmax(np.dot(X, B))\n        gradient = np.dot(X.T, predicted - y_true) / n_samples\n        B -= learning_rate * gradient\n        loss = cross_entropy(y_true, predicted)\n        losses.append(round(loss, 4))\n    return (B.round(4).tolist(), losses)\nX = np.array([[0.4, 0.6], [0.1, 0.9], [0.3, 0.7]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.1\niterations = 500"}
{"task_id": 105, "completion_id": 2, "solution": "import numpy as np\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / e_z.sum(axis=1, keepdims=True)\ndef one_hot(y, num_classes):\n    \"\"\"Convert class vector to binary class matrix.\"\"\"\n    return np.eye(num_classes)[y]\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (m, n) = X.shape\n    num_classes = np.max(y) + 1\n    Y_one_hot = one_hot(y, num_classes)\n    B = np.zeros((n, num_classes))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        probs = softmax(scores)\n        ce_loss = -np.sum(Y_one_hot * np.log(probs)) / m\n        losses.append(round(ce_loss, 4))\n        dLdB = np.dot(X.T, probs - Y_one_hot) / m\n        B = B - learning_rate * dLdB\n    B_list = np.around(B, decimals=4).tolist()\n    losses = np.around(losses, decimals=4).tolist()\n    return (B_list, losses)"}
{"task_id": 105, "completion_id": 3, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n = X.shape[0]\n    m = X.shape[1]\n    k = len(np.unique(y))\n    y = np.eye(k)[y]\n    W = np.zeros((m, k))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, W)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y * np.log(probs)) / n\n        grad_scores = probs - y\n        grad_W = np.dot(X.T, grad_scores) / n\n        W -= learning_rate * grad_W\n        losses.append(round(loss, 4))\n    B = np.around(W, 4).tolist()\n    return (B, losses)"}
{"task_id": 105, "completion_id": 4, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_outputs = len(np.unique(y))\n    B = np.zeros((n_features, n_outputs))\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        softmax_scores = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n        y_true = np.zeros((n_samples, n_outputs))\n        y_true[np.arange(n_samples), y] = 1\n        cross_entropy_loss = -np.sum(np.log(softmax_scores[range(n_samples), y])) / n_samples\n        losses.append(cross_entropy_loss.round(4))\n        gradient = -X.T @ (y_true - softmax_scores) / n_samples\n        B -= learning_rate * gradient\n    B = np.around(B, decimals=4)\n    return (B.tolist(), [losses[0]] if len(losses) == 1 else losses)"}
{"task_id": 105, "completion_id": 5, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        Loss : list[float], collected values of Cross Entropy loss rounded to 4 floating points\n    \"\"\"\n    (n, m) = X.shape\n    num_classes = len(np.unique(y))\n    y = np.eye(num_classes)[y]\n    W = np.random.normal(scale=0.01, size=(m, num_classes))\n    B = []\n    losses = []\n    for iteration in range(iterations):\n        scores = np.dot(X, W)\n        exp_scores = np.exp(scores)\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y * np.log(probs)) / n\n        losses.append(np.round(loss, 4))\n        dW = np.dot(X.T, probs - y) / n\n        W -= learning_rate * dW\n        B = W.tolist()\n    return (B, losses)"}
{"task_id": 105, "completion_id": 6, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (m, n) = X.shape\n    num_classes = len(set(y))\n    y = np.eye(num_classes)[y]\n    B = np.zeros((n, num_classes))\n    losses = []\n    for i in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y * np.log(probs)) / m\n        losses.append(round(loss, 4))\n        dscores = probs - y\n        dB = X.T @ dscores\n        B -= learning_rate * dB / m\n    return (B.round(4).tolist(), [loss_rounded for loss_rounded in losses])"}
{"task_id": 105, "completion_id": 7, "solution": "import numpy as np\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / e_z.sum(axis=1, keepdims=True)\ndef one_hot_encoding(y, n_classes):\n    \"\"\"Converts y into one-hot encoded matrix.\"\"\"\n    y_encoded = np.zeros((y.size, n_classes))\n    y_encoded[np.arange(y.size), y] = 1\n    return y_encoded\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Calculate Cross Entropy loss.\"\"\"\n    return -np.sum(y_true * np.log(y_pred + 1e-10)) / y_true.shape[0]\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    y_one_hot = one_hot_encoding(y, n_classes)\n    B = np.zeros((n_features, n_classes))\n    losses = []\n    for _ in range(iterations):\n        y_pred = softmax(X @ B)\n        loss = cross_entropy_loss(y_one_hot, y_pred)\n        losses.append(round(loss, 4))\n        gradient = 1 / n_samples * (X.T @ (y_pred - y_one_hot))\n        B -= learning_rate * gradient\n    return (B.round(4).tolist(), [round(loss, 4) for loss in losses])\nX = np.random.rand(100, 10)\ny = np.random.randint(0, 3, 100)\nlearning_rate = 0.01\niterations = 1000"}
{"task_id": 105, "completion_id": 8, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n\n    def softmax(S: np.ndarray) -> np.ndarray:\n        exps = np.exp(S - np.max(S, axis=1, keepdims=True))\n        return exps / np.sum(exps, axis=1, keepdims=True)\n\n    def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        n_samples = y_pred.shape[0]\n        log_likelihood = -np.log(y_pred[range(n_samples), y_true])\n        loss = np.sum(log_likelihood) / n_samples\n        return loss\n    (n_samples, n_features) = X.shape\n    n_classes = np.max(y) + 1\n    W = np.zeros((n_features, n_classes))\n    losses = []\n    B = []\n    for _ in range(iterations):\n        S = X @ W\n        P = softmax(S)\n        loss = cross_entropy(y, P)\n        losses.append(np.round(loss, 4))\n        grad = X.T @ (P - np.eye(n_classes)[y]) / n_samples\n        W -= learning_rate * grad\n    B = np.round(W, 4).tolist()\n    losses = [np.round(loss, 4) for loss in losses]\n    return (B, losses)\niterations = 1000"}
{"task_id": 105, "completion_id": 9, "solution": "import numpy as np\ndef softmax(z):\n    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\ndef cross_entropy_loss(h, y, m):\n    log_likelihood = -np.log(h[range(m), y])\n    loss = np.sum(log_likelihood) / m\n    return loss\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    (m, n) = X.shape\n    K = len(set(y))\n    W = np.random.randn(n, K) / np.sqrt(n)\n    losses = []\n    for epoch in range(iterations):\n        z = X @ W\n        h = softmax(z)\n        delta = h\n        delta[range(m), y] -= 1\n        grad = X.T @ delta / m\n        W -= learning_rate * grad\n        loss = cross_entropy_loss(h, y, m)\n        losses.append(round(loss, 4))\n    B = W.round(4).tolist()\n    return (B, [round(loss, 4) for loss in losses])"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z.\n\n    Parameters:\n    z (float or numpy.ndarray): The input value or array for sigmoid function.\n\n    Returns:\n    float or numpy.ndarray: The sigmoid output.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    X (np.ndarray): The input features matrix of shape (n_samples, n_features).\n    y (np.ndarray): The target values of shape (n_samples,).\n    learning_rate (float): The learning rate for gradient descent updates.\n    iterations (int): The number of iterations for gradient descent.\n\n    Returns:\n    tuple[list[float], list[float]]: A tuple containing the optimized coefficients and the collected loss values.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    losses = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = (-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)).mean()\n        losses.append(round(float(loss), 4))\n    return (weights.tolist(), losses)"}
{"task_id": 106, "completion_id": 1, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Calculating the sigmoid activation function.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, weights)\n        h = sigmoid(z)\n        gradient = np.dot(X.T, h - y) / m\n        weights -= learning_rate * gradient\n        loss = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n        losses.append(round(loss, 4))\n    return (weights.tolist(), losses)"}
{"task_id": 106, "completion_id": 2, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid function to compress the output of a linear model into probabilities.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    X (np.ndarray): The training data, with each row representing a sample and each column representing a feature.\n    y (np.ndarray): The labels for the training data.\n    learning_rate (float): The learning rate of the gradient descent.\n    iterations (int): The number of iterations to perform the gradient descent.\n    \n    Returns:\n    tuple[list[float], list[float]]: The optimized coefficients and the history of losses over iterations.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    intercept = 0\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + intercept\n        predictions = sigmoid(linear_model)\n        dw = 1 / n_samples * np.dot(X.T, predictions - y)\n        db = 1 / n_samples * np.sum(predictions - y)\n        weights -= learning_rate * dw\n        intercept -= learning_rate * db\n        loss = -1 / n_samples * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_history.append(round(loss, 4))\n    weights = np.append(weights, intercept)\n    return (weights.tolist(), loss_history)"}
{"task_id": 106, "completion_id": 3, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function that maps any real value into another value between 0 and 1.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(y, y_hat):\n    \"\"\"\n    Compute the binary cross entropy loss.\n    \"\"\"\n    y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n    loss = -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n    return np.mean(loss)\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    The function returns the optimized coefficients, each rounded to the nearest 4th decimal, and a list of loss values\n    over iterations.\n    \"\"\"\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    theta = np.zeros(X.shape[1])\n    loss_values = []\n    for i in range(iterations):\n        y_hat = sigmoid(np.dot(X, theta))\n        gradient = np.dot(X.T, y_hat - y) / y.size\n        theta -= learning_rate * gradient\n        loss = compute_loss(y, y_hat)\n        loss_values.append(round(loss, 4))\n    theta_rounded = np.round(theta, 4).tolist()\n    return (theta_rounded, loss_values)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([0, 0, 1, 1, 1])\nlearning_rate = 0.1\niterations = 1000"}
{"task_id": 106, "completion_id": 4, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"Sigmoid function that maps any real value into another value between 0 and 1.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list, list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, w) + b\n        y_pred = sigmoid(z)\n        dw = 1 / m * np.dot(X.T, y_pred - y)\n        db = 1 / m * np.sum(y_pred - y)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        loss = -1 / m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(np.round(loss, 4))\n    return ([w.tolist(), b], losses)\nX = np.array([[1, 1], [2, 2], [-1, -1], [-2, -2]])\ny = np.array([[1], [1], [0], [0]])\nlearning_rate = 0.1\niterations = 5000"}
{"task_id": 106, "completion_id": 5, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function to calculate the probability.\n    :param z: input value(s)\n    :return: Probability of being in class 1\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list, list]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    :param X: numpy.ndarray, input data\n    :param y: numpy.ndarray, response vector\n    :param learning_rate: float, learning rate for the gradient descent\n    :param iterations: int, total number of iterations for the gradient descent\n    :return: tuple containing the optimized coefficients as list and the list of loss values over iterations\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    losses = []\n    for i in range(iterations):\n        y_pred = sigmoid(X.dot(theta))\n        loss = -1 / m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        gradient = 1 / m * X.T.dot(y_pred - y)\n        theta -= learning_rate * gradient\n    return (theta.tolist(), losses)"}
{"task_id": 106, "completion_id": 6, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid activation function.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    y = y.reshape(-1, 1)\n    loss_values = []\n    for i in range(iterations):\n        z = np.dot(X, weights) + bias\n        a = sigmoid(z)\n        cost = -1 / m * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n        loss_values.append(np.around(cost, 4))\n        dw = 1 / m * np.dot(X.T, a - y)\n        db = 1 / m * np.sum(a - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    return (weights.tolist(), np.around(bias, 4).tolist(), loss_values)"}
{"task_id": 106, "completion_id": 7, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the output of a sigmoid function, which maps any real-valued number \n    into the (0, 1) interval.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Compute Binary Cross Entropy loss.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return np.round(loss, 4)\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters \n    with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    biases = 0\n    loss_over_iterations = []\n    for _ in range(iterations):\n        z = np.dot(X, weights) + biases\n        y_pred = sigmoid(z)\n        loss = compute_loss(y, y_pred)\n        loss_over_iterations.append(float(loss))\n        dz = y_pred - y\n        dw = 1 / m * np.dot(X.T, dz)\n        db = 1 / m * np.sum(dz)\n        weights = weights - learning_rate * dw\n        biases = biases - learning_rate * db\n    optimized_coefficients = np.append(weights, biases).tolist()\n    return (optimized_coefficients, loss_over_iterations)\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.01\niterations = 5000"}
{"task_id": 106, "completion_id": 8, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int):\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros((n, 1))\n    bias = 0\n    losses = []\n    for _ in range(iterations):\n        y_pred = sigmoid(np.dot(X, theta) + bias)\n        cost = -1 / m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(np.round(cost, 4))\n        dz = y_pred - y\n        dw = 1 / m * np.dot(X.T, dz)\n        db = 1 / m * np.sum(dz)\n        theta -= learning_rate * dw\n        bias -= learning_rate * db\n    optimized_coefs = np.append(theta, bias).tolist()\n    optimized_coefs = [np.round(coef, 4) for coef in optimized_coefs]\n    return (optimized_coefs, losses)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([[0], [0], [1], [1]])\nlearning_rate = 0.05\niterations = 100"}
{"task_id": 106, "completion_id": 9, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Computes the Binary Cross Entropy loss.\n    \"\"\"\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    ce_loss = -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n    return ce_loss\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    losses = []\n    for i in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        predictions = sigmoid(linear_model)\n        dw = 1 / m * np.dot(X.T, predictions - y)\n        db = 1 / m * np.sum(predictions - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = compute_loss(y, predictions)\n        losses.append(round(float(loss), 4))\n    return (weights.tolist(), bias, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    :param Q: Query matrix, shape (batch_size, sequence_length, d_k)\n    :param K: Key matrix, shape (batch_size, sequence_length, d_k)\n    :param V: Value matrix, shape (batch_size, sequence_length, d_v)\n    :param mask: Mask to apply, shape (batch_size, sequence_length, sequence_length)\n    :return: Output of the masked attention mechanism as a python list\n    \"\"\"\n    d_k = Q.shape[-1]\n    attention_scores = np.matmul(Q, K.transpose((0, 2, 1))) / np.sqrt(d_k)\n    attention_scores = np.where(mask == 0, -np.inf, attention_scores)\n    attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 1, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Parameters:\n    - Q: Query matrix.\n    - K: Key matrix.\n    - V: Value matrix.\n    - mask: The attention mask used to penalize or ignore certain parts of the matrix.\n\n    Returns:\n    - The output of the masked self-attention as a Python list.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    scores -= 1000000000.0 * mask\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.matmul(attention_weights, V)\n    return output.tolist()\nmask = np.array([[0, 1], [0, 0]])\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 107, "completion_id": 2, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> list:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n    Q (np.ndarray): Query matrix.\n    K (np.ndarray): Key matrix.\n    V (np.ndarray): Value matrix.\n    mask (np.ndarray): Attention mask.\n    \n    Returns:\n    list: Result after applying masked attention to the input matrices in list form.\n    \"\"\"\n    dot_product = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    dot_product = np.where(mask == 0, -np.inf, dot_product)\n    weight = np.exp(dot_product) / np.sum(np.exp(dot_product), axis=-1, keepdims=True)\n    output = np.dot(weight, V)\n    return output.tolist()\nmask = np.array([[1, 0, 0], [1, 1, 0], [1, 1, 1]])"}
{"task_id": 107, "completion_id": 3, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    The attention scores are computed based on queries and keys,\n    then masked and normalized, and then used to compute the\n    weighted sum of the values.\n\n    :param Q: Query matrix\n    :param K: Key matrix\n    :param V: Value matrix\n    :param mask: Mask matrix, has -np.inf where attention is not allowed and 0 elsewhere.\n    :return: The output of the self-attention mechanism with masking.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask\n    p_attn = np.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(-1), axis=-1, arr=scores)\n    output = np.dot(p_attn, V)\n    return output.tolist()\nmask = np.array([[-np.inf, 0], [0, -np.inf]])\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 107, "completion_id": 4, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = np.where(mask, scores, -np.inf)\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    attended = np.dot(weights, V)\n    return attended.tolist()\nmask = np.tril(np.ones((5, 5)))"}
{"task_id": 107, "completion_id": 5, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    :param Q: Query matrix of shape (batch_size, sequence_length, output_dim)\n    :param K: Key matrix of shape (batch_size, sequence_length, output_dim)\n    :param V: Value matrix of shape (batch_size, sequence_length, output_dim)\n    :param mask: Attention mask of shape (batch_size, sequence_length, sequence_length)\n    :return: Output matrix of the masked self-attention of shape (batch_size, sequence_length, output_dim)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.einsum('ijk,ilk->ijl', Q, K) / np.sqrt(d_k)\n    scores += (1 - mask) * -1000000000.0\n    attn_probs = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x), axis=-1), axis=-1, arr=scores)\n    output = np.einsum('ijk,ikl->ijl', attn_probs, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 6, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :param mask: Attention mask to apply.\n    :return: Output of the masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T)\n    masked_scores = np.where(mask, scores, -1000000000.0)\n    att_weights = np.exp(masked_scores) / np.sum(np.exp(masked_scores), axis=-1, keepdims=True)\n    output = np.dot(att_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 7, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Parameters:\n    Q (np.ndarray): The query matrix.\n    K (np.ndarray): The key matrix.\n    V (np.ndarray): The value matrix.\n    mask (np.ndarray): The mask matrix to apply to the attention distribution.\n\n    Returns:\n    np.ndarray: The masked self-attention output.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    attn_dist = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.dot(attn_dist, V)\n    return output.tolist()\nmask = np.tril(np.ones((10, 10))) == 0\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 107, "completion_id": 8, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    mask_value = -np.inf\n    masked_attention_scores = np.where(mask != 0, attention_scores, mask_value)\n    attention_probs = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), 1, masked_attention_scores)\n    output = np.dot(attention_probs, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 9, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    masked_scores = attention_scores + mask * -1000000000.0\n    attention_weights = np.softmax(masked_scores, axis=-1)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output\nmask = np.triu(np.ones((4, 4), dtype=bool), k=1) * -1000000000.0"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The function uses the concept of Shannon entropy as a measure of disorder\n    or uncertainty, which increases with the diversity and evenness of apple colors\n    in the basket.\n\n    Parameters:\n    apples (list): A list of integers representing apple colors.\n\n    Returns:\n    float: A floating point number representing the disorder, rounded to 4 decimal places.\n    \"\"\"\n    total_count = len(apples)\n    if total_count == 0:\n        return 0.0\n    color_counts = Counter(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_count\n        entropy -= probability * log2(probability)\n    return round(entropy, 4)\nlog2 = math.log2"}
{"task_id": 108, "completion_id": 1, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is calculated as the entropy of the distribution of apple colors.\n    \"\"\"\n    n = len(apples)\n    if n <= 1:\n        return 0.0\n    counts = Counter(apples)\n    disorder_measure = 0.0\n    for count in counts.values():\n        p = count / n\n        disorder_measure -= p * math.log2(p)\n    return round(disorder_measure, 4)"}
{"task_id": 108, "completion_id": 2, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is calculated using Shannon's entropy formula.\n    \"\"\"\n    apple_count = len(apples)\n    if apple_count <= 1:\n        return 0.0\n    counts = Counter(apples)\n    entropy = -sum((count / apple_count * math.log2(count / apple_count) for count in counts.values()))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 3, "solution": "import math\nfrom typing import List\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The entropy is used as a measure of disorder.\n    \"\"\"\n    total_apples = len(apples)\n    frequency = {}\n    for apple in apples:\n        frequency[apple] = frequency.get(apple, 0) + 1\n    entropy = 0\n    for color_freq in frequency.values():\n        prob = color_freq / total_apples\n        entropy -= prob * math.log2(prob)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 4, "solution": "import math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is calculated using the entropy of the distribution of colors.\n    \"\"\"\n    if len(apples) <= 1:\n        return 0\n    color_counts = {}\n    for color in apples:\n        if color in color_counts:\n            color_counts[color] += 1\n        else:\n            color_counts[color] = 1\n    entropy = 0.0\n    for color in color_counts:\n        probability = color_counts[color] / len(apples)\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 5, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    This function measures disorder as a kind of entropy based on the distribution \n    of colors. The more evenly distributed the colors are, the higher the disorder.\n    \"\"\"\n    from collections import Counter\n    import math\n    freq = Counter(apples)\n    total_apples = len(apples)\n    disorder = 0.0\n    for count in freq.values():\n        prob = count / total_apples\n        disorder -= prob * math.log2(prob)\n    return round(disorder, 4)"}
{"task_id": 108, "completion_id": 6, "solution": "from collections import Counter\nfrom math import log2\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder (entropy) in a basket of apples based on their colors.\n    The entropy is 0 if all apples are the same color and increases as the variety of colors increases.\n    \"\"\"\n    freq = dict(Counter(apples))\n    total = len(apples)\n    entropy = 0.0\n    for count in freq.values():\n        p = count / total\n        entropy -= p * log2(p)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 7, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \n    The disorder is calculated using the entropy formula which is a quantity used \n    to measure the unpredictability or variety of colors. The higher the entropy,\n    the higher the disorder in the apple basket.\n    \"\"\"\n    from collections import Counter\n    import math\n    if not apples:\n        return 0.0\n    total_apples = len(apples)\n    color_counts = Counter(apples)\n    disorder_value = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        disorder_value -= probability * math.log2(probability)\n    return round(disorder_value, 4)"}
{"task_id": 108, "completion_id": 8, "solution": "from typing import List\nfrom collections import Counter\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder here is based on the entropy of the color distribution.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 9, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder (entropy) in a basket of apples based on their colors.\n    \"\"\"\n    counts = Counter(apples)\n    num_apples = len(apples)\n    if num_apples == 0:\n        return 0.0\n    entropy = 0.0\n    for count in counts.values():\n        p = count / num_apples\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on the input tensor X. It normalizes the feature dimensions for each element in the\n    sequence across the batch. After normalization, it scales and shifts the tensor using learnable parameters gamma and beta.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scaling parameters of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameters of shape (feature_dim,).\n    epsilon (float): Small constant to avoid division by zero.\n    \n    Returns:\n    np.ndarray: Normalized, scaled, and shifted tensor of the same shape as input tensor X.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma * X_norm + beta\n    return np.round(X_scaled_shifted, decimals=5).tolist()"}
{"task_id": 109, "completion_id": 1, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Input tensor of shape (batch_size, sequence_length, feature_dimension).\n    gamma : np.ndarray\n        Scaling parameter. Expected to be of shape (feature_dimension,).\n    beta : np.ndarray\n        Shifting parameter. Expected to be of shape (feature_dimension,).\n    epsilon : float, optional\n        A small constant for numerical stability (default is 1e-5).\n        \n    Returns:\n    --------\n    np.ndarray\n        Normalized tensor with the same shape as X.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled_shifted = gamma * X_norm + beta\n    return np.round(X_scaled_shifted, 5).tolist()"}
{"task_id": 109, "completion_id": 2, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform layer normalization on the input tensor X.\n    :param X: A 3D numpy array of shape (batch_size, seq_len, feature_dim).\n    :param gamma: A 1D numpy array of shape (feature_dim,) containing scale parameters.\n    :param beta: A 1D numpy array of shape (feature_dim,) containing shift parameters.\n    :param epsilon: A small constant for numerical stability.\n    :return: A 3D numpy array containing the layer-normalized data.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    return normalized_X.round(5).tolist()"}
{"task_id": 109, "completion_id": 3, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Performs Layer Normalization on the input tensor.\n    Normalizes the data across the feature dimension for each sequence,\n    then applies scaling and shifting parameters.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim)\n    gamma (np.ndarray): Scaling parameters of shape (feature_dim,)\n    beta (np.ndarray): Shifting parameters of shape (feature_dim,)\n    epsilon (float): Small value for numeric stability\n\n    Returns:\n    np.ndarray: Normalized tensor\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    x_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = gamma * x_norm + beta\n    return np.round(X_normalized, 5).tolist()\nbatch_size = 2\nseq_length = 3\nfeature_dim = 4\nX = np.random.randn(batch_size, seq_length, feature_dim)\ngamma = np.random.randn(feature_dim)\nbeta = np.random.randn(feature_dim)"}
{"task_id": 109, "completion_id": 4, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies layer normalization to a 3D array. The array should have the shape\n    (batch_size, sequence_length, feature_dim). Normalizes across the feature_dim.\n\n    Parameters:\n    X (np.ndarray): The input array of shape (batch_size, sequence_length, feature_dim).\n    gamma (np.ndarray): Scale parameter of shape (feature_dim,).\n    beta (np.ndarray): Shift parameter of shape (feature_dim,).\n    epsilon (float): Small constant to prevent division by zero.\n\n    Returns:\n    np.ndarray: The layer normalized array.\n    \"\"\"\n    feature_dim = X.shape[-1]\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    x_centered = X - mean\n    std = np.sqrt(var + epsilon)\n    X_normalized = x_centered / std\n    X_scaled_shifted = gamma * X_normalized + beta\n    return np.round(X_scaled_shifted, decimals=5).tolist()\ngamma = np.array([1.5, 1.5], dtype=np.float32)\nbeta = np.array([0.5, 0.5], dtype=np.float32)"}
{"task_id": 109, "completion_id": 5, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Applies Layer Normalization on the input tensor X with scaling and shifting parameters gamma and beta.\n    \n    Parameters:\n    X (np.ndarray): A 3D input tensor of shape (batch_size, seq_len, feature_dim).\n    gamma (np.ndarray): A 1D array of scaling parameters of shape (feature_dim,).\n    beta (np.ndarray): A 1D array of shifting parameters of shape (feature_dim,).\n    epsilon (float): A small constant for numerical stability.\n    \n    Returns:\n    list: A 3D list representing the normalized input tensor X, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_final = X_norm * gamma + beta\n    return np.round(X_final, decimals=5).tolist()"}
{"task_id": 109, "completion_id": 6, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform layer normalization on a 3D numpy array X using scale factor gamma and shift factor beta.\n    The normalization is performed across the last dimension of X, i.e., the feature dimension.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim).\n    gamma (np.ndarray): Scale parameter tensor of shape (feature_dim,).\n    beta (np.ndarray): Shift parameter tensor of shape (feature_dim,).\n    epsilon (float): A small value to avoid division by zero.\n    \n    Returns:\n    np.ndarray: Layer normalized tensor of same shape as X, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / np.sqrt(var + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    normalized_X = np.round(normalized_X, decimals=5)\n    return normalized_X"}
{"task_id": 109, "completion_id": 7, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Performs Layer Normalization on the given 3D numpy array X with scaling and shifting parameters gamma and beta.\n    Normalization is performed across the feature dimension for each sequence.\n    \n    Parameters:\n    X: np.ndarray\n        The input tensor of shape (batch_size, sequence_length, feature_dimensions)\n    gamma: np.ndarray\n        Scaling parameter of shape (feature_dimensions,)\n    beta: np.ndarray\n        Shifting parameter of shape (feature_dimensions,)\n    epsilon: float, optional\n        Small constant to prevent division by zero (default is 1e-5)\n        \n    Returns:\n    np.ndarray\n        Layer normalized tensor rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled_shifted = gamma * X_normalized + beta\n    return np.round(X_scaled_shifted, decimals=5).tolist()"}
{"task_id": 109, "completion_id": 8, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n    \n    Layer Normalization normalizes the input X across the last dimension (features) for each example in the batch\n    and then applies scaling and shifting.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, num_features).\n    gamma (np.ndarray): Scaling parameter of shape (num_features,).\n    beta (np.ndarray): Shifting parameter of shape (num_features,).\n    epsilon (float): A small constant for numerical stability. Default is 1e-5.\n    \n    Returns:\n    list: A list containing the normalized input tensor X, rounded to 5 decimal places.\n    \"\"\"\n    mean = X.mean(axis=-1, keepdims=True)\n    var = X.var(axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = X_norm * gamma + beta\n    return np.round(X_scaled, decimals=5).tolist()"}
{"task_id": 109, "completion_id": 9, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Performs layer normalization over the last dimension (features) of the input tensor X,\n    then apply scaling and shifting using gamma and beta parameters.\n    \n    Parameters:\n    X: Input data of shape (batch_size, sequence_length, feature_dim)\n    gamma: Scaling parameter of shape (feature_dim,)\n    beta: Shifting parameter of shape (feature_dim,)\n    epsilon: Small constant added to the variance to avoid division by zero.\n    \n    Returns:\n    A numpy array of normalized and transformed values of shape (batch_size, sequence_length, feature_dim)\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_out = gamma * X_norm + beta\n    return np.round(X_out, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculates the METEOR score for a candidate translation against a reference.\n    \n    :param reference: A string representing the reference translation.\n    :param candidate: A string representing the candidate translation.\n    :param alpha: A parameter to control the weight of brevity penalty.\n    :param beta: A parameter to control the recall adjustment.\n    :param gamma: Brevity penalty parameter.\n    :return: The METEOR score as a float value rounded to 3 decimal places.\n    \"\"\"\n    reference_words = reference.split()\n    candidate_words = candidate.split()\n    ref_word_counts = Counter(reference_words)\n    can_word_counts = Counter(candidate_words)\n    matching_words = set(ref_word_counts.keys()) & set(can_word_counts.keys())\n    precision = sum((min(can_word_counts[w], ref_word_counts[w]) for w in matching_words)) / max(len(candidate_words), 1)\n    recall = sum((min(can_word_counts[w], ref_word_counts[w]) for w in matching_words)) / max(len(reference_words), 1)\n    precision_recall_sum = precision + recall\n    f_mean = (1 + beta ** 2) * precision * recall / max(precision_recall_sum, beta ** 2 * precision + recall)\n    brevity_penalty = np.exp(1 - len(reference_words) / (len(candidate_words) + 1e-13)) if len(candidate_words) < len(reference_words) else 1\n    meteor = (1 - alpha) * f_mean + alpha * brevity_penalty\n    if brevity_penalty == 0:\n        meteor *= np.exp(-gamma * (len(candidate_words) + 1e-13) ** 2 / len(reference_words) ** 2)\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"Calculate the harmonic mean for given precision, recall values and beta.\"\"\"\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall\n    return numerator / denominator if denominator != 0 else 0\ndef fragmentation_penalties(reference, candidate):\n    \"\"\"Calculate penalties based on word order fragmentation between reference and candidate.\"\"\"\n    ref_split = reference.split()\n    can_split = candidate.split()\n    ref_counts = Counter(zip(ref_split, ref_split[1:]))\n    can_counts = Counter(zip(can_split, can_split[1:]))\n    fragment_cost = len(ref_counts) + len(can_counts) - 2 * sum((min(ref_counts[item], can_counts[item]) for item in set(ref_counts) & set(can_counts)))\n    return fragment_cost\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"Calculate the METEOR score between a reference and candidate translation.\"\"\"\n    (ref_words, can_words) = (reference.split(), candidate.split())\n    match_count = sum((Counter(ref_words) & Counter(can_words)).values())\n    precision = match_count / len(can_words)\n    recall = match_count / len(ref_words)\n    harmonic = harmonic_mean(precision, recall, beta)\n    frag_penalties = gamma * fragmentation_penalties(reference, candidate)\n    (mean_match, not_match) = (np.maximum(len(ref_words), len(can_words)), np.minimum(len(ref_words), len(can_words)))\n    total_length_diff = mean_match - not_match\n    final_score = (1 - alpha) * harmonic + alpha * (1 - (frag_penalties + total_length_diff) / mean_match)\n    return round(final_score, 3)"}
{"task_id": 110, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef unzip_ngrams(ngrams, level):\n    \"\"\"\n    Helper function to unzip n-grams.\n    \"\"\"\n    return [tuple((x[level] for x in ngrams))]\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_ngrams = Counter(zip(*[reference[i:] for i in range(4)]))\n    cand_ngrams = Counter(zip(*[candidate[i:] for i in range(4)]))\n    matching_ngrams = ref_ngrams & cand_ngrams\n    prec_denom = max(1, sum(cand_ngrams.values()))\n    prec = sum(matching_ngrams.values()) / prec_denom\n    recall_denom = max(1, sum(ref_ngrams.values()))\n    recall = sum(matching_ngrams.values()) / recall_denom\n    if prec + recall == 0:\n        fmean = 0\n    else:\n        fmean = (1 + beta ** 2) * (prec * recall) / (beta ** 2 * prec + recall)\n    ref_matched = [x[0] for x in list(matching_ngrams.elements())]\n    cand_matched = [x[1] for x in list(matching_ngrams.elements())]\n    reordered_ref = []\n    for elem in ref_matched:\n        reordered_ref.extend(unzip_ngrams([elem], 0))\n    reordered_cand = []\n    for elem in cand_matched:\n        reordered_cand.extend(unzip_ngrams([elem], 1))\n    matches_diff = Counter(reordered_ref) - Counter(reordered_cand)\n    penalty = gamma * sum(matches_diff.values()) / len(candidate)\n    meteor = max(0, (1 - penalty) * fmean)\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Computes the METEOR score given a reference translation and a candidate translation.\n    \n    Parameters:\n    reference (str): The reference translation string.\n    candidate (str): The candidate translation string.\n    alpha (float): Weight factor for the harmonic mean between precision and recall.\n    beta (float): Parameter to control the importance of fragmentation penalty.\n    gamma (float): Parameter that controls the importance of the brevity penalty.\n    \n    Returns:\n    float: The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    ref_word_count = Counter(reference.split())\n    can_word_count = Counter(candidate.split())\n    matches = sum((ref_word_count & can_word_count).values())\n    recall = matches / len(reference.split())\n    precision = matches / len(candidate.split())\n    harmonic_mean = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    exact_match_positions_ref = [i for (i, word) in enumerate(reference.split()) if word in candidate.split()]\n    exact_match_positions_can = [i for (i, word) in enumerate(candidate.split()) if word in reference.split()]\n    exact_match_positions = sorted(set(exact_match_positions_ref) & set(exact_match_positions_can))\n    fragmentation = 0 if len(exact_match_positions) <= 1 else (len(exact_match_positions) - 1) / sum(np.diff(exact_match_positions))\n    fragmentation_penalty = 2 ** (-beta * fragmentation)\n    if len(candidate.split()) > len(reference.split()):\n        brevity_penalty = 1\n    else:\n        brevity_penalty = np.exp(1 - gamma * len(reference.split()) / len(candidate.split()))\n    meteor = brevity_penalty * fragmentation_penalty * harmonic_mean ** alpha\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for evaluating machine translation quality.\n  \n    Parameters:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float): The weight of the fragmentation penalty.\n    - beta (float): The trade-off parameter for precision and recall.\n    - gamma (float): The \"minimum recall penalty\".\n  \n    Returns:\n    - float: The computed METEOR score, rounded to 3 decimal places.\n    \"\"\"\n\n    def fmean(precision, recall):\n        if precision + recall:\n            return (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n        else:\n            return 0.0\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    counter = Counter(ref_words)\n    match_count = sum((Counter(cand_words) & counter).values())\n    precision = match_count / len(cand_words)\n    recall = match_count / len(ref_words)\n    fmean_score = fmean(precision, recall)\n    num_ref_words = len(ref_words)\n    penalty = 0\n    if recall < gamma:\n        penalty = 1\n    else:\n        cand_groups = [list(group) for (key, group) in itertools.groupby(cand_words)]\n        ref_groups = [list(group) for (key, group) in itertools.groupby(ref_words)]\n        num_cand_groups = sum([len(g) ** 2 / len(ref_words) for g in cand_groups if g[0] in ref_words])\n        num_ref_groups = sum([len(g) ** 2 / len(ref_words) for g in ref_groups])\n        penalty = num_ref_groups / (num_ref_groups + num_cand_groups / 2)\n    meteor = (1 - alpha) * fmean_score + alpha * penalty\n    meteor = 0 if fmean_score == 0 else meteor\n    meteor = min(meteor, 1.0)\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\ndef calc_stats(reference, candidate):\n    \"\"\"\n    Calculate the number of matches between reference and candidate.\n    :param reference: a list of tokens in the reference translation\n    :param candidate: a list of tokens in the candidate translation\n    :return: tuple (matches, excess_candidate_tokens, excess_reference_tokens)\n    \"\"\"\n    ref_counts = Counter(reference)\n    cand_counts = Counter(candidate)\n    matches = sum((ref_counts & cand_counts).values())\n    excess_candidate = sum((cand_counts - ref_counts).values())\n    excess_reference = sum((ref_counts - cand_counts).values())\n    return (matches, excess_candidate, excess_reference)\ndef compute_score(matches, excess, total):\n    \"\"\"\n    Computes precision, recall, and the F-score.\n    :param matches: number of matching tokens\n    :param excess: number of excess tokens\n    :param total: total number of reference (or candidate) tokens\n    :return: tuple (precision, recall, f_score)\n    \"\"\"\n    precision = matches / (matches + excess) if matches + excess > 0 else 0\n    recall = matches / total if total > 0 else 0\n    f_score = precision * recall / (beta ** 2 * precision + recall) * (1 + beta ** 2) if precision > 0 and recall > 0 else 0\n    return (precision, recall, f_score)\ndef penalty(ref_len, cand_len):\n    \"\"\"\n    Compute the penalty for word order fragmentation.\n    :param ref_len: reference translation length\n    :param cand_len: candidate translation length\n    :return: penalty score\n    \"\"\"\n    penalty = ((ref_len - cand_len) / ref_len) ** gamma if cand_len > ref_len else 0\n    return penalty\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score given the reference and candidate translations.\n    :param reference: a list of tokens in the reference translation\n    :param candidate: a list of tokens in the candidate translation\n    :param alpha: parameter controlling the importance of fragmentation penalty\n    :param beta: parameter for the F-score computation\n    :param gamma: parameter for the penalty computation\n    :return: meteor score as a float rounded to 3 decimal places\n    \"\"\"\n    ref_len = len(reference)\n    cand_len = len(candidate)\n    (matches, excess_cand, excess_ref) = calc_stats(reference, candidate)\n    (_, _, f_score) = compute_score(matches, excess_cand, ref_len)\n    frags_penalty = penalty(ref_len, cand_len)\n    meteor = (1 - alpha) * f_score / (1 + alpha * frags_penalty) if 1 + alpha * frags_penalty > 0 else 0\n    return round(meteor, 3)\nreference = 'the cat sat on the mat'.split()\ncandidate = 'the cat is sitting on the mat'.split()"}
{"task_id": 110, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\ndef unigram_match_score(reference, candidate):\n    \"\"\"Calculate the number of matching unigrams between the reference and candidate sentences.\"\"\"\n    reference_counter = Counter(reference)\n    candidate_counter = Counter(candidate)\n    match_counts = sum((reference_counter & candidate_counter).values())\n    return match_counts\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"Calculate the harmonic mean of precision and recall with a given beta.\"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\ndef penalty(candidate, reference):\n    \"\"\"Calculate the fragmentation penalty for the candidate sentence.\"\"\"\n    candidate_length = len(candidate)\n    reference_length = len(reference)\n    diff = abs(reference_length - candidate_length)\n    penalty_factor = 1 - np.exp(-1.0 * diff / reference_length)\n    return penalty_factor\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score for evaluating machine translation quality.\n    \n    :param reference: (list of str) The list of reference words.\n    :param candidate: (list of str) The list of candidate words.\n    :param alpha: (float) The weight parameter for penalty.\n    :param beta: (float) The parameter for recall importance in harmonic mean.\n    :param gamma: (float) The parameter for fragmentation penalty.\n    :return: (float) The METEOR score rounded to 3 decimal places.\n    \"\"\"\n    reference = reference.split()\n    candidate = candidate.split()\n    matches = unigram_match_score(reference, candidate)\n    precision = matches / len(candidate)\n    recall = matches / len(reference)\n    f_mean = harmonic_mean(precision, recall, beta)\n    fragmentation_penalty = penalty(candidate, reference)\n    if matches < f_mean:\n        fragmentation_penalty = fragmentation_penalty / gamma\n    meteor = (1 - alpha) * f_mean * (1 - fragmentation_penalty)\n    meteor += alpha * f_mean\n    return round(meteor, 3)\nreference = 'the little cat is sleeping in the sun'\ncandidate = 'the little cat is on the mat'"}
{"task_id": 110, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR Score for a candidate translation against a reference translation.\n    \n    Parameters:\n    - reference: The reference translation (string).\n    - candidate: The candidate translation (string).\n    - alpha: Weight for determining the importance of precision-recall F-mean vs. fragmentation penalty.\n    - beta: Determines how much emphasis is put on recall vs. precision in the F-mean.\n    - gamma: Weight for fragmentation penalty.\n    \n    Returns:\n    - A float value representing the METEOR Score, rounded to 3 decimal places.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    can_tokens = candidate.lower().split()\n    ref_count = Counter(ref_tokens)\n    can_count = Counter(can_tokens)\n    matches = sum((ref_count & can_count).values())\n    if matches == 0:\n        return 0.0\n    precision = matches / len(can_tokens)\n    recall = matches / len(ref_tokens)\n    fmean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    ref_positions = {token: i for (i, token) in enumerate(ref_tokens) if token in can_tokens}\n    can_positions = {token: i for (i, token) in enumerate(can_tokens) if token in ref_tokens}\n    ordered_positions = [can_positions[token] for token in ref_tokens if token in ref_positions]\n    inversions = sum((1 for i in range(len(ordered_positions) - 1) for j in range(i + 1, len(ordered_positions)) if ordered_positions[j] < ordered_positions[i]))\n    fragmentation_penalty = gamma ** inversions\n    meteor = (1 - alpha) * fmean + alpha * fragmentation_penalty\n    return round(meteor, 3)"}
{"task_id": 110, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"Calculates the harmonic mean based on precision, recall and beta.\"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\ndef penalty(length_ref, length_cand, fragment_count):\n    \"\"\"Calculates the fragmentation penalty.\"\"\"\n    if length_ref == length_cand:\n        return 1.0\n    length_diff = abs(length_ref - length_cand)\n    fragmentation = (length_diff + length_cand) / length_cand\n    alignment = (fragment_count + 1) / fragment_count\n    return 1 / (fragmentation * alignment)\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    ref_list = reference.split()\n    cand_list = candidate.split()\n    ref_counter = Counter(ref_list)\n    cand_counter = Counter(cand_list)\n    matches = sum((ref_counter & cand_counter).values())\n    precision = matches / len(cand_list)\n    recall = matches / len(ref_list)\n    fm = harmonic_mean(precision, recall, beta)\n    fragment_count = sum((1 for word in cand_list if word in ref_counter and ref_counter[word] > 0))\n    r = penalty(len(ref_list), len(cand_list), fragment_count)\n    score = (1 - alpha) * fm + alpha * r\n    score *= 1 - np.exp(-1 * gamma * matches)\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"Calculate the METEOR score for the translation.\n\n    :param reference: a str of the reference translation\n    :param candidate: a str of the candidate translation\n    :param alpha: parameter that balances precision and recall, defaults to 0.9\n    :param beta: parameter that controls the degree of fragmentation penalty, defaults to 3\n    :param gamma: parameter that controls the weight of fragmentation penalty, defaults to 0.5\n    :return: a float score rounded to 3 decimal places\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    match_count = sum((Counter(ref_words) & Counter(cand_words)).values())\n    if match_count == 0:\n        return 0.0\n    precision = match_count / len(cand_words)\n    recall = match_count / len(ref_words)\n    f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    matched_count = 0\n    total_count = 0\n    for i in range(1, min(len(ref_words), len(cand_words)) + 1):\n        for ref_chunk in set(zip(*[ref_words[j:] for j in range(i)])):\n            for cand_chunk in set(zip(*[cand_words[j:] for j in range(i)])):\n                if ref_chunk == cand_chunk:\n                    matched_count += 1\n        total_count += min(len(ref_words), len(cand_words)) - i + 1\n    fragmentation = matched_count / total_count\n    fragmentation_discount = 1 - gamma if fragmentation > gamma else 0\n    meteor = (1 - alpha) * fragmentation_discount * f_mean\n    meteor += alpha * (fragmentation_discount + f_mean)\n    meteor = min(meteor, 1)\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence counts of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts (int): The joint occurrence count of event X and event Y.\n    - total_counts_x (int): The total occurrences of event X.\n    - total_counts_y (int): The total occurrences of event Y.\n    - total_samples (int): The total number of samples observed.\n\n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0:\n        return float('-inf')\n    p_x_y = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_x_y / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 5\ntotal_counts_x = 12\ntotal_counts_y = 25\ntotal_samples = 1000"}
{"task_id": 111, "completion_id": 1, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for given joint counts, individual counts,\n    and total number of samples.\n\n    Parameters:\n    joint_counts (int): The count of joint occurrences of the two events.\n    total_counts_x (int): The total count of occurrences of event x.\n    total_counts_y (int): The total count of occurrences of event y.\n    total_samples (int): The total number of samples observed.\n\n    Returns:\n    float: The computed PMI rounded to 3 decimal points.\n    \n    Note: PMI is defined as log(P(x, y) / (P(x)P(y))), \n          where P(x, y) is the probability of x and y occurring together, \n          and P(x)P(y) is the product of probabilities of x and y occurring individually.\n          This assumes that joint_counts, total_counts_x, and total_counts_y are positive.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 2, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Args:\n    joint_counts (int): The joint occurrence count of event x and y.\n    total_counts_x (int): The total number of occurrences of event x.\n    total_counts_y (int): The total number of occurrences of event y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The computed PMI rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0:\n        return 0.0\n    prob_joint = joint_counts / total_samples\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n    pmi = np.log2(prob_joint / (prob_x * prob_y))\n    return round(pmi, 3)\njoint_counts = 25\ntotal_counts_x = 100\ntotal_counts_y = 125\ntotal_samples = 1000"}
{"task_id": 111, "completion_id": 3, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    This function computes the Pointwise Mutual Information (PMI) for two events.\n    PMI measures the deviation from independence of the two events.\n    \n    Parameters:\n    joint_counts (int): The count of the joint occurrence of the two events.\n    total_counts_x (int): The count of the first event's occurrence.\n    total_counts_y (int): The count of the second event's occurrence.\n    total_samples (int): The total number of samples observed.\n\n    Returns:\n    float: PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return float('-inf')\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 4, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI) given joint and individual counts.\n    \n    Parameters:\n    - joint_counts (int): The joint occurrence count of two events.\n    - total_counts_x (int): The total occurrence count of event x.\n    - total_counts_y (int): The total occurrence count of event y.\n    - total_samples (int): The total number of samples considered.\n    \n    Returns:\n    - float: The PMI value, rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_probability = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_probability = joint_counts / total_samples\n    epsilon = 1e-10\n    pmi_value = np.log2((actual_joint_probability + epsilon) / (expected_joint_probability + epsilon))\n    return round(pmi_value, 3)"}
{"task_id": 111, "completion_id": 5, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count \n    of two events, their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The number of times x and y occur together.\n    total_counts_x (int): The total number of occurrences of x.\n    total_counts_y (int): The total number of occurrences of y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The computed PMI rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return float('-inf')\n    pmi = np.log2(joint_counts * total_samples / (total_counts_x * total_counts_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 6, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of \n    two events, their individual counts, and the total number of samples.\n    \n    :param joint_counts: int, the count of joint occurrences of event x and event y\n    :param total_counts_x: int, the total count of occurrences of event x\n    :param total_counts_y: int, the total count of occurrences of event y\n    :param total_samples: int, the total number of samples\n    :return: float, PMI value rounded to 3 decimal places\n    \"\"\"\n    if joint_counts == 0:\n        return float('-inf')\n    expected_joint = total_counts_x * total_counts_y / total_samples\n    if expected_joint == 0:\n        return float('-inf')\n    pmi = np.log2(joint_counts / expected_joint)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 7, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for two events.\n    \n    PMI is defined as: PMI(x, y) = log(P(x, y) / (P(x)P(y))), with\n    P(x, y) = joint_counts / total_samples\n    P(x) = total_counts_x / total_samples\n    P(y) = total_counts_y / total_samples\n    \n    :param joint_counts: int, the joint occurrence count of two events\n    :param total_counts_x: int, the total number of occurrences of the first event\n    :param total_counts_y: int, the total number of occurrences of the second event\n    :param total_samples: int, the total number of samples\n    :return: float, PMI value rounded to 3 decimal places\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return -np.inf\n    p_x_y = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi_value = np.log2(p_x_y / (p_x * p_y))\n    return round(pmi_value, 3)\njoint_counts = 100\ntotal_counts_x = 200\ntotal_counts_y = 250\ntotal_samples = 1000"}
{"task_id": 111, "completion_id": 8, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) for two events given their joint and individual occurrence counts.\n    \n    Parameters:\n        joint_counts (int): The count of joint occurrences of both events.\n        total_counts_x (int): The count of occurrences of event X.\n        total_counts_y (int): The count of occurrences of event Y.\n        total_samples (int): The total number of samples.\n        \n    Returns:\n        float: The PMI rounded to 3 decimal places.\n    \"\"\"\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    if p_xy == 0:\n        return float('-inf')\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 9, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI) between two events.\n\n    :param joint_counts: The number of times the two events occur together.\n    :param total_counts_x: The total number of occurrences of the first event.\n    :param total_counts_y: The total number of occurrences of the second event.\n    :param total_samples: The total number of samples.\n    :return: The PMI rounded to 3 decimal places.\n    \"\"\"\n    probability_x = total_counts_x / total_samples\n    probability_y = total_counts_y / total_samples\n    probability_xy = joint_counts / total_samples\n    if probability_x == 0 or probability_y == 0 or probability_xy == 0:\n        return float('-inf')\n    pmi = np.log2(probability_xy / (probability_x * probability_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max normalization on a list of integers.\n\n    Args:\n    x (list of int): The list of integers to normalize.\n\n    Returns:\n    list of float: A new list where each number is scaled to the range [0, 1].\n    \"\"\"\n    if not x or max(x) == min(x):\n        return [0.0] * len(x)\n    max_val = max(x)\n    min_val = min(x)\n    return [round((i - min_val) / (max_val - min_val), 4) for i in x]"}
{"task_id": 112, "completion_id": 1, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Normalizes the input list of integers to the range [0, 1] using the Min-Max scaling technique.\n    \n    Args:\n    x (list of int): The input list of integers.\n    \n    Returns:\n    list of float: A list containing the normalized values rounded to 4 decimal places.\n    \"\"\"\n    if len(x) < 2 or max(x) == min(x):\n        return [0.0] * len(x)\n    min_val = min(x)\n    max_val = max(x)\n    return [round((i - min_val) / (max_val - min_val), 4) for i in x]"}
{"task_id": 112, "completion_id": 2, "solution": "import numpy as np\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, \n    scaling all values to the range [0, 1].\n    The function returns a list of floats rounded to 4 decimal places.\n    \"\"\"\n    if not x or len(set(x)) == 1:\n        raise ValueError('List must contain at least two different values.')\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(i - min_val) / (max_val - min_val) for i in x]\n    return [round(n, 4) for n in normalized]"}
{"task_id": 112, "completion_id": 3, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    :param x: A list of integers.\n    :return: A list of floats where each element is normalized to the range [0, 1].\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(n, 4) for n in normalized]"}
{"task_id": 112, "completion_id": 4, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    Each value is scaled to a range [0, 1] based on the formula:\n        x' = (x - min(x)) / (max(x) - min(x))\n    where 'x' is the original value, and 'x'' is the scaled value.\n    The function returns a list of floats rounded to 4 decimal places.\n    \n    :param x: List of integers to be normalized\n    :return: List of normalized floats rounded to 4 decimal places\n    \"\"\"\n    if not x or min(x) == max(x):\n        raise ValueError('List should contain at least two different numbers.')\n    (min_val, max_val) = (min(x), max(x))\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 112, "completion_id": 5, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    The values returned are rounded to 4 decimal places to ensure consistency.\n    \n    :param x: List of integers representing the features.\n    :return: A list of floats where each value is scaled to the range [0, 1] and rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0] * len(x)\n    normalized = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    normalized_rounded = [round(i, 4) for i in normalized]\n    return normalized_rounded"}
{"task_id": 112, "completion_id": 6, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    :param x: List of integer feature values.\n    :return: List of normalized float values, rounded to 4 decimal places.\n    \"\"\"\n    if not x or len(x) == 1:\n        return x\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized_x = [(point - min_val) / (max_val - min_val) for point in x]\n    return [round(norm_x, 4) for norm_x in normalized_x]"}
{"task_id": 112, "completion_id": 7, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    Args:\n    x (list[int]): The list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats that has been min-max normalized rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0] * len(x)\n    normalized_list = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    return [round(el, 4) for el in normalized_list]"}
{"task_id": 112, "completion_id": 8, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    This function takes a list of integers and performs Min-Max Normalization on them,\n    scaling all values to the range [0, 1].\n    \n    Parameters:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of floats, each rounded to 4 decimal places, representing the normalized values.\n    \"\"\"\n    if not x or len(x) < 2:\n        raise ValueError('Input list must contain at least two elements.')\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        raise ValueError('All elements in the list are the same; cannot perform normalization.')\n    norm_x = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in norm_x]"}
{"task_id": 112, "completion_id": 9, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Performs Min-Max normalization on a list of integers, scaling all values to the range [0, 1].\n    Each feature value is transformed to (value - min_feature_value) / (max_feature_value - min_feature_value)\n    Returns a list of floats rounded to 4 decimal places.\n    \"\"\"\n    if not x or max(x) == min(x):\n        return [0.0] * len(x)\n    max_val = max(x)\n    min_val = min(x)\n    normalized = [float(i - min_val) / (max_val - min_val) for i in x]\n    return [round(n, 4) for n in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implement a simple residual block using NumPy.\n    Args:\n    - x (np.ndarray): The input 1D array.\n    - w1 (np.ndarray): The first weight matrix. Shape should match the input size.\n    - w2 (np.ndarray): The second weight matrix. Should be square with the same size as w1.\n    \n    Returns:\n    - np.ndarray: The output after the residual block applied, rounded to 4 decimal places.\n    \"\"\"\n    relu_1 = np.maximum(0, w1 @ x)\n    relu_2 = np.maximum(0, w2 @ relu_1)\n    output = np.maximum(0, relu_2 + x)\n    return output.round(4).tolist()"}
{"task_id": 113, "completion_id": 1, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Applies a simple residual block with a shortcut connection on the input x.\n    \n    Parameters:\n    - x: np.ndarray, the input array. Should be 1D.\n    - w1: np.ndarray, the weight matrix for the first layer.\n    - w2: np.ndarray, the weight matrix for the second layer.\n    \n    Returns:\n    - output: list, the transformed output after applying the residual block,\n      rounded to 4 decimal places.\n    \"\"\"\n    x = x.reshape(-1, 1)\n    z1 = np.dot(w1, x)\n    a1 = np.maximum(z1, 0)\n    z2 = np.dot(w2, a1)\n    a2 = np.maximum(z2, 0)\n    z3 = a2 + x\n    a3 = np.maximum(z3, 0)\n    return np.round(a3.flatten(), 4).tolist()\nx = np.array([1, 2, 3])\nw1 = np.array([[0.2, 0.3, 0.5], [0.1, 0.4, 0.5], [0.6, 0.2, 0.2]])\nw2 = np.array([[0.7, 0.2, 0.1], [0.2, 0.5, 0.3], [0.1, 0.4, 0.5]])"}
{"task_id": 113, "completion_id": 2, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implement a simple residual block using NumPy.\n    \n    The block takes an input array 'x', processes it through two weight layers,\n    applies ReLU activations, and adds the original input via a shortcut connection\n    before a final ReLU activation.\n    \n    Parameters:\n    x : np.ndarray\n        The input array to the residual block.\n    w1 : np.ndarray\n        The weight matrix for the first layer.\n    w2 : np.ndarray\n        The weight matrix for the second layer.\n    \n    Returns:\n    np.ndarray\n        The output array of the residual block.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    x1 = relu(x.dot(w1))\n    x2 = x1.dot(w2)\n    output = relu(x2 + x)\n    return output.round(4).tolist()"}
{"task_id": 113, "completion_id": 3, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Parameters:\n    - x: Input numpy array of shape (n_features, 1)\n    - w1: Weight matrix for first layer. Shape should be compatible with x for matrix multiplication.\n    - w2: Weight matrix for second layer, with shape compatible with the output of the first layer.\n    \n    Returns:\n    - Output numpy array after passing through the residual block, rounded to 4 decimal places, as a list.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    y = relu(x @ w1)\n    y = y @ w2\n    y += x\n    y = relu(y)\n    return y.round(4).tolist()\nn_features = 5\nx = np.random.rand(n_features, 1)\nw1 = np.random.rand(n_features, n_features)\nw2 = np.random.rand(n_features, n_features)"}
{"task_id": 113, "completion_id": 4, "solution": "import numpy as np\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    return np.maximum(0, x)\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Create a simple residual block with shortcut connection.\n    \n    Args:\n    x (np.ndarray): Input array.\n    w1 (np.ndarray): The first weight matrix.\n    w2 (np.ndarray): The second weight matrix.\n    \n    Returns:\n    list: The output list after passing through the residual block rounded to 4 decimal places.\n    \"\"\"\n    y = np.matmul(x, w1)\n    y = relu(y)\n    y = np.matmul(y, w2)\n    y = relu(y)\n    y = x + y\n    y = relu(y)\n    y = np.round(y, 4)\n    return y.tolist()"}
{"task_id": 113, "completion_id": 5, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Implements a simple residual block with ReLU activations and a shortcut connection.\n    \n    Args:\n    - x (np.ndarray): 1D input array for the residual block.\n    - w1 (np.ndarray): Weight matrix for the first layer.\n    - w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n    - np.ndarray: Output of the residual block after applying two weight layers,\n      ReLU activations, and adding the input via a shortcut connection before\n      a final ReLU activation. The output is rounded to 4 decimal places.\n    \"\"\"\n    hidden = np.maximum(0, x @ w1)\n    output = np.maximum(0, hidden @ w2) + x\n    output = np.maximum(0, output)\n    output = np.round(output, decimals=4)\n    return output"}
{"task_id": 113, "completion_id": 6, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with shortcut connections using numpy.\n    \n    Args:\n    x (np.ndarray): Input array of shape (n_features, ).\n    w1 (np.ndarray): First weight matrix of shape (n_features, n_features).\n    w2 (np.ndarray): Second weight matrix of shape (n_features, n_features).\n    \n    Returns:\n    list: Output list after passing through the residual block.\n    \n    The output will be the result of a ReLU on the sum of x and the double linear transformation (ReLU(w2 * ReLU(w1 * x)))\n    of x, rounded to 4 decimal places.\n    \"\"\"\n    assert x.shape[0] == w1.shape[0] == w1.shape[1] == w2.shape[0] == w2.shape[1], 'Dimensions are not compatible for operations.'\n    input_transformed_1 = np.maximum(0, np.dot(x, w1))\n    input_transformed_2 = np.maximum(0, np.dot(input_transformed_1, w2))\n    output_before_relu = x + input_transformed_2\n    output_after_relu = np.maximum(0, output_before_relu)\n    output_rounded = np.round(output_after_relu, decimals=4).tolist()\n    return output_rounded\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])"}
{"task_id": 113, "completion_id": 7, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block using NumPy. The block processes the input\n    through two weight layers with ReLU activations in between. Adds the initial input\n    at the end via a shortcut connection and applies a final ReLU activation.\n    \n    Args:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weight matrix for the first layer.\n    w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n    np.ndarray: The processed output as a NumPy array, rounded to 4 decimal places.\n    \"\"\"\n    if len(x.shape) == 1:\n        x = x.reshape(1, -1)\n    layer1_output = np.maximum(0, np.dot(x, w1))\n    layer2_output = np.maximum(0, np.dot(layer1_output, w2))\n    output = np.maximum(0, layer2_output + x)\n    return np.round(output, 4).tolist()\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]])\nw2 = np.array([[0.5], [0.6]])"}
{"task_id": 113, "completion_id": 8, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with a shortcut connection using NumPy.\n    \n    Parameters:\n    - x: 1D input array of shape (n_features,)\n    - w1: Weight array of shape (n_features, n_units)\n    - w2: Weight array of shape (n_units, n_features)\n    \n    Returns:\n    - The output of the residual block as a list of floats, rounded to 4 decimal places.\n    \"\"\"\n    assert len(x.shape) == 1\n    x = x.reshape(1, -1)\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(z1, 0)\n    z2 = np.dot(a1, w2)\n    z2_shortcut = z2 + x\n    out = np.maximum(z2_shortcut, 0)\n    return out.flatten().tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nw2 = np.array([[0.7, 0.8], [0.9, 1.0]])"}
{"task_id": 113, "completion_id": 9, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Parameters:\n    - x: np.ndarray, the input data.\n    - w1: np.ndarray, the first weight matrix.\n    - w2: np.ndarray, the second weight matrix.\n    \n    Returns:\n    - list: The output of the residual block as a list, after rounding to 4 decimal places.\n    \"\"\"\n    assert x.shape[0] == w1.shape[0], 'Input and first weight matrix dimensions do not match.'\n    assert w1.shape[1] == w2.shape[0], 'Weight matrix dimensions do not match.'\n    y1 = np.maximum(0, x @ w1)\n    y2 = y1 @ w2\n    y_shortcut = x @ w1\n    y_shortcut = y_shortcut @ w2\n    y_add = y2 + y_shortcut\n    y_output = np.maximum(0, y_add)\n    y_output = np.round(y_output, 4).tolist()\n    return y_output"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Applies Global Average Pooling on a 3D array of shape (height, width, channels).\n    \n    Parameters:\n    x: np.ndarray\n        Input array of shape (height, width, channels), where 'channels' represents the number of feature maps,\n        and height x width represents the size of each feature map.\n    \n    Returns:\n    np.ndarray\n        A 1D array of shape (channels,) where each element is the average of the corresponding feature map.\n    \"\"\"\n    pooled = np.mean(x, axis=(0, 1))\n    return pooled"}
{"task_id": 114, "completion_id": 1, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform global average pooling on a 3D NumPy array of shape (height, width, channels).\n    \n    Parameters:\n        x (np.ndarray): The input array of shape (height, width, channels).\n        \n    Returns:\n        np.ndarray: A 1D array of shape (channels,) containing the average of each channel.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 2, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Applies global average pooling over the inputs.\n    \n    Parameters:\n    x (numpy.ndarray): A 3-dimensional array of shape (height, width, channels)\n    \n    Returns:\n    numpy.ndarray: A 1-dimensional array of shape (channels,) with the global average \n                   pooling result.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 3, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    - x: np.ndarray of shape (height, width, channels) representing feature maps.\n    \n    Returns:\n    - np.ndarray of shape (channels): 1D array where each element is the average\n      value of the corresponding feature map in the input.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 4, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    :param x: A 3D NumPy array of shape (height, width, channels)\n    :return: A 1D NumPy array of shape (channels,), where each value is the\n             average of the respective feature map in the input array.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 5, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels) representing \n                    feature maps from a convolutional layer.\n                    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) containing the average of \n                all values in each corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 6, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array with shape (height, width, channels) representing feature maps from a convolutional layer.\n\n    Returns:\n    np.ndarray: A 1D array with shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 7, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D numpy array.\n    \n    :param x: A numpy array of shape (height, width, channels) representing input feature maps\n    :return: A 1D numpy array of shape (channels,) where each value is the average of respective feature map\n    \"\"\"\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 114, "completion_id": 8, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Apply Global Average Pooling on the given 3D array.\n    \n    :param x: A numpy array of shape (height, width, channels)\n    :return: A 1D numpy array of shape (channels,) containing the global average pooled values\n    \"\"\"\n    pooled_values = np.mean(x, axis=(0, 1))\n    return pooled_values"}
{"task_id": 114, "completion_id": 9, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform global average pooling on a 3D NumPy array of shape (height, width, channels).\n    \n    Args:\n    x: np.ndarray - 3D input array representing feature maps.\n    \n    Returns:\n    np.ndarray - 1D array of shape (channels,), containing the average of each feature map.\n    \"\"\"\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output\nfeature_map = np.random.rand(7, 7, 128)\npooled_output = global_avg_pool(feature_map)"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Applies batch normalization on a 4D numpy array in the BCHW format.\n    \n    Parameters:\n        X (np.ndarray): Input array of shape (B, C, H, W) where B is the batch size, C is the number of channels,\n                        H is the height, and W is the width of the feature maps.\n        gamma (np.ndarray): Scale parameters for each channel, array shape should be (C,).\n        beta (np.ndarray): Shift parameters for each channel, array shape should be (C,).\n        epsilon (float): Small value to add to the variance to prevent division by zero.\n    \n    Returns:\n        list: The batch-normalized array, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_norm * gamma + beta\n    return np.round(X_normalized, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 1, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs batch normalization on a 4D numpy array.\n    \n    Parameters:\n    X: Input array of shape (batch_size, channels, height, width).\n    gamma: Scale parameter array of shape (channels,).\n    beta: Shift parameter array of shape (channels,).\n    epsilon: Small constant for numerical stability.\n    \n    Returns:\n    A list representing the normalized array.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    x_norm = (X - mean) / np.sqrt(var + epsilon)\n    x_norm = gamma[np.newaxis, :, np.newaxis, np.newaxis] * x_norm + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(x_norm, 4).tolist()"}
{"task_id": 115, "completion_id": 2, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies Batch Normalization to a BCHW 4D NumPy array.\n    \n    Parameters:\n    X (np.ndarray): A 4D input NumPy array in BCHW format.\n    gamma (np.ndarray): Scale parameter for Batch Normalization, shape (C,) where C is the channel count.\n    beta (np.ndarray): Shift parameter for Batch Normalization, shape (C,) where C is the channel count.\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized input array rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    C = X.shape[1]\n    gamma_reshaped = gamma.reshape((1, C, 1, 1))\n    beta_reshaped = beta.reshape((1, C, 1, 1))\n    output = X_normalized * gamma_reshaped + beta_reshaped\n    return np.round(output, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 3, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Apply batch normalization to a 4D Numpy array input in BCHW format,\n    using the provided scale (gamma) and shift (beta) parameters.\n    Normalization is done across the batch and spatial dimensions for each channel.\n    \n    Parameters:\n    X: Input 4D array in BCHW format.\n    gamma: Scale parameter array for multiplication after normalization.\n    beta: Shift parameter array for addition after scaling.\n    epsilon: Small constant to avoid division by zero.\n    \n    Returns:\n    List: The result as a list after applying batch normalization, rounding to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    x_normalized = (X - mean) / np.sqrt(var + epsilon)\n    out = x_normalized * gamma + beta\n    return np.round(out, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 4, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    This function takes a 4D numpy array in BCHW format alongside gamma and beta parameters to\n    perform batch normalization. It normalizes the data across the batch and spatial dimensions\n    per channel and applies scaling and shifting.\n    \n    Parameters:\n    X (np.ndarray): Input array in BCHW format.\n    gamma (np.ndarray): Scale parameter.\n    beta (np.ndarray): Shift parameter.\n    epsilon (float): A small value to add to the standard deviation for numerical stability.\n    \n    Returns:\n    np.ndarray: The result of applying batch normalization on input X, rounded to 4 decimal places.\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_norm + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(out, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 5, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D numpy array in BCHW format (batch, channels, height, width).\n    \n    Args:\n    X: np.ndarray representing the input in BCHW format.\n    gamma: np.ndarray for scaling factor to be applied on the normalized features (per channel).\n    beta: np.ndarray for shifting factor to be applied on the normalized features (per channel).\n    epsilon: float value to prevent division by zero, ensures numerical stability.\n    \n    Returns:\n    np.ndarray in BCHW format after applying batch normalization.\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_out = X_norm * gamma.reshape(1, channels, 1, 1) + beta.reshape(1, channels, 1, 1)\n    return np.round(X_out, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 6, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform Batch Normalization on 4D input numpy array X with channel first format (BCHW).\n    \n    Parameters:\n    X: np.ndarray - A 4D input in BCHW format.\n    gamma: np.ndarray - Scale parameter. Its size must equal the number of channels.\n    beta: np.ndarray - Shift parameter. Its size must equal the number of channels.\n    epsilon: float - A small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: A 4D numpy array after batch normalization.\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_bn = X_norm * gamma + beta\n    return np.round(X_bn, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 7, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Performs Batch Normalization on a 4D NumPy array in BCHW format (batch, channels, height, width).\n    Each channel is normalized across the batch and spatial dimensions, then scaled by gamma and shifted by beta.\n    Args:\n    - X (np.ndarray): Input array in BCHW format.\n    - gamma (np.ndarray): Scale parameter array for each channel.\n    - beta (np.ndarray): Shift parameter array for each channel.\n    - epsilon (float, optional): Small value to add to the variance for numerical stability.\n\n    Returns:\n    - np.ndarray: Normalized, scaled, and shifted array rounded to 4 decimal places.\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_trans = gamma[:, np.newaxis, np.newaxis] * X_norm + beta[:, np.newaxis, np.newaxis]\n    return np.round(X_trans, 4).tolist()"}
{"task_id": 115, "completion_id": 8, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies batch normalization to the input tensor X with the given scale (gamma) and shift (beta) parameters.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch, channels, height, width).\n    gamma (np.ndarray): Scale parameter of shape (channels,).\n    beta (np.ndarray): Shift parameter of shape (channels,).\n    epsilon (float): Small value to prevent division by zero. Defaults to 1e-5.\n    \n    Returns:\n    np.ndarray: Normalized tensor with the same shape as X.\n    \"\"\"\n    mean = X.mean(axis=(0, 2, 3), keepdims=True)\n    var = X.var(axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    out = gamma[:, None, None] * X_hat + beta[:, None, None]\n    return np.round(out, decimals=4).tolist()"}
{"task_id": 115, "completion_id": 9, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Performs batch normalization on the input 4D array X.\n    \n    Parameters:\n    X: input array of shape (batch_size, channels, height, width).\n    gamma: scale parameter of shape (channels,).\n    beta: shift parameter of shape (channels,).\n    epsilon: a small value for numerical stability.\n    \n    Returns:\n    A list representing the normalized array rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    out = X_hat * gamma + beta\n    return np.round(out, 4).tolist()\nX = np.random.random((2, 3, 4, 4))\ngamma = np.random.random((3,))\nbeta = np.random.random((3,))"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c*x^n at the given point x.\n\n    Args:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which to evaluate the derivative.\n    n (float): Exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 1, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which the derivative is to be calculated.\n    n (float): Exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative of the term at x, rounded to 4 decimal places.\n    \"\"\"\n    if n == 0:\n        return 0\n    else:\n        derivative = c * n * x ** (n - 1)\n        return round(derivative, 4)"}
{"task_id": 116, "completion_id": 2, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 3, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at the point x.\n    \n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative of the polynomial term.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * pow(x, n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 4, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term c*x^n at a point x.\n\n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is calculated.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at point x, rounded to four decimal places.\n    \"\"\"\n    derivative_value = n * c * pow(x, n - 1) if n != 0 else 0\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 5, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term c * x^n at point x.\n    \n    :param c: Coefficient of the polynomial term.\n    :param x: Point at which to evaluate the derivative.\n    :param n: Exponent of the polynomial term.\n    :return: Value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 6, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term in the form of c*x^n at a point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative of the polynomial term at x.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 7, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = n * c * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 8, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of the polynomial term c * x^n at a given point x.\n    \n    :param c: Coefficient of the polynomial term\n    :param x: Point at which to evaluate the derivative\n    :param n: Exponent of the polynomial term\n    :return: Value of the derivative at point x, rounded to 4 decimal places\n    \"\"\"\n    derivative_val = n * c * x ** (n - 1)\n    return round(derivative_val, 4)"}
{"task_id": 116, "completion_id": 9, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which the derivative is evaluated.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative of the term at point x, \n           as a float rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by the given vectors\n    using the Gram-Schmidt process with a specified tolerance for linear independence.\n    \n    :param vectors: List of 2D vectors.\n    :param tol: Tolerance value for determining linear independence.\n    :return: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    V = np.array(vectors)\n    V = V.astype(complex)\n    (Q, R) = np.linalg.qr(V.T)\n    basis = []\n    for i in range(Q.shape[1]):\n        if np.linalg.norm(Q[:, i]) > tol:\n            (real_part, imag_part) = (np.real(Q[:, i]), np.imag(Q[:, i]))\n            if np.max(np.abs(imag_part)) < tol:\n                basis.append(real_part)\n            else:\n                basis.append(Q[:, i])\n    basis = [np.round(np.array(vector).real, 4).tolist() for vector in basis]\n    return basis"}
{"task_id": 117, "completion_id": 1, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    :param vectors: A list of 2D vectors represented as lists of floats.\n    :param tol: Tolerance value to determine linear independence.\n    :return: An orthonormal basis (a list of vectors) rounded to 4 decimal places.\n    \"\"\"\n    matrix = np.array(vectors).T\n    (q, r) = np.linalg.qr(matrix)\n    significant = np.abs(np.diagonal(r)) > tol\n    basis = q[:, significant]\n    basis = np.around(basis, decimals=4)\n    return [vec.tolist() for vec in basis.T]\nvectors = [[1, 1], [2, 2], [1, 0]]\nbasis = orthonormal_basis(vectors)"}
{"task_id": 117, "completion_id": 2, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Apply the Gram-Schmidt process on the given set of 2D vectors to obtain an \n    orthonormal basis for the subspace they span. The length of the vectors \n    output is the number of linearly independent vectors found within the \n    input list, given the tolerance.\n\n    :param vectors: A list of 2D vectors.\n    :param tol: Tolerance for considering vectors as linearly independent.\n    :return: A list of orthonormal vectors representing the basis.\n    \"\"\"\n    if not vectors:\n        return []\n    V = np.array(vectors).T\n    num_vectors = V.shape[1]\n    basis = []\n    for i in range(num_vectors):\n        v = V[:, i]\n        for b in basis:\n            v -= np.dot(v, b) * b\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            basis.append(v / norm)\n    orthonormal_basis_final = [np.round(b, 4).tolist() for b in basis]\n    return orthonormal_basis_final"}
{"task_id": 117, "completion_id": 3, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Given a list of 2D vectors, compute an orthonormal basis using the modified Gram-Schmidt process.\n    Vectors in the returned basis should be rounded to 4 decimal places.\n    \n    Args:\n    vectors: List of 2D vectors represented as lists.\n    tol: Tolerance value for determining linear independence.\n    \n    Returns:\n    List of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n    V = np.array(vectors).T\n    (m, n) = V.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n    for j in range(n):\n        v = V[:, j]\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i].T, V[:, j])\n            v = v - R[i, j] * Q[:, i]\n        norm_v = np.linalg.norm(v)\n        if norm_v > tol:\n            R[j, j] = norm_v\n            Q[:, j] = v / R[j, j]\n        else:\n            V[:, j] = np.zeros(V[:, j].shape)\n    orthonormal_vectors = [vec.tolist() for vec in Q.T if np.linalg.norm(vec) > tol]\n    orthonormal_vectors_rounded = [np.round(vec, 4).tolist() for vec in orthonormal_vectors]\n    return orthonormal_vectors_rounded"}
{"task_id": 117, "completion_id": 4, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute the orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    :param vectors: A list of 2D vectors [x, y].\n    :param tol: Tolerance value to determine linear independence among vectors.\n    :return: A list of orthonormal vectors spanning the given subspace.\n    \"\"\"\n    if not vectors:\n        return []\n    basis = np.array(vectors)\n    result_basis = []\n    for vector in basis:\n        for basis_vector in result_basis:\n            vector -= np.dot(vector, basis_vector) * basis_vector\n        if np.linalg.norm(vector) > tol:\n            result_basis.append(vector / np.linalg.norm(vector))\n    return [np.round(np.array(v), decimals=4).tolist() for v in result_basis]"}
{"task_id": 117, "completion_id": 5, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process.\n    This function modifies the basis vectors to be unit vectors and checks for linear independence based on tolerance.\n    \n    :param vectors: List of 2D vectors to span the subspace.\n    :param tol: Tolerance to cut off negligible values to prevent numerical errors.\n    :return: List of orthonormal vectors that span the same subspace.\n    \"\"\"\n    V = np.array(vectors)\n    (num_vectors, dim) = V.shape\n    Q = np.zeros((num_vectors, dim))\n    for i in range(num_vectors):\n        v = V[i]\n        for j in range(i):\n            q = Q[j]\n            v -= np.dot(q, v) * q\n        if np.linalg.norm(v) > tol:\n            Q[i] = v / np.linalg.norm(v)\n        else:\n            Q[i] = np.zeros(dim)\n    orthonormal_basis = [q.round(4).tolist() for q in Q if np.linalg.norm(q) > tol]\n    return orthonormal_basis"}
{"task_id": 117, "completion_id": 6, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Takes a list of 2D vectors and computes an orthonormal basis for the subspace they span\n    using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: A list of 2D vectors.\n    - tol: A tolerance value to determine linear independence.\n    \n    Returns:\n    - A list of lists containing the orthonormal basis vectors.\n    \"\"\"\n    basis = []\n    transformed = []\n    for v in vectors:\n        w = np.array(v)\n        for u in basis:\n            w -= np.dot(w, u) * np.array(u)\n        if np.linalg.norm(w) > tol:\n            basis.append(w / np.linalg.norm(w))\n    for b in basis:\n        transformed.append(np.round(b, 4).tolist())\n    return transformed\nvectors = [[1, 2], [2, 4], [1, 0]]"}
{"task_id": 117, "completion_id": 7, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Given a list of 2D vectors, apply the Gram-Schmidt process to compute an orthonormal basis\n    for their span. Vectors in the returned list will be rounded to 4 decimal places.\n    \"\"\"\n\n    def orthogonalize(vectors, tol):\n        \"\"\"Perform Gram-Schmidt process to orthogonalize the vectors.\"\"\"\n        ortho = []\n        for v in vectors:\n            v = np.array(v, dtype=float)\n            for u in ortho:\n                u = np.array(u, dtype=float)\n                v -= np.dot(v, u) / np.dot(u, u) * u\n            if np.linalg.norm(v) > tol:\n                ortho.append(v)\n        return ortho\n    ortho_vectors = orthogonalize(vectors, tol)\n    orthonormal = [v / np.linalg.norm(v) for v in ortho_vectors]\n    return [np.round(v, 4).tolist() for v in orthonormal]\nvectors = [[1, 2], [2, 4], [1, 0], [0, 1]]"}
{"task_id": 117, "completion_id": 8, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    Args:\n    vectors: List of 2D vectors represented as nested lists.\n    tol: Tolerance value to determine linear independence.\n    \n    Returns:\n    A list of orthonormal vectors that span the same subspace, each rounded to 4 decimal places.\n    \"\"\"\n    V = np.array(vectors).T\n    (m, n) = V.shape\n    Q = np.zeros((m, n))\n    for i in range(n):\n        q = V[:, i]\n        for j in range(i):\n            q -= q @ Q[:, j] * Q[:, j]\n        norm_q = np.linalg.norm(q)\n        if norm_q < tol:\n            raise ValueError('Linearly dependent vectors detected.')\n        Q[:, i] = q / norm_q\n    independent_vectors = min(m, n)\n    orthonormal_vectors = [Q[:, i].tolist() for i in range(independent_vectors)]\n    return np.round(orthonormal_vectors, 4).tolist()"}
{"task_id": 117, "completion_id": 9, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    Parameters:\n    vectors (List[List[float]]): A list of 2D vectors.\n    tol (float): A tolerance value to determine linear independence.\n    \n    Returns:\n    List[List[float]]: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    V = np.column_stack([np.array(v) for v in vectors])\n    (Q, _) = np.linalg.qr(V)\n    orthonormal_vectors = []\n    for i in range(Q.shape[1]):\n        if np.linalg.norm(Q[:, i] > tol):\n            orthonormal_vectors.append(np.round(Q[:, i], 4).tolist())\n    return orthonormal_vectors\nvectors = [[1.5, 3.2], [0.7, 1.6], [-2.1, -4.4]]\northonormal_vectors = orthonormal_basis(vectors)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple of float): The first vector.\n    b (list/tuple of float): The second vector.\n    \n    Returns:\n    list: The cross product of vectors a and b as a list of float values, rounded to 4 decimal places.\n    \"\"\"\n    result = np.cross(a, b)\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()"}
{"task_id": 118, "completion_id": 1, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    The output is rounded to 4 decimal places and converted to a list.\n    \n    Parameters:\n    a (list): A 3D vector represented as a list of 3 floating-point numbers.\n    b (list): Another 3D vector represented as a list of 3 floating-point numbers.\n    \n    Returns:\n    list: The cross product of the two vectors as a list, rounded to 4 decimal places.\n    \"\"\"\n    cross_prod = np.cross(a, b)\n    return np.round(cross_prod, 4).tolist()"}
{"task_id": 118, "completion_id": 2, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Parameters:\n    a (list or np.ndarray): The first 3D vector.\n    b (list or np.ndarray): The second 3D vector.\n\n    Returns:\n    list: The resultant vector, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    cross = np.cross(a, b)\n    cross = np.round(cross, 4)\n    return cross.tolist()"}
{"task_id": 118, "completion_id": 3, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): The first 3D vector.\n    b (list/tuple): The second 3D vector.\n    \n    Returns:\n    list: The resulting 3D vector after computing cross product, rounded to 4 decimal places and converted to list.\n    \"\"\"\n    a_array = np.array(a)\n    b_array = np.array(b)\n    cross_result = np.cross(a_array, b_array)\n    return np.round(cross_result, 4).tolist()"}
{"task_id": 118, "completion_id": 4, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (np.array): A numpy array representing the first 3D vector.\n    b (np.array): A numpy array representing the second 3D vector.\n    \n    Returns:\n    list: The cross product as a list rounded to 4 decimal places.\n    \"\"\"\n    cross = np.cross(a, b)\n    cross = np.round(cross, 4)\n    return cross.tolist()"}
{"task_id": 118, "completion_id": 5, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Calculates the cross product of two 3-dimensional vectors.\n\n    :param a: A list or numpy array of 3 floats representing the first vector.\n    :param b: A list or numpy array of 3 floats representing the second vector.\n    :return: A list representing the vector that is the cross product of a and b,\n             rounded to 4 decimal places.\n    \"\"\"\n    if len(a) != 3 or len(b) != 3:\n        raise ValueError('Both vectors must be of dimension 3.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 6, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): A 3D vector.\n    b (list or np.ndarray): Another 3D vector.\n    \n    Returns:\n    list: The cross product vector, a result perpendicular to both a and b.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    result = np.cross(a_np, b_np)\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()"}
{"task_id": 118, "completion_id": 7, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross_product_result = np.cross(a, b)\n    return np.around(cross_product_result, decimals=4).tolist()"}
{"task_id": 118, "completion_id": 8, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list): First 3D vector.\n    b (list): Second 3D vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cross_prod = np.cross(a_np, b_np)\n    cross_prod_rounded = np.round(cross_prod, 4)\n    return cross_prod_rounded.tolist()"}
{"task_id": 118, "completion_id": 9, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): First input vector, must be 3D.\n    b (list/tuple): Second input vector, must be 3D.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    result = np.cross(a_np, b_np)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's rule.\n    \n    Parameters:\n    A (numpy.ndarray): A square coefficient matrix.\n    b (numpy.ndarray): A constant vector.\n    \n    Returns:\n    list or int: A list of solutions rounded to the 4th decimal or -1 if no unique solution exists.\n    \"\"\"\n    n = len(A)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()"}
{"task_id": 119, "completion_id": 1, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Cramer's Rule.\n    Returns the solution vector x or -1 if there is no unique solution.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    n = len(b)\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        temp_A = A.copy()\n        temp_A[:, i] = b\n        det_col_A = np.linalg.det(temp_A)\n        x_i = det_col_A / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = [[3, 0, 2], [2, -1, 0], [0, 1, 1]]\nb = [2, 4, -1]"}
{"task_id": 119, "completion_id": 2, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n    \n    Returns:\n    list: A list of solution variables of the system of equations if unique solution exists.\n    -1: If the system of equations does not have a unique solution.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-07:\n        return -1\n    n = A.shape[0]\n    solutions = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        solutions.append(det_Ai / det_A)\n    return np.round(solutions, decimals=4).tolist()\nA = [[1, 2], [2, 4]]\nb = [3, 6]"}
{"task_id": 119, "completion_id": 3, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (numpy.ndarray): Square matrix of the coefficients.\n    b (numpy.ndarray): Constant vector of the system.\n    \n    Returns:\n    list: The solution vector x, rounded to 4 decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    n = len(A)\n    if A.shape[0] != A.shape[1] or b.size != n:\n        return -1\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        detAi = np.linalg.det(Ai)\n        xi = detAi / detA\n        x.append(round(xi, 4))\n    return np.array(x).tolist()\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])"}
{"task_id": 119, "completion_id": 4, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n        A (np.array): A square coefficient matrix.\n        b (np.array): A constant vector.\n    \n    Returns:\n        list or int: The solution vector x as a list if the system has a unique solution, \n                     otherwise -1 if the system has no or infinite solutions.\n    \"\"\"\n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    solution = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x_i = det_Ai / det_A\n        solution.append(round(x_i, 4))\n    return solution\nA = np.array([[3, 2], [-1, -2]])\nb = np.array([18, -2])\nsolution = cramers_rule(A, b)"}
{"task_id": 119, "completion_id": 5, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (List[List[float]]): A square coefficient matrix.\n    b (List[float]): A constant vector.\n    \n    Returns:\n    List[float]: The solution vector x; returns -1 if there's no unique solution.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(A)\n    x = []\n    for i in range(n):\n        temp_A = A.copy()\n        temp_A[:, i] = b\n        det_temp_A = np.linalg.det(temp_A)\n        x.append(round(det_temp_A / det_A, 4))\n    return np.array(x).tolist()\nA = [[3, 2, -4], [2, 3, 3], [5, -3, 1]]\nb = [3, 15, 14]"}
{"task_id": 119, "completion_id": 6, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (numpy.ndarray): A square coefficient matrix.\n    b (numpy.ndarray): A vector of the constant terms.\n    \n    Returns:\n    list: The solution vector x rounded to the nearest 4 decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        return -1\n    n = len(b)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(n):\n        A_i = np.copy(A)\n        A_i[:, i] = b\n        x_i = np.linalg.det(A_i) / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\nb = np.array([1, -2, 0])"}
{"task_id": 119, "completion_id": 7, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A: numpy.ndarray - Square coefficient matrix.\n    b: numpy.ndarray - Constant vector.\n    \n    Returns:\n    Solution vector (list) if a unique solution exists, otherwise returns -1.\n    \"\"\"\n    n = len(A)\n    detA = np.linalg.det(A)\n    if detA == 0:\n        return -1\n    solutions = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        detAi = np.linalg.det(Ai)\n        xi = detAi / detA\n        solutions.append(round(xi, 4))\n    return np.array(solutions).tolist()\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])"}
{"task_id": 119, "completion_id": 8, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's rule.\n    \n    Parameters:\n    A (np.array): A square matrix representing the coefficients of linear equations.\n    b (np.array): A 1D array representing the constant terms of linear equations.\n    \n    Returns:\n    list: The solution vector x (rounded to the nearest 4th decimal), if exists.\n          Returns -1 if the system has no unique solution.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-09:\n        return -1\n    n = len(A)\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        x_i = np.linalg.det(A_i) / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = np.array([[3, 1], [1, 2]])\nb = np.array([5, 4])"}
{"task_id": 119, "completion_id": 9, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.ndarray): A square coefficient matrix for the system.\n    b (np.ndarray): A constant vector.\n    \n    Returns:\n    list: The solution vector x as a list, or -1 if there is no unique solution.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    n = len(A)\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    x = []\n    for i in range(n):\n        A_i = np.copy(A)\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(np.round(x_i, 4))\n    return np.array(x).tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n    \n    Args:\n        p (list[float]): The first probability distribution.\n        q (list[float]): The second probability distribution.\n    \n    Returns:\n        float: The Bhattacharyya distance, rounded to 4 decimal places. Returns 0.0 if the inputs are invalid.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc = 0.0\n    for (pi, qi) in zip(p, q):\n        bc += np.sqrt(pi * qi)\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 1, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions p and q.\n    \n    Args:\n    p (List[float]): First probability distribution.\n    q (List[float]): Second probability distribution.\n    \n    Returns:\n    float: Bhattacharyya distance between p and q, rounded to 4 decimal places.\n           Returns 0.0 if the inputs have different lengths or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    if p == q:\n        return 0.0\n    bc = sum((np.sqrt(x * y) for (x, y) in zip(p, q)))\n    if bc == 0:\n        return 0.0\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 2, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    p = np.array(p) / np.sum(p)\n    q = np.array(q) / np.sum(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)\np = [0.1, 0.6, 0.1, 0.2]\nq = [0.3, 0.4, 0.2, 0.1]"}
{"task_id": 120, "completion_id": 3, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n    :param p: A list of floats representing the first probability distribution.\n    :param q: A list of floats representing the second probability distribution.\n    :return: The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if the input distributions are not valid.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc = sum((np.sqrt(a * b) for (a, b) in zip(p, q)))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 4, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions.\n    \n    Parameters:\n    p (list of float): The first probability distribution.\n    q (list of float): The second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if the\n           inputs are not of equal length or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = np.sum(np.sqrt(np.array(p) * np.array(q)))\n    return round(-np.log(bc), 4)"}
{"task_id": 120, "completion_id": 5, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates Bhattacharyya distance between two probability distributions p and q.\n    \n    Parameters:\n    p (list of float): A list representing the first probability distribution.\n    q (list of float): A list representing the second probability distribution.\n    \n    Returns:\n    float: Bhattacharyya distance rounded to 4 decimal places or 0.0 if inputs are incompatible.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc_sum = 0.0\n    for (pi, qi) in zip(p, q):\n        bc_sum += np.sqrt(pi * qi)\n    if bc_sum == 0.0:\n        return 0.0\n    bhattacharyya_dist = -np.log(bc_sum)\n    return round(bhattacharyya_dist, 4)"}
{"task_id": 120, "completion_id": 6, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions p and q.\n    \n    Parameters:\n    p (list of float): The first probability distribution.\n    q (list of float): The second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if inputs differ in length or are empty.\n    \"\"\"\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n    sqrt_prod = np.sqrt(np.array(p) * np.array(q)).sum()\n    distance = -np.log(sqrt_prod)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 7, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate and return the Bhattacharyya distance between two probability distributions.\n    \n    Parameters:\n    p (list[float]): First probability distribution.\n    q (list[float]): Second probability distribution.\n    \n    Returns:\n    float: Bhattacharyya distance rounded to 4 decimal places, or 0.0 if inputs are invalid.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = np.sum(np.sqrt(np.array(p) * np.array(q)))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 8, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions p and q.\n    \n    Args:\n    p: A list of floats representing the first probability distribution.\n    q: A list of floats representing the second probability distribution.\n    \n    Returns:\n    A float representing the Bhattacharyya distance rounded to 4 decimal places.\n    Returns 0.0 if the lengths of p and q are different or if either is empty.\n    \"\"\"\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    coefficient = 0.0\n    for (p_val, q_val) in zip(p, q):\n        coefficient += np.sqrt(p_val * q_val)\n    if coefficient == 0:\n        return np.inf\n    distance = -np.log(coefficient)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 9, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n    The Bhattacharyya distance between two discrete probability distributions p and q is defined as:\n    D_B(p, q) = -ln(BC(p, q))\n    where BC(p, q) = sum(sqrt(p_i * q_i)) is the Bhattacharyya coefficient.\n    \n    Parameters:\n    p (list of float): A list of floats representing the first probability distribution.\n    q (list of float): A list of floats representing the second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance rounded to 4 decimal places, or 0.0 if inputs are invalid.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    bc = 0.0\n    for i in range(len(p)):\n        bc += np.sqrt(p[i] * q[i])\n    if bc == 0:\n        return 0.0\n    db = -1.0 * np.log(bc)\n    return round(db, 4)\np = [0.1, 0.6, 0.1, 0.2]\nq = [0.2, 0.4, 0.2, 0.2]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 1, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    - a: A list of integers or floats representing the first vector.\n    - b: A list of integers or floats representing the second vector.\n    \n    Returns:\n    - A list representing the element-wise sum of the vectors if they are of the same length, otherwise -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 2, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list of int/float): The first vector.\n    b (list of int/float): The second vector.\n    \n    Returns:\n    list of int/float: A new vector representing the element-wise sum of a and b.\n    int: -1 if the operation is invalid due to mismatched vector lengths.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 121, "completion_id": 3, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    if len(a) != len(b):\n        return -1\n    else:\n        return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 4, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Arguments:\n    a -- first vector (list of integers or floats)\n    b -- second vector (list of integers or floats)\n    \n    Returns:\n    A list representing the element-wise sum of vectors a and b, or -1 if a and b have different lengths.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 5, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Compute the element-wise sum of two vectors.\n    \n    :param a: list of int or float\n    :param b: list of int or float\n    :return: A new list representing the element-wise sum of the two vectors; if vectors have incompatible dimensions,\n             the function returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 6, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors a and b.\n    \n    Parameters:\n    a (List[int|float]): The first vector.\n    b (List[int|float]): The second vector.\n    \n    Returns:\n    List[int|float] | int: A new vector representing the element-wise sum of a and b, \n    or -1 if the vectors have different lengths.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 7, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    - a (list): The first vector.\n    - b (list): The second vector.\n    \n    Returns:\n    - list: The element-wise sum of the vectors if they are compatible; otherwise, returns -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 8, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors (lists).\n    \n    Parameters:\n    a (List[int|float]): First vector.\n    b (List[int|float]): Second vector.\n    \n    Returns:\n    List[int|float] | int: A new list representing the element-wise sum of the input vectors,\n                            or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 9, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using REINFORCE algorithm.\n    \n    Parameters:\n    theta (np.ndarray): Policy parameters of shape (num_states, num_actions).\n    episodes (list[list[tuple[int, int, float]]]): List of episodes, each a list of (state, action, reward) tuples.\n    \n    Returns:\n    np.ndarray: Averaged policy gradient.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        for (s, a, r) in reversed(episode):\n            G = r + G\n            policy = softmax(theta[s, :])\n            log_policy_a = np.log(policy[a])\n            gradient[s, a] += log_policy_a * G\n    num_episodes = len(episodes)\n    gradient /= num_episodes\n    return np.round(gradient, 4).tolist()\ndef softmax(x):\n    \"\"\"\n    Computes the softmax of vector x in a numerically stable way.\n    \n    Parameters:\n    x (np.ndarray): Input vector.\n    \n    Returns:\n    np.ndarray: Softmax of x.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\ntheta = np.array([[0, 0, 0], [0, 0, 0]])\nepisodes = [[(0, 0, 1), (1, 1, 1)], [(0, 2, 1), (1, 1, -1), (0, 1, 1)]]"}
{"task_id": 122, "completion_id": 1, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        episode_gradient = np.zeros_like(theta)\n        returns = 0\n        discounted_returns = []\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            returns = reward + 0.99 * returns\n            discounted_returns.append(returns)\n        discounted_returns.reverse()\n        for (t, (state, action, reward)) in enumerate(episode):\n            policy_state = theta[state, :]\n            action_probs = np.exp(policy_state) / np.sum(np.exp(policy_state))\n            for a in range(num_actions):\n                if a == action:\n                    gradient[state, a] += discounted_returns[t] * (1 - action_probs[a])\n                else:\n                    gradient[state, a] -= discounted_returns[t] * action_probs[a]\n    average_gradient = gradient / len(episodes)\n    average_gradient = np.round(average_gradient, 4)\n    return average_gradient.tolist()\ntheta = np.array([[1, 2, 3], [3, 2, 1]])\nepisodes = [[(0, 1, 1.0), (1, 2, 1.0)], [(0, 2, 1.0), (1, 0, 1.0)]]"}
{"task_id": 122, "completion_id": 2, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1).reshape(-1, 1)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = 0\n        discounted_returns = []\n        for (state, action, reward) in reversed(episode):\n            returns += reward\n            discounted_returns.append(returns)\n        discounted_returns.reverse()\n        for (t, (state, action, _)) in enumerate(episode):\n            probs = softmax(theta[state, :])\n            log_policy = np.log(probs[action])\n            gradient[state, action] += log_policy * discounted_returns[t]\n            gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()\ntheta = np.array([[0, 0], [0, 0]], dtype=np.float64)\nepisodes = [[(0, 1, 0.5), (1, 1, 0.5)], [(0, 0, 0.8), (1, 0, 0.8)], [(0, 1, -0.2), (1, 0, 0.1)]]"}
{"task_id": 122, "completion_id": 3, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = np.zeros(len(episode))\n        for t in range(len(returns) - 1, -1, -1):\n            G = episode[t][2]\n            if t < len(returns) - 1:\n                G += returns[t + 1]\n            returns[t] = G\n        for (t, (state, action, _)) in enumerate(episode):\n            log_policy = compute_log_policy(theta, state, action)\n            gradient[state][action] += log_policy * returns[t]\n    return [round(num, 4) for num in (gradient / len(episodes)).flatten().tolist()]\ndef compute_log_policy(theta: np.ndarray, state: int, action: int):\n    theta_s = theta[state, :]\n    exp_theta = np.exp(theta_s - np.max(theta_s))\n    policy = exp_theta / np.sum(exp_theta)\n    return np.log(policy[action])"}
{"task_id": 122, "completion_id": 4, "solution": "import numpy as np\ndef compute_log_policy(theta, state):\n    \"\"\"Compute the log-policy for a given state.\"\"\"\n    log_policy = theta[state, :] - np.log(np.sum(np.exp(theta[state, :])))\n    return log_policy\ndef compute_return(episodes):\n    \"\"\"Compute the returns for each state-action pair from the episodes.\"\"\"\n    returns = []\n    for episode in episodes:\n        G = 0\n        episode_returns = []\n        for (state, action, reward) in reversed(episode):\n            G = reward + G\n            episode_returns.append(G)\n        returns.append(list(reversed(episode_returns)))\n    return returns\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    returns = compute_return(episodes)\n    for (episode, episode_return) in zip(episodes, returns):\n        for ((state, action, _), G) in zip(episode, episode_return):\n            log_policy = compute_log_policy(theta, state)\n            gradient[state, action] += log_policy[action] * G\n            gradient[state, :] -= log_policy[action] * G\n    gradient /= len(episodes)\n    gradient = np.around(gradient, decimals=4)\n    return gradient.tolist()\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [[(0, 0, 1.0), (1, 1, 1.0)], [(0, 1, 2.0), (1, 0, -1.0)]]\ngradient = compute_policy_gradient(theta, episodes)"}
{"task_id": 122, "completion_id": 5, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n\n    def compute_discounted_returns(episode):\n        rewards = [r for (_, _, r) in episode]\n        discounted_returns = []\n        R = 0\n        for reward in reversed(rewards):\n            R = reward + 0.99 * R\n            discounted_returns.append(R)\n        return discounted_returns[::-1]\n\n    def compute_gradient_for_episode(episode):\n        grad = np.zeros_like(theta)\n        discounted_returns = compute_discounted_returns(episode)\n        for ((s, a, _), R) in zip(episode, discounted_returns):\n            policy = np.exp(theta[s, :]) / np.sum(np.exp(theta[s, :]))\n            for a_prime in range(num_actions):\n                grad[s, a_prime] += policy[a_prime]\n                if a_prime == a:\n                    grad[s, a_prime] -= 1\n            grad[s, :] *= R\n        return grad\n    for episode in episodes:\n        gradient += compute_gradient_for_episode(episode)\n    average_gradient = gradient / len(episodes)\n    return np.round(average_gradient, decimals=4).tolist()\ntheta = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nepisodes = [[(0, 1, 1.0), (1, 2, 1.0)], [(0, 0, -1.0), (1, 1, -1.0)]]"}
{"task_id": 122, "completion_id": 6, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient for a policy parameterized by softmax(Theta),\n    given a set of episodic trajectories.\n    \n    :param theta: A 2D NumPy array representing the policy parameters.\n    :param episodes: A list of episodes, where each episode is a list of (state, action, reward) tuples.\n    :return: The average policy gradient, rounded to the nearest 4th decimal and returned as a Python list.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient_accumulator = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = []\n        total_return = 0\n        for (_, _, reward) in reversed(episode):\n            total_return = gamma * total_return + reward\n            returns.insert(0, total_return)\n        for ((s, a, _), return_) in zip(episode, returns):\n            policy = softmax(theta[s, :])\n            policy_gradient = return_ * (one_hot(num_actions, a) - policy)\n            gradient_accumulator[s, :] += policy_gradient\n    avg_gradients = gradient_accumulator / len(episodes)\n    avg_gradients = np.round(avg_gradients, 4)\n    return avg_gradients.tolist()\ndef one_hot(n, pos):\n    \"\"\"Utility function to create one-hot vectors.\"\"\"\n    v = np.zeros(n)\n    v[pos] = 1\n    return v\ngamma = 0.99\ntheta = np.array([[1.0, 2.0], [2.0, 3.0]])\nepisodes = [[(0, 0, 1.0), (1, 1, 1.0)], [(0, 1, 1.0), (1, 0, 1.0)]]"}
{"task_id": 122, "completion_id": 7, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1).reshape(-1, 1)\ndef compute_policy_gradient(theta, episodes):\n    (num_states, num_actions) = theta.shape\n    policy_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        episode_length = len(episode)\n        for t in reversed(range(episode_length)):\n            (s, a, r) = episode[t]\n            G = r + G\n            policy = softmax(theta[s, :])\n            policy_gradient[s, a] += G\n            policy_gradient[s, :] -= G * policy[a]\n    policy_gradient /= len(episodes)\n    return np.round(policy_gradient, 4).tolist()\ntheta = (theta - theta.mean()) / theta.std()\nepisodes = [[(0, 0, 1.0), (1, 1, -1.0)], [(0, 1, 1.5), (1, 0, -0.5)]]"}
{"task_id": 122, "completion_id": 8, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"\n    Compute softmax values for each sets of scores in x.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1)[:, None]\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n    - theta: (num_states, num_actions) NumPy array, parameters of policy\n    - episodes: list of episodes, each episode a list of tuples (state, action, reward)\n    \n    Returns:\n    The average gradient of the log-policy times the return for each state-action pair.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        returns = np.cumsum(rewards[::-1])[::-1]\n        policy = softmax(theta[states, :])\n        log_policy = np.log(policy)\n        selected_log_policy = log_policy[np.arange(len(actions)), actions]\n        gradient_states = np.zeros(theta.shape)\n        gradient_states[states, actions] = 1\n        episode_gradient = gradient_states * returns[:, np.newaxis]\n        gradient += np.sum(episode_gradient, axis=0)\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()\ntheta = np.array([[1, 2, 3], [2, 3, 4]], dtype=float)\nepisodes = [[(0, 0, 1.0), (1, 2, 1.0)], [(0, 1, 1.0), (1, 1, 1.0)]]"}
{"task_id": 122, "completion_id": 9, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list):\n    (num_states, num_actions) = theta.shape\n    gradient_theta = np.zeros_like(theta)\n    for episode in episodes:\n        states_in_episode = np.array([state for (state, _, _) in episode])\n        rewards_in_episode = np.array([reward for (_, _, reward) in episode])\n        actions_in_episode = np.array([action for (_, action, _) in episode])\n        T = len(episode)\n        returns = np.zeros(T)\n        for t in range(T):\n            returns[t] = sum(rewards_in_episode[t:])\n        for t in range(T):\n            policy = softmax(theta[states_in_episode[t], :])\n            log_policy = np.log(policy[actions_in_episode[t]])\n            grad_theta = policy\n            grad_theta[actions_in_episode[t]] -= 1\n            gradient_theta[states_in_episode[t]] += returns[t] * grad_theta * log_policy\n    gradient_theta /= len(episodes)\n    gradient_theta = np.round(gradient_theta, 4).tolist()\n    return gradient_theta\ntheta = np.array([[10, 10, 20], [20, 10, 10], [10, 20, 10]], dtype=float)\nepisodes = [[(0, 2, 1), (1, 1, 1), (2, 2, 1)], [(0, 1, 2), (1, 1, 2), (2, 0, 2)]]"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculates the computational efficiency of MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts.\n    d_in (int): The input dimension.\n    d_out (int): The output dimension.\n    \n    Returns:\n    float: The savings percentage of FLOPs for the MoE layer compared to the dense layer, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings_percentage, 1)\nk_active = 3\nd_in = 512\nd_out = 256"}
{"task_id": 123, "completion_id": 1, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts in the MoE layer.\n    d_in (int): The dimension of the input to the layer.\n    d_out (int): The dimension of the output from the layer.\n    \n    Returns:\n    float: The percentage of FLOP savings by using the MoE layer instead of a dense layer.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * d_in * d_out * (k_active / n_experts)\n    flops_savings = flops_dense - flops_moe\n    savings_percentage = flops_savings / flops_dense * 100\n    return round(savings_percentage, 1)\nn_experts = 128\nk_active = 16\nd_in = 1024\nd_out = 1024"}
{"task_id": 123, "completion_id": 2, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational efficiency of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts used in computation.\n    d_in (int): The input dimension for each expert or dense layer.\n    d_out (int): The output dimension for each expert or dense layer.\n    \n    Returns:\n    float: The percentage of computational cost savings of the MoE layer over the dense layer, rounded to the nearest tenth.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = n_experts * d_in + k_active * d_out * d_in + k_active * d_out\n    flops_savings = flops_dense - flops_moe\n    savings_percentage = flops_savings / flops_dense * 100\n    return round(savings_percentage, 1)"}
{"task_id": 123, "completion_id": 3, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active experts used for each input.\n    d_in (int): The input dimension.\n    d_out (int): The output dimension.\n    \n    Returns:\n    float: The computational cost savings in percentage rounded to the nearest 1th decimal.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = 2 * d_in * d_out * k_active\n    moe_flops /= n_experts\n    savings = 1 - moe_flops / dense_flops\n    savings_percentage = savings * 100\n    return round(savings_percentage, 1)"}
{"task_id": 123, "completion_id": 4, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    import math\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out\n    routing_flops = 2 * d_in * n_experts\n    flops_moe += routing_flops\n    flops_saved = flops_dense - flops_moe\n    percent_savings = flops_saved / flops_dense * 100\n    return round(percent_savings, 1)"}
{"task_id": 123, "completion_id": 5, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active experts (k-way sparsity).\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n    \n    Returns:\n    float: Percentage of FLOPs saved by using MoE over a dense layer, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_expert = 2 * k_active * d_in * d_out\n    flops_gating = 2 * d_in * n_experts\n    flops_moe = flops_expert + flops_gating\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)"}
{"task_id": 123, "completion_id": 6, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer.\n    \n    :param n_experts: Number of experts in the MoE layer.\n    :param k_active: Number of active experts (sparsity).\n    :param d_in: Input dimension.\n    :param d_out: Output dimension.\n    :return: Percentage of computational cost savings of MoE layer over a dense layer, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * d_in * d_out * k_active\n    savings_percentage = (flops_dense - flops_moe / n_experts) / flops_dense * 100\n    return round(savings_percentage, 1)"}
{"task_id": 123, "completion_id": 7, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Computes the computational efficiency of a MoE layer compared to a dense layer.\n    \n    Parameters:\n    - n_experts (int): The total number of experts in the MoE layer.\n    - k_active (int): The number of active experts (k-active) used in the MoE layer.\n    - d_in (int): The input dimension.\n    - d_out (int): The output dimension.\n    \n    Returns:\n    - float: The percentage of computational savings rounded to the nearest tenth decimal.\n    \"\"\"\n    dense_flops = d_in * d_out\n    moe_flops = d_in * d_out * k_active / n_experts + k_active * d_in\n    flops_savings = dense_flops - moe_flops\n    savings_percentage = flops_savings / dense_flops * 100\n    return round(savings_percentage, 1)\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 512"}
{"task_id": 123, "completion_id": 8, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active experts.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n\n    Returns:\n    float: Percentage savings in computation when using MoE instead of a dense layer.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * (2 * d_in * d_out) + d_in * n_experts\n    savings = dense_flops - moe_flops\n    savings_percentage = savings / dense_flops * 100\n    savings_percentage = round(savings_percentage, 1)\n    return savings_percentage\nn_experts = 32\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 123, "completion_id": 9, "solution": "def compute_flops(n_experts, k_active, d_in, d_out):\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * (2 * d_in * d_out) / n_experts\n    return (flops_dense, flops_moe)\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    (flops_dense, flops_moe) = compute_flops(n_experts, k_active, d_in, d_out)\n    savings_flops = flops_dense - flops_moe\n    savings_percentage = savings_flops / flops_dense * 100\n    return round(savings_percentage, 1)\nn_experts = 128\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Computes the final gating probabilities matrix using the Noisy Top-K gating mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating.\n    W_noise (np.ndarray): Weighted noise matrix.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): Sparsity constraint indicating top-K experts.\n    \n    Returns:\n    list: A list containing the gating probabilities matrix.\n    \"\"\"\n    logits = X @ W_g\n    if W_noise is not None:\n        noisy_logits = logits + W_noise * N\n    else:\n        noisy_logits = logits\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    p = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    top_k_indices = np.argsort(p, axis=1)[:, -k:]\n    mask = np.zeros_like(p)\n    for (i, indices) in enumerate(top_k_indices):\n        mask[i, indices] = 1\n    p_masked = p * mask\n    p_masked = p_masked / p_masked.sum(axis=1, keepdims=True)\n    p_masked_rounded = np.round(p_masked, 4).tolist()\n    return p_masked_rounded"}
{"task_id": 124, "completion_id": 1, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Computes the noisy top-K gate probabilities for Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    X (np.ndarray): Input data matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Weight matrix for noise adjustment.\n    N (np.ndarray): Pre-sampled noise values to add to the probabilities.\n    k (int): Sparsity level, number of experts to select.\n\n    Returns:\n    list: A list of gate probabilities with noise and sparsity constraints applied.\n    \"\"\"\n    logits = np.dot(X, W_g)\n    noisy_logits = logits + N * W_noise\n    maxes = np.amax(noisy_logits, axis=-1, keepdims=True)\n    shifted = noisy_logits - maxes\n    exp_a = np.exp(shifted)\n    gating_probs = exp_a / np.sum(exp_a, axis=-1, keepdims=True)\n    top_scores = np.sort(gating_probs, axis=-1)[..., -k]\n    noise_mask = gating_probs >= top_scores\n    sparse_gating_probs = gating_probs * noise_mask / np.sum(exp_a * noise_mask, axis=-1, keepdims=True)\n    return np.round(sparse_gating_probs, decimals=4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6]])\nW_g = np.array([[0.5, -0.5], [-0.5, 0.5], [0.0, 0.0]])\nW_noise = np.array([1, 1])\nN = np.array([[0.1, 0.1], [0.1, 0.1]])\nk = 1"}
{"task_id": 124, "completion_id": 2, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implement the Noisy Top-K Gating Mechanism for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    X: Input data matrix (shape: [batch_size, d_model]).\n    W_g: Weight matrix for the gating network (shape: [d_model, num_experts]).\n    W_noise: Weight matrix for the noise injection (shape: [1, num_experts]).\n    N: Pre-sampled noise (shape: [batch_size, num_experts]).\n    k: Number of experts to retain for each input (sparsity constraint).\n    \n    Returns:\n    A python list containing the final gating probabilities (after applying top-k sparsity and noise).\n    \"\"\"\n    logits = X @ W_g\n    noise = N * W_noise\n    noisy_logits = logits + noise\n    exp_noisy_logits = np.exp(noisy_logits)\n    row_sums = exp_noisy_logits.sum(axis=1, keepdims=True)\n    softmax_noisy_logits = exp_noisy_logits / row_sums\n    topk_indices = np.argpartition(softmax_noisy_logits, -k, axis=1)[:, -k:]\n    topk_indices = np.sort(topk_indices)\n    topk_probs = np.zeros_like(softmax_noisy_logits)\n    for (i, indices) in enumerate(topk_indices):\n        for idx in indices:\n            topk_probs[i, idx] = softmax_noisy_logits[i, idx]\n    non_zero = topk_probs != 0\n    sums = np.sum(topk_probs, axis=1, keepdims=True)\n    topk_probs = np.where(sums != 0, topk_probs / sums, topk_probs)\n    topk_probs = np.round(topk_probs, decimals=4)\n    return topk_probs.tolist()\nX = np.random.rand(3, 5)\nW_g = np.random.rand(5, 8)\nW_noise = np.ones((1, 8))\nN = np.random.rand(3, 8)\nk = 2"}
{"task_id": 124, "completion_id": 3, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Implements the Noisy Top-K Gating mechanism for Mixture-of-Experts models.\n\n    Parameters:\n    X (np.ndarray): Input data matrix of shape (batch_size, input_dim).\n    W_g (np.ndarray): Gate weight matrix of shape (input_dim, num_experts).\n    W_noise (np.ndarray): Weight for the noise to be applied to the logits.\n    N (np.ndarray): Pre-sampled noise that needs to be multiplied by W_noise.\n    k (int): The sparsity constraint, i.e., number of top experts to consider.\n\n    Returns:\n    list: Final gating probabilities as a list.\n    \"\"\"\n    gate_activations = np.dot(X, W_g)\n    noisy_activations = gate_activations + N * W_noise\n    shifted_noisy_activations = noisy_activations - np.max(noisy_activations, axis=-1, keepdims=True)\n    exp_values = np.exp(shifted_noisy_activations)\n    probabilities = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    indices = np.argpartition(probabilities, -k, axis=-1)[:, -k:]\n    final_probabilities = np.zeros_like(probabilities)\n    batch_indices = np.repeat(np.arange(probabilities.shape[0]), k)\n    expert_indices = np.hstack([indices[i] for i in range(indices.shape[0])])\n    final_values = np.hstack([probabilities[i, indices[i]] for i in range(indices.shape[0])])\n    final_probabilities[batch_indices, expert_indices] = final_values\n    final_probabilities /= final_probabilities.sum(axis=-1, keepdims=True)\n    final_probabilities = np.round(final_probabilities, 4).tolist()\n    return final_probabilities\nX = np.array([[1, 2], [3, 4]])\nW_g = np.array([[0.5, -0.1, 0.1], [-0.2, 0.3, 0.2]])\nW_noise = np.array([0.01, 0.02, 0.03])\nN = np.array([[0.05, 0.01, -0.03], [-0.01, 0.02, 0.04]])\nk = 2\nprobabilities = noisy_topk_gating(X, W_g, W_noise, N, k)"}
{"task_id": 124, "completion_id": 4, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Compute the gating probabilities using the Noisy Top-K mechanism for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    X: The input array of shape (batch_size, input_features).\n    W_g: The gating weight matrix of shape (input_features, num_experts).\n    W_noise: The weight to apply to the noise of shape (num_experts,).\n    N: The pre-sampled noise matrix of shape (batch_size, num_experts).\n    k: The sparsity constraint; number of top experts to be used.\n    \n    Returns:\n    A list of the gating probabilities after reshaping and rounding to 4 decimals.\n    \"\"\"\n    preacts = np.dot(X, W_g) + W_noise * N\n    preacts -= np.max(preacts, axis=1, keepdims=True)\n    preacts_exp = np.exp(preacts)\n    probs = preacts_exp / np.sum(preacts_exp, axis=1, keepdims=True)\n    indices = np.argpartition(probs, -k)[:, -k:]\n    sparse_probs = np.zeros_like(probs)\n    for (i, index) in enumerate(indices):\n        sparse_probs[i, index] = probs[i, index]\n    sparse_probs = sparse_probs / np.sum(sparse_probs, axis=1, keepdims=True)\n    return sparse_probs.round(4).tolist()"}
{"task_id": 124, "completion_id": 5, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Implement the Noisy Top-K Gating Function for Mixture-of-Experts models.\n    \n    Parameters:\n    - X: Input matrix of shape (batch_size, input_dim).\n    - W_g: Weight matrix of shape (input_dim, num_experts).\n    - W_noise: Weight matrix for noise scaling, shape (input_dim, num_experts).\n    - N: Pre-sampled noise, shape (batch_size, num_experts).\n    - k: Number of experts to retain as nonzero in the final probability matrix.\n    \n    Returns:\n    - Probabilities matrix after noisy top-k gating as python list.\n    \"\"\"\n    num_experts = W_g.shape[1]\n    logits = X @ W_g\n    noise = torch.tensor(N)\n    logits += torch.tensor(X) @ torch.tensor(W_noise) * noise\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    sorted_indices = np.argsort(-probs, axis=1)\n    topk_indices = sorted_indices[:, :k]\n    for (i, indices) in enumerate(topk_indices):\n        probs[i, indices] = 1.0\n        probs[i, [idx for idx in range(num_experts) if idx not in indices]] = 0.0\n    row_sums = np.sum(probs, axis=1)\n    probs /= row_sums[:, np.newaxis]\n    probs = np.round(probs, decimals=4)\n    return probs.tolist()\nX = np.array([[1, 2], [3, 4]])\nW_g = np.array([[0.1, 0.2], [0.05, 0.3]])\nW_noise = np.array([[0.1, 0.05], [0.2, 0.01]])\nN = np.array([[0.01, 0.02], [0.02, 0.03]])\nk = 1"}
{"task_id": 124, "completion_id": 6, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Calculate the Noisy Top-K Gating Function for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    X (np.ndarray): Input data matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Weight matrix for applying noise.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): Sparsity control parameter i.e., how many experts to use per example.\n\n    Returns:\n    list: The final gating probabilities matrix as a python list.\n    \"\"\"\n    logits = np.dot(X, W_g)\n    noisy_logits = logits + np.dot(N, W_noise)\n    exp_logits = np.exp(noisy_logits)\n    top_k_indices = np.argpartition(-exp_logits, k)[:, :k]\n    mask = np.zeros_like(exp_logits)\n    np.put_along_axis(mask, top_k_indices, 1, axis=1)\n    masked_exp_logits = exp_logits * mask\n    sums = masked_exp_logits.sum(axis=1, keepdims=True)\n    final_probabilities = masked_exp_logits / sums\n    return np.round(final_probabilities, 4).tolist()\nX = np.array([[1, 2], [3, 4]])\nW_g = np.random.randn(X.shape[1], 10)\nW_noise = np.random.randn(10, 10)\nN = np.random.randn(X.shape[0], 10)\nk = 3"}
{"task_id": 124, "completion_id": 7, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Computes the noisy top-k gating mechanism for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    X : np.ndarray\n        Input data matrix.\n    W_g : np.ndarray\n        Weight matrix for gating.\n    W_noise : np.ndarray\n        Weight matrix for noise.\n    N : np.ndarray\n        Pre-sampled noise matrix.\n    k : int\n        Sparsity constraint, number of experts to select.\n        \n    Returns:\n    list\n        The final gating probabilities matrix as a list rounded to the 4th decimal.\n    \"\"\"\n    logits = X @ W_g + N * (X @ W_noise)\n    e_logit = np.exp(logits - logits.max(axis=-1, keepdims=True))\n    probs = (e_logit / e_logit.sum(axis=-1, keepdims=True)).round(4)\n    topk_probs = np.zeros_like(probs)\n    topk_indices = np.argsort(-probs, axis=1)[:, :k]\n    for (row, indices) in enumerate(topk_indices):\n        topk_probs[row, indices] = probs[row, indices]\n    topk_probs /= topk_probs.sum(axis=1, keepdims=True)\n    return topk_probs.round(4).tolist()\nX = np.array([[1.0, 2.0], [3.0, 4.0]])\nW_g = np.array([[-1.0, 1.5], [0.5, -1.0]])\nW_noise = np.array([[0.1, 0.2], [0.3, 0.4]])\nN = np.array([[0.15, -0.15], [-0.1, 0.1]])\nk = 1"}
{"task_id": 124, "completion_id": 8, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Implement the Noisy Top-K Gating Function used in Mixture-of-Experts models.\n    \n    Args:\n    - X: Input data matrix, shape (batch_size, feature_size).\n    - W_g: Weight matrix for gating, shape (feature_size, num_experts).\n    - W_noise: Weight matrix for noise in gating, shape (feature_size, num_experts).\n    - N: Noise matrix for added noise, shape (batch_size, num_experts).\n    - k: Sparsity level, the number of experts each example should gate to.\n    \n    Returns:\n    - A list of final gating probabilities, rounded to the nearest 4th decimal.\n    \"\"\"\n    noisy_logits = np.dot(X, W_g) + N * np.dot(X, W_noise)\n    logits = np.dot(X, W_g)\n    max_logits = np.max(logits, axis=1, keepdims=True)\n    logits = logits - max_logits\n    noisy_logits = noisy_logits - max_logits\n    probabilities = np.exp(noisy_logits)\n    probabilities_sum = np.sum(probabilities, axis=1, keepdims=True)\n    probabilities /= probabilities_sum\n    topk_probs = []\n    for prob in probabilities:\n        topk_indices = np.argsort(prob)[-k:]\n        new_prob = np.zeros_like(prob)\n        new_prob[topk_indices] = prob[topk_indices]\n        sum_prob = np.sum(new_prob)\n        if sum_prob > 0:\n            new_prob /= sum_prob\n        topk_probs.append(new_prob.tolist())\n    topk_probs_rounded = np.array(topk_probs).round(decimals=4).tolist()\n    return topk_probs_rounded"}
{"task_id": 124, "completion_id": 9, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implement the Noisy Top-K gating mechanism.\n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Weight matrix for scaling noise.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): The sparsity constraint, i.e., number of top experts to activate.\n\n    Returns:\n    np.ndarray: A matrix of gating probabilities, rounded to 4 decimal places.\n    \"\"\"\n    pre_acts = X @ W_g + N * W_noise\n    probs = np.exp(pre_acts) / np.sum(np.exp(pre_acts), axis=1).reshape(-1, 1)\n    top_k_probs = np.zeros(probs.shape)\n    idx = np.argsort(probs, axis=1)[:, -k:]\n    for (i, arr) in enumerate(idx):\n        top_k_probs[i, arr] = 1\n    final_probs = top_k_probs * probs\n    final_probs = final_probs / np.sum(final_probs, axis=1).reshape(-1, 1)\n    final_probs = np.nan_to_num(final_probs)\n    return final_probs.round(4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6]])\nW_g = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nW_noise = np.array([[0.05, 0.05], [0.01, 0.01]])\nN = np.array([[0.01, -0.02], [-0.03, 0.04]])\nk = 1"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Function to compute the Mixture-of-Experts output.\n    \n    Args:\n    x (np.ndarray): Input tensor of shape [batch_size, input_dim].\n    We (np.ndarray): Weight matrix of experts of shape [n_experts, input_dim, output_dim].\n    Wg (np.ndarray): Gating weight matrix of shape [input_dim, n_experts].\n    n_experts (int): Number of experts.\n    top_k (int): Number of experts to select for each token after applying the gating mechanism.\n    \n    Returns:\n    np.ndarray: Final MoE output tensor.\n    \"\"\"\n    gate_logits = np.dot(x, Wg)\n    gate_probs = np.exp(gate_logits) / np.sum(np.exp(gate_logits), axis=1, keepdims=True)\n    sorted_indices = np.argsort(-gate_probs, axis=1)\n    top_k_indices = sorted_indices[:, :top_k]\n    top_k_probs = np.take_along_axis(gate_probs, top_k_indices, axis=1)\n    batch_size = x.shape[0]\n    input_dim = x.shape[1]\n    output_dim = We.shape[2]\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert = top_k_indices[i, j]\n            expert_outputs[i, j] = np.dot(x[i], We[expert])\n    weighted_outputs = np.multiply(expert_outputs, np.expand_dims(top_k_probs, axis=2))\n    aggregated_outputs = np.sum(weighted_outputs, axis=1)\n    rounded_outputs = np.round(aggregated_outputs, decimals=4)\n    return rounded_outputs.tolist()"}
{"task_id": 125, "completion_id": 1, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implementation of a Mixture-of-Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts considered for each token.\n    \n    Returns:\n    np.ndarray: Output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[-1]\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores - gating_scores.max(axis=-1, keepdims=True))\n    gating_total = gating_probs.sum(axis=-1, keepdims=True)\n    gating_probs = gating_probs / gating_total\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    experts_output = np.einsum('bij,jkl->bilk', x[:, None, :], We)\n    top_k_experts_output = np.take_along_axis(experts_output.squeeze(axis=2), top_k_indices[:, :, None], axis=-2)\n    weighted_expert_outputs = top_k_experts_output * top_k_probs[..., None]\n    moe_output = weighted_expert_outputs.sum(axis=-2)\n    return np.round(moe_output, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 2, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x: np.ndarray - Input tensor of shape (batch_size, input_dim).\n    We: np.ndarray - Weight matrices for experts of shape (n_experts, input_dim, output_dim).\n    Wg: np.ndarray - Gating weight matrix of shape (input_dim, n_experts).\n    n_experts: int - Number of experts.\n    top_k: int - Number of top expert layers to consider for each input token.\n    \n    Returns:\n    np.ndarray - Output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores)\n    gating_probs = gating_probs / gating_probs.sum(axis=1, keepdims=True)\n    top_k_probs = np.partition(gating_probs, -top_k)[:, -top_k:]\n    top_k_indices = np.argpartition(gating_probs, -top_k)[:, -top_k:]\n    experts_out = np.dot(x[:, :, None], We)\n    final_out = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        for k in range(top_k):\n            expert_index = top_k_indices[i, k]\n            final_out[i] += gating_probs[i, expert_index] * experts_out[i, expert_index]\n    return final_out.round(4).tolist()"}
{"task_id": 125, "completion_id": 3, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Compute the Mixture-of-Experts (MoE) layer output.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Weight matrix for experts of shape (input_dim, n_experts, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to use.\n    \n    Returns:\n    np.ndarray: The MoE layer output, a tensor of the same shape as x.\n    \"\"\"\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores - gating_scores.max(axis=-1, keepdims=True))\n    sums = gating_probs.sum(axis=-1, keepdims=True)\n    gating_probs /= sums\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    output = np.zeros_like(x)\n    for i in range(x.shape[0]):\n        for expert_idx in top_k_indices[i]:\n            expert_output = np.dot(x[i], We[:, expert_idx])\n            output[i] += gating_probs[i, expert_idx] * expert_output\n    return output.round(4).tolist()"}
{"task_id": 125, "completion_id": 4, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, hidden_size).\n    - We: Expert weight tensor of shape (hidden_size, n_experts * expert_hidden_size).\n    - Wg: Gating weight matrix of shape (hidden_size, n_experts).\n    - n_experts: The number of experts.\n    - top_k: The number of top experts selected based on gating scores for each input token.\n    \n    Returns:\n    - Output tensor after MoE layer processing.\n    \"\"\"\n    (batch_size, hidden_size) = x.shape\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores)\n    gating_probs /= gating_probs.sum(axis=-1, keepdims=True)\n    sorted_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    sorted_probs = np.zeros_like(gating_probs)\n    for batch_idx in range(batch_size):\n        sorted_probs[batch_idx][sorted_indices[batch_idx]] = gating_probs[batch_idx][sorted_indices[batch_idx]]\n    sorted_probs /= sorted_probs.sum(axis=-1, keepdims=True)\n    selected_experts = np.take(We.T, indices=sorted_indices, axis=0).transpose((0, 2, 1))\n    selected_experts = selected_experts.reshape((batch_size, top_k, hidden_size, -1))\n    x_expanded = np.expand_dims(x, axis=1)\n    experts_output = np.matmul(x_expanded, selected_experts)\n    experts_output = experts_output.reshape(batch_size, top_k, -1)\n    final_output = np.matmul(sorted_probs[:, :, np.newaxis], experts_output).squeeze(axis=1)\n    return np.round(final_output.tolist(), 4)\nbatch_size = 2\nhidden_size = 4\nn_experts = 3\ntop_k = 2\nx = np.random.randn(batch_size, hidden_size)\nWg = np.random.randn(hidden_size, n_experts)"}
{"task_id": 125, "completion_id": 5, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Mixture-of-Experts (MoE) layer.\n\n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Experts' weight matrices. Shape is (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): The number of top experts to select for each input.\n    \n    Returns:\n    np.ndarray: Output tensor after applying MoE layer.\n    \"\"\"\n    gating_weights = x @ Wg\n    gating_probs = np.exp(gating_weights) / np.sum(np.exp(gating_weights), axis=-1, keepdims=True)\n    top_k_idx = np.argsort(-gating_probs, axis=-1)[:, :top_k]\n    expert_outputs = np.einsum('bij,jkl->bilk', x[:, np.newaxis, :], We)\n    selected_experts_outputs = np.take_along_axis(expert_outputs, top_k_idx[:, :, np.newaxis, np.newaxis], axis=1)\n    selected_gating_probs = np.take_along_axis(gating_probs, top_k_idx, axis=-1)\n    norm_selected_gating_probs = selected_gating_probs / np.sum(selected_gating_probs, axis=-1, keepdims=True)\n    final_output = np.sum(selected_experts_outputs * norm_selected_gating_probs[:, :, :, np.newaxis], axis=1)\n    return np.round(final_output, 4).tolist()\nx = np.array([[1.0, 2.0], [1.0, 2.0]])\nWe = np.array([[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.2, 0.3], [0.4, 0.5]]])\nWg = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\ntop_k = 2"}
{"task_id": 125, "completion_id": 6, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1)[:, None]\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.zeros_like(gating_probs)\n    for i in range(gating_probs.shape[0]):\n        top_k_probs[i, top_k_indices[i]] = gating_probs[i, top_k_indices[i]]\n    expert_outputs = np.dot(x, We)\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, None], axis=1)\n    top_k_expert_probs = np.take_along_axis(top_k_probs, top_k_indices, axis=1)[:, :, None]\n    aggregated_output = np.sum(top_k_expert_outputs * top_k_expert_probs, axis=1)\n    return np.round(aggregated_output, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 7, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    \"\"\"\n    Compute the Mixture-of-Experts (MoE) layer output for input x.\n    \n    Args:\n    x (np.ndarray): Input tensor of shape (batch_size, input_features).\n    We (np.ndarray): Experts weight matrix of shape (n_experts, input_features, expert_output_features).\n    Wg (np.ndarray): Gating weight matrix of shape (input_features, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of experts to be selected per token.\n    \n    Returns:\n    list: Final result of the MoE layer as a list, rounded to the 4th decimal.\n    \"\"\"\n    logits = x @ Wg\n    gating_probs = softmax(logits)\n    topk_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    topk_probs = np.take_along_axis(gating_probs, topk_indices, axis=1)\n    row_sums = topk_probs.sum(axis=1, keepdims=True)\n    normed_probs = topk_probs / row_sums\n    expert_results = np.stack([expert_transform(x, e_weights) for e_weights in We], axis=1)\n    topk_expert_results = np.take_along_axis(expert_results, topk_indices[:, :, np.newaxis], axis=1)\n    weighted_results = normed_probs[:, :, np.newaxis] * topk_expert_results\n    aggregated_results = np.sum(weighted_results, axis=1)\n    return np.round(aggregated_results, decimals=4).tolist()\ndef softmax(x):\n    \"\"\"Compute the softmax of x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef expert_transform(x, weights):\n    \"\"\"Apply expert transformation with weight matrix.\"\"\"\n    return x @ weights"}
{"task_id": 125, "completion_id": 8, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Mixture-of-Experts (MoE) layer.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, d_in).\n    - We: Experts weight matrix of shape (n_experts, d_in, d_out).\n    - Wg: Gating weight matrix of shape (d_in, n_experts).\n    - n_experts: Number of experts.\n    - top_k: Top-k selection parameter.\n\n    Returns:\n    - The computed MoE output as a numpy array.\n    \"\"\"\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    top_k_indices = np.argpartition(gating_probs, -top_k, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    top_k_probs = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    top_k_experts = np.zeros((x.shape[0], top_k, x.shape[1]))\n    for (i, top_indices) in enumerate(top_k_indices):\n        top_k_experts[i] = We[top_indices]\n    x_reshaped = np.expand_dims(x, axis=1)\n    expert_outputs = np.sum(x_reshaped * top_k_experts, axis=-1)\n    weighted_expert_outputs = expert_outputs * top_k_probs[:, :, None]\n    final_output = np.sum(weighted_expert_outputs, axis=1)\n    final_output = np.round(final_output, decimals=4)\n    return final_output.tolist()\nx = np.array([[1.0, 2.0], [3.0, 4.0]])\nWe = np.array([[[1.0, 0.0], [0.0, 1.0]], [[0.0, 1.0], [1.0, 0.0]]])\nWg = np.array([[1.0, 0.0], [0.0, 1.0]])\ntop_k = 1"}
{"task_id": 125, "completion_id": 9, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    \"\"\"\n    Implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    :param x: Input tensor of shape (batch size, sequence length, input dimension).\n    :param We: Expert weight matrix of shape (n_experts, input dimension, output dimension).\n    :param Wg: Gating weight matrix of shape (input dimension, n_experts).\n    :param n_experts: Number of experts.\n    :param top_k: Number of top experts to select per token.\n    :return: Final MoE output tensor, with shape (batch size, sequence length, output dimension).\n    \"\"\"\n    gating_scores = np.dot(x, Wg)\n    gating_probs = softmax(gating_scores)\n    top_k_idx = np.argsort(gating_probs, axis=-1)[:, :, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_idx, axis=-1)\n    x_expanded = np.expand_dims(x, axis=-2)\n    expert_outputs = np.tensordot(x_expanded, We, axes=[[-1], [-2]])\n    top_k_outputs = np.take_along_axis(expert_outputs, np.expand_dims(top_k_idx, axis=-1), axis=-2)\n    weighted_outputs = top_k_probs[..., np.newaxis] * top_k_outputs\n    final_output = np.sum(weighted_outputs, axis=-2)\n    return final_output.round(4).tolist()\ndef softmax(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\nx = np.random.randn(batch_size, seq_len, input_dim)\nWe = np.random.randn(n_experts, input_dim, output_dim)\nWg = np.random.randn(input_dim, n_experts)"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Applies Group Normalization to a 4D input tensor.\n    \n    Args:\n    X (np.ndarray): Input 4D tensor of shape (B, C, H, W).\n    gamma (np.ndarray): Scale parameter of shape (C,).\n    beta (np.ndarray): Shift parameter of shape (C,).\n    num_groups (int): Number of groups to divide the channels into.\n    epsilon (float): Small constant to add to the variance to avoid division by zero.\n    \n    Returns:\n    np.ndarray: Normalized tensor with applied scale and shift, rounded to the 4th decimal.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'Number of channels must be divisible by number of groups'\n    X = X.reshape(B, G, C // G, H, W)\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), ddof=0, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_normalized = gamma * X_norm + beta\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()"}
{"task_id": 126, "completion_id": 1, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Perform group normalization on the input tensor.\n    \n    Args:\n    X (np.ndarray): The input tensor of shape (B, C, H, W).\n    gamma (np.ndarray): The learned scale of shape (C,).\n    beta (np.ndarray): The learned shift of shape (C,).\n    num_groups (int): The number of groups to divide the channels into.\n    epsilon (float): A small value to avoid division by zero. Default is 1e-5.\n    \n    Returns:\n    list: The normalized input tensor, flattened and rounded to 4 decimal places.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels is not divisible by the number of groups')\n    G = num_groups\n    C_per_G = C // G\n    X = X.reshape(B, G, C_per_G, H, W)\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    X_hat = (X - mean) / np.sqrt(var + epsilon)\n    X_hat = X_hat.reshape(B, C, H, W)\n    X_out = gamma * X_hat + beta\n    return np.round(X_out, decimals=4).tolist()\nnum_groups = 8\nX = np.random.randn(B, C, H, W)\ngamma = np.random.randn(C)\nbeta = np.random.randn(C)\nepsilon = 1e-05"}
{"task_id": 126, "completion_id": 2, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups.'\n    X_groups = np.split(X, num_groups, axis=1)\n    X_norm = np.zeros_like(X)\n    for i in range(num_groups):\n        X_group = X_groups[i]\n        mean = X_group.mean(axis=(1, 2, 3), keepdims=True)\n        var = X_group.var(axis=(1, 2, 3), keepdims=True)\n        X_norm_group = (X_group - mean) / np.sqrt(var + epsilon)\n        X_norm[:, i * C // num_groups:(i + 1) * C // num_groups, :, :] = X_norm_group * gamma[i] + beta[i]\n    return np.round(X_norm, 4).tolist()"}
{"task_id": 126, "completion_id": 3, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    if C % G != 0:\n        raise ValueError('Number of channels must be divisible by num_groups')\n    X = X.reshape(B, G, -1, H, W)\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), ddof=0, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    Y = gamma * X_norm + beta\n    return np.round(Y, decimals=4).tolist()\nX = np.random.rand(2, 8, 7, 7)\ngamma = np.random.rand(8)\nbeta = np.random.rand(8)\nnum_groups = 2"}
{"task_id": 126, "completion_id": 4, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'Number of channels must be divisible by num_groups'\n    X = X.reshape(B, G, C // G, H, W)\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    out = gamma * X_norm + beta\n    return np.round(out, decimals=4).tolist()\nX = np.random.random((2, 8, 32, 32))\ngamma = np.ones(8)\nbeta = np.zeros(8)\nnum_groups = 2"}
{"task_id": 126, "completion_id": 5, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Performs Group Normalization on the input tensor.\n    X has shape (B, C, H, W), gamma and beta have shape (C),\n    where B is batch size, C is number of channels, H is height, W is width.\n    num_groups is the number of groups the channels are divided into.\n    \"\"\"\n    if X.ndim != 4:\n        raise ValueError('Input tensor must be 4D.')\n    if X.shape[1] % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by num_groups.')\n    (B, C, H, W) = X.shape\n    group_size = C // num_groups\n    X_reshape = X.reshape(B, num_groups, group_size, H, W)\n    mean = X_reshape.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshape.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshape - mean) / np.sqrt(var + epsilon)\n    X_scale_shift = X_norm.reshape(B, C, H, W) * gamma + beta\n    return np.round(X_scale_shift, 4).tolist()\ngamma = np.random.rand(12)\nbeta = np.random.rand(12)\nnum_groups = 3"}
{"task_id": 126, "completion_id": 6, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05) -> list:\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'channels must be divisible by num_groups'\n    X_reshaped = X.reshape(B, G, C // G, H, W)\n    mean = np.mean(X_reshaped, axis=(0, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(0, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    out = gamma * X_norm + beta\n    return np.round(out, decimals=4).tolist()\ngamma = np.random.randn(12, 1, 1)\nbeta = np.random.randn(12, 1, 1)\nnum_groups = 3"}
{"task_id": 126, "completion_id": 7, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (batch_size, num_channels, height, width) = X.shape\n    if num_channels % num_groups != 0:\n        raise ValueError(f'The number of channels ({num_channels}) must be divisible by the number of groups ({num_groups}).')\n    group_channels = num_channels // num_groups\n    X_reshaped = X.reshape(batch_size, num_groups, group_channels, height, width)\n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_out = X_norm.reshape(batch_size, num_channels, height, width)\n    X_out *= gamma\n    X_out += beta\n    return np.round(X_out, decimals=4).tolist()"}
{"task_id": 126, "completion_id": 8, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D tensor with shape (B, C, H, W).\n    \n    :param X: Input data of shape (B, C, H, W)\n    :param gamma: Scale parameter of shape (1, C, 1, 1)\n    :param beta: Shift parameter of shape (1, C, 1, 1)\n    :param num_groups: Number of groups to divide the channels into\n    :param epsilon: Small constant for numerical stability\n    :return: Normalized tensor of the same shape as X\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    X_grouped = X.reshape((B, num_groups, -1, H, W))\n    mean_group = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var_group = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean_group) / np.sqrt(var_group + epsilon)\n    X_normalized = X_normalized.reshape((B, C, H, W))\n    out = gamma * X_normalized + beta\n    out = np.round(out, decimals=4)\n    return out.tolist()"}
{"task_id": 126, "completion_id": 9, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform group normalization on a 4D input tensor (B, C, H, W).\n    \n    Parameters:\n    X (np.ndarray): Input tensor with shape (B, C, H, W).\n    gamma (np.ndarray): Scale parameter of shape (C,).\n    beta (np.ndarray): Shift parameter of shape (C,).\n    num_groups (int): Number of groups C should be divided into.\n    epsilon (float): A small number to avoid division by zero.\n    \n    Returns:\n    np.ndarray: Group normalized tensor with shape (B, C, H, W).\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups.'\n    X_grouped = X.reshape(B, num_groups, -1, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(X_normalized, decimals=4).tolist()\ngamma = np.ones(8)\nbeta = np.ones(8)\nnum_groups = 4"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f_prime(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        dx = f_prime(x)\n        new_x = x - learning_rate * dx\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 1, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def derivative(x: float) -> float:\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    tol = tolerance\n    for _ in range(max_iters):\n        grad = derivative(current_x)\n        next_x = current_x - learning_rate * grad\n        if abs(next_x - current_x) < tol:\n            break\n        current_x = next_x\n    return round(current_x, 4)"}
{"task_id": 127, "completion_id": 2, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Use gradient descent to find the minimum value of the function f(x) = x^4 - 3x^3 + 2 starting from start_x.\n    \n    :param start_x: The initial value of x.\n    :param learning_rate: The learning rate for gradient descent.\n    :param tolerance: The tolerance for convergence.\n    :param max_iters: The maximum number of iterations.\n    :return: The x value that minimizes f(x).\n    \"\"\"\n    x = start_x\n    for i in range(max_iters):\n        grad = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * grad\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)\nstart_x = np.random.uniform(-10, 10)"}
{"task_id": 127, "completion_id": 3, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2.\n    \"\"\"\n\n    def f(x: np.ndarray) -> np.ndarray:\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x: np.ndarray) -> np.ndarray:\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        x_next = x - learning_rate * gradient\n        if np.abs(x_next - x) < tolerance:\n            break\n        x = x_next\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 4, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Implements gradient descent to find the minimum of the function f(x) = x^4 - 3x^3 + 2.\n    \n    Parameters:\n    - start_x: initial point to start the search.\n    - learning_rate: step size at each iteration of the gradient descent algorithm.\n    - tolerance: the threshold for stopping the algorithm based on the change in x (convergence criterion).\n    - max_iters: maximum number of iterations to search for the minimum.\n    \n    Returns:\n    - The value of x at the minimum point, rounded to the 4th decimal place.\n    \"\"\"\n    x = start_x\n    for i in range(max_iters):\n        x_old = x\n        x -= learning_rate * (4 * x ** 3 - 9 * x ** 2)\n        if abs(x - x_old) < tolerance:\n            break\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 5, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Function to find the x value at which the function f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    Parameters:\n        start_x (float): The starting x value for the search.\n        learning_rate (float): The step size for gradient descent.\n        tolerance (float): The tolerance for convergence.\n        max_iters (int): The maximum number of iterations.\n    Returns:\n        float: The x value that minimizes f(x), rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        x_next = x - learning_rate * df(x)\n        if np.abs(x_next - x) < tolerance:\n            break\n        x = x_next\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 6, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f_prime(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    iters = 0\n    while iters < max_iters:\n        gradient = f_prime(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) <= tolerance:\n            return round(new_x, 4)\n        x = new_x\n        iters += 1\n    return round(x, 4)\nstart_x = 3.5"}
{"task_id": 127, "completion_id": 7, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the minimum of the function f(x) = x^4 - 3x^3 + 2 starting from start_x.\n    Parameters:\n    - start_x: Initial x position for gradient descent\n    - learning_rate: Step size for each iteration\n    - tolerance: Threshold for stopping the iterations when changes in x are small\n    - max_iters: Maximum number of iterations to run\n    \n    Returns:\n    - The x value at which the function reaches its minimum, rounded to 4 decimal places.\n    \"\"\"\n    x = start_x\n    step = np.inf\n    iters = 0\n    while abs(step) > tolerance and iters < max_iters:\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        step = -learning_rate * gradient\n        x += step\n        iters += 1\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 8, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the x value at which f(x) = x^4 - 3x^3 + 2 reaches its global minimum.\n    Uses gradient descent to approximate the optimal point, starting from start_x.\n    \n    Parameters:\n    start_x (float): The initial x position to start the search.\n    learning_rate (float): The step size for each iteration.\n    tolerance (float): The minimum difference in x values to declare convergence.\n    max_iters (int): Maximum number of iterations allowed before terminating the search.\n    \n    Returns:\n    float: The x value at which the function has a global minimum, rounded to 4 decimal places.\n    \"\"\"\n\n    def gradient(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        gradient_value = gradient(current_x)\n        next_x = current_x - learning_rate * gradient_value\n        if abs(next_x - current_x) < tolerance:\n            break\n        current_x = next_x\n    return round(current_x, 4)\nstart_x = np.random.uniform(-10, 10)"}
{"task_id": 127, "completion_id": 9, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input array x.\n    \n    Parameters:\n    x (np.ndarray): The input array to be transformed.\n    alpha (float): The scaling factor for the input array.\n    gamma (float): The offset factor applied after scaling.\n    beta (float): The factor applied after the tanh transformation to shift the output.\n    \n    Returns:\n    list[float]: The transformed array as a list of floats, rounded to 4 decimal places.\n    \"\"\"\n    transformed = np.tanh(alpha * x + gamma) + beta\n    return np.round(transformed, decimals=4).tolist()\nx = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\nalpha = 1.0\ngamma = 0.0\nbeta = 1.0"}
{"task_id": 128, "completion_id": 1, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Computes the Dynamic Tanh (DyT) transformation of a numpy array.\n    The function applies the formula: gamma * np.tanh(alpha * x + beta)\n    and returns the results as a list of float values rounded to 4 decimal places.\n    \n    Args:\n    - x: numpy.ndarray: The input array of features.\n    - alpha: float: Scaling factor for the tanh input.\n    - gamma: float: Scaling factor for the tanh output.\n    - beta: float: Bias for the tanh input.\n    \n    Returns:\n    - list[float]: The DyT-transformed values rounded to 4 decimal places.\n    \"\"\"\n    dyt_transformed = gamma * np.tanh(alpha * x + beta)\n    result_list = np.round(dyt_transformed, 4).tolist()\n    return result_list"}
{"task_id": 128, "completion_id": 2, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function.\n    \n    Parameters:\n    x (np.ndarray): An array of input data.\n    alpha (float): Scale factor for input.\n    gamma (float): Scale factor for tanh output.\n    beta (float): Bias for output.\n    \n    Returns:\n    list[float]: A list of float values after applying the DyT function and rounding to the nearest 4th decimal.\n    \"\"\"\n    scaled_input = alpha * x\n    tanh_output = np.tanh(scaled_input)\n    dyt_result = gamma * tanh_output + beta\n    rounded_result = np.round(dyt_result, 4)\n    return rounded_result.tolist()\nx = np.random.rand(10)\nalpha = 1.0\ngamma = 1.0\nbeta = 0.0"}
{"task_id": 128, "completion_id": 3, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function for normalization-free transformers.\n    DyT is given by alpha * (Tanh(gamma * x) - beta).\n    \n    Parameters:\n    x: np.ndarray\n        The input data to the DyT function.\n    alpha: float\n        The scaling factor.\n    gamma: float\n        The slope scale factor.\n    beta: float\n        The output shift factor.\n        \n    Returns:\n    list[float]\n        The transformed output as a list of floats, rounded to 4 decimal places.\n    \"\"\"\n    result = alpha * (np.tanh(gamma * x) - beta)\n    return np.round(result, decimals=4).tolist()"}
{"task_id": 128, "completion_id": 4, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which adjusts the tanh function\n    with additional parameters to replace layer normalization in Transformer architectures.\n    \n    Parameters:\n    x (np.ndarray): Input array.\n    alpha (float): Scale factor for the input.\n    gamma (float): Scale factor for the tanh output.\n    beta (float): Bias added after scaling with gamma.\n    \n    Returns:\n    list[float]: List of transformed values rounded to the nearest 4th decimal.\n    \"\"\"\n    transformed = gamma * np.tanh(alpha * x) + beta\n    return transformed.round(4).tolist()"}
{"task_id": 128, "completion_id": 5, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input data.\n\n    Parameters:\n    x (np.ndarray): 2D input data array.\n    alpha (float): Scaling factor for input data.\n    gamma (float): Scaling factor for the tanh output.\n    beta (float): Bias added to the tanh output.\n\n    Returns:\n    list[float]: List of transformed values rounded to 4 decimal places.\n    \"\"\"\n    scaled_x = alpha * x\n    dyt = gamma * np.tanh(scaled_x) + beta\n    dyt_rounded = np.round(dyt, 4)\n    return dyt_rounded.tolist()"}
{"task_id": 128, "completion_id": 6, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implement the Dynamic Tanh (DyT) function which applies a modified tanh activation.\n    This variant aims to maintain stability and squashing behavior:\n    DyT(x) = gamma * np.tanh(alpha * x) + beta\n    \n    Parameters:\n    x: Array of input values.\n    alpha: Scaling factor for the input.\n    gamma: Scaling factor for the output.\n    beta: Offset value.\n    \n    Returns:\n    A list of float values, each rounded to 4 decimal places.\n    \"\"\"\n    result = gamma * np.tanh(alpha * x) + beta\n    return np.round(result, decimals=4).tolist()\nx = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\nalpha = 1.0\ngamma = 2.0\nbeta = 0.5"}
{"task_id": 128, "completion_id": 7, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Compute the Dynamic Tanh (DyT) transformation on the input array x.\n    This transformation exaggerates the standard behavior of the Tanh function\n    by incorporating the scaling factor gamma, location bias beta, and slope factor alpha.\n    \n    Args:\n    x (np.ndarray): The input data.\n    alpha (float): The slope factor of DyT to control the curvature of the function.\n    gamma (float): The scaling factor of DyT, affecting the range of output.\n    beta (float): The bias factor that shifts the DyT curve along the x-axis.\n\n    Returns:\n    list[float]: The transformed data after applying Dynamic Tanh, with values rounded to the 4th decimal.\n    \"\"\"\n    dyt = gamma * np.tanh(alpha * (x - beta))\n    return np.round(dyt, 4).tolist()"}
{"task_id": 128, "completion_id": 8, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function which is normalization-free transformation.\n    The DyT function parameters are alpha, gamma, and beta which shape the dynamic tanh function.\n    The function returns the transformed values of the input x as a list of floats rounded to the fourth decimal.\n    \"\"\"\n    transformed_x = alpha * np.tanh(gamma * x + beta)\n    rounded_x = np.round(transformed_x, decimals=4)\n    return rounded_x.tolist()"}
{"task_id": 128, "completion_id": 9, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh function element-wise to a numpy array with given parameters.\n    Dynamic Tanh (DyT) modifies the output of Tanh by scaling, shifting, and squashing the values of the input.\n    Formula used: DyT(x) = alpha * tanh(gamma * x) + beta\n    \"\"\"\n    dyt_out = alpha * np.tanh(gamma * x) + beta\n    return np.round(dyt_out, 4).tolist()"}
