{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 1, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(b) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 2, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 3, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(b) == 0:\n        return -1\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 4, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot)\n    return result"}
{"task_id": 1, "completion_id": 5, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        dot_product = sum((row[i] * b[i] for i in range(len(row))))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 6, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if any((len(row) != len(b) for row in a)):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 7, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 8, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 1, "completion_id": 9, "solution": "from typing import List\ndef matrix_dot_vector(a: List[List[int | float]], b: List[int | float]) -> List[int | float]:\n    if any((len(row) != len(b) for row in a)):\n        return -1\n    result = [sum((x * y for (x, y) in zip(row, b))) for row in a]\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list[list[int | float]]): A matrix represented as a list of lists.\n    \n    Returns:\n    list[list[int | float]]: The transpose of the matrix.\n    \"\"\"\n    return [list(row) for row in zip(*a)]"}
{"task_id": 2, "completion_id": 1, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n        a (list of list of int or float): The input matrix.\n        \n    Returns:\n        list of list of int or float: The transposed matrix.\n    \"\"\"\n    return [[a[i][j] for i in range(len(a))] for j in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 2, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 0\n    transposed = [[a[i][j] for i in range(rows)] for j in range(cols)]\n    return transposed"}
{"task_id": 2, "completion_id": 3, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [[a[i][j] for i in range(len(a))] for j in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 4, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    :param a: A matrix represented as a list of lists of integers or floats\n    :return: The transpose of the matrix\n    \"\"\"\n    return [[row[i] for row in a] for i in range(len(a[0]))]"}
{"task_id": 2, "completion_id": 5, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]"}
{"task_id": 2, "completion_id": 6, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n\n    Parameters:\n        a (list of list of int/float): A 2D matrix represented as a list of lists.\n\n    Returns:\n        list of list of int/float: The transposed matrix.\n    \"\"\"\n    if not a or not a[0]:\n        return []\n    rows = len(a)\n    cols = len(a[0])\n    return [[a[i][j] for i in range(rows)] for j in range(cols)]"}
{"task_id": 2, "completion_id": 7, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 0\n    transpose = [[0 for _ in range(rows)] for _ in range(cols)]\n    for i in range(rows):\n        for j in range(cols):\n            transpose[j][i] = a[i][j]\n    return transpose"}
{"task_id": 2, "completion_id": 8, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]"}
{"task_id": 2, "completion_id": 9, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    rows = len(a)\n    cols = len(a[0]) if rows > 0 else 0\n    transposed = [[0 for _ in range(rows)] for _ in range(cols)]\n    for i in range(rows):\n        for j in range(cols):\n            transposed[j][i] = a[i][j]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    original_shape = (len(a), len(a[0]) if a else 0)\n    original_size = original_shape[0] * original_shape[1]\n    new_size = new_shape[0] * new_shape[1]\n    if original_size != new_size:\n        return []\n    a_np = np.array(a)\n    try:\n        reshaped = np.reshape(a_np, new_shape)\n        return reshaped.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 1, "solution": "def reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    arr = np.array(a)\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    try:\n        return np.reshape(arr, new_shape).tolist()\n    except:\n        return []"}
{"task_id": 3, "completion_id": 2, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    flat_list = [item for row in a for item in row]\n    if len(flat_list) != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = np.reshape(np.array(flat_list), new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 3, "completion_id": 3, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        arr = np.array(a)\n        reshaped_arr = arr.reshape(new_shape)\n        return reshaped_arr.tolist()\n    except ValueError:\n        return []"}
{"task_id": 3, "completion_id": 4, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    original_shape = (len(a), len(a[0]) if a else 0)\n    total_elements = original_shape[0] * original_shape[1]\n    new_total = new_shape[0] * new_shape[1]\n    if total_elements != new_total:\n        return []\n    flat_matrix = [item for sublist in a for item in sublist]\n    reshaped = np.array(flat_matrix).reshape(new_shape)\n    return reshaped.tolist()"}
{"task_id": 3, "completion_id": 5, "solution": "def reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes the given matrix into the new specified shape.\n    If the reshape is not possible (i.e., incorrect number of elements), returns an empty list.\n    \n    Args:\n        a (list of list of int/float): The original matrix.\n        new_shape (tuple of int): The desired shape (rows, columns).\n    \n    Returns:\n        list of list of int/float: The reshaped matrix or an empty list if not possible.\n    \"\"\"\n    a_np = np.array(a)\n    original_size = a_np.size\n    new_shape_size = new_shape[0] * new_shape[1]\n    if original_size != new_shape_size:\n        return []\n    reshaped = a_np.reshape(new_shape).tolist()\n    return reshaped"}
{"task_id": 3, "completion_id": 6, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    a_np = np.array(a)\n    if a_np.size != new_shape[0] * new_shape[1]:\n        return []\n    return np.reshape(a_np, new_shape).tolist()"}
{"task_id": 3, "completion_id": 7, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    flat = [item for sublist in a for item in sublist]\n    if len(flat) != new_shape[0] * new_shape[1]:\n        return []\n    arr = np.array(flat)\n    reshaped_arr = arr.reshape(*new_shape)\n    return reshaped_arr.tolist()"}
{"task_id": 3, "completion_id": 8, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given matrix into a new shape.\n    If reshaping is not possible, returns an empty list.\n    \n    Args:\n        a: Original 2D matrix (list of lists).\n        new_shape: Tuple containing the desired number of rows and columns.\n    \n    Returns:\n        Reshaped matrix as a list of lists, or an empty list if not possible.\n    \"\"\"\n    arr = np.array(a)\n    original_size = arr.size\n    new_size = new_shape[0] * new_shape[1]\n    if original_size != new_size:\n        return []\n    reshaped = arr.reshape(new_shape)\n    return reshaped.tolist()"}
{"task_id": 3, "completion_id": 9, "solution": "def reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    arr = np.array(a)\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    return arr.reshape(new_shape).tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        return [sum((matrix[row][col] for row in range(num_rows))) / num_rows for col in range(num_cols)]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 4, "completion_id": 1, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0]) if matrix else 0\n        return [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 2, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        return [sum((matrix[row][col] for row in range(num_rows))) / num_rows for col in range(num_cols)]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 4, "completion_id": 3, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or column.\n\n    Parameters:\n    - matrix: list[list[float]] - The input matrix.\n    - mode: str - 'row' or 'column' to specify the calculation mode.\n\n    Returns:\n    - list[float]: List of means based on the specified mode.\n    \"\"\"\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if matrix else 0\n        transposed = zip(*matrix)\n        return [sum(column) / num_rows for column in transposed]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 4, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of each row or column in a matrix.\n\n    Parameters:\n    - matrix (list[list[float]]): A 2D list (matrix) of numbers.\n    - mode (str): 'row' to calculate means row-wise, 'column' for column-wise.\n\n    Returns:\n    - list[float]: A list of mean values based on the specified mode.\n    \"\"\"\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed = list(zip(*matrix))\n        return [sum(col) / len(col) for col in transposed]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 4, "completion_id": 5, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        means = []\n        for col_index in range(num_cols):\n            column_sum = sum((matrix[row_index][col_index] for row_index in range(num_rows)))\n            means.append(column_sum / num_rows)\n        return means\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 6, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        return [sum((matrix[i][j] for i in range(len(matrix)))) / len(matrix) for j in range(num_columns)]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")"}
{"task_id": 4, "completion_id": 7, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or column.\n\n    Args:\n        matrix: A 2D list of floats.\n        mode: A string specifying the mode, either 'row' or 'column'.\n\n    Returns:\n        A list of mean values corresponding to each row or column.\n    \"\"\"\n    if mode == 'row':\n        return [sum(row) / len(row) if row else 0.0 for row in matrix]\n    elif mode == 'column':\n        if not matrix or not matrix[0]:\n            return []\n        return [sum((matrix[i][j] for i in range(len(matrix)))) / len(matrix) for j in range(len(matrix[0]))]\n    else:\n        raise ValueError(\"Invalid mode. Expected 'row' or 'column'.\")"}
{"task_id": 4, "completion_id": 8, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either row-wise or column-wise.\n\n    Args:\n        matrix (list[list[float]]): A 2D list (matrix) of floats.\n        mode (str): Either 'row' or 'column' to specify the axis for calculation.\n\n    Returns:\n        list[float]: A list of mean values according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed = zip(*matrix)\n        return [sum(col) / len(col) for col in transposed]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 4, "completion_id": 9, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        return [sum((matrix[row][col] for row in range(num_rows))) / num_rows for col in range(num_cols)]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Parameters:\n    - matrix (list of lists of int or float): The input matrix.\n    - scalar (int or float): The scalar value to multiply with the matrix.\n    \n    Returns:\n    - list of lists of int or float: The scalar-multiplied matrix.\n    \"\"\"\n    result = [[element * scalar for element in row] for row in matrix]\n    return result"}
{"task_id": 5, "completion_id": 1, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the scalar and returns the resulting matrix.\n    \n    Parameters:\n    matrix (list of list of int/float): Input matrix to be multiplied.\n    scalar (int/float): Scalar value for multiplication.\n    \n    Returns:\n    list of list of int/float: Resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 2, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Parameters:\n    - matrix (list[list[int | float]]): The input matrix to be multiplied.\n    - scalar (int | float): The scalar value to multiply with the matrix.\n\n    Returns:\n    - list[list[int | float]]: The matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 3, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a matrix by a scalar.\n    \n    Args:\n        matrix: A 2D list of integers or floats.\n        scalar: An integer or float to multiply with each element of the matrix.\n    \n    Returns:\n        A new 2D list representing the result of the scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 4, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    Parameters:\n    matrix (list of list of int or float): The input matrix.\n    scalar (int or float): The scalar value to multiply with.\n\n    Returns:\n    list of list of int or float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[element * scalar for element in row] for row in matrix]\n    return result"}
{"task_id": 5, "completion_id": 5, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar value.\n    \n    Parameters:\n        matrix (list of lists): A matrix containing integers or floats.\n        scalar (int or float): The scalar value to multiply with the matrix.\n        \n    Returns:\n        list of lists: A new matrix where each element is multiplied by the scalar.\n    \"\"\"\n    result = [[element * scalar for element in row] for row in matrix]\n    return result"}
{"task_id": 5, "completion_id": 6, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar.\n    \n    Parameters:\n        matrix (list of list of int/float): The input matrix.\n        scalar (int or float): The scalar value to multiply the matrix by.\n    \n    Returns:\n        list of list of int/float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = []\n    for row in matrix:\n        result_row = [element * scalar for element in row]\n        result.append(result_row)\n    return result"}
{"task_id": 5, "completion_id": 7, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the given scalar.\n    \n    :param matrix: A 2D list of integers or floats.\n    :param scalar: An integer or float to multiply the matrix by.\n    :return: A new 2D list which is the scalar multiplication result.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 8, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Args:\n        matrix: A list of lists representing the matrix to be multiplied.\n        scalar: The scalar value to multiply with.\n    \n    Returns:\n        A new matrix with the same dimensions where each element is multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 5, "completion_id": 9, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Parameters:\n    matrix (list of list of int/float): The input matrix.\n    scalar (int/float): The scalar to multiply with the matrix.\n    \n    Returns:\n    list of list of int/float: The matrix after scalar multiplication.\n    \"\"\"\n    result = [[cell * scalar for cell in row] for row in matrix]\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigen1 = (trace + math.sqrt(discriminant)) / 2\n    eigen2 = (trace - math.sqrt(discriminant)) / 2\n    return sorted([eigen1, eigen2], reverse=True)"}
{"task_id": 6, "completion_id": 1, "solution": "from typing import List\nimport math\ndef calculate_eigenvalues(matrix: List[List[float | int]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Parameters:\n        matrix (List[List[float | int]]): A 2x2 matrix represented as a list of lists.\n        \n    Returns:\n        List[float]: A list of two eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('The input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)"}
{"task_id": 6, "completion_id": 2, "solution": "from typing import List\nfrom math import sqrt\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = sqrt(discriminant)\n    lambda1 = (trace + sqrt_discriminant) / 2\n    lambda2 = (trace - sqrt_discriminant) / 2\n    return [lambda1, lambda2]"}
{"task_id": 6, "completion_id": 3, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    det = a * d - b * c\n    discriminant = trace ** 2 - 4 * det\n    lambda1 = (trace + math.sqrt(discriminant)) / 2\n    lambda2 = (trace - math.sqrt(discriminant)) / 2\n    return [lambda1, lambda2] if lambda1 > lambda2 else [lambda2, lambda1]\nmatrix = [[4, 1], [2, 3]]"}
{"task_id": 6, "completion_id": 4, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    return [eigenvalue1, eigenvalue2] if eigenvalue1 > eigenvalue2 else [eigenvalue2, eigenvalue1]"}
{"task_id": 6, "completion_id": 5, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    tr = a + d\n    det = a * d - b * c\n    discriminant = tr ** 2 - 4 * det\n    sqrt_discriminant = math.sqrt(discriminant)\n    lambda1 = (tr + sqrt_discriminant) / 2\n    lambda2 = (tr - sqrt_discriminant) / 2\n    return [lambda1, lambda2] if lambda1 >= lambda2 else [lambda2, lambda1]"}
{"task_id": 6, "completion_id": 6, "solution": "from math import sqrt\nfrom typing import List\ndef calculate_eigenvalues(matrix: List[List[float | int]]) -> List[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n\n    Parameters:\n    matrix (List[List[float | int]]): A 2x2 matrix represented as a list of lists.\n\n    Returns:\n    List[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    lambda1 = (trace + sqrt(discriminant)) / 2\n    lambda2 = (trace - sqrt(discriminant)) / 2\n    return [lambda1, lambda2] if lambda1 >= lambda2 else [lambda2, lambda1]"}
{"task_id": 6, "completion_id": 7, "solution": "import math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted in descending order.\n    \n    Parameters:\n        matrix (list[list[float | int]]): A 2x2 matrix represented as a list of lists.\n        \n    Returns:\n        list[float]: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    if not (len(matrix) == 2 and all((len(row) == 2 for row in matrix))):\n        raise ValueError('Matrix must be 2x2.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = math.sqrt(trace ** 2 - 4 * determinant)\n    eigenvalue1 = (trace + discriminant) / 2\n    eigenvalue2 = (trace - discriminant) / 2\n    return [eigenvalue1, eigenvalue2] if eigenvalue1 >= eigenvalue2 else [eigenvalue2, eigenvalue1]\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 6, "completion_id": 8, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    return [eigenvalue1, eigenvalue2] if eigenvalue1 > eigenvalue2 else [eigenvalue2, eigenvalue1]"}
{"task_id": 6, "completion_id": 9, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted in descending order.\n\n    Parameters:\n    matrix (list[list[float | int]]): A 2x2 matrix represented as a list of two lists.\n\n    Returns:\n    list[float]: A list with the two eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = discriminant ** 0.5\n    eigenvalue1 = (trace + sqrt_discriminant) / 2\n    eigenvalue2 = (trace - sqrt_discriminant) / 2\n    eigenvalues = [eigenvalue1, eigenvalue2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_mat = np.array(A, dtype=float)\n        T_mat = np.array(T, dtype=float)\n        S_mat = np.array(S, dtype=float)\n        if T_mat.shape[0] != T_mat.shape[1] or S_mat.shape[0] != S_mat.shape[1]:\n            raise np.linalg.LinAlgError('T and S must be square matrices')\n        if np.isclose(np.linalg.det(T_mat), 0) or np.isclose(np.linalg.det(S_mat), 0):\n            return -1\n        T_inv = np.linalg.inv(T_mat)\n        transformed = T_inv @ A_mat @ S_mat\n        transformed = np.round(transformed, 4)\n        return transformed.tolist()\n    except np.linalg.LinAlgError:\n        return -1\n    except Exception:\n        return -1"}
{"task_id": 7, "completion_id": 1, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        if A_np.shape[0] != T_np.shape[0] or A_np.shape[1] != S_np.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T), 0) or np.isclose(np.linalg.det(S), 0):\n            return -1\n        result = np.round(np.linalg.inv(T_np) @ A_np @ S_np, 4)\n        return result.tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 7, "completion_id": 2, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A = np.array(A)\n        T = np.array(T)\n        S = np.array(S)\n        if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T), 0) or np.isclose(np.linalg.det(S), 0):\n            return -1\n        T_inv = np.linalg.inv(T)\n        result = T_inv @ A @ S\n        return np.around(result, decimals=4).tolist()\n    except Exception as e:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 3]]\nS = [[1, 1], [0, 1]]\nresult = transform_matrix(A, T, S)"}
{"task_id": 7, "completion_id": 3, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS.\n\n    Parameters:\n    A (list of lists): The input matrix to transform.\n    T (list of lists): The matrix for left transformation (must be square and invertible).\n    S (list of lists): The matrix for right transformation (must be square and invertible).\n\n    Returns:\n    list of lists: Transformed matrix as a nested Python list, rounded to 4 decimal places.\n    \"\"\"\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    if T_np.shape[0] != A_np.shape[0] or S_np.shape[1] != A_np.shape[1]:\n        return -1\n    if np.isclose(np.linalg.det(T_np), 0):\n        return -1\n    if np.isclose(np.linalg.det(S_np), 0):\n        return -1\n    transformed = np.around(np.linalg.inv(T_np) @ A_np @ S_np, 4)\n    return transformed.tolist()"}
{"task_id": 7, "completion_id": 4, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A = np.array(A, dtype=float)\n        T = np.array(T, dtype=float)\n        S = np.array(S, dtype=float)\n        if T.shape[0] != T.shape[1] or np.isclose(np.linalg.det(T), 0):\n            return -1\n        if S.shape[0] != S.shape[1] or np.isclose(np.linalg.det(S), 0):\n            return -1\n        T_inv = np.linalg.inv(T)\n        transformed = T_inv @ A @ S\n        transformed = np.round(transformed, 4)\n        return transformed.tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 7, "completion_id": 5, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T_np), 0):\n            return -1\n        if np.isclose(np.linalg.det(S_np), 0):\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        result = np.round(T_inv @ A_np @ S_np, 4)\n        return result.tolist()\n    except Exception as e:\n        return -1"}
{"task_id": 7, "completion_id": 6, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n            return -1\n        T_inv = np.linalg.inv(T_np)\n        transformed = T_inv @ A_np @ S_np\n        return np.around(transformed, 4).tolist()\n    except Exception as e:\n        return -1"}
{"task_id": 7, "completion_id": 7, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[float]] | int:\n    try:\n        A = np.array(A, dtype=float)\n        T = np.array(T, dtype=float)\n        S = np.array(S, dtype=float)\n        if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T), 0):\n            return -1\n        if np.isclose(np.linalg.det(S), 0):\n            return -1\n        result = np.linalg.inv(T) @ A @ S\n        result = np.round(result, 4)\n        return result.tolist()\n    except Exception as e:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 1], [0, 1]]\nresult = transform_matrix(A, T, S)"}
{"task_id": 7, "completion_id": 8, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[float]]:\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.isclose(np.linalg.det(T), 0):\n        return -1\n    if np.isclose(np.linalg.det(S), 0):\n        return -1\n    try:\n        result = np.linalg.inv(T) @ A @ S\n        result = np.round(result, 4)\n        return result.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 1], [0, 1]]"}
{"task_id": 7, "completion_id": 9, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A = np.array(A, dtype=float)\n        T = np.array(T, dtype=float)\n        S = np.array(S, dtype=float)\n        if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T), 0.0) or np.isclose(np.linalg.det(S), 0.0):\n            return -1\n        result = np.linalg.inv(T) @ A @ S\n        result = np.round(result, 4)\n        return result.tolist()\n    except Exception:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None"}
{"task_id": 8, "completion_id": 1, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inv"}
{"task_id": 8, "completion_id": 2, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_determinant = 1 / determinant\n    inverse = [[d * inv_determinant, -b * inv_determinant], [-c * inv_determinant, a * inv_determinant]]\n    return inverse"}
{"task_id": 8, "completion_id": 3, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix\nmatrix = [[4, 7], [2, 6]]"}
{"task_id": 8, "completion_id": 4, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse"}
{"task_id": 8, "completion_id": 5, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv = [[d / det, -b / det], [-c / det, a / det]]\n    return inv"}
{"task_id": 8, "completion_id": 6, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inv"}
{"task_id": 8, "completion_id": 7, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input must be a 2x2 matrix.')\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inv"}
{"task_id": 8, "completion_id": 8, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv = [[d / det, -b / det], [-c / det, a / det]]\n    return inv"}
{"task_id": 8, "completion_id": 9, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv = [[d / det, -b / det], [-c / det, a / det]]\n    return inv"}
{"task_id": 9, "completion_id": 0, "solution": "from typing import list\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\na = [[1, 2, 3], [4, 5, 6]]\nb = [[7, 8], [9, 10]]"}
{"task_id": 9, "completion_id": 1, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 2, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 3, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 4, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 5, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 6, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\na = [[1, 2, 3], [4, 5, 6]]\nb = [[7, 8], [9, 10], [11, 12]]"}
{"task_id": 9, "completion_id": 7, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 8, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 9, "completion_id": 9, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Computes the covariance matrix for a given set of feature vectors.\n    \n    Parameters:\n        vectors (list[list[float]]): A list of lists, where each inner list\n                                   represents a feature with its observations.\n    \n    Returns:\n        list[list[float]]: The covariance matrix where the element at position [i][j]\n                          is the covariance between the i-th and j-th features.\n    \"\"\"\n    n_samples = len(vectors[0])\n    for v in vectors:\n        if len(v) != n_samples:\n            raise ValueError('All feature vectors must have the same length.')\n    n_features = len(vectors)\n    means = [sum(feature) / n_samples for feature in vectors]\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            covariance = sum(((vectors[i][k] - means[i]) * (vectors[j][k] - means[j]) for k in range(n_samples))) / (n_samples - 1)\n            cov_matrix[i][j] = covariance\n    return cov_matrix"}
{"task_id": 10, "completion_id": 1, "solution": "from typing import List\nimport numpy as np\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculates the covariance matrix of the given list of feature vectors.\n    \n    Args:\n        vectors: A list of lists where each inner list represents a feature.\n    \n    Returns:\n        A covariance matrix as a list of lists [float].\n    \"\"\"\n    num_features = len(vectors)\n    num_observations = len(vectors[0])\n    transpose = [[vectors[feature][obs] for feature in range(num_features)] for obs in range(num_observations)]\n    means = [sum(x) / num_observations for x in vectors]\n    centered = [[observation[i] - means[i] for i in range(num_features)] for observation in transpose]\n    covariance_matrix = [[sum((centered[i][k] * centered[j][k] for k in range(num_observations))) / (num_observations - 1) for j in range(num_features)] for i in range(num_features)]\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 2, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a list of feature vectors (each list contains the observations).\n\n    Args:\n        vectors (list[list[float]]): A list of vectors (features), where each vector is a list of float values.\n\n    Returns:\n        list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    n = len(vectors[0])\n    m = len(vectors)\n    if not all((len(vec) == n for vec in vectors)):\n        raise ValueError('All feature vectors must have the same length.')\n    means = [sum(vector) / n for vector in vectors]\n    covariance_matrix = [[0.0 for _ in range(m)] for _ in range(m)]\n    for i in range(m):\n        for j in range(m):\n            cov = 0.0\n            for k in range(n):\n                cov += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance_matrix[i][j] = cov / (n - 1)\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 3, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n_vectors = len(vectors)\n    n_samples = len(vectors[0])\n    for vec in vectors:\n        if len(vec) != n_samples:\n            raise ValueError('All vectors must have the same number of observations.')\n    means = [sum(vec) / n_samples for vec in vectors]\n    cov_matrix = [[0.0 for _ in range(n_vectors)] for _ in range(n_vectors)]\n    for i in range(n_vectors):\n        for j in range(n_vectors):\n            covariance = sum(((vectors[i][k] - means[i]) * (vectors[j][k] - means[j]) for k in range(n_samples)))\n            covariance /= n_samples\n            cov_matrix[i][j] = covariance\n    return cov_matrix"}
{"task_id": 10, "completion_id": 4, "solution": "from typing import List\nfrom statistics import mean\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for the given list of feature vectors.\n    \n    Parameters:\n    - vectors (List[List[float]]): A list where each inner list corresponds to a feature and contains observations.\n\n    Returns:\n    - List[List[float]]: Covariance matrix as a 2D list.\n    \"\"\"\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    if any((len(vector) != n_observations for vector in vectors)):\n        raise ValueError('All vectors must have the same number of observations')\n    means = [mean(vector) for vector in vectors]\n    cov_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            cov = 0.0\n            for k in range(n_observations):\n                cov += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov /= n_observations - 1\n            cov_matrix[i][j] = cov\n    return cov_matrix"}
{"task_id": 10, "completion_id": 5, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Computes the covariance matrix for a given list of feature vectors.\n    \n    Parameters:\n        vectors (list of lists): A list where each inner list represents a different feature.\n    \n    Returns:\n        list of lists: The covariance matrix.\n    \"\"\"\n    n = len(vectors)\n    m = len(vectors[0])\n    if not all((len(v) == m for v in vectors)):\n        raise ValueError('All feature vectors must be of the same length.')\n    means = [sum(feature) / m for feature in vectors]\n    centered_vectors = [[x - mean for x in feature] for (feature, mean) in zip(vectors, means)]\n    covariance = [[0.0 for _ in range(n)] for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            covariance[i][j] = sum((centered_vectors[i][k] * centered_vectors[j][k] for k in range(m))) / (m - 1)\n    return covariance"}
{"task_id": 10, "completion_id": 6, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given list of feature vectors.\n    \n    Parameters:\n    vectors (List[List[float]]): A list of feature lists, where each list contains the observations\n                                 for a single feature. All lists must be of the same length.\n    \n    Returns:\n    List[List[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    n = len(vectors[0])\n    m = len(vectors)\n    means = [sum(vector) / n for vector in vectors]\n    covariance_matrix = [[0.0 for _ in range(m)] for _ in range(m)]\n    for i in range(m):\n        for j in range(m):\n            covariance = sum(((vectors[i][k] - means[i]) * (vectors[j][k] - means[j]) for k in range(n))) / (n - 1)\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix"}
{"task_id": 10, "completion_id": 7, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    if not vectors:\n        raise ValueError('The vectors list is empty.')\n    n = len(vectors[0])\n    if any((len(vector) != n for vector in vectors)):\n        raise ValueError('All vectors must have the same length.')\n    means = [sum(vector) / n for vector in vectors]\n    centered_vectors = [[x - m for (x, m) in zip(vector, means)] for vector in vectors]\n    cov_matrix = [[0.0] * len(vectors) for _ in range(len(vectors))]\n    for i in range(len(vectors)):\n        for j in range(len(vectors)):\n            covariance = sum((centered_vectors[i][k] * centered_vectors[j][k] for k in range(n))) / (n - 1)\n            cov_matrix[i][j] = covariance\n            if i != j:\n                cov_matrix[j][i] = covariance\n    return cov_matrix"}
{"task_id": 10, "completion_id": 8, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Computes the covariance matrix of the given vectors.\n    \n    Parameters:\n        vectors (list of list of float): A list of features, where each sublist is a list of observations.\n    \n    Returns:\n        list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    n = len(vectors[0])\n    num_features = len(vectors)\n    means = [sum(feature) / n for feature in vectors]\n    cov_matrix = [[0.0 for _ in range(num_features)] for _ in range(num_features)]\n    for i in range(num_features):\n        for j in range(num_features):\n            cov = 0.0\n            for k in range(n):\n                cov += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            cov /= n - 1\n            cov_matrix[i][j] = cov\n    return cov_matrix"}
{"task_id": 10, "completion_id": 9, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    for vec in vectors:\n        if len(vec) != n_observations:\n            raise ValueError('All vectors must have the same number of observations.')\n    means = [sum(vec) / n_observations for vec in vectors]\n    covariance_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            covariance = 0.0\n            for k in range(n_observations):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= n_observations - 1\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    assert A.shape[0] == A.shape[1] and A.shape[0] == b.shape[0], 'Invalid dimensions'\n    x = np.zeros_like(b, dtype=np.float64)\n    x_new = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s = np.sum((A[i, j] * x[j] for j in range(A.shape[1]) if j != i))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new.copy()\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 1, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi method for n iterations.\n    \n    Parameters:\n    - A : np.ndarray - The coefficient matrix of the system (n x n).\n    - b : np.ndarray - The constant vector (n x 1).\n    - n : int - Number of iterations to perform.\n    \n    Returns:\n    - list - The approximate solution vector rounded to 4 decimals and converted to a list.\n    \"\"\"\n    N = A.shape[0]\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s = 0\n            for j in range(N):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 2, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    Iterates n times and rounds intermediate solution values to 4 decimals.\n    \n    Parameters:\n    A : np.ndarray -- coefficient matrix\n    b : np.ndarray -- solution vector\n    n : int -- number of iterations\n    \n    Returns:\n    list -- approximate solution x after n iterations, rounded to 4 decimals\n    \"\"\"\n    (m, _) = A.shape\n    x = np.zeros(m)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(m):\n            s = np.sum((A[i, j] * x[j] for j in range(m) if j != i))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 3, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi method.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix (n x n)\n        b (np.ndarray): Right-hand side vector (n x 1)\n        n (int): Number of iterations\n    \n    Returns:\n        list: Approximate solution vector x after n iterations, rounded to 4 decimals.\n    \"\"\"\n    x = np.zeros_like(b, dtype=np.float64)\n    size = len(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(size):\n            s = sum((A[i, j] * x[j] for j in range(size) if j != i))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 11, "completion_id": 4, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if b.ndim == 1:\n        b = b.reshape(-1, 1)\n    elif b.shape[1] != 1:\n        raise ValueError('Vector b must be a column vector.')\n    x = np.zeros_like(b, dtype=np.float64)\n    D = np.diag(np.diag(A))\n    LU = A - D\n    for _ in range(n):\n        x = np.dot(np.linalg.inv(D), b - np.dot(LU, x))\n        x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[4, 1, -1], [1, 4, -1], [0, 1, 4]], dtype=np.float64)\nb = np.array([[5], [5], [5]], dtype=np.float64)\nn = 10"}
{"task_id": 11, "completion_id": 5, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system Ax = b using the Jacobi iteration method.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix (n x n).\n        b (np.ndarray): Constant vector (n x 1).\n        n (int): Number of iterations to perform.\n    \n    Returns:\n        list: Approximate solution vector x after n iterations, rounded to 4 decimals.\n    \"\"\"\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s = sum((A[i][j] * x[j] for j in range(N) if j != i))\n            x_new[i] = (b[i] - s) / A[i][i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[10, 2], [2, 20]], dtype=np.float64)\nb = np.array([7, 5], dtype=np.float64)\nn = 5"}
{"task_id": 11, "completion_id": 6, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix (n x n).\n    b (np.ndarray): Constant vector (n x 1).\n    n (int): Number of iterations to perform.\n    \n    Returns:\n    list: Approximate solution x as a flattened list after n iterations,\n          with each value rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    b = np.array(b, dtype=np.float64).flatten()\n    size = A.shape[0]\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(size):\n            s = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = round((b[i] - s) / A[i, i], 4)\n        x = x_new\n    return round(x, 4).tolist()"}
{"task_id": 11, "completion_id": 7, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Matrix A and vector b must have the same number of rows.')\n    size = A.shape[0]\n    x = np.zeros(size)\n    for _ in range(n):\n        x_new = np.zeros(size)\n        for i in range(size):\n            sum_terms = np.sum(A[i, :] * x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - sum_terms) / A[i, i]\n        x = np.round(x_new, 4)\n    return list(np.round(x, 4).tolist())"}
{"task_id": 11, "completion_id": 8, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Matrix A and vector b must have the same number of rows.')\n    x = np.zeros_like(b, dtype=np.float64)\n    x_new = np.zeros_like(x)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s = b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = s / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=np.float64)\nb = np.array([4, 7, 3], dtype=np.float64)\nn = 5"}
{"task_id": 11, "completion_id": 9, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s = np.sum((A[i, j] * x[j] for j in range(N) if j != i))\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new\n        x = np.round(x, 4)\n    return x.reshape(-1).tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the singular values and corresponding V and U matrices (approximate up to 4 decimal points)\n    for a 2x2 matrix using Jacobi iteration method for SVD.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 real matrix.\n    \n    Returns:\n        tuple: (U, \u03a3, V^T), where:\n            - U (np.ndarray): Left singular vectors (2x2)\n            - \u03a3 (np.ndarray): Diagonal matrix of singular values (1D array of 2 singular values)\n            - V^T (np.ndarray): Transpose of right singular vectors (2x2)\n    \"\"\"\n    A = A.copy()\n    (m, n) = A.shape\n    assert m == n and n == 2, 'Only 2x2 matrices are supported.'\n    size = A.shape[0]\n    U = np.eye(size)\n    VT = np.eye(size)\n    max_iter = 1000\n    tolerance = 1e-06\n    for _ in range(max_iter):\n        (i, j) = np.unravel_index(np.argmax(np.abs(np.tril(A, -1))), A.shape)\n        aii = A[i, i]\n        aij = A[i, j]\n        aji = A[j, i]\n        ajj = A[j, j]\n        if np.abs(aij) < tolerance:\n            continue\n        theta = 0.5 * np.arctan2(2 * aij, aii - ajj)\n        cos_t = np.cos(theta)\n        sin_t = np.sin(theta)\n        A[[i, j], :] = np.dot(np.array([[cos_t, -sin_t], [sin_t, cos_t]]), A[[i, j], :])\n        A[:, [i, j]] = np.dot(A[:, [i, j]], np.array([[cos_t, -sin_t], [sin_t, cos_t]]))\n        U[:, [i, j]] = np.dot(U[:, [i, j]], np.array([[cos_t, -sin_t], [sin_t, cos_t]]))\n        VT[[i, j], :] = np.dot(np.array([[cos_t, -sin_t], [sin_t, cos_t]]), VT[[i, j], :])\n        if np.abs(aij) < tolerance:\n            break\n    S = np.diag(A).copy()\n    S = np.sqrt(S * S)\n    S = np.sort(S)[::-1]\n    idx = np.argsort(S)[::-1]\n    S = S[idx]\n    U = U[:, idx]\n    VT = VT[:, idx]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    VT = np.round(VT, 4)\n    return (U, S, VT)"}
{"task_id": 12, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def round_all(x):\n        return np.round(x, 4)\n    B = A.copy()\n    max_iter = 1000\n    tol = 1e-10\n    for _ in range(max_iter):\n        off_diag = B[0, 1]\n        if abs(off_diag) < tol:\n            break\n        if B[0, 0] == B[1, 1]:\n            theta = np.pi / 4\n        else:\n            theta = 0.5 * np.arctan2(2 * off_diag, B[0, 0] - B[1, 1])\n        c = np.cos(theta)\n        s = np.sin(theta)\n        Q = np.array([[c, s], [-s, c]], dtype=np.float64)\n        B = Q.T @ B @ Q\n    sigma1 = np.sqrt(np.abs(B[0, 0]))\n    sigma2 = np.sqrt(np.abs(B[1, 1]))\n    sigmas = np.sort([sigma1, sigma2])[::-1]\n    return (round_all(sigmas[0]), round_all(sigmas[1]))"}
{"task_id": 12, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix using the Jacobi method.\n\n    Args:\n        A (np.ndarray): A 2x2 numeric matrix.\n\n    Returns:\n        tuple: A tuple containing two singular values as floats, rounded to 4 decimal places.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    B = A.T @ A\n    a = B[0, 0]\n    b = B[0, 1]\n    c = B[1, 1]\n    eps = 1e-08\n    max_iter = 1000\n    angle = 0\n    for _ in range(max_iter):\n        if abs(b) < eps:\n            break\n        theta = 0.5 * np.arctan(2 * b / (a - c)) if a != c else np.pi / 4\n        ctheta = np.cos(theta)\n        stheta = np.sin(theta)\n        a_new = a * ctheta ** 2 - 2 * b * stheta * ctheta + c * stheta ** 2\n        c_new = c * ctheta ** 2 + 2 * b * stheta * ctheta + a * stheta ** 2\n        b_new = (a - c) * stheta * ctheta + b * (ctheta ** 2 - stheta ** 2)\n        a = a_new\n        c = c_new\n        b = b_new\n    s1 = np.sqrt(np.max([a, c]))\n    s2 = np.sqrt(np.min([a, c]))\n    return (round(s1, 4), round(s2, 4))"}
{"task_id": 12, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition of a 2x2 matrix using the Jacobi method.\n    Returns the singular values of the matrix.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 matrix\n    \n    Returns:\n        tuple: A tuple of two singular values, sorted in descending order, rounded to 4 decimals.\n    \"\"\"\n    tolerance = 1e-10\n    max_iterations = 100\n    iteration = 0\n    At = A.T\n    ATA = At @ A\n    while iteration < max_iterations:\n        t = (ATA[0, 1] + ATA[1, 0]) / (2 * (ATA[1, 1] - ATA[0, 0] + 1e-12))\n        theta = np.arctan(t)\n        c = np.cos(theta)\n        s = np.sin(theta)\n        R = np.array([[c, s], [-s, c]])\n        ATA = R.T @ ATA @ R\n        if abs(ATA[0, 1]) < tolerance:\n            break\n        iteration += 1\n    sigma1 = np.sqrt(ATA[0, 0])\n    sigma2 = np.sqrt(ATA[1, 1])\n    return (round(sigma1, 4), round(sigma2, 4))"}
{"task_id": 12, "completion_id": 4, "solution": "import numpy as np\nimport math\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the singular values of a 2x2 matrix using the Jacobian (QR) method.\n\n    Args:\n        A (np.ndarray): A 2x2 real matrix.\n\n    Returns:\n        tuple: A tuple of 2 singular values rounded to 4 decimal places.\n    \"\"\"\n    max_iter = 1000\n    tolerance = 1e-08\n    B = np.dot(A.T, A)\n    for _ in range(max_iter):\n        off = (B[0, 1] ** 2 + B[1, 0] ** 2) ** 0.5\n        if off < tolerance:\n            break\n        if B[0, 0] == B[1, 1]:\n            theta = np.pi / 4\n        else:\n            theta = 0.5 * np.arctan(2 * B[0, 1] / (B[0, 0] - B[1, 1]))\n        c = np.cos(theta)\n        s = np.sin(theta)\n        B_new = np.array([[c ** 2 * B[0, 0] + s ** 2 * B[1, 1] + 2 * c * s * B[0, 1], -c * s * (B[0, 0] - B[1, 1]) + (c ** 2 - s ** 2) * B[0, 1]], [-c * s * (B[0, 0] - B[1, 1]) + (c ** 2 - s ** 2) * B[0, 1], s ** 2 * B[0, 0] + c ** 2 * B[1, 1] - 2 * c * s * B[0, 1]]])\n        B = B_new\n    singular_values = np.sqrt(np.diag(B).round(8).tolist())\n    return tuple(np.sort(singular_values)[::-1].round(4).tolist())"}
{"task_id": 12, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2), 'Input matrix must be 2x2'\n    A_T_A = A.T @ A\n    a = A_T_A[0, 0]\n    b = A_T_A[0, 1]\n    c = A_T_A[1, 1]\n    tolerance = 1e-10\n    max_iterations = 100\n    for _ in range(max_iterations):\n        if abs(b) < tolerance:\n            break\n        theta = 0.5 * np.arctan2(2 * b, a - c)\n        cos_theta = np.cos(theta)\n        sin_theta = np.sin(theta)\n        a_new = a * cos_theta ** 2 - 2 * b * sin_theta * cos_theta + c * sin_theta ** 2\n        c_new = c * cos_theta ** 2 + 2 * b * sin_theta * cos_theta + a * sin_theta ** 2\n        b_new = b * (cos_theta - sin_theta) * (cos_theta + sin_theta)\n        (a, b, c) = (a_new, b_new, c_new)\n    singular_values = np.round(np.sqrt([a, c]), 4)\n    return tuple(singular_values)\nA = np.array([[3, 2], [2, 0]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 12, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"Compute approximate SVD for a 2x2 matrix using the Jacobian method.\n\n    Parameters:\n        A (np.ndarray): A 2x2 input matrix.\n\n    Returns:\n        tuple: (U, S, V), where U and V are 2x2 matrices (orthogonal),\n               and S is a vector of singular values (length 2),\n               all rounded to 4 decimal places.\n    \"\"\"\n    assert A.shape == (2, 2), 'Only 2x2 matrices are supported.'\n\n    def jacobian_rotation(M):\n        \"\"\"Apply Jacobian rotation to symmetric matrix M to zero out off-diagonal element.\"\"\"\n        if M[0, 1] == M[1, 0] == 0:\n            return (M, np.eye(2))\n        tan2theta = 2 * M[0, 1] / (M[0, 0] - M[1, 1])\n        if tan2theta == float('inf'):\n            theta = np.pi / 4\n        elif tan2theta == -float('inf'):\n            theta = -np.pi / 4\n        else:\n            theta = 0.5 * np.arctan(tan2theta)\n        c = np.cos(theta)\n        s = np.sin(theta)\n        J = np.array([[c, s], [-s, c]])\n        rotated = J.T @ M @ J\n        return (rotated, J)\n    B = A.T @ A\n    diag = np.copy(B)\n    V_rotations = np.eye(2)\n    for _ in range(10):\n        (diag, J) = jacobian_rotation(diag)\n        V_rotations = V_rotations @ J\n    S = np.sqrt(np.diag(diag))\n    S = np.sort(np.abs(S))[::-1]\n    V = V_rotations\n    inv_S_diag = np.diag(1 / S)\n    U = A @ V @ inv_S_diag\n    U = np.round(U, decimals=4)\n    S = np.round(S, decimals=4)\n    V = np.round(V, decimals=4)\n    return (U, S, V)"}
{"task_id": 12, "completion_id": 7, "solution": "import numpy as np\ndef jacobian_rotation(A, k, l, max_iter=1000, tol=1e-10):\n    \"\"\"\n    Perform a single Jacobi rotation on a matrix A.\n    \"\"\"\n    if abs(A[k, l]) < tol:\n        return (A, 0.0, 0.0)\n    theta = 0.5 * np.arctan(2 * A[k, l] / (A[k, k] - A[l, l]))\n    c = np.cos(theta)\n    s = np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    A = R.T @ A @ R\n    return (A, c, s)\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the SVD of a 2x2 matrix using the Jacobian iterative method.\n    Returns:\n        - the two singular values rounded to 4 decimal places.\n    \"\"\"\n    A_Atrans = A.T @ A\n    A = A_Atrans.copy()\n    for _ in range(100):\n        (A, c, s) = jacobian_rotation(A, 0, 1)\n        if abs(A[0, 1]) < 1e-10:\n            break\n    svals = np.sqrt(np.diag(A))\n    singular_values = tuple((round(val, 4) for val in svals))\n    return singular_values\nA = np.array([[3, 4], [4, 5]])"}
{"task_id": 12, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the SVD of a 2x2 matrix using the Jacobian method.\n    \n    Returns a tuple of the two singular values, rounded to 4 decimal places.\n    \"\"\"\n\n    def jacobi_symmetric(matrix, eps=1e-10, max_iter=1000):\n        n = matrix.shape[0]\n        mat_v = np.identity(n)\n        mat_diag = np.copy(matrix)\n        for _ in range(max_iter):\n            off_diag = mat_diag[np.triu_indices(n, k=1)]\n            if np.sum(np.square(off_diag)) < eps:\n                break\n            idx = np.argmax(np.abs(off_diag))\n            i = idx // n\n            j = idx % n\n            theta = 0.5 * np.arctan2(2 * mat_diag[i][j], mat_diag[i][i] - mat_diag[j][j])\n            c = np.cos(theta)\n            s = np.sin(theta)\n            rotation = np.identity(n)\n            rotation[i][j] = -s\n            rotation[j][i] = s\n            rotation[j][j] = c\n            rotation[i][i] = c\n            mat_diag = rotation.T @ mat_diag @ rotation\n            mat_v = mat_v @ rotation\n        return (np.sqrt(mat_diag[np.diag_indices(2)]), mat_v)\n    AtA = A.T @ A\n    (singular_values, _) = jacobi_symmetric(AtA)\n    return tuple((round(val, 4) for val in singular_values))"}
{"task_id": 12, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Matrix must be 2x2')\n    A_T_A = A.T @ A\n    max_iter = 100\n    eps = 1e-10\n    M = A_T_A.copy()\n    for _ in range(max_iter):\n        (a, b, c) = M.flatten()\n        if abs(b) < eps:\n            break\n        tan_2theta = 2 * b / (a - c)\n        theta = 0.5 * np.arctan(tan_2theta)\n        sin_theta = np.sin(theta)\n        cos_theta = np.cos(theta)\n        J = np.array([[cos_theta, -sin_theta], [sin_theta, cos_theta]])\n        M = J.T @ M @ J\n    singular_values = np.sqrt(np.diag(M))\n    singular_values = np.sort(singular_values)[::-1]\n    return tuple(np.round(singular_values, 4))"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant(matrix: list[list[int | float]]) -> float:\n        n = len(matrix)\n        if n == 1:\n            return matrix[0][0]\n        if n == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        det = 0\n        for col in range(n):\n            minor = [row[:col] + row[col + 1:] for row in matrix[1:]]\n            det += (-1) ** col * matrix[0][col] * determinant(minor)\n        return det\n    return determinant(matrix)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 1, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def get_minor(matrix: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        \"\"\"Returns the minor matrix after removing the given row and column.\"\"\"\n        return [row[:col] + row[col + 1:] for row in matrix[:row] + matrix[row + 1:]]\n\n    def determinant(matrix: list[list[int | float]]) -> float:\n        \"\"\"Recursively calculates the determinant using Laplace's expansion.\"\"\"\n        if len(matrix) == 1:\n            return matrix[0][0]\n        if len(matrix) == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        det = 0\n        for col in range(len(matrix)):\n            minor = get_minor(matrix, 0, col)\n            cofactor = (-1) ** col * determinant(minor)\n            det += matrix[0][col] * cofactor\n        return det\n    return determinant(matrix)"}
{"task_id": 13, "completion_id": 2, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def get_minor(matrix: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        \"\"\"Returns the minor matrix after removing the given row and column.\"\"\"\n        return [[matrix[i][j] for j in range(len(matrix)) if j != col] for i in range(len(matrix)) if i != row]\n\n    def determinant_3x3(matrix: list[list[int | float]]) -> float:\n        \"\"\"Calculates the determinant of a 3x3 matrix.\"\"\"\n        (a, b, c) = matrix[0]\n        (d, e, f) = matrix[1]\n        (g, h, i) = matrix[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    det = 0\n    for col in range(4):\n        minor = get_minor(matrix, 0, col)\n        sign = (-1) ** col\n        det += sign * matrix[0][col] * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 3, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n        matrix: A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n        The determinant of the matrix.\n    \"\"\"\n\n    def create_minor(matrix, row, col):\n        \"\"\"Returns a new matrix with specified row and column removed.\"\"\"\n        return [row[:col] + row[col + 1:] for row in matrix[:row] + matrix[row + 1:]]\n\n    def determinant_3x3(mat):\n        \"\"\"Calculates the determinant of a 3x3 matrix.\"\"\"\n        return mat[0][0] * (mat[1][1] * mat[2][2] - mat[2][1] * mat[1][2]) - mat[0][1] * (mat[1][0] * mat[2][2] - mat[2][0] * mat[1][2]) + mat[0][2] * (mat[1][0] * mat[2][1] - mat[2][0] * mat[1][1])\n    det = 0\n    for col in range(4):\n        minor = create_minor(matrix, 0, col)\n        sign = (-1) ** col\n        det += sign * matrix[0][col] * determinant_3x3(minor)\n    return det"}
{"task_id": 13, "completion_id": 4, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Computes the determinant of a 4x4 matrix using Laplace's Expansion.\n    \"\"\"\n\n    def get_minor(matrix: list[list[float]], row: int, col: int) -> list[list[float]]:\n        return [[matrix[i][j] for j in range(len(matrix[i])) if j != col] for i in range(len(matrix)) if i != row]\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if len(matrix) == 3:\n        return matrix[0][0] * (matrix[1][1] * matrix[2][2] - matrix[1][2] * matrix[2][1]) - matrix[0][1] * (matrix[1][0] * matrix[2][2] - matrix[1][2] * matrix[2][0]) + matrix[0][2] * (matrix[1][0] * matrix[2][1] - matrix[1][1] * matrix[2][0])\n    det = 0.0\n    for col in range(len(matrix)):\n        minor = get_minor(matrix, 0, col)\n        cofactor = (-1) ** col * matrix[0][col]\n        det += cofactor * determinant_4x4(minor)\n    return det"}
{"task_id": 13, "completion_id": 5, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant(matrix: list[list[int | float]]) -> float:\n        if len(matrix) == 1:\n            return matrix[0][0]\n        if len(matrix) == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        det = 0\n        for col in range(len(matrix)):\n            minor = [row[:col] + row[col + 1:] for row in matrix[1:]]\n            det += (-1) ** col * matrix[0][col] * determinant(minor)\n        return det\n    return determinant(matrix)"}
{"task_id": 13, "completion_id": 6, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(mat: list[list[float]]) -> float:\n        return mat[0][0] * mat[1][1] * mat[2][2] + mat[0][1] * mat[1][2] * mat[2][0] + mat[0][2] * mat[1][0] * mat[2][1] - mat[2][0] * mat[1][1] * mat[0][2] - mat[2][1] * mat[1][2] * mat[0][0] - mat[2][2] * mat[1][0] * mat[0][1]\n\n    def get_minor(mat: list[list[float]], row: int, col: int) -> list[list[float]]:\n        return [[mat[i][j] for j in range(len(mat)) if j != col] for i in range(len(mat)) if i != row]\n    det = 0.0\n    for col in range(4):\n        minor = get_minor(matrix, 0, col)\n        sign = (-1) ** col\n        det += matrix[0][col] * sign * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 7, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(mat):\n        return mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) - mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) + mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0])\n    det = 0.0\n    for col in range(4):\n        minor = [[matrix[i][j] for j in range(4) if j != col] for i in range(1, 4)]\n        cofactor = matrix[0][col] * determinant_3x3(minor)\n        if col % 2 == 1:\n            cofactor *= -1\n        det += cofactor\n    return det"}
{"task_id": 13, "completion_id": 8, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(m):\n        return m[0][0] * (m[1][1] * m[2][2] - m[1][2] * m[2][1]) - m[0][1] * (m[1][0] * m[2][2] - m[1][2] * m[2][0]) + m[0][2] * (m[1][0] * m[2][1] - m[1][1] * m[2][0])\n\n    def determinant(matrix):\n        if len(matrix) == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        elif len(matrix) == 3:\n            return determinant_3x3(matrix)\n        else:\n            det = 0\n            for col in range(len(matrix)):\n                minor = [row[:col] + row[col + 1:] for row in matrix[1:]]\n                sign = (-1) ** col\n                det += sign * matrix[0][col] * determinant(minor)\n            return det\n    return determinant(matrix)\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 13, "completion_id": 9, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_2x2(matrix: list[list[int | float]]) -> float:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n\n    def determinant_3x3(matrix: list[list[int | float]]) -> float:\n        d1 = matrix[0][0] * matrix[1][1] * matrix[2][2]\n        d2 = matrix[0][1] * matrix[1][2] * matrix[2][0]\n        d3 = matrix[0][2] * matrix[1][0] * matrix[2][1]\n        d4 = matrix[2][0] * matrix[1][1] * matrix[0][2]\n        d5 = matrix[2][1] * matrix[1][2] * matrix[0][0]\n        d6 = matrix[2][2] * matrix[1][0] * matrix[0][1]\n        return d1 + d2 + d3 - (d4 + d5 + d6)\n    det = 0\n    for col in range(4):\n        if matrix[0][col] == 0:\n            continue\n        minor = [[matrix[i][j] for j in range(4) if j != col] for i in range(1, 4)]\n        sign = (-1) ** col\n        det += matrix[0][col] * sign * determinant_3x3(minor)\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X : list[list[float]] - The feature matrix (should *not* include an intercept column)\n    y : list[float] - The target vector\n    \n    Returns:\n    list[float] - The coefficients for the linear regression model (including the intercept)\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_np\n    theta = theta.round(4)\n    return theta.tolist()"}
{"task_id": 14, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    - X: list of lists, each inner list represents a sample's feature vector.\n    - y: list of float values, the target values.\n    \n    Returns:\n    - A list of coefficients (weights) of the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    y_np = y_np.reshape(-1, 1)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_np\n    return [round(float(coef), 4) for coef in theta]"}
{"task_id": 14, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    return [round(w, 4) for w in theta.tolist()]\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\ny = [3.0, 5.0, 7.0]"}
{"task_id": 14, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X, dtype=float)\n    y_np = np.array(y, dtype=float)\n    X_bias = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y_np\n    theta_rounded = [round(float(coeff), 4) for coeff in theta]\n    return theta_rounded\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 5.0]]\ny = [1.0, 2.0, 3.0]"}
{"task_id": 14, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X)\n    y_array = np.array(y).reshape(-1, 1)\n    X_T = X_array.T\n    theta = np.linalg.inv(X_T @ X_array) @ X_T @ y_array\n    coefficients = [round(float(coef), 4) for coef in theta.flatten()]\n    return coefficients\nX = [[1, 2], [1, 3], [1, 4], [1, 5]]"}
{"task_id": 14, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X)\n    y_array = np.array(y)\n    X_b = np.c_[np.ones((X_array.shape[0], 1)), X_array]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_array\n    theta_rounded = [round(float(t), 4) for t in theta]\n    return theta_rounded"}
{"task_id": 14, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_np\n    theta_rounded = [round(val, 4) if abs(val) < 1e-05 else round(val, 4) for val in theta]\n    return theta_rounded\nX = [[1.0, 2.0], [2.0, 4.0], [3.0, 6.0]]\ny = [3.0, 5.0, 7.0]"}
{"task_id": 14, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X, dtype=np.float64)\n    y_np = np.array(y, dtype=np.float64)\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    theta_rounded = [round(t, 4) for t in theta]\n    return theta_rounded"}
{"task_id": 14, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X)\n    y_array = np.array(y)\n    X_transpose = X_array.T\n    X_TX = X_transpose @ X_array\n    X_Ty = X_transpose @ y_array\n    theta = np.linalg.inv(X_TX) @ X_Ty\n    theta_rounded = [round(float(coef), 4) if abs(coef) > 0.0001 else -0.0 for coef in theta]\n    return theta_rounded\nX = [[1.0, 2.0], [1.0, 3.0], [1.0, 4.0]]\ny = [3.0, 5.0, 7.0]"}
{"task_id": 14, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list[list[float]]): The feature matrix.\n    y (list[float]): The target vector.\n    \n    Returns:\n    list[float]: The regression coefficients rounded to 4 decimal places.\n    \"\"\"\n    X_matrix = np.array(X)\n    y_vector = np.array(y)\n    theta = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_vector\n    return [round(value, 4) for value in theta.tolist()]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = y.shape[0]\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = X @ theta\n        error = h - y\n        gradient = 1 / m * X.T @ error\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 1, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = X @ theta\n        error = h - y\n        gradient = 1 / m * X.T @ error\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 2, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        h = np.dot(X, theta)\n        gradient = np.dot(X.T, h - y) / m\n        theta -= alpha * gradient\n    return [round(theta[i], 4) for i in range(len(theta))]"}
{"task_id": 15, "completion_id": 3, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = X @ theta\n        error = h - y\n        gradient = X.T @ error / m\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 4, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        h = X @ theta\n        gradient = 1 / m * X.T @ (h - y)\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, decimals=4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 5, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X @ theta\n        error = predictions - y\n        gradient = 1 / m * X.T @ error\n        theta -= alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 15, "completion_id": 6, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        hypotheses = X @ theta\n        errors = hypotheses - y\n        gradient = 1 / m * X.T @ errors\n        theta -= alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 15, "completion_id": 7, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        gradient = X.T.dot(errors) / m\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()\ny = np.array([2, 4, 6, 8])\nalpha = 0.01\niterations = 1000"}
{"task_id": 15, "completion_id": 8, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = 1 / m * X.T @ errors\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 15, "completion_id": 9, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    n = X.shape[1]\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        hypothesis = X @ theta\n        loss = hypothesis - y\n        gradient = X.T @ loss / m\n        theta -= alpha * gradient\n    return np.around(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n    standardized_data = np.round(standardized_data, 4)\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    minmax_data = (data - min_vals) / (max_vals - min_vals)\n    minmax_data = np.round(minmax_data, 4)\n    standardized_list = standardized_data.tolist()\n    minmax_list = minmax_data.tolist()\n    return (standardized_list, minmax_list)"}
{"task_id": 16, "completion_id": 1, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    standardized_list = np.around(standardized, decimals=4).tolist()\n    min_max_list = np.around(min_max_scaled, decimals=4).tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 16, "completion_id": 2, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    minmax_scaled = (data - min_vals) / (max_vals - min_vals)\n    standardized_rounded = np.round(standardized, 4)\n    minmax_rounded = np.round(minmax_scaled, 4)\n    standardized_list = standardized_rounded.tolist()\n    minmax_list = minmax_rounded.tolist()\n    return (standardized_list, minmax_list)"}
{"task_id": 16, "completion_id": 3, "solution": "import numpy as np\nimport numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    data_min = np.min(data, axis=0)\n    data_max = np.max(data, axis=0)\n    min_max_scaled_data = (data - data_min) / (data_max - data_min)\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_scaled_data_rounded = np.round(min_max_scaled_data, 4)\n    return (standardized_data_rounded.tolist(), min_max_scaled_data_rounded.tolist())\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])"}
{"task_id": 16, "completion_id": 4, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Applies feature scaling using standardization and min-max normalization to a 2D NumPy array.\n\n    Parameters:\n        data (np.ndarray): A 2D array where each row is a sample and each column is a feature.\n\n    Returns:\n        tuple of list of list of floats:\n            - Standardized data\n            - Min-max normalized data\n    \"\"\"\n    standardized = (data - data.mean(axis=0)) / data.std(axis=0)\n    standardized = np.round(standardized, 4)\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    min_max_scaled = np.round(min_max_scaled, 4)\n    return (standardized.tolist(), min_max_scaled.tolist())"}
{"task_id": 16, "completion_id": 5, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized = (data - data.mean(axis=0)) / data.std(axis=0)\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    standardized_rounded = np.round(standardized, 4)\n    min_max_rounded = np.round(min_max_scaled, 4)\n    standardized_list = standardized_rounded.tolist()\n    min_max_list = min_max_rounded.tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 16, "completion_id": 6, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized = (data - data.mean(axis=0)) / data.std(axis=0)\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    standardized = np.round(standardized, 4)\n    min_max_scaled = np.round(min_max_scaled, 4)\n    standardized_list = standardized.tolist()\n    min_max_list = min_max_scaled.tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 16, "completion_id": 7, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.round(standardized_data, 4)\n    min_max_data = np.round(min_max_data, 4)\n    standardized_list = standardized_data.tolist()\n    min_max_list = min_max_data.tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 16, "completion_id": 8, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    standardized = np.round(standardized, 4).tolist()\n    min_max_scaled = np.round(min_max_scaled, 4).tolist()\n    return (standardized, min_max_scaled)"}
{"task_id": 16, "completion_id": 9, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n    min_vals = data.min(axis=0)\n    max_vals = data.max(axis=0)\n    min_max_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data = np.round(standardized_data, 4)\n    min_max_data = np.round(min_max_data, 4)\n    return (standardized_data.tolist(), min_max_data.tolist())"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points_array[:, np.newaxis, :] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = []\n        for i in range(k):\n            cluster_points = points_array[labels == i]\n            if cluster_points.size > 0:\n                new_centroids.append(cluster_points.mean(axis=0))\n            else:\n                new_centroids.append(centroids[i])\n        new_centroids = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 1, "solution": "import numpy as np\nfrom typing import List, Tuple"}
{"task_id": 17, "completion_id": 2, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Perform k-means clustering on a set of 2D points.\n    \n    Parameters:\n    - points: List of tuples, where each tuple represents a 2D point (x, y)\n    - k: Number of clusters\n    - initial_centroids: Initial guess of centroid positions\n    - max_iterations: Maximum number of iterations to perform\n\n    Returns:\n    - List of final centroids rounded to the nearest fourth decimal\n    \"\"\"\n    np_points = np.array(points)\n    np_initial_centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(np.sum((np_points[:, np.newaxis, :] - np_initial_centroids) ** 2, axis=2))\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.zeros_like(np_initial_centroids)\n        for i in range(k):\n            cluster_points = np_points[cluster_assignments == i]\n            if cluster_points.size > 0:\n                new_centroids[i] = np.mean(cluster_points, axis=0)\n            else:\n                new_centroids[i] = np_initial_centroids[i]\n        if np.allclose(np_initial_centroids, new_centroids):\n            break\n        np_initial_centroids = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in np_initial_centroids]\n    return final_centroids\npoints = [(1, 2), (1.5, 1.8), (5, 8), (8, 8), (1, 0.6), (9, 11)]\nk = 2\ninitial_centroids = [(2, 2), (6, 6)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 3, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        cluster_indices = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[cluster_indices == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return rounded_centroids"}
{"task_id": 17, "completion_id": 4, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(centroid.round(4)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 5, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_np = np.array(points, dtype=np.float64)\n    centroids_np = np.array(initial_centroids, dtype=np.float64)\n    for iteration in range(max_iterations):\n        distances = np.sqrt(((points_np[:, np.newaxis, :] - centroids_np[np.newaxis, :, :]) ** 2).sum(axis=2))\n        closest_centroids_indices = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_np[closest_centroids_indices == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids_np, new_centroids):\n            break\n        centroids_np = new_centroids\n    final_centroids_tuple = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids_np]\n    return final_centroids_tuple\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.1, 1.0), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (5.0, 7.0)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 6, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering and return the final centroids.\n\n    Parameters:\n    - points: List of 2D points (tuples).\n    - k: Number of clusters.\n    - initial_centroids: List of initial centroid coordinates.\n    - max_iterations: Maximum number of iterations to run.\n\n    Returns:\n    - final_centroids: List of final centroid coordinates, rounded to 4 decimals.\n    \"\"\"\n    centroids = np.array([[cx, cy] for (cx, cy) in initial_centroids], dtype=np.float64)\n    points = np.array(points, dtype=np.float64)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == j].mean(axis=0) if np.any(labels == j) else centroids[j] for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids.tolist()]\n    return final_centroids\npoints = [(1, 1), (1.5, 2), (3, 4), (5, 7), (3.5, 5), (4.5, 5), (3.5, 4.5)]\nk = 2\ninitial_centroids = [(1, 1), (3, 3)]\nmax_iterations = 100"}
{"task_id": 17, "completion_id": 7, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_np = np.array(points)\n    centroids_np = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_np - centroids_np[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_np[labels == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids_np, new_centroids):\n            break\n        centroids_np = new_centroids\n    rounded_centroids = [tuple(map(lambda x: round(x, 4), centroid)) for centroid in centroids_np.tolist()]\n    return rounded_centroids"}
{"task_id": 17, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        cluster_indices = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[cluster_indices == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids, atol=0.0001):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return final_centroids"}
{"task_id": 17, "completion_id": 9, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points[:, np.newaxis, :] - centroids) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([np.mean(points[labels == i], axis=0) if np.sum(labels == i) > 0 else centroids[i] for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\npoints = [(1.0, 1.0), (1.5, 2.0), (3.0, 4.0), (5.0, 7.0), (3.5, 5.0), (4.5, 5.0), (3.5, 4.5)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (5.0, 7.0)]\nmax_iterations = 10"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate indices for K-Fold Cross Validation.\n\n    Parameters:\n    ----------\n    X : np.ndarray\n        Feature array of shape (n_samples, n_features)\n    y : np.ndarray\n        Target array of shape (n_samples,)\n    k : int, optional\n        Number of folds (default is 5)\n    shuffle : bool, optional\n        Whether to shuffle the data before splitting (default is True)\n    random_seed : int, optional\n        Random seed for reproducibility (default is None)\n\n    Returns:\n    -------\n    List of tuples\n        Each tuple contains the train indices and test indices for a fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples < k:\n        raise ValueError('Number of samples cannot be less than the number of folds.')\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    indices = np.arange(n_samples)\n    if shuffle:\n        indices = np.random.permutation(indices)\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        test_indices = indices[i * fold_size:(i + 1) * fold_size]\n        train_indices = np.concatenate([indices[0:i * fold_size], indices[(i + 1) * fold_size:]])\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 1, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Labels or target array of shape (n_samples,).\n    k : int, default=5\n        Number of folds.\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting.\n    random_seed : int or None, default=None\n        Seed for random number generator for reproducibility.\n\n    Returns:\n    --------\n    list of tuples\n        Each tuple contains (train_indices, test_indices) for one fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(indices)\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i != k - 1 else n_samples\n        test_idx = indices[start:end]\n        train_idx = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_idx, test_idx))\n    return folds"}
{"task_id": 18, "completion_id": 2, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for k-fold cross-validation.\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features)\n    y : np.ndarray\n        Target array of shape (n_samples,)\n    k : int, default=5\n        Number of folds\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting\n    random_seed : int or None, default=None\n        Seed for random number generation\n\n    Returns:\n    --------\n    List of tuples\n        Each tuple contains (train_indices, test_indices)\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.RandomState(random_seed)\n        rng.shuffle(indices)\n    folds = []\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start = 0\n    for i in range(k):\n        fold_end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:fold_end]\n        train_indices = np.concatenate((indices[:start], indices[fold_end:]))\n        folds.append((train_indices, test_indices))\n        start = fold_end\n    return folds"}
{"task_id": 18, "completion_id": 3, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): Feature data of shape (n_samples, n_features)\n    y (np.ndarray): Target data of shape (n_samples,)\n    k (int): Number of folds\n    shuffle (bool): Whether to shuffle the data before splitting\n    random_seed (int or None): Seed for random shuffling\n\n    Returns:\n    list of tuples: Each tuple contains the indices for training and testing datasets\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        rng.shuffle(indices)\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = (i + 1) * fold_size if i < k - 1 else n_samples\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 4, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    - X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    - y (np.ndarray): Label vector of shape (n_samples,)\n    - k (int): Number of folds\n    - shuffle (bool): Whether to shuffle the data before splitting\n    - random_seed (int): Seed for random number generator for reproducibility\n    \n    Returns:\n    - list of tuples: Each tuple contains (train_indices, test_indices)\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples % k != 0:\n        raise ValueError('Number of samples should be divisible by k for equal folding.')\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        indices = np.random.permutation(n_samples)\n    else:\n        indices = np.arange(n_samples)\n    fold_size = n_samples // k\n    folds = [indices[i * fold_size:(i + 1) * fold_size] for i in range(k)]\n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate(folds[:i] + folds[i + 1:])\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 18, "completion_id": 5, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X (np.ndarray): Feature dataset with shape (n_samples, n_features).\n    - y (np.ndarray): Target labels with shape (n_samples,).\n    - k (int): Number of folds.\n    - shuffle (bool): Whether to shuffle the data before splitting.\n    - random_seed (int, optional): Random seed for reproducibility.\n\n    Returns:\n    - List of tuples: Each tuple contains (train_indices, test_indices) for one fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    if k <= 1:\n        raise ValueError('k should be greater than 1 for cross-validation.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    folds = []\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        folds.append((train_indices, test_indices))\n        start = end\n    return folds\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\ny = np.array([0, 1, 0, 1, 0])\nfolds = k_fold_cross_validation(X, y, k=2, shuffle=True, random_seed=42)"}
{"task_id": 18, "completion_id": 6, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate indices to split data into k-fold cross-validation.\n    \n    Parameters:\n    - X: Input features (numpy array)\n    - y: Target labels (numpy array)\n    - k: Number of folds (default is 5)\n    - shuffle: Whether to shuffle the data before splitting (default is True)\n    - random_seed: Random seed for reproducibility (default is None)\n    \n    Returns:\n    - A list of length k, where each element is a tuple (train_indices, test_indices).\n    \"\"\"\n    if k < 2:\n        raise ValueError('Number of folds must be at least 2.')\n    if len(X) != len(y):\n        raise ValueError('X and y must have the same length.')\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    folds = []\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_indices.astype(int), test_indices.astype(int)))\n        start = end\n    return folds"}
{"task_id": 18, "completion_id": 7, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    - X: np.ndarray; features of the dataset.\n    - y: np.ndarray; target labels.\n    - k: int; number of folds.\n    - shuffle: bool; whether to shuffle the data before splitting.\n    - random_seed: int; seed for random shuffling (for reproducibility).\n    \n    Returns:\n    - List of tuples, where each tuple is (train_indices, test_indices).\n    \"\"\"\n    N = X.shape[0]\n    indices = np.arange(N)\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    folds = []\n    fold_size = N // k\n    remainder = N % k\n    start = 0\n    for i in range(k):\n        fold_end = start + fold_size + (1 if i < remainder else 0)\n        fold_indices = indices[start:fold_end]\n        test_indices = fold_indices\n        train_indices = np.concatenate((indices[:start], indices[fold_end:]))\n        folds.append((train_indices, test_indices))\n        start = fold_end\n    return folds"}
{"task_id": 18, "completion_id": 8, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n    \n    Parameters:\n    - X (np.ndarray): Features of the dataset.\n    - y (np.ndarray): Target variable of the dataset.\n    - k (int): Number of folds.\n    - shuffle (bool): Whether to shuffle the data before splitting.\n    - random_seed (int): Seed for random number generation.\n    \n    Returns:\n    - List of tuples: Each tuple contains (train_indices, test_indices).\n    \"\"\"\n    n_samples = X.shape[0]\n    y = np.array(y).ravel()\n    if n_samples < k:\n        raise ValueError(\"Number of folds 'k' cannot exceed the number of samples.\")\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size if i < k - 1 else n_samples\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:])) if i == 0 else np.concatenate((indices[:start], indices[end:]))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 18, "completion_id": 9, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    n_samples = X.shape[0]\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        test_indices = indices[current:current + fold_size]\n        train_indices = np.concatenate((indices[:current], indices[current + fold_size:]))\n        folds.append((train_indices, test_indices))\n        current += fold_size\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) from scratch.\n\n    Parameters:\n    - data: np.ndarray, 2D array where each row is a data sample and each column is a feature\n    - k: int, number of principal components to return\n\n    Returns:\n    - list of lists, each list is a principal component (eigenvector), rounded to 4 decimal places\n    \"\"\"\n    data_std = (data - np.mean(data, axis=0)) / np.std(data, axis=0, ddof=0)\n    cov_matrix = np.cov(data_std, rowvar=False)\n    (eigen_values, eigen_vectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigen_values)[::-1]\n    sorted_eigen_vectors = eigen_vectors[:, sorted_indices]\n    principal_components = sorted_eigen_vectors[:, :k]\n    return [list(v.round(4)) for v in principal_components.T]"}
{"task_id": 19, "completion_id": 1, "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_idx = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_idx]\n    sorted_eigenvectors = eigenvectors[:, sorted_idx]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = [list(map(lambda x: round(x, 4), vector)) for vector in principal_components.T]\n    return principal_components_rounded\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9]])"}
{"task_id": 19, "completion_id": 2, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on the input dataset.\n\n    Parameters:\n    - data: 2D numpy array, where each row is a data sample and each column is a feature\n    - k: integer, number of principal components to return\n\n    Returns:\n    - list of lists, where each sublist is a principal component (eigenvector)\n      corresponding to a top eigenvalue, rounded to 4 decimal places\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    n_samples = standardized_data.shape[0]\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    principal_components = [[float(np.round(coord, 4)) for coord in eigenvector] for eigenvector in top_k_eigenvectors.T]\n    return principal_components"}
{"task_id": 19, "completion_id": 3, "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    result = [list(np.round(pc, 4)) for pc in principal_components]\n    return result\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9]])"}
{"task_id": 19, "completion_id": 4, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) from scratch.\n    \n    Parameters:\n        data (np.ndarray): A 2D array where rows are samples and columns are features.\n        k (int): Number of principal components to return.\n        \n    Returns:\n        list[list[float]]: The top k principal components (eigenvectors).\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    n_samples = standardized_data.shape[0]\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, idx]\n    principal_components = sorted_eigenvectors[:, :k].real\n    principal_components_rounded = [list(row.round(4)) for row in principal_components]\n    return principal_components_rounded"}
{"task_id": 19, "completion_id": 5, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, idx]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components = np.round(principal_components, 4)\n    return principal_components.tolist()"}
{"task_id": 19, "completion_id": 6, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[idx]\n    sorted_eigenvectors = eigenvectors[:, idx]\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    top_k_eigenvectors_rounded = [[round(component, 4) for component in vector] for vector in top_k_eigenvectors.T]\n    return top_k_eigenvectors_rounded"}
{"task_id": 19, "completion_id": 7, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    centered_data = data - mean\n    cov_matrix = np.cov(centered_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    top_k_eigenvectors = sorted_eigenvectors[:, :k]\n    return [list(map(lambda x: round(x, 4), vector)) for vector in top_k_eigenvectors.T]"}
{"task_id": 19, "completion_id": 8, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_index = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_index]\n    sorted_eigenvectors = eigenvectors[:, sorted_index]\n    principal_components = sorted_eigenvectors[:, :k].real\n    principal_components = [[round(element, 4) for element in vector] for vector in principal_components]\n    return principal_components"}
{"task_id": 19, "completion_id": 9, "solution": "import numpy as np\nimport numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    n_samples = standardized_data.shape[0]\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    principal_components = eigenvectors[:, :k]\n    result = [list(np.round(vec, 4)) for vec in principal_components]\n    return result\ndata = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2.0, 1.6], [1.0, 1.1], [1.5, 1.6], [1.1, 0.9]])"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n        return {'class': examples[0][target_attr]}\n    if not attributes:\n        class_counts = Counter((example[target_attr] for example in examples))\n        return {'class': class_counts.most_common(1)[0][0]}\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    if best_attr is None:\n        class_counts = Counter((example[target_attr] for example in examples))\n        return {'class': class_counts.most_common(1)[0][0]}\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    tree = {best_attr: {}}\n    for value in set((example[best_attr] for example in examples)):\n        subset = [ex for ex in examples if ex[best_attr] == value]\n        if not subset:\n            class_counts = Counter((example[target_attr] for example in examples))\n            tree[best_attr][value] = {'class': class_counts.most_common(1)[0][0]}\n        else:\n            subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n            tree[best_attr][value] = subtree\n    return tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"Calculate entropy of a list of examples based on the target attribute.\"\"\"\n    class_counts = Counter((example[target_attr] for example in examples))\n    total = len(examples)\n    return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"Choose the attribute with the highest information gain.\"\"\"\n    parent_entropy = entropy(examples, target_attr)\n    best_gain = 0\n    best_attr = None\n    for attr in attributes:\n        split_data = {}\n        for example in examples:\n            value = example[attr]\n            if value not in split_data:\n                split_data[value] = []\n            split_data[value].append(example)\n        weighted_child_entropy = 0\n        total_examples = len(examples)\n        for (value, subset) in split_data.items():\n            entropy_value = entropy(subset, target_attr)\n            weighted_child_entropy += len(subset) / total_examples * entropy_value\n        gain = parent_entropy - weighted_child_entropy\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    return best_attr"}
{"task_id": 20, "completion_id": 1, "solution": "import math\nfrom collections import Counter\nfrom typing import Any, Dict, List\ndef entropy(labels: List[Any]) -> float:\n    \"\"\"Calculate the entropy of a list of class labels.\"\"\"\n    n = len(labels)\n    if n == 0:\n        return 0.0\n    label_counts = Counter(labels)\n    return -sum((count / n * math.log2(count / n) for count in label_counts.values()))\ndef information_gain(examples: List[Dict[str, str]], attr: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain of splitting on `attr`.\"\"\"\n    total_entropy = entropy([example[target_attr] for example in examples])\n    values = set((example[attr] for example in examples))\n    weighted_entropy = sum((len([e for e in examples if e[attr] == value]) / len(examples) * entropy([e[target_attr] for e in examples if e[attr] == value]) for value in values))\n    return total_entropy - weighted_entropy\ndef most_common_label(examples: List[Dict[str, str]], target_attr: str) -> str:\n    \"\"\"Return the most common label in the examples.\"\"\"\n    labels = [example[target_attr] for example in examples]\n    return Counter(labels).most_common(1)[0][0]\ndef learn_decision_tree(examples: List[Dict[str, str]], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Learn a decision tree using recursive binary splitting\n    based on entropy and information gain.\n    \"\"\"\n    if not examples:\n        return 'None'\n    labels = [example[target_attr] for example in examples]\n    if all((label == labels[0] for label in labels)):\n        return labels[0]\n    if not attributes:\n        return most_common_label(examples, target_attr)\n    best_attr = max((attribute for attribute in attributes if attribute != target_attr), key=lambda a: information_gain(examples, a, target_attr))\n    tree = {best_attr: {}}\n    remaining_attrs = [a for a in attributes if a != best_attr]\n    for value in set((example[best_attr] for example in examples)):\n        subset = [e for e in examples if e[best_attr] == value]\n        subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n        tree[best_attr][value] = subtree\n    return tree"}
{"task_id": 20, "completion_id": 2, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Optional\nfrom pprint import pprint\ndef learn_decision_tree(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Builds a decision tree using entropy and information gain for classification.\n    \n    Args:\n        examples: List of dictionaries with attribute-value pairs.\n        attributes: List of feature names to consider.\n        target_attr: Name of the target class attribute.\n\n    Returns:\n        A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(class_values: List[Any]) -> float:\n        \"\"\"Calculate entropy for a list of class values\"\"\"\n        counts = Counter(class_values)\n        total = len(class_values)\n        return -sum((count / total * math.log2(count / total) for count in counts.values()))\n\n    def best_split(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> Optional[str]:\n        \"\"\"Find the attribute with the highest information gain\"\"\"\n        best_info_gain = -1\n        best_attribute = None\n        target_values = [example[target_attr] for example in examples]\n        baseline_entropy = entropy(target_values)\n        for attr in attributes:\n            if attr == target_attr:\n                continue\n            attribute_values = set((example[attr] for example in examples))\n            weighted_entropy = 0.0\n            for value in attribute_values:\n                subset = [ex for ex in examples if ex[attr] == value]\n                target_subset = [ex[target_attr] for ex in subset]\n                weight = len(subset) / len(examples)\n                weighted_entropy += weight * entropy(target_subset)\n            info_gain = baseline_entropy - weighted_entropy\n            if info_gain > best_info_gain:\n                best_info_gain = info_gain\n                best_attribute = attr\n        return best_info_gain > 0 and best_attribute\n\n    def majority_class(examples: List[Dict[str, Any]], target_attr: str) -> Any:\n        \"\"\"Return the most frequent class label among the examples\"\"\"\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n\n    def decision_tree_learning(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n        \"\"\"Recursively builds the decision tree using the ID3-like algorithm.\"\"\"\n        if not examples:\n            return majority_class(curr_examples, target_attr)\n        class_labels = [example[target_attr] for example in examples]\n        if len(set(class_labels)) == 1:\n            return {'class': class_labels[0]}\n        if not attributes:\n            return {'class': majority_class(examples, target_attr)}\n        best_attr = best_split(examples, attributes, target_attr)\n        if best_attr is None:\n            return {'class': majority_class(examples, target_attr)}\n        attributes_left = [attr for attr in attributes if attr != best_attr]\n        tree = {best_attr: {}}\n        for value in set((example[best_attr] for example in examples)):\n            examples_subset = [ex for ex in examples if ex[best_attr] == value]\n            subtree = decision_tree_learning(examples_subset, attributes_left, target_attr)\n            tree[best_attr][value] = subtree\n        return tree\n    return decision_tree_learning(examples, attributes, target_attr)\nexamples = [{'Outlook': 'sunny', 'Temperature': 'hot', 'Humidity': 'high', 'Wind': 'weak', 'PlayTennis': 'no'}, {'Outlook': 'sunny', 'Temperature': 'hot', 'Humidity': 'high', 'Wind': 'strong', 'PlayTennis': 'no'}, {'Outlook': 'overcast', 'Temperature': 'hot', 'Humidity': 'high', 'Wind': 'weak', 'PlayTennis': 'yes'}, {'Outlook': 'rain', 'Temperature': 'mild', 'Humidity': 'high', 'Wind': 'weak', 'PlayTennis': 'yes'}, {'Outlook': 'rain', 'Temperature': 'cool', 'Humidity': 'normal', 'Wind': 'weak', 'PlayTennis': 'yes'}, {'Outlook': 'rain', 'Temperature': 'cool', 'Humidity': 'normal', 'Wind': 'strong', 'PlayTennis': 'no'}, {'Outlook': 'overcast', 'Temperature': 'cool', 'Humidity': 'normal', 'Wind': 'strong', 'PlayTennis': 'yes'}, {'Outlook': 'sunny', 'Temperature': 'mild', 'Humidity': 'high', 'Wind': 'weak', 'PlayTennis': 'no'}, {'Outlook': 'sunny', 'Temperature': 'cool', 'Humidity': 'normal', 'Wind': 'weak', 'PlayTennis': 'yes'}, {'Outlook': 'rain', 'Temperature': 'mild', 'Humidity': 'normal', 'Wind': 'weak', 'PlayTennis': 'yes'}]\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind', 'PlayTennis']\ntarget_attr = 'PlayTennis'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 20, "completion_id": 3, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, Any, List, Tuple, Callable, Optional\nExample = Dict[str, Any]\nTree = Dict[str, Any]\ndef learn_decision_tree(examples: list[Example], attributes: list[str], target_attr: str) -> Tree:\n    \"\"\"Build a decision tree using entropy-based information gain.\"\"\"\n    if not examples:\n        return {}\n    if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n        return {'leaf': examples[0][target_attr]}\n    if not attributes:\n        counts = Counter((example[target_attr] for example in examples))\n        return {'leaf': counts.most_common(1)[0][0]}\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    for value in get_unique_values(examples, best_attr):\n        subset = [ex for ex in examples if ex[best_attr] == value]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef get_unique_values(examples: list[Example], attribute: str) -> list[Any]:\n    \"\"\"Get the set of unique values for the given attribute in the examples.\"\"\"\n    return list({example[attribute] for example in examples})\ndef entropy(examples: list[Example], target_attr: str) -> float:\n    \"\"\"Compute entropy of the target attribute in the examples.\"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    total = len(examples)\n    ent = 0.0\n    for count in counts.values():\n        p = count / total\n        ent -= p * math.log2(p)\n    return ent\ndef information_gain(examples: list[Example], attribute: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain of using an attribute for splitting.\"\"\"\n    total_entropy = entropy(examples, target_attr)\n    unique_values = get_unique_values(examples, attribute)\n    weighted_entropy = 0.0\n    for value in unique_values:\n        subset = [ex for ex in examples if ex[attribute] == value]\n        subset_entropy = entropy(subset, target_attr)\n        weighted_entropy += len(subset) / len(examples) * subset_entropy\n    return total_entropy - weighted_entropy\ndef choose_best_attribute(examples: list[Example], attributes: list[str], target_attr: str) -> str:\n    \"\"\"Select the attribute with the highest information gain.\"\"\"\n    best_gain = -1.0\n    best_attr = None\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        gain = information_gain(examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    if best_attr is None:\n        counts = Counter((example[target_attr] for example in examples))\n        return str(counts.most_common(1)[0][0])\n    return best_attr"}
{"task_id": 20, "completion_id": 4, "solution": "import math\nfrom collections import Counter\nimport pprint\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Learns a decision tree using entropy and information gain.\n    \n    Parameters:\n    ----------\n    examples : list[dict]\n        List of examples where each example is a dictionary of attribute-value pairs.\n    attributes : list[str]\n        List of available attribute names (excluding the target).\n    target_attr : str\n        Name of the target attribute to be predicted.\n    \n    Returns:\n    -------\n    dict\n        Nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return 'NotFound'\n    class_counts = Counter((example[target_attr] for example in examples))\n    if len(class_counts) == 1:\n        return next(iter(class_counts))\n    if not attributes:\n        return class_counts.most_common(1)[0][0]\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    remaining_attributes = [a for a in attributes if a != best_attr]\n    decision_tree = {best_attr: {}}\n    for value in get_values(examples, best_attr):\n        subexamples = [e for e in examples if e[best_attr] == value]\n        decision_tree[best_attr][value] = learn_decision_tree(subexamples, remaining_attributes, target_attr)\n    return decision_tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculate entropy of the examples for the target attribute.\n    \"\"\"\n    class_counts = Counter((example[target_attr] for example in examples))\n    total = len(examples)\n    return -sum((count / total * math.log2(count / total) for count in class_counts.values()))\ndef information_gain(examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain from splitting on a specific attribute.\n    \"\"\"\n    total_entropy = entropy(examples, target_attr)\n    values = get_values(examples, attr)\n    weighted_entropy = 0.0\n    total = len(examples)\n    for value in values:\n        value_examples = [e for e in examples if e[attr] == value]\n        entropy_val = entropy(value_examples, target_attr)\n        weighted_entropy += len(value_examples) / total * entropy_val\n    return total_entropy - weighted_entropy\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Choose the attribute with the highest information gain.\n    \"\"\"\n    gains = [(attr, information_gain(examples, attr, target_attr)) for attr in attributes]\n    best_attr = max(gains, key=lambda x: x[1])[0]\n    return best_attr\ndef get_values(examples: list[dict], attribute: str) -> list:\n    \"\"\"\n    Get unique values of an attribute from the examples.\n    \"\"\"\n    return list({example[attribute] for example in examples})\nexamples = [{'Outlook': 'Sunny', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temp': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temp': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temp': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temp': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temp': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}]\nattributes = ['Outlook', 'Temp', 'Humidity', 'Wind']\ntarget_attr = 'PlayTennis'"}
{"task_id": 20, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Build a decision tree using recursive binary splitting based on entropy and information gain.\n    \n    Parameters:\n    examples (list[dict]): A list of examples, where each example is a dictionary of attribute value pairs.\n    attributes (list[str]): A list of attribute names to consider for building the decision tree.\n    target_attr (str): The name of the target attribute for classification.\n    \n    Returns:\n    dict: A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return 'None'\n    if all((example[target_attr] == examples[0][target_attr] for example in examples)):\n        return examples[0][target_attr]\n    if not attributes:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    remaining_attributes = [a for a in attributes if a != best_attr]\n    for value in get_values(examples, best_attr):\n        sub_examples = [example for example in examples if example[best_attr] == value]\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculate entropy for a set of examples.\n    \"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    total = len(examples)\n    return -sum((count / total * math.log2(count / total) for count in counts.values()))\ndef information_gain(parent_entropy: float, examples: list[dict], attr: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate information gain when splitting on a given attribute.\n    \"\"\"\n    values = get_values(examples, attr)\n    total = len(examples)\n    weighted_child_entropy = 0.0\n    for value in values:\n        sub_examples = [example for example in examples if example[attr] == value]\n        if not sub_examples:\n            continue\n        fraction = len(sub_examples) / total\n        weighted_child_entropy += fraction * entropy(sub_examples, target_attr)\n    return parent_entropy - weighted_child_entropy\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Select the attribute in 'attributes' that gives the highest information gain.\n    \"\"\"\n    parent_entropy = entropy(examples, target_attr)\n    best_attr = None\n    max_gain = -1\n    for attr in attributes:\n        gain = information_gain(parent_entropy, examples, attr, target_attr)\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    return best_attr\ndef get_values(examples: list[dict], attribute: str) -> list:\n    \"\"\"\n    Get unique values for the given attribute in the examples.\n    \"\"\"\n    return list({example[attribute] for example in examples if attribute in example})"}
{"task_id": 20, "completion_id": 6, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List\ndef learn_decision_tree(examples: List[Dict], attributes: List[str], target_attr: str) -> Dict:\n    \"\"\"\n    Learn a decision tree recursively using entropy and information gain.\n    \n    :param examples: List of examples, each represented as a dictionary.\n    :param attributes: List of attribute names (excluding target attribute).\n    :param target_attr: The name of the target attribute (class label).\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    class_values = [example[target_attr] for example in examples]\n    if all((class_values[0] == val for val in class_values)):\n        return class_values[0]\n    if not attributes:\n        return Counter(class_values).most_common(1)[0][0]\n    best_attr = find_best_attribute(examples, attributes, target_attr)\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n    tree = {best_attr: {}}\n    for value in get_possible_values(examples, best_attr):\n        subset = [example for example in examples if example[best_attr] == value]\n        if not subset:\n            tree[best_attr][value] = Counter(class_values).most_common(1)[0][0]\n        else:\n            tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n    return tree\ndef entropy(class_distribution: List[str]) -> float:\n    \"\"\"Calculate entropy.\"\"\"\n    total = len(class_distribution)\n    if total == 0:\n        return 0\n    counter = Counter(class_distribution)\n    return -sum((count / total * math.log2(count / total) for count in counter.values()))\ndef information_gain(original_entropy: float, values: List[str], class_distribution: List[List[str]]) -> float:\n    \"\"\"Calculate information gain from splitting on the attribute.\"\"\"\n    total = sum((len(part) for part in class_distribution))\n    weighted_entropy = sum((len(part) / total * entropy(part) for part in class_distribution))\n    return original_entropy - weighted_entropy\ndef find_best_attribute(examples: List[Dict], attributes: List[str], target_attr: str) -> str:\n    \"\"\"Find the attribute with the highest information gain.\"\"\"\n    class_distribution = [example[target_attr] for example in examples]\n    orig_entropy = entropy(class_distribution)\n    max_gain = -1\n    best_attr = None\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        parts = []\n        class_parts = []\n        for example in examples:\n            parts.append(example[attr])\n            class_parts.append(example[target_attr])\n        grouped_classes = []\n        for (val, group) in group_by_attribute_value(examples, attr):\n            grouped_classes.append([example[target_attr] for example in group])\n        gain = information_gain(orig_entropy, parts, grouped_classes)\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    return best_attr\ndef group_by_attribute_value(examples: List[Dict], attribute: str) -> List[tuple]:\n    \"\"\"Groups examples by the value of the specified attribute.\"\"\"\n    result = {}\n    for example in examples:\n        val = example[attribute]\n        if val not in result:\n            result[val] = []\n        result[val].append(example)\n    return list(result.items())\ndef get_possible_values(examples: List[Dict], attribute: str) -> List[str]:\n    \"\"\"Returns all unique values for the given attribute in examples.\"\"\"\n    return list({example[attribute] for example in examples})\ndata = [{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}]\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\ntarget_attr = 'PlayTennis'\ntree = learn_decision_tree(data, attributes, target_attr)"}
{"task_id": 20, "completion_id": 7, "solution": "import math\nfrom collections import Counter\nfrom typing import List, Dict, Any\ndef learn_decision_tree(examples: List[Dict], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Build a decision tree using entropy and information gain.\n\n    Parameters:\n    - examples: List of example dictionaries with attribute-value pairs.\n    - attributes: List of attribute names (excluding target).\n    - target_attr: Name of the target attribute.\n\n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return {}\n    targets = [e[target_attr] for e in examples]\n    if len(set(targets)) == 1:\n        return {target_attr: targets[0]}\n    if not attributes:\n        counts = Counter(targets)\n        return {target_attr: counts.most_common(1)[0][0]}\n    best_attribute = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attribute: {}}\n    remaining_attributes = [a for a in attributes if a != best_attribute]\n    for value in get_values(examples, best_attribute):\n        sub_examples = [e for e in examples if e[best_attribute] == value]\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\ndef entropy(examples: List[Dict], target_attr: str) -> float:\n    \"\"\"Compute entropy for a list of examples based on target attribute.\"\"\"\n    if not examples:\n        return 0.0\n    targets = [e[target_attr] for e in examples]\n    counts = Counter(targets)\n    total = len(targets)\n    return -sum((count / total * math.log2(count / total) for count in counts.values()))\ndef information_gain(parent_entropy: float, examples: List[Dict], attr: str, target_attr: str) -> float:\n    \"\"\"Calculate information gain of an attribute.\"\"\"\n    values = get_values(examples, attr)\n    total = len(examples)\n    gain = parent_entropy\n    for value in values:\n        subset = [e for e in examples if e[attr] == value]\n        subset_entropy = entropy(subset, target_attr)\n        gain -= len(subset) / total * subset_entropy\n    return gain\ndef get_values(examples: List[Dict], attr: str) -> List[Any]:\n    \"\"\"Get all possible values for an attribute.\"\"\"\n    return list(set((e[attr] for e in examples)))\ndef choose_best_attribute(examples: List[Dict], attributes: List[str], target_attr: str) -> str:\n    \"\"\"Choose the attribute with the highest information gain.\"\"\"\n    parent_entropy = entropy(examples, target_attr)\n    best_gain = -1.0\n    best_attr = None\n    for attr in attributes:\n        gain = information_gain(parent_entropy, examples, attr, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    return best_attr"}
{"task_id": 20, "completion_id": 8, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List\ndef entropy(counts: Dict) -> float:\n    \"\"\"Calculate the entropy of a list of class counts.\"\"\"\n    total = sum(counts.values())\n    if total == 0:\n        return 0.0\n    return -sum((count / total * math.log2(count / total) for count in counts.values() if count > 0))\ndef information_gain(parent_entropy: float, subset_entropies: Dict[float, float], subset_sizes: Dict[float, int]) -> float:\n    \"\"\"Calculate the information gain of a split.\"\"\"\n    total = sum(subset_sizes.values())\n    weighted_entropy = 0.0\n    for (value, size) in subset_sizes.items():\n        weighted_entropy += size / total * subset_entropies[value]\n    return parent_entropy - weighted_entropy\ndef split_dataset(examples: List[Dict], attribute: str, value) -> List[Dict]:\n    \"\"\"Return the subset of examples that have the specified attribute value.\"\"\"\n    return [ex for ex in examples if ex.get(attribute) == value]\ndef majority_class(examples: List[Dict], target_attr: str) -> str:\n    \"\"\"Return the class with the highest count in examples.\"\"\"\n    counts = Counter((ex[target_attr] for ex in examples if target_attr in ex))\n    return counts.most_common(1)[0][0] if counts else None\ndef learn_decision_tree(examples: List[dict], attributes: List[str], target_attr: str) -> dict:\n    \"\"\"Learn a decision tree using entropy and information gain.\"\"\"\n    if not examples:\n        return majority_class(examples, target_attr)\n    class_values = [ex[target_attr] for ex in examples]\n    if len(set(class_values)) == 1:\n        return class_values[0]\n    if not attributes:\n        return majority_class(examples, target_attr)\n    best_attribute = None\n    best_gain = -1\n    parent_entropy = entropy(Counter((ex[target_attr] for ex in examples)))\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        attr_values = set((ex[attr] for ex in examples if attr in ex))\n        subsets = {value: split_dataset(examples, attr, value) for value in attr_values}\n        subset_entropies = {value: entropy(Counter((ex[target_attr] for ex in sub))) for (value, sub) in subsets.items()}\n        subset_sizes = {value: len(sub) for (value, sub) in subsets.items()}\n        gain = information_gain(parent_entropy, subset_entropies, subset_sizes)\n        if gain > best_gain:\n            best_gain = gain\n            best_attribute = attr\n    if best_gain == 0:\n        return majority_class(examples, target_attr)\n    remaining_attributes = [a for a in attributes if a != best_attribute]\n    tree = {best_attribute: {}}\n    for (value, subset) in split_dataset_by_attribute(examples, best_attribute).items():\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'color': 'red', 'shape': 'circle', 'size': 'big', 'label': 'yes'}, {'color': 'red', 'shape': 'square', 'size': 'small', 'label': 'no'}, {'color': 'blue', 'shape': 'circle', 'size': 'small', 'label': 'yes'}, {'color': 'blue', 'shape': 'circle', 'size': 'big', 'label': 'no'}, {'color': 'blue', 'shape': 'square', 'size': 'big', 'label': 'yes'}]\nattributes = ['color', 'shape', 'size']\ntarget_attr = 'label'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 20, "completion_id": 9, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List, Any\ndef entropy(labels: List[Any]) -> float:\n    \"\"\"Calculate the entropy of a list of class labels.\"\"\"\n    counter = Counter(labels)\n    total = len(labels)\n    if total == 0:\n        return 0.0\n    entropy_value = 0.0\n    for count in counter.values():\n        prob = count / total\n        entropy_value -= prob * math.log2(prob)\n    return entropy_value\ndef information_gain(data: List[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain of a given attribute for classification.\"\"\"\n    total_entropy = entropy([example[target_attr] for example in data])\n    values = set((example[attribute] for example in data))\n    weighted_entropy = 0.0\n    for value in values:\n        subset = [example for example in data if example[attribute] == value]\n        if len(subset) == 0:\n            continue\n        prob = len(subset) / len(data)\n        weighted_entropy += prob * entropy([example[target_attr] for example in subset])\n    return total_entropy - weighted_entropy\ndef major_class(labels: List[Any]) -> Any:\n    \"\"\"Return the class with the majority label.\"\"\"\n    return Counter(labels).most_common(1)[0][0]\ndef split_data(data: List[dict], attribute: str, value: Any) -> List[dict]:\n    \"\"\"Split data into subset where attribute has the given value.\"\"\"\n    return [example for example in data if example[attribute] == value]\ndef learn_decision_tree(examples: List[dict], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"Learn a decision tree from examples using entropy and information gain.\"\"\"\n    if not examples:\n        return major_class([example[target_attr] for example in examples])\n    labels = [example[target_attr] for example in examples]\n    if len(set(labels)) == 1:\n        return labels[0]\n    if not attributes:\n        return major_class(labels)\n    best_attr = max(attributes, key=lambda a: information_gain(examples, a, target_attr))\n    tree = {best_attr: {}}\n    remaining_attributes = [a for a in attributes if a != best_attr]\n    for value in set((example[best_attr] for example in examples)):\n        subset = split_data(examples, best_attr, value)\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    if kernel == 'linear':\n\n        def kernel_func(x1, x2):\n            return np.dot(x1, x2)\n    elif kernel == 'rbf':\n\n        def kernel_func(x1, x2):\n            return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel_func(data[i], data[j])\n    for _ in range(iterations):\n        t = _ + 1\n        eta = 1.0 / (lambda_val * t)\n        obj_vals = np.dot(alphas * labels, K) + b\n        margins = labels * obj_vals\n        alphas -= eta * (alphas / t - np.where(margins < 1, labels, 0))\n        alphas = np.clip(alphas, 0, 1 / (lambda_val * t))\n    support_vectors = alphas > 1e-06\n    b = np.mean(labels[support_vectors] - np.dot(alphas * labels, K[support_vectors, :])[:, support_vectors].flatten())\n    alphas = np.round(alphas, 4)\n    b = round(b, 4)\n    return (alphas.tolist(), b)\ndata = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 9]])\nlabels = np.array([1, 1, -1, -1, -1, 1])"}
{"task_id": 21, "completion_id": 1, "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Trains a kernel SVM using a deterministic (non-stochastic) version of the Pegasos algorithm.\n    \n    Parameters:\n    - data: np.ndarray, shape (n_samples, n_features)\n        Input features.\n    - labels: np.ndarray, shape (n_samples,)\n        Binary class labels (+1 or -1).\n    - kernel: str, {'linear', 'rbf'}\n        Choice of kernel function.\n    - lambda_val: float\n        Regularization parameter.\n    - iterations: int\n        Number of iterations to perform.\n    - sigma: float\n        Bandwidth parameter for the RBF kernel.\n    \n    Returns:\n    - alphas: np.ndarray, shape (n_samples,)\n        Alpha coefficients rounded to 4 decimals.\n    - bias: float\n        Bias term rounded to 4 decimals.\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples, dtype=np.float64)\n    b = 0.0\n\n    def kernel_func(X1, X2):\n        if kernel == 'linear':\n            return np.dot(X1, X2.T)\n        elif kernel == 'rbf':\n            dist_sq = np.sum((X1[:, np.newaxis, :] - X2[np.newaxis, :, :]) ** 2, axis=2)\n            return np.exp(-dist_sq / (2 * sigma ** 2))\n        else:\n            raise ValueError(\"Unknown kernel type. Use 'linear' or 'rbf'.\")\n    K = kernel_func(data, data)\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        scores = K @ (labels * alphas) + b\n        loss = np.maximum(1 - labels * scores, 0)\n        grad = np.zeros(n_samples)\n        for i in range(n_samples):\n            if loss[i] > 0:\n                grad += -labels[i] * K[:, i]\n        alphas = (1 - learning_rate * lambda_val) * alphas - learning_rate * grad / n_samples\n        print_max = np.max(np.abs(alphas))\n        if print_max > 1 / lambda_val:\n            alphas = alphas.clip(-1 / lambda_val, 1 / lambda_val)\n    valid_indices = np.where((alphas > 1e-06) & (np.maximum(1 - labels * (K @ (labels * alphas) + b), 0) < 1e-06))[0]\n    if valid_indices.size > 0:\n        b = np.mean(labels[valid_indices] - K[valid_indices, :] @ (labels * alphas))\n    alphas = np.round(alphas, 4)\n    b = round(b, 4)\n    return (alphas.tolist(), b)"}
{"task_id": 21, "completion_id": 2, "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    bias = 0\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2.T)\n\n    def rbf_kernel(x1, x2, sigma):\n        if x1.ndim == 1:\n            x1 = x1.reshape(1, -1)\n        if x2.ndim == 1:\n            x2 = x2.reshape(1, -1)\n        return np.exp(-np.linalg.norm(x1[:, np.newaxis] - x2[np.newaxis, :], axis=2) ** 2 / (2 * sigma ** 2))\n    if kernel == 'linear':\n        k_func = linear_kernel\n    elif kernel == 'rbf':\n        k_func = lambda x1, x2: rbf_kernel(x1, x2, sigma)\n    else:\n        raise ValueError(\"Kernel not supported. Choose 'linear' or 'rbf'.\")\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        K = k_func(data, data)\n        scores = np.dot(K, alpha * labels) + bias\n        hinge_losses = 1 - labels * scores\n        hinge_losses[hinge_losses < 0] = 0\n        gradient_alpha = K * labels[:, np.newaxis] * labels @ hinge_losses\n        alpha -= eta * lambda_val * alpha - eta * gradient_alpha / n_samples\n        alpha[alpha < 0] = 0\n        bias -= eta * (-labels / n_samples) @ hinge_losses\n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n    return (alpha.tolist(), bias)"}
{"task_id": 21, "completion_id": 3, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the PEGASOS algorithm for Kernel SVM.\n    \n    Parameters:\n    - data: 2D NumPy array of shape (n_samples, n_features)\n    - labels: 1D NumPy array of shape (n_samples,) with values in {-1, 1}\n    - kernel: str, either 'linear' or 'rbf'\n    - lambda_val: regularization parameter (float)\n    - iterations: number of iterations (int)\n    - sigma: bandwidth parameter for RBF kernel (float)\n    \n    Returns:\n    - alphas: alpha coefficients as list after converting to Python list\n    - bias: float, bias term\n    \"\"\"\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n\n    def kernel_func(x1, x2):\n        if kernel == 'linear':\n            return np.dot(x1, x2)\n        elif kernel == 'rbf':\n            return np.exp(-np.sum((x1 - x2) ** 2) / (2 * sigma ** 2))\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel_func(data[i], data[j])\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            y_pred = bias + np.dot(alpha, labels * K[:, i])\n            if labels[i] * y_pred < 1:\n                alpha[i] += 1 / t * (1.0 - lambda_val * eta * alpha[i])\n            else:\n                alpha[i] += 1 / t * (-lambda_val * eta * alpha[i])\n    epsilon = 1e-06\n    bias_candidates = []\n    for i in range(n_samples):\n        if abs(alpha[i]) > epsilon:\n            y_i = labels[i]\n            prediction = np.dot(alpha * labels, K[:, i]) + bias\n            if y_i * prediction <= 1 + epsilon:\n                bias_candidates.append(y_i - np.dot(alpha, y_i * labels * K[:, i]))"}
{"task_id": 21, "completion_id": 4, "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic PEGASOS algorithm for kernel SVM.\n    \n    Parameters:\n    - data: 2D NumPy array (n_samples, n_features)\n    - labels: 1D NumPy array (n_samples)\n    - kernel: 'linear' or 'rbf'\n    - lambda_val: Regularization parameter\n    - iterations: Number of iterations\n    - sigma: RBF kernel bandwidth (only used for 'rbf')\n    \n    Returns:\n    - alphas: Python list of learned alpha coefficients (n_samples,)\n    - bias: Python list of the learned bias term (float)\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        if kernel == 'linear':\n            kernel_matrix = np.dot(data, data.T)\n        elif kernel == 'rbf':\n            distances = np.sum(data ** 2, axis=1, keepdims=True) - 2 * np.dot(data, data.T) + np.sum(data ** 2, axis=1)\n            kernel_matrix = np.exp(-distances / (2 * sigma ** 2))\n        else:\n            raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n        K = np.diag(labels * alphas @ kernel_matrix)\n        predictions = K + labels * bias\n        update_mask = 1 - predictions > 0\n        alphas = (1 - eta_t * lambda_val) * alphas + eta_t * labels[update_mask][:, np.newaxis] * kernel_matrix[update_mask, :]\n        bias_update = eta_t * np.sum(labels[update_mask] * np.ones(n_samples)[update_mask][:, np.newaxis])\n        bias += bias_update\n        bias *= 1 - eta_t * lambda_val\n    return ([round(a, 4) for a in alphas.tolist()], round(bias, 4))\ndata = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [5.0, 6.0]])\nlabels = np.array([1, 1, -1, -1])"}
{"task_id": 21, "completion_id": 5, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the PEGASOS kernel SVM algorithm.\n    \n    Parameters:\n    - data: 2D NumPy array of shape (n_samples, n_features)\n    - labels: 1D NumPy array of shape (n_samples,)\n    - kernel: 'linear' or 'rbf'\n    - lambda_val: Regularization parameter\n    - iterations: Number of iterations (deterministic)\n    - sigma: RBF kernel bandwidth (for RBF kernel only)\n    \n    Returns:\n    - alphas: Alpha coefficients (numpy array) as a python list\n    - bias: Bias term (float) as a float\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        margin = np.zeros(n_samples)\n        for i in range(n_samples):\n            k_sum = 0.0\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    k = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    k = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n                else:\n                    raise ValueError(\"Unsupported kernel: choose 'linear' or 'rbf'\")\n                k_sum += alphas[j] * labels[j] * k\n            margin[i] = labels[i] * (k_sum + b)\n        error_indices = np.where(margin < 1)[0]\n        for i in error_indices:\n            alphas[i] += eta_t\n        decision_func = np.zeros(n_samples)\n        for i in range(n_samples):\n            k_sum = 0.0\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    k = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    k = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n                else:\n                    raise ValueError(\"Unsupported kernel: choose 'linear' or 'rbf'\")\n                k_sum += alphas[j] * labels[j] * k\n            decision_func[i] = k_sum + b\n        misclassified = np.where(labels * decision_func < 1)[0]\n        if len(misclassified) > 0:\n            sum_phi = np.zeros(n_samples)\n            for i in misclassified:\n                k_sum = 0.0\n                for j in range(n_samples):\n                    if kernel == 'linear':\n                        k = np.dot(data[i], data[j])\n                    elif kernel == 'rbf':\n                        k = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n                    else:\n                        raise ValueError(\"Unsupported kernel: choose 'linear' or 'rbf'\")\n                    k_sum += labels[i] * k\n                sum_phi[i] = k_sum\n            b += eta_t * np.mean(labels[misclassified] * np.ones_like(misclassified).astype(float))\n        if t % 10 == 0:\n            shrink_factor = 1 - eta_t / t\n            alphas = alphas * shrink_factor\n    alphas = np.round(alphas, 4)\n    b = np.round(b, 4)\n    return (alphas.tolist(), float(b))"}
{"task_id": 21, "completion_id": 6, "solution": "import numpy as np\nimport numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the PEGASOS kernel SVM algorithm.\n    \n    Parameters:\n    -----------\n    data : np.ndarray\n        Features (2D: samples x features).\n    labels : np.ndarray\n        Labels (1D: samples).\n    kernel : str\n        Kernel type: 'linear' or 'rbf'.\n    lambda_val : float\n        Regularization parameter.\n    iterations : int\n        Number of training iterations.\n    sigma : float\n        Kernel width for RBF kernel.\n\n    Returns:\n    --------\n    alphas : list\n        Alpha coefficients (rounded to 4 decimal places).\n    bias : float\n        Bias term (rounded to 4 decimal places).\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros((n_samples,), dtype=float)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    K[i, j] = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n        scores = np.dot(K, alphas * labels) + bias\n        for i in range(n_samples):\n            if labels[i] * (scores[i] + bias) <= 1:\n                alphas[i] = np.minimum(alphas[i] + eta_t * labels[i], 1.0 / (lambda_val * t))\n        alphas = np.clip(alphas, 0, 1.0 / (lambda_val * t))\n        bias_diff = 0\n        num = 0\n        for i in range(n_samples):\n            if labels[i] * (np.dot(K[i], alphas * labels) + bias) <= 1:\n                bias_diff += labels[i]\n                num += 1\n        if num > 0:\n            bias += bias_diff / num\n    alphas = np.round(alphas, 4)\n    bias = round(bias, 4)\n    return (alphas.tolist(), bias)"}
{"task_id": 21, "completion_id": 7, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        K = np.zeros((n_samples, n_samples))\n        if kernel == 'linear':\n            K = np.dot(data, data.T)\n        elif kernel == 'rbf':\n            for i in range(n_samples):\n                for j in range(n_samples):\n                    K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n        predicted = np.dot(alphas, labels * K.T) + bias\n        alphas = (1 - 1.0 / t) * alphas + 1.0 / t * (lambda_val * labels * (labels * predicted < 1)).reshape(-1, 1) * K.sum(axis=0)\n        eta_alphas = (labels[labels * predicted < 1] * K[labels * predicted < 1].T).sum(axis=0) / n_samples\n        bias = bias + 1.0 / (lambda_val * t) * eta_alphas\n    return ([round(alpha, 4) for alpha in alphas.tolist()], round(bias, 4))"}
{"task_id": 21, "completion_id": 8, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implementation of the deterministic version of the Pegasos Kernel SVM algorithm.\n    \n    Parameters:\n        data (np.ndarray): A 2D NumPy array where each row is a data sample.\n        labels (np.ndarray): A 1D NumPy array containing the class labels.\n        kernel (str): The kernel to use ('linear' or 'rbf').\n        lambda_val (float): Regularization parameter.\n        iterations (int): Number of training iterations.\n        sigma (float): The parameter for the RBF kernel.\n        \n    Returns:\n        alpha (list): The alpha coefficients of the trained model (rounded to 4 decimals).\n        bias (float): The bias of the trained model (rounded to 4 decimals).\n    \"\"\"\n    n_samples = data.shape[0]\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2.T)\n\n    def rbf_kernel(x1, x2):\n        if x1.ndim == 1:\n            x1 = x1[np.newaxis, :]\n        if x2.ndim == 1:\n            x2 = x2[np.newaxis, :]\n        distances = np.sum(x1 ** 2, axis=1, keepdims=True) + np.sum(x2 ** 2, axis=1) - 2 * np.dot(x1, x2.T)\n        return np.exp(-distances / (2 * sigma ** 2))\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        predictions = np.zeros(n_samples)\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    k = linear_kernel(data[i], data[j])\n                elif kernel == 'rbf':\n                    k = rbf_kernel(data[i], data[j])[0, j]\n                predictions[i] += alpha[j] * labels[j] * k\n        predictions += bias\n        for i in range(n_samples):\n            if labels[i] * predictions[i] < 1:\n                alpha[i] += eta_t * labels[i]\n        bias += eta_t * np.mean(labels * (predictions < 1))\n        alpha *= 1 - eta_t * lambda_val\n    return (alpha.round(4).tolist(), round(bias, 4))"}
{"task_id": 21, "completion_id": 9, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos algorithm for Kernel SVM.\n    \n    Parameters:\n        data (np.ndarray): 2D input data, shape (n_samples, n_features)\n        labels (np.ndarray): 1D array of labels, shape (n_samples,)\n        kernel (str): 'linear' or 'rbf'\n        lambda_val (float): Regularization parameter\n        iterations (int): Number of iterations\n        sigma (float): Kernel bandwidth for RBF kernel (ignored if kernel is 'linear')\n        \n    Returns:\n        alphas (list): Alpha coefficients rounded to 4 decimals\n        bias (float): Bias rounded to 4 decimals\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta_t = 1.0 / (lambda_val * t)\n        predictions = np.zeros(n_samples)\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    K[i, j] = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n        predictions = np.dot(alphas, K) + bias\n        for i in range(n_samples):\n            loss = labels[i] * (np.dot(alphas, K[i, :]) + bias)\n            if loss < 1:\n                alphas[i] += eta_t * labels[i]\n            else:\n                alphas[i] = (1 - eta_t) * alphas[i]\n        alphas *= 1 - eta_t\n        margin = labels * (np.dot(alphas, K) + bias)\n        misclassified_indices = np.where(margin < 1)[0]\n        if len(misclassified_indices) > 0:\n            margin_loss = labels[misclassified_indices] * (np.dot(alphas, K[misclassified_indices, :]) + bias)\n            loss = np.sum(margin_loss)\n            bias += eta_t * np.sum(labels[misclassified_indices] * (1 - margin_loss))\n        else:\n            bias *= 1 - eta_t\n    rounded_alphas = np.round(alphas, 4).tolist()\n    rounded_bias = np.round(bias, 4)\n    return (rounded_alphas, rounded_bias)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 1, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): Input value.\n    \n    Returns:\n    float: Sigmoid of z, rounded to 4 decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 2, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of the input value z.\n\n    Parameters:\n    z (float): Input value.\n\n    Returns:\n    float: Sigmoid output rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 3, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid of a given input z.\n    \n    Parameters:\n    z (float): The input value.\n    \n    Returns:\n    float: The sigmoid of z, rounded to 4 decimal places.\n    \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    return round(sigmoid_value, 4)"}
{"task_id": 22, "completion_id": 4, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 5, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 6, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 7, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid of z, rounded to four decimal places.\n    \n    The sigmoid function is defined as:\n    sigmoid(z) = 1 / (1 + e^(-z))\n    \n    Parameters:\n    z (float): Input value\n    \n    Returns:\n    float: Sigmoid of z, rounded to four decimal places\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 8, "solution": "import math\ndef sigmoid(z: float) -> float:\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 22, "completion_id": 9, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of a single value z.\n    \n    The sigmoid function is defined as: 1 / (1 + e^(-z))\n    \n    Parameters:\n    z (float): Input to the sigmoid function.\n    \n    Returns:\n    float: Result of the sigmoid function, rounded to 4 decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 1, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [score / sum_exp_scores for score in exp_scores]\n    return [round(value, 4) for value in softmax_values]\nscores = [2.0, 1.0, 0.1]"}
{"task_id": 23, "completion_id": 2, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [ex / sum_exp for ex in exp_scores]\n    return [round(value, 4) for value in softmax_values]"}
{"task_id": 23, "completion_id": 3, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_scores = [round(exp / sum_exp, 4) for exp in exp_scores]\n    return softmax_scores"}
{"task_id": 23, "completion_id": 4, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp / sum_exp_scores, 4) for exp in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 5, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [round(score / sum_exp, 4) for score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 6, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp for exp_score in exp_scores]\n    rounded_softmax_values = [round(value, 4) for value in softmax_values]\n    return rounded_softmax_values"}
{"task_id": 23, "completion_id": 7, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_result = [round(score_exp / sum_exp, 4) for score_exp in exp_scores]\n    return softmax_result"}
{"task_id": 23, "completion_id": 8, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values"}
{"task_id": 23, "completion_id": 9, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    if len(weights) != len(features[0]):\n        raise ValueError('Number of weights must match the number of features in each example')\n    num_samples = len(features)\n    predictions = []\n    for x in features:\n        z = sum((w * f for (w, f) in zip(weights, x))) + bias\n        prob = 1 / (1 + math.exp(-z))\n        predictions.append(round(prob, 4))\n    mse = sum(((y_pred - y_true) ** 2 for (y_pred, y_true) in zip(predictions, labels))) / num_samples\n    mse_rounded = round(mse, 4)\n    return (predictions, mse_rounded)\nfeatures = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\nlabels = [0, 1, 1]\nweights = [0.5, 0.5]\nbias = -2.0"}
{"task_id": 24, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features_arr = np.array(features)\n    labels_arr = np.array(labels)\n    weights_arr = np.array(weights)\n    linear_output = np.dot(features_arr, weights_arr) + bias\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    vfunc = np.vectorize(sigmoid)\n    predicted_probs = vfunc(linear_output)\n    mse = np.mean((predicted_probs - labels_arr) ** 2)\n    predicted_probs_rounded = np.round(predicted_probs, decimals=4)\n    mse_rounded = round(mse, 4)\n    predicted_probs_list = predicted_probs_rounded.tolist()\n    return (predicted_probs_list, mse_rounded)"}
{"task_id": 24, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    z = np.dot(features_array, weights_array) + bias\n    predicted_probabilities = 1 / (1 + np.exp(-z))\n    predicted_probabilities = np.round(predicted_probabilities, 4)\n    mse = np.mean((predicted_probabilities - np.array(labels)) ** 2)\n    mse = round(mse, 4)\n    predicted_probabilities = predicted_probabilities.tolist()\n    return (predicted_probabilities, mse)"}
{"task_id": 24, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation for binary classification.\n    \n    Parameters:\n    - features: List of feature vectors (2D list) where each inner list represents one example.\n    - labels: List of true binary labels (0 or 1).\n    - weights: List of weights, one for each feature.\n    - bias: The bias term for the neuron.\n    \n    Returns:\n    - A tuple containing:\n        - A list of predicted probabilities after applying the sigmoid activation function.\n        - The mean squared error between the predicted probabilities and the true labels.\n    \"\"\"\n\n    def sigmoid(z: float) -> float:\n        \"\"\"Apply the sigmoid activation function.\"\"\"\n        return 1.0 / (1 + math.exp(-z))\n    predicted_probabilities = []\n    for example in features:\n        weighted_sum = sum((weight * feature for (weight, feature) in zip(weights, example))) + bias\n        probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(round(probability, 4))\n    predicted_probabilities_array = np.array(predicted_probabilities).reshape((-1, 1))\n    predicted_probabilities = predicted_probabilities_array.tolist()\n    mse = sum(((label - prob) ** 2 for (label, prob) in zip(labels, predicted_probabilities))) / len(labels)\n    mse = round(mse, 4)\n    return (predicted_probabilities, mse)"}
{"task_id": 24, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    predicted_probabilities = []\n    for x in features:\n        z = sum((w * f for (w, f) in zip(weights, x))) + bias\n        probability = 1 / (1 + math.exp(-z))\n        predicted_probabilities.append(probability)\n    predicted_probabilities = [round(p, 4) for p in predicted_probabilities]\n    mse = sum(((p - l) ** 2 for (p, l) in zip(predicted_probabilities, labels))) / len(labels)\n    mse = round(mse, 4)\n    return (np.array(predicted_probabilities).tolist(), mse)"}
{"task_id": 24, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n\n    Parameters:\n    - features: List of feature vectors (each is a list of floats)\n    - labels: List of true binary labels (0 or 1)\n    - weights: List of floats, one for each feature\n    - bias: float bias term for the neuron\n\n    Returns:\n    - predicted_probabilities: Predicted probabilities after applying sigmoid activation\n    - mean_squared_error: MSE between the predicted probabilities and true labels\n    \"\"\"\n    predicted_probabilities = []\n    for x in features:\n        z = sum((w * f for (w, f) in zip(weights, x))) + bias\n        prob = 1 / (1 + math.exp(-z))\n        predicted_probabilities.append(prob)\n    predicted_probabilities = np.array(predicted_probabilities).round(4)\n    labels_array = np.array(labels)\n    mse = np.mean((labels_array - predicted_probabilities) ** 2).round(4)\n    return (predicted_probabilities.tolist(), mse)"}
{"task_id": 24, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features (list of list of float): List of feature vectors for multiple examples\n    - labels (list of int): Binary true labels for the examples\n    - weights (list of float): One weight for each feature\n    - bias (float): Bias of the neuron\n    \n    Returns:\n    - predicted_probabilities (list of float): Predicted probabilities for each example\n    - mean_squared_error (float): MSE between the predicted and true labels\n    \"\"\"\n    assert len(features) == len(labels), 'Features and labels should have the same length'\n    assert all((len(f) == len(weights) for f in features)), 'Each feature vector must match the number of weights'\n    predicted_probabilities = []\n    for x in features:\n        z = sum((w * f for (w, f) in zip(weights, x))) + bias\n        y_hat = 1 / (1 + math.exp(-z))\n        predicted_probabilities.append(y_hat)\n    predicted_probabilities = np.array(predicted_probabilities).round(4).tolist()\n    errors = [(y - p) ** 2 for (y, p) in zip(labels, predicted_probabilities)]\n    mean_squared_error = sum(errors) / len(errors)\n    mean_squared_error = round(mean_squared_error, 4)\n    return (predicted_probabilities, mean_squared_error)"}
{"task_id": 24, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron model with sigmoid activation for binary classification.\n    \n    Parameters:\n    - features: A list of feature vectors, where each vector is a list of floats.\n    - labels: A list of binary labels (0 or 1), one for each feature vector.\n    - weights: A list of weights, one for each feature in the vector.\n    - bias: A single float representing the neuron's bias.\n    \n    Returns:\n    - A tuple:\n        1. A list of predicted probabilities (after applying the sigmoid function), rounded to 4 decimals.\n        2. The mean squared error (MSE) between predicted probabilities and true labels, rounded to 4 decimals.\n    \"\"\"\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    n = len(features)\n    for x in features:\n        z = bias + sum([w * f for (w, f) in zip(weights, x)])\n        pred = sigmoid(z)\n        predictions.append(pred)\n    predictions = np.round(np.array(predictions), 4)\n    predictions = predictions.tolist()\n    labels = np.array(labels)\n    mse = np.round(np.mean((np.array(predictions) - labels) ** 2), 4)\n    return (predictions, mse)"}
{"task_id": 24, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    X = np.array(features)\n    y = np.array(labels)\n    w = np.array(weights)\n    z = X.dot(w) + bias\n    predictions = 1 / (1 + np.exp(-z))\n    predictions = np.round(predictions, 4)\n    mse = np.mean((predictions - y) ** 2)\n    mse = round(mse, 4)\n    return (predictions.tolist(), mse)"}
{"task_id": 24, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features (list[list[float]]): Input feature vectors.\n    - labels (list[int]): True binary labels (0 or 1).\n    - weights (list[float]): Neuron's weights for each feature.\n    - bias (float): Neuron's bias term.\n    \n    Returns:\n    - list[float]: Predicted probabilities after applying sigmoid activation.\n    - float: Mean squared error (MSE) rounded to 4 decimal places.\n    \"\"\"\n    predicted_probabilities = []\n    for x in features:\n        z = sum((w * f for (w, f) in zip(weights, x))) + bias\n        probability = 1 / (1 + math.exp(-z))\n        predicted_probabilities.append(probability)\n    labels = [float(label) for label in labels]\n    mse = np.mean([(pred - true) ** 2 for (pred, true) in zip(predicted_probabilities, labels)])\n    predicted_probabilities_rounded = np.round(predicted_probabilities, 4).tolist()\n    mse_rounded = round(mse, 4)\n    return (predicted_probabilities_rounded, mse_rounded)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    cost_history = []\n    for epoch in range(epochs):\n        linear = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear))\n        mse = np.mean((predictions - labels) ** 2)\n        cost_history.append(round(mse, 4))\n        error = predictions - labels\n        d_weights = 1 / features.shape[0] * np.dot(features.T, error * predictions * (1 - predictions))\n        d_bias = 1 / features.shape[0] * np.sum(error * predictions * (1 - predictions))\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    updated_weights = np.around(weights, decimals=4).tolist()\n    updated_bias = round(bias, 4)\n    rounded_cost_history = [round(mse, 4) for mse in cost_history]\n    return (updated_weights, updated_bias, rounded_cost_history)"}
{"task_id": 25, "completion_id": 1, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    for epoch in range(epochs):\n        total_error = 0.0\n        m = features.shape[0]\n        z = np.dot(features, weights) + bias\n        y_pred = 1 / (1 + np.exp(-z))\n        mse = np.mean((y_pred - labels) ** 2)\n        mse_history.append(np.round(mse, 4))\n        error = y_pred - labels\n        dz = error * y_pred * (1 - y_pred)\n        d_weights = 1 / m * np.dot(features.T, dz)\n        d_bias = 1 / m * np.sum(dz)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = np.round(bias, 4)\n    mse_history = [np.round(val, 4) for val in mse_history]\n    return (weights_rounded.tolist(), bias_rounded, mse_history)\nfeatures = np.array([[0.5, 0.2], [1.0, 0.5], [0.0, 1.0]])\nlabels = np.array([0, 1, 1])\ninitial_bias = 0.3\nlearning_rate = 0.1\nepochs = 10"}
{"task_id": 25, "completion_id": 2, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-z))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_list.append(round(mse, 4))\n        error = predictions - labels\n        dw = np.dot(features.T, error) / features.shape[0]\n        db = np.mean(error)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    mse_list = [round(mse, 4) for mse in mse_list]\n    return (weights.tolist(), bias, mse_list)"}
{"task_id": 25, "completion_id": 3, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> tuple[np.ndarray, float, list[float]]:\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_output))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_history.append(round(mse, 4))\n        error = predictions - labels\n        d_weights = 1 / features.shape[0] * np.dot(features.T, error * predictions * (1 - predictions))\n        d_bias = 1 / features.shape[0] * np.sum(error * predictions * (1 - predictions))\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    mse_history = [round(mse, 4) for mse in mse_history]\n    return (weights, bias, mse_history)"}
{"task_id": 25, "completion_id": 4, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation and trains it using backpropagation\n    with gradient descent to minimize the MSE loss.\n\n    Parameters:\n    - features: 2D numpy array of feature vectors (shape: [num_samples, num_features])\n    - labels: 1D numpy array of target labels (shape: [num_samples])\n    - initial_weights: Initial weights (shape: [num_features])\n    - initial_bias: Initial bias (float)\n    - learning_rate: Learning rate for gradient descent\n    - epochs: Number of training iterations\n\n    Returns:\n    - weights: Updated weights after training\n    - bias: Updated bias after training\n    - mse_values: List of MSE for each epoch, rounded to 4 decimals\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        activation = 1 / (1 + np.exp(-linear_output))\n        mse = np.mean((activation - labels) ** 2)\n        mse_values.append(round(float(mse), 4))\n        error = activation - labels\n        d_activation = activation * (1 - activation)\n        d_output = error * d_activation\n        dw = np.dot(d_output, features) / len(labels)\n        db = np.mean(d_output)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    updated_weights = np.round(weights, 4)\n    updated_bias = round(bias, 4)\n    rounded_mse_values = [round(m, 4) for m in mse_values]\n    return (updated_weights, updated_bias, rounded_mse_values)"}
{"task_id": 25, "completion_id": 5, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_output))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(float(mse), 4))\n        error = predictions - labels\n        dloss_dpred = error\n        dpred_dz = predictions * (1 - predictions)\n        dz_weights = features\n        dz_bias = np.ones_like(bias)\n        dMSE_weights = np.dot(dz_weights.T, dloss_dpred * dpred_dz) / len(labels)\n        dMSE_bias = np.mean(dloss_dpred * dpred_dz)\n        weights -= learning_rate * dMSE_weights\n        bias -= learning_rate * dMSE_bias\n    final_weights = np.round(weights, 4)\n    final_bias = round(bias, 4)\n    final_mse_values = [round(val, 4) for val in mse_values]\n    return (final_weights.tolist(), final_bias, final_mse_values)"}
{"task_id": 25, "completion_id": 6, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-z))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_list.append(round(mse, 4))\n        error = predictions - labels\n        d_weights = np.dot(features.T, error * predictions * (1 - predictions)) / features.shape[0]\n        d_bias = np.mean(error * predictions * (1 - predictions))\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    mse_list = [round(mse, 4) for mse in mse_list]\n    return (weights, bias, mse_list)"}
{"task_id": 25, "completion_id": 7, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_output))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_list.append(round(float(mse), 4))\n        error = predictions - labels\n        d_weights = np.dot(features.T, error) / features.shape[0]\n        d_bias = np.mean(error)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    updated_weights = np.round(weights, 4).tolist()\n    updated_bias = round(bias, 4)\n    mse_list = [round(mse, 4) for mse in mse_list]\n    return (updated_weights, updated_bias, mse_list)"}
{"task_id": 25, "completion_id": 8, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def mse(y_true, y_pred):\n        return np.mean((y_true - y_pred) ** 2)\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_pred = sigmoid(z)\n        current_mse = mse(labels, y_pred)\n        mse_values.append(current_mse)\n        error = y_pred - labels\n        dz = error * y_pred * (1 - y_pred)\n        dw = np.dot(features.T, dz) / features.shape[0]\n        db = np.mean(dz)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    mse_values = [round(value, 4) for value in mse_values]\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    return (weights.tolist(), bias, mse_values)"}
{"task_id": 25, "completion_id": 9, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def mse_loss(predictions, true_labels):\n        return np.mean((predictions - true_labels) ** 2)\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        loss = mse_loss(predictions, labels)\n        mse_history.append(round(loss, 4))\n        error = predictions - labels\n        d_weights = np.dot(features.T, error) / len(labels)\n        d_bias = np.mean(error)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    rounded_weights = np.round(weights, 4)\n    rounded_bias = round(bias, 4)\n    rounded_mse_history = [round(val, 4) for val in mse_history]\n    return (rounded_weights, rounded_bias, rounded_mse_history)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'Exponent must be a scalar'\n        out = Value(self.data ** other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 1, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'Only supporting int/float powers'\n        out = Value(self.data ** other, _children=(self,), _op=f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 2, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'only supporting int/float powers'\n        out = Value(self.data ** other, _children=(self,), _op=f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 3, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __radd__(self, other):\n        return self + other\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'only supporting int/float exponents'\n        out = Value(self.data ** other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += out.grad * (1 if self.data > 0 else 0)\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 4, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad}, op='{self._op}')\""}
{"task_id": 26, "completion_id": 5, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def __relu__(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += out.grad * (1 if self.data > 0 else 0)\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 6, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 7, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()"}
{"task_id": 26, "completion_id": 8, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 26, "completion_id": 9, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data ** other.data, _children=(self, other), _op='pow')\n\n        def _backward():\n            self.grad += out.grad * other.data * self.data ** (other.data - 1)\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C in R^3.\n    \n    Parameters:\n    - B: List of basis vectors in basis B.\n    - C: List of basis vectors in basis C.\n    \n    Returns:\n    - P: Transformation matrix from basis B to C, as a list of lists.\n    \"\"\"\n    B_array = np.array(B, dtype=float)\n    C_array = np.array(C, dtype=float)\n    B_inv = np.linalg.inv(B_array)\n    P = C_array @ B_inv\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 27, "completion_id": 1, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n        B (list of list of int): Basis vectors in B.\n        C (list of list of int): Basis vectors in C.\n        \n    Returns:\n        list of list of float: Transformation matrix from B to C, rounded to 4 decimals.\n    \"\"\"\n    B = np.array(B, dtype=float)\n    C = np.array(C, dtype=float)\n    inv_B = np.linalg.inv(B)\n    P = inv_B @ C\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]"}
{"task_id": 27, "completion_id": 2, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix from basis B to basis C.\n    \n    Parameters:\n    B (list of list of int): The basis B in R^3 as a list of column vectors.\n    C (list of list of int): The basis C in R^3 as a list of column vectors.\n    \n    Returns:\n    list of list of float: The transformation matrix P from B to C, rounded to 4 decimals.\n    \"\"\"\n    B_array = np.array(B).T\n    C_array = np.array(C).T\n    P = np.linalg.inv(C_array) @ B_array\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 27, "completion_id": 3, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    B (list[list[int]]): A list of 3 basis vectors (lists) in basis B.\n    C (list[list[int]]): A list of 3 basis vectors (lists) in basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix (3x3) from B to C, rounded to 4 decimals.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    C_inv = np.linalg.inv(C_array)\n    P = np.dot(C_inv, B_array)\n    return np.round(P, 4).tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)"}
{"task_id": 27, "completion_id": 4, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix P from basis B to basis C.\n\n    Args:\n        B (list of list of int): Basis vectors in basis B (3x3 matrix).\n        C (list of list of int): Basis vectors in basis C (3x3 matrix).\n\n    Returns:\n        list of list of float: Transformation matrix P from B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B, dtype=float)\n    C_array = np.array(C, dtype=float)\n    B_inverse = np.linalg.inv(B_array)\n    P = B_inverse @ C_array\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 5, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix from basis B to basis C.\n    \n    Args:\n        B: A list of 3 basis vectors in R^3 (each a list of 3 integers)\n        C: A list of 3 basis vectors in R^3 (each a list of 3 integers)\n    \n    Returns:\n        A 3x3 transformation matrix as a nested list of floats,\n        rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    B_to_E = B_array\n    C_to_E = C_array\n    E_to_C = np.linalg.inv(C_to_E)\n    P = E_to_C @ B_to_E\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 6, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix from basis B to basis C in R^3.\n    \n    Parameters:\n    B (list of lists): Basis vectors for basis B.\n    C (list of lists): Basis vectors for basis C.\n    \n    Returns:\n    list of lists: Transformation matrix from B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    C_inv = np.linalg.inv(C_matrix)\n    P = C_inv @ B_matrix\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 7, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix from basis B to basis C.\n    \n    Parameters:\n    - B: List of basis vectors defining the original basis (in standard coordinates).\n    - C: List of basis vectors defining the target basis (in standard coordinates).\n    \n    Returns:\n    - P: Transformation matrix P such that [x]_C = P [x]_B.\n    \"\"\"\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    C_inv = np.linalg.inv(C_matrix)\n    P = C_inv @ B_matrix\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 8, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    C_inv = np.linalg.inv(C_matrix)\n    P = C_inv @ B_matrix\n    return np.round(P, 4).tolist()"}
{"task_id": 27, "completion_id": 9, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Compute the transformation matrix from basis B to basis C.\n    \n    Parameters:\n    - B: List of three lists, each representing a vector in basis B.\n    - C: List of three lists, each representing a vector in basis C.\n    \n    Returns:\n    - A 3x3 list of lists representing the transformation matrix P from B to C,\n      rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    P = np.linalg.inv(C_array) @ B_array\n    P_rounded = np.round(P, decimals=4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using eigenvalues and eigenvectors of A^T A.\n    Returns:\n        U (list): Left singular vectors as a 2x2 matrix, transposed\n        S (list): Diagonal matrix of singular values (as a 2x2 diag matrix)\n        V (list): Right singular vectors as a 2x2 matrix\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    order = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[order]\n    eigenvectors = eigenvectors[:, order]\n    singular_values = np.sqrt(np.abs(eigenvalues))\n    S = np.diag(singular_values)\n    V = eigenvectors.T\n    col1 = A @ V[0, :] / (singular_values[0] + 1e-10)\n    col2 = A @ V[1, :] / (singular_values[1] + 1e-10)\n    U = np.column_stack([col1, col2])\n    U[:, 0] /= np.linalg.norm(U[:, 0])\n    U[:, 1] /= np.linalg.norm(U[:, 1])\n    if np.linalg.det(U) < 0:\n        U[:, 1] *= -1\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[4, 0], [0, 3]])"}
{"task_id": 28, "completion_id": 1, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    AtA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(AtA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    sigma = np.sqrt(eigenvalues)\n    S = np.diag(sigma)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sigma[i] > 1e-10:\n            Ui = np.dot(A, V[:, i]) / sigma[i]\n            Ui /= np.linalg.norm(Ui)\n        else:\n            if i == 0:\n                Ui = np.array([1, 0])\n            else:\n                Ui = np.array([0, 1])\n            Ui = Ui / np.linalg.norm(Ui)\n        U[:, i] = Ui\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nU = [[...], [...]]\nS = [[...], [...]]\nV = [[...], [...]]"}
{"task_id": 28, "completion_id": 2, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigen_values, eigen_vectors) = np.linalg.eig(A_T_A)\n    sigma_squared = abs(eigen_values)\n    singular_values = np.sqrt(sigma_squared)\n    sigma_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sigma_indices]\n    eigen_vectors = eigen_vectors[:, sigma_indices]\n    S = np.diag(singular_values)\n    V = eigen_vectors\n    U_cols = []\n    for i in range(2):\n        if np.isclose(singular_values[i], 0):\n            U_col = np.zeros_like(A[:, 0])\n        else:\n            U_col = np.dot(A, V[:, i]) / singular_values[i]\n        U_cols.append(U_col)\n    U = np.column_stack(U_cols)\n    (Q, _) = np.linalg.qr(U)\n    U = Q\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 28, "completion_id": 3, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using eigenvalues and vectors of A^T A.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 matrix as a numpy array.\n    \n    Returns:\n    tuple: U, S, V where:\n        U is a 2x2 orthogonal matrix,\n        S is a 1D array of singular values,\n        V is a 2x2 orthogonal matrix.\n    \"\"\"\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.sqrt(eigenvalues)\n    V = eigenvectors.T\n    U = np.zeros_like(A)\n    for i in range(2):\n        if S[i] > 1e-10:\n            U[:, i] = (A @ V[i]).flatten()\n            U[:, i] /= np.linalg.norm(U[:, i])\n        else:\n            U[:, i] = np.random.rand(2)\n            U[:, i] = U[:, i] / np.linalg.norm(U[:, i])\n    (U, _) = np.linalg.qr(U)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[3, 2], [2, 6]])"}
{"task_id": 28, "completion_id": 4, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    sigma = np.sqrt(np.abs(eigenvalues))\n    S = np.diag(sigma).round(4)\n    V = eigenvectors.round(4)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sigma[i] > 1e-10:\n            U[:, i] = (np.dot(A, V[:, i]) / sigma[i]).round(4)\n        elif i == 0:\n            U[:, i] = [1, 0]\n        else:\n            U[:, i] = [0, 1]\n    (Q, R) = np.linalg.qr(U)\n    U = Q.round(4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = [[3, 4], [1, 2]]"}
{"task_id": 28, "completion_id": 5, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    singular_values = np.sqrt(eigenvalues)\n    S_diag = np.diag(singular_values)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = singular_values[idx]\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = A @ V[:, i] / singular_values[i]\n        else:\n            u_i = np.array([0, 0])\n        U[:, i] = u_i\n    for i in range(2):\n        if np.linalg.norm(U[:, i]) > 1e-10:\n            U[:, i] = U[:, i] / np.linalg.norm(U[:, i])\n        elif i == 1:\n            U[:, 1] = -U[:, 0] if np.linalg.norm(U[:, 0]) > 1e-10 else np.array([1, 0])\n    U = np.round(U, 4)\n    S = np.round(S_diag, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[3, -1], [2, 2]])"}
{"task_id": 28, "completion_id": 6, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S_diag = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        u_i = np.dot(A, V[:, i])\n        if singular_values[i] > 1e-10:\n            u_i = u_i / singular_values[i]\n        else:\n            u_i = np.array([-U[1, i], U[0, i]]) if i == 1 else np.array([1.0, 0.0])\n            u_i = u_i / np.sqrt(np.dot(u_i, u_i))\n        U[:, i] = u_i\n\n    def round_and_list(matrix):\n        return np.round(matrix, 4).tolist()\n    return (round_and_list(U), round_and_list(S_diag), round_and_list(V))"}
{"task_id": 28, "completion_id": 7, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using eigendecomposition on A^T A.\n\n    Args:\n        A (np.ndarray): A 2x2 input matrix.\n\n    Returns:\n        tuple: A tuple of (U, S, V) where:\n            U is a 2x2 orthogonal matrix,\n            S is a 1D array of singular values,\n            V is a 2x2 orthogonal matrix such that A = USV^T.\n    \"\"\"\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.sqrt(eigenvalues).round(4)\n    S_matrix = np.diag(S)\n    V = eigenvectors.astype(np.float64).round(4)\n    U = np.zeros((2, 2), dtype=np.float64)\n    for i in range(2):\n        if S[i] != 0:\n            u_i = np.dot(A, V[:, i]) / S[i]\n            U[:, i] = u_i / np.linalg.norm(u_i)\n        else:\n            U[:, i] = np.random.randn(2)\n            U[:, i] -= np.dot(U[:, i], U[:, (i + 1) % 2]) * U[:, (i + 1) % 2]\n            U[:, i] /= np.linalg.norm(U[:, i])\n    U = U.round(4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 28, "completion_id": 8, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = np.real(eigenvalues)[idx]\n    eigenvectors = np.real(eigenvectors[:, idx])\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors.T\n    U = np.zeros((2, 2))\n    for i in range(2):\n        v_i = V[i].reshape(2, 1)\n        av_i = np.dot(A, v_i).flatten()\n        sigma_i = singular_values[i]\n        if sigma_i > 1e-10:\n            U_i = av_i / sigma_i\n        else:\n            U_i = np.zeros(2)\n        U[:, i] = U_i\n    U[:, 0] /= np.linalg.norm(U[:, 0])\n    U[:, 1] /= np.linalg.norm(U[:, 1])\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 28, "completion_id": 9, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using eigendecomposition of A^T A.\n    \n    Parameters:\n        A (np.ndarray): 2x2 input matrix.\n        \n    Returns:\n        tuple: Matrices U, S, V^T such that A \u2248 U @ S @ V^T\n    \"\"\"\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        sigma = singular_values[i]\n        if sigma > 1e-10:\n            U[:, i] = A @ V[:, i] / sigma\n        else:\n            U[:, i] = np.zeros(2)\n    U_rounded = np.round(U, 4)\n    S_rounded = np.round(S, 4)\n    V_rounded = np.round(V, 4)\n    return (U_rounded.tolist(), S_rounded.tolist(), V_rounded.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays X and y in the same random order to maintain their correspondence.\n    \n    Parameters:\n        X (np.ndarray): Feature data of shape (n_samples, n_features)\n        y (np.ndarray): Target data of shape (n_samples,)\n        seed (int, optional): Seed for random number generator for reproducibility\n\n    Returns:\n        list: A list of the shuffled arrays converted to native Python lists using tolist()\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 1, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in X and y while preserving the corresponding order.\n    \n    Parameters:\n    - X (np.ndarray): Features array of shape (n_samples, n_features)\n    - y (np.ndarray): Labels array of shape (n_samples,)\n    - seed (int, optional): Seed for the random number generator for reproducibility\n    \n    Returns:\n    - X_shuffled (list): Shuffled features as a list\n    - y_shuffled (list): Shuffled labels as a list\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    shuffled_indices = np.random.permutation(len(X))\n    X_shuffled = X[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 2, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two NumPy arrays X and y in the same order.\n\n    Parameters:\n        X (np.ndarray): Features array.\n        y (np.ndarray): Labels or targets array.\n        seed (int, optional): Seed for random number generator for reproducibility.\n\n    Returns:\n        list: A list containing the shuffled X and y as Python lists.\n    \"\"\"\n    if not (isinstance(X, np.ndarray) and isinstance(y, np.ndarray)):\n        raise ValueError('Both X and y must be numpy arrays.')\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    rng = np.random.default_rng(seed)\n    indices = rng.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_list = X_shuffled.tolist()\n    y_list = y_shuffled.tolist()\n    return [X_list, y_list]"}
{"task_id": 29, "completion_id": 3, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays, X and y, in the same random order while maintaining their correspondence.\n    \n    Parameters:\n        X (np.ndarray): Feature array.\n        y (np.ndarray): Label or target array.\n        seed (int, optional): Seed value for reproducibility. Default is None.\n    \n    Returns:\n        list: A list containing shuffled X and y as Python lists.\n    \"\"\"\n    np.random.seed(seed)\n    shuffled_indices = np.random.permutation(len(X))\n    X_shuffled = X[shuffled_indices]\n    y_shuffled = y[shuffled_indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 29, "completion_id": 4, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in X and y arrays while preserving their corresponding order.\n    \n    Parameters:\n        X (np.ndarray): Feature data.\n        y (np.ndarray): Target data.\n        seed (int, optional): Seed for random shuffling for reproducibility.\n        \n    Returns:\n        [list, list]: A list of X and y after being shuffled and converted to Python lists.\n    \"\"\"\n    np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 5, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the numpy arrays X and y while maintaining the corresponding order.\n    \n    Parameters:\n        X (np.ndarray): Input feature matrix.\n        y (np.ndarray): Target/output vector.\n        seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n        list: A list containing two elements, X_shuffled and y_shuffled, as lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return [X_shuffled_list, y_shuffled_list]"}
{"task_id": 29, "completion_id": 6, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffle two numpy arrays X and y in the same order while maintaining alignment.\n    \n    Parameters:\n    X (np.ndarray): Input feature array of shape (n_samples, ...).\n    y (np.ndarray): Target array of shape (n_samples, ...).\n    seed (int, optional): Random seed for reproducibility. Default is None.\n    \n    Returns:\n    list: List of shuffled arrays after reshaping using tolist().\n    \"\"\"\n    np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 29, "completion_id": 7, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two NumPy arrays X and y in the same random order.\n    \n    Parameters:\n    X (np.ndarray): Features of shape (n_samples, n_features)\n    y (np.ndarray): Labels of shape (n_samples,)\n    seed (int, optional): Seed for random number generator for reproducibility\n    \n    Returns:\n    list: A list where the first element is X and the second is y, both shuffled and converted to lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    idxs = np.random.permutation(X.shape[0])\n    X_shuffled = X[idxs]\n    y_shuffled = y[idxs]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 29, "completion_id": 8, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles two NumPy arrays, X and y, in unison to maintain the\n    corresponding indices.\n\n    Parameters:\n        X (np.ndarray): Feature data.\n        y (np.ndarray): Label data.\n        seed (int, optional): Random seed for reproducibility. Defaults to None.\n\n    Returns:\n        tuple of lists: Shuffled X and y as Python lists.\n    \"\"\"\n    X = np.asarray(X)\n    y = np.asarray(y)\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must be of the same length along the first dimension')\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 29, "completion_id": 9, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays X and y in the same random order while maintaining their pairing.\n    \n    Parameters:\n    - X (np.ndarray): Feature array.\n    - y (np.ndarray): Target array.\n    - seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    - list: A list containing two shuffled lists [X_shuffled, y_shuffled].\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches from the dataset (X, y or just X).\n    \n    Parameters:\n    X (np.ndarray): Input data.\n    y (np.ndarray, optional): Target data. Defaults to None.\n    batch_size (int): Size of each batch.\n    \n    Yields:\n    tuple or list: If y is provided, yields (X_batch, y_batch); otherwise, yields X_batch.\n    \"\"\"\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for i in range(0, X.shape[0], batch_size):\n        batch_indices = indices[i:i + batch_size]\n        X_batch = X[batch_indices].tolist()\n        if y is not None:\n            y_batch = y[batch_indices].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 30, "completion_id": 1, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of a given size from numpy arrays X and optional y.\n    \n    Parameters:\n        X (np.ndarray): Input data array.\n        y (np.ndarray, optional): Labels array.\n        batch_size (int): Size of each batch to yield.\n    \n    Yields:\n        tuple: A batch of (X_batch, y_batch) if y is provided, else (X_batch,).\n    \"\"\"\n    n_samples = X.shape[0]\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        X_batch = X[start_idx:end_idx].tolist()\n        if y is not None:\n            y_batch = y[start_idx:end_idx].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield (X_batch,)"}
{"task_id": 30, "completion_id": 2, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A batch iterator that yields batches from the dataset.\n    \n    Parameters:\n        X (np.ndarray): Input data of shape (num_samples, ...).\n        y (np.ndarray, optional): Labels of shape (num_samples, ...).\n        batch_size (int): Number of samples per batch.\n    \n    Yields:\n        If y is not None, yields (X_batch, y_batch) as lists.\n        If y is None, yields X_batch as a list.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i + batch_size].tolist()\n        if y is not None:\n            y_batch = y[i:i + batch_size].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 30, "completion_id": 3, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from the input arrays X and y (optional).\n    \n    Parameters:\n        X (np.ndarray): Input data array of shape (n_samples, ...).\n        y (np.ndarray, optional): Target array of shape (n_samples, ...).\n        batch_size (int): Size of each batch.\n\n    Yields:\n        If y is provided: (X_batch_list, y_batch_list)\n        If y is not provided: X_batch_list\n    \"\"\"\n    n_samples = X.shape[0]\n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        X_batch = X[start:end]\n        if y is not None:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 4, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, n_samples, batch_size):\n        batch_indices = indices[start_idx:start_idx + batch_size]\n        X_batch = X[batch_indices]\n        if y is not None:\n            y_batch = y[batch_indices]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 5, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of a specified size from the numpy array X and optionally from y.\n    \n    Parameters:\n        X (np.ndarray): Input data array.\n        y (np.ndarray, optional): Labels array. If None, only yields batches of X.\n        batch_size (int): Size of each batch.\n    \n    Yields:\n        tuple or np.ndarray: If y is not None, yields (X_batch, y_batch).\n                             Otherwise, yields X_batch.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        X_batch = X[i:i + batch_size]\n        if y is not None:\n            y_batch = y[i:i + batch_size]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 6, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of (X, y) or X if y is not provided.\n\n    Parameters:\n    - X: np.ndarray, input data samples.\n    - y: np.ndarray or None, optional labels for the input samples.\n    - batch_size: int, number of samples per batch.\n\n    Yields:\n    - List of X (or (X, y) if y is provided) in batches, each element converted to a Python list.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_X = X[i:i + batch_size]\n        if y is not None:\n            batch_y = y[i:i + batch_size]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 30, "completion_id": 7, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches from the input data X (and optionally y).\n    \n    Parameters:\n    X (np.ndarray): Input data of shape (n_samples, ...).\n    y (np.ndarray, optional): Labels of shape (n_samples, ...). Defaults to None.\n    batch_size (int): Number of samples per batch.\n\n    Yields:\n    np.ndarray: A batch of X (and y if provided);\n      batch is converted to list using tolist().\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_end = i + batch_size\n        batch_X = X[i:batch_end]\n        if y is not None:\n            batch_y = y[i:batch_end]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 30, "completion_id": 8, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generator function that yields batches from a numpy array X and an optional numpy array y.\n    If y is provided, yields batches as (X_batch, y_batch) pairs.\n    If y is not provided, yields batches as X_batch only.\n    Each batch is returned as a Python list via the tolist() method.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    for i in range(0, n_samples, batch_size):\n        batch_indices = indices[i:i + batch_size]\n        X_batch = X[batch_indices]\n        if y is not None:\n            y_batch = y[batch_indices]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 30, "completion_id": 9, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A generator that yields batches from the dataset X (and optionally y).\n    \n    Parameters:\n    - X (np.ndarray): Input data of shape (n_samples, ...).\n    - y (np.ndarray, optional): Labels of shape (n_samples, ...).\n    - batch_size (int): Size of each batch.\n    \n    Yields:\n    - If y is provided, yields (X_batch, y_batch).\n    - If y is not provided, yields X_batch.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_end = min(i + batch_size, n_samples)\n        X_batch = X[i:batch_end].tolist()\n        if y is not None:\n            y_batch = y[i:batch_end].tolist()\n            yield (X_batch, y_batch)\n        else:\n            yield X_batch"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X based on whether the value of a specified feature is \n    greater than or equal to a given threshold.\n\n    Parameters:\n    X (np.ndarray): Input dataset of shape (n_samples, n_features)\n    feature_i (int): Index of the feature to be used for splitting\n    threshold (float): Threshold value to divide the dataset\n\n    Returns:\n    list: left_subset - Samples where feature_i >= threshold\n    list: right_subset - Samples where feature_i < threshold\n    \"\"\"\n    X = np.array(X)\n    left_mask = X[:, feature_i] >= threshold\n    right_mask = X[:, feature_i] < threshold\n    left_subset = X[left_mask].tolist()\n    right_subset = X[right_mask].tolist()\n    return (left_subset, right_subset)"}
{"task_id": 31, "completion_id": 1, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset into two subsets based on feature_i being greater than or equal to threshold.\n\n    Parameters:\n    X (np.ndarray): Input dataset of shape (n_samples, n_features).\n    feature_i (int): Index of the feature to use for splitting.\n    threshold (float): The threshold value to compare against.\n\n    Returns:\n    list: Two lists, where the first contains samples where feature_i >= threshold,\n          and the second contains those where feature_i < threshold.\n    \"\"\"\n    left = X[X[:, feature_i] >= threshold]\n    right = X[X[:, feature_i] < threshold]\n    left_subset = left.reshape(-1, X.shape[1]).tolist()\n    right_subset = right.reshape(-1, X.shape[1]).tolist()\n    return (left_subset, right_subset)"}
{"task_id": 31, "completion_id": 2, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset into two subsets based on whether the value\n    of a specified feature is >= threshold.\n\n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - feature_i: index of the feature (int)\n    - threshold: threshold value to split the data (float or int)\n\n    Returns:\n    - left: list of samples where feature_i >= threshold\n    - right: list of samples where feature_i < threshold\n    \"\"\"\n    feature_values = X[:, feature_i]\n    mask = feature_values >= threshold\n    left = X[mask].tolist()\n    right = X[~mask].tolist()\n    return (left, right)"}
{"task_id": 31, "completion_id": 3, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset into two subsets based on whether the value of the specified feature is\n    greater than or equal to the given threshold.\n\n    Parameters:\n    - X: numpy array or list, the dataset (samples at rows, features at columns)\n    - feature_i: int, the index of the feature to use for splitting\n    - threshold: float or int, the threshold value for dividing the dataset\n\n    Returns:\n    - left: list of samples where feature value >= threshold\n    - right: list of samples where feature value < threshold\n    \"\"\"\n    X = np.array(X)\n    left_mask = X[:, feature_i] >= threshold\n    right_mask = ~left_mask\n    left = X[left_mask].tolist()\n    right = X[right_mask].tolist()\n    return (left, right)"}
{"task_id": 31, "completion_id": 4, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    left_mask = X[:, feature_i] >= threshold\n    right_mask = ~left_mask\n    left_subset = X[left_mask].tolist()\n    right_subset = X[right_mask].tolist()\n    return (left_subset, right_subset)"}
{"task_id": 31, "completion_id": 5, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset into two subsets based on the value of a specific feature.\n\n    Parameters:\n    - X (np.ndarray): The input dataset as a 2D NumPy array.\n    - feature_i (int): The index of the feature to use for division.\n    - threshold (float): The threshold value to compare the feature against.\n\n    Returns:\n    - left, right (list, list): Two Python lists representing the left and right subsets.\n    \"\"\"\n    feature_values = X[:, feature_i]\n    mask = feature_values >= threshold\n    left = X[mask].tolist()\n    right = X[~mask].tolist()\n    return (left, right)"}
{"task_id": 31, "completion_id": 6, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset based on whether the value of a specified feature\n    is greater than or equal to a given threshold.\n\n    Parameters:\n        X (np.ndarray): The dataset (2D array) to split.\n        feature_i (int): Index of the feature to check.\n        threshold (float): Value against which to compare the feature.\n\n    Returns:\n        Tuple[list]: Two lists containing subsets of the dataset.\n                     - The first list includes samples where feature_i >= threshold.\n                     - The second list includes samples where feature_i < threshold.\n    \"\"\"\n    X = np.array(X)\n    mask = X[:, feature_i] >= threshold\n    return (X[mask].tolist(), X[~mask].tolist())"}
{"task_id": 31, "completion_id": 7, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset into two subsets based on whether the specified feature\n    is greater than or equal to the given threshold.\n\n    Parameters:\n    - X: numpy array representing the dataset\n    - feature_i: int, the index of the feature to split on\n    - threshold: float or int, the threshold value\n\n    Returns:\n    - left_subset: list of samples where feature value >= threshold\n    - right_subset: list of samples where feature value < threshold\n    \"\"\"\n    left_mask = X[:, feature_i] >= threshold\n    right_mask = X[:, feature_i] < threshold\n    left_subset = X[left_mask].tolist()\n    right_subset = X[right_mask].tolist()\n    return (left_subset, right_subset)"}
{"task_id": 31, "completion_id": 8, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset `X` based on the value of a specified feature (`feature_i`) \n    compared to `threshold`.\n    \n    Returns:\n    - left:  List of samples where feature_i >= threshold\n    - right: List of samples where feature_i < threshold\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    mask = X[:, feature_i] >= threshold\n    left = X[mask].tolist()\n    right = X[~mask].tolist()\n    return (left, right)"}
{"task_id": 31, "completion_id": 9, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset based on whether the value of a specified feature \n    is greater than or equal to a given threshold.\n\n    Parameters:\n        X (np.ndarray): The input dataset (2D array).\n        feature_i (int): Index of the feature to evaluate.\n        threshold (float): Threshold value for splitting the dataset.\n\n    Returns:\n        list: A list of two subsets of the dataset.\n              The first subset has samples where feature_i >= threshold.\n              The second subset has the rest.\n    \"\"\"\n    cond = X[:, feature_i] >= threshold\n    left_subset = X[cond].tolist()\n    right_subset = X[~cond].tolist()\n    return [left_subset, right_subset]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    indices = list(range(n_features))\n    powers = []\n    for deg in range(1, degree + 1):\n        for comb in combinations_with_replacement(indices, deg):\n            if sum((1 for c in comb if c >= 0)) <= deg:\n                powers.append(list(comb))\n    poly = np.ones((n_samples, 1))\n    for p in powers:\n        term = 1\n        for idx in p:\n            term = term * X[:, idx].reshape(-1, 1)\n        poly = np.hstack((poly, term))\n    return poly.tolist()"}
{"task_id": 32, "completion_id": 1, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    indices = list(combinations_with_replacement(range(n_features), degree))\n    n_output_features = len(indices)\n    X_poly = np.ndarray((n_samples, n_output_features))\n    for (i, index) in enumerate(indices):\n        X_poly[:, i] = np.prod(X[:, index], axis=1)\n    return X_poly.tolist()"}
{"task_id": 32, "completion_id": 2, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    poly = []\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), d):\n            poly.append(np.prod(X[:, indices], axis=1))\n    return np.column_stack(poly).tolist()"}
{"task_id": 32, "completion_id": 3, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    feature_indices = combinations_with_replacement(range(n_features), degree)\n    polynomials = np.ones((n_samples, 1))\n    for indices in feature_indices:\n        term = 1.0\n        for i in indices:\n            term *= X[:, i].reshape(-1, 1)\n        polynomials = np.hstack((polynomials, term))\n    return polynomials[:, 1:].tolist()"}
{"task_id": 32, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nfrom math import prod\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    polymonial_indices = []\n    for total_degree in range(1, degree + 1):\n        for c in combinations_with_replacement(range(n_features), total_degree):\n            indices = [0] * n_features\n            for index in c:\n                indices[index] += 1\n            polymonial_indices.append(indices)\n    polynomial_array = np.ones((n_samples, 1))\n    for index_tuple in polymonial_indices:\n        terms = X[:, index_tuple] ** index_tuple\n        product = np.prod(terms, axis=1)\n        polynomial_array = np.column_stack((polynomial_array, product))\n    return polynomial_array.tolist()"}
{"task_id": 32, "completion_id": 5, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nfrom collections import defaultdict\ndef polynomial_features(X, degree):\n\n    def get_polynomial_indexes(n_features, degree):\n        indices = set()\n        for total_degree in range(1, degree + 1):\n            for c in combinations_with_replacement(range(n_features), total_degree):\n                count = defaultdict(int)\n                for idx in c:\n                    count[idx] += 1\n                index_tuple = tuple(sorted(count.items()))\n                indices.add(index_tuple)\n        return sorted(indices)\n    (n_samples, n_features) = X.shape\n    index_list = get_polynomial_indexes(n_features, degree)\n    n_polynomials = len(index_list)\n    result = np.ones((n_samples, 1))\n    for index_tuple in index_list:\n        feature_column = np.ones(n_samples)\n        for (feat_idx, deg) in index_tuple:\n            feature_column *= X[:, feat_idx] ** deg\n        result = np.hstack((result, feature_column.reshape(-1, 1)))\n    return result.tolist()"}
{"task_id": 32, "completion_id": 6, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    cols = []\n    for d in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), d):\n            col = np.prod(X[:, combo], axis=1)\n            cols.append(col)\n    poly_X = np.column_stack(cols)\n    return poly_X.tolist()"}
{"task_id": 32, "completion_id": 7, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    feature_indices = np.arange(n_features)\n    indices = []\n    for d in range(1, degree + 1):\n        indices.extend(combinations_with_replacement(feature_indices, d))\n    poly_X = np.empty((n_samples, len(indices)))\n    for (i, idx) in enumerate(indices):\n        poly_X[:, i] = np.prod(X[:, idx], axis=1)\n    return poly_X.tolist()"}
{"task_id": 32, "completion_id": 8, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    feature_indices = combinations_with_replacement(range(n_features), degree)\n    export = np.empty((n_samples, 0))\n    for feature_index in feature_indices:\n        data = 1.0\n        for idx in feature_index:\n            data *= X[:, idx].reshape(-1, 1)\n        export = np.hstack((export, data))\n    return export.tolist()"}
{"task_id": 32, "completion_id": 9, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - degree: integer, degree of the polynomial\n\n    Returns:\n    - 2D numpy array of polynomial features (n_samples, n_polynomial_features) as a Python list\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    feature_indices = np.arange(n_features)\n    feature_powers = []\n    for total_degree in range(1, degree + 1):\n        for combo in combinations_with_replacement(feature_indices, total_degree):\n            if sum(combo) <= degree:\n                feature_powers.append(combo)\n    polynomial_matrix = np.ones((n_samples, 1))\n    for powers in feature_powers:\n        columns = X[:, powers]\n        feature_product = np.prod(columns, axis=1)\n        polynomial_matrix = np.column_stack((polynomial_matrix, feature_product))\n    return polynomial_matrix.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of the given dataset.\n    \n    Parameters:\n    - X (np.ndarray): 2D input matrix of shape (n_samples, n_features).\n    - y (np.ndarray): 1D array of labels of shape (n_samples,).\n    - n_subsets (int): Number of subsets to generate.\n    - replacements (bool): Whether to sample with replacement (bootstrap).\n    - seed (int): Random seed for reproducibility.\n    \n    Returns:\n    - List of tuples: [(X_subset, y_subset), ...] using tolist().\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(x.tolist(), y.tolist()) for (x, y) in subsets]"}
{"task_id": 33, "completion_id": 1, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array representing the features.\n    - y: 1D numpy array representing the labels.\n    - n_subsets: Number of random subsets to generate.\n    - replacements: Boolean indicating if sampling is done with replacement (True) \n      or without replacement (False).\n    - seed: Integer to seed the random number generator for reproducibility.\n\n    Returns:\n    - List of n_subsets, where each subset is a tuple (X_subset, y_subset).\n      The arrays are converted to Python lists using numpy's tolist().\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 2, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of labels.\n    - n_subsets: Number of subsets to generate.\n    - replacements: Boolean indicating whether sampling is with replacement.\n    - seed: Seed for random number generator for reproducibility.\n\n    Returns:\n    - List of tuples, each containing a subset (X_subset, y_subset),     \n      as Python lists after reshaping using tolist().\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 33, "completion_id": 3, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of labels.\n    - n_subsets: Number of random subsets to generate.\n    - replacements: Whether to sample with replacement (True) or without (False).\n    - seed: Random seed for reproducibility.\n\n    Returns:\n    - List of tuples, each containing (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(X_sub.tolist(), y_sub.tolist()) for (X_sub, y_sub) in subsets]"}
{"task_id": 33, "completion_id": 4, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of labels.\n    - n_subsets: Number of subsets to generate.\n    - replacements: Boolean indicating whether sampling should be with replacement.\n    - seed: Seed for random number generator for reproducibility.\n\n    Returns:\n    - A list of n_subsets tuples (X_subset, y_subset), where each is a random subset\n      of the dataset.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(X_sub.tolist(), y_sub.tolist()) for (X_sub, y_sub) in subsets]"}
{"task_id": 33, "completion_id": 5, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of the given dataset.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - y: 1D numpy array of shape (n_samples,)\n    - n_subsets: number of subsets to generate\n    - replacements: boolean indicating whether to sample with replacement\n    - seed: random seed for reproducibility\n\n    Returns:\n    - list of tuples (X_subset, y_subset), n_subsets in total\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 33, "completion_id": 6, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of the dataset.\n\n    Parameters:\n    - X: 2D numpy array, the data samples (shape: [n_samples, n_features])\n    - y: 1D numpy array, the labels (shape: [n_samples])\n    - n_subsets: Number of random subsets to generate\n    - replacements: Boolean, whether to sample data with or without replacement\n    - seed: Random seed for reproducibility\n\n    Returns:\n    - List containing tuples of (X_subset, y_subset) for each subset\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(xs, ys) for (xs, ys) in subsets]"}
{"task_id": 33, "completion_id": 7, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(x.tolist(), y.tolist()) for (x, y) in subsets]"}
{"task_id": 33, "completion_id": 8, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array of features.\n    - y: 1D numpy array of labels.\n    - n_subsets: Number of random subsets to generate.\n    - replacements: Boolean, whether to sample with replacement.\n    - seed: Integer seed for random number generator.\n    \n    Returns:\n    - List of tuples, where each tuple contains a subset (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(x.tolist(), y.tolist()) for (x, y) in subsets]"}
{"task_id": 33, "completion_id": 9, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate n_subsets random subsets of a dataset (X, y).\n\n    Parameters:\n    - X (np.ndarray): 2D input data array of shape (n_samples, n_features)\n    - y (np.ndarray): 1D target array of shape (n_samples,)\n    - n_subsets (int): Number of random subsets to generate\n    - replacements (bool): Whether or not to sample with replacement\n    - seed (int): Random seed for reproducibility\n\n    Returns:\n    - List of tuples, where each tuple is (X_subset, y_subset)\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(X_sub.tolist(), y_sub.tolist()) for (X_sub, y_sub) in subsets]"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integers.\n\n    Parameters:\n    x (np.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns in the resulting one-hot encoded matrix.\n                           If None, it is inferred from the max value in x + 1.\n\n    Returns:\n    list of lists: One-hot encoded matrix as a Python list.\n    \"\"\"\n    x = np.asarray(x)\n    n_row = x.shape[0]\n    if n_col is None:\n        n_col = x.max() + 1\n    elif n_col < x.max() + 1:\n        raise ValueError(f'n_col must be at least {x.max() + 1}, but got {n_col}')\n    one_hot = np.zeros((n_row, n_col))\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 1, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array.\n\n    Parameters:\n        x (np.ndarray): 1D numpy array of integer values.\n        n_col (int, optional): Number of columns for one-hot encoding. \n                               If None, automatically determined from `x`.\n\n    Returns:\n        list: One-hot encoded array as a list.\n    \"\"\"\n    if n_col is None:\n        n_col = len(np.unique(x))\n    y = np.zeros((x.size, n_col))\n    y[np.arange(x.size), x] = 1\n    return y.tolist()"}
{"task_id": 34, "completion_id": 2, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    One-hot encodes a 1D numpy array of integer values.\n    \n    Parameters:\n        x (np.ndarray): 1D array of integer values.\n        n_col (int, optional): The number of columns in the one-hot encoded matrix.\n                              If not provided, it is inferred from the max value in x + 1.\n    \n    Returns:\n        list: A list representation of the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    n_row = len(x)\n    one_hot = np.zeros((n_row, n_col), dtype=np.int8)\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()\nx = [2, 0, 1, 3, 2]"}
{"task_id": 34, "completion_id": 3, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    n_row = x.shape[0]\n    n_col = n_col or int(np.max(x)) + 1\n    categorical = np.zeros((n_row, n_col), dtype=int)\n    categorical[np.arange(n_row), x] = 1\n    return categorical.tolist()\nx = np.array([0, 1, 2, 1, 0, 3])"}
{"task_id": 34, "completion_id": 4, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: 1D numpy array of integer labels.\n    - n_col: Optional; integer representing the number of output columns.\n              If not provided, it is inferred from the maximum value in x.\n    \n    Returns:\n    - A list of lists representing the one-hot encoded matrix.\n    \"\"\"\n    x = np.asarray(x).flatten()\n    if n_col is None:\n        n_col = np.max(x) + 1\n    n_row = x.shape[0]\n    one_hot = np.zeros((n_row, n_col), dtype=int)\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 5, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    x = np.array(x)\n    if n_col is None:\n        n_col = np.max(x) + 1\n    n_row = x.shape[0]\n    one_hot = np.zeros((n_row, n_col))\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 6, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    x = np.asarray(x)\n    if n_col is None:\n        n_col = np.max(x) + 1\n    eye_matrix = np.eye(n_col, dtype=int)\n    one_hot = eye_matrix[x]\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 7, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D array of integer values into a one-hot encoded matrix.\n\n    Parameters:\n    x (np.ndarray): A 1D numpy array of integers.\n    n_col (int, optional): The number of columns in the output (number of unique categories).\n                           Defaults to the maximum integer in `x` + 1.\n\n    Returns:\n    list: A list representation of the one-hot encoded matrix.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    n_row = x.shape[0]\n    one_hot = np.zeros((n_row, n_col), dtype=np.int32)\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 8, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integer values into a one-hot encoded\n    2D numpy array, returned as a Python list.\n    \n    Parameters:\n        x (np.ndarray): 1D array of integer values\n        n_col (int): Optional; number of output columns. If not provided,\n                     it will be determined from the input values.\n    \n    Returns:\n        list: One-hot encoded array as a Python list\n    \"\"\"\n    x = np.array(x, dtype=int)\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col, dtype=int)[x]\n    return one_hot.tolist()"}
{"task_id": 34, "completion_id": 9, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = x.max() + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    x = np.asarray(x).ravel()\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 1, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix, using numpy's tolist() method.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 2, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing a diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 3, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3, 4])"}
{"task_id": 35, "completion_id": 4, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix and returns it as a Python list.\n\n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    x = np.asarray(x).ravel()\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 5, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 6, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n    \n    Returns:\n    list: A 2D python list representation of the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 35, "completion_id": 7, "solution": "import numpy as np\ndef make_diagonal(x):\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()"}
{"task_id": 35, "completion_id": 8, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 35, "completion_id": 9, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true labels and predicted labels.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 1, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Compute the accuracy score between true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must have the same length.')\n    correct = np.sum(y_true == y_pred)\n    accuracy = correct / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 2, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true labels and predicted labels.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).ravel()\n    y_pred = np.asarray(y_pred).ravel()\n    correct = np.sum(y_true == y_pred)\n    total = y_true.shape[0]\n    accuracy = round(correct / total, 4)\n    return accuracy"}
{"task_id": 36, "completion_id": 3, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score from true labels and predicted labels.\n\n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    acc = correct / total\n    return round(acc, 4)"}
{"task_id": 36, "completion_id": 4, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between the true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must be of the same length.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 5, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Length of y_true and y_pred must be the same.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 6, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must be of the same length.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 7, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): 1D array of true labels.\n    y_pred (np.array): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 8, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): A 1D array of true labels.\n    y_pred (np.ndarray): A 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if len(y_true.shape) != 1 or len(y_pred.shape) != 1:\n        raise ValueError('Input arrays must be 1D.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 36, "completion_id": 9, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for the input data.\n\n    Parameters:\n    - X: 2D numpy array (n_samples x n_features).\n    - Y: Optional 2D numpy array to compute cross-correlation with X.\n\n    Returns:\n    - Correlation matrix as a 2D list (rounded to 4 decimal places).\n    \"\"\"\n    if Y is None:\n        dataset = X\n    else:\n        dataset = np.hstack((X, Y)) if X.ndim == 1 or Y.ndim == 1 else np.column_stack((X, Y))\n    mean = np.mean(dataset, axis=0)\n    centered = dataset - mean\n    std = np.std(dataset, axis=0)\n    std[std == 0] = 1e-10\n    correlation = centered.T @ centered / (dataset.shape[0] - 1)\n    std_matrix = np.outer(std, std)\n    correlation /= std_matrix\n    return np.round(correlation, 4).tolist()"}
{"task_id": 37, "completion_id": 1, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    covariance = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0, ddof=1)\n    std_Y = np.std(Y, axis=0, ddof=1)\n    corr = covariance / np.outer(std_X, std_Y)\n    return np.round(corr, 4).tolist()"}
{"task_id": 37, "completion_id": 2, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y.\n    \n    Parameters:\n    X (np.ndarray): A 2D numpy array representing the first dataset (shape: [n_samples, n_features]).\n    Y (np.ndarray, optional): A 2D numpy array representing the second dataset (if provided, shape: [n_samples, m_features]).\n    \n    Returns:\n    list: The correlation matrix as a Python list (rounded to 4 decimal places).\n    \"\"\"\n    if Y is None:\n        Y = X\n    mean_x = np.mean(X, axis=0)\n    mean_y = np.mean(Y, axis=0)\n    X_centered = X - mean_x\n    Y_centered = Y - mean_y\n    cov_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    std_dev_x = np.std(X, axis=0, ddof=1)\n    std_dev_y = np.std(Y, axis=0, ddof=1)\n    std_dev_x[std_dev_x == 0] = 1e-10\n    std_dev_y[std_dev_y == 0] = 1e-10\n    correlation_matrix = cov_matrix / np.outer(std_dev_x, std_dev_y)\n    return np.round(correlation_matrix, 4).tolist()"}
{"task_id": 37, "completion_id": 3, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between the columns of X and optionally Y.\n    \n    Parameters:\n        X (np.ndarray): A 2D numpy array.\n        Y (np.ndarray, optional): A second 2D numpy array. Defaults to None.\n    \n    Returns:\n        np.ndarray: The correlation matrix as a 2D numpy array, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X = X.astype(float)\n    Y = Y.astype(float)\n    X_mean = X - np.mean(X, axis=0)\n    Y_mean = Y - np.mean(Y, axis=0)\n    covariance = X_mean.T @ Y_mean / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    diag_product = np.outer(std_X, std_Y)\n    diag_product[diag_product == 0] = 1e-10\n    correlation = covariance / diag_product\n    return np.round(correlation, 4).tolist()"}
{"task_id": 37, "completion_id": 4, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y (or between the features in X if Y is None).\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array (n_samples x n_features)\n    Y (np.ndarray, optional): 2D numpy array (n_samples x m_features). If None, computes correlation between features of X.\n    \n    Returns:\n    list: Correlation matrix as a Python list rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        cov_matrix = np.cov(X, rowvar=False)\n        stds = np.sqrt(np.diag(cov_matrix))\n        correlation_matrix = cov_matrix / np.outer(stds, stds)\n    else:\n        cov_matrix = np.cov(X, Y, rowvar=False)\n        n_features_X = X.shape[1]\n        n_features_Y = Y.shape[1]\n        stds_X = np.sqrt(np.diag(cov_matrix[:n_features_X, :n_features_X]))\n        stds_Y = np.sqrt(np.diag(cov_matrix[n_features_X:, n_features_X:]))\n        stds = np.concatenate((stds_X, stds_Y))\n        correlation_matrix = cov_matrix / np.outer(stds, stds)\n    return np.round(correlation_matrix, 4).tolist()"}
{"task_id": 37, "completion_id": 5, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between arrays X (and Y if provided).\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features_X)\n    - Y: Optional 2D numpy array of shape (n_samples, n_features_Y)\n         If not provided, correlation is calculated between features of X.\n    \n    Returns:\n    - Correlation matrix as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        covariance = np.cov(X, rowvar=False)\n        std = np.std(X, axis=0, ddof=1)\n    else:\n        XX = np.hstack((X, Y))\n        covariance = np.cov(XX, rowvar=False)\n        std = np.std(XX, axis=0, ddof=1)\n        std_X = std[:X.shape[1]]\n        std_Y = std[X.shape[1]:]\n        std = np.hstack((std_X, std_Y))\n    ones_matrix = np.ones_like(covariance)\n    correlation = covariance / np.outer(std, std)\n    correlation[np.isnan(correlation)] = 0\n    return np.round(correlation, 4).tolist()"}
{"task_id": 37, "completion_id": 6, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y.\n    \n    Parameters:\n    - X: A 2D NumPy array (n_samples, n_features_X)\n    - Y: A 2D NumPy array (n_samples, n_features_Y) or None\n    \n    Returns:\n    - A correlation matrix as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    corr_matrix = np.corrcoef(X, Y)\n    return np.round(corr_matrix, 4).tolist()"}
{"task_id": 37, "completion_id": 7, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y.\n    If Y is not provided, the correlation is computed between the rows/columns of X itself.\n    \n    Parameters:\n    X (np.ndarray): A 2D numpy array.\n    Y (np.ndarray, optional): A 2D numpy array. If not provided, defaults to X.\n\n    Returns:\n    list: Correlation matrix as a 2D list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_mean = X - np.mean(X, axis=0)\n    Y_mean = Y - np.mean(Y, axis=0)\n    numerator = np.dot(X_mean.T, Y_mean)\n    std_X = np.std(X, axis=0, ddof=0)\n    std_Y = np.std(Y, axis=0, ddof=0)\n    corr_matrix = numerator / np.outer(std_X, std_Y)\n    return np.around(corr_matrix, decimals=4).tolist()"}
{"task_id": 37, "completion_id": 8, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix between X and Y (or X with itself if Y is None).\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - Y: Optional 2D numpy array of shape (n_samples, m_features), default is None\n\n    Returns:\n    - Correlation matrix as a 2D list with values rounded to 4 decimal places\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_mean = X - np.mean(X, axis=0)\n    Y_mean = Y - np.mean(Y, axis=0)\n    numerator = np.dot(X_mean.T, Y_mean)\n    denominator = np.outer(np.std(X, axis=0), np.std(Y, axis=0))\n    correlation_matrix = np.divide(numerator, denominator, where=denominator != 0)\n    return np.around(correlation_matrix, 4).tolist()"}
{"task_id": 37, "completion_id": 9, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    std_X_grid = std_X.reshape(1, -1)\n    std_Y_grid = std_Y.reshape(1, -1)\n    correlation_matrix = cov_matrix / np.dot(std_X_grid.T, std_Y_grid)\n    return np.round(correlation_matrix, 4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    D = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_theta = None\n        best_direction = None\n        best_alpha = None\n        for feat in range(n_features):\n            unique_vals = np.unique(X[:, feat])\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2\n            for theta in thresholds:\n                for direction in [-1, 1]:\n                    predictions = np.where(X[:, feat] * direction >= theta * direction, 1, -1)\n                    error = D[y != predictions].sum()\n                    if error > 0.5:\n                        continue\n                    alpha = 0.5 * np.log((1 - error) / error)\n                    if error < best_error:\n                        best_error = error\n                        best_theta = theta\n                        best_direction = direction\n                        best_alpha = alpha\n        feat_idx = int(X[:, 0] * best_direction >= best_theta * best_direction)\n        predictions = np.where(X[:, int(feat_idx)] * best_direction >= best_theta * best_direction, 1, -1)\n        D *= np.exp(-best_alpha * y * predictions)\n        D /= D.sum()\n        classifiers.append({'feature_idx': int(np.where(X[0] == X[0, 0])[0][0]), 'theta': round(best_theta, 4), 'direction': best_direction, 'alpha': round(best_alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 1, "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_threshold = None\n        best_feature_index = None\n        best_direction = None\n        best_prediction = np.zeros(n_samples)\n        for feature_index in range(n_features):\n            thresholds = np.unique(X[:, feature_index])\n            for threshold in thresholds:\n                prediction = np.zeros(n_samples)\n                prediction[X[:, feature_index] <= threshold] = 1\n                error = 0\n                for i in range(n_samples):\n                    if prediction[i] != y[i]:\n                        error += w[i]\n                prediction = 1 - prediction\n                if error > 0.5:\n                    error = 1 - error\n                    prediction = 1 - prediction\n                if error < best_error:\n                    best_error = error\n                    best_threshold = threshold\n                    best_feature_index = feature_index\n                    best_prediction = prediction\n                    best_direction = '>=' if error > 0.5 else '<='\n        if best_error == 0:\n            best_error = 1e-10\n        elif best_error >= 0.5:\n            break\n        epsilon = best_error\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        w *= np.exp(-alpha * y * best_prediction)\n        w /= np.sum(w)\n        classifier = {'feature_index': best_feature_index, 'threshold': round(best_threshold, 4), 'direction': best_direction, 'alpha': round(alpha, 4)}\n        classifiers.append(classifier)\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([1, 1, -1, -1, -1])\nn_clf = 2"}
{"task_id": 38, "completion_id": 2, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_threshold = 0\n        best_feature = 0\n        best_direction = 1\n        for feature in range(n_features):\n            sorted_indices = np.argsort(X[:, feature])\n            sorted_X = X[sorted_indices, feature]\n            sorted_y = y[sorted_indices]\n            weight_cpy = weights.copy()\n            weight_cpy[sorted_indices] = weights[sorted_indices]\n            weight_sum = np.cumsum(weight_cpy)\n            for i in range(n_samples):\n                threshold = sorted_X[i]\n                error = weight_sum[i]\n                if error < best_error:\n                    best_error = error\n                    best_threshold = threshold\n                    best_feature = feature\n                    best_direction = 1\n                error = weight_sum[-1] - weight_sum[i]\n                if error < best_error:\n                    best_error = error\n                    best_threshold = threshold\n                    best_feature = feature\n                    best_direction = -1\n        epsilon = best_error\n        epsilon = max(epsilon, 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        predictions = []\n        for j in range(n_samples):\n            if best_direction == 1 and X[j, best_feature] >= best_threshold:\n                predictions.append(1)\n            elif best_direction == -1 and X[j, best_feature] <= best_threshold:\n                predictions.append(1)\n            else:\n                predictions.append(-1)\n        predictions = np.array(predictions)\n        weights *= np.exp(-alpha * y * predictions)\n        weights /= np.sum(weights)\n        classifiers.append({'feature': round(best_feature, 4), 'threshold': round(best_threshold, 4), 'direction': best_direction, 'alpha': round(alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 3, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for i in range(n_clf):\n        best_error = float('inf')\n        best_thresh = None\n        best_feature = None\n        best_pred = None\n        best_direction = None\n        for feature in range(n_features):\n            unique_vals = np.unique(X[:, feature])\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2\n            for thresh in thresholds:\n                for direction in [1, -1]:\n                    y_pred = np.where(X[:, feature] <= thresh if direction == 1 else X[:, feature] > thresh, 1, -1)\n                    weighted_error = np.sum(weights[y != y_pred])\n                    if weighted_error < best_error:\n                        best_error = weighted_error\n                        best_thresh = thresh\n                        best_feature = feature\n                        best_pred = y_pred\n                        best_direction = 1 if direction == 1 else -1\n        epsilon = max(best_error, 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        alpha = round(alpha, 4)\n        best_error = round(best_error, 4)\n        weights = weights * np.exp(-alpha * y * best_pred)\n        weights = weights / np.sum(weights)\n        classifiers.append({'feature': best_feature, 'threshold': round(best_thresh, 4), 'alpha': alpha, 'direction': best_direction})\n    return classifiers"}
{"task_id": 38, "completion_id": 4, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_rule = None\n        best_pred = None\n        for feat_idx in range(n_features):\n            sorted_indices = np.argsort(X[:, feat_idx])\n            sorted_X = X[sorted_indices, feat_idx]\n            sorted_y = y[sorted_indices]\n            sorted_w = w[sorted_indices]\n            thresholds = np.unique(sorted_X)\n            thresholds = np.insert(thresholds, 0, -np.inf)\n            thresholds = np.append(thresholds, np.inf)\n            for t in thresholds:\n                pred = np.where(X[:, feat_idx] <= t, 1, -1)\n                error = np.sum(w[y != pred])\n                if error > 0.5:\n                    error = 1 - error\n                    pred = -pred\n                if error < best_error:\n                    best_error = error\n                    best_rule = (feat_idx, t, 1 if error == best_error else -1)\n                    best_pred = pred\n        epsilon = best_error\n        alpha = 0.5 * np.log((1 - epsilon + 1e-10) / (epsilon + 1e-10))\n        alpha = round(alpha, 4)\n        (feat_idx, threshold, direction) = best_rule\n        pred = best_pred\n        w *= np.exp(-alpha * y * pred)\n        w /= np.sum(w)\n        classifier = {'feature': best_rule[0], 'threshold': round(best_rule[1], 4), 'direction': best_rule[2], 'alpha': alpha}\n        classifiers.append(classifier)\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 1], [4, 4]])\ny = np.array([1, -1, 1, -1])\nn_clf = 2\nclassifiers = adaboost_fit(X, y, n_clf)"}
{"task_id": 38, "completion_id": 5, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_rule = None\n        for feature_idx in range(n_features):\n            x = X[:, feature_idx]\n            unique_values = np.unique(x)\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for threshold in sorted(thresholds):\n                predictions = np.where(x <= threshold, -1, 1)\n                errors = np.sum(sample_weights[y != predictions])\n                if errors > 0.5:\n                    predictions = -predictions\n                    errors = np.sum(sample_weights[y != predictions])\n                if errors < best_error:\n                    best_error = errors\n                    direction = -1 if (predictions != np.where(x <= threshold, -1, 1)).all() else 1\n                    best_rule = {'feature': feature_idx, 'threshold': threshold, 'direction': direction, 'error': errors}\n        epsilon = best_error\n        if epsilon == 0:\n            alpha = float('inf')\n        else:\n            alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        predictions = np.where(X[:, best_rule['feature']] <= best_rule['threshold'], best_rule['direction'] * -1, best_rule['direction'] * 1)\n        sample_weights *= np.exp(-alpha * y * predictions)\n        sample_weights /= np.sum(sample_weights)\n        best_rule['alpha'] = round(alpha, 4)\n        classifiers.append(best_rule)\n    for clf in classifiers:\n        for key in ['threshold', 'alpha']:\n            clf[key] = round(clf[key], 4)\n    return classifiers\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([-1, -1, 1, 1])\nclassifiers = adaboost_fit(X, y, n_clf=2)"}
{"task_id": 38, "completion_id": 6, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    clfs = []\n    for i in range(n_clf):\n        best_error = float('inf')\n        best_rule = None\n        for feature in range(n_features):\n            values = X[:, feature]\n            unique_values = np.unique(values)\n            thresholds = (unique_values[1:] + unique_values[:-1]) / 2\n            for threshold in thresholds:\n                for direction in [1, -1]:\n                    prediction = np.ones(n_samples)\n                    if direction == 1:\n                        prediction[values <= threshold] = -1\n                    else:\n                        prediction[values > threshold] = -1\n                    error = np.sum(weights[y != prediction])\n                    if error > 0.5:\n                        error = 1 - error\n                        prediction *= -1\n                    if error < best_error:\n                        best_error = error\n                        best_rule = {'feature': feature, 'threshold': threshold, 'direction': direction, 'error': error, 'prediction': prediction}\n        error = best_rule['error']\n        epsilon = max(1e-10, error)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        best_rule['alpha'] = alpha\n        prediction = best_rule['prediction']\n        weights *= np.exp(-alpha * y * prediction)\n        weights /= np.sum(weights)\n        clfs.append(best_rule)\n    for clf in clfs:\n        clf['threshold'] = round(clf['threshold'], 4)\n        clf['error'] = round(clf['error'], 4)\n        clf['alpha'] = round(clf['alpha'], 4)\n    return clfs"}
{"task_id": 38, "completion_id": 7, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    y = np.where(y == 0, -1, y)\n    classifiers = []\n    weights = np.full(n_samples, 1 / n_samples)\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_model = None\n        best_threshold = None\n        best_feature = None\n        for feature in range(n_features):\n            unique_vals = np.sort(np.unique(X[:, feature])).tolist()\n            thresholds = [(unique_vals[i] + unique_vals[i + 1]) / 2 for i in range(len(unique_vals) - 1)]\n            for threshold in thresholds:\n                prediction = np.where(X[:, feature] <= threshold, 1, -1)\n                errors = weights * (prediction != y)\n                error = np.sum(errors)\n                if abs(error < best_error):\n                    best_error = error\n                    best_threshold = threshold\n                    best_model = 'stump'\n                    best_feature = feature\n        if best_error == 0.0:\n            best_error = 1e-10\n        elif best_error == 0.5:\n            break\n        epsilon = best_error\n        alpha = (math.log(1 - epsilon) - math.log(epsilon)) / 2\n        prediction = np.where(X[:, best_feature] <= best_threshold, 1, -1)\n        weights *= np.exp(-alpha * y * prediction)\n        weights /= np.sum(weights)\n        classifier = {'feature': best_feature, 'threshold': best_threshold, 'alpha': float(np.round(alpha, 4)), 'direction': 1 if best_threshold is not None else 0}\n        classifiers.append(classifier)\n    result = []\n    for clf in classifiers:\n        result.append({'feature': int(clf['feature']), 'threshold': float(np.round(clf['threshold'], 4)), 'alpha': float(np.round(clf['alpha'], 4)), 'direction': 1})\n    return result"}
{"task_id": 38, "completion_id": 8, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    D = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = math.inf\n        best_rule = None\n        best_alpha = 0\n        best_threshold = 0\n        best_polarity = 0\n        for feat_idx in range(n_features):\n            x = X[:, feat_idx]\n            unique_values = np.unique(x)\n            for threshold in unique_values:\n                for polarity in [1, -1]:\n                    prediction = polarity * (x > threshold).astype(int)\n                    error = np.dot(D, (prediction != y).astype(float))\n                    if error > 0.5:\n                        prediction = -polarity * prediction\n                        error = np.dot(D, (prediction != y).astype(float))\n                    if error < best_error:\n                        best_error = error\n                        best_polarity = polarity\n                        best_threshold = threshold\n                        best_rule = (feat_idx, threshold, best_polarity)\n                        best_alpha = 0.5 * math.log((1 - best_error) / best_error)\n        predictions = best_polarity * (X[:, best_rule[0]] > best_rule[1]).astype(int)\n        D *= np.exp(-best_alpha * y * predictions)\n        D /= D.sum()\n        classifiers.append({'feature': best_rule[0], 'threshold': round(best_threshold, 4), 'polarity': best_polarity, 'alpha': round(best_alpha, 4)})\n    return classifiers"}
{"task_id": 38, "completion_id": 9, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_alpha = 0\n        best_feature = -1\n        best_threshold = 0\n        best_direction = 0\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            unique_values = np.unique(feature_values)\n            for threshold in unique_values:\n                for direction in [0, 1]:\n                    predictions = np.ones(n_samples)\n                    if direction == 0:\n                        predictions[feature_values <= threshold] = -1\n                    else:\n                        predictions[feature_values > threshold] = -1\n                    error = np.sum(sample_weights[y != predictions])\n                    if error < best_error and error > 0 and (error < 1):\n                        best_error = error\n                        best_feature = feature_idx\n                        best_threshold = threshold\n                        best_direction = direction\n                        best_alpha = 0.5 * np.log((1 - best_error) / best_error)\n        feature_values = X[:, best_feature]\n        predictions = np.ones(n_samples)\n        if best_direction == 0:\n            predictions[feature_values <= best_threshold] = -1\n        else:\n            predictions[feature_values > best_threshold] = -1\n        epsilon = 1e-10\n        sample_weights *= np.exp(-best_alpha * y * predictions + epsilon)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append({'feature_idx': best_feature, 'threshold': round(best_threshold, 4), 'direction': best_direction, 'alpha': round(best_alpha, 4)})\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    scores_shifted = scores - np.max(scores)\n    log_softmax_values = scores_shifted - np.log(np.sum(np.exp(scores_shifted)))\n    return np.around(log_softmax_values, 4).tolist()"}
{"task_id": 39, "completion_id": 1, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    shift_scores = scores - np.max(scores)\n    exp_scores = np.exp(shift_scores)\n    sum_exp = np.sum(exp_scores)\n    log_softmax_result = shift_scores - np.log(sum_exp)\n    return np.round(log_softmax_result, 4).tolist()"}
{"task_id": 39, "completion_id": 2, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    max_score = np.max(scores)\n    exps = np.exp(scores - max_score)\n    sum_exps = np.sum(exps)\n    log_softmax_output = np.log(exps / sum_exps)\n    return [round(x, 4) for x in log_softmax_output.tolist()]"}
{"task_id": 39, "completion_id": 3, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a 1D array of scores for numerical stability.\n    \n    Parameters:\n    - scores (list): A list of numerical values (scores or logits).\n    \n    Returns:\n    - Python list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores_arr = np.array(scores, dtype=np.float64)\n    scores_arr -= np.max(scores_arr)\n    exp_scores = np.exp(scores_arr)\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    log_softmax_scores = np.log(softmax_scores)\n    return np.round(log_softmax_scores, 4).tolist()"}
{"task_id": 39, "completion_id": 4, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    max_score = np.max(scores)\n    log_denominator = np.log(np.sum(np.exp(scores - max_score)))\n    log_softmax_values = scores - max_score - log_denominator\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 39, "completion_id": 5, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D array of scores.\n    \n    Args:\n        scores (list): A 1D list of numerical values (scores).\n        \n    Returns:\n        list: A 1D list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores, dtype=np.float64)\n    scores_shifted = scores - np.max(scores)\n    log_softmax_values = scores_shifted - np.log(np.sum(np.exp(scores_shifted)))\n    return np.around(log_softmax_values, decimals=4).tolist()"}
{"task_id": 39, "completion_id": 6, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a 1D list of scores.\n    \n    Parameters:\n    scores (list): 1D list of numerical values (scores).\n    \n    Returns:\n    list: Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    scores_array = np.array(scores, dtype=np.float64)\n    scores_shifted = scores_array - np.max(scores_array)\n    exp_sum = np.sum(np.exp(scores_shifted))\n    log_softmax_values = scores_shifted - np.log(exp_sum)\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 39, "completion_id": 7, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a 1D array of scores.\n    \n    Parameters:\n    - scores (list): A 1D list of numeric scores.\n    \n    Returns:\n    - list: The log-softmax values rounded to the nearest 4th decimal.\n    \"\"\"\n    scores = np.array(scores, dtype=np.float64)\n    max_score = np.max(scores)\n    stable_scores = scores - max_score\n    exp_scores = np.exp(stable_scores)\n    log_softmax_result = stable_scores - np.log(np.sum(exp_scores))\n    return np.round(log_softmax_result, 4).tolist()"}
{"task_id": 39, "completion_id": 8, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a 1D list of scores.\n    \n    Args:\n        scores (list): 1D list of numerical scores.\n        \n    Returns:\n        list: Log-softmax values rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores, dtype=np.float64)\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)\n    sum_exp = np.sum(exp_scores)\n    log_softmax_values = np.log(exp_scores / sum_exp)\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 39, "completion_id": 9, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    shift_scores = scores - np.max(scores)\n    exp_scores = np.exp(shift_scores)\n    log_sum_exp = np.log(np.sum(exp_scores))\n    log_softmax_values = shift_scores - log_sum_exp\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer=None):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.W_opt = copy.copy(optimizer)\n        self.w0_opt = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        grad_input = np.dot(accum_grad, W.T)\n        if self.trainable:\n            grad_W = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0)\n            self.W = self.W_opt.update(self.W, grad_W)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 1, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.param_optimizers = {}\n\n    def initialize(self, optimizer=None):\n        fan_in = self.input_shape[0]\n        limit = 1.0 / math.sqrt(fan_in)\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        if optimizer is not None:\n            for param in ['W', 'w0']:\n                self.param_optimizers[param] = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        in_grad = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            W_grad = np.dot(self.layer_input.T, accum_grad)\n            w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n            for param in self.param_optimizers:\n                if param == 'W':\n                    param_grad = W_grad\n                elif param == 'w0':\n                    param_grad = w0_grad\n                if param in self.param_optimizers:\n                    self.param_optimizers[param].update_rule(param, self, param_grad)\n        return in_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 2, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizers = {}\n\n    def initialize(self, optimizer):\n        if self.input_shape is None:\n            raise ValueError('Input shape must be provided for initialization.')\n        in_dim = self.input_shape[0]\n        limit = 1.0 / math.sqrt(in_dim)\n        self.W = np.random.uniform(-limit, limit, size=(in_dim, self.n_units)).round(4)\n        self.w0 = np.zeros((1, self.n_units)).round(4)\n        self.optimizers[self.W] = copy.copy(optimizer)\n        self.optimizers[self.w0] = copy.copy(optimizer)\n\n    def parameters(self):\n        if self.W is None or self.w0 is None:\n            raise ValueError('Weights and biases must be initialized before counting parameters.')\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output.round(4)\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        if accum_grad.ndim == 1:\n            accum_grad = accum_grad.reshape(1, -1)\n        input_grad = np.dot(accum_grad, self.W.T).round(4)\n        if self.trainable:\n            dW = np.dot(X.T, accum_grad).round(4)\n            dw0 = np.sum(accum_grad, axis=0, keepdims=True).round(4)\n            self.optimizers[self.W].update(self.W, dW)\n            self.optimizers[self.w0].update(self.w0, dw0)\n        return input_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 3, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        if not hasattr(self, 'W'):\n            raise ValueError('Weights not initialized. Call initialize() first.')\n        self.layer_input = X\n        output = X.dot(self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        grad_input = accum_grad.dot(self.W.T)\n        if self.trainable:\n            grad_W = X.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0)\n            self.W = self.optimizer_W.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 4, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer=None):\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n        weight_limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-weight_limit, weight_limit, size=(self.input_shape[0], self.n_units)).round(4)\n        self.w0 = np.zeros((1, self.n_units)).round(4)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = (np.dot(X, self.W) + self.w0).round(4)\n        return output.tolist()\n\n    def backward_pass(self, accum_grad):\n        W_grad = np.dot(self.layer_input.T, accum_grad).round(4)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True).round(4)\n        input_grad = np.dot(accum_grad, self.W.T).round(4)\n        if self.trainable:\n            self.W = self.optimizer_W.update_params(self.W, W_grad)\n            self.w0 = self.optimizer_w0.update_params(self.w0, w0_grad)\n        return input_grad.tolist()\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 5, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        input_size = self.input_shape[0]\n        limit = 1.0 / math.sqrt(input_size)\n        self.W = np.random.uniform(-limit, limit, size=(input_size, self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer_W = optimizer\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        if self.trainable:\n            grad_W = np.dot(X.T, accum_grad)\n            grad_b = np.sum(accum_grad, axis=0)\n            self.W = self.optimizer_W.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_b)\n        grad_input = np.dot(accum_grad, self.W.T)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 6, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizers = {}\n\n    def initialize(self, optimizers):\n        self.optimizers = optimizers\n        input_dim = self.input_shape[0]\n        limit = 1 / math.sqrt(input_dim)\n        self.W = np.random.uniform(-limit, limit, size=(input_dim, self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizers['W'] = copy.copy(optimizers['default'])\n        self.optimizers['w0'] = copy.copy(optimizers['default'])\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        grad_W = np.dot(X.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimizers['W'].update(self.W, grad_W)\n            self.w0 = self.optimizers['w0'].update(self.w0, grad_w0)\n        accum_grad = np.dot(accum_grad, self.W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 7, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizers = {}\n\n    def initialize(self, optimizer_name='sgd'):\n        \"\"\"Initialize weights and biases using uniform distribution and set up optimizers.\"\"\"\n        input_dim = self.input_shape[0] if self.input_shape else self.input_dim\n        limit = 1 / math.sqrt(input_dim)\n        self.W = np.random.uniform(-limit, limit, size=(input_dim, self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizers['W'] = self._create_optimizer(optimizer_name, self.W)\n        self.optimizers['w0'] = self._create_optimizer(optimizer_name, self.w0)\n\n    def _create_optimizer(self, optimizer_name, param):\n        if optimizer_name == 'sgd':\n            return {'method': 'sgd', 'param': copy.deepcopy(param)}\n        else:\n            return {'method': 'sgd', 'param': copy.deepcopy(param)}\n\n    def parameters(self):\n        \"\"\"Return the number of trainable parameters in the layer.\"\"\"\n        if self.W is None or self.w0 is None:\n            return 0\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        \"\"\"Compute the output of the layer.\"\"\"\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        \"\"\"Calculate gradients and update weights and biases if training.\"\"\"\n        X = self.layer_input\n        grad_weights = np.dot(X.T, accum_grad)\n        grad_biases = np.sum(accum_grad, axis=0, keepdims=True)\n        grad_input = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            self._update_weights(grad_weights)\n            self._update_weights(grad_biases, param_name='w0')\n        return grad_input\n\n    def _update_weights(self, grad, param_name='W'):\n        optimizer = self.optimizers[param_name]\n        optimizer['param'] -= grad\n        if param_name == 'W':\n            self.W = optimizer['param']\n        elif param_name == 'w0':\n            self.w0 = optimizer['param']\n\n    def output_shape(self):\n        \"\"\"Return the output shape of the layer.\"\"\"\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 8, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n        self.param_names = ['W', 'w0']\n\n    def initialize(self, optimizer):\n        if self.input_shape is None:\n            raise ValueError('input_shape is required when initializing Dense layer.')\n        in_size = self.input_shape[0]\n        limit = 1 / math.sqrt(in_size)\n        self.W = np.random.uniform(-limit, limit, size=(in_size, self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer = optimizer\n        if hasattr(self.optimizer, 'initialize'):\n            params = {'W': self.W, 'w0': self.w0}\n            self.optimizer.initialize(params)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        in_grad = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            w_grad = np.dot(X.T, accum_grad)\n            w0_grad = np.sum(accum_grad, axis=0)\n            self.W = self.optimizer.update(self.W, w_grad)\n            self.w0 = self.optimizer.update(self.w0, w0_grad)\n        return in_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 40, "completion_id": 9, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        if self.input_shape is None:\n            raise ValueError('Input shape should be defined for the first layer')\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        grad_W = np.dot(X.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        grad_prev = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            self.W = self.optimizer.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        return grad_prev\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    2D Convolution operation on input_matrix using kernel, with optional padding and stride.\n    \n    Args:\n        input_matrix (np.ndarray): Input 2D matrix.\n        kernel (np.ndarray): Convolutional kernel (also 2D matrix with odd-sized dimensions).\n        padding (int): Number of zero-padding layers to apply on each side of input_matrix.\n        stride (int): Stride to slide the kernel across the input.\n        \n    Returns:\n        np.ndarray: Resulting 2D feature map after convolution.\n    \"\"\"\n    (H, W) = input_matrix.shape\n    (K_h, K_w) = kernel.shape\n    out_h = (H + 2 * padding - K_h) // stride + 1\n    out_w = (W + 2 * padding - K_w) // stride + 1\n    if out_h <= 0 or out_w <= 0:\n        raise ValueError('Invalid configuration: output dimensions must be positive.')\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output = np.zeros((out_h, out_w))\n    for i in range(out_h):\n        for j in range(out_w):\n            h_start = i * stride\n            w_start = j * stride\n            region = padded_input[h_start:h_start + K_h, w_start:w_start + K_w]\n            output[i, j] = np.sum(region * kernel)\n    return np.around(output, decimals=4).tolist()"}
{"task_id": 41, "completion_id": 1, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    if input_matrix.ndim != 2:\n        raise ValueError('Input matrix must be 2-dimensional.')\n    if kernel.ndim != 2:\n        raise ValueError('Kernel must be 2-dimensional.')\n    (in_height, in_width) = input_matrix.shape\n    (k_height, k_width) = kernel.shape\n    input_padded = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    out_height = (in_height + 2 * padding - k_height) // stride + 1\n    out_width = (in_width + 2 * padding - k_width) // stride + 1\n    if out_height <= 0 or out_width <= 0:\n        raise ValueError('Invalid combination of input size, kernel size, padding, or stride.')\n    output = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            start_i = i * stride\n            start_j = j * stride\n            region = input_padded[start_i:start_i + k_height, start_j:start_j + k_width]\n            if region.shape != kernel.shape:\n                raise ValueError('Sliding window must match kernel size after padding.')\n            output[i, j] = np.sum(region * kernel)\n    output = np.around(output, decimals=4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 2, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution operation on the input_matrix using the kernel.\n    \n    Parameters:\n    - input_matrix: Input 2D array (H x W)\n    - kernel: Convolutional kernel (kH x kW)\n    - padding: Number of zero-padding layers to apply\n    - stride: Step size for kernel movement\n    \n    Returns:\n    - Output of the convolution as a Python list, after rounding to 4 decimal places\n    \"\"\"\n    (height, width) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    input_padded = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_h = (height + 2 * padding - kernel_h) // stride + 1\n    output_w = (width + 2 * padding - kernel_w) // stride + 1\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            roi = input_padded[i * stride:i * stride + kernel_h, j * stride:j * stride + kernel_w]\n            output[i, j] = np.sum(np.multiply(roi, kernel))\n    return np.round(output, 4).tolist()"}
{"task_id": 41, "completion_id": 3, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            start_i = i * stride\n            start_j = j * stride\n            region = padded_input[start_i:start_i + kernel_h, start_j:start_j + kernel_w]\n            output[i, j] = np.sum(region * kernel)\n    output = np.around(output, decimals=4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 4, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (h_in, w_in) = input_matrix.shape\n    (k_h, k_w) = kernel.shape\n    input_padded = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    (h_padded, w_padded) = input_padded.shape\n    h_out = (h_padded - k_h) // stride + 1\n    w_out = (w_padded - k_w) // stride + 1\n    output = np.zeros((h_out, w_out), dtype=np.float32)\n    for i in range(h_out):\n        for j in range(w_out):\n            h_start = i * stride\n            w_start = j * stride\n            window = input_padded[h_start:h_start + k_h, w_start:w_start + k_w]\n            output[i, j] = np.sum(window * kernel)\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()"}
{"task_id": 41, "completion_id": 5, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution on the input matrix.\n    \n    Parameters:\n    input_matrix (np.ndarray): 2D input matrix.\n    kernel (np.ndarray): 2D convolutional kernel.\n    padding (int): Number of zero-padding layers to use.\n    stride (int): Stride of the convolution.\n    \n    Returns:\n    list: Output matrix as a Python list (rounded to the 4th decimal).\n    \"\"\"\n    (in_height, in_width) = input_matrix.shape\n    (k_height, k_width) = kernel.shape\n    out_height = (in_height - k_height + 2 * padding) // stride + 1\n    out_width = (in_width - k_width + 2 * padding) // stride + 1\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output = np.zeros((out_height, out_width))\n    for i in range(out_height):\n        for j in range(out_width):\n            h_start = i * stride\n            w_start = j * stride\n            region = padded_input[h_start:h_start + k_height, w_start:w_start + k_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, decimal=4)\n    return output.tolist()"}
{"task_id": 41, "completion_id": 6, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution on the input matrix with the given kernel, padding, and stride.\n    \n    Parameters:\n    - input_matrix: 2D numpy array, the input feature map.\n    - kernel: 2D numpy array, the convolutional kernel.\n    - padding: int, number of zero-padding layers to apply around the input.\n    - stride: int, the stride of the convolution operation.\n    \n    Returns:\n    - output_matrix: 2D numpy array, the output after applying the convolution, as a python list.\n    \"\"\"\n    (in_height, in_width) = input_matrix.shape\n    (k_height, k_width) = kernel.shape\n    out_height = (in_height - k_height + 2 * padding) // stride + 1\n    out_width = (in_width - k_width + 2 * padding) // stride + 1\n    if out_height < 0 or out_width < 0:\n        raise ValueError('Invalid padding/stride, resulting in negative output dimensions.')\n    output_matrix = np.zeros((out_height, out_width), dtype=np.float64)\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    for i in range(out_height):\n        for j in range(out_width):\n            i_start = i * stride\n            j_start = j * stride\n            region = padded_input[i_start:i_start + k_height, j_start:j_start + k_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()"}
{"task_id": 41, "completion_id": 7, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution operation on the input matrix.\n\n    Parameters:\n    - input_matrix (np.ndarray): Input 2D matrix to convolve.\n    - kernel (np.ndarray): 2D convolution kernel.\n    - padding (int): Number of zero-padding layers to add around input.\n    - stride (int): Step size for moving the kernel over the input.\n\n    Returns:\n    - np.ndarray.tolist(): Output as a list rounded to 4 decimal places.\n    \"\"\"\n    (h_in, w_in) = input_matrix.shape\n    (k_h, k_w) = kernel.shape\n    padded = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    h_out = (h_in + 2 * padding - k_h) // stride + 1\n    w_out = (w_in + 2 * padding - k_w) // stride + 1\n    output = np.zeros((h_out, w_out))\n    for i in range(0, h_out):\n        for j in range(0, w_out):\n            h_start = i * stride\n            w_start = j * stride\n            region = padded[h_start:h_start + k_h, w_start:w_start + k_w]\n            output[i, j] = np.sum(region * kernel)\n    return np.round(output, 4).tolist()"}
{"task_id": 41, "completion_id": 8, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform 2D convolution on a given input matrix with specified kernel, padding, and stride.\n    \n    Parameters:\n        input_matrix (np.ndarray): Input matrix (2D).\n        kernel (np.ndarray): Convolution kernel (2D).\n        padding (int): Padding around the input matrix.\n        stride (int): Stride of the sliding window.\n        \n    Returns:\n        np.ndarray: Convolved output as a Python list after reshaping using tolist().\n    \"\"\"\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height - kernel_height + 2 * padding) // stride + 1\n    out_width = (in_width - kernel_width + 2 * padding) // stride + 1\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output_matrix = np.zeros((out_height, out_width), dtype=np.float64)\n    for i in range(out_height):\n        for j in range(out_width):\n            i_start = i * stride\n            j_start = j * stride\n            region = padded_input[i_start:i_start + kernel_height, j_start:j_start + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n    result = np.round(output_matrix, decimals=4).tolist()\n    return result"}
{"task_id": 41, "completion_id": 9, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.array(input_matrix)\n    kernel = np.array(kernel)\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded_input = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            h_start = i * stride\n            w_start = j * stride\n            region = padded_input[h_start:h_start + kernel_h, w_start:w_start + kernel_w]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 1, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 2, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 3, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 4, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 5, "solution": "def relu(z: float) -> float:\n    return max(0, z)"}
{"task_id": 42, "completion_id": 6, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 7, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 8, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 42, "completion_id": 9, "solution": "def relu(z: float) -> float:\n    return z if z > 0 else 0.0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n    \n    Parameters:\n    - X: np.ndarray, 2D array of shape (n_samples, n_features) representing the feature matrix.\n    - w: np.ndarray, 1D array of shape (n_features,) representing the model coefficients.\n    - y_true: np.ndarray, 1D array of shape (n_samples,) representing the true labels.\n    - alpha: float, regularization parameter.\n    \n    Returns:\n    - float: Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * 0.5 * np.dot(w, w)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 1, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 2, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss function.\n\n    Parameters:\n    - X: 2D numpy array, feature matrix of shape (n_samples, n_features)\n    - w: 1D numpy array, coefficient vector of shape (n_features,)\n    - y_true: 1D numpy array, true labels of shape (n_samples,)\n    - alpha: float, regularization parameter\n\n    Returns:\n    - float: the Ridge loss rounded to 4 decimal places\n    \"\"\"\n    n_samples = X.shape[0]\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_penalty = alpha * np.sum(w ** 2)\n    total_loss = mse + l2_penalty\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 3, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss function.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features) - Feature matrix\n    - w: 1D numpy array of shape (n_features,) - Coefficients (weights)\n    - y_true: 1D numpy array of shape (n_samples,) - True labels\n    - alpha: float - Regularization strength\n    \n    Returns:\n    - loss: float - Ridge Regression loss, rounded to 4 decimal places\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 4, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_penalty = np.sum(w[1:] ** 2)\n    loss = mse + alpha * l2_penalty\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 5, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 6, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss function.\n    \n    Parameters:\n    - X: 2D numpy array, shape (n_samples, n_features)\n    - w: 1D numpy array, shape (n_features,)\n    - y_true: 1D numpy array, shape (n_samples,)\n    - alpha: float, regularization strength (lambda)\n    \n    Returns:\n    - loss: float, computed ridge loss rounded to 4 decimal places\n    \"\"\"\n    n_samples = X.shape[0]\n    y_pred = X @ w\n    mae = np.mean((y_pred - y_true) ** 2)\n    reg_term = np.sum(w ** 2)\n    loss = mae + alpha * reg_term\n    return round(loss, 4)"}
{"task_id": 43, "completion_id": 7, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features), the feature matrix.\n    - w: 1D numpy array of shape (n_features,), the model coefficients.\n    - y_true: 1D numpy array of shape (n_samples,), the true labels.\n    - alpha: float, the regularization parameter.\n    \n    Returns:\n    - float: The Ridge loss (MSE + L2 regularization term), rounded to 4 decimal places.\n    \"\"\"\n    n_samples = X.shape[0]\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization_term = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization_term\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 8, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 43, "completion_id": 9, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function.\n    \n    Parameters:\n        X (np.ndarray): Feature matrix (n_samples, n_features)\n        w (np.ndarray): Coefficients (n_features,)\n        y_true (np.ndarray): True target values (n_samples,)\n        alpha (float): Regularization strength\n    \n    Returns:\n        float: The Ridge loss, rounded to 4 decimal places\n    \"\"\"\n    y_pred = X @ w\n    n_samples = y_true.shape[0]\n    mse = 1 / n_samples * np.sum((y_true - y_pred) ** 2)\n    l2_penalty = alpha * np.sum(w ** 2)\n    total_loss = mse + l2_penalty\n    return round(total_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float): Slope for negative inputs (default is 0.01).\n    \n    Returns:\n    float | int: The Leaky ReLU of the input.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 1, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function.\n\n    Parameters:\n    z (float): Input value.\n    alpha (float, optional): Slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n    float | int: Output after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 2, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input z.\n    \n    Parameters:\n    - z (float): The input value.\n    - alpha (float, optional): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n    - float | int: The result of applying Leaky ReLU to z.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 3, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky ReLU activation function to the input value `z`.\n    \n    Parameters:\n        z (float): Input value.\n        alpha (float): Slope for negative inputs (default is 0.01).\n    \n    Returns:\n        float | int: Output after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 4, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 44, "completion_id": 5, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n        z (float): Input value.\n        alpha (float): Slope for negative inputs, default is 0.01.\n    \n    Returns:\n        float: The result of applying the Leaky ReLU function to `z`.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 6, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): Input value to the activation function.\n    alpha (float, optional): Slope for negative inputs, defaults to 0.01.\n\n    Returns:\n    float: Result of the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 7, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function.\n    \n    Parameters:\n        z (float): Input value.\n        alpha (float): Slope coefficient for negative values, default is 0.01.\n    \n    Returns:\n        float: Output after applying Leaky ReLU.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 8, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    return z if z > 0 else alpha * z"}
{"task_id": 44, "completion_id": 9, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n        z (float): Input value.\n        alpha (float, optional): Slope for negative values. Defaults to 0.01.\n\n    Returns:\n        float: Output after applying Leaky ReLU.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: Linear kernel value (dot product) between x1 and x2.\n    \"\"\"\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 1, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel (dot product) of x1 and x2.\n    \"\"\"\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 2, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n\n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: The linear kernel (dot product) of the two input vectors.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 3, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors x1 and x2.\n\n    Parameters:\n    x1 (array-like): First input vector\n    x2 (array-like): Second input vector\n\n    Returns:\n    float: The linear kernel (dot product) of x1 and x2\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 4, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n    The linear kernel is simply the dot product of the two vectors.\n\n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: The dot product (linear kernel).\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 5, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 6, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors.\n\n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n\n    Returns:\n    float: The linear kernel, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 7, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: Dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 8, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: Dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 45, "completion_id": 9, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel (dot product) between two input vectors.\n\n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n\n    Returns:\n    float: The linear kernel value (dot product).\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Compute the precision for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true binary labels (0 or 1)\n    y_pred (np.ndarray): Array of predicted binary labels (0 or 1)\n    \n    Returns:\n    float: Precision score\n    \"\"\"\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 1, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric between two binary numpy arrays.\n    \n    Parameters:\n    - y_true: numpy array of true binary labels.\n    - y_pred: numpy array of predicted binary labels.\n    \n    Returns:\n    - precision: float value of the precision metric.\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    precision = TP / (TP + FP)\n    return precision"}
{"task_id": 46, "completion_id": 2, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n    \n    Parameters:\n    y_true (np.ndarray): True binary labels.\n    y_pred (np.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: Precision score.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])"}
{"task_id": 46, "completion_id": 3, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])"}
{"task_id": 46, "completion_id": 4, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Input must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 5, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels (0 or 1).\n    y_pred (np.array): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision value.\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    else:\n        return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 6, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n    \n    Returns:\n    float: Precision metric, TP / (TP + FP)\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 7, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1)\n    y_pred (np.ndarray): Predicted binary labels (0 or 1)\n\n    Returns:\n    float: Precision metric\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    if tp + fp == 0:\n        return 0.0\n    return tp / (tp + fp)"}
{"task_id": 46, "completion_id": 8, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    else:\n        return true_positives / (true_positives + false_positives)"}
{"task_id": 46, "completion_id": 9, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels (0 or 1).\n    y_pred (np.array): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision score, defined as TP / (TP + FP).\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization with MSE loss.\n\n    Parameters:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    y (np.ndarray): Target vector of shape (n_samples,)\n    weights (np.ndarray): Initial weights of shape (n_features,)\n    learning_rate (float): Learning rate for the gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int): Size of each batch for Mini-Batch and Stochastic\n    method (str): Gradient descent method - 'batch', 'sgd', or 'mini-batch'\n\n    Returns:\n    np.ndarray: Updated weights after the iterations\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    updated_weights = weights.copy()\n    for _ in range(n_iterations):\n        indices = np.arange(n_samples)\n        if method in ['sgd', 'mini-batch']:\n            np.random.shuffle(indices)\n            X = X[indices]\n            y = y[indices]\n        samples_to_use = n_samples if method == 'batch' else batch_size\n        steps = n_samples // samples_to_use\n        for i in range(steps):\n            start = i * samples_to_use\n            end = start + samples_to_use\n            if method == 'sgd' and batch_size != 1:\n                end = start + 1\n                samples_to_use = 1\n            X_batch = X[start:end]\n            y_batch = y[start:end]\n            y_pred = X_batch @ updated_weights\n            error = y_pred - y_batch\n            gradient = X_batch.T @ error / len(X_batch)\n            updated_weights -= learning_rate * gradient\n    return np.around(updated_weights, decimals=4).tolist()"}
{"task_id": 47, "completion_id": 1, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent with the specified method using MSE loss.\n\n    Parameters:\n        X (np.ndarray): Feature matrix of shape (samples, features)\n        y (np.ndarray): Target vector of shape (samples,)\n        weights (np.ndarray): Initial weights of shape (features,)\n        learning_rate (float): Step size for updates\n        n_iterations (int): Number of iterations\n        batch_size (int): Size of data to sample (required only for 'sgd' and 'mini_batch')\n        method (str): 'batch', 'sgd', or 'mini_batch'\n\n    Returns:\n        np.ndarray: Updated weights after n_iterations (rounded to 4 decimal places)\n    \"\"\"\n    n_samples = X.shape[0]\n    weights = weights.copy()\n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = X @ weights\n            error = predictions - y\n            gradient = 2 / n_samples * X.T @ error\n            weights -= learning_rate * gradient\n        elif method == 'sgd':\n            indices = np.random.permutation(n_samples)\n            for i in indices:\n                xi = X[i]\n                yi = y[i]\n                prediction = xi @ weights\n                error = prediction - yi\n                gradient = 2 * xi * error\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            indices = np.random.permutation(n_samples)\n            for i in range(0, n_samples, batch_size):\n                batch_indices = indices[i:i + batch_size]\n                X_batch = X[batch_indices]\n                y_batch = y[batch_indices]\n                predictions = X_batch @ weights\n                error = predictions - y_batch\n                gradient = 2 / batch_size * X_batch.T @ error\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose from 'batch', 'sgd', or 'mini_batch'.\")\n    return np.round(weights, 4).tolist()"}
{"task_id": 47, "completion_id": 2, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using the specified method.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples,)\n    - weights: numpy array of shape (n_features,)\n    - learning_rate: float, learning rate for gradient descent\n    - n_iterations: int, number of iterations for training\n    - batch_size: int, number of samples per batch (used in SGD and mini-batch)\n    - method: string, one of 'batch', 'sgd', or 'mini-batch'\n    \n    Returns:\n    - weights: optimized weights as a python list (reshaped to 1D)\n    - loss_history: list of loss values over iterations (rounded to 4 decimals)\n    \"\"\"\n    n_samples = X.shape[0]\n    loss_history = []\n    for _ in range(n_iterations):\n        if method == 'batch':\n            indices = np.arange(n_samples)\n        elif method == 'sgd':\n            indices = np.random.permutation(n_samples)\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n        else:\n            raise ValueError(\"Method must be one of 'batch', 'sgd', or 'mini-batch'.\")\n        for i in range(0, n_samples, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            y_pred = X_batch @ weights\n            loss = np.mean((y_batch - y_pred) ** 2)\n            loss_history.append(round(loss, 4))\n            gradient = -2 * X_batch.T @ (y_batch - y_pred) / batch_size\n            weights -= learning_rate * gradient\n    return (weights.tolist(), np.array(loss_history).tolist())"}
{"task_id": 47, "completion_id": 3, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using the specified method (batch, stochastic, mini-batch)\n    and the Mean Squared Error (MSE) loss function.\n    \n    Parameters:\n    - X: ndarray of shape (n_samples, n_features)\n    - y: ndarray of shape (n_samples,)\n    - weights: initial weights to use (ndarray of shape (n_features,))\n    - learning_rate: step size for weight updates\n    - n_iterations: number of iterations to perform\n    - batch_size: size of the batches (used for mini-batch or stochastic)\n    - method: 'batch', 'stochastic', or 'mini-batch'\n    \n    Returns:\n    - weights: updated weights after the descent\n    - losses: list of MSE values after each iteration\n    \"\"\"\n    n_samples = X.shape[0]\n    weights_history = np.copy(weights)\n    losses = []\n    if method == 'stochastic':\n        batch_size = 1\n    elif method == 'batch':\n        batch_size = n_samples\n    elif method != 'mini-batch':\n        raise ValueError(\"Method should be either 'batch', 'stochastic', or 'mini-batch'\")\n    for _ in range(n_iterations):\n        for i in range(0, n_samples, batch_size):\n            X_batch = X[i:i + batch_size]\n            y_batch = y[i:i + batch_size]\n            y_pred = X_batch @ weights\n            gradient = 2 / batch_size * (X_batch.T @ (y_pred - y_batch))\n            weights -= learning_rate * gradient\n        y_pred_full = X @ weights\n        loss = np.mean((y_pred_full - y) ** 2)\n        losses.append(loss)\n    weights_rounded = np.round(weights, decimals=4)\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (weights_rounded.tolist(), losses_rounded)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([3, 5, 7, 9, 11])\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.01\nn_iterations = 100\nbatch_size = 2\nmethod = 'mini-batch'"}
{"task_id": 47, "completion_id": 4, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using the specified method (batch, stochastic, or mini-batch)\n    with Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    X (np.ndarray): Design matrix (shape: (n_samples, n_features))\n    y (np.ndarray): Target values (shape: (n_samples,))\n    weights (np.ndarray): Initial weight vector (shape: (n_features,))\n    learning_rate (float): Learning rate for weight updates\n    n_iterations (int): Number of iterations to run\n    batch_size (int): Size of mini-batch for mini-batch gradient descent\n    method (str): Gradient descent type ('batch', 'sgd', 'mini-batch')\n    \n    Returns:\n    weights (list): Optimized weights as a list (rounded to 4 decimal places)\n    \"\"\"\n    n_samples = X.shape[0]\n    weights_history = []\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_indices = np.arange(n_samples)\n        elif method == 'sgd':\n            batch_indices = np.random.choice(n_samples, size=1, replace=False)\n        elif method == 'mini-batch':\n            batch_indices = np.random.choice(n_samples, size=batch_size, replace=False)\n        else:\n            raise ValueError(\"Method must be 'batch', 'sgd', or 'mini-batch'\")\n        X_batch = X[batch_indices]\n        y_batch = y[batch_indices]\n        gradient = 2.0 / len(batch_indices) * X_batch.T @ (X_batch @ weights - y_batch)\n        weights = weights - learning_rate * gradient\n        weights_history.append(weights.copy())\n    rounded_weights = [round(weight, 4) for weight in weights_history[-1]]\n    return rounded_weights"}
{"task_id": 47, "completion_id": 5, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using MSE loss.\n    \n    Parameters:\n    X (np.ndarray): Input features (n_samples, n_features)\n    y (np.ndarray): Target values (n_samples,)\n    weights (np.ndarray): Initial weights (n_features,)\n    learning_rate (float): Learning rate for gradient descent\n    n_iterations (int): Number of iterations\n    batch_size (int): Size of the mini-batch (ignored for 'batch' and 'sgd')\n    method (str): One of 'batch', 'sgd', or 'mini_batch'\n\n    Returns:\n    weights (list): Updated weights after gradient descent\n    losses (list): List of computed losses at each iteration (rounded to 4 decimals)\n    \"\"\"\n    losses = []\n    n_samples = X.shape[0]\n    weights = np.array(weights).copy()\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_indices = np.arange(n_samples)\n        elif method == 'sgd':\n            batch_indices = np.random.choice(n_samples, size=1, replace=False)\n        elif method == 'mini_batch':\n            batch_indices = np.random.choice(n_samples, size=batch_size, replace=False)\n        else:\n            raise ValueError(\"Method must be one of 'batch', 'sgd', or 'mini_batch'\")\n        X_batch = X[batch_indices]\n        y_batch = y[batch_indices]\n        predictions = X_batch @ weights\n        error = predictions - y_batch\n        gradient = X_batch.T @ error / len(batch_indices)\n        weights -= learning_rate * gradient\n        full_predictions = X @ weights\n        loss = np.mean((full_predictions - y) ** 2)\n        losses.append(round(loss, 4))\n    return (weights.tolist(), [round(loss, 4) for loss in losses])"}
{"task_id": 47, "completion_id": 6, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform Gradient Descent using the specified variant and MSE loss.\n    \n    Parameters:\n    - X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    - y (np.ndarray): Target vector of shape (n_samples,)\n    - weights (np.ndarray): Weight vector of shape (n_features,)\n    - learning_rate (float): Learning rate for weight update\n    - n_iterations (int): Number of iterations to run the algorithm\n    - batch_size (int): Size of batch for Mini-batch GD (default is 1)\n    - method (str): Specifies which variant to use. One of: 'batch', 'stochastic', 'mini_batch'\n    \n    Returns:\n    - weights (np.ndarray): Updated weights after gradient descent\n    - loss_history (list): History of MSE loss after each iteration (rounded to 4 decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    loss_history = []\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            gradients = 2 / n_samples * X.T @ (X @ weights - y)\n            weights -= learning_rate * gradients\n            loss = 1 / n_samples * np.sum((X @ weights - y) ** 2)\n        elif method == 'stochastic':\n            idx = np.random.randint(n_samples)\n            xi = X[idx, :].reshape(1, -1)\n            yi = y[idx].reshape(1, -1)\n            gradients = 2 * xi.T @ (xi @ weights - yi)\n            weights -= learning_rate * gradients\n            loss = 1 / n_samples * np.sum((X @ weights - y) ** 2)\n        elif method == 'mini_batch':\n            indices = np.random.choice(n_samples, size=batch_size, replace=False)\n            xi = X[indices, :]\n            yi = y[indices]\n            gradients = 2 / batch_size * xi.T @ (xi @ weights - yi)\n            weights -= learning_rate * gradients\n            loss = 1 / n_samples * np.sum((X @ weights - y) ** 2)\n        else:\n            raise ValueError(\"Method must be one of 'batch', 'stochastic', or 'mini_batch'\")\n        loss_history.append(np.round(loss, 4))\n    return (weights, loss_history)"}
{"task_id": 47, "completion_id": 7, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent (batch, stochastic, or mini-batch) with MSE loss.\n\n    Parameters:\n    - X: Input features (shape: [n_samples, n_features])\n    - y: Target variable (shape: [n_samples,])\n    - weights: Initial weights (shape: [n_features + 1, ]) (include bias)\n    - learning_rate: Learning rate\n    - n_iterations: Number of iterations\n    - batch_size: Size of the batch\n    - method: 'batch', 'stochastic', or 'mini_batch'\n\n    Returns:\n    A dictionary with keys:\n    - 'weights': Final weights (reshaped to 1D list)\n    - 'loss_history': List of MSE loss at each iteration (rounded to 4 decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.array(weights).copy()\n    loss_history = []\n    if method == 'batch':\n        for _ in range(n_iterations):\n            predictions = X.dot(weights[1:]) + weights[0]\n            mse = np.mean((predictions - y) ** 2)\n            loss_history.append(mse)\n            error = predictions - y\n            gradient = np.zeros(weights.shape)\n            gradient[0] = np.mean(error)\n            for i in range(n_features):\n                gradient[i + 1] = np.mean(error * X[:, i])\n            weights -= learning_rate * gradient\n    elif method == 'stochastic':\n        for _ in range(n_iterations):\n            idx = np.random.choice(n_samples)\n            xi = X[idx]\n            yi = y[idx]\n            prediction = np.dot(xi, weights[1:]) + weights[0]\n            error = prediction - yi\n            gradient = np.zeros(weights.shape)\n            gradient[0] = error\n            gradient[1:] = error * xi\n            weights -= learning_rate * gradient\n            preds = X.dot(weights[1:]) + weights[0]\n            mse = np.mean((preds - y) ** 2)\n            loss_history.append(mse)\n    elif method == 'mini_batch':\n        indices = np.arange(n_samples)\n        for _ in range(n_iterations):\n            np.random.shuffle(indices)\n            batches = [indices[i:i + batch_size] for i in range(0, n_samples, batch_size)]\n            for batch in batches:\n                X_batch = X[batch]\n                y_batch = y[batch]\n                predictions = X_batch.dot(weights[1:]) + weights[0]\n                error = predictions - y_batch\n                gradient = np.zeros(weights.shape)\n                gradient[0] = np.mean(error)\n                for i in range(n_features):\n                    gradient[i + 1] = np.mean(error * X_batch[:, i])\n                weights -= learning_rate * gradient\n            preds = X.dot(weights[1:]) + weights[0]\n            mse = np.mean((preds - y) ** 2)\n            loss_history.append(mse)\n    else:\n        raise ValueError(\"Invalid method. Choose from 'batch', 'stochastic', or 'mini_batch'.\")\n    loss_history = [np.round(loss, 4) for loss in loss_history]\n    weights_vector = np.round(weights, 4)\n    weights_list = weights_vector.tolist()\n    return {'weights': weights_list, 'loss_history': loss_history}"}
{"task_id": 47, "completion_id": 8, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization for linear regression with MSE loss.\n    \n    Parameters:\n    - X: numpy.ndarray, shape (n_samples, n_features)\n         Input feature matrix\n    - y: numpy.ndarray, shape (n_samples,)\n         Target values\n    - weights: numpy.ndarray, shape (n_features,)\n               Initial weights\n    - learning_rate: float\n                     Learning rate for gradient descent\n    - n_iterations: int\n                    Number of iterations to run\n    - batch_size: int\n                  Size of each mini-batch (for 'minibatch' method)\n    - method: str\n              One of: 'sgd', 'batch', 'minibatch'\n    \n    Returns:\n    - weights: numpy.ndarray, shape (n_features,)\n               Optimized weights\n    - all_weights: list of lists\n                   List of weights at each iteration\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights_history = []\n    for _ in range(n_iterations):\n        if method == 'batch':\n            X_batch = X\n            y_batch = y\n        elif method == 'sgd':\n            index = np.random.randint(n_samples)\n            X_batch = X[index:index + 1]\n            y_batch = y[index:index + 1]\n        elif method == 'minibatch':\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            X_batch = X[indices]\n            y_batch = y[indices]\n        else:\n            raise ValueError(\"Method must be 'batch', 'sgd' or 'minibatch'\")\n        y_pred = X_batch @ weights\n        error = y_pred - y_batch\n        gradient = 2 / len(y_batch) * X_batch.T @ error\n        weights -= learning_rate * gradient\n        weights_history.append(np.round(weights, 4).tolist())\n    return (np.round(weights, 4).tolist(), weights_history)\nX = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\ny = np.array([3, 5, 7, 9])\nweights = np.zeros(2)\nlearning_rate = 0.01\nn_iterations = 100"}
{"task_id": 47, "completion_id": 9, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform Gradient Descent with the specified method.\n\n    Parameters:\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features)\n    y : np.ndarray\n        Target vector of shape (n_samples,)\n    weights : np.ndarray\n        Initial weights of shape (n_features,)\n    learning_rate : float\n        Learning rate for the updates\n    n_iterations : int\n        Number of iterations to perform\n    batch_size : int, optional\n        Size of the mini-batch for mini-batch gradient descent (default is 1 for SGD)\n    method : str, optional\n        Method to use ('batch', 'sgd', or 'mini-batch')\n\n    Returns:\n    -------\n    weights : np.ndarray\n        Updated weights after training\n    loss_history : list\n        List of MSE values after each iteration (rounded to 4 decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = weights.copy()\n    loss_history = []\n    for i in range(n_iterations):\n        if method == 'batch':\n            indices = np.arange(n_samples)\n        elif method == 'sgd':\n            indices = np.random.permutation(n_samples)\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n        else:\n            raise ValueError(\"Invalid method. Use 'batch', 'sgd', or 'mini-batch'.\")\n        start = 0\n        while start < n_samples:\n            end = min(start + batch_size, n_samples)\n            batch_indices = indices[start:end]\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            y_pred = np.dot(X_batch, weights)\n            gradient = 2 / X_batch.shape[0] * np.dot(X_batch.T, y_pred - y_batch)\n            weights -= learning_rate * gradient\n            if i == n_iterations - 1 or method == 'sgd':\n                pred_y = np.dot(X, weights)\n                loss = np.mean((pred_y - y) ** 2)\n                loss_history.append(round(loss, 4))\n            start += batch_size\n    return (weights.tolist(), loss_history)\nX = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\ny = np.array([2, 3, 4, 5])\nweights = np.array([0.5, 0.5])\nlearning_rate = 0.01\nn_iterations = 100\nbatch_size = 2"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a matrix into Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing a matrix.\n\n    Returns:\n    list of lists: The matrix in Reduced Row Echelon Form, as a Python list.\n    \"\"\"\n    mat = np.array(matrix, dtype=float)\n    (rows, cols) = mat.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            return mat.tolist()\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return mat.tolist()\n        mat[[i, r]] = mat[[r, i]]\n        pivot = mat[r, lead]\n        mat[r] = mat[r] / pivot\n        for row in range(rows):\n            if row != r and mat[row, lead] != 0:\n                mat[row] = mat[row] - mat[row, lead] * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 48, "completion_id": 1, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists or np.ndarray): The input matrix to be converted.\n    \n    Returns:\n    list: The RREF of the matrix as a Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    lead = 0\n    (n_rows, n_cols) = matrix.shape\n    for r in range(n_rows):\n        if lead >= n_cols:\n            return matrix.tolist()\n        i = r\n        while matrix[i, lead] == 0:\n            i += 1\n            if i == n_rows:\n                i = r\n                lead += 1\n                if lead == n_cols:\n                    return matrix.tolist()\n        matrix[[i, r]] = matrix[[r, i]]\n        pivot = matrix[r, lead]\n        matrix[r] = matrix[r] / pivot\n        above_and_below = np.arange(n_rows) != r\n        matrix[above_and_below, lead:] -= matrix[r, lead:] * matrix[above_and_below, lead, np.newaxis]\n        lead += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 2, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (m, n) = A.shape\n    pivot_col = 0\n    rank = 0\n    for r in range(m):\n        if pivot_col >= n:\n            break\n        pivot_row = -1\n        for i in range(r, m):\n            if A[i, pivot_col] != 0:\n                pivot_row = i\n                break\n        if pivot_row == -1:\n            pivot_col += 1\n            continue\n        A[[r, pivot_row]] = A[[pivot_row, r]]\n        pivot = A[r, pivot_col]\n        A[r] /= pivot\n        for i in range(m):\n            if i != r and A[i, pivot_col] != 0:\n                factor = A[i, pivot_col]\n                A[i] = A[i] - factor * A[r]\n        pivot_col += 1\n        rank += 1\n    zero_rows = []\n    non_zero_rows = []\n    for row in A:\n        if np.allclose(row, np.zeros_like(row)):\n            zero_rows.append(row)\n        else:\n            non_zero_rows.append(row)\n    A = np.array(non_zero_rows + zero_rows)\n    return A.tolist()"}
{"task_id": 48, "completion_id": 3, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n        matrix (list of lists or np.ndarray): The input matrix.\n    \n    Returns:\n        list of lists: The RREF of the matrix.\n    \"\"\"\n    mat = np.array(matrix, dtype=np.float64)\n    (m, n) = mat.shape\n    r = 0\n    for c in range(n):\n        pivot_row = np.argmax(np.abs(mat[r:, c])) + r\n        if mat[pivot_row, c] == 0:\n            continue\n        mat[[r, pivot_row]] = mat[[pivot_row, r]]\n        mat[r, :] /= mat[r, c]\n        mat[mat[:, c] != 0] -= mat[r, :] * mat[mat[:, c] != 0, c]\n        r += 1\n    return mat.tolist()"}
{"task_id": 48, "completion_id": 4, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    r = 0\n    for c in range(cols):\n        pivot_row = None\n        for i in range(r, rows):\n            if matrix[i, c] != 0:\n                pivot_row = i\n                break\n        if pivot_row is None:\n            continue\n        matrix[[r, pivot_row]] = matrix[[pivot_row, r]]\n        pivot = matrix[r, c]\n        matrix[r] = matrix[r] / pivot\n        for i in range(rows):\n            if i != r and abs(matrix[i, c]) > 1e-10:\n                factor = matrix[i, c]\n                matrix[i] = matrix[i] - factor * matrix[r]\n        r += 1\n        if r >= rows:\n            break\n    zero_rows = np.all(matrix == 0, axis=1)\n    matrix = matrix[~zero_rows]\n    matrix = np.vstack((matrix, np.zeros((np.sum(zero_rows), cols), dtype=float)))\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 5, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n        matrix (list of list or 2D array-like): The input matrix to be reduced.\n    \n    Returns:\n        list: The matrix in RREF form, as a Python list.\n    \"\"\"\n    m = np.array(matrix, dtype=float)\n    (rows, cols) = m.shape\n    r = 0\n    for c in range(cols):\n        pivot_row = -1\n        for i in range(r, rows):\n            if m[i, c] != 0:\n                pivot_row = i\n                break\n        if pivot_row == -1:\n            continue\n        m[[r, pivot_row]] = m[[pivot_row, r]]\n        m[r] = m[r] / m[r, c]\n        for i in range(rows):\n            if i != r and m[i, c] != 0:\n                m[i] = m[i] - m[i, c] * m[r]\n        r += 1\n    return m.tolist()"}
{"task_id": 48, "completion_id": 6, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n        matrix (list of lists or 2D np.ndarray): Input matrix.\n    \n    Returns:\n        list: RREF of the input matrix as a 2D Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (m, n) = matrix.shape\n    r = 0\n    for c in range(n):\n        pivot_row = None\n        for i in range(r, m):\n            if matrix[i, c] != 0:\n                pivot_row = i\n                break\n        if pivot_row is None:\n            continue\n        matrix[[r, pivot_row]] = matrix[[pivot_row, r]]\n        pivot = matrix[r, c]\n        matrix[r] = matrix[r] / pivot\n        for i in range(m):\n            if i != r and matrix[i, c] != 0:\n                factor = matrix[i, c]\n                matrix[i] = matrix[i] - factor * matrix[r]\n        r += 1\n    row_norms = np.linalg.norm(matrix, axis=1)\n    non_zero_rows = matrix[row_norms != 0]\n    if non_zero_rows.shape[0] < m:\n        missing_rows = m - non_zero_rows.shape[0]\n        zero_rows = np.zeros((missing_rows, n), dtype=float)\n        non_zero_rows = np.vstack((non_zero_rows, zero_rows[:missing_rows]))\n    return non_zero_rows.tolist()"}
{"task_id": 48, "completion_id": 7, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n        matrix (list of lists or np.ndarray): The input matrix.\n    \n    Returns:\n        list of lists: The RREF of the input matrix, converted to a list using tolist().\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            return matrix.tolist()\n        i = r\n        while matrix[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return matrix.tolist()\n        matrix[[i, r]] = matrix[[r, i]]\n        matrix[r] = matrix[r] / matrix[r, lead]\n        for row in range(rows):\n            if row != r and matrix[row, lead] != 0:\n                matrix[row] = matrix[row] - matrix[row, lead] * matrix[r]\n        lead += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 8, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    r = 0\n    for c in range(cols):\n        pivot_row = None\n        for i in range(r, rows):\n            if matrix[i, c] != 0:\n                pivot_row = i\n                break\n        if pivot_row is None:\n            continue\n        matrix[[r, pivot_row]] = matrix[[pivot_row, r]]\n        matrix[r] = matrix[r] / matrix[r, c]\n        for i in range(rows):\n            if i != r and matrix[i, c] != 0:\n                matrix[i] = matrix[i] - matrix[i, c] * matrix[r]\n        r += 1\n    return matrix.tolist()"}
{"task_id": 48, "completion_id": 9, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        for i in range(r, rows):\n            if abs(matrix[i, lead]) > 1e-10:\n                matrix[[r, i]] = matrix[[i, r]]\n                break\n        else:\n            lead += 1\n            continue\n        pivot = matrix[r, lead]\n        matrix[r] = matrix[r] / pivot\n        for i in range(rows):\n            if i != r and abs(matrix[i, lead]) > 1e-10:\n                factor = matrix[i, lead]\n                matrix[i] = matrix[i] - factor * matrix[r]\n        lead += 1\n    result = np.round(matrix, 9)\n    return result.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Optimizes the parameters of the function f using the Adam optimization algorithm.\n\n    Parameters:\n        f (function): The objective function to be minimized.\n        grad (function): Computes gradient of f.\n        x0 (array-like): Initial parameter values.\n        learning_rate (float): Step size (default: 0.001).\n        beta1 (float): Exponential decay rate for the first moment estimates (default: 0.9).\n        beta2 (float): Exponential decay rate for the second moment estimates (default: 0.999).\n        epsilon (float): Small constant for numerical stability (default: 1e-8).\n        num_iterations (int): Number of optimization steps (default: 10).\n\n    Returns:\n        list: Optimized parameters after rounding to 4 decimal places.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_val = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_val\n        v = beta2 * v + (1 - beta2) * grad_val ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Optimizes the parameters of a given function using the Adam optimization algorithm.\n    \n    Parameters:\n    - f: Objective function to be optimized\n    - grad: Function that computes the gradient of `f`\n    - x0: Initial parameter values\n    - learning_rate: The step size\n    - beta1: Exponential decay rate for the first moment estimates\n    - beta2: Exponential decay rate for the second moment estimates\n    - epsilon: A small constant for numerical stability\n    - num_iterations: Number of iterations to run the optimizer\n    \n    Returns:\n    - Optimized parameters as a Python list rounded to the nearest 4th decimal.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Perform Adam optimization to find the minimum of function f with gradient grad.\n    \n    Parameters:\n    - f: objective function to minimize\n    - grad: function that returns gradient of f\n    - x0: initial parameter vector\n    - learning_rate: step size for the update\n    - beta1: exponential decay rate for first moment estimates\n    - beta2: exponential decay rate for second moment estimates\n    - epsilon: small constant for numerical stability\n    - num_iterations: number of iterations to run\n    \n    Returns:\n    - x: optimized parameter vector\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_x = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_x\n        v = beta2 * v + (1 - beta2) * grad_x ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_val = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_val\n        v = beta2 * v + (1 - beta2) * grad_val ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g_t = grad(x)\n        m = beta1 * m + (1 - beta1) * g_t\n        v = beta2 * v + (1 - beta2) * g_t ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Perform optimization using the Adam algorithm.\n    \n    Parameters:\n        f (function): The objective function to be optimized.\n        grad (function): A function that computes the gradient of `f`.\n        x0 (array-like): Initial parameter values.\n        learning_rate (float): The step size.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        epsilon (float): A small constant for numerical stability.\n        num_iterations (int): Number of iterations to run the optimizer.\n        \n    Returns:\n        list: Optimized parameters as a flattened list, rounded to 4 decimal places.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_val = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_val\n        v = beta2 * v + (1 - beta2) * grad_val ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    beta1 = np.array(beta1, dtype=np.float64)\n    beta2 = np.array(beta2, dtype=np.float64)\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x), dtype=np.float64)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Optimizes the parameters of a given function using the Adam optimizer.\n    \n    Parameters:\n    - f: Objective function to be optimized\n    - grad: Function that computes gradients of f\n    - x0: Initial parameter values (numpy array)\n    - learning_rate: Step size (default: 0.001)\n    - beta1: Exponential decay rate for first moment estimates (default: 0.9)\n    - beta2: Exponential decay rate for second moment estimates (default: 0.999)\n    - epsilon: Small constant for numerical stability (default: 1e-8)\n    - num_iterations: Number of iterations (default: 10)\n    \n    Returns:\n    - Optimized parameters as a list after rounding and flattening\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 1\n    x = np.copy(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\ndef grad(x):\n    return x\nx0 = np.array([5.0, 3.0])"}
{"task_id": 49, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Optimize the given function using the Adam algorithm.\n\n    Parameters:\n        f (function): The objective function to be optimized.\n        grad (function): A function that computes the gradient of `f`.\n        x0 (array-like): Initial parameter values.\n        learning_rate (float, optional): The step size. Default is 0.001.\n        beta1 (float, optional): Exponential decay rate for the first moment estimates. Default is 0.9.\n        beta2 (float, optional): Exponential decay rate for the second moment estimates. Default is 0.999.\n        epsilon (float, optional): A small constant for numerical stability. Default is 1e-8.\n        num_iterations (int, optional): Number of iterations to run the optimizer. Default is 10.\n\n    Returns:\n        list: Optimized parameters, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g_t = np.array(grad(x), dtype=np.float64)\n        m = beta1 * m + (1 - beta1) * g_t\n        v = beta2 * v + (1 - beta2) * g_t ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 49, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Perform optimization using the Adam algorithm.\n\n    Parameters:\n    - f: Objective function to be optimized\n    - grad: Function that computes the gradient of f\n    - x0: Initial parameters (numpy array or list)\n    - learning_rate: Step size (default: 0.001)\n    - beta1: Exponential decay rate for first moment (default: 0.9)\n    - beta2: Exponential decay rate for second moment (default: 0.999)\n    - epsilon: Small constant for numerical stability (default: 1e-8)\n    - num_iterations: Number of iterations to run the optimizer (default: 10)\n\n    Returns:\n    - Optimized parameter vector as a Python list (rounded to 4 decimal places).\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_val = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_val\n        v = beta2 * v + (1 - beta2) * grad_val ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\nx0 = [5.0, 5.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent.\n    \n    Parameters:\n    X : np.array\n        Input feature matrix (n_samples, n_features)\n    y : np.array\n        Target vector (n_samples,)\n    alpha : float\n        Regularization strength (lambda for L1 penalty)\n    learning_rate : float\n        Learning rate for gradient descent\n    max_iter : int\n        Maximum number of iterations\n    tol : float\n        Tolerance for stopping condition\n\n    Returns:\n    weights : list\n        Final weights (reshaped to 1D list, rounded to 4 decimals)\n    bias : float\n        Final bias (rounded to 4 decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for _ in range(max_iter):\n        y_pred = X @ weights + bias\n        dw = -1 / n_samples * (X.T @ (y - y_pred)) + alpha * np.sign(weights)\n        db = -1 / n_samples * np.sum(y - y_pred)\n        weights_new = weights - learning_rate * dw\n        bias_new = bias - learning_rate * db\n        if np.linalg.norm(weights_new - weights) < tol and abs(bias_new - bias) < tol:\n            break\n        weights = weights_new\n        bias = bias_new\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return (weights.tolist(), float(bias))\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([10, 20, 30])"}
{"task_id": 50, "completion_id": 1, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for iter in range(max_iter):\n        y_pred = X @ w + b\n        dw = -1 / n_samples * X.T @ (y - y_pred) + alpha * np.sign(w)\n        db = -1 / n_samples * np.sum(y - y_pred)\n        w_prev = w.copy()\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.linalg.norm(w - w_prev) < tol:\n            break\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    return (w_rounded.tolist(), b_rounded)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([3, 5, 7, 9])"}
{"task_id": 50, "completion_id": 2, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Gradient Descent for Lasso Regression with L1 Regularization.\n\n    Parameters:\n    -----------\n    X : np.array\n        Training data (n_samples, n_features)\n    y : np.array\n        Target values (n_samples, )\n    alpha : float\n        Regularization strength (lambda)\n    learning_rate : float\n        Step size for updating parameters\n    max_iter : int\n        Maximum number of iterations\n    tol : float\n        Convergence tolerance\n\n    Returns:\n    --------\n    weights : List of floats (reshaped and rounded)\n    bias : float (rounded)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        grad_weights = 1 / n_samples * np.dot(X.T, y_pred - y) + alpha / n_samples * np.sign(weights)\n        grad_bias = 1 / n_samples * np.sum(y_pred - y)\n        weights_prev = np.copy(weights)\n        bias_prev = bias\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n        if np.sqrt(np.sum((weights - weights_prev) ** 2) + (bias - bias_prev) ** 2) < tol:\n            break\n    weights_list = np.around(weights, 4).tolist()\n    bias_rounded = np.around(bias, 4)\n    return (weights_list, bias_rounded)"}
{"task_id": 50, "completion_id": 3, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha * np.sign(w)\n        db = -2 / n_samples * np.sum(y - y_pred)\n        prev_w = w.copy()\n        prev_b = b\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.sqrt(np.sum((w - prev_w) ** 2) + (b - prev_b) ** 2) < tol:\n            break\n    w_rounded = np.round(w, 4)\n    b_rounded = np.round(b, 4)\n    return (w_rounded.tolist(), b_rounded)"}
{"task_id": 50, "completion_id": 4, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        error = y_pred - y\n        grad_w = 1 / n_samples * np.dot(X.T, error) + alpha / n_samples * np.sign(w)\n        grad_b = 1 / n_samples * np.sum(error)\n        w_new = w - learning_rate * grad_w\n        b_new = b - learning_rate * grad_b\n        if np.max(np.abs(w_new - w)) < tol and abs(b_new - b) < tol:\n            break\n        w = w_new\n        b = b_new\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 50, "completion_id": 5, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0\n    for iter in range(max_iter):\n        y_pred = X @ w + b\n        residuals = y_pred - y\n        dw = X.T @ residuals / n_samples\n        db = residuals.mean()\n        w -= learning_rate * (dw + alpha * np.sign(w))\n        b -= learning_rate * db\n        if np.sqrt(dw.dot(dw) + db ** 2) < tol:\n            break\n    w = np.round(w, decimals=4)\n    b = round(b, 4)\n    return (w.tolist(), b)\nX = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]])\ny = np.array([3.0, 5.0, 7.0, 9.0])"}
{"task_id": 50, "completion_id": 6, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent with L1 Regularization.\n    \n    Parameters:\n    -----------\n    X: np.array\n        Feature matrix of shape (n_samples, n_features)\n    y: np.array\n        Target vector of shape (n_samples,)\n    alpha: float\n        Regularization strength (lambda)\n    learning_rate: float\n        Step size for gradient descent\n    max_iter: int\n        Maximum number of iterations\n    tol: float\n        Tolerance for stopping condition\n    \n    Returns:\n    --------\n    tuple\n        (weights, bias): Python list containing weights and bias (rounded to 4 decimals)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    prev_cost = float('inf')\n    for iter in range(max_iter):\n        y_pred = X @ weights + bias\n        loss = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2)\n        cost = loss + alpha / (2 * n_samples) * np.sum(np.abs(weights))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n        grad_weights = -1 / n_samples * X.T @ (y - y_pred) + alpha / n_samples * np.sign(weights)\n        grad_bias = -1 / n_samples * np.sum(y - y_pred)\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n    weights_rounded = np.round(weights, 4).tolist()\n    bias_rounded = round(bias, 4)\n    return (weights_rounded, bias_rounded)"}
{"task_id": 50, "completion_id": 7, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    cost_history = []\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        db = -2 / n_samples * np.sum(y - y_pred)\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha / n_samples * np.sign(weights)\n        weights = weights - learning_rate * dw\n        bias = bias - learning_rate * db\n        cost = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha / (2 * n_samples) * np.sum(np.abs(weights))\n        cost_history.append(cost)\n        if len(cost_history) > 1 and abs(cost_history[-2] - cost) < tol:\n            break\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = np.round(bias, 4)\n    return (weights_rounded.tolist(), bias_rounded)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([3, 5, 7, 9])\nalpha = 0.5\nlearning_rate = 0.01\nmax_iter = 1000"}
{"task_id": 50, "completion_id": 8, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent with an L1 regularization (penalty).\n\n    Parameters:\n    - X: np.array, Training input samples (shape: n_samples x p_features)\n    - y: np.array, Target values (shape: n_samples)\n    - alpha: float, Regularization strength (lambda in Lasso)\n    - learning_rate: float, Step size for gradient descent\n    - max_iter: int, Maximum number of iterations to run\n    - tol: float, Convergence tolerance threshold for weight change\n    \n    Returns:\n    - w: list, Coefficients (rounded to 4 decimal places)\n    - b: float, Bias (rounded to 4 decimal places)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        residuals = y_pred - y\n        dw = 1 / n_samples * np.dot(X.T, residuals) + alpha / n_samples * np.sign(w)\n        db = 1 / n_samples * np.sum(residuals)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.linalg.norm(dw) < tol:\n            break\n    w_rounded = np.round(w, 4).tolist()\n    b_rounded = round(b, 4)\n    return (w_rounded, b_rounded)"}
{"task_id": 50, "completion_id": 9, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    history = []\n    for i in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        error = y_pred - y\n        cost = np.mean(error ** 2) / 2 + alpha * np.sum(np.abs(w))\n        history.append(cost)\n        dw = np.dot(X.T, error) / n_samples + alpha * np.sign(w)\n        db = np.mean(error)\n        prev_w = np.copy(w)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        if np.linalg.norm(w - prev_w) < tol:\n            break\n    w = np.round(w, decimals=4)\n    b = np.round(b, decimals=4)\n    history = np.round(history, decimals=4).tolist()\n    return (w.tolist(), b, history)\nX = np.random.rand(100, 3)\ntrue_weights = np.array([2.5, -3.1, 0])\ntrue_bias = 1.5\ny = X @ true_weights + true_bias + np.random.normal(0, 0.1, 100)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                sub_cost = dp[i - 1][j - 1] + 1\n                ins_cost = dp[i][j - 1] + 1\n                del_cost = dp[i - 1][j] + 1\n                min_cost = min(sub_cost, ins_cost, del_cost)\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    trans_cost = dp[i - 2][j - 2] + 1\n                    min_cost = min(min_cost, trans_cost)\n                dp[i][j] = min_cost\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 1, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"Calculate the Optimal String Alignment (OSA) distance between two strings.\"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 2] == target[j - 1]) and (source[i - 1] == target[j - 2]):\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len_source][len_target]"}
{"task_id": 51, "completion_id": 2, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment (OSA) distance between two strings.\n    \n    OSA (also known as Damerau-Levenshtein distance with the restriction that transpositions are only\n    allowed between adjacent characters and are not allowed to overlap).\n    \n    Parameters:\n    source (str): The source string to transform from\n    target (str): The target string to transform to\n    \n    Returns:\n    int: The OSA distance, i.e., the minimum number of edit operations needed to convert source to target\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return int(dp[m][n])"}
{"task_id": 51, "completion_id": 3, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i][j - 1] + 1, dp[i - 1][j] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 4, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculate the Optimal String Alignment (OSA) distance between two strings.\n    This is the minimum number of edit operations (insert, delete, substitute,\n    transpose adjacent characters) required to transform source into target.\n    \"\"\"\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return int(dp[m][n])"}
{"task_id": 51, "completion_id": 5, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                substitution = dp[i - 1][j - 1] + 1\n                insertion = dp[i][j - 1] + 1\n                deletion = dp[i - 1][j] + 1\n                dp[i][j] = min(substitution, insertion, deletion)\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    transposition = dp[i - 2][j - 2] + 1\n                    dp[i][j] = min(dp[i][j], transposition)\n    return int(dp[m][n])"}
{"task_id": 51, "completion_id": 6, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                sub_del_ins = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    transpose = dp[i - 2][j - 2] + 1\n                    dp[i][j] = min(sub_del_ins, transpose)\n                else:\n                    dp[i][j] = sub_del_ins\n    return dp[m][n]"}
{"task_id": 51, "completion_id": 7, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            substitution = dp[i - 1, j - 1] + cost\n            insertion = dp[i, j - 1] + 1\n            deletion = dp[i - 1, j] + 1\n            transposition = dp[i - 2, j - 2] + 1 if i >= 2 and j >= 2 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]) else float('inf')\n            dp[i, j] = min(substitution, insertion, deletion, transposition)\n    return int(dp[m, n])"}
{"task_id": 51, "completion_id": 8, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(1, len_source + 1):\n        dp[i][0] = i\n    for j in range(1, len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            substitution_cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + substitution_cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len_source][len_target]"}
{"task_id": 51, "completion_id": 9, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment Distance (OSA) between source and target.\n    Allowed operations: insert, delete, substitute, transpose (all cost 1).\n    \n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The OSA distance.\n    \"\"\"\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(1, m + 1):\n        dp[i][0] = i\n    for j in range(1, n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    rec = TP / denominator\n    return round(rec, 3)"}
{"task_id": 52, "completion_id": 1, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the Recall metric for binary classification.\n    \n    Parameters:\n        y_true (list): True binary labels (0 or 1).\n        y_pred (list): Predicted binary labels (0 or 1).\n    \n    Returns:\n        float: Recall rounded to 3 decimal places.\n    \"\"\"\n    true_positive = 0\n    false_negative = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1:\n            if pred == 1:\n                true_positive += 1\n            else:\n                false_negative += 1\n    denominator = true_positive + false_negative\n    if denominator == 0:\n        return 0.0\n    recall_value = true_positive / denominator\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 2, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall for a binary classification.\n    \n    Parameters:\n    y_true (list): True binary labels (0 or 1).\n    y_pred (list): Predicted binary labels (0 or 1).\n    \n    Returns:\n    float: Recall value rounded to 3 decimal places.\n    \"\"\"\n    if not y_true or not y_pred:\n        return 0.0\n    tp = np.sum(np.logical_and(np.array(y_true) == 1, np.array(y_pred) == 1))\n    fn = np.sum(np.logical_and(np.array(y_true) == 1, np.array(y_pred) == 0))\n    if tp + fn == 0:\n        return 0.0\n    return round(tp / (tp + fn), 3)"}
{"task_id": 52, "completion_id": 3, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    y_true (list of int): True binary labels (0 or 1).\n    y_pred (list of int): Predicted binary labels (0 or 1).\n    \n    Returns:\n    float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(true_positives / denominator, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 0]"}
{"task_id": 52, "completion_id": 4, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall for a binary classification problem.\n    \n    Parameters:\n    y_true (list of int): True binary labels (0 or 1).\n    y_pred (list of int): Predicted binary labels (0 or 1).\n    \n    Returns:\n    float: Recall metric rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(true_positives / denominator, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 0, 1, 1, 0]"}
{"task_id": 52, "completion_id": 5, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    try:\n        recall_val = TP / (TP + FN)\n    except ZeroDivisionError:\n        recall_val = 0.0\n    return round(recall_val, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]"}
{"task_id": 52, "completion_id": 6, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(true_positives / denominator, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 0]"}
{"task_id": 52, "completion_id": 7, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n        y_true (list): True binary labels (0 or 1).\n        y_pred (list): Predicted binary labels (0 or 1).\n        \n    Returns:\n        float: Recall rounded to three decimal places.\n    \"\"\"\n    true_positives = sum((yt == 1 and yp == 1 for (yt, yp) in zip(y_true, y_pred)))\n    false_negatives = sum((yt == 1 and yp == 0 for (yt, yp) in zip(y_true, y_pred)))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    return round(true_positives / denominator, 3)"}
{"task_id": 52, "completion_id": 8, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    FN = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    recall_value = TP / denominator\n    return round(recall_value, 3)"}
{"task_id": 52, "completion_id": 9, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    y_true (list): True binary labels (0 or 1)\n    y_pred (list): Predicted binary labels (0 or 1)\n    \n    Returns:\n    float: Recall value, rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention mechanism.\n\n    Parameters:\n        X (np.ndarray): Input matrix of shape (sequence_length, input_dim)\n        W_q (np.ndarray): Query weight matrix of shape (input_dim, dim_k)\n        W_k (np.ndarray): Key weight matrix of shape (input_dim, dim_k)\n        W_v (np.ndarray): Value weight matrix of shape (input_dim, dim_v)\n\n    Returns:\n        list: Self-attention output as a list after reshaping to a list using tolist()\n              Values are rounded to 4 decimal places.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = attention_weights @ V\n    attention_output = np.round(attention_output, decimals=4)\n    return attention_output.tolist()"}
{"task_id": 53, "completion_id": 1, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism for a given input X.\n    \n    Parameters:\n    - X: Input sequence (sequence_length x features)\n    - W_q, W_k, W_v: Weight matrices for Query, Key, and Value\n    \n    Returns:\n    - Attention output as a Python list (rounded to 4 decimal places)\n    \"\"\"\n    d_k = W_q.shape[0]\n    Q = np.dot(X, W_q.T)\n    K = np.dot(X, W_k.T)\n    V = np.dot(X, W_v.T)\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    exp_scores = np.exp(scores)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = np.dot(attention_weights, V)\n    rounded_output = np.around(attention_output, decimals=4)\n    return rounded_output.tolist()"}
{"task_id": 53, "completion_id": 2, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = K.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    exp_scores = np.exp(scores)\n    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    output = attention_weights @ V\n    return np.round(output, 4).tolist()"}
{"task_id": 53, "completion_id": 3, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism.\n\n    Parameters:\n    X (np.ndarray): Input matrix of shape (seq_len, emb_dim)\n    W_q (np.ndarray): Weight matrix for queries of shape (emb_dim, d_k)\n    W_k (np.ndarray): Weight matrix for keys of shape (emb_dim, d_k)\n    W_v (np.ndarray): Weight matrix for values of shape (emb_dim, d_v)\n    \n    Returns:\n    np.ndarray: Output of self-attention as a NumPy array\n    \"\"\"\n    seq_len = X.shape[0]\n    d_k = W_q.shape[1]\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    exp_scores = np.exp(scores)\n    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return (output * 10000).round() / 10000.0"}
{"task_id": 53, "completion_id": 4, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    Parameters:\n    - X: Input of shape (seq_len, embed_dim)\n    - W_q: Query projection matrix of shape (embed_dim, embed_dim)\n    - W_k: Key projection matrix of shape (embed_dim, embed_dim)\n    - W_v: Value projection matrix of shape (embed_dim, embed_dim)\n    \n    Returns:\n    - attention_output: Result of self-attention as a Python list.\n    \"\"\"\n    (seq_len, embed_dim) = X.shape\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T\n    scale = np.sqrt(embed_dim)\n    scores /= scale\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    attention_output = attention_weights @ V\n    return attention_output.round(4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\nW_q = np.random.rand(3, 3)\nW_k = np.random.rand(3, 3)\nW_v = np.random.rand(3, 3)"}
{"task_id": 53, "completion_id": 5, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes self-attention mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (sequence_length, input_dimension)\n    W_q, W_k, W_v (np.ndarray): Query, Key, Value weight matrices of shape (input_dimension, output_dimension)\n    \n    Returns:\n    list: Self-attention output as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    (seq_len, input_dim) = X.shape\n    output_dim = W_q.shape[1]\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T\n    scaled_scores = scores / np.sqrt(output_dim)\n    exp_scores = np.exp(scaled_scores)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = attention_weights @ V\n    return np.round(attention_output, 4).tolist()"}
{"task_id": 53, "completion_id": 6, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention output.\n\n    Parameters:\n        X (np.ndarray): Input matrix of shape (n, d), where n is the sequence length and d is the embedding dimension.\n        W_q, W_k, W_v (np.ndarray): Weight matrices for query, key, and value respectively. Each of shape (d, d_k/d_v).\n\n    Returns:\n        np.ndarray: Self-attention output as a list after reshaping and rounding.\n    \"\"\"\n    (n, d) = X.shape\n    d_k = W_q.shape[0]\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=1)\n    output = attention_weights @ V\n    return output.round(4).tolist()"}
{"task_id": 53, "completion_id": 7, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute self-attention mechanism.\n    \n    Parameters:\n    - X: Input matrix of shape (sequence_length, embedding_dim)\n    - W_q: Query weight matrix of shape (embedding_dim, d)\n    - W_k: Key weight matrix of shape (embedding_dim, d)\n    - W_v: Value weight matrix of shape (embedding_dim, d)\n    \n    Returns:\n    - attention_output: Output of the self-attention as a list after reshaping\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T\n    d = K.shape[1]\n    scores_scaled = scores / np.sqrt(d)\n    exp_scores = np.exp(scores_scaled - np.max(scores_scaled, axis=1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    attention_output = attention_weights @ V\n    attention_output = np.round(attention_output, 4).tolist()\n    return attention_output"}
{"task_id": 53, "completion_id": 8, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention mechanism output.\n\n    Parameters:\n    - X (np.ndarray): Input sequence with shape (seq_len, input_dim)\n    - W_q (np.ndarray): Query weight matrix with shape (input_dim, feature_dim)\n    - W_k (np.ndarray): Key weight matrix with shape (input_dim, feature_dim)\n    - W_v (np.ndarray): Value weight matrix with shape (input_dim, feature_dim)\n\n    Returns:\n    - list: Self-attention output as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    scores = Q @ K.T\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x), keepdims=True), axis=1, arr=scores)\n    attention_output = attention_weights @ V\n    return attention_output.round(4).tolist()\nX = np.array([[1, 0, 1], [0, 2, 0], [1, 1, 0]])\nW_q = np.array([[0.1, 0.3], [0.2, 0.4], [0.5, 0.1]])\nW_k = np.array([[0.6, 0.2], [0.4, 0.3], [0.5, 0.1]])\nW_v = np.array([[0.3, 0.2], [0.1, 0.4], [0.5, 0.1]])"}
{"task_id": 53, "completion_id": 9, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = K.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.exp(scores)\n    attention_weights = attention_weights / np.sum(attention_weights, axis=1, keepdims=True)\n    output = attention_weights @ V\n    output = np.around(output, 4)\n    return output.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nW_q = np.array([[0.1, 0.2], [0.3, 0.4]])\nW_k = np.array([[0.1, 0.3], [0.2, 0.4]])\nW_v = np.array([[0.1, 0.2], [0.3, 0.4]])"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass of a simple RNN cell.\n    \n    Args:\n        input_sequence (list of lists): Sequence of input vectors.\n        initial_hidden_state (list): Initial hidden state vector.\n        Wx (list of lists): Weight matrix for input-to-hidden connections.\n        Wh (list of lists): Weight matrix for hidden-to-hidden connections.\n        b (list): Bias vector.\n    \n    Returns:\n        list: Final hidden state after processing the sequence, rounded to 4 decimal places.\n    \"\"\"\n    H_prev = np.array(initial_hidden_state, dtype=np.float64)\n    for X_t in input_sequence:\n        X_t = np.array(X_t, dtype=np.float64)\n        Wx_np = np.array(Wx, dtype=np.float64)\n        Wh_np = np.array(Wh, dtype=np.float64)\n        b_np = np.array(b, dtype=np.float64)\n        H_tilde = np.dot(Wx_np, X_t) + np.dot(Wh_np, H_prev) + b_np\n        H_prev = np.tanh(H_tilde)\n    return np.round(H_prev, 4).tolist()"}
{"task_id": 54, "completion_id": 1, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Performs a forward pass through a simple RNN cell.\n    \n    Parameters:\n    - input_sequence: A list of input vectors (each a list of floats).\n    - initial_hidden_state: The initial hidden state vector (a list of floats).\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - A list representing the final hidden state after processing the entire sequence.\n    \"\"\"\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        pre_activation = Wx @ x_t + Wh @ h_prev + b\n        h_prev = np.tanh(pre_activation)\n    final_hidden_state = np.round(h_prev, 4).tolist()\n    return final_hidden_state"}
{"task_id": 54, "completion_id": 2, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        h_prev = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n    return h_prev.round(4).tolist()"}
{"task_id": 54, "completion_id": 3, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Computes the RNN forward pass for a sequence.\n    \n    Parameters:\n    -----------\n    input_sequence: list of list of float\n        List of input vectors for each time step.\n    initial_hidden_state: list of float\n        The initial hidden state.\n    Wx: list of list of float\n        Weight matrix for input-to-hidden.\n    Wh: list of list of float\n        Weight matrix for hidden-to-hidden.\n    b: list of float\n        Bias vector.\n    \n    Returns:\n    --------\n    list of float\n        Final hidden state after processing the entire sequence.\n    \"\"\"\n    h_prev = np.array(initial_hidden_state, dtype=np.float64)\n    Wx = np.array(Wx, dtype=np.float64)\n    Wh = np.array(Wh, dtype=np.float64)\n    b = np.array(b, dtype=np.float64)\n    for x_t in input_sequence:\n        x_t = np.array(x_t, dtype=np.float64)\n        h_t = np.tanh(x_t @ Wx + h_prev @ Wh + b)\n        h_prev = h_t\n    return np.around(h_prev, decimals=4).tolist()\ninput_sequence = [[1.0, 0.5], [0.2, 0.3]]\ninitial_hidden_state = [0.1, 0.1]\nWx = [[1.0, 0.2], [0.3, 1.0]]\nWh = [[0.5, -0.1], [-0.2, 0.6]]\nb = [0.1, 0.1]"}
{"task_id": 54, "completion_id": 4, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Performs a forward pass through a simple RNN cell.\n\n    Parameters:\n    - input_sequence (list of list of floats): The sequence of input vectors.\n    - initial_hidden_state (list of floats): The initial hidden state vector.\n    - Wx (list of list of floats): Weight matrix for input-to-hidden connections.\n    - Wh (list of list of floats): Weight matrix for hidden-to-hidden connections.\n    - b (list of floats): Bias vector.\n\n    Returns:\n    - final_hidden_state (list of floats): The final hidden state after processing the sequence.\n    \"\"\"\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        h_next = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    final_hidden_state = np.round(h_prev, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]"}
{"task_id": 54, "completion_id": 5, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN forward pass and returns the final hidden state.\n\n    Args:\n        input_sequence: List of input vectors at each time step.\n        initial_hidden_state: Initial hidden state vector.\n        Wx: Weight matrix for input-to-hidden connections (hidden_size x input_size).\n        Wh: Weight matrix for hidden-to-hidden connections (hidden_size x hidden_size).\n        b: Bias vector (hidden_size).\n\n    Returns:\n        Final hidden state after processing the sequence.\n    \"\"\"\n    H_prev = np.array(initial_hidden_state)\n    for X_t in input_sequence:\n        X_t = np.array(X_t)\n        H_prev = np.tanh(np.dot(np.array(Wx), X_t) + np.dot(np.array(Wh), H_prev) + np.array(b))\n    return np.round(H_prev, 4).tolist()"}
{"task_id": 54, "completion_id": 6, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x_vec = np.array(x)\n        h_next = np.tanh(np.dot(Wx, x_vec) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 54, "completion_id": 7, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Perform the forward pass through a simple RNN cell.\n\n    Parameters:\n    - input_sequence: A list of input vectors of shape (sequence_length, input_size)\n    - initial_hidden_state: Initial hidden state vector of shape (hidden_size,)\n    - Wx: Weight matrix for input-to-hidden connections of shape (hidden_size, input_size)\n    - Wh: Weight matrix for hidden-to-hidden connections of shape (hidden_size, hidden_size)\n    - b: Bias vector of shape (hidden_size,)\n\n    Returns:\n    - final_hidden_state: The hidden state after processing all input vectors of shape (hidden_size,)\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        pre_activation = np.dot(Wx, x_t) + np.dot(Wh, hidden_state) + b\n        hidden_state = np.tanh(pre_activation)\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    return final_hidden_state"}
{"task_id": 54, "completion_id": 8, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Computes the forward pass of a simple RNN cell.\n\n    Args:\n        input_sequence: A list of input vectors, each is a list of floats.\n        initial_hidden_state: Initial hidden state as a list of floats.\n        Wx: Input-to-hidden weight matrix (np.ndarray).\n        Wh: Hidden-to-hidden weight matrix (np.ndarray).\n        b: Bias vector (np.ndarray).\n\n    Returns:\n        final_hidden_state: The final hidden state after processing the entire sequence.\n    \"\"\"\n    H_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t_array = np.array(x_t)\n        H = np.tanh(Wx @ x_t_array + Wh @ H_prev + b)\n        H_prev = H\n    return H_prev.round(4).tolist()\ninput_sequence = [[1.0, 0.5], [0.5, 1.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.2]"}
{"task_id": 54, "completion_id": 9, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a forward pass for a simple RNN.\n    \n    Parameters:\n    - input_sequence: list of input vectors, each of shape (input_dim, 1)\n    - initial_hidden_state: initial hidden state, of shape (hidden_dim, 1)\n    - Wx: weight matrix for input-to-hidden connections, of shape (hidden_dim, input_dim)\n    - Wh: weight matrix for hidden-to-hidden connections, of shape (hidden_dim, hidden_dim)\n    - b: bias vector, of shape (hidden_dim, 1)\n    \n    Returns:\n    - final_hidden_state: the final hidden state after processing the input sequence, of shape (hidden_dim, 1)\n    \"\"\"\n    h_prev = np.array(initial_hidden_state).reshape(-1, 1)\n    for x in input_sequence:\n        x_t = np.array(x).reshape(-1, 1)\n        pre_activation = np.dot(np.array(Wx), x_t) + np.dot(np.array(Wh), h_prev) + np.array(b)\n        h_prev = np.tanh(pre_activation)\n    return np.around(h_prev.flatten(), decimals=4).tolist()\ninput_sequence = [[0.1, 0.2], [0.3, 0.4]]\nWx = [[1.0, 0.5], [0.5, 1.0]]\nWh = [[0.8, 0.2], [0.2, 0.8]]\nb = [0.1, 0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    homogeneous_points = np.hstack([points_array, np.ones((points_array.shape[0], 1))])\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed_points = homogeneous_points @ translation_matrix.T\n    translated_points = transformed_points[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 5\nty = 10"}
{"task_id": 55, "completion_id": 1, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=float)\n    translation_vector = np.array([tx, ty], dtype=float)\n    translated_points = points_array + translation_vector\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 2, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=float)\n    translation = np.array([tx, ty])\n    translated_points = points_array + translation\n    return translated_points.tolist()\ntx = 2\nty = -1"}
{"task_id": 55, "completion_id": 3, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = translated_points[:, :2]\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 4, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of [x, y] points.\n    \n    Parameters:\n    points (list of lists): A list of [x, y] points.\n    tx (float): Translation amount in the x-direction.\n    ty (float): Translation amount in the y-direction.\n    \n    Returns:\n    list: Translated points as a list of [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    translation = np.array([tx, ty])\n    translated_points = points_array + translation\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 5, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((len(points_array), 1))))\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_2d_points = translated_points[:, :2]\n    return translated_2d_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]"}
{"task_id": 55, "completion_id": 6, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=np.float32)\n    homogeneous_points = np.column_stack([points_array, np.ones(len(points_array))])\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    translated_points = homogeneous_points @ translation_matrix.T\n    result = translated_points[:, :2].tolist()\n    return result"}
{"task_id": 55, "completion_id": 7, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=float)\n    translation_vector = np.array([tx, ty])\n    translated_points = points_array + translation_vector\n    return translated_points.tolist()"}
{"task_id": 55, "completion_id": 8, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n\n    Parameters:\n    points (list of list of float): A list of [x, y] coordinates.\n    tx (float): Translation distance in x-direction.\n    ty (float): Translation distance in y-direction.\n\n    Returns:\n    list of list of float: A new list of translated [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points, dtype=np.float64)\n    points_array = np.column_stack((points_array, np.ones(points_array.shape[0])))\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed = np.dot(points_array, translation_matrix.T)\n    translated_points = transformed[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3"}
{"task_id": 55, "completion_id": 9, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of points.\n    \n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation in x direction.\n    - ty: Translation in y direction.\n    \n    Returns:\n    - List of translated [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    x = points_array[:, 0]\n    y = points_array[:, 1]\n    x_translated = x + tx\n    y_translated = y + ty\n    translated_points_array = np.column_stack((x_translated, y_translated))\n    return translated_points_array.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 10\nty = 20"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two univariate normal distributions.\n    \n    Parameters:\n    -----------\n    mu_p : float\n        Mean of distribution P\n    sigma_p : float\n        Standard deviation of distribution P (must be > 0)\n    mu_q : float\n        Mean of distribution Q\n    sigma_q : float\n        Standard deviation of distribution Q (must be > 0)\n    \n    Returns:\n    --------\n    kl : float\n        KL divergence D_KL(P || Q)\n    \"\"\"\n    kl = np.log(sigma_q / sigma_p) + 0.5 * (sigma_p ** 2 / sigma_q ** 2 + (mu_q - mu_p) ** 2 / sigma_q ** 2 - 1)\n    return kl"}
{"task_id": 56, "completion_id": 1, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence from distribution P to Q,\n    where both are univariate normal distributions:\n      - P ~ N(mu_p, sigma_p^2)\n      - Q ~ N(mu_q, sigma_q^2)\n\n    Parameters:\n        mu_p (float): Mean of distribution P\n        sigma_p (float): Standard deviation of distribution P\n        mu_q (float): Mean of distribution Q\n        sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n        float: KL divergence D_KL(P || Q)\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 56, "completion_id": 2, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n    float: The KL divergence D_KL(P || Q)\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 56, "completion_id": 3, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes KL divergence between two univariate normal distributions P and Q.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q (reference distribution)\n    sigma_q (float): Standard deviation of distribution Q (reference distribution)\n    \n    Returns:\n    float: KL divergence D_KL(P || Q)\n    \"\"\"\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 56, "completion_id": 4, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two univariate normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n    float: KL divergence D_{KL}(P || Q)\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + 0.5 * (sigma_p ** 2 / sigma_q ** 2 + (mu_p - mu_q) ** 2 / sigma_q ** 2 - 1)"}
{"task_id": 56, "completion_id": 5, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two univariate normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n    float: The KL divergence of P from Q, KL(P || Q)\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl"}
{"task_id": 56, "completion_id": 6, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 56, "completion_id": 7, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two univariate normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of distribution P\n    - sigma_p: Standard deviation of distribution P\n    - mu_q: Mean of distribution Q\n    - sigma_q: Standard deviation of distribution Q\n    \n    Returns:\n    - KL divergence D_KL(P || Q) as a float\n    \"\"\"\n    return 0.5 * ((sigma_q ** 2 + (mu_q - mu_p) ** 2) / sigma_p ** 2 - 1 + 2 * np.log(sigma_p / sigma_q))"}
{"task_id": 56, "completion_id": 8, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two univariate normal distributions.\n    \n    Parameters:\n    - mu_p (float): mean of distribution P\n    - sigma_p (float): standard deviation of distribution P\n    - mu_q (float): mean of distribution Q\n    - sigma_q (float): standard deviation of distribution Q\n    \n    Returns:\n    - kl (float): KL divergence D_KL(P || Q)\n    \"\"\"\n    return 0.5 * ((sigma_q ** 2 + (mu_q - mu_p) ** 2) / sigma_p ** 2 - 1 + np.log(sigma_p ** 2 / sigma_q ** 2))"}
{"task_id": 56, "completion_id": 9, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two univariate normal distributions.\n    \n    Parameters:\n        mu_p (float): Mean of distribution P\n        sigma_p (float): Standard deviation of distribution P\n        mu_q (float): Mean of distribution Q\n        sigma_q (float): Standard deviation of distribution Q\n    \n    Returns:\n        float: The KL divergence D_KL(P || Q)\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Gauss-Seidel iterative method.\n    \n    Parameters:\n    - A: Square coefficient matrix (n x n)\n    - b: Right-hand side vector of size n\n    - n: Number of iterations\n    - x_ini: Optional initial guess for x; defaults to a vector of zeros\n    \n    Returns:\n    - x: Approximated solution vector after n iterations, rounded to 4 decimal places\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 1, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the linear system Ax = b using the Gauss-Seidel method for n iterations.\n\n    Parameters:\n    - A (list of lists or 2D array): Square coefficient matrix\n    - b (list or array): Right-hand side vector\n    - n (int): Number of iterations\n    - x_ini (list or array): Optional initial guess for x (default is vector of zeros)\n\n    Returns:\n    - x (list): Solution vector after n iterations, rounded to 4 decimal places\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()\nA = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\nb = [4, 7, 3]\nx = gauss_seidel(A, b, n=100)"}
{"task_id": 57, "completion_id": 2, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: Square matrix of coefficients (numpy array)\n    - b: Right-hand side vector (numpy array)\n    - n: Number of iterations\n    - x_ini: Optional initial guess (default is a zero vector)\n    \n    Returns:\n    - x: Approximated solution vector after n iterations (list)\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    (m, n_col) = A.shape\n    if m != n_col:\n        raise ValueError('Matrix A must be a square matrix.')\n    if b.shape[0] != m:\n        raise ValueError('Vector b must match the number of rows in A.')\n    x = np.zeros(m) if x_ini is None else np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(m):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 57, "completion_id": 3, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = len(b)\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(size):\n            s = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 4, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    x = np.zeros_like(b) if x_ini is None else np.array(x_ini, dtype=float)\n    m = A.shape[0]\n    for _ in range(n):\n        for i in range(m):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 5, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    - A: A square coefficient matrix (numpy.ndarray)\n    - b: The right-hand side vector (numpy.ndarray)\n    - n: Number of iterations\n    - x_ini: Optional initial guess for the solution vector (numpy.ndarray or None)\n    \n    Returns:\n    - x: The approximated solution vector (list) after n iterations\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Dimensions of A and b are not compatible.')\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            val = b[i]\n            for j in range(A.shape[1]):\n                if j != i:\n                    val -= A[i, j] * x[j]\n            x[i] = val / A[i, i]\n    return np.round(x, 4).tolist()\nA = [[4, -2, 1], [3, 6, -4], [2, -1, 5]]\nb = [0, 5, 3]\nx = gauss_seidel(A, b, n=50)"}
{"task_id": 57, "completion_id": 6, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 7, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    size = len(b)\n    for _ in range(n):\n        for i in range(size):\n            s = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 57, "completion_id": 8, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations using the Gauss-Seidel method.\n\n    Parameters:\n    A (list of lists or 2D array): Square matrix of coefficients.\n    b (list or 1D array): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (list or 1D array, optional): Initial guess for the solution vector.\n\n    Returns:\n    list: Approximated solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    m = len(b)\n    for _ in range(n):\n        for i in range(m):\n            s1 = sum((A[i][j] * x[j] for j in range(i)))\n            s2 = sum((A[i][j] * x[j] for j in range(i + 1, m)))\n            x[i] = (b[i] - s1 - s2) / A[i][i]\n    return np.round(x, 4).tolist()\nA = [[4, 1, -1], [1, 4, -1], [1, -1, 4]]\nb = [0, 5, 0]\nn = 50\nx_ini = [1, 1, 1]"}
{"task_id": 57, "completion_id": 9, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the system Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters:\n    A (np.ndarray): Square coefficient matrix.\n    b (np.ndarray): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (np.ndarray, optional): Initial guess vector. Defaults to zeros if not provided.\n\n    Returns:\n    list: The approximated solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    m = len(b)\n    for _ in range(n):\n        for i in range(m):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix of shape (n, n)\n    b (np.ndarray): Right-hand side vector of shape (n, )\n\n    Returns:\n    list: Solution vector x rounded to 4 decimals, as a Python list.\n    \"\"\"\n    n = len(b)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if abs(A[max_row, i]) < 1e-10:\n            raise ValueError('Matrix is singular or nearly singular.')\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 1, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Args:\n        A (list of lists or np.ndarray): Coefficient matrix.\n        b (list or np.ndarray): Right-hand side vector.\n    \n    Returns:\n        list: Solution vector x (rounded to 4 decimal places).\n    \"\"\"\n    n = len(A)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    for k in range(n):\n        max_row = np.argmax(np.abs(A[k:n, k])) + k\n        if A[max_row, k] == 0:\n            raise ValueError('Matrix is singular.')\n        A[[k, max_row]] = A[[max_row, k]]\n        b[[k, max_row]] = b[[max_row, k]]\n        for i in range(k + 1, n):\n            factor = A[i, k] / A[k, k]\n            A[i, k:] -= factor * A[k, k:]\n            b[i] -= factor * b[k]\n    x = np.zeros(n, dtype=float)\n    for i in reversed(range(n)):\n        if A[i, i] == 0:\n            raise ValueError('Matrix is singular.')\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 2, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j] = A[j] - factor * A[i]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 3, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system Ax = b using Gaussian Elimination with Partial Pivoting.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix\n        b (np.ndarray): Right-hand side vector\n    \n    Returns:\n        list: Solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = i + np.abs(Ab[i:n, i]).argmax()\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] = Ab[j, i:] - factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:])) / Ab[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 4, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype='float64', copy=True)\n    b = np.array(b, dtype='float64', copy=True)\n    n = len(b)\n    Ab = np.column_stack((A, b))\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        if max_row != i:\n            Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n, dtype='float64')\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 58, "completion_id": 5, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n        A (list of lists or np.ndarray): Coefficient matrix.\n        b (list or np.ndarray): Right-hand side vector.\n    \n    Returns:\n        x (list): Solution vector, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.column_stack((A, b))\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] = Ab[j, i:] - factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 6, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix (n x n)\n        b (np.ndarray): Constant vector (n x 1)\n    \n    Returns:\n        list: Solution vector (x) rounded to 4 decimal places as a Python list\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:n, i])) + i\n        if Ab[max_row, i] == 0:\n            raise ValueError('Matrix is singular (no unique solution).')\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i:-1], x[i + 1:])) / Ab[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 7, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.column_stack((A, b))\n    for i in range(n - 1):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n, dtype=float)\n    x[n - 1] = Ab[n - 1, n] / Ab[n - 1, n - 1]\n    for i in range(n - 2, -1, -1):\n        x[i] = (Ab[i, n] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    return [round(val, 4) for val in x.tolist()]\nA = [[2, 1, -1], [4, 1, 0], [-2, 2, 1]]\nb = [2, 3, 1]"}
{"task_id": 58, "completion_id": 8, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A -- Coefficient matrix (n x n list of lists)\n    b -- Right-hand side vector (list of n elements)\n    \n    Returns:\n    x -- Solution vector rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j] = A[j] - factor * A[i]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 9, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix (n x n)\n    b (np.ndarray): Constant vector (n x 1)\n    \n    Returns:\n    list: Solution vector (x) rounded to 4 decimal places\n    \"\"\"\n    n = A.shape[0]\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] = np.round(Ab[j, i:] - factor * Ab[i, i:], decimals=10)\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:])\n        x[i] /= Ab[i, i]\n        x[i] = round(x[i], 4)\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Parameters:\n        x (numpy array): Input sequence of shape (input_size, sequence_length)\n        initial_hidden_state (numpy array): Initial hidden state of shape (hidden_size, 1)\n        initial_cell_state (numpy array): Initial cell state of shape (hidden_size, 1)\n        \n        Returns:\n        tuple: (hidden_states, final_hidden_state, final_cell_state)\n        \"\"\"\n        sequence_length = x.shape[1]\n        hidden_states = np.zeros((self.hidden_size, sequence_length))\n        h = initial_hidden_state\n        c = initial_cell_state\n        for t in range(sequence_length):\n            xt = x[:, t].reshape(-1, 1)\n            concat = np.concatenate((xt, h), axis=0)\n            i = self._sigmoid(np.dot(self.Wi, concat) + self.bi)\n            f = self._sigmoid(np.dot(self.Wf, concat) + self.bf)\n            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c = f * c + i * cc\n            o = self._sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h = o * np.tanh(c)\n            hidden_states[:, t] = h.flatten()\n        hidden_states = np.round(hidden_states, 4).tolist()\n        final_hidden_state = np.round(h, 4).tolist()\n        final_cell_state = np.round(c, 4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\n\n    def _sigmoid(self, x):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 1, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        hidden_state = np.array(initial_hidden_state).reshape((self.hidden_size, 1))\n        cell_state = np.array(initial_cell_state).reshape((self.hidden_size, 1))\n        for x_t in x:\n            concat = np.hstack((x_t, hidden_state))\n            f_t = 1 / (1 + np.exp(-1 * (np.dot(self.Wf, concat) + self.bf)))\n            i_t = 1 / (1 + np.exp(-1 * (np.dot(self.Wi, concat) + self.bi)))\n            c_candidate = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            cell_state = f_t * cell_state + i_t * c_candidate\n            o_t = 1 / (1 + np.exp(-1 * (np.dot(self.Wo, concat) + self.bo)))\n            hidden_state = o_t * np.tanh(cell_state)\n            hidden_state_list = np.round(hidden_state, 4).tolist()\n            hidden_states.append(hidden_state_list)\n        hidden_states = [item for sublist in hidden_states for item in sublist]\n        final_hidden_state = np.round(hidden_state, 4).tolist()\n        final_cell_state = np.round(cell_state, 4).tolist()\n        return {'hidden_states': hidden_states, 'final_hidden_state': final_hidden_state, 'final_cell_state': final_cell_state}"}
{"task_id": 59, "completion_id": 2, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h = initial_hidden_state\n        c = initial_cell_state\n        h_sequence = []\n        for t in range(x.shape[1]):\n            x_h = np.concatenate((x[:, t], h), axis=0).reshape(-1, 1)\n            f = self._sigmoid(np.dot(self.Wf, x_h) + self.bf)\n            i = self._sigmoid(np.dot(self.Wi, x_h) + self.bi)\n            g = np.tanh(np.dot(self.Wc, x_h) + self.bc)\n            c = f * c + i * g\n            o = self._sigmoid(np.dot(self.Wo, x_h) + self.bo)\n            h = o * np.tanh(c)\n            h_sequence.append(np.round(h, 4))\n        h_sequence = np.dstack(h_sequence)\n        h_sequence = np.round(h_sequence, 4).tolist()\n        final_hidden_state = np.round(h, 4).tolist()\n        final_cell_state = np.round(c, 4).tolist()\n        return (h_sequence, final_hidden_state, final_cell_state)\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 3, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h = [initial_hidden_state]\n        c = [initial_cell_state]\n        for t in range(x.shape[1]):\n            xt = x[:, t].reshape(-1, 1)\n            ht_prev = h[t]\n            inputs = np.concatenate((xt, ht_prev), axis=0)\n            ft = self.sigmoid(np.dot(self.Wf, inputs) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, inputs) + self.bi)\n            cct = np.tanh(np.dot(self.Wc, inputs) + self.bc)\n            ct = ft * c[t] + it * cct\n            c.append(ct)\n            ot = self.sigmoid(np.dot(self.Wo, inputs) + self.bo)\n            ht = ot * np.tanh(ct)\n            h.append(ht)\n        h = [np.round(h_t, 4) for h_t in h]\n        c = [np.round(c_t, 4) for c_t in c]\n        return ([val.tolist() for val in h], h[-1].tolist(), c[-1].tolist())\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 4, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = np.zeros((self.hidden_size, sequence_length))\n        c = np.zeros((self.hidden_size, sequence_length))\n        h_states = []\n        c_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        h_states.append(h_t.tolist())\n        for t in range(sequence_length):\n            x_t = x[:, t].reshape(-1, 1)\n            z = np.row_stack((x_t, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, z) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, z) + self.bi)\n            g_t = np.tanh(np.dot(self.Wc, z) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, z) + self.bo)\n            c_t = f_t * c_t + i_t * g_t\n            h_t = o_t * np.tanh(c_t)\n            h_states.append(h_t.tolist())\n            c_states.append(c_t.tolist())\n        h_states_rounded = [[round(val, 4) for val in sublist] for sublist in h_states]\n        c_states_rounded = [[round(val, 4) for val in sublist] for sublist in c_states]\n        return (h_states_rounded, h_t.tolist(), c_t.tolist())\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 5, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = [initial_hidden_state]\n        cell_states = [initial_cell_state]\n        for xt in x:\n            ht_prev = hidden_states[-1]\n            concat = np.row_stack((xt, ht_prev))\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            ct_candidate = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ct = ft * cell_states[-1] + it * ct_candidate\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ht = ot * np.tanh(ct)\n            cell_states.append(ct)\n            hidden_states.append(ht)\n        hidden_states = [np.round(ht, 4).tolist() for ht in hidden_states]\n        final_hidden_state = np.round(hidden_states[-1], 4).tolist()\n        final_cell_state = np.round(cell_states[-1], 4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 6, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Args:\n            x: numpy array of shape (input_size, sequence_length)\n            initial_hidden_state: numpy array of shape (hidden_size, 1)\n            initial_cell_state: numpy array of shape (hidden_size, 1)\n            \n        Returns:\n            hidden_states: List of hidden states at each time step (sequence_length + 1, each shape (hidden_size, 1))\n            final_hidden_state: Final hidden state at the end of the sequence (shape (hidden_size, 1))\n            final_cell_state: Final cell state at the end of the sequence (shape (hidden_size, 1))\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = [initial_hidden_state.copy()]\n        c = [initial_cell_state.copy()]\n        hidden_states = [initial_hidden_state.copy()]\n        for t in range(sequence_length):\n            x_t = x[:, t].reshape(-1, 1)\n            concat = np.concatenate((x_t, h[-1]), axis=0)\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_candidate_t = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c[-1] + i_t * c_candidate_t\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            c.append(c_t)\n            h.append(h_t)\n            hidden_states.append(h_t)\n        hidden_states_rounded = [np.round(state, 4).tolist() for state in hidden_states]\n        final_hidden_state_rounded = np.round(h[-1], 4).tolist()\n        final_cell_state_rounded = np.round(c[-1], 4).tolist()\n        return (hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 7, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        x: A list of input matrices where each matrix is of shape (input_size, sequence_length)\n        initial_hidden_state: initial hidden state of shape (hidden_size, 1)\n        initial_cell_state: initial cell state of shape (hidden_size, 1)\n\n        Returns:\n        hidden_states: A list of hidden states at each time step\n        final_hidden_state: final hidden state\n        final_cell_state: final cell state\n        \"\"\"\n        x = np.concatenate(x, axis=1)\n        h = [initial_hidden_state]\n        c = [initial_cell_state]\n        hidden_states = []\n        for t in range(x.shape[1]):\n            xt = x[:, t].reshape(-1, 1)\n            ht_prev = h[-1]\n            ct_prev = c[-1]\n            concat = np.concatenate((xt, ht_prev), axis=0).reshape(-1, 1)\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            ct_candidate = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ct = ft * ct_prev + it * ct_candidate\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ht = ot * np.tanh(ct)\n            h.append(ht)\n            c.append(ct)\n            hidden_states.append(np.round(ht, 4))\n        final_hidden_state = np.round(h[-1], 4)\n        final_cell_state = np.round(c[-1], 4)\n        hidden_states = [h.tolist() for h in hidden_states]\n        final_hidden_state = final_hidden_state.tolist()\n        final_cell_state = final_cell_state.tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)"}
{"task_id": 59, "completion_id": 8, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        (seq_len, _) = x.shape\n        hidden_size = self.hidden_size\n        h = np.zeros((hidden_size, seq_len))\n        c = np.zeros((hidden_size, seq_len))\n        h[:, 0] = initial_hidden_state.reshape(hidden_size)\n        c[:, 0] = initial_cell_state.reshape(hidden_size)\n        for t in range(seq_len):\n            xt = x[t, :].reshape(self.input_size, 1)\n            ht_prev = h[:, t - 1].reshape(hidden_size, 1)\n            ct_prev = c[:, t - 1].reshape(hidden_size, 1)\n            concat = np.vstack((xt, ht_prev))\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            ct_hat = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ct = ft * ct_prev + it * ct_hat\n            c[:, t] = ct.reshape((hidden_size,))\n            ht = ot * np.tanh(ct)\n            h[:, t] = ht.reshape((hidden_size,))\n        hidden_states = np.round(h, 4)\n        final_hidden_state = np.round(h[:, -1], 4)\n        final_cell_state = np.round(c[:, -1], 4)\n        return (hidden_states.tolist(), final_hidden_state.tolist(), final_cell_state.tolist())\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 59, "completion_id": 9, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(x.shape[1]):\n            x_t = x[:, t].reshape(-1, 1)\n            concat = np.vstack((x_t, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            g_t = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            c_t = f_t * c_t + i_t * g_t\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(np.round(h_t, decimals=4))\n        hidden_states_list = [h.reshape(-1).tolist() for h in hidden_states]\n        return {'hidden_states': [np.round(h, decimals=4).tolist() for h in hidden_states], 'final_hidden_state': np.round(h_t, decimals=4).tolist(), 'final_cell_state': np.round(c_t, decimals=4).tolist()}"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores for the given query words across a corpus of documents.\n\n    Parameters:\n    - corpus: list of list of str, where each document is a list of words.\n    - query: list of str, the words for which to compute TF-IDF.\n\n    Returns:\n    - list of lists: TF-IDF scores for each query term in each document,\n                     rounded to five decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('Corpus must not be empty.')\n    num_docs = len(corpus)\n    vocabulary = set((word for doc in corpus for word in doc))\n    vocabulary.update(query)\n    vocabulary = list(vocabulary)\n    num_terms = len(vocabulary)\n    term_to_index = {term: i for (i, term) in enumerate(vocabulary)}\n    tf = np.zeros((num_docs, num_terms), dtype=np.float64)\n    for (doc_idx, doc) in enumerate(corpus):\n        if not doc:\n            continue\n        doc_len = len(doc)\n        term_counts = {}\n        for word in doc:\n            term_counts[word] = term_counts.get(word, 0) + 1\n        for (term, count) in term_counts.items():\n            tf[doc_idx, term_to_index[term]] = count / doc_len if doc_len > 0 else 0\n    df = np.zeros(num_terms, dtype=np.float64)\n    for doc in corpus:\n        if not doc:\n            continue\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df[term_to_index[term]] += 1\n    idf = np.log((num_docs + 1) / (df + 1)) + 1.0\n    tf_idf = tf.dot(np.diag(idf))\n    query_indices = [term_to_index[word] for word in query]\n    result = tf_idf[:, query_indices]\n    result = np.round(result, 5)\n    return result.tolist()\ncorpus = [['hello', 'world'], ['hello', 'there', 'world'], ['hi', 'there', 'everyone']]\nquery = ['hello', 'there', 'hi']"}
{"task_id": 60, "completion_id": 1, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Cannot compute TF-IDF.')\n    all_terms = set((word for doc in corpus for word in doc))\n    query_terms = set(query)\n    query_terms = query_terms.intersection(all_terms)\n    if not query_terms:\n        return [[0.0] * len(query_terms)] * len(corpus)\n    tf = []\n    for doc in corpus:\n        term_freq = {}\n        for word in doc:\n            term_freq[word] = term_freq.get(word, 0) + 1\n        doc_tf = [term_freq.get(term, 0) / len(doc) if doc else 0 for term in query_terms]\n        tf.append(doc_tf)\n    df = {term: 0 for term in query_terms}\n    for doc in corpus:\n        doc_terms = set(doc)\n        for term in query_terms:\n            if term in doc_terms:\n                df[term] += 1\n    num_docs = len(corpus)\n    idf = {}\n    for term in query_terms:\n        idf[term] = np.log((num_docs + 1) / (df[term] + 1))\n    tf_idf = []\n    for i in range(num_docs):\n        doc_scores = [round(tf[i][idx] * idf[term], 5) for (idx, term) in enumerate(query_terms)]\n        tf_idf.append(doc_scores)\n    return tf_idf\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'lazy', 'dog'], ['the', 'quick', 'dog']]\nquery = ['quick', 'dog', 'the']"}
{"task_id": 60, "completion_id": 2, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute the TF-IDF scores for a query against a given corpus of documents.\n\n    Parameters:\n    corpus (list of list of str): List of documents, where each document is a list of words.\n    query (list of str): List of words for which to compute the TF-IDF scores.\n\n    Returns:\n    list: A list of lists containing the TF-IDF scores for the query words in each document,\n          rounded to 5 decimal places.\n    \"\"\"\n    if not corpus:\n        raise ValueError('The corpus is empty. Cannot compute TF-IDF scores.')\n    all_terms = set()\n    for doc in corpus:\n        all_terms.update(doc)\n    all_terms.update(query)\n    num_docs = len(corpus)\n    tf_scores = []\n    df_counts = {}\n    for doc in corpus:\n        term_freq = {}\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df_counts[term] = df_counts.get(term, 0) + 1\n        for term in all_terms:\n            term_freq[term] = doc.count(term) / len(doc) if len(doc) > 0 else 0\n        tf_scores.append(term_freq)\n    idf_scores = {}\n    for term in all_terms:\n        df = df_counts.get(term, 0)\n        idf_scores[term] = np.log((num_docs + 1) / (df + 1)) + 1e-08\n    tf_idf_results = []\n    for tf in tf_scores:\n        document_tfidf = []\n        for word in query:\n            document_tfidf.append(tf.get(word, 0) * idf_scores.get(word, 0))\n        tf_idf_results.append(document_tfidf)\n    output = np.round(np.array(tf_idf_results), 5).tolist()\n    return output"}
{"task_id": 60, "completion_id": 3, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Please provide at least one document.')\n    vocabulary = set()\n    for doc in corpus:\n        vocabulary.update(doc)\n    vocabulary.update(query)\n    vocabulary = list(vocabulary)\n    vocab_size = len(vocabulary)\n    tf = np.zeros((len(corpus), vocab_size))\n    df = np.zeros(vocab_size)\n    for (idx, word) in enumerate(vocabulary):\n        for (doc_id, doc) in enumerate(corpus):\n            count = doc.count(word)\n            tf[doc_id, idx] = count / len(doc) if len(doc) > 0 else 0\n            if count > 0:\n                df[idx] += 1\n    num_docs = len(corpus)\n    idf = np.log((num_docs + 1) / (df + 1)) + 1\n    tf_idf = np.zeros((len(corpus), len(query)))\n    for (q_idx, word) in enumerate(query):\n        if word not in vocabulary:\n            tf_idf[:, q_idx] = 0.0\n        else:\n            vocab_idx = vocabulary.index(word)\n            tf_idf[:, q_idx] = tf[:, vocab_idx] * idf[vocab_idx]\n    return np.round(tf_idf, 4).tolist()"}
{"task_id": 60, "completion_id": 4, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for the given query across the provided corpus.\n\n    Args:\n        corpus (list of list of str): List of documents, each document is a list of words.\n        query (list of str): List of words to compute TF-IDF scores for.\n\n    Returns:\n        list of list of float: A matrix where each row corresponds to a document,\n                              and each column corresponds to a query word.\n    \"\"\"\n    if not corpus:\n        return []\n    tf = []\n    for doc in corpus:\n        if not doc:\n            tf.append(np.zeros(len(query)))\n            continue\n        freq = {}\n        for word in doc:\n            freq[word] = freq.get(word, 0) + 1\n        tf_doc = [freq.get(term, 0) / len(doc) for term in query]\n        tf.append(tf_doc)\n    num_docs = len(corpus)\n    df = {}\n    for term in query:\n        df[term] = sum((1 for doc in corpus if term in doc))\n    idf = [np.log((num_docs + 1) / (df.get(term, 0) + 1)) for term in query]\n    tf_idf = np.array(tf) * np.array(idf)\n    return np.round(tf_idf, 4).tolist()"}
{"task_id": 60, "completion_id": 5, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Cannot compute TF-IDF.')\n    all_terms = set()\n    for doc in corpus:\n        all_terms.update(doc)\n    vocab = list(all_terms)\n    vocab_to_index = {word: idx for (idx, word) in enumerate(vocab)}\n    df = np.zeros(len(vocab))\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            if term in vocab_to_index:\n                df[vocab_to_index[term]] += 1\n    N = len(corpus)\n    idf = np.log((N + 1) / (df + 1))\n    valid_query = [term for term in query if term in vocab_to_index]\n    if not valid_query:\n        return [[0.0] * len(valid_query)] * N\n    term_indices = [vocab_to_index[term] for term in valid_query]\n    M = len(term_indices)\n    tf_idf = np.zeros((N, M))\n    for (doc_idx, doc) in enumerate(corpus):\n        if not doc:\n            tf_vec = np.zeros(len(term_indices))\n        else:\n            tf = {}\n            for term in doc:\n                tf[term] = tf.get(term, 0) + 1\n            tf_vec = [tf.get(term, 0) / len(doc) for term in valid_query]\n        tf_idf[doc_idx, :] = np.array(tf_vec) * idf[term_indices]\n    return np.around(tf_idf, decimals=4).tolist()"}
{"task_id": 60, "completion_id": 6, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty!')\n    vocab = set()\n    for doc in corpus:\n        vocab.update(doc)\n    vocab.update(query)\n    vocab = list(vocab)\n    num_documents = len(corpus)\n    num_terms = len(vocab)\n    tf_matrix = np.zeros((num_documents, num_terms))\n    for (doc_idx, doc) in enumerate(corpus):\n        term_counts = np.zeros(num_terms)\n        for word in doc:\n            for (term_idx, term) in enumerate(vocab):\n                if word == term:\n                    term_counts[term_idx] += 1\n        term_freqs = term_counts / max(1, len(doc))\n        tf_matrix[doc_idx, :] = term_freqs\n    df_vector = np.zeros(num_terms)\n    for (term_idx, term) in enumerate(vocab):\n        count = 0\n        for doc in corpus:\n            if term in doc:\n                count += 1\n        df_vector[term_idx] = np.log((1 + num_documents) / (1 + count)) + 1\n    tf_idf_matrix = tf_matrix * df_vector\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()"}
{"task_id": 60, "completion_id": 7, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        return []\n    vocabulary = set()\n    for doc in corpus:\n        vocabulary.update(doc)\n    vocabulary.update(query)\n    vocabulary = list(vocabulary)\n    num_documents = len(corpus)\n    num_terms = len(vocabulary)\n    num_query_terms = len(query)\n    term_to_idx = {term: i for (i, term) in enumerate(vocabulary)}\n    query_to_idx = {term: i for (i, term) in enumerate(query)}\n    tf_idf = np.zeros((num_documents, num_query_terms))\n    df = np.zeros(num_terms)\n    for doc in corpus:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df[term_to_idx[term]] += 1\n    for (doc_idx, doc) in enumerate(corpus):\n        doc_len = len(doc)\n        for term in doc:\n            if term in query_to_idx:\n                tf = doc.count(term) / doc_len\n                idf = np.log((num_documents + 1) / (df[term_to_idx[term]] + 1) + 1)\n                tf_idf[doc_idx, query_to_idx[term]] += tf * idf\n    return np.round(tf_idf, 4).tolist()"}
{"task_id": 60, "completion_id": 8, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus is empty. Cannot compute TF-IDF scores.')\n    if not query:\n        return []\n    query_terms = set(query)\n    df = {}\n    for term in query_terms:\n        df[term] = sum((1 for doc in corpus if term in doc))\n    num_docs = len(corpus)\n    idf = {}\n    for term in query_terms:\n        idf[term] = np.log((num_docs + 1) / (df[term] + 1)) + 1e-08\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query_terms:\n            tf = doc.count(term) / max(1, len(doc))\n            tf_idf = tf * idf[term]\n            doc_scores.append(tf_idf)\n        tf_idf_scores.append(doc_scores)\n    return np.round(np.array(tf_idf_scores), 5).tolist()"}
{"task_id": 60, "completion_id": 9, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty. Cannot compute TF-IDF.')\n    vocabulary = set()\n    for doc in corpus:\n        vocabulary.update(doc)\n    vocabulary.update(query)\n    vocabulary = list(vocabulary)\n    vocab_size = len(vocabulary)\n    num_docs = len(corpus)\n    word_to_index = {word: i for (i, word) in enumerate(vocabulary)}\n    tf_idf_matrix = np.zeros((num_docs, vocab_size))\n    df = np.zeros(vocab_size)\n    for (i, word) in enumerate(vocabulary):\n        for doc in corpus:\n            if word in doc:\n                df[i] += 1\n    idf = np.log((num_docs + 1) / (df + 1)) + 1\n    for (doc_idx, doc) in enumerate(corpus):\n        word_counts = {}\n        total_words = len(doc)\n        for word in doc:\n            word_counts[word] = word_counts.get(word, 0) + 1\n        for (word, count) in word_counts.items():\n            if total_words == 0:\n                continue\n            word_idx = word_to_index[word]\n            tf = count / total_words\n            tf_idf_matrix[doc_idx, word_idx] = tf * idf[word_idx]\n    tf_idf_matrix = np.round(tf_idf_matrix, 4)\n    return tf_idf_matrix.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall\n    f_score = numerator / denominator if denominator > 0 else 0\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 1, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_pos = np.sum((y_true == 1) & (y_pred == 1))\n    false_pos = np.sum((y_true == 0) & (y_pred == 1))\n    false_neg = np.sum((y_true == 1) & (y_pred == 0))\n    if true_pos + false_pos == 0:\n        precision = 0.0\n    else:\n        precision = true_pos / (true_pos + false_pos)\n    if true_pos + false_neg == 0:\n        recall = 0.0\n    else:\n        recall = true_pos / (true_pos + false_neg)\n    if precision * recall == 0 or beta ** 2 * precision + recall == 0:\n        return 0.0\n    f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 61, "completion_id": 2, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred should be NumPy arrays.')\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('y_true and y_pred must contain only binary values (0 or 1).')\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) if precision + recall > 0 else 0\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 3, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = tp / (tp + fp + 1e-10)\n    recall = tp / (tp + fn + 1e-10)\n    score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall + 1e-10)\n    return round(score, 3)"}
{"task_id": 61, "completion_id": 4, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall > 0 else 0\n    return round(f_score, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 0])"}
{"task_id": 61, "completion_id": 5, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    fscore = (1 + beta ** 2) * (precision * recall / (beta ** 2 * precision + recall)) if precision + recall > 0 else 0.0\n    return round(fscore, 3)"}
{"task_id": 61, "completion_id": 6, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall if beta ** 2 * precision + recall > 0 else 0.0\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(numerator / denominator, 3)"}
{"task_id": 61, "completion_id": 7, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if beta ** 2 * precision + recall != 0 else 0\n    return round(f_score, 3)"}
{"task_id": 61, "completion_id": 8, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_positives = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    false_negatives = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0.0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0.0\n    f_score_val = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall) if precision + recall > 0 else 0.0\n    return round(f_score_val, 3)"}
{"task_id": 61, "completion_id": 9, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    true_pos = np.sum((y_true == 1) & (y_pred == 1))\n    false_pos = np.sum((y_true == 0) & (y_pred == 1))\n    false_neg = np.sum((y_true == 1) & (y_pred == 0))\n    precision = true_pos / (true_pos + false_pos) if true_pos + false_pos > 0 else 0\n    recall = true_pos / (true_pos + false_neg) if true_pos + false_neg > 0 else 0\n    numerator = (1 + beta ** 2) * precision * recall\n    denominator = beta ** 2 * precision + recall\n    fscore = numerator / denominator if denominator > 0 else 0\n    return round(fscore, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Args:\n            x: 2D array of shape (input_size, sequence_length)\n        Returns:\n            outputs: output of the RNN at each time step\n            hiddens: hidden states at each time step\n            inputs: the reshaped input vectors for use in backward pass\n        \"\"\"\n        T = x.shape[1]\n        h = np.zeros((self.hidden_size, T + 1))\n        y = np.zeros((self.W_hy.shape[0], T))\n        h[:, 0] = np.zeros((self.hidden_size,))\n        for t in range(T):\n            h[:, t] = np.tanh(np.dot(self.W_xh, x[:, t]) + np.dot(self.W_hh, h[:, t - 1]) + self.b_h)\n            y[:, t] = np.dot(self.W_hy, h[:, t]) + self.b_y\n        inputs = x.T\n        last_inputs = inputs[-1]\n        last_hiddens = h[:, -1]\n        return (y, h, inputs, last_inputs, last_hiddens)\n\n    def backward(self, x_seq, expected_output, outputs, hiddens, inputs, last_inputs, last_hiddens, learning_rate=0.01):\n        \"\"\"\n        Backward pass using BPTT.\n        Args:\n            x_seq: Input sequence (T, input_size)\n            expected_output: Expected output (output_size, T)\n            outputs: Computed outputs (output_size, T)\n            hiddens: Hidden states (hidden_size, T+1)\n            inputs: Reshaped inputs (T, input_size)\n            last_inputs: Last input in sequence\n            last_hiddens: Last hidden state\n            learning_rate: Learning rate for weight updates\n        \"\"\"\n        dh_next = np.zeros_like(hiddens[:, 0])\n        T = len(inputs)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        for t in reversed(range(T)):\n            dy = outputs[:, t] - expected_output[:, t]\n            dW_hy += np.dot(dy, hiddens[:, t].reshape((-1, 1)).T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dhraw = (1 - hiddens[:, t] ** 2) * dh\n            db_h += dhraw\n            dW_xh += np.dot(dhraw, x_seq[t, :].reshape((-1, 1)))\n            dW_hh += np.dot(dhraw, hiddens[:, t - 1].reshape((-1, 1)))\n            dh_next = dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, x_seq, y_seq, learning_rate=0.01):\n        \"\"\"\n        Train the RNN on one sequence.\n        Args:\n            x_seq: Input sequence (sequence_length, input_size)\n            y_seq: Expected output (sequence_length, output_size)\n            learning_rate: Learning rate\n        \"\"\"\n        x = x_seq.T\n        y_expected = y_seq.T\n        (y, h, inputs, last_inputs, last_hiddens) = self.forward(x)\n        self.backward(x_seq, y_expected, y, h, inputs, last_inputs, last_hiddens, learning_rate)\n\n    def predict(self, x, last_hidden=None):\n        \"\"\"\n        Predict the output given the input.\n        \"\"\"\n        if last_hidden is None:\n            h = np.zeros((self.hidden_size, 1))\n        else:\n            h = last_hidden\n        outputs = []\n        for input_sample in x:\n            h = np.tanh(np.dot(self.W_xh, input_sample) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n        return np.array(outputs)"}
{"task_id": 62, "completion_id": 1, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def softmax(self, z):\n        \"\"\"Compute softmax of input z.\"\"\"\n        exps = np.exp(z - np.max(z, axis=0))\n        return exps / exps.sum(axis=0)\n\n    def tanh(self, z):\n        \"\"\"Compute the tanh activation function.\"\"\"\n        return np.tanh(z)\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n        - input_sequence: a 2D array of shape (input_size, sequence_length)\n        \n        Returns:\n        - outputs: output at each time step\n        - hiddens: hidden states at each time step\n        - inputs: the input sequence\n        \"\"\"\n        sequence_length = input_sequence.shape[1]\n        hiddens = np.zeros((self.hidden_size, sequence_length + 1))\n        outputs = np.zeros((self.W_hy.shape[0], sequence_length))\n        h_next = hiddens[:, 0]\n        inputs = input_sequence\n        for t in range(sequence_length):\n            h_next = np.tanh(np.dot(self.W_xh, inputs[:, t:t + 1]) + np.dot(self.W_hh, h_next) + self.b_h)\n            hiddens[:, t + 1:t + 2] = h_next\n            out = np.dot(self.W_hy, h_next) + self.b_y\n            outputs[:, t:t + 1] = out\n        return (outputs, hiddens, inputs)\n\n    def compute_loss(self, outputs, targets):\n        \"\"\"\n        Compute the loss using 1/2 * Mean Squared Error.\n        \n        Args:\n        - outputs: predicted outputs from forward pass\n        - targets: true targets\n        \n        Returns:\n        - loss: average loss over the sequence\n        \"\"\"\n        loss = 0.5 * np.sum((outputs - targets) ** 2)\n        return loss\n\n    def backward(self, input_sequence, targets, outputs, hiddens, learning_rate=0.01):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights.\n        \n        Args:\n        - input_sequence: sequence of input vectors\n        - targets: sequence of target vectors\n        - outputs: outputs from the forward pass\n        - hiddens: hidden states from the forward pass\n        - learning_rate: learning rate for weight updates\n        \n        Returns:\n        - Updated weights and biases\n        \"\"\"\n        sequence_length = input_sequence.shape[1]\n        input_size = input_sequence.shape[0]\n        output_size = outputs.shape[0]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        def dtanh(x):\n            return 1 - np.tanh(x) ** 2\n        dh_next = np.zeros_like(hiddens[:, 0])\n        for t in reversed(range(sequence_length)):\n            dy = outputs[:, t:t + 1] - targets[:, t:t + 1]\n            dW_hy += np.dot(dy, hiddens[:, t:t + 1].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dtanh_h = dtanh(hiddens[:, t + 1:t + 2])\n            dhraw = dh * dtanh_h\n            db_h += dhraw\n            dW_xh += np.dot(dhraw, input_sequence[:, t:t + 1].T)\n            dW_hh += np.dot(dhraw, hiddens[:, t:t + 1].T)\n            dh_next = np.dot(self.W_hh.T, dhraw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 62, "completion_id": 2, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Returns: outputs, inputs_sequence, hiddens_sequence\n        \"\"\"\n        (batch_size, seq_len, input_dim) = input_sequence.shape\n        outputs = np.zeros((seq_len, output_dim))\n        hiddens = np.zeros((seq_len, hidden_size))\n        inputs_sequence = input_sequence.reshape(seq_len, input_dim)\n        h = np.zeros((hidden_size, 1))\n        for t in range(seq_len):\n            x_t = inputs_sequence[t, np.newaxis].T\n            h = np.tanh(self.W_xh @ x_t + self.W_hh @ h + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            outputs[t] = y.ravel()\n            hiddens[t] = h.ravel()\n        return (outputs, hiddens, inputs_sequence)\n\n    def backward(self, input_sequence, expected_output, learning_rate):\n        \"\"\"\n        Backward pass using backpropagation through time.\n        \"\"\"\n        (batch_size, seq_len, input_dim) = input_sequence.shape\n        (outputs, hiddens, inputs_sequence) = self.forward(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0, :])\n        for t in reversed(range(seq_len)):\n            dy = outputs[t] - expected_output[t]\n            dW_hy += dy[np.newaxis, np.newaxis] @ hiddens[t, :][np.newaxis, np.newaxis].T\n            db_y += dy[:, np.newaxis]\n            dh = self.W_hy.T @ dy[np.newaxis, np.newaxis].T + dh_next\n            dh_raw = (1 - hiddens[t] ** 2) * dh\n            dx = self.W_xh.T @ dh_raw\n            dW_xh += dh_raw @ inputs_sequence[t, np.newaxis][np.newaxis, np.newaxis].T\n            dW_hh += dh_raw @ hiddens[t - 1 if t > 0 else 0, np.newaxis][np.newaxis, np.newaxis].T\n            db_h += dh_raw\n            dh_next = dh_raw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        loss = 0.5 * np.mean((outputs - expected_output) ** 2)\n        return loss"}
{"task_id": 62, "completion_id": 3, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            \n        Returns:\n            outputs: Outputs at each time step of shape (output_size, sequence_length)\n            hiddens: Hidden states at each time step of shape (hidden_size, sequence_length)\n        \"\"\"\n        sequence_length = x.shape[1]\n        hiddens = np.zeros((self.hidden_size, sequence_length))\n        outputs = np.zeros((self.W_hy.shape[0], sequence_length))\n        h_prev = np.zeros((self.hidden_size, 1))\n        for t in range(sequence_length):\n            h_t = np.tanh(np.dot(self.W_xh, x[:, t].reshape(-1, 1)) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n            hiddens[:, t] = h_t.ravel()\n            outputs[:, t] = y_t.ravel()\n            h_prev = h_t\n        return (outputs, hiddens)\n\n    def backward(self, x, y, outputs, hiddens, learning_rate):\n        \"\"\"\n        Backward pass using BPTT for the RNN.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            y: Target sequence of shape (output_size, sequence_length)\n            outputs: Outputs from the forward pass\n            hiddens: Hidden states from the forward pass\n            learning_rate: Learning rate for gradient descent\n            \n        Returns:\n            loss: Total loss across all time steps\n        \"\"\"\n        sequence_length = x.shape[1]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[:, 0])\n        loss = 0\n        for t in reversed(range(sequence_length)):\n            dy = outputs[:, t:t + 1] - y[:, t:t + 1]\n            dW_hy += np.dot(dy, hiddens[:, t:t + 1].T)\n            db_y += dy\n            dhidden = np.dot(self.W_hy.T, dy) + dh_next\n            dtanh = (1 - hiddens[:, t:t + 1] ** 2) * dhidden\n            dW_xh += np.dot(dtanh, x[:, t:t + 1].T)\n            dW_hh += np.dot(dtanh, hiddens[:, t:t + 1].T)\n            db_h += dtanh\n            dh_next = np.dot(self.W_hh.T, dtanh)\n            loss += 0.5 * np.sum(dy ** 2)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\n\n    def train(self, input_sequence, expected_output, learning_rate):\n        \"\"\"\n        Train the RNN on a sequence.\n        \n        Args:\n            input_sequence: Input sequence of shape (input_size, sequence_length)\n            expected_output: Expected output sequence of shape (output_size, sequence_length)\n            learning_rate: Learning rate for weight updates\n            \n        Returns:\n            loss: Total loss for the sequence\n        \"\"\"\n        (outputs, hiddens) = self.forward(input_sequence)\n        loss = self.backward(input_sequence, expected_output, outputs, hiddens, learning_rate)\n        return loss"}
{"task_id": 62, "completion_id": 4, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Parameters:\n        - x: a numpy array of shape (input_size, sequence_length)\n        Returns:\n        - outputs: outputs at each time step (output_size, sequence_length)\n        - hiddens: hidden states at each time step (hidden_size, sequence_length)\n        - inputs: input sequence\n        \"\"\"\n        sequence_length = x.shape[1]\n        hiddens = np.zeros((self.hidden_size, sequence_length))\n        outputs = np.zeros((self.W_hy.shape[0], sequence_length))\n        h = np.zeros((self.hidden_size, 1))\n        for t in range(sequence_length):\n            x_t = x[:, t].reshape(-1, 1)\n            h = np.tanh(self.W_xh @ x_t + self.W_hh @ h + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            hiddens[:, t] = h.flatten()\n            outputs[:, t] = y.flatten()\n        return (outputs, hiddens, x, h)\n\n    def backward(self, x, y, outputs, hiddens, learning_rate, sequence_length):\n        \"\"\"\n        Backward pass using BPTT.\n        Parameters:\n        - x: input sequence (input_size, sequence_length)\n        - y: expected output (output_size, sequence_length)\n        - outputs: predicted outputs (output_size, sequence_length)\n        - hiddens: hidden states (hidden_size, sequence_length)\n        - learning_rate: learning rate for gradient descent\n        - sequence_length: length of the input sequence\n        Returns:\n        - dW_xh: gradient of weight matrix W_xh\n        - dW_hh: gradient of weight matrix W_hh\n        - dW_hy: gradient of weight matrix W_hy\n        - db_h: gradient of bias b_h\n        - db_y: gradient of bias b_y\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        for t in reversed(range(sequence_length)):\n            x_t = x[:, t].reshape(-1, 1)\n            h_t = hiddens[:, t].reshape(-1, 1)\n            y_t = outputs[:, t].reshape(-1, 1)\n            dy = y_t - y[:, t].reshape(-1, 1)\n            dW_hy += dy @ h_t.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dhraw = (1 - h_t ** 2) * dh\n            dW_xh += dhraw @ x_t.T\n            dW_hh += dhraw @ hiddens[:, max(0, t - 1)].reshape(-1, 1).T\n            db_h += dhraw\n            dh_next = dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        loss = 0.5 * np.sum(dy ** 2)\n        return loss\n\n    def train(self, input_sequence, expected_output, learning_rate, epochs):\n        \"\"\"\n        Trains the RNN using backpropagation through time (BPTT).\n        Parameters:\n        - input_sequence: a numpy array of shape (input_size, sequence_length)\n        - expected_output: a numpy array of shape (output_size, sequence_length)\n        - learning_rate: learning rate for gradient descent\n        - epochs: number of training iterations\n        \"\"\"\n        sequence_length = input_sequence.shape[1]\n        for _ in range(epochs):\n            (outputs, hiddens, inputs, last_h) = self.forward(input_sequence)\n            loss = self.backward(input_sequence, expected_output, outputs, hiddens, learning_rate, sequence_length)\n            if _ % 100 == 0:\n                print(f'Epoch {_}: Loss = {loss}')"}
{"task_id": 62, "completion_id": 5, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.tanh(self.W_xh @ x + self.W_hh @ self.h_prev + self.b_h)\n        y = self.W_hy @ h + self.b_y\n        outputs = {'h': h, 'y': y}\n        self.h_prev = h\n        return (y, outputs)\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the outputs, inputs, and hidden states.\n        \"\"\"\n        n_steps = input_sequence.shape[1]\n        outputs = []\n        hiddens = []\n        inputs = []\n        self.h_prev = np.zeros((self.hidden_size, 1))\n        for i in range(n_steps):\n            x = input_sequence[:, i:i + 1]\n            (y, outputs_dict) = self.forward(x)\n            outputs.append(y)\n            hiddens.append(outputs_dict['h'])\n            inputs.append(x)\n        return (np.dstack(outputs), np.dstack(hiddens), np.dstack(inputs))\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, hiddens, inputs, learning_rate):\n        \"\"\"\n        Performs backward pass through the RNN using BPTT.\n        \"\"\"\n        n_steps = input_sequence.shape[1]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n        self.h_prev = hiddens[-1]\n        for t in reversed(range(n_steps)):\n            y = outputs[:, :, t]\n            h = hiddens[:, :, t]\n            x = inputs[:, :, t]\n            dy = np.copy(y)\n            dy -= expected_output[:, :, t]\n            dW_hy += dy @ h.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h ** 2) * dh\n            dW_hh += dh_raw @ hiddens[t - 1].T if t > 0 else dh_raw @ self.h_prev.T\n            db_h += dh_raw\n            dW_xh += dh_raw @ x.T\n            dh_next = self.W_hh.T @ dh_raw\n        for param in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(param, -1, 1, out=param)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate=0.01):\n        \"\"\"\n        Trains the RNN on a sequence of inputs and targets using BPTT.\n        Returns the list of loss values per step.\n        \"\"\"\n        (outputs, hiddens, inputs) = self.rnn_forward(input_sequence)\n        self.rnn_backward(input_sequence, expected_output, outputs, hiddens, inputs, learning_rate)"}
{"task_id": 62, "completion_id": 6, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            x (np.array): Input sequence of shape (input_size, sequence_length)\n        \n        Returns:\n            np.array: Outputs at each time step of shape (output_size, sequence_length)\n            list: Last input\n            list: Hidden states at each time step\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = np.zeros((self.hidden_size, 1))\n        h_sequence = [h]\n        outputs = []\n        inputs = []\n        for t in range(sequence_length):\n            x_t = x[:, t].reshape(-1, 1)\n            inputs.append(x_t)\n            h = np.tanh(self.W_xh @ x_t + self.W_hh @ h + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            h_sequence.append(h)\n            outputs.append(y)\n        return (np.hstack(outputs), inputs, h_sequence)\n\n    def backward(self, x, y, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT (Backpropagation Through Time).\n        \n        Args:\n            x (np.array): Input sequence of shape (input_size, sequence_length)\n            y (np.array): Expected output sequence of shape (output_size, sequence_length)\n            learning_rate (float): Learning rate for gradient descent\n            \n        Returns:\n            float: Total loss over the sequence\n        \"\"\"\n        output_size = y.shape[0]\n        sequence_length = x.shape[1]\n        (outputs, inputs, h_sequence) = self.forward(x)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(h_sequence[0])\n        loss = 0\n        for t in reversed(range(sequence_length)):\n            x_t = inputs[t]\n            h_prev = h_sequence[t]\n            h_current = h_sequence[t + 1]\n            dy = outputs[:, t].reshape(-1, 1) - y[:, t].reshape(-1, 1)\n            loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += dy @ h_current.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h_current ** 2) * dh\n            dW_xh += dh_raw @ x_t.T\n            db_h += dh_raw\n            dW_hh += dh_raw @ h_prev.T\n            dh_next = self.W_hh @ dh_raw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\n\n    def train(self, X, y, learning_rate=0.01, epochs=1000):\n        \"\"\"\n        Train the RNN on sequence data.\n        \n        Args:\n            X (np.array): Input data of shape (input_size, num_sequences, sequence_length)\n            y (np.array): Target data of shape (output_size, num_sequences, sequence_length)\n            learning_rate (float): Learning rate for gradient descent\n            epochs (int): Number of training iterations\n        \"\"\"\n        for i in range(epochs):\n            total_loss = 0\n            for j in range(X.shape[1]):\n                x = X[:, j, :]\n                y_seq = y[:, j, :]\n                loss = self.backward(x, y_seq, learning_rate)\n                total_loss += loss\n            if i % 100 == 0:\n                print(f'Epoch {i} Loss: {total_loss}')\n\n    def predict(self, x):\n        \"\"\"\n        Predict the output for a given input sequence.\n        \n        Args:\n            x (np.array): Input sequence\n            \n        Returns:\n            np.array: Predicted output sequence\n        \"\"\"\n        (outputs, _, _) = self.forward(x)\n        return outputs"}
{"task_id": 62, "completion_id": 7, "solution": "import numpy as np\ndef tanh_derivative(x):\n    return 1.0 - np.tanh(x) ** 2\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        Inputs:\n            x: 2D input array (input_size x sequence_length)\n        Returns:\n            outputs: outputs of the RNN for each time step (output_size x sequence_length)\n            last_inputs: the last inputs for backprop\n            last_hiddens: hidden states at each time step\n        \"\"\"\n        input_sequence = x\n        T = input_sequence.shape[1]\n        h = np.zeros((self.hidden_size, T))\n        x = input_sequence\n        outputs = np.zeros((self.W_hy.shape[0], T))\n        h[:, -1] = np.zeros((self.hidden_size, 1))\n        for t in range(T):\n            h[:, t] = np.tanh(self.W_xh @ x[:, t].reshape(-1, 1) + self.W_hh @ h[:, t - 1].reshape(-1, 1) + self.b_h).flatten()\n            outputs[:, t] = (self.W_hy @ h[:, t].reshape(-1, 1) + self.b_y).flatten()\n        return (outputs, x, h)\n\n    def backward(self, input_sequence, expected_output, learning_rate=0.01):\n        \"\"\"\n        Backward pass (BPTT) through the RNN.\n        \"\"\"\n        T = input_sequence.shape[1]\n        (outputs, last_inputs, last_hiddens) = self.forward(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        for t in reversed(range(T)):\n            dy = outputs[:, t] - expected_output[:, t]\n            dW_hy += dy.reshape(-1, 1) @ last_hiddens[:, t].reshape(1, -1)\n            db_y += dy.reshape(-1, 1)\n            dh = self.W_hy.T @ dy.reshape(-1, 1) + dh_next\n            dtanh = tanh_derivative(self.W_xh @ last_inputs[:, t].reshape(-1, 1) + self.W_hh @ last_hiddens[:, t - 1].reshape(-1, 1) + self.b_h)\n            dW_xh += dtanh * (last_inputs[:, t].reshape(-1, 1) @ last_hiddens[:, t].reshape(1, -1))\n            dW_hh += dtanh * (last_hiddens[:, t - 1].reshape(-1, 1) @ last_hiddens[:, t].reshape(1, -1))\n            db_h += dtanh\n            dh_next = dh\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h.flatten()[:, np.newaxis]\n        self.b_y -= learning_rate * db_y\n        total_loss = 0.5 * np.sum((outputs - expected_output) ** 2)\n        return total_loss"}
{"task_id": 62, "completion_id": 8, "solution": "import numpy as np\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\ndef tanh_derivative(x):\n    \"\"\"Derivative of the tanh function.\"\"\"\n    return 1.0 - x ** 2\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            input_sequence (list of arrays): List of input vectors.\n\n        Returns:\n            outputs (list of arrays): List of output vectors at each time step.\n            hidden_states (list of arrays): Sequence of hidden layers by time step.\n            inputs (list of arrays): Sequence of input layers by time step.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        hidden_states = [h]\n        outputs = []\n        inputs = []\n        for x in input_sequence:\n            x = x.reshape(-1, 1)\n            inputs.append(x)\n            h = tanh(self.W_xh @ x + self.W_hh @ h + self.b_h)\n            hidden_states.append(h)\n            y = self.W_hy @ h + self.b_y\n            outputs.append(y)\n        return (outputs, hidden_states, inputs)\n\n    def backward(self, input_sequence, expected_output, outputs, hidden_states, inputs, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to train the network.\n        \n        Args:\n            input_sequence (list of arrays): List of input vectors.\n            expected_output (list of arrays): List of expected output vectors.\n            outputs (list of arrays): List of predicted output vectors.\n            hidden_states (list of arrays): List of hidden vectors.\n            inputs (list of arrays): List of input vectors used in forward pass.\n            learning_rate (float): Learning rate for gradient descent.\n\n        Returns:\n            loss (float): Total loss for the sequence.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros((self.hidden_size, 1))\n        loss = 0.0\n        for t in reversed(range(T)):\n            dy = outputs[t] - expected_output[t]\n            loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += dy @ hidden_states[t].T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dtanh = tanh_derivative(hidden_states[t + 1])\n            dhraw = dh * dtanh\n            dW_xh += dhraw @ inputs[t].T\n            dW_hh += dhraw @ hidden_states[t].T\n            db_h += dhraw\n            dh_next = self.W_hh.T @ dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 62, "completion_id": 9, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Performs the forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            input_sequence: A 2D array of shape (input_size, sequence_length)\n        \n        Returns:\n            outputs: A list of output vectors for each time step\n            x_list: A list of input vectors for each time step\n            h_list: A list of hidden states for each time step\n        \"\"\"\n        sequence_length = input_sequence.shape[1]\n        outputs = []\n        x_list = []\n        h_list = []\n        h_t = np.zeros((self.hidden_size, 1))\n        for t in range(sequence_length):\n            x_t = input_sequence[:, t].reshape(-1, 1)\n            x_list.append(x_t)\n            h_list.append(h_t)\n            h_t = np.tanh(np.dot(self.W_xh, x_t) + np.dot(self.W_hh, h_t) + self.b_h)\n            y_t = np.dot(self.W_hy, h_t) + self.b_y\n            outputs.append(y_t)\n        return (outputs, x_list, h_list)\n\n    def backward(self, input_sequence, expected_output, outputs, x_list, h_list, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to update the weights.\n        \n        Args:\n            input_sequence: A 2D array of shape (input_size, sequence_length)\n            expected_output: A 2D array of shape (output_size, sequence_length)\n            outputs: A list of output vectors from the forward pass\n            x_list: A list of input vectors for each time step\n            h_list: A list of hidden states for each time step\n            learning_rate: The learning rate for weight updates\n        \n        Returns:\n            gradients and updated weights (not explicitly returned - modified in-place)\n        \"\"\"\n        sequence_length = len(outputs)\n        input_size = input_sequence.shape[0]\n        output_size = expected_output.shape[0]\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(h_list[0])\n        for t in reversed(range(sequence_length)):\n            y_t = outputs[t]\n            expected_y_t = expected_output[:, t].reshape(-1, 1)\n            dy = y_t - expected_y_t\n            dW_hy += np.dot(dy, h_list[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy)\n            dh += dh_next\n            dhraw = (1 - h_list[t] ** 2) * dh\n            db_h += dhraw\n            dW_xh += np.dot(dhraw, x_list[t].T)\n            dW_hh += np.dot(dhraw, h_list[t].T)\n            dh_next = dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    @staticmethod\n    def initialize_weights(input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \n        Returns:\n            W_xh, W_hh, W_hy, b_h, b_y: The initialized weights and biases\n        \"\"\"\n        W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        b_h = np.zeros((hidden_size, 1))\n        b_y = np.zeros((output_size, 1))\n        return (W_xh, W_hh, W_hy, b_h, b_y)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    m = len(b)\n    if x0 is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x0)\n    r = b - A @ x\n    d = r.copy()\n    r_norm_squared = r @ r\n    for i in range(n):\n        if np.sqrt(r_norm_squared) < tol:\n            break\n        Ad = A @ d\n        alpha = r_norm_squared / (d @ Ad)\n        x = x + alpha * d\n        r_new = r - alpha * Ad\n        r_new_norm_squared = r_new @ r_new\n        beta = r_new_norm_squared / r_norm_squared\n        d = r_new + beta * d\n        r_norm_squared = r_new_norm_squared\n        r = r_new\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nx = conjugate_gradient(A, b, n=100)"}
{"task_id": 63, "completion_id": 1, "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x, rounded to 8 decimal places and converted to list\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new) < tol:\n            break\n        p = r + rs_new / rs_old * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\nx = conjugate_gradient(A, b, n)"}
{"task_id": 63, "completion_id": 2, "solution": "import numpy as np\nimport numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded and converted to list)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    r_norm_squared = np.dot(r, r)\n    for i in range(n):\n        Ap = A @ p\n        alpha = r_norm_squared / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        new_r_norm_squared = np.dot(r, r)\n        if np.sqrt(new_r_norm_squared) < tol:\n            break\n        beta = new_r_norm_squared / r_norm_squared\n        p = r + beta * p\n        r_norm_squared = new_r_norm_squared\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]], dtype=np.float64)\nb = np.array([1, 2], dtype=np.float64)"}
{"task_id": 63, "completion_id": 3, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x rounded to 8 decimal places and converted to a list\n    \"\"\"\n    (m, p) = A.shape\n    if m != p:\n        raise ValueError('Matrix A must be square (number of rows must equal number of columns)')\n    if m != b.shape[0]:\n        raise ValueError('Matrix A and vector b must have compatible dimensions')\n    if x0 is None:\n        x = np.zeros(p)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    d = r.copy()\n    rs_old = r @ r\n    for i in range(n):\n        if np.sqrt(rs_old) < tol:\n            break\n        Ad = A @ d\n        alpha = rs_old / (d @ Ad)\n        x = x + alpha * d\n        r = r - alpha * Ad\n        rs_new = r @ r\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        d = r + beta * d\n        rs_old = rs_new\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 4, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = r @ r\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rsold / (p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = r @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 5, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    m = len(b)\n    if x0 is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x0, copy=True)\n    r = b - A @ x\n    d = r.copy()\n    delta_new = np.dot(r, r)\n    for i in range(n):\n        if delta_new < tol ** 2:\n            break\n        q = A @ d\n        alpha = delta_new / np.dot(d, q)\n        x += alpha * d\n        r -= alpha * q\n        delta_old = delta_new\n        delta_new = np.dot(r, r)\n        beta = delta_new / delta_old\n        d = r + beta * d\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 6, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to a list)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = r @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / (p @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    else:\n        print(f'Warning: Conjugate Gradient did not converge in {n} iterations.')\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 7, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded and converted to a list)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = r @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rsold / (p @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = r @ r\n        if np.sqrt(rsnew) < tol:\n            break\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]], dtype=np.float64)\nb = np.array([1, 2], dtype=np.float64)"}
{"task_id": 63, "completion_id": 8, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    d = r.copy()\n    rsold = r @ r\n    for i in range(n):\n        if np.sqrt(rsold) < tol:\n            break\n        Ad = A @ d\n        alpha = rsold / (d @ Ad)\n        x = x + alpha * d\n        r = r - alpha * Ad\n        rsnew = r @ r\n        beta = rsnew / rsold\n        d = r + beta * d\n        rsold = rsnew\n    return np.round(x, 8).tolist()"}
{"task_id": 63, "completion_id": 9, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimals and as list)\n    \"\"\"\n    A = np.asarray(A)\n    b = np.asarray(b)\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x0, dtype=np.float64)\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = r @ r\n        if np.sqrt(rs_new) < tol:\n            break\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    n = len(y)\n    prob_squares = sum(((count / n) ** 2 for count in counts.values()))\n    gini = 1 - prob_squares\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 1, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    total_samples = len(y)\n    counts = {}\n    for label in y:\n        counts[label] = counts.get(label, 0) + 1\n    gini = 1.0\n    for count in counts.values():\n        prob = count / total_samples\n        gini -= prob * prob\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 2, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    from collections import Counter\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    n_samples = len(y)\n    gini = 1.0\n    for count in counts.values():\n        probability = count / n_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 3, "solution": "import numpy as np\nfrom typing import List\ndef gini_impurity(y: List[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = np.unique(y, return_counts=True)[1]\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 4, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = np.unique(y, return_counts=True)[1]\n    total = len(y)\n    gini = 1.0 - sum(((count / total) ** 2 for count in counts))\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 5, "solution": "import numpy as np\nfrom typing import List\ndef gini_impurity(y: List[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    total = len(y)\n    sum_p_squared = 0\n    for count in counts:\n        p = count / total\n        sum_p_squared += p ** 2\n    gini = 1 - sum_p_squared\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 6, "solution": "import numpy as np\nfrom typing import List\ndef gini_impurity(y: List[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 7, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    unique_classes = np.unique(y)\n    n_samples = len(y)\n    gini = 1.0\n    for cls in unique_classes:\n        p_cls = np.sum(y == cls) / n_samples\n        gini -= p_cls ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List\ndef gini_impurity(y: List[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    class_counts = {}\n    for label in y:\n        if label in class_counts:\n            class_counts[label] += 1\n        else:\n            class_counts[label] = 1\n    n = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        prob = count / n\n        gini -= prob ** 2\n    return round(gini, 3)"}
{"task_id": 64, "completion_id": 9, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    unique_classes = np.unique(y)\n    n = len(y)\n    gini = 1.0\n    for cls in unique_classes:\n        p = y.count(cls) / n\n        gini -= p ** 2\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or any((len(row) != len(dense_matrix[0]) for row in dense_matrix)):\n        raise ValueError('Input must be a non-empty 2D matrix with uniform row lengths.')\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        row_pointer.append(row_pointer[-1] + count)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [5, 0, 0, 0], [0, 2, 0, 7]]"}
{"task_id": 65, "completion_id": 1, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        row_count = 0\n        for (col_idx, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                row_count += 1\n        row_pointer.append(row_pointer[-1] + row_count)\n    return (values, column_indices, row_pointer)"}
{"task_id": 65, "completion_id": 2, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                count += 1\n        row_pointer.append(row_pointer[-1] + count)\n    return (values, column_indices, row_pointer)"}
{"task_id": 65, "completion_id": 3, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col_idx)\n                count += 1\n        row_ptr.append(row_ptr[-1] + count)\n    return (values, col_indices, row_ptr)"}
{"task_id": 65, "completion_id": 4, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 5, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        row_pointers.append(row_pointers[-1] + count)\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 6, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        nnz_in_row = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                nnz_in_row += 1\n        row_pointers.append(row_pointers[-1] + nnz_in_row)\n    return (values, column_indices, row_pointers)"}
{"task_id": 65, "completion_id": 7, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_index, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_index)\n                count += 1\n        row_pointer.append(row_pointer[-1] + count)\n    return (values, column_indices, row_pointer)"}
{"task_id": 65, "completion_id": 8, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    m = len(dense_matrix)\n    n = len(dense_matrix[0])\n    values = []\n    column_indices = []\n    row_pointer = [0] * (m + 1)\n    for i in range(m):\n        count = 0\n        for j in range(n):\n            value = dense_matrix[i][j]\n            if value != 0:\n                values.append(value)\n                column_indices.append(j)\n                count += 1\n        row_pointer[i + 1] = row_pointer[i] + count\n    return (values, column_indices, row_pointer)"}
{"task_id": 65, "completion_id": 9, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_index, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_array = np.array(v)\n    L_array = np.array(L)\n    dot_vL = np.dot(v_array, L_array)\n    dot_LL = np.dot(L_array, L_array)\n    if dot_LL == 0:\n        raise ValueError('Projection cannot be computed: L is a zero vector.')\n    scalar_projection = dot_vL / dot_LL\n    projection_vector = scalar_projection * L_array\n    return [round(x, 3) for x in projection_vector.tolist()]"}
{"task_id": 66, "completion_id": 1, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_vL = sum((x * y for (x, y) in zip(v, L)))\n    dot_LL = sum((x * y for (x, y) in zip(L, L)))\n    if dot_LL == 0:\n        raise ValueError('The line vector L cannot be a zero vector.')\n    scale_factor = dot_vL / dot_LL\n    proj = [round(scale_factor * x, 3) for x in L]\n    return proj"}
{"task_id": 66, "completion_id": 2, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_vL = sum((x * y for (x, y) in zip(v, L)))\n    dot_LL = sum((x * x for x in L))\n    scalar = dot_vL / dot_LL\n    projection = [round(scalar * x, 3) for x in L]\n    return projection"}
{"task_id": 66, "completion_id": 3, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    if dot_product_LL == 0:\n        raise ValueError('The line vector L cannot be a zero vector.')\n    scalar_proj = dot_product_vL / dot_product_LL\n    proj_vector = scalar_proj * L\n    return list(np.round(proj_vector, 3))"}
{"task_id": 66, "completion_id": 4, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = [float(x) for x in v]\n    L = [float(x) for x in L]\n    dot_product_vL = sum((vox * lox for (vox, lox) in zip(v, L)))\n    dot_product_LL = sum((lox * lox for lox in L))\n    if dot_product_LL == 0:\n        raise ValueError('The vector L cannot be a zero vector for projection.')\n    scale = dot_product_vL / dot_product_LL\n    proj_vector = [scale * li for li in L]\n    proj_vector = [round(x, 3) for x in proj_vector]\n    return proj_vector"}
{"task_id": 66, "completion_id": 5, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of floats)\n    :param L: The line vector defining the direction of projection (list of floats)\n    :return: List representing the projection of v onto L, rounded to 3 decimal places\n    \"\"\"\n    import numpy as np\n    v_arr = np.array(v, dtype=float)\n    L_arr = np.array(L, dtype=float)\n    dot_vL = np.dot(v_arr, L_arr)\n    dot_LL = np.dot(L_arr, L_arr)\n    if dot_LL == 0:\n        return [0.0] * len(v)\n    proj_scalar = dot_vL / dot_LL\n    proj_vector = proj_scalar * L_arr\n    return [round(x, 3) for x in proj_vector.tolist()]"}
{"task_id": 66, "completion_id": 6, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_v_L = sum((x * y for (x, y) in zip(v, L)))\n    dot_L_L = sum((x * x for x in L))\n    if dot_L_L == 0:\n        return [0.0] * len(v)\n    scalar_projection = dot_v_L / dot_L_L\n    projection = [scalar_projection * x for x in L]\n    projection_rounded = [round(x, 3) for x in projection]\n    return projection_rounded"}
{"task_id": 66, "completion_id": 7, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v_array = np.array(v)\n    L_array = np.array(L)\n    dot_v_L = np.dot(v_array, L_array)\n    dot_L_L = np.dot(L_array, L_array)\n    if dot_L_L == 0:\n        raise ValueError('The line vector L cannot be a zero vector.')\n    proj_scalar = dot_v_L / dot_L_L\n    proj_vector = proj_scalar * L_array\n    return [round(component, 3) for component in proj_vector]\nv = [3, 4]\nL = [1, 2]"}
{"task_id": 66, "completion_id": 8, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product_v_L = sum((x * y for (x, y) in zip(v, L)))\n    dot_product_L_L = sum((x * x for x in L))\n    if dot_product_L_L == 0:\n        raise ValueError('The direction vector L cannot be a zero vector.')\n    scalar_projection = dot_product_v_L / dot_product_L_L\n    projection = [round(scalar_projection * x, 3) for x in L]\n    return projection"}
{"task_id": 66, "completion_id": 9, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_vL = sum((x * y for (x, y) in zip(v, L)))\n    dot_LL = sum((x * x for x in L))\n    if dot_LL == 0:\n        raise ValueError('The vector L cannot be a zero vector.')\n    scalar_projection = dot_vL / dot_LL\n    projection = [scalar_projection * x for x in L]\n    return [round(x, 3) for x in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n        - values: List of non-zero elements in column-major order\n        - row_indices: List of row indices corresponding to each value\n        - col_ptr: List indicating the starting index of each column in values\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        count = 0\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                count += 1\n        col_ptr.append(col_ptr[-1] + count)\n    return (values, row_indices, col_ptr)"}
{"task_id": 67, "completion_id": 1, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        col_values = []\n        col_row_indices = []\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                col_values.append(value)\n                col_row_indices.append(row)\n        values.extend(col_values)\n        row_indices.extend(col_row_indices)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 2, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        count = 0\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                count += 1\n        column_pointer.append(column_pointer[-1] + count)\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 3, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        count = 0\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                count += 1\n        column_pointer.append(column_pointer[-1] + count)\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 4, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        raise ValueError('Input matrix must not be empty')\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 67, "completion_id": 5, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        count = 0\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                count += 1\n        column_pointer.append(column_pointer[-1] + count)\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 6, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 7, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    col_count = 0\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n                col_count += 1\n        column_pointer.append(col_count)\n    return (values, row_indices, column_pointer)"}
{"task_id": 67, "completion_id": 8, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_pointer.append(len(values))\n    return (values, row_indices, col_pointer)"}
{"task_id": 67, "completion_id": 9, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of a matrix A using row echelon form.\n\n    Parameters:\n    A (list of lists / numpy array): The input matrix as a 2D array.\n\n    Returns:\n    list: A list of basis vectors (as lists) that span the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    rank = np.linalg.matrix_rank(A)\n    (rref, pivots) = sympy.Matrix(A).rref()\n    pivots = list(pivots)\n    basis_columns = A[:, pivots]\n    return np.round(basis_columns, 8).tolist()"}
{"task_id": 68, "completion_id": 1, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (rows, cols) = A.shape\n    pivot_cols = []\n    matrix = A.copy()\n    for col in range(cols):\n        pivot_row = None\n        for r in range(rows):\n            if abs(matrix[r, col]) > 1e-10:\n                pivot_row = r\n                break\n        if pivot_row is None:\n            continue\n        if pivot_row != 0:\n            matrix[[0, pivot_row], :] = matrix[[pivot_row, 0], :]\n        pivot_value = matrix[0, col]\n        matrix[0, :] /= pivot_value\n        for r in range(rows):\n            if r != 0 and abs(matrix[r, col]) > 1e-10:\n                factor = matrix[r, col]\n                matrix[r, :] -= factor * matrix[0, :]\n        matrix = matrix[1:, :]\n        rows = matrix.shape[0]\n        pivot_cols.append(col)\n    image_basis = A[:, pivot_cols]\n    return np.round(image_basis, 8).tolist()"}
{"task_id": 68, "completion_id": 2, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (rows, cols) = A.shape\n    B = A.copy()\n    pivot_cols = []\n    col_indices = list(range(cols))\n    col_permutation = list(range(cols))\n    pivot_index = 0\n    for col in range(cols):\n        if pivot_index >= rows:\n            break\n        non_zero_row = None\n        for r in range(pivot_index, rows):\n            if abs(B[r, col]) > 1e-10:\n                non_zero_row = r\n                break\n        if non_zero_row is None:\n            continue\n        B[[pivot_index, non_zero_row]] = B[[non_zero_row, pivot_index]]\n        (col_permutation[pivot_index], col_permutation[non_zero_row]) = (col_permutation[non_zero_row], col_permutation[pivot_index])\n        pivot_row = B[pivot_index]\n        pivot_val = pivot_row[col]\n        pivot_row /= pivot_val\n        for r in range(pivot_index + 1, rows):\n            factor = B[r, col]\n            B[r] = B[r] - factor * pivot_row\n        pivot_cols.append(col)\n        pivot_index += 1\n    image_basis = A[:, [col_permutation[i] for i in pivot_cols]]\n    return np.round(image_basis, 8).tolist()"}
{"task_id": 68, "completion_id": 3, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of matrix A using Row Reduced Echelon Form.\n    \n    Parameters:\n        A (np.ndarray): The input matrix.\n        \n    Returns:\n        list: List of column vectors (basis of column space) rounded to 8 decimals.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (rref, pivots) = (np.linalg.qr(A, mode='reduced')[0], np.array([]))\n    tolerance = 1e-08\n    for i in range(rref.shape[0]):\n        col = rref[i, :]\n        non_zero = np.where(np.abs(col) > tolerance)[0]\n        if len(non_zero) > 0:\n            pivots = np.append(pivots, non_zero[0])\n    pivot_columns = pivots.astype(int)\n    independent_columns = A[:, pivot_columns]\n    return np.round(independent_columns, 8).tolist()"}
{"task_id": 68, "completion_id": 4, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, pivots) = np.linalg.matrix_rank(A, tol=1e-08, return_rank=True)\n    pivot_cols = np.linalg.matrix_rank(np.hstack([A[:, :i + 1] for i in range(A.shape[1])]), tol=1e-08, return_rank=True)[1]\n    image_basis = A[:, pivot_cols]\n    return np.round(image_basis, 8).tolist()"}
{"task_id": 68, "completion_id": 5, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Computes the basis for the column space (image) of a matrix A.\n    \n    Args:\n        A (np.ndarray or list): A 2D matrix.\n        \n    Returns:\n        A list of basis vectors (rounded to 8 decimal places) spanning the image of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    B = A.copy()\n    n = B.shape[1]\n    pivot_cols = []\n    r = 0\n    for c in range(n):\n        if r >= B.shape[0]:\n            break\n        if np.any(np.abs(B[r:, c]) > 1e-10):\n            pivot_row = np.argmax(np.abs(B[r:, c])) + r\n            B[[r, pivot_row]] = B[[pivot_row, r]]\n            B[r] = B[r] / B[r, c]\n            B[r + 1:, :] -= B[r, np.newaxis] * B[r + 1:, c, np.newaxis]\n            r += 1\n    for i in range(r):\n        for j in range(n):\n            if np.allclose(B[i, j], 1) and np.all(np.isclose(B[i + 1:, j], 0)):\n                pivot_cols.append(j)\n                break\n    image_basis = A[:, pivot_cols].tolist()\n    return np.round(image_basis, 8).tolist()\nA = [[1, 2, 3, 4], [2, 4, 6, 8], [3, 6, 9, 12]]"}
{"task_id": 68, "completion_id": 6, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=np.float64)\n    (_, pivots) = np.linalg.matrix_rank(A, tol=1e-10, return_rank=True)\n    image_basis = A[:, pivots]\n    return np.round(image_basis, 8).tolist()"}
{"task_id": 68, "completion_id": 7, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Returns the basis vectors of the column space (image) of matrix A.\n    \n    Parameters:\n    A (np.ndarray): Input matrix (2D), with shape (m, n)\n    \n    Returns:\n    list: List of basis vectors (columns from original matrix A), rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    M = np.hstack([A, np.eye(n)])\n    from sympy import Matrix as SymMatrix\n    import sympy\n    sym_A = SymMatrix(A)\n    (rref, _) = sym_A.rref()\n    rref = np.array(rref).astype(float)\n    pivot_cols = []\n    for i in range(min(rref.shape)):\n        if not (rref[i, :] == np.zeros_like(rref[i, :])).all():\n            pivot_cols.append(i)\n    image_basis = A[:, pivot_cols]\n    image_basis = np.round(image_basis, 8)\n    return image_basis.tolist()"}
{"task_id": 68, "completion_id": 8, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Computes the basis for the column space (image) of a matrix A.\n    Independent columns from the original matrix are returned as a list.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    B = A.copy()\n    rank = np.linalg.matrix_rank(B)\n    pivot_cols = []\n    for i in range(n):\n        if np.any(B[:, i] != 0):\n            pivot_cols.append(i)\n            if len(pivot_cols) == rank:\n                break\n    image_basis = A[:, pivot_cols]\n    return np.around(image_basis, decimals=8).tolist()"}
{"task_id": 68, "completion_id": 9, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Computes the basis for the column space (image) of the matrix A.\n    \n    Parameters:\n    A (list of lists or np.ndarray): The input matrix.\n    \n    Returns:\n    list: A list of basis vectors (rounded to 8 decimal places).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (m, n) = A.shape\n    (rref, pivot_indices) = rref_with_pivot(A)\n    basis_columns = A[:, pivot_indices]\n    return [col.tolist() for col in basis_columns.T]\ndef rref_with_pivot(A):\n    \"\"\"\n    Transforms matrix A into reduced row echelon form (rref) and returns\n    the pivot indices.\n\n    Returns:\n    rref: The reduced row echelon form of A\n    pivot_cols: Indices of the pivot columns\n    \"\"\"\n    A = A.copy()\n    (m, n) = A.shape\n    pivot_cols = []\n    pivot_row = 0\n    for col in range(n):\n        max_row = np.argmax(np.abs(A[pivot_row:, col])) + pivot_row\n        if A[max_row, col] != 0:\n            A[[pivot_row, max_row]] = A[[max_row, pivot_row]]\n            A[pivot_row] = A[pivot_row] / A[pivot_row, col]\n            for r in range(m):\n                if r != pivot_row and A[r, col] != 0:\n                    A[r] = A[r] - A[r, col] * A[pivot_row]\n            pivot_cols.append(col)\n            pivot_row += 1\n        if pivot_row >= m:\n            break\n    return (A, np.array(pivot_cols))"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression analysis.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    mean_y_true = np.mean(y_true)\n    total_sum_of_squares = np.sum((y_true - mean_y_true) ** 2)\n    residual_sum_of_squares = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - residual_sum_of_squares / total_sum_of_squares\n    return np.round(r2, 3)"}
{"task_id": 69, "completion_id": 1, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 2, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) value.\n    \n    Parameters:\n    y_true (array-like): Array of true values.\n    y_pred (array-like): Array of predicted values.\n    \n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 3, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) value.\n\n    Parameters:\n    y_true (array-like): True values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_residual = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_residual / ss_total\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 4, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) value.\n    \n    Parameters:\n    y_true (array-like): True (observed) values.\n    y_pred (array-like): Predicted values.\n    \n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_residual = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_residual / ss_total\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 5, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    mean_y_true = np.mean(y_true)\n    tss = np.sum((y_true - mean_y_true) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 6, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    total_sum_squares = np.sum((y_true - y_mean) ** 2)\n    residual_sum_squares = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - residual_sum_squares / total_sum_squares\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 7, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 8, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    if tss == 0:\n        return 1.0 if rss == 0 else 0.0\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 69, "completion_id": 9, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    rows = len(img)\n    cols = len(img[0])\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        if len(row) != cols:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 1, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0])\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 2, "solution": "def calculate_brightness(img):\n    if not img or not any(img):\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0])\n    for row in img:\n        if len(row) != num_cols:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or pixel < 0 or pixel > 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    total_pixels = num_rows * num_cols\n    average = total_brightness / total_pixels\n    return round(average, 2)"}
{"task_id": 70, "completion_id": 3, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    row_length = len(img[0])\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if not row or len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, (int, float)) or pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 4, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    rows = len(img)\n    first_row_length = len(img[0])\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n    for row in img:\n        for pixel in row:\n            if not isinstance(pixel, int) or not 0 <= pixel <= 255:\n                return -1\n    total = 0\n    num_pixels = 0\n    for row in img:\n        for pixel in row:\n            total += pixel\n            num_pixels += 1\n    average = total / num_pixels\n    return round(average, 2)"}
{"task_id": 70, "completion_id": 5, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    num_rows = len(img)\n    expected_width = len(img[0])\n    for row in img:\n        if len(row) != expected_width:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or not 0 <= pixel <= 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    total_pixels = num_rows * expected_width\n    average_brightness = round(total_brightness / total_pixels, 2)\n    return average_brightness"}
{"task_id": 70, "completion_id": 6, "solution": "def calculate_brightness(img):\n    if not img or not isinstance(img, list) or (not all((isinstance(row, list) for row in img))):\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if not isinstance(row, list) or len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, (int, float)) or pixel < 0 or pixel > 255:\n                return -1\n    total_brightness = sum((sum(row) for row in img))\n    num_pixels = len(img) * len(img[0])\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 7, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    first_row_length = len(img[0])\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n    total_pixels = 0\n    brightness_sum = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            brightness_sum += pixel\n            total_pixels += 1\n    average_brightness = brightness_sum / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 70, "completion_id": 8, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    rows = len(img)\n    cols = len(img[0])\n    if any((len(row) != cols for row in img)):\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average = total_brightness / total_pixels\n    return round(average, 2)"}
{"task_id": 70, "completion_id": 9, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    first_row_length = len(img[0])\n    total_pixels = 0\n    brightness_sum = 0\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or not 0 <= pixel <= 255:\n                return -1\n            brightness_sum += pixel\n            total_pixels += 1\n    average_brightness = round(brightness_sum / total_pixels, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between actual and predicted values.\n\n    Parameters:\n    y_true (array-like): Actual values.\n    y_pred (array-like): Predicted values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the inputs are not valid or have mismatched sizes.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if y_true.ndim > 2 or y_pred.ndim > 2:\n        raise ValueError('Input arrays must be 1D or 2D.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if not np.issubdtype(y_true.dtype, np.number) or not np.issubdtype(y_pred.dtype, np.number):\n        raise TypeError('Input arrays must contain numeric values.')\n    y_true = y_true.ravel()\n    y_pred = y_pred.ravel()\n    n = y_true.size\n    error = y_true - y_pred\n    squared_errors = error ** 2\n    mean_squared_error = squared_errors.mean()\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 1, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \"\"\"\n    try:\n        y_true = np.array(y_true, dtype=np.float64)\n        y_pred = np.array(y_pred, dtype=np.float64)\n    except:\n        raise ValueError('Invalid input: inputs must be array-like and convertible to float.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    squared_error = np.square(y_true - y_pred)\n    mean_squared_error = np.mean(squared_error)\n    rmse_value = np.sqrt(mean_squared_error)\n    return round(rmse_value, 3)"}
{"task_id": 71, "completion_id": 2, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n    \n    Returns:\n    float: RMSE rounded to 3 decimal places.\n    \n    Raises:\n    ValueError: If inputs are not valid (e.g., different lengths or empty).\n    TypeError: If inputs are of incorrect type.\n    \"\"\"\n    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)):\n        raise TypeError('Both y_true and y_pred must be list or numpy array.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 71, "completion_id": 3, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: The RMSE rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays are not of the same length or empty, or if they are not array-like.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        if y_true.shape != y_pred.shape:\n            raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n        if y_true.size == 0 or y_pred.size == 0:\n            raise ValueError('Input arrays cannot be empty.')\n        residuals = y_true - y_pred\n        squared_residuals = residuals ** 2\n        mean_squared_error = np.mean(squared_residuals)\n        root_mean_squared_error = np.sqrt(mean_squared_error)\n        return round(root_mean_squared_error, 3)\n    except Exception as e:\n        raise ValueError(f'Invalid input: {e}')\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 71, "completion_id": 4, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are not of the same length, are empty, or cannot be converted into arrays.\n    TypeError: If inputs are not array-like or numbers.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except (ValueError, TypeError):\n        raise ValueError('Input values must be array-like or numeric.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if len(y_true) == 0:\n        raise ValueError('Input arrays must not be empty.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 71, "completion_id": 5, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n    \n    Parameters:\n        y_true (array-like): Array of true values.\n        y_pred (array-like): Array of predicted values.\n        \n    Returns:\n        float: RMSE value rounded to three decimal places.\n        \n    Raises:\n        ValueError: If the input arrays are not of equal length or are empty.\n        TypeError: If the input types are not supported.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        if y_true.ndim != 1 or y_pred.ndim != 1:\n            raise ValueError('Both y_true and y_pred must be 1-dimensional arrays.')\n        if len(y_true) != len(y_pred):\n            raise ValueError('The length of y_true and y_pred must be equal.')\n        if len(y_true) == 0:\n            raise ValueError('Input arrays cannot be empty.')\n        squared_errors = (y_true - y_pred) ** 2\n        mean_squared_error = np.mean(squared_errors)\n        root_mean_squared_error = np.sqrt(mean_squared_error)\n        return round(root_mean_squared_error, 3)\n    except (ValueError, TypeError) as e:\n        raise e"}
{"task_id": 71, "completion_id": 6, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n    \n    Parameters:\n    y_true (array-like): True values.\n    y_pred (array-like): Predicted values.\n    \n    Returns:\n    float: RMSE rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays are not of the same shape or are empty.\n    TypeError: If inputs are not array-like or numeric.\n    \"\"\"\n    try:\n        y_true = np.array(y_true)\n        y_pred = np.array(y_pred)\n        if y_true.shape != y_pred.shape:\n            raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n        if y_true.size == 0 or y_pred.size == 0:\n            raise ValueError('Input arrays must not be empty.')\n        squared_errors = (y_true - y_pred) ** 2\n        mean_squared_error = np.mean(squared_errors)\n        rmse_value = np.sqrt(mean_squared_error)\n        return round(rmse_value, 3)\n    except (ValueError, TypeError) as e:\n        raise e\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 71, "completion_id": 7, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If inputs are of different lengths or empty.\n    TypeError: If inputs are not array-like or convertible to arrays.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except:\n        raise TypeError('Inputs must be array-like and convertible to NumPy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Shapes of input arrays are mismatched: {y_true.shape} vs {y_pred.shape}')\n    if y_true.ndim > 1:\n        raise ValueError('Input arrays must be one-dimensional.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 71, "completion_id": 8, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculates the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: RMSE rounded to three decimal places.\n\n    Raises:\n    ValueError: If inputs are not compatible in size or are invalid types.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except Exception:\n        raise ValueError('Inputs could not be converted to NumPy arrays.')\n    if y_true.dtype.kind not in 'iu' and y_true.dtype.kind != 'f':\n        raise ValueError('y_true contains non-numeric data.')\n    if y_pred.dtype.kind not in 'iu' and y_pred.dtype.kind != 'f':\n        raise ValueError('y_pred contains non-numeric data.')\n    if y_true.size != y_pred.size:\n        raise ValueError(f'Mismatched array sizes: {y_true.size} != {y_pred.size}')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_diff = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_diff)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 71, "completion_id": 9, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n        if y_true.shape != y_pred.shape:\n            raise ValueError('y_true and y_pred must be of the same shape.')\n        if y_true.size == 0 or y_pred.size == 0:\n            raise ValueError('Input arrays must not be empty.')\n        squared_errors = (y_true - y_pred) ** 2\n        mean_squared_error = np.mean(squared_errors)\n        root_mean_squared_error = np.sqrt(mean_squared_error)\n        return round(root_mean_squared_error, 3)\n    except ValueError as ve:\n        print(f'ValueError: {ve}')\n        return None\n    except Exception as e:\n        print(f'An error occurred: {e}')\n        return None"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index between two binary arrays.\n\n    Parameters:\n    - y_true: numpy array or list of binary values (0 or 1)\n    - y_pred: numpy array or list of binary values (0 or 1)\n\n    Returns:\n    - float: Jaccard Index rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Input arrays must only contain binary values (0 or 1).')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must have the same length.')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0 if intersection == 0 else float('nan')\n    return round(intersection / union, 3)"}
{"task_id": 72, "completion_id": 1, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) labels.\n    y_pred (array-like): Predicted labels.\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays must have the same length.')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]"}
{"task_id": 72, "completion_id": 2, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    jaccard_index = intersection / union\n    return round(jaccard_index, 3)"}
{"task_id": 72, "completion_id": 3, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Compute the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): True binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 4, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0 if intersection == 0 else float('nan')\n    return round(intersection / union, 3)"}
{"task_id": 72, "completion_id": 5, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 6, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jindex = intersection / union\n    return round(jindex, 3)"}
{"task_id": 72, "completion_id": 7, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n        y_true (array-like): True binary labels.\n        y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n        float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true).astype(int)\n    y_pred = np.asarray(y_pred).astype(int)\n    if np.any((y_true != 0) & (y_true != 1)) or np.any((y_pred != 0) & (y_pred != 1)):\n        raise ValueError('Input arrays must be binary (0 and 1 only).')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard_index_value = intersection / union\n    return round(jaccard_index_value, 3)"}
{"task_id": 72, "completion_id": 8, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 72, "completion_id": 9, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays y_true and y_pred.\n    \n    Parameters:\n    y_true (array-like): Ground truth binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0 and intersection == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient or F1-score) \n    between two binary arrays.\n    \n    Parameters:\n    - y_true (array-like): Ground truth (correct) binary labels.\n    - y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    - float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]"}
{"task_id": 73, "completion_id": 1, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated targets as returned by a classifier.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Both y_true and y_pred must be binary arrays (0 and 1 only).')\n    intersection = np.sum(y_true * y_pred)\n    if np.sum(y_true) + np.sum(y_pred) == 0:\n        return 1.0\n    dice = 2.0 * intersection / (np.sum(y_true) + np.sum(y_pred))\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 2, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) labels.\n    y_pred (array-like): Predicted labels.\n    \n    Returns:\n    float: Dice score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if not np.all(np.isin(y_true, [0, 1])) or not np.all(np.isin(y_pred, [0, 1])):\n        raise ValueError('Input arrays must contain only 0s and 1s (binary values).')\n    intersection = np.logical_and(y_true, y_pred).sum()\n    true_sum = y_true.sum()\n    pred_sum = y_pred.sum()\n    if true_sum == 0 and pred_sum == 0:\n        return 0.0\n    dice = 2.0 * intersection / (true_sum + pred_sum + 1e-08)\n    return round(dice, 3)\ny_true = [1, 0, 1, 0, 1]\ny_pred = [1, 0, 0, 0, 0]"}
{"task_id": 73, "completion_id": 3, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): True binary labels\n    y_pred (array-like): Predicted binary labels\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    sum_sets = np.sum(y_true) + np.sum(y_pred)\n    if sum_sets == 0:\n        return 0.0\n    score = 2.0 * intersection / sum_sets\n    return round(score, 3)"}
{"task_id": 73, "completion_id": 4, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    true_sum = y_true.sum()\n    pred_sum = y_pred.sum()\n    if true_sum == 0 and pred_sum == 0:\n        return 1.0\n    score = 2.0 * intersection / (true_sum + pred_sum)\n    return round(score, 3)\ny_true = [1, 0, 1, 1, 0, 0, 0]\ny_pred = [1, 0, 1, 0, 0, 0, 1]"}
{"task_id": 73, "completion_id": 5, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred + 1e-08)\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 6, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) binary labels.\n    y_pred (array-like): Predicted binary labels.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    total = np.sum(y_true) + np.sum(y_pred)\n    if total == 0:\n        return 1.0\n    dice = 2.0 * intersection / total\n    return round(dice, 3)\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 1, 0, 1, 0]"}
{"task_id": 73, "completion_id": 7, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient or F1-score)\n    between two binary arrays.\n    \n    Parameters:\n    - y_true: A 1D numpy array of true binary labels (0 or 1)\n    - y_pred: A 1D numpy array of predicted binary labels (0 or 1)\n    \n    Returns:\n    - Dice score as a float, rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true).astype(np.bool_)\n    y_pred = np.array(y_pred).astype(np.bool_)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    sum_sizes = np.sum(y_true) + np.sum(y_pred)\n    if sum_sizes == 0:\n        return 1.0 if intersection == 0 else intersection / sum_sizes\n    dice = 2 * intersection / sum_sizes\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 8, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: The Dice Score, rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_counts = np.sum(y_true) + np.sum(y_pred)\n    if sum_counts == 0:\n        return 1.0\n    dice = 2.0 * intersection / sum_counts\n    return round(dice, 3)"}
{"task_id": 73, "completion_id": 9, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: dict, a dictionary where keys are feature names and values are their corresponding values.\n    - dim: int, the dimensionality of the hypervectors.\n    - random_seeds: dict, a dictionary of seeds for each feature to ensure reproducibility.\n    \n    Returns:\n    - A composite hypervector (as a list) representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        seed = random_seeds[feature_name]\n        np.random.seed(seed)\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        np.random.seed(seed)\n        value_hv = np.random.choice([-1, 1], size=dim)\n        binding = feature_hv * value_hv\n        composite_hv += binding\n    return (composite_hv / np.sqrt(dim)).tolist()"}
{"task_id": 74, "completion_id": 1, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = np.zeros(dim)\n    for (feature_name, value) in row.items():\n        feature_seed = random_seeds.get(feature_name, 0)\n        rng = np.random.RandomState(feature_seed)\n        hv_name = rng.choice([-1, 1], size=dim)\n        value_seed = hash(str(value)) % 2 ** 32\n        value_rng = np.random.RandomState(value_seed)\n        hv_value = value_rng.choice([-1, 1], size=dim)\n        bound_hv = hv_name * hv_value\n        composite_hv += bound_hv\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 2, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using HDC.\n\n    Parameters:\n    - row (dict): A dictionary representing a dataset row with feature names as keys and values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary mapping each feature name to its respective random seed.\n\n    Returns:\n    - list: The composite hypervector as a list.\n    \"\"\"\n\n    def generate_random_hv(d, seed):\n        \"\"\"Helper function to generate a random hypervector given a seed.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=d)\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_seed = random_seeds.get(feature_name, 0)\n        np.random.seed(feature_seed)\n        feature_hv = generate_random_hv(dim, feature_seed)\n        value_seed = hash(str(feature_value)) % 2 ** 32\n        value_hv = generate_random_hv(dim, value_seed)\n        bound_hv = np.multiply(feature_hv, value_hv)\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 3, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row (feature names to values).\n    - dim (int): Dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary of feature names to integer seeds for reproducibility.\n    \n    Returns:\n    - list: A list representing the composite hypervector of the row.\n    \"\"\"\n\n    def generate_hv(seed):\n        \"\"\"Generate a random hypervector.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n    composite_hv = None\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, hash(feature) % 2 ** 32)\n        value_seed = hash(value)\n        feature_hv = generate_hv(seed)\n        value_hv = generate_hv(value_seed)\n        feature_value_hv = feature_hv * value_hv\n        if composite_hv is None:\n            composite_hv = feature_value_hv\n        else:\n            composite_hv += feature_value_hv\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 4, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Each feature in the row is represented by binding the feature name hypervector and the value hypervector.\n    Hypervectors are created using random seeds to ensure reproducibility.\n    Finally, all feature hypervectors are bundled to create the composite hypervector for the row.\n    \n    Parameters:\n    - row: Dictionary representing the row, with feature names as keys and values as values.\n    - dim: Dimensionality of the hypervectors.\n    - random_seeds: Dictionary of seeds for reproducibility, with feature names as keys.\n    \n    Returns:\n    - The composite hypervector as a list (converted from NumPy array).\n    \"\"\"\n    composite_hv = None\n    for (feature_name, value) in row.items():\n        seed = random_seeds.get(feature_name, hash(feature_name))\n        np.random.seed(seed)\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        value_seed = seed + abs(hash(str(value)))\n        np.random.seed(value_seed)\n        value_hv = np.random.choice([-1, 1], size=dim)\n        bound_hv = feature_hv * value_hv\n        if composite_hv is None:\n            composite_hv = bound_hv\n        else:\n            composite_hv = (composite_hv + bound_hv) / 2\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 5, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n\n    Parameters:\n    - row (dict): A dictionary where keys are feature names and values are the corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): Dictionary mapping feature names to random seeds for reproducibility.\n\n    Returns:\n    - list: The composite hypervector representing the row.\n    \"\"\"\n\n    def generate_hv(seed, dim):\n        \"\"\"Generates a bipolar hypervector with \u00b11 values using a specified seed.\"\"\"\n        np.random.seed(seed)\n        return 2 * (np.random.rand(dim) > 0.5) - 1\n\n    def bind(hv1, hv2):\n        \"\"\"Performs element-wise multiplication (Hadamard product) to bind two hypervectors.\"\"\"\n        return hv1 * hv2\n\n    def bundle(*hvs):\n        \"\"\"Performs element-wise addition (normalized) to bundle hypervectors.\"\"\"\n        avg = np.mean(hvs, axis=0)\n        return np.sign(avg)\n    row_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_hv = generate_hv(seed=hash(feature) % 2 ** 32, dim=dim)\n        value_hv = generate_hv(seed=feature_seed, dim=dim)\n        bound_hv = bind(feature_hv, value_hv)\n        row_hv += bound_hv\n    if np.all(row_hv == 0):\n        fallback_hv = generate_hv(13, dim)\n        composite_hv = fallback_hv\n    else:\n        composite_hv = np.sign(row_hv)\n    return composite_hv.tolist()"}
{"task_id": 74, "completion_id": 6, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using HDC principles.\n    \n    Parameters:\n        row (dict): Dictionary where keys are feature names and values are their values.\n        dim (int): Dimensionality of the hypervectors.\n        random_seeds (dict): Dictionary mapping feature names to random seeds.\n        \n    Returns:\n        list: Composite hypervector for the row as a Python list.\n    \"\"\"\n    base_seed = 42\n    base_random = np.random.RandomState(base_seed)\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_index = hash(feature_name) % 100\n        feature_seed = base_random.randint(0, 10 ** 6)\n        value_seed = random_seeds.get(feature_name, base_seed)\n        feature_rs = np.random.RandomState(feature_seed)\n        value_rs = np.random.RandomState(value_seed)\n        feature_hv = feature_rs.randn(dim)\n        value_hv = value_rs.randn(dim)\n        feature_hv = feature_hv / np.linalg.norm(feature_hv)\n        value_hv = value_hv / np.linalg.norm(value_hv)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = composite_hv / np.linalg.norm(composite_hv)\n    return composite_hv.tolist()\nrow = {'age': 30, 'income': 'high', 'location': 'NY'}\nrandom_seeds = {'age': 12345, 'income': 67890, 'location': 54321}\ncomposite_hv = create_row_hv(row, dim=100, random_seeds=random_seeds)"}
{"task_id": 74, "completion_id": 7, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a row of data using Hyperdimensional Computing (HDC).\n    \n    Args:\n        row (dict): A dictionary representing a row of data (e.g., {'color': 'red', 'size': 'large'}).\n        dim (int): Dimensionality of each hypervector.\n        random_seeds (dict): Dictionary mapping feature names to random seeds for reproducibility.\n\n    Returns:\n        list: Composite hypervector as a list.\n    \"\"\"\n    np.random.seed()\n    feature_hv = {}\n    for (feature, value) in row.items():\n        value_seed = random_seeds.get(feature, 0)\n        feature_seed = hash(feature) % 2 ** 32\n        np.random.seed(feature_seed)\n        feature_vec = np.random.choice([-1, 1], size=dim)\n        np.random.seed(value_seed)\n        value_vec = np.random.choice([-1, 1], size=dim)\n        bound_hv = feature_vec * value_vec\n        feature_hv[feature] = bound_hv\n    composite_hv = np.sum(list(feature_hv.values()), axis=0)\n    return composite_hv.tolist()\ndim = 100"}
{"task_id": 74, "completion_id": 8, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        if feature_name not in random_seeds:\n            raise ValueError(f'No random seed provided for feature: {feature_name}')\n        rs = np.random.RandomState(random_seeds[feature_name])\n        feature_hv = rs.uniform(-1, 1, size=dim)\n        feature_hv = np.where(feature_hv >= 0, 1, -1)\n        value_hv = np.zeros(dim)\n        for ch in str(feature_value):\n            char_hv = rs.uniform(-1, 1, size=dim)\n            char_hv = np.where(char_hv >= 0, 1, -1)\n            value_hv += char_hv\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = composite_hv / np.linalg.norm(composite_hv)\n    return composite_hv.tolist()\nrow = {'color': 'red', 'shape': 'circle', 'size': 'large'}\ndim = 1000\nrandom_seeds = {'color': 42, 'shape': 123, 'size': 789}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 74, "completion_id": 9, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        seed = random_seeds.get(feature, 0)\n        np.random.seed(seed)\n        value_hv = np.random.choice([-1, 1], dim)\n        composite_hv += value_hv\n    return (composite_hv / np.linalg.norm(composite_hv)).tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for a binary classification problem.\n    \n    Parameters:\n        data (list of lists): A list of [y_true, y_pred] pairs.\n    \n    Returns:\n        list of lists: A 2x2 confusion matrix in the form:\n                       [[True Negative, False Positive],\n                        [False Negative, True Positive]]\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            matrix[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            matrix[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            matrix[1][0] += 1\n        elif y_true == 1 and y_pred == 1:\n            matrix[1][1] += 1\n    return matrix"}
{"task_id": 75, "completion_id": 1, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for binary classification based on the input data.\n    \n    Parameters:\n        data (list of lists): A list where each inner list is [y_true, y_pred]\n        \n    Returns:\n        list of lists: A 2x2 confusion matrix in the format:\n                        [[True Negative, False Positive],\n                         [False Negative, True Positive]]\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0:\n            if y_pred == 0:\n                matrix[0][0] += 1\n            else:\n                matrix[0][1] += 1\n        elif y_pred == 0:\n            matrix[1][0] += 1\n        else:\n            matrix[1][1] += 1\n    return matrix"}
{"task_id": 75, "completion_id": 2, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    Parameters:\n        data (list of lists): A list of pairs [y_true, y_pred] for each observation.\n    \n    Returns:\n        list of lists: A 2x2 confusion matrix with the structure:\n            [\n                [True Negatives, False Positives],\n                [False Negatives, True Positives]\n            ]\n    \"\"\"\n    (tn, fp, fn, tp) = (0, 0, 0, 0)\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 1 and y_pred == 1:\n            tp += 1\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 75, "completion_id": 3, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n\n    Parameters:\n    - data: A list of lists where each inner list is [y_true, y_pred]\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists:\n      [[True Negative, False Positive],\n       [False Negative, True Positive]]\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        matrix[y_true][y_pred] += 1\n    return matrix"}
{"task_id": 75, "completion_id": 4, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Computes the confusion matrix for a binary classification problem.\n    \n    Parameters:\n        data (list of lists): Each sublist contains [y_true, y_pred] for an observation.\n        \n    Returns:\n        list of lists: A 2x2 confusion matrix in the form:\n                       [[true_negative, false_positive],\n                        [false_negative, true_positive]]\n    \"\"\"\n    (tn, fp, fn, tp) = (0, 0, 0, 0)\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 1 and y_pred == 1:\n            tp += 1\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 75, "completion_id": 5, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for a binary classification task.\n    \n    Parameters:\n    - data: A list of [y_true, y_pred] pairs.\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists:\n      [[TN, FP],\n       [FN, TP]]\n    \"\"\"\n    counts = Counter(data)\n    tp = counts.get([1, 1], 0)\n    tn = counts.get([0, 0], 0)\n    fp = counts.get([0, 1], 0)\n    fn = counts.get([1, 0], 0)\n    cm = [[tn, fp], [fn, tp]]\n    return cm"}
{"task_id": 75, "completion_id": 6, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    \n    Parameters:\n    data (list of lists): A list of [y_true, y_pred] pairs.\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix:\n                   [[True Negatives, False Positives],\n                    [False Negatives, True Positives]]\n    \"\"\"\n    counter = Counter(((true, pred) for (true, pred) in data))\n    true_neg = counter[0, 0]\n    false_pos = counter[0, 1]\n    false_neg = counter[1, 0]\n    true_pos = counter[1, 1]\n    return [[true_neg, false_pos], [false_neg, true_pos]]"}
{"task_id": 75, "completion_id": 7, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    \n    Parameters:\n        data (list of lists): Each inner list is [y_true, y_pred], representing\n                              the true and predicted label for one observation.\n    \n    Returns:\n        list of lists: A 2x2 confusion matrix in the format:\n                      [[True Negatives, False Positives],\n                       [False Negatives, True Positives]]\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            matrix[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            matrix[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            matrix[1][0] += 1\n        elif y_true == 1 and y_pred == 1:\n            matrix[1][1] += 1\n    return matrix"}
{"task_id": 75, "completion_id": 8, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for a binary classification problem.\n    \n    Parameters:\n    - data: List of lists, where each inner list is [y_true, y_pred]\n    \n    Returns:\n    - A 2x2 confusion matrix [[TN, FP], [FN, TP]]\n    \"\"\"\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0:\n            if y_pred == 0:\n                matrix[0][0] += 1\n            else:\n                matrix[0][1] += 1\n        elif y_pred == 0:\n            matrix[1][0] += 1\n        else:\n            matrix[1][1] += 1\n    return matrix"}
{"task_id": 75, "completion_id": 9, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a 2x2 confusion matrix for a binary classification problem.\n    \n    Parameters:\n    ----------\n    data : list of lists\n        Each inner list is a pair [y_true, y_pred] where:\n            y_true: Actual class label (0 or 1)\n            y_pred: Predicted class label (0 or 1)\n    \n    Returns:\n    -------\n    list of lists\n        A 2x2 confusion matrix [[TN, FP], [FN, TP]]\n    \"\"\"\n    (tn, fp, fn, tp) = (0, 0, 0, 0)\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            tn += 1\n        elif y_true == 0 and y_pred == 1:\n            fp += 1\n        elif y_true == 1 and y_pred == 0:\n            fn += 1\n        elif y_true == 1 and y_pred == 1:\n            tp += 1\n    return [[tn, fp], [fn, tp]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors must have non-zero magnitude.')\n    similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 1, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 2, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 3, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Input vectors must not have zero magnitude.')\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 4, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 5, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1, v2: numpy arrays of the same shape and non-zero magnitude\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places\n    \"\"\"\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return np.round(similarity, 3)"}
{"task_id": 76, "completion_id": 6, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (np.ndarray): First input vector.\n    v2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: Cosine similarity between the vectors, rounded to three decimal places.\n    \"\"\"\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    if norm_v1 == 0 or norm_v2 == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 76, "completion_id": 7, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 8, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Vectors cannot have zero magnitude.')\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 76, "completion_id": 9, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must be of the same length.')\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError('All elements in actual and predicted must be 0 or 1.')\n    tp = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    fp = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    tn = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    fn = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total = tp + tn + fp + fn\n    accuracy = (tp + tn) / total if total > 0 else 0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    npv = tn / (tn + fn) if tn + fn > 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(npv, 3))"}
{"task_id": 77, "completion_id": 1, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    for val in actual + predicted:\n        if val not in {0, 1}:\n            raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    TP = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 1))\n    TN = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 0))\n    FP = sum((1 for (a, p) in zip(actual, predicted) if a == 0 and p == 1))\n    FN = sum((1 for (a, p) in zip(actual, predicted) if a == 1 and p == 0))\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = TN / (TN + FP) if TN + FP > 0 else 0.0\n    negative_predictive_value = TN / (TN + FN) if TN + FN > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 2, "solution": "from collections import Counter\nfrom typing import Tuple, List\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    \"\"\"\n    Calculates performance metrics for a binary classification problem.\n    \n    Parameters:\n    - actual (list of int): The true class labels (0 or 1).\n    - predicted (list of int): The predicted class labels (0 or 1).\n    \n    Returns:\n    - confusion_matrix: A 2x2 matrix containing [True Negative, False Positive]\n                               [False Negative, True Positive]\n    - accuracy: Accuracy of the model.\n    - f1_score: F1 score of the model.\n    - specificity: Specificity of the model.\n    - negative_predictive_value: Negative Predictive Value of the model.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The lengths of actual and predicted lists must be equal.')\n    tn = fp = fn = tp = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tn + tp) / (tn + fp + fn + tp) if tn + fp + fn + tp > 0 else 0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 3, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple:\n    \"\"\"\n    Computes performance metrics for a binary classification model.\n    \n    Parameters:\n    - actual: List of actual class labels (0 or 1).\n    - predicted: List of predicted class labels (0 or 1).\n    \n    Returns:\n    - confusion_matrix: A 2x2 list of true negatives, false negatives, false positives, true positives.\n    - accuracy: Accuracy of the model.\n    - f1_score: F1 score of the model.\n    - specificity: Specificity of the model.\n    - negative_predictive_value: Negative Predictive Value (NPV).\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n    confusion_matrix = [[tn, fn], [fp, tp]]\n    total = len(actual)\n    accuracy = (tp + tn) / total if total > 0 else 0.0\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 4, "solution": "from collections import Counter\nfrom typing import list, tuple\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('Input lists must have the same length.')\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError('All elements in actual and predicted lists must be 0 or 1.')\n    (TP, FP, TN, FN) = (0, 0, 0, 0)\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = len(actual)\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    precision = TP / (TP + FP) if TP + FP != 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN != 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0.0\n    specificity = TN / (TN + FP) if TN + FP != 0 else 0.0\n    negative_predictive_value = TN / (TN + FN) if TN + FN != 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 5, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculates performance metrics for a binary classification problem.\n    \n    Parameters:\n        actual (list[int]): The actual class labels (0 or 1).\n        predicted (list[int]): The predicted class labels (0 or 1).\n        \n    Returns:\n        tuple: (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"The lengths of 'actual' and 'predicted' lists must be the same.\")\n    if any((x not in (0, 1) for x in actual)) or any((x not in (0, 1) for x in predicted)):\n        raise ValueError('All elements in the lists must be either 0 or 1.')\n    tp = fp = tn = fn = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            tp += 1\n        elif a == 0 and p == 0:\n            tn += 1\n        elif a == 0 and p == 1:\n            fp += 1\n        elif a == 1 and p == 0:\n            fn += 1\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 77, "completion_id": 6, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for binary classification.\n\n    Parameters:\n    actual (list[int]): The actual class labels (0 or 1).\n    predicted (list[int]): The predicted class labels (0 or 1).\n    \n    Returns:\n    tuple: (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in {0, 1} for x in actual)) or not all((x in {0, 1} for x in predicted)):\n        raise ValueError(\"All values in 'actual' and 'predicted' must be 0 or 1.\")\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0.0\n    npv = tn / (tn + fn) if tn + fn > 0 else 0.0\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(npv, 3))"}
{"task_id": 77, "completion_id": 7, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Parameters:\n        actual (list[int]): True class labels (0 or 1).\n        predicted (list[int]): Predicted class labels (0 or 1).\n\n    Returns:\n        tuple: (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual)) or not all((x in [0, 1] for x in predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' must be 0 or 1.\")\n    TP = TN = FP = FN = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    accuracy = round((TP + TN) / (TP + TN + FP + FN), 3) if TP + TN + FP + FN > 0 else 0.0\n    precision = round(TP / (TP + FP), 3) if TP + FP > 0 else 0.0\n    recall = round(TP / (TP + FN), 3) if TP + FN > 0 else 0.0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0.0\n    specificity = round(TN / (TN + FP), 3) if TN + FP > 0 else 0.0\n    negative_predictive_value = round(TN / (TN + FN), 3) if TN + FN > 0 else 0.0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 8, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The lengths of 'actual' and 'predicted' lists must be the same.\")\n    for val in actual + predicted:\n        if val not in [0, 1]:\n            raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 77, "completion_id": 9, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('actual and predicted lists must be the same length.')\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError('All elements in actual and predicted must be 0 or 1.')\n    TP = 0\n    TN = 0\n    FP = 0\n    FN = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    accuracy = (TP + TN) / (TP + TN + FP + FN) if TP + TN + FP + FN > 0 else 0.0\n    precision = TP / (TP + FP) if TP + FP > 0 else 0.0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0.0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n    specificity = TN / (TN + FP) if TN + FP > 0 else 0.0\n    negative_predictive_value = TN / (TN + FN) if TN + FN > 0 else 0.0\n    metrics = (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))\n    return metrics"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate descriptive statistics for a given dataset.\n\n    Parameters:\n    - data (list or numpy.ndarray): Numerical data\n\n    Returns:\n    - dict: A dictionary with various statistical metrics\n    \"\"\"\n    data = np.array(data)\n    stats_dict = {'mean': round(np.mean(data), 4), 'median': round(np.median(data), 4), 'mode': stats.mode(data).mode[0], 'variance': round(np.var(data, ddof=1), 4), 'standard_deviation': round(np.std(data, ddof=1), 4), '25th_percentile': np.percentile(data, 25), '50th_percentile': np.percentile(data, 50), '75th_percentile': np.percentile(data, 75), 'interquartile_range': np.percentile(data, 75) - np.percentile(data, 25)}\n    return stats_dict"}
{"task_id": 78, "completion_id": 1, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=True).mode[0]\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr = p75 - p25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return stats_dict"}
{"task_id": 78, "completion_id": 2, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=True).mode[0]\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    stats_dict = {'mean': round(float(mean), 4), 'median': round(float(median), 4), 'mode': float(mode), 'variance': round(float(variance), 4), 'standard_deviation': round(float(standard_deviation), 4), '25th_percentile': round(float(percentile_25), 4), '50th_percentile': round(float(percentile_50), 4), '75th_percentile': round(float(percentile_75), 4), 'interquartile_range': round(float(interquartile_range), 4)}\n    return stats_dict"}
{"task_id": 78, "completion_id": 3, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data, dtype=float)\n    mean = round(np.mean(data), 4)\n    median = round(np.median(data), 4)\n    mode_result = stats.mode(data, keepdims=False)\n    mode = mode_result.mode\n    variance = round(np.var(data, ddof=0), 4)\n    standard_deviation = round(np.std(data, ddof=0), 4)\n    p25 = round(np.percentile(data, 25), 4)\n    p50 = round(np.percentile(data, 50), 4)\n    p75 = round(np.percentile(data, 75), 4)\n    iqr = p75 - p25\n    result = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': p25, '50th_percentile': p50, '75th_percentile': p75, 'interquartile_range': iqr}\n    return result"}
{"task_id": 78, "completion_id": 4, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    data_array = np.array(data, dtype=float)\n    mean = np.mean(data_array)\n    median = np.median(data_array)\n    mode = float(np.round(np.nan if np.isnan(np.ma.mode(data_array).data) else np.ma.mode(data_array).data[0], 4))\n    variance = np.nanvar(data_array)\n    standard_deviation = np.nanstd(data_array)\n    q25 = np.percentile(data_array, 25)\n    q50 = np.percentile(data_array, 50)\n    q75 = np.percentile(data_array, 75)\n    iqr = q75 - q25\n    statistics = {'mean': np.round(mean, 4), 'median': np.round(median, 4), 'mode': mode, 'variance': np.round(variance, 4), 'standard_deviation': np.round(standard_deviation, 4), '25th_percentile': np.round(q25, 4), '50th_percentile': np.round(q50, 4), '75th_percentile': np.round(q75, 4), 'interquartile_range': np.round(iqr, 4)}\n    return statistics\ndata = [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]"}
{"task_id": 78, "completion_id": 5, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=True).mode[0]\n    variance = np.var(data, ddof=0)\n    standard_deviation = np.std(data, ddof=0)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    return {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\ndata = [10, 12, 23, 23, 16, 23, 21, 16]"}
{"task_id": 78, "completion_id": 6, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data_array = np.array(data)\n    mean = np.mean(data_array)\n    median = np.median(data_array)\n    mode = stats.mode(data_array, keepdims=True).mode[0]\n    variance = np.var(data_array, ddof=0)\n    standard_deviation = np.std(data_array, ddof=0)\n    percentile_25 = np.percentile(data_array, 25)\n    percentile_50 = np.percentile(data_array, 50)\n    percentile_75 = np.percentile(data_array, 75)\n    interquartile_range = percentile_75 - percentile_25\n    mean = round(mean, 4)\n    median = round(median, 4)\n    variance = round(variance, 4)\n    standard_deviation = round(standard_deviation, 4)\n    result = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}\n    return result"}
{"task_id": 78, "completion_id": 7, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates and returns various descriptive statistics for a given dataset.\n\n    Parameters:\n        data (list or numpy.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n        dict: A dictionary containing mean, median, mode, variance, standard deviation,\n              25th percentile, 50th percentile, 75th percentile, and IQR.\n    \"\"\"\n    data = np.array(data, dtype=float)\n    result = {'mean': np.round(np.mean(data), 4), 'median': np.round(np.median(data), 4), 'mode': stats.mode(data, keepdims=True).mode[0], 'variance': np.round(np.var(data, ddof=0), 4), 'standard_deviation': np.round(np.std(data, ddof=0), 4), '25th_percentile': np.percentile(data, 25), '50th_percentile': np.percentile(data, 50), '75th_percentile': np.percentile(data, 75), 'interquartile_range': np.percentile(data, 75) - np.percentile(data, 25)}\n    return result"}
{"task_id": 78, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n    \n    Parameters:\n        data (list or np.ndarray): A list or numpy array of numerical values.\n        \n    Returns:\n        dict: A dictionary containing the mean, median, mode, variance, \n              standard deviation, percentiles, and IQR.\n    \"\"\"\n    data = np.asarray(data)\n    stats = {}\n    stats['mean'] = round(np.mean(data), 4)\n    stats['median'] = np.median(data)\n    counts = Counter(data)\n    max_count = max(counts.values())\n    modes = [k for (k, v) in counts.items() if v == max_count]\n    if len(modes) == len(data):\n        stats['mode'] = np.median(data)\n    else:\n        stats['mode'] = modes[0] if len(modes) == 1 else modes\n    stats['variance'] = round(np.var(data, ddof=0), 4)\n    stats['standard_deviation'] = round(np.std(data, ddof=0), 4)\n    stats['25th_percentile'] = np.percentile(data, 25)\n    stats['50th_percentile'] = np.median(data)\n    stats['75th_percentile'] = np.percentile(data, 75)\n    stats['interquartile_range'] = stats['75th_percentile'] - stats['25th_percentile']\n    return stats"}
{"task_id": 78, "completion_id": 9, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data, keepdims=True).mode[0]\n    variance = np.var(data, ddof=0).round(4)\n    standard_deviation = np.std(data, ddof=0).round(4)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    return {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 1, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    comb = math.comb(n, k)\n    probability = comb * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 2, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 3, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 4, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    \n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 5, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 6, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    if k < 0 or k > n or p < 0 or (p > 1):\n        return 0.0\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 7, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 8, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 79, "completion_id": 9, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1.0 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 1, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nx = 1.0\nmean = 0.0\nstd_dev = 1.0"}
{"task_id": 80, "completion_id": 2, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 3, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 4, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nx = 100\nmean = 100\nstd_dev = 15"}
{"task_id": 80, "completion_id": 5, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    \n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 6, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be positive.')\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 80, "completion_id": 7, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 8, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 80, "completion_id": 9, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf = coefficient * math.exp(exponent)\n    return round(pdf, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 1, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer.')\n    return round(math.exp(-lam) * lam ** k / math.factorial(k), 5)"}
{"task_id": 81, "completion_id": 2, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 3, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 4, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 5, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 6, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: The Poisson probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError('k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 7, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability (rounded to 5 decimal places)\n    \"\"\"\n    probability = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 8, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing k events, rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 81, "completion_id": 9, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: Contrast value as the difference between maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 1, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: Contrast value, computed as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale).')\n    if img.dtype != np.uint8:\n        raise ValueError('Input image must have pixel values of type uint8 (0-255).')\n    max_pixel = img.max()\n    min_pixel = img.min()\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 2, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, defined as the difference between maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale image).')\n    if img.dtype != np.uint8:\n        raise ValueError('Input image must have pixel values of type uint8 (0 to 255).')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 3, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 4, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: Contrast value, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale).')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 5, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    \n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n        \n    Returns:\n        int: Contrast value, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale).')\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val"}
{"task_id": 82, "completion_id": 6, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value as the difference between max and min pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 82, "completion_id": 7, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: Contrast value, defined as the difference between max and min pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    contrast = np.max(img) - np.min(img)\n    return contrast"}
{"task_id": 82, "completion_id": 8, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: Contrast value, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    return max_pixel - min_pixel"}
{"task_id": 82, "completion_id": 9, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast value, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale).')\n    if img.dtype not in [np.uint8, np.int8, np.uint16, np.int16]:\n        raise ValueError('Input image must have an appropriate integer data type.')\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D vectors.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 1, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 2, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 3, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 4, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 5, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 6, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 7, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D NumPy arrays.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must have the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 8, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \n    Raises:\n        ValueError: If the vectors are not of the same length or are not 1D.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D NumPy arrays.')\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 83, "completion_id": 9, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = []\n        for d in range(1, degree + 1):\n            features.append(np.round(x ** d, 8))\n        transformed_data.append(features)\n    return transformed_data"}
{"task_id": 84, "completion_id": 1, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each sublist contains polynomial features of a data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for x in data:\n        features = [round(x ** d, 8) for d in range(1, degree + 1)]\n        result.append(features)\n    return result"}
{"task_id": 84, "completion_id": 2, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list of polynomial features for each data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = [round(x ** i, 8) for i in range(1, degree + 1)]\n        transformed_data.append(features)\n    return transformed_data"}
{"task_id": 84, "completion_id": 3, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for x in data:\n        features = [x ** i for i in range(1, degree + 1)]\n        result.append([round(val, 8) for val in features])\n    return result"}
{"task_id": 84, "completion_id": 4, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\nfrom collections import Counter\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists containing the polynomial features of each data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    def polynomial_basis(features: list[float], degree: int) -> list[float]:\n        n = len(features)\n        indices = combinations_with_replacement(range(n), degree)\n        terms = []\n        for idx in indices:\n            coeff = 1.0\n            for i in idx:\n                coeff *= features[i]\n            terms.append(coeff)\n        return terms\n    result = []\n    for point in data:\n        features = [1.0]\n        for d in range(1, degree + 1):\n            features += polynomial_basis(point, d)\n        result.append([round(val, 8) for val in features])\n    return result"}
{"task_id": 84, "completion_id": 5, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    d = len(data)\n    result = []\n    for dpt in data:\n        transformed_features = []\n        for deg in range(1, degree + 1):\n            for combo in combinations_with_replacement(range(d), deg):\n                val = 1.0\n                for idx in combo:\n                    val *= dpt[idx]\n                transformed_features.append(val)\n        result.append(transformed_features)\n    return [list(map(lambda x: round(x, 8), point)) for point in result]"}
{"task_id": 84, "completion_id": 6, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = []\n        for d in range(1, degree + 1):\n            features.append(round(x ** d, 8))\n        transformed_data.append(features)\n    return transformed_data"}
{"task_id": 84, "completion_id": 7, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point, rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = [x ** i for i in range(1, degree + 1)]\n        transformed_data.append([round(val, 8) for val in features])\n    return transformed_data"}
{"task_id": 84, "completion_id": 8, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists containing the polynomial feature transformations.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = [x ** d for d in range(1, degree + 1)]\n        rounding_mask = [round(feature, 8) for feature in features]\n        transformed_data.append(rounding_mask)\n    return transformed_data"}
{"task_id": 84, "completion_id": 9, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list represents the polynomial features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed = []\n    for x in data:\n        features = [x ** d for d in range(1, degree + 1)]\n        transformed.append([round(val, 8) for val in features])\n    return transformed\ndata = [1.0, 2.0, 3.0]\ndegree = 3"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            divisor = np.float16(10000 ** (2 * (i // 2) / d_model))\n            pe[pos, i] = np.sin(pos / divisor)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / divisor)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 1, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Compute positional encoding for a given position and model dimension.\n    \n    Args:\n        position (int): Position in the sequence.\n        d_model (int): Dimensionality of the model (embedding dimension).\n    \n    Returns:\n        list: Positional encoding as a 1D list of np.float16.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    position = np.arange(position)[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    encoding[:, 0::2] = np.sin(position * div_term).astype(np.float16)\n    encoding[:, 1::2] = np.cos(position * div_term).astype(np.float16)\n    return encoding[-1].tolist()"}
{"task_id": 85, "completion_id": 2, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i + 1 <= d_model:\n                angle = pos / 10000 ** (i / d_model)\n                pe[pos, i] = np.sin(angle).astype(np.float16)\n                pe[pos, i + 1] = np.cos(angle).astype(np.float16)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 3, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position <= 0 or d_model <= 0:\n        return -1\n    pos = np.arange(position)\n    denominator = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    pe[:, 0::2] = np.sin(pos[:, np.newaxis] * denominator)\n    pe[:, 1::2] = np.cos(pos[:, np.newaxis] * denominator)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 4, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position <= 0 or d_model <= 0:\n        return -1\n    encodings = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i + 1 < d_model:\n                angle = pos / 10000 ** (i / d_model)\n                encodings[pos, i] = np.sin(angle).astype(np.float16)\n                encodings[pos, i + 1] = np.cos(angle).astype(np.float16)\n            else:\n                angle = pos / 10000 ** (i / d_model)\n                encodings[pos, i] = np.sin(angle).astype(np.float16)\n    return encodings.tolist()"}
{"task_id": 85, "completion_id": 5, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model)).astype(np.float16)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * i / d_model)).astype(np.float16)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 6, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle_rads = 1 / 10000 ** (i / d_model)\n            encoding[pos, i] = np.sin(angle_rads * pos).astype(np.float16)\n            if i + 1 < d_model:\n                encoding[pos, i + 1] = np.cos(angle_rads * pos).astype(np.float16)\n    return encoding.tolist()"}
{"task_id": 85, "completion_id": 7, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i + 1 <= d_model:\n                angle = pos / 10000 ** (i / d_model)\n                pos_encoding[pos, i] = np.sin(angle).astype(np.float16)\n                if i + 1 < d_model:\n                    pos_encoding[pos, i + 1] = np.cos(angle).astype(np.float16)\n    return pos_encoding.tolist()"}
{"task_id": 85, "completion_id": 8, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i + 1 <= d_model:\n                angle_rates = 1 / np.power(10000, 2 * (i // 2) / np.float16(d_model))\n                pe[pos, i] = np.sin(angle_rates * pos).astype(np.float16)\n                pe[pos, i + 1] = np.cos(angle_rates * pos).astype(np.float16)\n    return pe.tolist()"}
{"task_id": 85, "completion_id": 9, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a given position and model dimensionality.\n    \n    Parameters:\n    - position (int): The position in the sequence.\n    - d_model (int): The model dimensionality.\n    \n    Returns:\n    - List[np.float16]: A list of positional encodings with np.float16 dtype.\n    - int: Returns -1 if input is invalid (position <= 0 or d_model <= 0).\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position = np.arange(position, dtype=np.float16)[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float16) * (-np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 1, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 2, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 3, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 4, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 5, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 6, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 7, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 86, "completion_id": 8, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1' (Overfitting), '-1' (Underfitting), or '0' (Good fit).\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    if training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    return 0"}
{"task_id": 86, "completion_id": 9, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist() if isinstance(updated_parameter, (np.ndarray, np.generic)) else round(updated_parameter, 5)\n    m = np.round(m, 5).tolist() if isinstance(m, (np.ndarray, np.generic)) else round(m, 5)\n    v = np.round(v, 5).tolist() if isinstance(v, (np.ndarray, np.generic)) else round(v, 5)\n    return (updated_parameter, m, v)\ngrad = 0.8\nm = 0.1\nv = 0.2\nt = 1"}
{"task_id": 87, "completion_id": 1, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    parameter = np.round(parameter, 5).tolist()\n    m = np.round(m, 5).tolist()\n    v = np.round(v, 5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 2, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    m_corrected = m_new / (1 - beta1 ** t)\n    v_corrected = v_new / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    updated_parameter = np.round(updated_parameter, 5).tolist()\n    m_new = np.round(m_new, 5).tolist()\n    v_new = np.round(v_new, 5).tolist()\n    return (updated_parameter, m_new, v_new)"}
{"task_id": 87, "completion_id": 3, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = np.round(parameter, 5).tolist()\n    m = np.round(m, 5).tolist()\n    v = np.round(v, 5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 4, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 5, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (updated_parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())\nparameter = 0.5\ngrad = 0.3\nm = 0.2\nv = 0.4\nt = 10"}
{"task_id": 87, "completion_id": 6, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = np.round(parameter, 5).tolist()\n    m = np.round(m, 5).tolist()\n    v = np.round(v, 5).tolist()\n    return (parameter, m, v)"}
{"task_id": 87, "completion_id": 7, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * np.square(grad)\n    m_corrected = m / (1 - np.power(beta1, t))\n    v_corrected = v / (1 - np.power(beta2, t))\n    parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 87, "completion_id": 8, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.asarray(parameter)\n    grad = np.asarray(grad)\n    m = np.asarray(m)\n    v = np.asarray(v)\n    m_new = beta1 * m + (1 - beta1) * grad\n    v_new = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m_new / (1 - beta1 ** t)\n    v_hat = v_new / (1 - beta2 ** t)\n    updated_parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    def round_and_to_list(arr):\n        return np.round(arr, 5).tolist()\n    return (round_and_to_list(updated_parameter), round_and_to_list(m_new), round_and_to_list(v_new))"}
{"task_id": 87, "completion_id": 9, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5)\n    updated_m = np.round(m, 5)\n    updated_v = np.round(v, 5)\n    return (updated_parameter.tolist(), updated_m.tolist(), updated_v.tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12, 'n_embd': 10, 'n_layer': 2, 'n_vocab': 3}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(10, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}} for _ in range(hparams['n_layer'])], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef softmax(logits, axis=-1):\n    exps = np.exp(logits - np.max(logits, axis=axis, keepdims=True))\n    return exps / np.sum(exps, axis=axis, keepdims=True)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    xhat = (x - mean) / np.sqrt(variance + eps)\n    return g * xhat + b\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    vocab_size = hparams['n_vocab']\n    n_head = hparams['n_head']\n    past = {}\n    tokens = encoder.encode(prompt)\n    context = np.array(tokens, dtype=np.int32)\n    context = np.expand_dims(context, 0)\n    token_embeddings = params['wte']\n    positional_embeddings = params['wpe']\n    n_embd = hparams['n_embd']\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(tokens)\n        pos = seq_len\n        if seq_len > hparams['n_ctx']:\n            raise ValueError('Sequence length exceeds model context limit')\n        emb = token_embeddings[context]\n        pos_emb = positional_embeddings[:seq_len]\n        x = emb + np.expand_dims(pos_emb, axis=0)\n        for block in params['blocks']:\n            residual = x\n            x = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n            c_attn = block['attn']['c_attn']\n            qkv = np.dot(x, c_attn)\n            (q, k, v) = np.split(qkv, 3, axis=-1)\n            (batch, seqlen, dim) = q.shape\n            q = q.reshape(batch, seqlen, n_head, dim // 3)\n            k = k.reshape(batch, seqlen, n_head, dim // 3)\n            v = v.reshape(batch, seqlen, n_head, dim // 3)\n            q = np.transpose(q, (2, 0, 1, 3))\n            k = np.transpose(k, (2, 0, 1, 3))\n            v = np.transpose(v, (2, 0, 1, 3))\n            norm_factor = 1.0 / np.sqrt(dim // 3)\n            attn_weights = np.einsum('bqhd,bkhd->bhqk', q, k) * norm_factor\n            attn_weights = softmax(attn_weights, axis=-1)\n            a = np.einsum('bhqk,bkhd->bqhd', attn_weights, v)\n            a = np.transpose(a, (1, 2, 0, 3)).reshape(x.shape)\n            x = layer_norm(x, block['attn']['c_proj'], None)\n            x = np.dot(a, block['attn']['c_proj'])\n            x = residual + x\n            residual = x\n            x = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n            mlp_cfc = gelu(np.dot(x, block['mlp']['c_fc']))\n            x = np.dot(mlp_cfc, block['mlp']['c_proj'])\n            x = residual + x\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.dot(x, token_embeddings.T)\n        logits = logits[:, -1, :]\n        probabilities = softmax(logits)\n        next_token = np.random.choice(vocab_size, p=probabilities[0])\n        tokens.append(int(next_token))\n    return encoder.decode(tokens)"}
{"task_id": 88, "completion_id": 1, "solution": "import numpy as np"}
{"task_id": 88, "completion_id": 2, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"\n    Generate text using a simplified GPT-2-like architecture\n    \n    Args:\n        prompt: Initial text to guide the generation\n        n_tokens_to_generate: Number of tokens to generate\n        \n    Returns:\n        Generated text as a string\n    \"\"\"\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    context = encoder.encode(prompt)\n    token_ids = np.array(context).astype('int32')\n    n_ctx = hparams['n_ctx']\n    token_ids = token_ids[-n_ctx:].tolist()\n    for _ in range(n_tokens_to_generate):\n        position = len(token_ids)\n        pos_emb = params['wpe'][position]\n        tok_emb = params['wte'][token_ids[-1]]\n        emb = tok_emb + pos_emb\n        normalized_emb = emb * params['ln_f']['g'] + params['ln_f']['b']\n        next_token_id = np.argmax(normalized_emb)\n        token_ids.append(next_token_id)\n        if next_token_id == 0:\n            break\n    return encoder.decode(token_ids)"}
{"task_id": 88, "completion_id": 3, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    context_length = len(token_ids)\n    output = np.array(token_ids, dtype=np.int32)\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(output)\n        token_embedding = params['wte'][output]\n        positional_embedding = params['wpe'][:seq_len]\n        x = token_embedding + positional_embedding\n        mean = np.mean(x, axis=-1, keepdims=True)\n        variance = np.var(x, axis=-1, keepdims=True)\n        x = (x - mean) / np.sqrt(variance + 1e-06)\n        x = x * params['ln_f']['g'] + params['ln_f']['b']\n        logits = np.dot(x, params['wte'].T)\n        last_logits = logits[-1]\n        probs = np.exp(last_logits) / np.sum(np.exp(last_logits))\n        next_token = np.random.choice(len(probs), p=probs)\n        output = np.append(output, next_token)\n    return encoder.decode(output)"}
{"task_id": 88, "completion_id": 4, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    n_embd = params['wte'].shape[1]\n    for _ in range(n_tokens_to_generate):\n        seq_len = len(token_ids)\n        tok_emb = params['wte'][token_ids]\n        pos_emb = params['wpe'][:seq_len]\n        x = tok_emb + pos_emb\n        x = (x - x.mean(axis=-1, keepdims=True)) / (x.std(axis=-1, keepdims=True) + 1e-08)\n        x = x * params['ln_f']['g'][None, :] + params['ln_f']['b'][None, :]\n        logits = x[-1:] @ params['wte'].T\n        probs = np_softmax(logits, axis=-1)\n        next_token_id = np.random.choice(len(probs[0]), p=probs[0])\n        token_ids.append(next_token_id)\n    generated_text = encoder.decode(token_ids[n_tokens_to_generate:])\n    return generated_text\ndef np_softmax(x, axis=None):\n    x = x - np.max(x, axis=axis, keepdims=True)\n    exp_x = np.exp(x)\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"}
{"task_id": 88, "completion_id": 5, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    block = {'attn': {'c_attn': {'w': np.random.rand(10, 30), 'b': np.zeros(30)}, 'c_proj': {'w': np.random.rand(10, 10), 'b': np.zeros(10)}}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': {'w': np.random.rand(10, 40), 'b': np.zeros(40)}, 'c_proj': {'w': np.random.rand(40, 10), 'b': np.zeros(10)}}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}\n    params['blocks'].append(block)\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    context = encoder.encode(prompt)\n    tokens = np.array(context)\n    token_embeddings = params['wte'][tokens]\n    positions = np.arange(tokens.shape[0])\n    positional_embeddings = params['wpe'][positions]\n    total_embeddings = token_embeddings + positional_embeddings\n    for _ in range(n_tokens_to_generate):\n        x = total_embeddings\n        for block in params['blocks']:\n            residual = x\n            (x, mean, var) = layernorm(x, block['ln_1']['g'], block['ln_1']['b'])\n            (b, n, d) = x.shape\n            c_attn_out = x @ block['attn']['c_attn']['w'] + block['attn']['c_attn']['b']\n            d_hid = d / 3\n            (q, k, v) = np.split(c_attn_out, 3, axis=-1)\n            attn_scores = q @ k.transpose(0, 2, 1) / np.sqrt(d_hid)\n            attn_weights = softmax(attn_scores)\n            attn_weights = causal_mask(attn_weights, n)\n            attn_output = attn_weights @ v\n            proj = attn_output @ block['attn']['c_proj']['w'] + block['attn']['c_proj']['b']\n            x = proj + residual\n            residual = x\n            (x, mean, var) = layernorm(x, block['ln_2']['g'], block['ln_2']['b'])\n            mlp_in = x @ block['mlp']['c_fc']['w'] + block['mlp']['c_fc']['b']\n            mlp_out = mlp_in * (mlp_in > 0)\n            mlp_output = mlp_out @ block['mlp']['c_proj']['w'] + block['mlp']['c_proj']['b']\n            x = mlp_output + residual\n        (x, _, _) = layernorm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = x[0, -1, :]\n        probs = softmax(logits)\n        next_token = np.random.choice(len(probs), p=probs)\n        tokens = np.append(tokens, next_token)\n        pos = len(tokens) - 1\n        token_embedding = params['wte'][next_token]\n        positional_embedding = params['wpe'][pos]\n        new_embedding = token_embedding + positional_embedding\n        x = np.concatenate([x[0], new_embedding.reshape(1, -1)], axis=0)\n        total_embeddings = x[np.newaxis, ...]\n    return encoder.decode(tokens.tolist())\ndef layernorm(x, gamma, beta, eps=1e-05):\n    \"\"\"Layer normalization implementation.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    x_hat = (x - mean) / np.sqrt(var + eps)\n    out = gamma * x_hat + beta\n    return (out, mean, var)\ndef softmax(x):\n    \"\"\"Softmax implementation.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\ndef causal_mask(x, n):\n    \"\"\"Apply causal mask to attention weights.\"\"\"\n    mask = 1.0 - np.triu(np.ones((n, n)), 1)\n    return x * mask"}
{"task_id": 88, "completion_id": 6, "solution": "import numpy as np"}
{"task_id": 88, "completion_id": 7, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12, 'n_embd': 10}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': {'w': np.random.rand(10, 10)}, 'c_proj': {'w': np.random.rand(10, 10)}}, 'mlp': {'c_fc': {'w': np.random.rand(10, 10)}, 'c_proj_mlp': {'w': np.random.rand(10, 10)}}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef softmax(x):\n    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exps / exps.sum(axis=-1, keepdims=True)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = x.mean(axis=-1, keepdims=True)\n    var = x.var(axis=-1, keepdims=True)\n    std = np.sqrt(var + eps)\n    return g * (x - mean) / std + b\ndef matmul(a, b):\n    return a @ b\ndef multi_head_attention(Q, K, V, head_size, n_head):\n\n    def split_heads(x):\n        return x.reshape(x.shape[0], x.shape[1], n_head, -1).transpose(0, 2, 1, 3)\n    Q = split_heads(Q)\n    K = split_heads(K)\n    V = split_heads(V)\n    scores = matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(head_size)\n    attention_weights = softmax(scores)\n    output = matmul(attention_weights, V)\n    output = output.transpose(0, 2, 1, 3).reshape(output.shape[0], output.shape[2], -1)\n    return output\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    vocab_size = params['wte'].shape[0]\n    embed_dim = params['wte'].shape[1]\n    n_head = hparams['n_head']\n    head_size = embed_dim // n_head\n    block = params['blocks'][0]\n    token_ids = encoder.encode(prompt)\n    token_ids = np.array(token_ids).astype(np.int32)\n    generation_tokens = len(token_ids)\n    for _ in range(n_tokens_to_generate):\n        wte = params['wte'][token_ids]\n        wpe = params['wpe'][:generation_tokens]\n        x = wte + wpe\n        ln_1 = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n        qkv = matmul(ln_1, block['attn']['c_attn']['w'])\n        (Q, K, V) = (qkv[:, :, :embed_dim], qkv[:, :, embed_dim:2 * embed_dim], qkv[:, :, 2 * embed_dim:])\n        attention_out = multi_head_attention(Q, K, V, head_size, n_head)\n        attention_proj = matmul(attention_out, block['attn']['c_proj']['w'])\n        x = x + attention_proj\n        ln_2 = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n        mlp_in = matmul(ln_2, block['mlp']['c_fc']['w'])\n        mlp_out = matmul(np.tanh(mlp_in), block['mlp']['c_proj_mlp']['w'])\n        x = x + mlp_out\n        final_out = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = matmul(final_out, params['wte'].T)\n        last_logit = logits[-1, :]\n        new_token_id = np.argmax(last_logit)\n        token_ids = np.append(token_ids, new_token_id)\n        generation_tokens += 1\n    return encoder.decode(token_ids.tolist())"}
{"task_id": 88, "completion_id": 8, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_head = hparams['n_head']\n\n    def softmax(x):\n        x -= np.max(x, axis=-1, keepdims=True)\n        return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n\n    def layer_norm(x, g, b, eps=1e-05):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        variance = np.var(x, axis=-1, keepdims=True)\n        x_norm = (x - mean) / np.sqrt(variance + eps)\n        return g * x_norm + b\n\n    def gelu(x):\n        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\n\n    def attention(q, k, v):\n        d_k = q.shape[-1]\n        scores = q @ k.T / np.sqrt(d_k)\n        scores = softmax(scores)\n        attention_output = scores @ v\n        return attention_output\n\n    def multi_head_attention(q, k, v, n_head):\n        (batch_size, seq_len, d_model) = q.shape\n        d_k = d_model // n_head\n        q_heads = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n        k_heads = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n        v_heads = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n        attention_output = [attention(q_heads[i], k_heads[i], v_heads[i]) for i in range(n_head)]\n        attention_output = np.stack(attention_output, axis=1)\n        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n        return attention_output\n    token_ids = encoder.encode(prompt)\n    n_seq = len(token_ids)\n    input_emb = params['wte'][token_ids, :] + params['wpe'][:n_seq, :]\n    block_params = {'attn': {'c_attn': np.random.randn(10, 30), 'c_proj': np.random.randn(10, 10)}, 'mlp': {'c_fc': np.random.randn(10, 20), 'c_proj': np.random.randn(20, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}\n    params['blocks'] = [block_params]\n    for _ in range(n_tokens_to_generate):\n        n_block = len(params['blocks'])\n        for block_idx in range(n_block):\n            block = params['blocks'][block_idx]\n            x = input_emb\n            x_ln_1 = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n            qkv = x_ln_1 @ block['attn']['c_attn']\n            (q, k, v) = np.split(qkv, 3, axis=-1)\n            attention_output = multi_head_attention(q, k, v, n_head)\n            x = x + attention_output\n            x_ln_2 = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n            fc_output = gelu(x_ln_2 @ block['mlp']['c_fc'])\n            mlp_output = fc_output @ block['mlp']['c_proj']\n            input_emb = x + mlp_output\n        x = layer_norm(input_emb, params['ln_f']['g'], params['ln_f']['b'])\n        logits = x @ params['wte'].T\n        next_token_id = np.argmax(logits[-1])\n        token_ids.append(next_token_id)\n        n_seq += 1\n        wpe = params['wpe'][:n_seq, :]\n        input_emb = params['wte'][token_ids, :] + wpe\n    return encoder.decode(token_ids)"}
{"task_id": 88, "completion_id": 9, "solution": "import numpy as np\nimport copy\ndef softmax(x):\n    \"\"\"Compute softmax values for each row.\"\"\"\n    exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_values / exp_values.sum(axis=-1, keepdims=True)\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x ** 3)))\ndef layer_norm(x, gamma, beta, eps=1e-05):\n    \"\"\"Layer normalization.\"\"\"\n    mean = x.mean(axis=-1, keepdims=True)\n    variance = x.var(axis=-1, keepdims=True)\n    std = np.sqrt(variance + eps)\n    return gamma * (x - mean) / std + beta\ndef split_heads(x, n_head):\n    \"\"\"Reshape and transpose for multi-head attention.\"\"\"\n    (N, T, D) = x.shape\n    d_head = D // n_head\n    return x.reshape(N, T, n_head, d_head).transpose(0, 2, 1, 3)\ndef combine_heads(x):\n    \"\"\"Transpose and reshape back from multi-head attention.\"\"\"\n    (N, H, T, D_H) = x.shape\n    return x.transpose(0, 2, 1, 3).reshape(N, T, H * D_H)\ndef attention(query, key, value, mask=None):\n    \"\"\"Scaled dot-product attention.\"\"\"\n    d_k = query.shape[-1]\n    scores = query @ key.transpose(0, 1, 3, 2) / np.sqrt(d_k)\n    if mask is not None:\n        scores = np.where(mask, scores, -1000000000.0)\n    attention_weights = softmax(scores)\n    output = attention_weights @ value\n    return output\ndef forward_pass(x, wte, wpe, blocks, ln_f, hparams, generate=False, past=None):\n    \"\"\"Forward pass through a GPT-2-like model.\"\"\"\n    n_head = hparams['n_head']\n    max_length = hparams['n_ctx']\n    (N, T) = x.shape\n    token_embeddings = wte[x]\n    pos_embeddings = wpe[:T]\n    x = token_embeddings + pos_embeddings\n    if past is None:\n        past_length = 0\n    else:\n        past_length = past[0][0].shape[1]\n    presents = []\n    for (i, block) in enumerate(blocks):\n        query = block['attn']['c_attn']['wq'] @ x\n        key = block['attn']['c_attn']['wk'] @ x\n        value = block['attn']['c_attn']['wv'] @ x\n        query = split_heads(query, n_head)\n        key = split_heads(key, n_head)\n        value = split_heads(value, n_head)\n        (bsz, _, seq_len, _) = query.shape\n        mask = np.triu(np.ones((1, 1, seq_len, seq_len)), k=1).astype('bool')\n        mask = np.tile(mask, [bsz, 1, 1, 1])\n        if past is not None:\n            past_key = past[2 * i]\n            past_value = past[2 * i + 1]\n            key = np.concatenate([past_key, key], axis=2)\n            value = np.concatenate([past_value, value], axis=2)\n        present = (key, value)\n        output = attention(query, key, value, mask=mask)\n        output = combine_heads(output)\n        output = block['attn']['c_proj']['w'] @ output\n        x = x + output\n        x = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n        ffn1 = gelu(block['mlp']['c_fc']['w'] @ x)\n        ffn2 = block['mlp']['c_proj']['w'] @ ffn1\n        x = x + ffn2\n        x = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n        if past is not None:\n            presents.append(present)\n    x = layer_norm(x, ln_f['g'], ln_f['b'])\n    if generate:\n        return (block['attn']['c_attn']['wq'] @ x, presents)\n    else:\n        return x @ block['attn']['c_attn']['wq'].T\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(20, 10), 'blocks': [{'attn': {'c_attn': {'wq': np.random.rand(10, 10), 'wk': np.random.rand(10, 10), 'wv': np.random.rand(10, 10)}, 'c_proj': {'w': np.random.rand(10, 40)}}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': {'w': np.random.rand(10, 20)}, 'c_proj': {'w': np.random.rand(20, 10)}}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        input_tensor = np.array([input_ids[-1:]])\n        (logits, past) = forward_pass(input_tensor, params['wte'], params['wpe'], params['blocks'], params['ln_f'], hparams, generate=True)\n        probs = softmax(logits[0, -1, :])\n        next_token_id = int(np.random.choice(len(probs), p=probs))\n        input_ids.append(next_token_id)\n    return encoder.decode(input_ids)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        \"\"\"Apply the softmax function to the given values.\"\"\"\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    W_q = np.random.randn(dimension, dimension)\n    W_k = np.random.randn(dimension, dimension)\n    W_v = np.random.randn(dimension, dimension)\n    Q = np.dot(crystal_values, W_q)\n    K = np.dot(crystal_values, W_k)\n    V = np.dot(crystal_values, W_v)\n    scores = np.dot(Q, K.T)\n    attention_weights = softmax(scores / np.sqrt(dimension))\n    pattern_output = np.dot(attention_weights, V)\n    output = [round(val, 4) for val in pattern_output.flatten()]\n    return output"}
{"task_id": 89, "completion_id": 1, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        values_exp = np.exp(values - np.max(values))\n        return values_exp / np.sum(values_exp)\n    values = np.array(crystal_values).reshape(1, n)\n    W_q = np.random.randn(n, dimension)\n    W_k = np.random.randn(n, dimension)\n    W_v = np.random.randn(n, dimension)\n    Q = np.dot(values, W_q)\n    K = np.dot(values, W_k)\n    V = np.dot(values, W_v)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    output = np.dot(attention_weights, V)\n    result = list(map(lambda x: round(x, 4), output[0]))\n    return result"}
{"task_id": 89, "completion_id": 2, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exps = np.exp(values - np.max(values))\n        return exps / exps.sum(axis=1, keepdims=True)\n    q = np.reshape(crystal_values, (1, n, dimension))\n    k = np.reshape(crystal_values, (1, n, dimension))\n    v = np.reshape(crystal_values, (1, n, dimension))\n    attention_scores = np.matmul(q, k.transpose(0, 2, 1))\n    attention_weights = softmax(attention_scores[0])\n    weighted_pattern = np.matmul(attention_weights, v[0])\n    result = [round(x[0], 4) for x in weighted_pattern]\n    return result"}
{"task_id": 89, "completion_id": 3, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values, axis=-1, keepdims=True))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    crystals = np.array(crystal_values).reshape(1, n, 1)\n    W_q = np.random.randn(dimension)\n    W_k = np.random.randn(dimension)\n    queries = np.dot(crystals, W_q).reshape(1, n)\n    keys = np.dot(crystals, W_k).reshape(1, n)\n    scores = np.matmul(queries, keys.T)\n    attention_weights = softmax(scores)\n    values = crystals.reshape(1, n)\n    output = np.sum(attention_weights * values, axis=1)\n    return [round(float(score), 4) for score in output[0]]"}
{"task_id": 89, "completion_id": 4, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    values = np.array(crystal_values).reshape(n, -1)\n    Q = values\n    K = values\n    V = values\n    d_k = dimension\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = softmax(attention_scores)\n    output_patterns = np.matmul(attention_weights, V)\n    final_output = [round(float(val), 4) for val in output_patterns.flatten()]\n    return final_output\nn = 3\ndimension = 1"}
{"task_id": 89, "completion_id": 5, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum()\n    crystal_values = np.array(crystal_values)\n    attention_scores = np.dot(crystal_values, crystal_values.T)\n    attention_weights = np.zeros((n, n))\n    for i in range(n):\n        attention_weights[i] = softmax(attention_scores[i])\n    pattern = np.dot(attention_weights, crystal_values)\n    pattern_rounded = [float(np.round(val, 4)) for val in pattern]\n    return pattern_rounded"}
{"task_id": 89, "completion_id": 6, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum(axis=-1, keepdims=True)\n    crystal_vectors = np.array(crystal_values).reshape(n, dimension)\n    keys = crystal_vectors\n    queries = crystal_vectors\n    attention_scores = np.dot(queries, keys.T) / np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    values = crystal_vectors\n    output = np.dot(attention_weights, values)\n    final_pattern = output.flatten()\n    return [round(x, 4) for x in final_pattern]"}
{"task_id": 89, "completion_id": 7, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values, axis=-1, keepdims=True))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    embedding_matrix = np.random.rand(n, dimension)\n    q = embedding_matrix @ crystal_values\n    k = embedding_matrix @ crystal_values\n    v = embedding_matrix @ crystal_values\n    attention_scores = q @ k.T\n    attention_scores /= np.sqrt(dimension)\n    attention_weights = softmax(attention_scores)\n    output = attention_weights @ v\n    return [round(float(val), 4) for val in output.diagonal()]\nn = 3\ncrystal_values = np.array([1.0, 2.0, 3.0])\ndimension = 2"}
{"task_id": 89, "completion_id": 8, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    W_q = np.random.randn(dimension, n)\n    W_k = np.random.randn(dimension, n)\n    W_v = np.random.randn(dimension, n)\n    Q = np.dot(W_q, crystal_values)\n    K = np.dot(W_k, crystal_values)\n    V = np.dot(W_v, crystal_values)\n    scores = np.dot(Q, K.T) / np.sqrt(dimension)\n    attention_weights = np.apply_along_axis(softmax, 1, scores)\n    output = np.dot(attention_weights, V)\n    return [round(val, 4) for val in output]\nn = 4\ncrystal_values = np.array([1.0, 2.0, 3.0, 4.0])\ndimension = 2"}
{"task_id": 89, "completion_id": 9, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum(axis=-1, keepdims=True)\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n    Q = crystal_values\n    K = crystal_values\n    V = crystal_values\n    d_k = dimension\n    attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = softmax(attention_scores)\n    output = np.matmul(attention_weights, V)\n    final_pattern = [round(float(value), 4) for value in output.flatten()]\n    return final_pattern"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    query_terms = query.lower().split()\n    corpus_terms = [doc.lower().split() for doc in corpus]\n    df = Counter()\n    for terms in corpus_terms:\n        unique_terms = set(terms)\n        for term in unique_terms:\n            df[term] += 1\n    N = len(corpus)\n    idf = {term: np.log((N - df[term] + 0.5) / (df[term] + 0.5)) for term in df}\n    doc_lengths = np.array([len(terms) for terms in corpus_terms])\n    avg_doc_length = np.mean(doc_lengths)\n    scores = []\n    for (i, terms) in enumerate(corpus_terms):\n        total_score = 0.0\n        doc_len = len(terms)\n        for term in query_terms:\n            if term in idf:\n                tf = terms.count(term)\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_length))\n                score_term = idf[term] * (numerator / denominator)\n                total_score += score_term\n        scores.append(round(total_score, 3))\n    return scores\ncorpus = ['Information retrieval is an important field in computer science.', 'BM25 is a ranking function used in information retrieval systems.', 'Python is a popular programming language for data analysis.']\nquery = 'information retrieval'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 90, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Compute BM25 scores for a given query and corpus of documents.\n    \n    Parameters:\n        - corpus: list of str, the documents in the collection.\n        - query: str, the search query.\n        - k1, b: BM25 parameters for term frequency and length normalization.\n    \n    Returns:\n        - list of float: BM25 scores for each document in the corpus,\n                         normalized and rounded to three decimal places.\n    \"\"\"\n\n    def tokenize(text):\n        return text.lower().split()\n    query_terms = tokenize(query)\n    avgdl = 0\n    N = len(corpus)\n    doc_sizes = []\n    term_freqs = []\n    for doc in corpus:\n        doc_tokens = tokenize(doc)\n        doc_size = len(doc_tokens)\n        doc_sizes.append(doc_size)\n        avgdl += doc_size\n        term_freqs.append(Counter(doc_tokens))\n    avgdl /= N\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0.0\n        doc_size = doc_sizes[i]\n        freqs = term_freqs[i]\n        for term in query_terms:\n            if term in freqs:\n                tf = freqs[term]\n                df = sum((1 for d in term_freqs if term in d))\n                idf = np.log((N - df + 0.5) / (df + 0.5) + 1)\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * doc_size / avgdl)\n                score += idf * numerator / denominator\n        scores.append(score)\n    return [round(s, 3) for s in scores]"}
{"task_id": 90, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus against the given query.\n    \n    Parameters:\n        corpus (list of str): List of documents.\n        query (str): The query string.\n        k1 (float): Term frequency saturation parameter (default: 1.5)\n        b (float): Document length normalization constant (default: 0.75)\n    \n    Returns:\n        list of float: BM25 scores for each document in the corpus.\n    \"\"\"\n    corpus_tokens = [doc.lower().split() for doc in corpus]\n    query_tokens = query.lower().split()\n    N = len(corpus)\n    avgdl = np.mean([len(tokens) for tokens in corpus_tokens])\n    df = Counter()\n    for tokens in corpus_tokens:\n        df.update(set(tokens))\n    scores = []\n    for (doc_id, doc_tokens) in enumerate(corpus_tokens):\n        score = 0.0\n        dl = len(doc_tokens)\n        for q_term in query_tokens:\n            if q_term in df:\n                ni = df[q_term]\n                fi = doc_tokens.count(q_term)\n                idf = np.log((N - ni + 0.5) / (ni + 0.5) + 1)\n                ctf = fi\n                term_score = idf * (ctf * (k1 + 1) / (ctf + k1 * (1 - b + b * (dl / avgdl))))\n                score += term_score\n        scores.append(score)\n    return [round(score, 3) for score in scores]"}
{"task_id": 90, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    N = len(corpus)\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    query_terms = set(query.lower().split())\n    doc_freq = {}\n    for term in query_terms:\n        df = 0\n        for doc in corpus:\n            if term in doc.lower():\n                df += 1\n        doc_freq[term] = df\n    bm25_scores = []\n    for doc in corpus:\n        doc_terms = doc.lower().split()\n        doc_len = len(doc_terms)\n        score = 0.0\n        for term in query_terms:\n            tf = doc_terms.count(term)\n            if tf == 0:\n                continue\n            df = doc_freq[term] if term in doc_freq else 0\n            idf = log((N - df + 0.5) / (df + 0.5))\n            ctf = tf * (k1 + 1) / (tf + k1 * (1 - b + b * (doc_len / avgdl)))\n            score += idf * ctf\n        bm25_scores.append(round(score, 3))\n    return bm25_scores\ncorpus = ['Natural language processing is a field of artificial intelligence.', 'Information retrieval is an important part of natural language processing.', 'Machine learning can be used to improve search engines and document retrieval.', 'BM25 is a ranking function used in information retrieval.']"}
{"task_id": 90, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    documents = [doc.lower().split() for doc in corpus]\n    query_terms = query.lower().split()\n    num_docs = len(documents)\n    avgdl = np.mean([len(doc) for doc in documents])\n    df = Counter()\n    for doc in documents:\n        unique_terms = set(doc)\n        for term in unique_terms:\n            df[term] += 1\n    idf = {}\n    for term in query_terms:\n        idf[term] = np.log((num_docs - df[term] + 0.5) / (df[term] + 0.5) + 1.0)\n    scores = []\n    for doc in documents:\n        doc_len = len(doc)\n        score = 0.0\n        for term in query_terms:\n            if term in df:\n                tf = doc.count(term)\n                numerator = tf * (k1 + 1)\n                denominator = tf + k1 * (1 - b + b * doc_len / avgdl)\n                score += idf[term] * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['The quick brown fox jumps over the lazy dog', 'Never jump over the lazy dog quickly', 'The lazy dog often lounges around', 'Foxes are clever and quick']"}
{"task_id": 90, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def tokenize(text):\n        return text.lower().split()\n    documents = [tokenize(doc) for doc in corpus]\n    doc_lengths = [len(doc) for doc in documents]\n    avg_doc_length = np.mean(doc_lengths)\n    vocab = set((term for doc in documents for term in doc))\n    N = len(documents)\n    idf = {}\n    for term in vocab:\n        num_docs_with_term = sum((1 for doc in documents if term in doc))\n        idf[term] = log((N - num_docs_with_term + 0.5) / (num_docs_with_term + 0.5))\n    scores = []\n    for (i, doc) in enumerate(documents):\n        score = 0.0\n        for term in query:\n            term = term.lower()\n            if term in idf:\n                tf = doc.count(term)\n                K = k1 * (1 - b + b * (doc_lengths[i] / avg_doc_length))\n                score += idf[term] * (tf * (k1 + 1)) / (tf + K)\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['The quick brown fox jumps over the lazy dog', 'Never jump over the fence without thinking', 'The fox is quick and very brown', 'Lazy dogs tend to be less active']\nquery = 'quick fox'"}
{"task_id": 90, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus against the given query.\n    \n    Parameters:\n    - corpus (list of str): List of document texts.\n    - query (str): The query string.\n    - k1 (float): Term frequency saturation parameter.\n    - b (float): Document length normalization parameter.\n    \n    Returns:\n    - list of float: BM25 score for each document, rounded to 3 decimal places.\n    \"\"\"\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    doc_token_counts = [Counter(doc.split()) for doc in corpus]\n    query_terms = set(query.split())\n    N = len(corpus)\n    scores = []\n    for (i, tokens) in enumerate(doc_token_counts):\n        score = 0.0\n        doc_length = len(tokens)\n        for term in query_terms:\n            tf = tokens.get(term, 0)\n            doc_freq = sum((1 for d in corpus if term in d.split()))\n            idf = np.log((N - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * doc_length / avgdl)\n            score += idf * numerator / denominator\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['Information retrieval is an important topic.', 'Machine learning techniques are changing the field of AI.', 'Information retrieval systems use algorithms to find documents.']"}
{"task_id": 90, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter\nfrom math import log\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    corpus = [doc.lower().split() for doc in corpus]\n    query = query.lower().split()\n    N = len(corpus)\n    df = Counter()\n    doc_lengths = []\n    avgdl = 0\n    for doc in corpus:\n        doc_lengths.append(len(doc))\n        avgdl += len(doc)\n    avgdl /= N\n    for term in set(query):\n        for (i, doc) in enumerate(corpus):\n            if term in doc:\n                df[term] += 1\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        doc_counter = Counter(doc)\n        score = 0.0\n        for term in query:\n            if term in df and df[term] > 0:\n                ni = df[term]\n                fi = doc_counter.get(term, 0)\n                k = k1 * (1 - b + b * (doc_lengths[i] / avgdl))\n                score += log(N / ni) * (fi * (k1 + 1) / (fi + k))\n        scores.append(score)\n    scores = [round(score, 3) for score in scores]\n    return scores"}
{"task_id": 90, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\nimport math\nfrom typing import List\ndef calculate_bm25_scores(corpus: List[str], query: str, k1: float=1.5, b: float=0.75) -> List[float]:\n    \"\"\"\n    Calculate BM25 scores for a given query on a corpus of documents.\n    \n    Parameters:\n    -----------\n    corpus: List[str]\n        A list of strings, each string is a document.\n    query: str\n        The query string.\n    k1: float, optional (default=1.5)\n        Term frequency saturation parameter.\n    b: float, optional (default=0.75)\n        Document length normalization parameter.\n\n    Returns:\n    --------\n    List[float]\n        BM25 scores of all documents, rounded to three decimal places.\n    \"\"\"\n    corpus_tokens = [doc.lower().split() for doc in corpus]\n    query_tokens = query.lower().split()\n    query_token_set = set(query_tokens)\n    N = len(corpus)\n    doc_lengths = [len(doc) for doc in corpus_tokens]\n    avgdl = sum(doc_lengths) / N\n\n    def idf(term):\n        n_q = sum((1 for doc in corpus_tokens if term in doc))\n        return math.log((N - n_q + 0.5) / (n_q + 0.5) + 1)"}
{"task_id": 90, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    N = len(corpus)\n    doc_lengths = [len(doc.split()) for doc in corpus]\n    avgdl = np.mean(doc_lengths)\n    tf = [Counter(doc.split()) for doc in corpus]\n    query_terms = set(query.split())\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0.0\n        doc_len = len(doc.split())\n        for term in query_terms:\n            ni = sum((1 for d in corpus if term in d.split()))\n            idf = np.log((N - ni + 0.5) / (ni + 0.5) + 1)\n            tf_id = tf[i].get(term, 0)\n            score += idf * (tf_id * (k1 + 1) / (tf_id + k1 * (1 - b + b * (doc_len / avgdl))))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['machine learning is fun', 'deep learning is also fascinating', 'natural language processing is interesting', 'I love python programming']\nquery = 'learning python'\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be equal.')\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true_label, pred_label) in zip(y_true, y_pred):\n        if true_label == 1 and pred_label == 1:\n            true_positives += 1\n        elif true_label == 0 and pred_label == 1:\n            false_positives += 1\n        elif true_label == 1 and pred_label == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0.0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0.0\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 1, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = fn = fp = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 91, "completion_id": 2, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 1))\n    fp = sum((1 for (t, p) in zip(y_true, y_pred) if t == 0 and p == 1))\n    fn = sum((1 for (t, p) in zip(y_true, y_pred) if t == 1 and p == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        return 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)"}
{"task_id": 91, "completion_id": 3, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((t == 1 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fp = sum((t == 0 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fn = sum((t == 1 and p == 0 for (t, p) in zip(y_true, y_pred)))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0]\ny_pred = [1, 0, 0, 1, 0]"}
{"task_id": 91, "completion_id": 4, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must be of the same length.')\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if pred == 1 and true == 1:\n            tp += 1\n        elif pred == 1 and true == 0:\n            fp += 1\n        elif pred == 0 and true == 1:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)"}
{"task_id": 91, "completion_id": 5, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f1 = 0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 0]\nf1 = calculate_f1_score(y_true, y_pred)"}
{"task_id": 91, "completion_id": 6, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be equal.')\n    TP = sum((t == 1 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    FP = sum((t == 0 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    FN = sum((t == 1 and p == 0 for (t, p) in zip(y_true, y_pred)))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 0]"}
{"task_id": 91, "completion_id": 7, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if pred == 1 and true == 1:\n            tp += 1\n        elif pred == 1 and true == 0:\n            fp += 1\n        elif pred == 0 and true == 1:\n            fn += 1\n    if tp == 0:\n        precision = 0.0\n        recall = 0.0\n    else:\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 91, "completion_id": 8, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum((t == 1 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fp = sum((t == 0 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fn = sum((t == 1 and p == 0 for (t, p) in zip(y_true, y_pred)))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 1]"}
{"task_id": 91, "completion_id": 9, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('True and predicted label lists must be of the same length.')\n    tp = fp = fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    day_numbers = np.array([i + 1 for i in range(len(consumption_data))])\n    fluctuations = 10 * np.sin(2 * PI * day_numbers / 10)\n    detrended_data = consumption_data - fluctuations\n    n = len(day_numbers)\n    x = day_numbers\n    y = detrended_data\n    x_avg = np.mean(x)\n    y_avg = np.mean(y)\n    numerator = np.sum((x - x_avg) * (y - y_avg))\n    denominator = np.sum((x - x_avg) ** 2)\n    m = numerator / denominator\n    b = y_avg - m * x_avg\n    base_consumption_day15 = m * 15 + b\n    fluctuation_day15 = 10 * np.sin(2 * PI * 15 / 10)\n    total_consumption_day15 = base_consumption_day15 + fluctuation_day15\n    rounded_prediction = round(total_consumption_day15)\n    safety_margin = int(rounded_prediction * 1.05)\n    return safety_margin\nconsumption_data = [120, 130, 125, 135, 140, 120, 115, 110, 105, 110]"}
{"task_id": 92, "completion_id": 1, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    days = list(range(1, 11))\n    n = len(days)\n    sum_x = sum(days)\n    sum_y = sum(detrended_data)\n    sum_xy = sum((x * y for (x, y) in zip(days, detrended_data)))\n    sum_x2 = sum((x * x for x in days))\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n    day_15_base = m * 15 + b\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_prediction = day_15_base + fluctuation_day_15\n    safety_margin = total_prediction * 0.05\n    final_prediction = total_prediction + safety_margin\n    final_energy_needed = math.ceil(final_prediction)\n    return final_energy_needed"}
{"task_id": 92, "completion_id": 2, "solution": "import math\nimport numpy as np\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended = consumption_data[i - 1] - fluctuation\n        detrended_data.append(detrended)\n    x = np.array([1 + i for i in range(10)])\n    y = np.array(detrended_data)\n    n = len(x)\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    numerator = sum(((x[i] - x_mean) * (y[i] - y_mean) for i in range(n)))\n    denominator = sum(((x[i] - x_mean) ** 2 for i in range(n)))\n    slope = numerator / denominator\n    intercept = y_mean - slope * x_mean\n    base_consumption_day15 = slope * 15 + intercept\n    fluctuation_day15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_consumption_day15 = base_consumption_day15 + fluctuation_day15\n    safety_margin = total_consumption_day15 * 0.05\n    forecasted_value = total_consumption_day15 + safety_margin\n    final_forecast = round(forecasted_value)\n    return final_forecast"}
{"task_id": 92, "completion_id": 3, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended = consumption_data[i - 1] - fluctuation\n        detrended_data.append(detrended)\n    n = len(detrended_data)\n    x = list(range(1, n + 1))\n    y = detrended_data\n    sum_x = sum(x)\n    sum_y = sum(y)\n    sum_xy = sum([xi * yi for (xi, yi) in zip(x, y)])\n    sum_xx = sum([xi ** 2 for xi in x])\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n    base_consumption_day15 = m * 15 + b\n    fluctuation_day15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_consumption_day15 = base_consumption_day15 + fluctuation_day15\n    safety_margin = 1.05\n    final_consumption = math.ceil(total_consumption_day15 * safety_margin)\n    return final_consumption\nconsumption_data = [55.3, 63.1, 72.5, 80.8, 88.5, 96.0, 102.7, 108.5, 113.9, 118.1]"}
{"task_id": 92, "completion_id": 4, "solution": "import math\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended = []\n    for (i, data) in enumerate(consumption_data):\n        fluctuation = 10 * math.sin(2 * PI * (i + 1) / 10)\n        detrended.append(data - fluctuation)\n    X = np.array([i + 1 for i in range(10)]).reshape((-1, 1))\n    y = np.array(detrended).reshape((-1, 1))\n    A = np.hstack([X, np.ones((X.shape[0], 1))])\n    beta = np.linalg.inv(A.T @ A) @ A.T @ y\n    (slope, intercept) = beta.flat\n    day_15_base = slope * 15 + intercept\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = day_15_base + day_15_fluctuation\n    rounded = round(predicted_consumption)\n    safety_margin = math.ceil(rounded * 1.05)\n    return safety_margin"}
{"task_id": 92, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef power_grid_forecast(consumption_data):\n    day_numbers = np.array([i for i in range(1, 11)])\n    fluctuations = 10 * np.sin(2 * math.pi * day_numbers / 10)\n    detrended_data = np.array(consumption_data) - fluctuations\n    X = np.vstack([day_numbers, np.ones(len(day_numbers))]).T\n    (slope, intercept) = np.linalg.lstsq(X, detrended_data, rcond=None)[0]\n    day_15 = 15\n    base_consumption_day_15 = slope * day_15 + intercept\n    fluctuation_day_15 = 10 * math.sin(2 * math.pi * day_15 / 10)\n    total_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    safety_margin_day_15 = total_consumption_day_15 * 1.05\n    final_forecast = math.ceil(safety_margin_day_15 + 0.5)\n    return int(final_forecast)\nconsumption_data = [120, 132, 143, 155, 171, 186, 199, 207, 210, 223]"}
{"task_id": 92, "completion_id": 6, "solution": "import math\nfrom math import pi\nfrom math import isclose\ndef round_up(x):\n    return int(x + 1e-08)\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * math.pi * (i + 1) / 10) for i in range(10)]\n    days = list(range(1, 11))\n    n = len(days)\n    sum_x = sum(days)\n    sum_y = sum(detrended_data)\n    sum_xy = sum([days[i] * detrended_data[i] for i in range(n)])\n    sum_x2 = sum([x ** 2 for x in days])\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n    day_15_base = m * 15 + b\n    fluctuation_15 = 10 * math.sin(2 * math.pi * 15 / 10)\n    day_15_forecast = day_15_base + fluctuation_15\n    safety_margin = day_15_forecast * 1.05\n    final_forecast = round_up(safety_margin)\n    return final_forecast"}
{"task_id": 92, "completion_id": 7, "solution": "import math\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * math.pi * i / 10)\n        detrended = consumption - fluctuation\n        detrended_data.append((i, detrended))\n    n = len(detrended_data)\n    sum_x = sum((day for (day, _) in detrended_data))\n    sum_y = sum((value for (_, value) in detrended_data))\n    sum_xy = sum((day * value for (day, value) in detrended_data))\n    sum_x2 = sum((day * day for (day, _) in detrended_data))\n    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)\n    intercept = (sum_y - slope * sum_x) / n\n    base_day_15 = slope * 15 + intercept\n    fluctuation_day_15 = 10 * math.sin(2 * math.pi * 15 / 10)\n    predicted_consumption = base_day_15 + fluctuation_day_15\n    rounded_consumption = round(predicted_consumption)\n    safety_margin = math.ceil(rounded_consumption * 0.05)\n    final_forecast = rounded_consumption + safety_margin\n    return final_forecast"}
{"task_id": 92, "completion_id": 8, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * math.pi * (i + 1) / 10) for i in range(10)]\n    x = np.array([i + 1 for i in range(10)]).reshape(-1, 1)\n    y = np.array(detrended_data).reshape(-1, 1)\n    reg = LinearRegression().fit(x, y)\n    slope = reg.coef_[0][0]\n    intercept = reg.intercept_[0]\n    base_consumption_day_15 = slope * 15 + intercept\n    fluctuation_day_15 = 10 * math.sin(2 * math.pi * 15 / 10)\n    total_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    safety_margin = 1.05\n    final_forecast = math.ceil(total_consumption_day_15 * safety_margin)\n    return final_forecast\nconsumption_data = [120, 130, 145, 160, 170, 185, 200, 215, 230, 245]"}
{"task_id": 92, "completion_id": 9, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended.append(consumption_data[i - 1] - fluctuation)\n    days = list(range(1, 11))\n    n = len(detrended)\n    sum_x = sum(days)\n    sum_y = sum(detrended)\n    sum_xy = sum([d * y for (d, y) in zip(days, detrended)])\n    sum_x2 = sum([d ** 2 for d in days])\n    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    intercept = (sum_y - slope * sum_x) / n\n    day_15_base = slope * 15 + intercept\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_prediction = day_15_base + day_15_fluctuation\n    safety_margin = day_15_prediction * 0.05\n    final_prediction = day_15_prediction + safety_margin\n    final_prediction_rounded = math.ceil(final_prediction + 1e-09)\n    return final_prediction_rounded\nconsumption_data = [110, 120, 130, 140, 150, 160, 170, 180, 190, 200]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 93, "completion_id": 1, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Arrays 'y_true' and 'y_pred' must have the same length.\")\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 93, "completion_id": 2, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    return np.mean(np.abs(y_true - y_pred)).round(3)"}
{"task_id": 93, "completion_id": 3, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)"}
{"task_id": 93, "completion_id": 4, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)"}
{"task_id": 93, "completion_id": 5, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    return np.mean(np.abs(y_true - y_pred)).round(3)"}
{"task_id": 93, "completion_id": 6, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 93, "completion_id": 7, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    return round(np.mean(np.abs(y_true - y_pred)), 3)"}
{"task_id": 93, "completion_id": 8, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 93, "completion_id": 9, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must be of the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Query, Key, and Value matrices from the input features.\n    \n    Parameters:\n        X (np.ndarray): Input token embeddings (shape: [n_tokens, d_model])\n        W_q (np.ndarray): Query weights (shape: [d_model, d_model])\n        W_k (np.ndarray): Key weights (shape: [d_model, d_model])\n        W_v (np.ndarray): Value weights (shape: [d_model, d_model])\n    \n    Returns:\n        tuple: (Q, K, V) each of shape [n_tokens, d_model]\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs scaled dot-product self-attention.\n    \n    Parameters:\n        Q (np.ndarray): Query matrix (shape: [n_tokens, d_model])\n        K (np.ndarray): Key matrix (shape: [n_tokens, d_model])\n        V (np.ndarray): Value matrix (shape: [n_tokens, d_model])\n    \n    Returns:\n        np.ndarray: Weighted sum of V (shape: [n_tokens, d_model])\n    \"\"\"\n    d_k = Q.shape[1]\n    attention_scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(attention_scores, axis=1)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Multi-Head Self-Attention.\n    \n    Parameters:\n        X (np.ndarray): Input token embeddings (shape: [n_tokens, d_model])\n        W_q (np.ndarray): Combined Query weights for all heads (shape: [n_heads, d_model, d_model])\n        W_k (np.ndarray): Combined Key weights for all heads (shape: [n_heads, d_model, d_model])\n        W_v (np.ndarray): Combined Value weights for all heads (shape: [n_heads, d_model, d_model])\n        n_heads (int): Number of attention heads\n    \n    Returns:\n        list: Result of multi-head attention as a Python list (after tolist())\n    \"\"\"\n    (n_tokens, d_model) = X.shape\n    head_size = d_model // n_heads\n    outputs = []\n    for i in range(n_heads):\n        W_q_head = W_q[i, :, :]\n        W_k_head = W_k[i, :, :]\n        W_v_head = W_v[i, :, :]\n        (Q, K, V) = compute_qkv(X, W_q_head, W_k_head, W_v_head)\n        head_output = self_attention(Q, K, V)\n        outputs.append(head_output.reshape(n_tokens, head_size))\n    concatenated_output = np.concatenate(outputs, axis=1)\n    rounded_output = np.round(concatenated_output, 4)\n    return rounded_output.tolist()\nX = np.random.rand(3, 128)\nW_q = np.random.rand(8, 128, 128)\nW_k = np.random.rand(8, 128, 128)\nW_v = np.random.rand(8, 128, 128)"}
{"task_id": 94, "completion_id": 1, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, Value matrices.\n    \n    Args:\n        X (np.ndarray): Input array of shape (seq_len, d_model)\n        W_q (np.ndarray): Query weight matrix of shape (d_model, d_k)\n        W_k (np.ndarray): Key weight matrix of shape (d_model, d_k)\n        W_v (np.ndarray): Value weight matrix of shape (d_model, d_v)\n    \n    Returns:\n        Q (np.ndarray): Query matrix\n        K (np.ndarray): Key matrix\n        V (np.ndarray): Value matrix\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute single-head self-attention.\n    \n    Args:\n        Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n        K (np.ndarray): Key matrix of shape (seq_len, d_k)\n        V (np.ndarray): Value matrix of shape (seq_len, d_v)\n    \n    Returns:\n        Attention output of shape (seq_len, d_v)\n    \"\"\"\n    d_k = K.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=1)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention.\n    \n    Args:\n        X (np.ndarray): Input array of shape (seq_len, d_model)\n        W_q (np.ndarray): Query weights for multiple heads of shape (n_heads, d_model, d_head)\n        W_k (np.ndarray): Key weights for multiple heads of shape (n_heads, d_model, d_head)\n        W_v (np.ndarray): Value weights for multiple heads of shape (n_heads, d_model, d_head)\n        n_heads (int): Number of attention heads\n    \n    Returns:\n        Python list: Output of shape (seq_len, n_heads * d_head), after rounding to 4th decimal and converting to list\n    \"\"\"\n    (seq_len, d_model) = X.shape\n    d_head = W_q.shape[2]\n    head_outputs = []\n    for i in range(n_heads):\n        (W_q_head, W_k_head, W_v_head) = (W_q[i], W_k[i], W_v[i])\n        (Q_i, K_i, V_i) = compute_qkv(X, W_q_head, W_k_head, W_v_head)\n        attention_i = self_attention(Q_i, K_i, V_i)\n        head_outputs.append(np.round(attention_i, 4))\n    output = np.hstack(head_outputs)\n    return output.tolist()"}
{"task_id": 94, "completion_id": 2, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    Arguments:\n        X: Input matrix of shape (seq_len, input_dim)\n        W_q, W_k, W_v: Weight matrices for Q, K, V\n    Returns:\n        Q, K, V: Matrices for Query, Key, Value\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute attention output for a single head.\n    Arguments:\n        Q, K, V: Query, Key, and Value matrices of shape (seq_len, d_k or d_v)\n    Returns:\n        Attention output of shape (seq_len, d_v)\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    weights = np.softmax(scores, axis=1)\n    output = np.dot(weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n    Arguments:\n        X: Input matrix of shape (seq_len, input_dim)\n        W_q, W_k, W_v: Weight matrices of shape (input_dim, d_k or d_v)\n        n_heads: Number of attention heads\n    Returns:\n        List of attention outputs from all heads (rounded to 4 decimals).\n    \"\"\"\n    result = []\n    (seq_len, input_dim) = X.shape\n    d_k = W_q.shape[1]\n    d_v = W_v.shape[1]\n    for head in range(n_heads):\n        (W_q_head, W_k_head, W_v_head) = (W_q[head::n_heads], W_k[head::n_heads], W_v[head::n_heads])\n        (Q, K, V) = compute_qkv(X, W_q_head, W_k_head, W_v_head)\n        attention_output = self_attention(Q, K, V)\n        result.append(np.round(attention_output, 4).tolist())\n    return result\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\nW_q = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]], dtype=np.float64)\nW_k = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=np.float64)\nW_v = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=np.float64)\nn_heads = 3\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 94, "completion_id": 3, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Q (Query), K (Key), and V (Value) matrices.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (seq_len, d_model).\n        W_q, W_k, W_v (np.ndarray): Weight matrices for Q, K, and V of shape (d_model, d_model).\n\n    Returns:\n        tuple of Q, K, V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product self-attention.\n\n    Args:\n        Q, K, V (np.ndarray): Matrices for Query, Key, and Value.\n\n    Returns:\n        np.ndarray: Output of the attention mechanism.\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (seq_len, d_model).\n        W_q, W_k, W_v (np.ndarray): Weight matrices for Q, K, and V of shape (n_heads * d_model, d_model).\n        n_heads (int): Number of attention heads.\n\n    Returns:\n        list: Concatenated output from all attention heads.\n    \"\"\"\n    d_model = X.shape[1]\n    head_dim = d_model // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = np.split(Q, n_heads, axis=1)\n    K_heads = np.split(K, n_heads, axis=1)\n    V_heads = np.split(V, n_heads, axis=1)\n    outputs = []\n    for (q, k, v) in zip(Q_heads, K_heads, V_heads):\n        out = self_attention(q, k, v)\n        outputs.append(out)\n    stacked = np.stack(outputs, axis=0)\n    final_output = stacked.reshape(-1, n_heads * head_dim)\n    return np.round(final_output, decimals=4).tolist()"}
{"task_id": 94, "completion_id": 4, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute queries, keys, and values for multi-head attention.\n    Args:\n        X: Input tensor with shape (sequence_length, input_dim)\n        W_q, W_k, W_v: Weight matrices for Q, K, V\n    Returns:\n        Q, K, V: Tensors after linear transformation\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product attention.\n    Args:\n        Q, K, V: Tensor with shape (sequence_length, dim_head)\n    Returns:\n        Output after attention with shape (sequence_length, dim_head)\n    \"\"\"\n    d_k = K.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attn_weights = np.softmax(scores, axis=-1)\n    output = attn_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Multi-head attention mechanism.\n    Args:\n        X: Input tensor (sequence_length, input_dim)\n        W_q, W_k, W_v: Weight matrices for Q, K, V with (input_dim, dim_head)\n        n_heads: Number of attention heads\n    Returns:\n        Combined attention expressions as a Python list (rounded to 4 decimal places)\n    \"\"\"\n    input_dim = X.shape[1]\n    dim_head = W_q.shape[1]\n    W_q_heads = W_q.reshape(input_dim, n_heads, dim_head)\n    W_k_heads = W_k.reshape(input_dim, n_heads, dim_head)\n    W_v_heads = W_v.reshape(input_dim, n_heads, dim_head)\n    outputs = []\n    for i in range(n_heads):\n        W_qi = W_q_heads[:, i, :].T\n        W_ki = W_k_heads[:, i, :].T\n        W_vi = W_v_heads[:, i, :].T\n        (Q, K, V) = compute_qkv(X, W_qi, W_ki, W_vi)\n        head_output = self_attention(Q, K, V)\n        outputs.append(head_output)\n    final_output = np.concatenate(outputs, axis=1)\n    return np.around(final_output, decimals=4).tolist()"}
{"task_id": 94, "completion_id": 5, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, and Value matrices from input X using the respective weight matrices.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (n_samples, input_dim)\n        W_q (np.ndarray): Query weight matrix of shape (input_dim, d_k)\n        W_k (np.ndarray): Key weight matrix of shape (input_dim, d_k)\n        W_v (np.ndarray): Value weight matrix of shape (input_dim, d_v)\n\n    Returns:\n        tuple: (Q, K, V) each of shape (n_samples, d_k) or (n_samples, d_v)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product attention over a single-head attention.\n\n    Args:\n        Q (np.ndarray): Query matrix of shape (n_samples, d_k)\n        K (np.ndarray): Key matrix of shape (n_samples, d_k)\n        V (np.ndarray): Value matrix of shape (n_samples, d_v)\n\n    Returns:\n        np.ndarray: Attention output of shape (n_samples, d_v)\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=1)\n    output = attention_weights @ V\n    return np.round(output, 4)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Apply multi-head attention to input data by combining multiple attention heads.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (n_samples, input_dim)\n        W_q (np.ndarray): Query weights matrix of shape (input_dim, n_heads * d_k)\n        W_k (np.ndarray): Key weights matrix of shape (input_dim, n_heads * d_k)\n        W_v (np.ndarray): Value weights matrix of shape (input_dim, n_heads * d_v)\n        n_heads (int): Number of attention heads\n\n    Returns:\n        list: Multi-head attention output as a Python list\n    \"\"\"\n    (input_dim, d_q) = W_q.shape\n    (_, d_k) = W_k.shape\n    (_, d_v) = W_v.shape\n    W_q_heads = np.split(W_q, n_heads, axis=1)\n    W_k_heads = np.split(W_k, n_heads, axis=1)\n    W_v_heads = np.split(W_v, n_heads, axis=1)\n    outputs = []\n    for i in range(n_heads):\n        (Q_i, K_i, V_i) = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        output_i = self_attention(Q_i, K_i, V_i)\n        outputs.append(output_i)\n    output = np.hstack(outputs)\n    return np.round(output, 4).tolist()"}
{"task_id": 94, "completion_id": 6, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, d_k: int) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product attention.\n    \"\"\"\n    d_k = d_k\n    scores = Q @ K.T / np.sqrt(d_k)\n    weights = np.softmax(scores, axis=-1)\n    attention_output = weights @ V\n    return attention_output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Multi-Head Attention Mechanism.\n    \"\"\"\n    (seq_len, d_model) = X.shape\n    assert d_model % n_heads == 0, 'd_model must be divisible by n_heads'\n    d_head = d_model // n_heads\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n\n    def reshape_heads(mat):\n        return mat.reshape(seq_len, n_heads, d_head).transpose(1, 0, 2)\n    Q_heads = reshape_heads(Q)\n    K_heads = reshape_heads(K)\n    V_heads = reshape_heads(V)\n    heads_outputs = []\n    for (q_head, k_head, v_head) in zip(Q_heads, K_heads, V_heads):\n        output = self_attention(q_head, k_head, v_head, d_head)\n        heads_outputs.append(output)\n    heads_outputs = np.array(heads_outputs)\n    output = heads_outputs.transpose(1, 0, 2).reshape(seq_len, d_model)\n    return np.round(output, 4).tolist()"}
{"task_id": 94, "completion_id": 7, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Performs scaled dot-product self-attention.\"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Performs multi-head attention mechanism using the given weight matrices.\"\"\"\n    d_k = W_q.shape[1] // n_heads\n    Q_slices = np.split(W_q, n_heads, axis=1)\n    K_slices = np.split(W_k, n_heads, axis=1)\n    V_slices = np.split(W_v, n_heads, axis=1)\n    outputs = []\n    for i in range(n_heads):\n        q_i = np.dot(X, Q_slices[i])\n        k_i = np.dot(X, K_slices[i])\n        v_i = np.dot(X, V_slices[i])\n        head_output = self_attention(q_i, k_i, v_i)\n        outputs.append(head_output)\n    final_output = np.vstack(outputs).T\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 94, "completion_id": 8, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, eps: float=1e-08) -> np.ndarray:\n    \"\"\"\n    Perform scaled dot-product attention.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.where(scores == np.inf, 0, np.exp(scores)).T\n    attention_weights /= np.sum(attention_weights, axis=0) + eps\n    output = V @ attention_weights.T\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n\n    Args:\n        X: input array (num_sequences, dim)\n        W_q: weight matrix for queries (dim, d_model)\n        W_k: weight matrix for keys (dim, d_model)\n        W_v: weight matrix for values (dim, d_model)\n        n_heads: number of attention heads\n\n    Returns:\n        A list of the multi-head attention outputs, rounded to 4th decimal.\n    \"\"\"\n    d_model = W_q.shape[1]\n    head_dim = d_model // n_heads\n    if d_model % n_heads != 0:\n        raise ValueError('d_model must be divisible by n_heads')\n    (Q_heads, K_heads, V_heads) = ([], [], [])\n    for i in range(n_heads):\n        start = i * head_dim\n        end = (i + 1) * head_dim\n        W_q_head = W_q[:, start:end]\n        W_k_head = W_k[:, start:end]\n        W_v_head = W_v[:, start:end]\n        Q_heads.append(X @ W_q_head)\n        K_heads.append(X @ W_k_head)\n        V_heads.append(X @ W_v_head)\n    head_outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q_heads[i], K_heads[i], V_heads[i])\n        head_outputs.append(head_output)\n    final_output = np.hstack(head_outputs)\n    return np.round(final_output, 4).tolist()\nX = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nW_q = np.random.randn(4, 8)\nW_k = np.random.randn(4, 8)\nW_v = np.random.randn(4, 8)\nn_heads = 2\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)"}
{"task_id": 94, "completion_id": 9, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    (n_seq, d_in) = X.shape\n    n_heads = W_q.shape[1] // W_q.shape[0]\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_head = d_in\n    Q = Q.reshape(n_seq, n_heads, -1)\n    K = K.reshape(n_seq, n_heads, -1)\n    V = V.reshape(n_seq, n_heads, -1)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray=None) -> np.ndarray:\n    (n_seq, n_heads, d_head) = Q.shape\n    scores = Q @ K.transpose(1, 2, 0) / np.sqrt(d_head)\n    if mask is not None:\n        scores = np.where(mask == 0, -np.inf, scores)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=2, keepdims=True)\n    output = attention_weights @ V\n    output = np.round(output, 4)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    attn_output = self_attention(Q, K, V)\n    (n_seq, _, d_head) = attn_output.shape\n    final_output = attn_output.reshape(n_seq, -1)\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of equal length.')\n    n00 = n01 = n10 = n11 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 0 and yi == 0:\n            n00 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 1 and yi == 1:\n            n11 += 1\n    row1 = n00 + n01\n    row2 = n10 + n11\n    col1 = n00 + n10\n    col2 = n01 + n11\n    total = row1 + row2\n    numerator = n11 * n00 - n10 * n01\n    denominator = (row1 * row2 * col1 * col2) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [0, 1, 0, 1, 1, 0, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0, 0, 1, 1, 1, 0]"}
{"task_id": 95, "completion_id": 1, "solution": "from typing import List\nimport math\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of the same length.')\n    n11 = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 1))\n    n10 = sum((1 for i in range(len(x)) if x[i] == 1 and y[i] == 0))\n    n01 = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 1))\n    n00 = sum((1 for i in range(len(x)) if x[i] == 0 and y[i] == 0))\n    n = len(x)\n    numerator = n * (n11 * n00 - n10 * n01)\n    denominator = math.sqrt((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01))\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0, 1]\ny = [1, 1, 1, 0, 0, 1, 0]"}
{"task_id": 95, "completion_id": 2, "solution": "from typing import List\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    assert len(x) == len(y), 'Input lists must be of the same length.'\n    n11 = n10 = n01 = n00 = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            n11 += 1\n        elif xi == 1 and yi == 0:\n            n10 += 1\n        elif xi == 0 and yi == 1:\n            n01 += 1\n        else:\n            n00 += 1\n    n = n11 + n10 + n01 + n00\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0, 1]\ny = [0, 0, 1, 1, 0, 1, 1]"}
{"task_id": 95, "completion_id": 3, "solution": "from math import sqrt\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists x and y must be of the same length.')\n    a = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    b = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    c = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    d = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    numerator = a * d - b * c\n    denominator = sqrt((a + b) * (a + c) * (b + d) * (c + d))\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 1, 0, 1, 0, 0, 1]\ny = [1, 0, 0, 1, 1, 0, 1]"}
{"task_id": 95, "completion_id": 4, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Lists x and y must be of the same length.')\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    nx1 = n11 + n10\n    n1x = n11 + n01\n    nx0 = n10 + n00\n    n0x = n01 + n00\n    n = nx1 + n0x\n    numerator = n11 * n00 - n10 * n01\n    denominator = (nx1 * nx0 * n1x * n0x) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [1, 1, 0, 1, 0, 0]"}
{"task_id": 95, "completion_id": 5, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n11 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 1))\n    n10 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 0))\n    n01 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 1))\n    n00 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 0))\n    row1 = n11 + n10\n    row0 = n01 + n00\n    col1 = n11 + n01\n    col0 = n10 + n00\n    total = row1 + row0\n    chi2 = total * (n11 * n00 - n10 * n01) ** 2 / (row1 * row0 * col1 * col0)\n    phi = (n11 * n00 - n10 * n01) / (row1 * row0 * col1 * col0) ** 0.5 if all([row1, row0, col1, col0]) else 0.0\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0]\ny = [1, 1, 0, 1, 0, 1]"}
{"task_id": 95, "completion_id": 6, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length.')\n    n11 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 1))\n    n00 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 0))\n    n10 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 0))\n    n01 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 1))\n    total = len(x)\n    sum1 = n11 + n10\n    sum2 = n11 + n01\n    sum3 = n00 + n10\n    sum4 = n00 + n01\n    numerator = n11 * n00 - n10 * n01\n    denominator = (sum1 * sum2 * sum3 * sum4) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)"}
{"task_id": 95, "completion_id": 7, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    from collections import Counter\n    counts = Counter(zip(x, y))\n    a = counts[1, 1]\n    b = counts[1, 0]\n    c = counts[0, 1]\n    d = counts[0, 0]\n    if len(x) != len(y):\n        raise ValueError('Input lists must be of the same length.')\n    a = a or 0\n    b = b or 0\n    c = c or 0\n    d = d or 0\n    n = a + b + c + d\n    phi = (a * d - b * c) / ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [1, 1, 1, 0, 0, 1]"}
{"task_id": 95, "completion_id": 8, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists x and y must be of the same length.')\n    n11 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 1))\n    n10 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 0))\n    n01 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 1))\n    n00 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 0))\n    n = len(x)\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 1, 0, 0, 1, 1, 0, 0]\ny = [1, 0, 1, 0, 1, 0, 1, 0]"}
{"task_id": 95, "completion_id": 9, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length')\n    (tp, tn, fp, fn) = (0, 0, 0, 0)\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            tp += 1\n        elif xi == 0 and yi == 0:\n            tn += 1\n        elif xi == 1 and yi == 0:\n            fp += 1\n        elif xi == 0 and yi == 1:\n            fn += 1\n    numerator = tp * tn - fp * fn\n    denominator = ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1]\ny = [0, 0, 1, 1, 0, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    The Hard Sigmoid is a piecewise linear approximation of the standard sigmoid function.\n    It is defined as follows:\n    - 0 if x <= -2.5\n    - 1 if x >= 2.5\n    - 0.2 * x + 0.5 if -2.5 < x < 2.5\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 1, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif -2.5 < x < 2.5:\n        return x / 5.0 + 0.5\n    else:\n        return 1.0"}
{"task_id": 96, "completion_id": 2, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return x / 6 + 0.5"}
{"task_id": 96, "completion_id": 3, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))"}
{"task_id": 96, "completion_id": 4, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 5, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 6, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 7, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 96, "completion_id": 8, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    The Hard Sigmoid is a piecewise linear approximation to the standard sigmoid function.\n    It is defined as follows:\n    - 0 if x < -2.5\n    - 1 if x > 2.5\n    - x / 6 + 0.5 if x is between -2.5 and 2.5\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return x / 6.0 + 0.5"}
{"task_id": 96, "completion_id": 9, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1.0), 4)"}
{"task_id": 97, "completion_id": 1, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 2, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 3, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 4, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 5, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    return x if x > 0 else alpha * (math.exp(x) - 1)"}
{"task_id": 97, "completion_id": 6, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 7, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 97, "completion_id": 8, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1.0), 4)"}
{"task_id": 97, "completion_id": 9, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 1, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 2, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 3, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 98, "completion_id": 4, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 5, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 6, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 7, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 98, "completion_id": 8, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(x, 0) if x >= 0 else alpha * x"}
{"task_id": 98, "completion_id": 9, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x < 0:\n        return alpha * x\n    else:\n        return x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -30:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 1, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 18:\n        return round(x, 4)\n    elif x < -18:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1.0 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 2, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return 0.0\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 3, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x < -50:\n        return round(x, 4)\n    if x > 50:\n        return round(x, 4)\n    return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 4, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    return round(math.log1p(math.exp(-abs(x))) + max(x, 0), 4)"}
{"task_id": 99, "completion_id": 5, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x >= 20:\n        return round(x, 4)\n    elif x <= -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 6, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place.\n    \"\"\"\n    if x < -30:\n        return round(math.exp(x), 4)\n    elif x > 30:\n        return round(x, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 7, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 30:\n        return round(x, 4)\n    elif x < -30:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 99, "completion_id": 8, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places.\n    \"\"\"\n    if x < -30:\n        result = 0.0\n    else:\n        result = math.log(1.0 + math.exp(x))\n    return round(result, 4)"}
{"task_id": 99, "completion_id": 9, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -30:\n        return 0.0\n    elif x > 30:\n        return x\n    else:\n        return round(math.log(1.0 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 1, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 2, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 3, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 4, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 5, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 100, "completion_id": 6, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 7, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 8, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 100, "completion_id": 9, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    assert len(rhos) == len(A) == len(pi_theta_old) == len(pi_theta_ref)\n    N = len(rhos)\n    obj = 0.0\n    for i in range(N):\n        rho_i_ref = pi_theta_ref[i] / pi_theta_old[i]\n        rho_i_ref_clipped = np.clip(rho_i_ref, a_min=1 - epsilon, a_max=1 + epsilon)\n        unclipped_term = rhos[i] * A[i]\n        clipped_term = np.clip(rhos[i], a_min=(1 - epsilon) * rho_i_ref, a_max=(1 + epsilon) * rho_i_ref) * A[i]\n        obj += np.minimum(unclipped_term, clipped_term)\n        kl = pi_theta_old[i] * np.log(pi_theta_old[i] / pi_theta_ref[i])\n        obj -= beta * kl\n    return round(obj, 6)"}
{"task_id": 101, "completion_id": 1, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    surrogate = [rho * A[i] for (i, rho) in enumerate(rhos)]\n    clipped_surrogate = []\n    for (i, rho) in enumerate(rhos):\n        r_ref = pi_theta_ref[i] / pi_theta_old[i]\n        rho_clipped = np.clip(rho, 1 - epsilon, 1 + epsilon)\n        clipped_surrogate.append(min(rho * A[i], rho_clipped * A[i]))\n    r_ref = np.array(pi_theta_ref) / np.array(pi_theta_old)\n    kl_penalty = beta * np.sum((rhos - r_ref) * np.log(rhos / r_ref))\n    objective = np.sum(clipped_surrogate) - kl_penalty\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 2, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (rho_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    if len(rhos) != len(A) or len(rhos) != len(pi_theta_old) or len(rhos) != len(pi_theta_ref):\n        raise ValueError('All input arrays must have the same length')\n    clipped_ratios = np.clip(pi_theta_old / pi_theta_ref, (1 - epsilon) * rhos, (1 + epsilon) * rhos)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    unclipped = np.sum(rhos * A)\n    clipped = np.sum(clipped_ratios * A)\n    objective = np.minimum(clipped, unclipped) - beta * kl_divergence\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 3, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    if not len(rhos) == len(A) == len(pi_theta_old) == len(pi_theta_ref):\n        raise ValueError('All input arrays must have the same length.')\n    rho_clipped = [min(rho, 1 + epsilon) for rho in rhos]\n    unclipped_sum = sum((rho * a for (rho, a) in zip(rhos, A)))\n    clipped_sum = sum((rho_clipped[i] * A[i] for i in range(len(rho_clipped))))\n    policy_objective = 0.5 * unclipped_sum + 0.5 * clipped_sum\n    kl_divergence = sum((pi * np.log(pi / ref) for (pi, ref) in zip(pi_theta_old, pi_theta_ref)))\n    objective = policy_objective - beta * kl_divergence\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 4, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clip_factor = (1 - epsilon) * (pi_theta_ref / pi_theta_old)\n    rho_clipped = np.clip(rhos, a_min=clip_factor, a_max=1.0)\n    policy_gradient_unclipped = rhos * A\n    policy_gradient_clipped = rho_clipped * A\n    policy_gradient = np.min([policy_gradient_unclipped, policy_gradient_clipped], axis=0)\n    kl_divergence = pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref))\n    kl_penalty = beta * np.sum(kl_divergence)\n    objective = np.sum(policy_gradient) - kl_penalty\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 5, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    L_unclipped = rhos * A\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    L_clipped = clipped_rhos * A\n    L_surr = np.minimum(L_unclipped, L_clipped)\n    KLD = np.sum(pi_theta_old * (np.log(pi_theta_old + 1e-10) - np.log(pi_theta_ref + 1e-10)))\n    objective = np.mean(L_surr - beta * KLD)\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 6, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value, rounded to the nearest 6th decimal.\n    \"\"\"\n    rho_refs = np.array(pi_theta_old) / np.array(pi_theta_ref)\n    clipped_rhos = np.clip(np.array(rhos) / rho_refs, 1 - epsilon, 1 + epsilon)\n    unclipped_term = np.array(rhos) * np.array(A)\n    clipped_term = clipped_rhos * np.array(A)\n    policy_gradient = np.minimum(unclipped_term, clipped_term).sum()\n    kl_term = np.array(pi_theta_old) / np.array(pi_theta_ref) * np.log(np.array(pi_theta_old) / np.array(pi_theta_ref))\n    objective = policy_gradient - beta * kl_term.sum()\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 7, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    num_samples = len(rhos)\n    objective = 0.0\n    for i in range(num_samples):\n        rho_i = rhos[i] * pi_theta_old[i] / pi_theta_ref[i]\n        unclipped_term = rho_i * A[i]\n        clamp_rho = np.clip(rho_i, 1 - epsilon, 1 + epsilon)\n        clipped_term = clamp_rho * A[i]\n        kl_old_ref = np.sum(pi_theta_old[i] * np.log(pi_theta_old[i] / pi_theta_ref[i]))\n        kl_theta_old = np.sum(rhos[i] * pi_theta_old[i] * np.log(pi_theta_old[i] / pi_theta_ref[i]))\n        kl_total = kl_old_ref + kl_theta_old\n        objective += clipped_term - beta * kl_total\n    return round(objective, 6)"}
{"task_id": 101, "completion_id": 8, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (rho_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rho_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    adv_term = rhos * A\n    clipped_adv_term = rho_clipped * A\n    surrogate = np.minimum(adv_term, clipped_adv_term)\n    kl_divergence = np.log(pi_theta_ref / pi_theta_old)\n    penalty = beta * kl_divergence\n    total_objective = np.sum(surrogate - penalty)\n    return round(total_objective, 6)"}
{"task_id": 101, "completion_id": 9, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    assert len(rhos) == len(A) == len(pi_theta_old) == len(pi_theta_ref), 'All input lists must be of equal length.'\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    kl_div = np.sum(np.log(pi_theta_old / pi_theta_ref) * pi_theta_old)\n    unclipped_surrogate = np.sum(rhos * A)\n    clipped_surrogate = np.sum(rhos_clipped * A)\n    policy_objective = np.minimum(unclipped_surrogate, clipped_surrogate)\n    objective = policy_objective - beta * kl_div\n    return round(float(objective), 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x / (1 + math.exp(-x))"}
{"task_id": 102, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x / (1 + math.exp(-x))"}
{"task_id": 102, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * np.reciprocal(1 + np.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Swish(x) = x * \u03c3(x), where \u03c3(x) is the sigmoid function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places.\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 102, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 102, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sig = 1.0 / (1.0 + math.exp(-x))\n    result = x * sig\n    return round(result, 4)"}
{"task_id": 102, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sig = 1 / (1 + math.exp(-x))\n    return round(x * sig, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (np.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 1, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 2, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 103, "completion_id": 3, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(scale * result, 4)"}
{"task_id": 103, "completion_id": 4, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (np.exp(x) - 1)\n    return round(scale * result, 4)"}
{"task_id": 103, "completion_id": 5, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (np.exp(x) - 1)\n    return round(scale * result, 4)"}
{"task_id": 103, "completion_id": 6, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)"}
{"task_id": 103, "completion_id": 7, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(scale * result, 4)"}
{"task_id": 103, "completion_id": 8, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(scale * result, 4)"}
{"task_id": 103, "completion_id": 9, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value (rounded to 4 decimal places)\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (np.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nweights = np.array([0.5, -0.5])\nbias = 0.0\npredictions = predict_logistic(X, weights, bias)"}
{"task_id": 104, "completion_id": 1, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 2, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    scores = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-scores))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.reshape(-1).tolist()"}
{"task_id": 104, "completion_id": 3, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()\nX = np.array([[2.0, 3.0], [1.0, 1.0], [3.0, 5.0]])\nweights = np.array([0.5, 0.75])\nbias = -1.0"}
{"task_id": 104, "completion_id": 4, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = X @ weights + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 5, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.reshape(-1).tolist()"}
{"task_id": 104, "completion_id": 6, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_output = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_output))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 7, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    scores = np.dot(X, weights) + bias\n    probabilities = 1.0 / (1 + np.exp(-scores))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 104, "completion_id": 8, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.reshape(-1).tolist()"}
{"task_id": 104, "completion_id": 9, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.flatten().tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    from sklearn.preprocessing import LabelBinarizer\n    (n_samples, n_features) = X.shape\n    label_binarizer = LabelBinarizer()\n    y_one_hot = label_binarizer.fit_transform(y)\n    n_classes = y_one_hot.shape[1]\n    np.random.seed(0)\n    B = np.random.randn(n_features, n_classes)\n    losses = []\n    for _ in range(iterations):\n        z = X @ B\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        softmax = exp_z / exp_z.sum(axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_one_hot * np.log(softmax + 1e-15), axis=1))\n        losses.append(round(loss, 4))\n        grad_B = X.T @ (softmax - y_one_hot) / n_samples\n        B -= learning_rate * grad_B\n    B = B.round(4).tolist()\n    losses = [round(loss, 4) for loss in losses] if losses else []\n    return (B, losses)"}
{"task_id": 105, "completion_id": 1, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Parameters:\n        X : np.ndarray, shape (N, D), input data matrix (N samples, D features)\n        y : np.ndarray, shape (N,), ground truth labels (0-based integers)\n        learning_rate : float, learning rate for gradient descent\n        iterations : int, number of iterations to run the optimization\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of Cross Entropy loss rounded to 4 floating points\n    \"\"\"\n    (N, D) = X.shape\n    C = len(np.unique(y))\n    W = np.random.randn(C, D) * 0.01\n    b = np.zeros((C, 1))\n    if 1 not in X:\n        X_biased = np.hstack((X, np.ones((N, 1))))\n    else:\n        X_biased = X\n    Y_onehot = np.eye(C)[y].T\n    losses = []\n    for _ in range(iterations):\n        Z = np.dot(W, X_biased) + b\n        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n        exp_sum = np.sum(exp_Z, axis=0, keepdims=True)\n        A = exp_Z / exp_sum\n        epsilon = 1e-15\n        A = np.clip(A, epsilon, 1 - epsilon)\n        loss = -np.sum(Y_onehot * np.log(A)) / N\n        losses.append(round(loss, 4))\n        dZ = A - Y_onehot\n        dW = np.dot(dZ, X_biased.T) / N\n        db = np.sum(dZ, axis=1, keepdims=True) / N\n        W -= learning_rate * dW\n        b -= learning_rate * db\n    B = np.concatenate((W, b), axis=1).round(4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)\nX = np.array([[1, 2], [2, 3], [1, 1], [3, 2]])\ny = np.array([0, 1, 0, 1])"}
{"task_id": 105, "completion_id": 2, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    K = np.max(y) + 1\n    y_encoded = np.zeros((n_samples, K))\n    y_encoded[np.arange(n_samples), y] = 1\n    np.random.seed(42)\n    B = np.random.randn(n_features, K) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores)\n        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n        cross_entropy_loss = -np.mean(np.log(probs[np.arange(n_samples), y]))\n        losses.append(round(cross_entropy_loss, 4))\n        grad_B = X.T @ (probs - y_encoded) / n_samples\n        B -= learning_rate * grad_B\n    B = np.round(B, 4).flatten().tolist()\n    losses = np.round(np.array(losses), 4).tolist()\n    return (B, losses)"}
{"task_id": 105, "completion_id": 3, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = len(np.unique(y))\n    Y_onehot = np.zeros((N, C))\n    Y_onehot[np.arange(N), y.astype(int)] = 1\n    B = np.random.randn(M, C) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        exp_sums = np.sum(exp_scores, axis=1, keepdims=True)\n        probs = exp_scores / exp_sums\n        loss = -np.mean(np.log(probs[np.arange(N), y.astype(int)]))\n        losses.append(np.round(loss, 4))\n        grad = X.T @ (probs - Y_onehot) / N\n        B -= learning_rate * grad\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [np.round(loss, 4) for loss in losses]\n    return (B_rounded, losses_rounded)"}
{"task_id": 105, "completion_id": 4, "solution": "import numpy as np\nimport numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = np.unique(y).shape[0]\n    y_onehot = np.zeros((N, C))\n    y_onehot[np.arange(N), y.astype(int)] = 1\n    B = np.random.randn(M, C)\n    b = np.zeros((1, C))\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B + b\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n        NLL = -np.mean(np.log(probabilities[np.arange(N), y.astype(int)]))\n        losses.append(np.round(NLL, 4))\n        grad = probabilities - y_onehot\n        dB = X.T @ grad / N\n        db = np.mean(grad, axis=0)\n        B -= learning_rate * dB\n        b -= learning_rate * db\n    B_list = np.round(B, 4).tolist()\n    losses = np.round(losses, 4).tolist()\n    return (B_list, losses)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\nlearning_rate = 0.1\niterations = 100"}
{"task_id": 105, "completion_id": 5, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (C, M) = X.shape\n    K = len(np.unique(y))\n\n    def to_one_hot(y, K):\n        return np.eye(K)[y]\n    Y_true = to_one_hot(y, K)\n    W = np.random.randn(K, M) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = W @ X.T\n        exp_scores = np.exp(scores)\n        exp_sums = exp_scores.sum(axis=0)\n        P = exp_scores / exp_sums\n        grad = P - Y_true.T\n        dW = grad @ X\n        dW = dW / C\n        W -= learning_rate * dW\n        log_likelihood = np.sum(Y_true * np.log(P.T + 1e-15))\n        loss = -log_likelihood / C\n        losses.append(round(loss, 4))\n    B = np.round(W, 4).flatten().tolist()\n    losses = np.round(losses, 4).tolist()\n    return (B, losses)"}
{"task_id": 105, "completion_id": 6, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X : np.ndarray, input feature matrix of shape (N, M)\n        y : np.ndarray, one-hot encoded labels of shape (N, C)\n        learning_rate : float, learning rate for the gradient descent\n        iterations : int, number of iterations for optimization\n\n    Returns:\n        B : list[float], C*M flattened updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = y.shape[1]\n    B = np.random.randn(M, C)\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probabilities = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n        correct_logprobs = -np.log(probabilities[np.arange(N), np.argmax(y, axis=1)] + 1e-12)\n        loss = np.mean(correct_logprobs)\n        losses.append(round(loss, 4))\n        dscores = probabilities\n        dscores[np.arange(N), np.argmax(y, axis=1)] -= 1\n        dscores /= N\n        dB = X.T @ dscores\n        B -= learning_rate * dB\n    B_flat = B.flatten().round(4)\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_flat.tolist(), losses_rounded)"}
{"task_id": 105, "completion_id": 7, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    y_onehot = np.eye(n_classes)[y.astype(int)]\n    B = np.random.randn(n_features, n_classes) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        cross_entropy_loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-12), axis=1))\n        losses.append(round(cross_entropy_loss, 4))\n        grad = X.T @ (probs - y_onehot) / n_samples\n        B -= learning_rate * grad\n    B = np.round(B, 4).flatten().tolist()\n    losses = np.round(losses, 4).tolist()\n    return (B, losses)"}
{"task_id": 105, "completion_id": 8, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    m = X.shape[0]\n    n_features = X.shape[1]\n    n_classes = len(np.unique(y))\n    Y = np.eye(n_classes)[y]\n    X_b = np.c_[np.ones((m, 1)), X]\n    B = np.zeros((n_classes, n_features + 1))\n    losses = []\n    for _ in range(iterations):\n        Z = X_b @ B.T\n        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n        A = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n        log_likelihoods = Y * np.log(A + 1e-15)\n        loss = -np.sum(log_likelihoods) / m\n        losses.append(round(loss, 4))\n        gradient = X_b.T @ (A - Y) / m\n        B -= learning_rate * gradient\n    B_rounded = np.round(B, 4)\n    losses = [round(loss, 4) for loss in losses]\n    return (B_rounded.tolist(), losses)"}
{"task_id": 105, "completion_id": 9, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Parameters:\n        X : np.ndarray, input data matrix of shape (N, M)\n        y : np.ndarray, one-hot encoded label matrix of shape (N, C)\n        learning_rate : float, learning rate for gradient descent\n        iterations : int, number of iterations to perform\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = y.shape[1]\n    np.random.seed(0)\n    B = np.random.randn(C, M) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B.T\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n        log_probs = np.log(probs)\n        correct_logprobs = y * log_probs\n        loss = -np.sum(correct_logprobs) / N\n        losses.append(round(loss, 4))\n        dscores = probs - y\n        dB = dscores.T @ X\n        dB /= N\n        dB += 0.001 * B\n        B -= learning_rate * dB\n    B = np.round(B, 4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    weights = np.zeros(X.shape[1])\n    losses = []\n\n    def sigmoid(z: np.ndarray) -> np.ndarray:\n        return 1 / (1 + np.exp(-z))\n\n    def compute_loss(y: np.ndarray, y_pred: np.ndarray) -> float:\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return (-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)).mean()\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        loss = compute_loss(y, y_pred)\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, y_pred - y) / y.shape[0]\n        weights -= learning_rate * gradient\n    weights_rounded = [round(w, 4) for w in weights]\n    losses_rounded = [round(l, 4) for l in losses]\n    return [weights_rounded, losses_rounded]"}
{"task_id": 106, "completion_id": 1, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n        X (np.ndarray): Input feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target vector of shape (n_samples,)\n        learning_rate (float): Learning rate for gradient descent\n        iterations (int): Number of iterations for training\n\n    Returns:\n        tuple[list[float], list[float]]: Optimized coefficients and loss values over iterations\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    X = np.hstack((np.ones((n_samples, 1)), X))\n    coefficients = np.zeros(n_features + 1)\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, coefficients)\n        y_hat = 1 / (1 + np.exp(-z))\n        epsilon = 1e-15\n        y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n        loss = -1 / n_samples * np.sum(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n        losses.append(round(loss, 4))\n        gradient = 1 / n_samples * np.dot(X.T, y_hat - y)\n        coefficients -= learning_rate * gradient\n    coefficients_rounded = [round(coef, 4) for coef in coefficients]\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (coefficients_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 2, "solution": "import numpy as np\nimport numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n        X (np.ndarray): Feature matrix (shape: [n_samples, n_features]).\n        y (np.ndarray): Target vector (shape: [n_samples, ]).\n        learning_rate (float): Learning rate for gradient descent.\n        iterations (int): Number of training iterations.\n        \n    Returns:\n        tuple[list[float], list[float]]: Optimized coefficients and collected loss values per iteration.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    losses = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def compute_loss(y_true, y_pred):\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        loss = compute_loss(y, y_pred)\n        losses.append(round(loss, 4))\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights, 4).tolist()\n    bias = round(bias, 4)\n    losses = [round(loss, 4) for loss in losses]\n    return [weights + [bias], losses]\nX = np.array([[0.5, 1.5], [1.5, 2.0], [2.5, 1.0], [3.0, 0.5], [1.0, 0.5]])\ny = np.array([0, 0, 1, 1, 0])"}
{"task_id": 106, "completion_id": 3, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    - X: Design matrix (shape: [n_samples, n_features])\n    - y: Target vector (shape: [n_samples])\n    - learning_rate: Learning rate for the gradient descent optimizer\n    - iterations: Number of iterations to perform gradient descent\n    \n    Returns:\n    - params: Python list of optimized model coefficients [w0, w1, ..., wn]\n    - losses: Python list of collected loss values over iterations (rounded to 4th decimal)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    weights = np.insert(weights, 0, 0.0)\n    X_b = np.c_[np.ones(n_samples), X]\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X_b, weights)\n        y_pred = 1 / (1 + np.exp(-z))\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        grad = np.dot(X_b.T, y_pred - y) / n_samples\n        weights -= learning_rate * grad\n    params = [round(w, 4) for w in weights]\n    losses = [round(l, 4) for l in losses]\n    return (params, losses.tolist())"}
{"task_id": 106, "completion_id": 4, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n        X (np.ndarray): Input features with shape (n_samples, n_features).\n        y (np.ndarray): Target labels with shape (n_samples,).\n        learning_rate (float): Learning rate for gradient descent.\n        iterations (int): Number of iterations to train the model.\n    \n    Returns:\n        tuple(list[float], list[float]): Optimized coefficients and loss values over iterations.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_history.append(round(loss, 4))\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights, 4).tolist()\n    bias = np.round(bias, 4)\n    loss_history = np.round(np.array(loss_history), 4).tolist()\n    return [weights + [bias], loss_history]"}
{"task_id": 106, "completion_id": 5, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0.0\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_history.append(np.round(loss, 4))\n        dw = 1 / m * np.dot(X.T, y_pred - y)\n        db = 1 / m * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights, 4).tolist()\n    bias = round(bias, 4)\n    loss_history = [round(loss, 4) for loss in loss_history]\n    return [weights, bias, loss_history]\nX = np.array([[1, 2], [4, 2], [3, 6], [4, 8]])\ny = np.array([0, 0, 1, 1])\nlearning_rate = 0.01\niterations = 100"}
{"task_id": 106, "completion_id": 6, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    X (np.ndarray): Input feature matrix (m x n).\n    y (np.ndarray): Target variable (m x 1).\n    learning_rate (float): Learning rate for gradient descent.\n    iterations (int): Number of iterations for gradient descent.\n    \n    Returns:\n    tuple: A tuple containing the optimized coefficients and collected loss values over iterations.\n    \"\"\"\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    losses = []\n    for _ in range(iterations):\n        linear_pred = X @ w + b\n        y_pred = 1 / (1 + np.exp(-linear_pred))\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -(1 / m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        dw = 1 / m * (X.T @ (y_pred - y))\n        db = 1 / m * np.sum(y_pred - y)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n    return ([round(wi, 4) for wi in w.tolist()], [round(b, 4)], losses)"}
{"task_id": 106, "completion_id": 7, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    y (np.ndarray): Target vector of shape (n_samples,)\n    learning_rate (float): Step size for gradient descent\n    iterations (int): Number of iterations for training\n    \n    Returns:\n    tuple: (coefficients, losses) - Model coefficients and collected losses rounded to 4 decimals.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    losses = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def binary_cross_entropy(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred + 1e-12) + (1 - y_true) * np.log(1 - y_pred + 1e-12))\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        loss = binary_cross_entropy(y, y_pred)\n        losses.append(round(loss, 4))\n        dw = np.dot(X.T, y_pred - y) / n_samples\n        db = np.mean(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    weights = np.round(weights, 4).tolist()\n    bias = round(bias, 4)\n    losses = [round(loss, 4) for loss in losses]\n    return (weights + [bias], losses)"}
{"task_id": 106, "completion_id": 8, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Args:\n        X (np.ndarray): Input features with shape (n_samples, n_features)\n        y (np.ndarray): Target values with shape (n_samples,)\n        learning_rate (float): Learning rate for gradient descent\n        iterations (int): Number of iterations to run the optimizer\n    \n    Returns:\n        tuple[list[float], list[float]]: Optimized coefficients and collected loss values over iterations\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    theta = np.zeros(n_features)\n    loss_history = []\n    for _ in range(iterations):\n        linear_model = X @ theta\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        loss = -(1 / n_samples) * np.sum(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n        loss_history.append(round(loss, 4))\n        gradient = 1 / n_samples * X.T @ (y_pred - y)\n        theta -= learning_rate * gradient\n    return ([round(t, 4) for t in theta.tolist()], [round(l, 4) for l in loss_history])"}
{"task_id": 106, "completion_id": 9, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    - X: Feature matrix (shape: [n_samples, n_features])\n    - y: Target vector (shape: [n_samples,])\n    - learning_rate: Step size for gradient descent\n    - iterations: Number of iterations to train\n\n    Returns:\n    - coefficients: List of optimized model coefficients\n    - losses: List of loss values after each iteration (rounded to 4 decimals)\n    \"\"\"\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    (n_samples, n_features) = X.shape\n    coefficients = np.zeros(n_features)\n    losses = []\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    for _ in range(iterations):\n        z = np.dot(X, coefficients)\n        y_pred = sigmoid(z)\n        loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n        losses.append(round(loss, 4))\n        gradient = np.dot(X.T, y_pred - y) / n_samples\n        coefficients -= learning_rate * gradient\n    coefficients = np.round(coefficients, 4).tolist()\n    losses = np.round(np.array(losses), 4).tolist()\n    return (coefficients, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Args:\n        Q (np.ndarray): Query matrix of shape (n, d)\n        K (np.ndarray): Key matrix of shape (n, d)\n        V (np.ndarray): Value matrix of shape (n, d)\n        mask (np.ndarray): Attention mask of shape (n, n), where 0s indicate positions to mask\n\n    Returns:\n        np.ndarray: Output of masked attention of shape (n, d)\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -np.inf, scores)\n    exp_scores = np.exp(scores)\n    sum_exp = np.sum(exp_scores, axis=1, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 107, "completion_id": 1, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n\n    Parameters:\n    - Q, K, V: Query, Key, Value matrices of shape (seq_len, d_k)\n    - mask: Attention mask of shape (seq_len, seq_len)\n\n    Returns:\n    Output of attention layer of shape (seq_len, d_k) as a Python list\n    \"\"\"\n    d_k = Q.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = scores + (1 - mask) * -1000000000.0\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 2, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    - Q: Query matrix of shape (n, d)\n    - K: Key matrix of shape (n, d)\n    - V: Value matrix of shape (n, d)\n    - mask: Attention mask of shape (n, n), with 0s for positions to be masked\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T)\n    scores = np.where(mask == 0, -np.inf, scores)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    attention_output = np.dot(attention_weights, V)\n    return attention_output"}
{"task_id": 107, "completion_id": 3, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n    - Q: Query matrix (seq_len x d_k)\n    - K: Key matrix (seq_len x d_k)\n    - V: Value matrix (seq_len x d_v)\n    - mask: Binary mask matrix (seq_len x seq_len), where 0 = mask, 1 = keep\n    \n    Returns:\n    - A list representing the output of the masked self-attention (after tolist())\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 1, scores, -np.inf)\n    attention_weights = np.softmax(scores, axis=1)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 4, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Args:\n    - Q: Query matrix of shape (seq_len, d_k)\n    - K: Key matrix of shape (seq_len, d_k)\n    - V: Value matrix of shape (seq_len, d_v)\n    - mask: Attention mask of shape (seq_len, seq_len), with 0 indicating positions to ignore\n\n    Returns:\n    - Output of masked attention as a Python list after reshaping.\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -np.inf, scores)\n    exp_scores = np.exp(scores)\n    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 5, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q (numpy.ndarray): Query matrix of shape (seq_len, d_k)\n        K (numpy.ndarray): Key matrix of shape (seq_len, d_k)\n        V (numpy.ndarray): Value matrix of shape (seq_len, d_v)\n        mask (numpy.ndarray): Mask matrix of shape (seq_len, seq_len) with 0 or 1 values\n\n    Returns:\n        Output of masked self-attention (numpy.ndarray).\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    attention_weights = np_softmax(scores)\n    output = np.dot(attention_weights, V)\n    return output\ndef np_softmax(x):\n    \"\"\"\n    Softmax function for 2D arrays.\n    \"\"\"\n    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return exps / np.sum(exps, axis=1, keepdims=True)"}
{"task_id": 107, "completion_id": 6, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q, K, V (np.ndarray): Query, Key, Value matrices of shape (seq_len, head_dim)\n        mask (np.ndarray): Attention mask of shape (seq_len, seq_len), where 0 = allow attention, -inf = mask out\n        \n    Returns:\n        Output of attention (np.ndarray) as Python list (after reshaping and tolist).\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 7, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Args:\n        Q (np.ndarray): Query matrix with shape (batch_size, seq_len, d_k)\n        K (np.ndarray): Key matrix with shape (batch_size, seq_len, d_k)\n        V (np.ndarray): Value matrix with shape (batch_size, seq_len, d_v)\n        mask (np.ndarray): Attention mask with shape (batch_size, seq_len, seq_len)\n\n    Returns:\n        np.ndarray: Output of the masked attention, with shape (batch_size, seq_len, d_v)\n    \"\"\"\n    d_k = K.shape[2]\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    scores = np.where(mask, scores, -np.inf)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output.tolist()"}
{"task_id": 107, "completion_id": 8, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -np.inf, scores)\n    attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 107, "completion_id": 9, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    Q: Query matrix of shape (seq_len, dim)\n    K: Key matrix of shape (seq_len, dim)\n    V: Value matrix of shape (seq_len, dim)\n    mask: Attention mask with the same shape as attention scores (seq_len, seq_len)\n    Returns:\n        Output matrix of shape (seq_len, dim)\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure increases as the variety of colors increases.\n    Returns a float rounded to 4 decimal places.\n    \"\"\"\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    counts = Counter(apples)\n    entropy = 0.0\n    for count in counts.values():\n        prob = count / total\n        entropy -= prob * math.log2(prob)\n    return round(entropy / math.log2(total), 4)"}
{"task_id": 108, "completion_id": 1, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses entropy as the measure of disorder.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        probability = count / total\n        entropy -= probability * math.log(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 2, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is computed using a variant of Gini Impurity, which increases \n    with color diversity.\n    \"\"\"\n    from collections import Counter\n    n = len(apples)\n    if n == 0:\n        return 0.0\n    color_counts = Counter(apples)\n    total_pairs = n * (n - 1) / 2\n    same_color_pairs = 0\n    for count in color_counts.values():\n        same_color_pairs += count * (count - 1) // 2\n    different_color_pairs = total_pairs - same_color_pairs\n    disorder_value = different_color_pairs / total_pairs if total_pairs > 0 else 0.0\n    return round(disorder_value, 4)"}
{"task_id": 108, "completion_id": 3, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is computed using Gini impurity.\n    \"\"\"\n    if not apples:\n        return 0.0\n    count = len(apples)\n    color_frequencies = Counter(apples)\n    gini = 1.0 - sum(((freq / count) ** 2 for freq in color_frequencies.values()))\n    return round(gini, 4)"}
{"task_id": 108, "completion_id": 4, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon Entropy to compute the disorder.\n    Returns the result rounded to four decimal places.\n    \"\"\"\n    if not apples:\n        return 0.0\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        prob = count / total\n        entropy -= prob * math.log2(prob)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 5, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is computed using Shannon Entropy.\n    \"\"\"\n    if not apples:\n        return 0.0\n    total = len(apples)\n    counter = Counter(apples)\n    entropy = 0.0\n    for count in counter.values():\n        probability = count / total\n        entropy -= probability * math.log2(probability)\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 6, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Returns a float rounded to 4 decimal places.\n    \"\"\"\n    count = len(apples)\n    if count == 0:\n        return 0.0\n    color_counts = Counter(apples)\n    probabilities = [v / count for v in color_counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 7, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Disorder increases with greater variety of colors. \n    Returns a float, rounded to 4 decimal places.\n    \"\"\"\n    if not apples:\n        return 0.0\n    count = len(apples)\n    color_counts = Counter(apples)\n    probabilities = [freq / count for freq in color_counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities))\n    max_entropy = math.log2(len(color_counts))\n    normalized_disorder = entropy / max_entropy if max_entropy > 0 else 0.0\n    return round(normalized_disorder, 4)"}
{"task_id": 108, "completion_id": 8, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    counts = Counter(apples)\n    probabilities = [freq / total for freq in counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities))\n    return round(entropy, 4)"}
{"task_id": 108, "completion_id": 9, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    Uses Shannon Entropy to compute the disorder.\n    \"\"\"\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n    for count in counts.values():\n        if count > 0:\n            p = count / total\n            entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: Input array of shape (batch_size, sequence_length, features)\n    - gamma: Scale parameter of shape (features,)\n    - beta: Shift parameter of shape (features,)\n    - epsilon: Small value to prevent division by zero\n    \n    Returns:\n    - normalized_X: The result of applying layer normalization, rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma * X_normalized + beta\n    return X_scaled_shifted.round(5).tolist()"}
{"task_id": 109, "completion_id": 1, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, features)\n    - gamma: Scaling parameter (same shape as features)\n    - beta: Shifting parameter (same shape as features)\n    - epsilon: Small value to avoid division by zero\n    \n    Returns:\n    - Normalized output tensor as a list, rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = gamma * X_normalized + beta\n    return np.round(X_scaled, 5).tolist()"}
{"task_id": 109, "completion_id": 2, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor.\n\n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, feature_dim)\n    - gamma: Scale parameter of shape (feature_dim,)\n    - beta: Shift parameter of shape (feature_dim,)\n    - epsilon: Small value to avoid division by zero\n    \n    Returns:\n    - X_normalized: Normalized and scaled tensor, rounded to 5 decimal places and converted to list format\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = X_normalized * gamma + beta\n    return np.round(X_normalized, 5).tolist()"}
{"task_id": 109, "completion_id": 3, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, features)\n    gamma (np.ndarray): Scale parameters (length = features)\n    beta (np.ndarray): Shift parameters (length = features)\n    epsilon (float): Small value to avoid division by zero\n\n    Returns:\n    np.ndarray: Normalized tensor as list after rounding to 5 decimal places\n    \"\"\"\n    X_transposed = X.transpose(0, 2, 1)\n    mean = np.mean(X_transposed, axis=2, keepdims=True)\n    var = np.var(X_transposed, axis=2, keepdims=True)\n    X_normalized = (X_transposed - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma * X_normalized + beta\n    X_final = X_scaled_shifted.transpose(0, 2, 1)\n    return np.round(X_final, 5).tolist()"}
{"task_id": 109, "completion_id": 4, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = gamma * X_normalized + beta\n    return np.round(X_scaled, 5).tolist()"}
{"task_id": 109, "completion_id": 5, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, sequence_length, features)\n    gamma (np.ndarray): Scaling parameters of shape (features,)\n    beta (np.ndarray): Shifting parameters of shape (features,)\n    epsilon (float): Small value to prevent division by zero\n\n    Returns:\n    List: Normalized tensor rounded to 5 decimal places and converted to a list.\n    \"\"\"\n    if gamma.ndim != 1 or beta.ndim != 1 or gamma.shape != beta.shape:\n        raise ValueError('gamma and beta must be 1D arrays with the same shape as feature dimension')\n    if X.shape[2] != gamma.shape[0]:\n        raise ValueError('Last dimension of X must match the shape of gamma and beta')\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_norm * gamma + beta\n    gamma_expanded = gamma[np.newaxis, np.newaxis, :]\n    beta_expanded = beta[np.newaxis, np.newaxis, :]\n    X_normalized = X_norm * gamma_expanded + beta_expanded\n    return np.round(X_normalized, 5).tolist()"}
{"task_id": 109, "completion_id": 6, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization over the feature dimension of the input tensor.\n    \n    Input:\n        X (np.ndarray): 3D tensor of shape (batch_size, sequence_length, features)\n        gamma (np.ndarray): Learnable scaling parameters of shape (features,)\n        beta (np.ndarray): Learnable shifting parameters of shape (features,)\n        epsilon (float): Small value to avoid division by zero\n    \n    Output:\n        np.ndarray: Normalized and scaled tensor, rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    variance = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = gamma * X_normalized\n    X_shifted = X_scaled + beta\n    return np.round(X_shifted, 5).tolist()"}
{"task_id": 109, "completion_id": 7, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n\n    Parameters:\n    - X: 3D numpy array of shape (batch_size, sequence_length, features)\n    - gamma: 1D numpy array of shape (features,), scale parameters\n    - beta: 1D numpy array of shape (features,), shift parameters\n    - epsilon: Small value to avoid division by zero\n\n    Returns:\n    - normalized_X: 3D numpy array of normalized values, rounded to 5 decimal places and converted to list\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = X_normalized * gamma + beta\n    return np.around(X_scaled_shifted, decimals=5).tolist()"}
{"task_id": 109, "completion_id": 8, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on an input tensor.\n    \n    Parameters:\n    - X: Input tensor of shape (batch_size, sequence_length, feature_dim)\n    - gamma: Scale parameter of shape (feature_dim,)\n    - beta: Shift parameter of shape (feature_dim,)\n    - epsilon: A small value to prevent division by zero\n    \n    Returns:\n    - normalized_X: The normalized tensor rounded to 5 decimal places and converted to a Python list.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = X_normalized * gamma + beta\n    normalized_X = np.round(X_scaled, 5).tolist()\n    return normalized_X"}
{"task_id": 109, "completion_id": 9, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape [batch_size, sequence_length, feature_dim]\n    - gamma (np.ndarray): Scale parameter of shape [feature_dim]\n    - beta (np.ndarray): Shift parameter of shape [feature_dim]\n    - epsilon (float): Small value for numerical stability\n\n    Returns:\n    - np.ndarray.tolist(): Normalized tensor as a Python list, rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * X_normalized + beta\n    return np.round(output, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter, defaultdict\nfrom itertools import combinations\nimport math\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    ref_counter = Counter(reference)\n    can_counter = Counter(candidate)\n    common_unigrams = set(can_counter) & set(ref_counter)\n    match_count = sum((min(can_counter[w], ref_counter[w]) for w in common_unigrams))\n    total_candidate = len(candidate)\n    total_reference = len(reference)\n    if not match_count:\n        return 0.0\n    precision = match_count / total_candidate\n    recall = match_count / total_reference\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    f_mean = math.sqrt(f_mean)\n    length_ratio = total_candidate / total_reference\n    if length_ratio < 1:\n        length_penalty = 1\n    else:\n        length_penalty = 1 - math.exp(1 - length_ratio)\n    length_norm = min(1, total_candidate ** 0.25)\n    minimum = np.minimum(len(candidate), len(reference))\n    pos = 0\n    wins = defaultdict(list)\n    used = set()\n    while pos < len(candidate):\n        matched = None\n        for r in range(max(0, pos - 2), min(len(reference), pos + 3)):\n            if r in used:\n                continue\n            if candidate[pos] == reference[r]:\n                matched = r\n                used.add(r)\n                break\n        if matched is None:\n            for idx in list(wins.keys()):\n                if idx > max(used or [0]) and reference[idx] == candidate[pos]:\n                    matched = idx\n                    used.add(idx)\n                    wins.pop(idx)\n                    break\n            else:\n                wins[pos].append(pos)\n        pos += 1\n    aligned_pairs = list(wins.values())\n    chunks = sum((1 for indices in aligned_pairs if len(set((indices[i + 1] - indices[i] for i in range(len(indices) - 1)))) == 1))\n    smooth_chunks = max(1, chunks)\n    m = len(reference)\n    f = 1 - smooth_chunks / m if m > 0 else 0\n    score = (1 - gamma * length_penalty) * (f_mean * (1 - beta * f))\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 1, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import combinations\nimport re\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = re.findall('\\\\b\\\\w+\\\\b', reference.lower())\n    candidate = re.findall('\\\\b\\\\w+\\\\b', candidate.lower())\n    match_count = 0\n    for word in candidate:\n        if word in reference:\n            match_count += 1\n            reference.remove(word)\n    p = match_count / len(candidate) if len(candidate) > 0 else 0\n    r = match_count / len(reference) if len(reference) > 0 else 0\n    f_mean = (1 + beta ** 2) * (p * r) / (beta ** 2 * p + r) if p + r > 0 else 0\n\n    def lcs(x, y):\n        matrix = np.zeros((len(x) + 1, len(y) + 1), dtype=int)\n        for i in range(1, len(x) + 1):\n            for j in range(1, len(y) + 1):\n                if x[i - 1] == y[j - 1]:\n                    matrix[i][j] = matrix[i - 1][j - 1] + 1\n                else:\n                    matrix[i][j] = max(matrix[i - 1][j], matrix[i][j - 1])\n        return matrix[len(x)][len(y)]\n    match_pos = []\n    ref_pos = Counter()\n    for (i, token) in enumerate(reference):\n        ref_pos[token] = ref_pos[token] + 1 if token in ref_pos else 1\n    for (i, token) in enumerate(candidate):\n        if token in ref_pos and ref_pos[token] > 0:\n            ref_pos[token] -= 1\n            match_pos.append((i, reference.index(token, sum((1 for x in reference[:reference.index(token)] if x == token)))))\n    if len(match_pos) > 0:\n\n        def get_ordered_words(pos_list, text):\n            indices = [i for (i, _) in pos_list]\n            return [text[i] for i in sorted(indices)]\n        candidate_ordered = get_ordered_words(match_pos, candidate)\n        reference_ordered = get_ordered_words(match_pos, reference)\n        m = lcs(candidate_ordered, reference_ordered)\n        pen = 1 - np.exp(-(gamma * np.sqrt(m / (len(candidate_ordered) + 1e-09))))\n    else:\n        pen = 1\n    score = (1 - alpha) * (p + r) / 2 + alpha * f_mean * (1 - pen)\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 2, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import combinations\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference_tokens = reference.lower().split()\n    candidate_tokens = candidate.lower().split()\n    match_tokens = set(reference_tokens) & set(candidate_tokens)\n    num_matches = len(match_tokens)\n    if num_matches == 0:\n        return 0.0\n    precision = num_matches / len(candidate_tokens)\n    recall = num_matches / len(reference_tokens)\n    f_score = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    f_score = 1 / f_score\n\n    def lcs(X, Y):\n        (m, n) = (len(X), len(Y))\n        dp = np.zeros((m + 1, n + 1))\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if X[i - 1] == Y[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[m][n]\n    lcs_length = lcs(reference_tokens, candidate_tokens)\n    m = len(candidate_tokens)\n    n = len(reference_tokens)\n    if lcs_length > 0 and m * n > 0:\n        penalties = 1 - lcs_length / (m + n - lcs_length)\n    else:\n        penalties = 0.0\n    score = gamma * f_score + (1 - gamma) * (1 - penalties)\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 3, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import product\nfrom math import exp\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.split()\n    candidate = candidate.split()\n    ngram_matcher = Counter(reference) & Counter(candidate)\n    n_match = sum(ngram_matcher.values())\n    (n_ref, n_candidate) = (len(reference), len(candidate))\n    precision = n_match / n_candidate if n_candidate > 0 else 0\n    recall = n_match / n_ref if n_ref > 0 else 0\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall) if precision + recall > 0 else 0\n\n    def lcs(X, Y):\n        (m, n) = (len(X), len(Y))\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        for i in range(m + 1):\n            for j in range(n + 1):\n                if i == 0 or j == 0:\n                    dp[i][j] = 0\n                elif X[i - 1] == Y[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[m][n]\n    lcs_length = lcs(reference, candidate)\n    if n_match > 0:\n        pen = 1 - exp(-1 * gamma * (lcs_length / n_match))\n    else:\n        pen = 0\n    score = (1 - beta) * f_mean * pen + beta * f_mean\n    return round(score, 3)\nreference = 'The cat is sitting on the mat'\ncandidate = 'The cat is sitting on the mat'"}
{"task_id": 110, "completion_id": 4, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score between a reference translation and a candidate translation.\n    \n    Parameters:\n    - reference (str): The reference translation.\n    - candidate (str): The candidate translation.\n    - alpha (float): Weight for precision-recall F-mean (default: 0.9).\n    - beta (float): Weight for long common subsequences (default: 3).\n    - gamma (float): Weight for fragmentation penalty (default: 0.5).\n\n    Returns:\n    - float: METEOR score (rounded to 3 decimal places).\n    \"\"\"\n    reference_tokens = reference.lower().split()\n    candidate_tokens = candidate.lower().split()\n    matches = set(reference_tokens) & set(candidate_tokens)\n    num_matches = len(matches)\n    if num_matches == 0:\n        return 0.0\n    precision = num_matches / len(candidate_tokens)\n    recall = num_matches / len(reference_tokens)\n    f_mean = ((1 / alpha) ** 2 + 1) * precision * recall / (precision + recall)\n\n    def lcs(x, y):\n        \"\"\" Helper function to find the length of the longest common subsequence \"\"\"\n        dp = [[0] * (len(y) + 1) for _ in range(len(x) + 1)]\n        for i in range(len(x)):\n            for j in range(len(y)):\n                if x[i] == y[j]:\n                    dp[i + 1][j + 1] = dp[i][j] + 1\n                else:\n                    dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n        return dp[len(x)][len(y)]\n    lcs_len = lcs(reference_tokens, candidate_tokens)\n    ref_counter = Counter(reference_tokens)\n    match_positions = []\n    for token in candidate_tokens:\n        if ref_counter[token] > 0:\n            match_positions.append(True)\n            ref_counter[token] -= 1\n        else:\n            match_positions.append(False)\n    chunks = 0\n    in_chunk = False\n    for is_match in match_positions:\n        if is_match:\n            if not in_chunk:\n                chunks += 1\n                in_chunk = True\n        else:\n            in_chunk = False\n    if lcs_len == 0 or chunks == 0:\n        fragment_penalty = 1.0\n    else:\n        fragment_penalty = 1 - gamma * (chunks - lcs_len) / lcs_len\n    score = f_mean * fragment_penalty\n    length_ratio = len(candidate_tokens) / len(reference_tokens)\n    if length_ratio > 1:\n        score *= 1 / beta ** (length_ratio - 1)\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 5, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import product\nfrom typing import List\ndef meteor_score(reference: str, candidate: str, alpha: float=0.9, beta: float=3, gamma: float=0.5) -> float:\n    \"\"\"\n    Compute the METEOR score between a reference translation and a candidate translation.\n\n    Args:\n        reference (str): The reference translation.\n        candidate (str): The candidate translation.\n        alpha (float): Weight for the F-mean score.\n        beta (float): Weight for the length penalty.\n        gamma (float): Weight for the chunk penalty.\n\n    Returns:\n        float: METEOR score rounded to 3 decimal places.\n    \"\"\"\n    reference_words = reference.lower().split()\n    candidate_words = candidate.lower().split()\n\n    def count_unigrams(words):\n        return Counter(words)\n    reference_unigrams = count_unigrams(reference_words)\n    candidate_unigrams = count_unigrams(candidate_words)\n    matches = sum((min(reference_unigrams[word], candidate_unigrams.get(word, 0)) for word in reference_unigrams))\n    if matches == 0:\n        return 0.0\n    n = len(candidate_words)\n    m = len(reference_words)\n    precision = matches / n\n    recall = matches / m\n    f_mean = (precision * recall / (alpha * precision + (1 - alpha) * recall)) ** beta\n    len_ratio = abs(n - m) / max(n, m)\n    penalty_length = 1 - gamma * len_ratio\n\n    def longest_common_subsequence(a: List[str], b: List[str]) -> int:\n        dp = np.zeros((len(a) + 1, len(b) + 1))\n        for (i, j) in product(range(1, len(a) + 1), range(1, len(b) + 1)):\n            if a[i - 1] == b[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1] + 1\n            else:\n                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[len(a)][len(b)]\n    lcs = longest_common_subsequence(reference_words, candidate_words)\n    chunks = 1.0\n    if matches > 0:\n        chunks = 0\n        for i in range(m):\n            if reference_words[i] in candidate_unigrams:\n                chunks += 1\n        chunks = matches / lcs if lcs > 0 else 1\n        chunks_penalty = 1 / (1 + gamma * chunks)\n    else:\n        chunks_penalty = 0\n    score = f_mean * penalty_length * chunks_penalty\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 6, "solution": "import numpy as np\nfrom collections import Counter, deque\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def tokenize(text):\n        return text.split()\n    r_tokens = tokenize(reference)\n    c_tokens = tokenize(candidate)\n    if not c_tokens or not r_tokens:\n        return 0.0\n    r_counter = Counter(r_tokens)\n    c_counter = Counter(c_tokens)\n    matches = sum((c_counter & r_counter).values())\n    precision = matches / len(c_tokens)\n    recall = matches / len(r_tokens)\n\n    def f_mean(p, r, beta=3):\n        return (1 + beta ** 2) * p * r / (beta ** 2 * p + r)\n    f_mean_score = f_mean(precision, recall, beta)\n\n    def lcs_length(a, b):\n        n = len(a)\n        m = len(b)\n        dp = np.zeros((n + 1, m + 1))\n        for i in range(1, n + 1):\n            for j in range(1, m + 1):\n                if a[i - 1] == b[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n        return dp[n][m]\n    lcs = lcs_length(r_tokens, c_tokens)\n    sentence_length_ratio = (len(r_tokens) + len(c_tokens)) / 2\n    chunkiness = 1 - lcs / sentence_length_ratio\n    final_score = (1 - alpha) * precision + alpha * f_mean_score\n    final_score = (1 - gamma) * final_score + gamma * (1 - chunkiness)\n    return round(final_score, 3)"}
{"task_id": 110, "completion_id": 7, "solution": "import numpy as np\nfrom collections import Counter, defaultdict\nimport math\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    ref_counter = Counter(reference)\n    match_counter = Counter()\n    matches = 0\n    for word in candidate:\n        if word in ref_counter and ref_counter[word] > 0:\n            ref_counter[word] -= 1\n            match_counter[word] += 1\n            matches += 1\n    precision = matches / len(candidate) if len(candidate) > 0 else 0\n    recall = matches / len(reference) if len(reference) > 0 else 0\n    if precision == 0 or recall == 0:\n        return 0.0\n    f_mean = 10 * precision * recall / (9 * precision + recall)\n    aligned_indices = []\n    ref_positions = defaultdict(list)\n    for (i, word) in enumerate(reference):\n        ref_positions[word].append(i)\n    for word in match_counter:\n        for _ in range(match_counter[word]):\n            positions = ref_positions[word]\n            if positions:\n                ref_idx = positions.pop(0)\n                candidate_idx = candidate.index(word, *[len(aligned_indices)])\n                aligned_indices.append((ref_idx, candidate_idx))\n    if not aligned_indices:\n        frag = 1.0\n    else:\n        aligned_indices.sort()\n        path_length = 1\n        (prev_ref, prev_can) = aligned_indices[0]\n        for i in range(1, len(aligned_indices)):\n            (curr_ref, curr_can) = aligned_indices[i]\n            path_length += 1 + abs(curr_can - prev_can)\n            path_length += 1 + abs(curr_ref - prev_ref)\n            (prev_ref, prev_can) = (curr_ref, curr_can)\n        frag_length = path_length / len(aligned_indices)\n        frag = 1 / (1 + gamma * (frag_length - 1))\n    score = alpha * f_mean + (1 - alpha) * frag\n    return round(score, 3)"}
{"task_id": 110, "completion_id": 8, "solution": "import numpy as np\nfrom collections import Counter\nfrom nltk.util import ngrams\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.strip().lower().split()\n    candidate = candidate.strip().lower().split()\n    matched_ngrams = set(reference) & set(candidate)\n    ngram_matches = len(matched_ngrams)\n    prec = ngram_matches / len(candidate) if len(candidate) > 0 else 0\n    recall = ngram_matches / len(reference) if len(reference) > 0 else 0\n    if prec + recall == 0:\n        fmean = 0.0\n    else:\n        fmean = (1 + beta ** 2) * prec * recall / (beta ** 2 * prec + recall)\n\n    def generate_bigrams(tokens):\n        return list(ngrams(tokens, 2))\n    reference_bigrams = generate_bigrams(reference)\n    candidate_bigrams = generate_bigrams(candidate)\n\n    def get_chunk_matches(a, b, bigrams_a, bigrams_b):\n        len_a = len(a)\n        len_b = len(b)\n        i = j = chunk_count = 0\n        while i < len_a and j < len_b:\n            a_bigram = bigrams_a[i]\n            b_bigram = bigrams_b[j]\n            if a_bigram == b_bigram:\n                chunk_count += 1\n                i += 1\n                j += 1\n            else:\n                i += 1\n                j += 1\n        return chunk_count\n    chunk_matches = get_chunk_matches(reference, candidate, reference_bigrams, candidate_bigrams)\n    max_chunks = min(len(reference_bigrams), len(candidate_bigrams)) if reference_bigrams else 1\n    ratio = chunk_matches / max_chunks if max_chunks > 0 else 0\n    penalty = 1 - np.exp(-gamma * ratio)\n    score = (1 - alpha) * fmean + alpha * prec - penalty\n    return round(score, 3)\nreference = 'The cat is on the mat'\ncandidate = 'The cat is on the mat'"}
{"task_id": 110, "completion_id": 9, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    matches = set(reference) & set(candidate)\n    n_matches = len(matches)\n    n_candidate = len(candidate)\n    n_reference = len(reference)\n    precision = n_matches / n_candidate if n_candidate > 0 else 0\n    recall = n_matches / n_reference if n_reference > 0 else 0\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    reference_bigrams = set(zip(reference, reference[1:]))\n    candidate_bigrams = set(zip(candidate, candidate[1:]))\n    aligned_m = set()\n    for (i, word) in enumerate(candidate):\n        if word in matches:\n            for (j, ref_word) in enumerate(reference):\n                if ref_word == word:\n                    aligned_m.add((i, j))\n                    break\n    aligned_m = sorted(aligned_m, key=lambda x: x[1])\n    pending_refs = set(range(n_reference))\n    matched_refs = set()\n    total_chunks = 0\n    i = 0\n    while i < n_reference:\n        if i in pending_refs:\n            matched_refs.add(i)\n            total_chunks += 1\n            j = i + 1\n            while j < n_reference and j in pending_refs:\n                matched_refs.add(j)\n                j += 1\n        if i in matched_refs:\n            pending_refs.remove(i)\n        i += 1\n    chunk_count = total_chunks\n    longest_fragment = 0\n    current = 0\n    for i in range(n_reference):\n        if i in matched_refs:\n            current += 1\n            longest_fragment = max(longest_fragment, current)\n        else:\n            current = 0\n    if chunk_count > 0:\n        g = longest_fragment / chunk_count\n    else:\n        g = 0\n    if f_mean == 0 or g == 0:\n        return 0.0\n    score = f_mean * gamma ** g\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 1, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n    \n    Parameters:\n    - joint_counts (int): The number of times both events occurred together.\n    - total_counts_x (int): The total number of times event X occurred.\n    - total_counts_y (int): The total number of times event Y occurred.\n    - total_samples (int): The total number of samples.\n    \n    Returns:\n    - float: The PMI rounded to 3 decimal places.\n    \"\"\"\n    expected = total_counts_x * total_counts_y / total_samples\n    if joint_counts == 0:\n        pmi = -np.inf\n    else:\n        pmi = np.log2(joint_counts / expected)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 2, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n    \n    Parameters:\n    joint_counts (int): Number of times both events x and y co-occurred\n    total_counts_x (int): Total occurrences of event x\n    total_counts_y (int): Total occurrences of event y\n    total_samples (int): Total number of samples\n\n    Returns:\n    float: PMI value rounded to 3 decimal places\n    \"\"\"\n    if joint_counts <= 0 or total_counts_x <= 0 or total_counts_y <= 0 or (total_samples <= 0):\n        return 0.0\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 3, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 4, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n    \n    Parameters:\n    joint_counts (int): Number of times the two events occurred together.\n    total_counts_x (int): Total occurrences of event x.\n    total_counts_y (int): Total occurrences of event y.\n    total_samples (int): Total number of samples/observations.\n    \n    Returns:\n    float: PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts <= 0 or total_counts_x <= 0 or total_counts_y <= 0 or (total_samples <= 0):\n        return float('-inf')\n    expected_count = total_counts_x * total_counts_y / total_samples\n    pmi = np.log(joint_counts / expected_count)\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 5, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    prob_x = total_counts_x / total_samples\n    prob_y = total_counts_y / total_samples\n    expected_joint = prob_x * prob_y * total_samples\n    epsilon = 1e-10\n    pmi = np.log2((joint_counts + epsilon) / (expected_joint + epsilon))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 6, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x == 0 or p_y == 0 or p_xy == 0:\n        return 0.0\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 5\ntotal_counts_x = 50\ntotal_counts_y = 70\ntotal_samples = 500"}
{"task_id": 111, "completion_id": 7, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) between two events.\n\n    Parameters:\n    - joint_counts: int, number of times both events occurred together\n    - total_counts_x: int, total number of times event X occurred\n    - total_counts_y: int, total number of times event Y occurred\n    - total_samples: int, total number of samples in the data\n\n    Returns:\n    - float: PMI value rounded to 3 decimal places\n    \"\"\"\n    if total_counts_x == 0 or total_counts_y == 0 or total_samples == 0:\n        return 0.0\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    if p_xy == 0 or p_x * p_y == 0:\n        return 0.0\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 8, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute Pointwise Mutual Information (PMI) between two events.\n    \n    Parameters:\n    - joint_counts: int, number of times both events occur together\n    - total_counts_x: int, total occurrences of event x\n    - total_counts_y: int, total occurrences of event y\n    - total_samples: int, total number of observations\n    \n    Returns:\n    - pmi: float, PMI value rounded to 3 decimal places\n    \"\"\"\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return float('-inf')\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 111, "completion_id": 9, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x or len(x) < 2:\n        raise ValueError('Input list must contain at least two elements for normalization.')\n    x_min = min(x)\n    x_max = max(x)\n    if x_min == x_max:\n        return [1.0 for _ in x]\n    normalized = [(val - x_min) / (x_max - x_min) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 1, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 112, "completion_id": 2, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 3, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(value - min_val) / (max_val - min_val) for value in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 4, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 5, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 6, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    if len(set(x)) == 1:\n        return [0.0 for _ in x]\n    x_min = min(x)\n    x_max = max(x)\n    normalized = [(val - x_min) / (x_max - x_min) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 112, "completion_id": 7, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 112, "completion_id": 8, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 112, "completion_id": 9, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    h1 = np.maximum(0, x @ w1)\n    h2 = np.maximum(0, h1 @ w2)\n    output = h2 + x\n    output = np.maximum(0, output)\n    return [round(val, 4) for val in output.tolist()]"}
{"task_id": 113, "completion_id": 1, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    A simple residual block implementation using NumPy.\n    \n    Args:\n        x (np.ndarray): Input array (1D).\n        w1 (np.ndarray): First weight matrix.\n        w2 (np.ndarray): Second weight matrix.\n        \n    Returns:\n        list: Output as a list of floats with 4 decimal places.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    a1 = relu(np.dot(x, w1))\n    a2 = np.dot(a1, w2)\n    out = a2 + x\n    final_output = relu(out)\n    return [round(i, 4) for i in final_output.tolist()]"}
{"task_id": 113, "completion_id": 2, "solution": "import numpy as np\ndef relu(x: np.ndarray) -> np.ndarray:\n    return np.maximum(0, x)\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = relu(z1)\n    z2 = np.dot(a1, w2)\n    a2 = relu(z2)\n    output = x + a2\n    output = relu(output)\n    return [round(val, 4) for val in output.tolist()]\nx = np.array([1.0, 2.0])\nw1 = np.array([[0.5, -0.3], [0.8, 0.2]])\nw2 = np.array([[0.1, 0.4], [-0.2, 0.7]])"}
{"task_id": 113, "completion_id": 3, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with two weight matrices (w1 and w2),\n    ReLU activations, and a shortcut connection.\n    \n    Parameters:\n    - x (np.ndarray): 1D input array\n    - w1 (np.ndarray): First weight matrix\n    - w2 (np.ndarray): Second weight matrix\n    \n    Returns:\n    - List of floats: Final output after processing with residual connection,\n                     rounded to 4 decimal places.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    out = np.dot(x, w1)\n    out = relu(out)\n    out = np.dot(out, w2)\n    out = relu(out)\n    out += x\n    out = relu(out)\n    return np.round(out, 4).tolist()"}
{"task_id": 113, "completion_id": 4, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n\n    def relu(z):\n        return np.maximum(0, z)\n    x1 = np.dot(x, w1)\n    x1_relu = relu(x1)\n    x2 = np.dot(x1_relu, w2)\n    output = x + x2\n    output_relu = relu(output)\n    return np.round(output_relu, 4).tolist()"}
{"task_id": 113, "completion_id": 5, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    out = np.dot(x, w1)\n    out = np.maximum(0, out)\n    out = np.dot(out, w2)\n    out += x\n    out = np.maximum(0, out)\n    return [round(val, 4) for val in out.tolist()]"}
{"task_id": 113, "completion_id": 6, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x1 = np.dot(x, w1)\n    x1_relu = np.maximum(0, x1)\n    x2 = np.dot(x1_relu, w2)\n    x2_relu = np.maximum(0, x2)\n    output = x + x2_relu\n    output_relu = np.maximum(0, output)\n    return output_relu.round(4).tolist()"}
{"task_id": 113, "completion_id": 7, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    out = x + a2\n    final_out = np.maximum(0, out)\n    return [np.round(val, 4) for val in final_out.tolist()]"}
{"task_id": 113, "completion_id": 8, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    z1 = np.dot(w1, x)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(w2, a1)\n    a2 = np.maximum(0, z2)\n    output = a2 + x\n    final_output = np.maximum(0, output)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 113, "completion_id": 9, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with two weight layers and shortcut connection.\n\n    Parameters:\n    - x: Input 1D array (shape: (input_dim,))\n    - w1: Weight matrix for the first linear transformation (shape: (hidden_dim, input_dim))\n    - w2: Weight matrix for the second linear transformation (shape: (input_dim, hidden_dim))\n\n    Returns:\n    - Output after applying the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    a1 = relu(np.dot(w1, x))\n    a2 = np.dot(w2, a1)\n    out = relu(x + a2)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array of shape (height, width, channels).\n\n    Returns\n    -------\n    np.ndarray\n        Output array of shape (channels,).\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 1, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input feature maps of shape (height, width, channels)\n\n    Returns:\n    np.ndarray: Output of shape (channels,), where each element is the average over the corresponding channel.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D NumPy array (height, width, channels).')\n    result = np.mean(x, axis=(0, 1))\n    return result\nx = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])"}
{"task_id": 114, "completion_id": 2, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: Output array of shape (channels,).\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array (height, width, channels).')\n    (height, width, channels) = x.shape\n    output = np.mean(x, axis=(0, 1))\n    return output"}
{"task_id": 114, "completion_id": 3, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    - x: np.ndarray of shape (height, width, channels)\n    \n    Returns:\n    - np.ndarray of shape (channels,), where each element is the average of values in the corresponding feature map\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 4, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input with shape (height, width, channels)\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) with the average of each channel\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array with shape (height, width, channels)')\n    (height, width, channels) = x.shape\n    return x.mean(axis=(0, 1))"}
{"task_id": 114, "completion_id": 5, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Applies Global Average Pooling to a 3D array (height, width, channels).\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: Output array of shape (channels,).\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f'Input must be a 3D NumPy array. Got {x.ndim}D array instead.')\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 6, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels)\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) with the average of each channel.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 7, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n        x (np.ndarray): Input of shape (height, width, channels)\n    \n    Returns:\n        np.ndarray: Output of shape (channels,), where each element is the average of the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array with shape (height, width, channels).')\n    (height, width, channels) = x.shape\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 8, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Apply Global Average Pooling to a 3D NumPy array of shape (height, width, channels).\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) with average values across each feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 114, "completion_id": 9, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D input array.\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: 1D array of shape (channels,), where each element is the average\n                of the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D numpy array (height, width, channels)')\n    result = np.mean(x, axis=(0, 1))\n    return result"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in the BCHW format.\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Input array of shape (batch, channels, height, width)\n    gamma : np.ndarray\n        Scale parameters (one per channel), shape (channels,)\n    beta : np.ndarray\n        Shift parameters (one per channel), shape (channels,)\n    epsilon : float, optional\n        A small value added to the variance to avoid division by zero.\n\n    Returns:\n    --------\n    list\n        Normalized output as a list (rounded to 4 decimal places)\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    assert gamma.shape == (channels,), 'Gamma shape mismatch'\n    assert beta.shape == (channels,), 'Beta shape mismatch'\n    output = np.zeros_like(X)\n    for c in range(channels):\n        X_channel = X[:, c, :, :]\n        X_channel_flat = X_channel.reshape(-1)\n        mean = np.mean(X_channel_flat)\n        var = np.var(X_channel_flat)\n        X_normalized = (X_channel_flat - mean) / np.sqrt(var + epsilon)\n        X_normalized = X_normalized.reshape(batch_size, height, width)\n        output[:, c, :, :] = gamma[c] * X_normalized + beta[c]\n    return np.round(output, 4).tolist()"}
{"task_id": 115, "completion_id": 1, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in BCHW format.\n    \n    Parameters:\n    - X: Input tensor with shape (batch, channels, height, width)\n    - gamma: Scale parameters with shape (channels,)\n    - beta: Shift parameters with shape (channels,)\n    - epsilon: Small value used for numerical stability\n    \n    Returns:\n    - Output tensor after applying batch normalization, rounded to 4 decimal places and converted to a list\n    \"\"\"\n    (batch, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_centered = X - mean\n    X_normalized = X_centered / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma[None, :, None, None] * X_normalized + beta[None, :, None, None]\n    return np.round(X_scaled_shifted, 4).tolist()"}
{"task_id": 115, "completion_id": 2, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (N, C, H, W) = X.shape\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(C, -1)\n    mean = np.mean(X_reshaped, axis=1, keepdims=True)\n    var = np.var(X_reshaped, axis=1, keepdims=True)\n    X_norm_reshaped = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm_reshaped.reshape(C, N, H, W).transpose(1, 0, 2, 3)\n    output = gamma * X_norm + beta\n    return np.round(output, 4).tolist()"}
{"task_id": 115, "completion_id": 3, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D input with shape (batch, channels, height, width) [BCHW].\n    \n    Parameters:\n    - X: 4D input array of shape (B, C, H, W)\n    - gamma: Scale parameter of shape (C,)\n    - beta: Shift parameter of shape (C,)\n    - epsilon: Small value to avoid division by zero\n    \n    Returns:\n    - Output of batch normalization rounded to 4 decimal places as a Python list.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    X_normalized = (X - mean.reshape(1, C, 1, 1)) / np.sqrt(var + epsilon).reshape(1, C, 1, 1)\n    X_output = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n    return np.round(X_output, 4).tolist()"}
{"task_id": 115, "completion_id": 4, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X: Input array of shape (batch, channels, height, width)\n    - gamma: Scale parameter array of shape (channels,)\n    - beta: Shift parameter array of shape (channels,)\n    - epsilon: Small value for numerical stability\n\n    Returns:\n    - Normalized output array rounded to 4 decimals and converted to a list.\n    \"\"\"\n    (batch, channels, height, width) = X.shape\n    X_reshaped = X.reshape(batch, channels, -1)\n    mean = np.mean(X_reshaped, axis=(0, 2))\n    variance = np.var(X_reshaped, axis=(0, 2))\n    mean = mean.reshape(1, channels, 1, 1)\n    variance = variance.reshape(1, channels, 1, 1)\n    X_norm = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma.reshape(1, channels, 1, 1) * X_norm + beta.reshape(1, channels, 1, 1)\n    return np.round(X_normalized, 4).tolist()"}
{"task_id": 115, "completion_id": 5, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in the BCHW format.\n\n    Parameters:\n    - X (np.ndarray): Input in the shape (batch, channels, height, width)\n    - gamma (np.ndarray): Channel-wise scale parameters in the shape (channels,)\n    - beta (np.ndarray): Channel-wise shift parameters in the shape (channels,)\n    - epsilon (float): Small float added to variance to avoid division by zero\n\n    Returns:\n    - np.ndarray: Normalized tensor, rounded to 4 decimal places\n    \"\"\"\n    (batch, channels, height, width) = X.shape\n    gamma = gamma.reshape((1, channels, 1, 1))\n    beta = beta.reshape((1, channels, 1, 1))\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = gamma * X_normalized + beta\n    return np.round(output, 4).tolist()"}
{"task_id": 115, "completion_id": 6, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (batch_size, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    X_normalized = (X - mean[:, None, None]) / np.sqrt(var[:, None, None] + epsilon)\n    X_output = gamma[:, None, None] * X_normalized + beta[:, None, None]\n    X_output_rounded = np.round(X_output, 4)\n    return X_output_rounded.tolist()\nX = np.random.randn(10, 3, 28, 28)\ngamma = np.random.randn(3)\nbeta = np.random.randn(3)"}
{"task_id": 115, "completion_id": 7, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D BCHW (batch, channels, height, width) NumPy array.\n    \n    Parameters:\n        X (np.ndarray): Input array of shape (batch, channels, height, width)\n        gamma (np.ndarray): Scale parameter of shape (channels,)\n        beta (np.ndarray): Shift parameter of shape (channels,)\n        epsilon (float): Small epsilon value for numerical stability\n    \n    Returns:\n        np.ndarray: Normalized output, rounded to 4 decimal places\n    \"\"\"\n    (B, C, H, W) = X.shape\n    X_reshaped = X.reshape(B, C, -1)\n    mean = np.mean(X_reshaped, axis=(0, 2))\n    var = np.var(X_reshaped, axis=(0, 2))\n    std = np.sqrt(var + epsilon)\n    X_norm = (X_reshaped - mean[:, np.newaxis]) / std[:, np.newaxis]\n    X_norm = X_norm.reshape(B, C, H, W)\n    output = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_norm + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(output, 4).tolist()"}
{"task_id": 115, "completion_id": 8, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    (batch, channels, height, width) = X.shape\n    X_re = X.transpose(1, 0, 2, 3).reshape(channels, -1).T\n    mean = np.mean(X_re, axis=0)\n    var = np.var(X_re, axis=0)\n    X_norm = (X - mean[None, :, None, None]) / np.sqrt(var[None, :, None, None] + epsilon)\n    output = gamma[None, :, None, None] * X_norm + beta[None, :, None, None]\n    return output.round(4).tolist()"}
{"task_id": 115, "completion_id": 9, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X: Input tensor of shape (batch, channels, height, width)\n    - gamma: Scale parameter of shape (channels,)\n    - beta: Shift parameter of shape (channels,)\n    - epsilon: Small value for numerical stability\n\n    Returns:\n    - Normalized output tensor rounded to 4 decimal places.\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(channels, -1)\n    mean = X_reshaped.mean(axis=1).reshape(-1, 1)\n    var = X_reshaped.var(axis=1).reshape(-1, 1)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized * gamma.reshape(-1, 1) + beta.reshape(-1, 1)\n    X_normalized = X_normalized.reshape(channels, batch_size, height, width).transpose(1, 0, 2, 3)\n    return np.round(X_normalized, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 1, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of the polynomial term c * x^n evaluated at x.\n\n    Args:\n        c (float): The coefficient of the polynomial term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent of the term.\n\n    Returns:\n        float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 2, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of the polynomial term c * x^n at the given point x.\n    \n    The derivative is computed using the power rule: d/dx (c * x^n) = c * n * x^(n - 1).\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n    \n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 3, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of the polynomial term c * x^n at a given x.\n\n    Parameters:\n    c (float): Coefficient of the polynomial term\n    x (float): Point at which to evaluate the derivative\n    n (float): Exponent of the variable x\n\n    Returns:\n    float: The derivative evaluated at x, rounded to 4 decimal places\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 116, "completion_id": 4, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n\n    Parameters:\n    - c: Coefficient (float)\n    - x: Point at which to evaluate the derivative (float)\n    - n: Exponent (float)\n\n    Returns:\n    - The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 5, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 6, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term c * x^n at point x\n    using the power rule: d/dx (c * x^n) = c * n * x^(n-1)\n    \n    Args:\n        c (float): The coefficient.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent.\n\n    Returns:\n        float: The derivative evaluated at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 7, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term `c * x^n` at a given point `x`.\n    \n    Parameters:\n    - c: The coefficient of the polynomial term.\n    - x: The point at which the derivative is evaluated.\n    - n: The exponent of the variable x in the polynomial term.\n    \n    Returns:\n    - The derivative of the term, evaluated at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 8, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 116, "completion_id": 9, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = [np.array(v, dtype=float) for v in vectors]\n    basis = []\n    for v in vectors:\n        u = np.array(v)\n        for b in basis:\n            u = u - np.dot(u, b) / np.dot(b, b) * b\n        if np.linalg.norm(u) > tol:\n            u = u / np.linalg.norm(u)\n            basis.append(u)\n    return [np.round(b, 4).tolist() for b in basis]"}
{"task_id": 117, "completion_id": 1, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    Parameters:\n    - vectors: list of vectors, each of which is a list of two floats.\n    - tol: tolerance for determining linear independence.\n    \n    Returns:\n    - List of orthonormal basis vectors, each rounded to 4 decimal places.\n    \"\"\"\n    vectors = np.array(vectors)\n    n_vectors = vectors.shape[0]\n    vectors = vectors / np.linalg.norm(vectors, axis=1)[:, np.newaxis]\n    basis = []\n    for i in range(n_vectors):\n        v = vectors[i].copy()\n        for u in basis:\n            proj = np.dot(v, u) * u\n            v = v - proj\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            u = v / norm\n            basis.append(u)\n    return [np.round(vec, 4).tolist() for vec in basis]"}
{"task_id": 117, "completion_id": 2, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors).T\n    (n, m) = vectors.shape\n    basis = []\n    for i in range(n):\n        u = vectors[i].copy()\n        for j in range(len(basis)):\n            u -= np.dot(vectors[i], basis[j]) * basis[j]\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            e = u / norm\n            basis.append(e)\n    orthonormal_basis = np.array(basis)\n    return [np.round(vector, 4).tolist() for vector in orthonormal_basis.T]\nvectors = [[1, 2], [3, 4]]"}
{"task_id": 117, "completion_id": 3, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    B = np.array(vectors, dtype=float).T\n    n_vectors = B.shape[1]\n    orthonormal = np.zeros_like(B)\n    for i in range(n_vectors):\n        v = B[:, i]\n        for j in range(i):\n            if np.linalg.norm(orthonormal[:, j]) > tol:\n                v -= np.dot(v, orthonormal[:, j]) * orthonormal[:, j]\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            orthonormal[:, i] = v / norm\n    orthonormal_basis_list = [list(np.round(vec, 4)) for vec in orthonormal.T if np.linalg.norm(vec) > tol]\n    return orthonormal_basis_list"}
{"task_id": 117, "completion_id": 4, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by the input 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    - vectors: list of 2D vectors (list of lists)\n    - tol: tolerance for determining linear independence\n\n    Returns:\n    - list of orthonormal 2D vectors (rounded to 4 decimal places)\n      that span the same subspace\n    \"\"\"\n    V = np.array(vectors)\n    n = V.shape[0]\n    d = V.shape[1]\n    basis = []\n    for i in range(n):\n        v = V[i, :].copy()\n        for u in basis:\n            proj = np.dot(v, u) / np.dot(u, u) * u\n            v -= proj\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            u = v / norm\n            basis.append(u)\n    basis = [np.round(vec, 4).tolist() for vec in basis]\n    return basis"}
{"task_id": 117, "completion_id": 5, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors)\n    (n, d) = vectors.shape\n    if d != 2:\n        return []\n    ortho_basis = []\n    for v in vectors:\n        v = np.array(v)\n        if len(ortho_basis) == 0:\n            u = v\n        else:\n            for u_hat in ortho_basis:\n                u = v - np.dot(v, u_hat) * u_hat\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            u_unit = u / norm\n            ortho_basis.append(u_unit)\n    return [np.round(b, 4).tolist() for b in ortho_basis]"}
{"task_id": 117, "completion_id": 6, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    - vectors: List of 2D vectors (each is a list of two floats)\n    - tol: Tolerance value to check for linear independence\n\n    Returns:\n    - orthonormal_basis: List of orthonormal vectors (each is a list of two floats, rounded to 4 decimal places)\n    \"\"\"\n    vectors = np.array(vectors).T\n    n_vectors = vectors.shape[1]\n    basis = []\n    for i in range(n_vectors):\n        v = vectors[:, i].copy()\n        for u in basis:\n            v -= np.dot(v, u) * u\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            u = v / norm\n            basis.append(u)\n    return [np.round(vec, 4).tolist() for vec in basis]"}
{"task_id": 117, "completion_id": 7, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by the input 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    - vectors: List of 2D vectors (lists of two floats)\n    - tol: Tolerance for determining linear independence\n\n    Returns:\n    - A list of 2D orthonormal vectors that span the subspace (rounded to 4 decimals)\n    \"\"\"\n    vectors = np.array(vectors)\n    (num_vectors, dim) = vectors.shape\n    if dim != 2:\n        raise ValueError('All vectors must be 2-dimensional.')\n    basis = []\n    for v in vectors:\n        v_proj = np.zeros_like(v)\n        for u in basis:\n            proj = np.dot(v, u) / np.dot(u, u) * u\n            v_proj += proj\n        e = v - v_proj\n        norm = np.linalg.norm(e)\n        if norm > tol:\n            e_normalized = e / norm\n            basis.append(e_normalized)\n    basis_rounded = [np.round(vec, 4).tolist() for vec in basis]\n    return basis_rounded\nvectors = [[1, 1], [1, 0], [2, 1]]"}
{"task_id": 117, "completion_id": 8, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthogonal_vectors = []\n    for v in vectors:\n        for u in orthogonal_vectors:\n            v -= np.dot(v, u) / np.dot(u, u) * u\n        if np.linalg.norm(v) > tol:\n            v = v / np.linalg.norm(v)\n            orthogonal_vectors.append(v.tolist())\n    orthogonal_vectors = [list(map(lambda x: round(x, 4), vec)) for vec in orthogonal_vectors]\n    return orthogonal_vectors"}
{"task_id": 117, "completion_id": 9, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors)\n    n_vectors = vectors.shape[0]\n    basis = []\n    for v in vectors:\n        v_proj = np.array(v, dtype=float)\n        for u in basis:\n            proj = np.dot(v_proj, u) / np.dot(u, u) * u\n            v_proj -= proj\n        norm = np.linalg.norm(v_proj)\n        if norm > tol:\n            basis.append(v_proj / norm)\n    basis = [vec.round(4).tolist() for vec in basis]\n    return basis"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Parameters:\n    a (list or array-like): First vector (3D).\n    b (list or array-like): Second vector (3D).\n\n    Returns:\n    list: Resultant vector after cross product, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 1, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    return np.round(cross, 4).tolist()"}
{"task_id": 118, "completion_id": 2, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or numpy.ndarray): First 3D vector.\n    b (list or numpy.ndarray): Second 3D vector.\n    \n    Returns:\n    list: Cross product result, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a, dtype=float)\n    b = np.array(b, dtype=float)\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\nresult = cross_product(vector1, vector2)"}
{"task_id": 118, "completion_id": 3, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): First 3D vector.\n    b (list or np.ndarray): Second 3D vector.\n    \n    Returns:\n    list: Cross product vector rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    result = np.cross(a_np, b_np)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 4, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): First 3D vector.\n    b (list or np.ndarray): Second 3D vector.\n    \n    Returns:\n    list: Cross product vector as a list, rounded to 4 decimal places.\n    \"\"\"\n    a_array = np.array(a)\n    b_array = np.array(b)\n    result = np.cross(a_array, b_array)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 5, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors a and b.\n    Returns the resulting vector as a list rounded to 4 decimal places.\n    \"\"\"\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 6, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): A 3D vector (list or NumPy array of 3 elements).\n    b (list or np.ndarray): A 3D vector (list or NumPy array of 3 elements).\n    \n    Returns:\n    list: The cross product of the two vectors, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    result = np.cross(a_np, b_np)\n    return np.round(result, 4).tolist()\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\nresult = cross_product(vector_a, vector_b)"}
{"task_id": 118, "completion_id": 7, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n\n    Parameters:\n    a (list or np.ndarray): First 3D vector.\n    b (list or np.ndarray): Second 3D vector.\n\n    Returns:\n    list: Cross product vector as a list rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('Both inputs must be 3-dimensional vectors.')\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 8, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 118, "completion_id": 9, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cp = np.cross(a, b)\n    return np.round(cp, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        return -1\n    if A.shape[0] != len(b):\n        return -1\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    n = len(b)\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x\nA = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]\nb = [8, -11, -3]"}
{"task_id": 119, "completion_id": 1, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n        A (list of lists or 2D numpy array): Coefficient matrix (n x n)\n        b (list or 1D numpy array): Constant vector (length n)\n    \n    Returns:\n        list: Solution vector x (rounded to 4 decimal places), or -1 if no unique solution.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    if A.shape[1] != n or len(b) != n:\n        raise ValueError('Matrix A must be square and vector b must have the same length as matrix size.')\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(x_i)\n    x_solution = np.round(np.array(x), 4).tolist()\n    return x_solution"}
{"task_id": 119, "completion_id": 2, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (list of lists or numpy array): Square coefficient matrix.\n    b (list or numpy array): Constant vector.\n    \n    Returns:\n    list: Solution vector x, rounded to 4 decimal places.\n    -1: If the determinant of A is zero (no unique solution).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x = np.round(x, 4)\n    return x.tolist()\nA = [[2, 1], [1, 2]]\nb = [7, 5]"}
{"task_id": 119, "completion_id": 3, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    if A.shape[0] != A.shape[1] or A.shape[0] != len(b):\n        return -1\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        xi = det_Ai / det_A\n        x.append(round(xi, 4))\n    return x"}
{"task_id": 119, "completion_id": 4, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (numpy.ndarray): Square coefficient matrix (n x n)\n    b (numpy.ndarray): Constant vector (n x 1)\n\n    Returns:\n    list: Solution vector x rounded to 4 decimal places if a unique solution exists.\n          Returns -1 if the determinant of A is zero (i.e., no unique solution).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    n = A.shape[0]\n    if len(b) != n:\n        raise ValueError('The length of vector b must match the size of the square matrix A.')\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x_i = det_Ai / det_A\n        x.append(x_i)\n    x = [round(val, 4) for val in x]\n    return np.array(x).tolist()"}
{"task_id": 119, "completion_id": 5, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve the system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (list of lists or numpy.ndarray): Coefficient matrix (Square matrix).\n    b (list or numpy.ndarray): Constant vector.\n\n    Returns:\n    list: The solution vector x (rounded to 4 decimals), or -1 if no unique solution.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    if A.shape[1] != n:\n        return -1\n    if b.size != n:\n        return -1\n    determinant_A = np.linalg.det(A)\n    if np.isclose(determinant_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x_i = det_Ai / determinant_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 119, "completion_id": 6, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    if A.shape[1] != n or b.ndim != 1 or b.shape[0] != n:\n        return -1\n    determinant_A = np.linalg.det(A)\n    if np.isclose(determinant_A, 0):\n        return -1\n    x = np.zeros(n)\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        x[i] = np.linalg.det(Ai) / determinant_A\n    return np.round(x, 4).tolist()"}
{"task_id": 119, "completion_id": 7, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    - A (list of lists): Square matrix of coefficients.\n    - b (list): Constant vector.\n    \n    Returns:\n    - list: Solution vector x if a unique solution exists.\n    - -1: If the determinant of A is zero (no unique solution).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = A.shape[0]\n    if A.shape[1] != n:\n        raise ValueError('Matrix A is not square.')\n    if b.shape != (n,):\n        raise ValueError('Vector b has incorrect dimensions.')\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    x = []\n    for i in range(n):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    return x\nA = [[2, 1], [1, 1]]\nb = [3, 2]"}
{"task_id": 119, "completion_id": 8, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A)\n    b = np.array(b)\n    n = A.shape[0]\n    if A.shape[1] != n:\n        return -1\n    if b.shape[0] != n:\n        return -1\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x[i] = det_Ai / det_A\n    return np.round(x, 4).tolist()\nA = [[2, 1], [1, -1]]\nb = [3, 0]"}
{"task_id": 119, "completion_id": 9, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float).flatten()\n    if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:\n        return -1\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-10:\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    bc = np.sum(np.sqrt(p_array * q_array))\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 120, "completion_id": 1, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p, dtype=np.float64)\n    q_array = np.array(q, dtype=np.array)\n    bc = np.sum(np.sqrt(p_array * q_array))\n    if bc <= 0:\n        return 0.0\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 120, "completion_id": 2, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    if not all((x >= 0 for x in p)) or not all((x >= 0 for x in q)):\n        return 0.0\n    if abs(sum(p) - 1.0) > 1e-08 or abs(sum(q) - 1.0) > 1e-08:\n        return 0.0\n    bc = np.sum((np.sqrt(p[i] * q[i]) for i in range(len(p))))\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 120, "completion_id": 3, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p, dtype=np.float64)\n    q_array = np.array(q, dtype=np.float64)\n    if np.any(p_array < 0) or np.any(q_array < 0):\n        return 0.0\n    if not np.isclose(np.sum(p_array), 1.0) or not np.isclose(np.sum(q_array), 1.0):\n        return 0.0\n    bc = np.sum(np.sqrt(p_array * q_array))\n    epsilon = 1e-10\n    bc = np.clip(bc, epsilon, 1.0)\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 120, "completion_id": 4, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    if any((x < 0 for x in p)) or any((x < 0 for x in q)):\n        return 0.0\n    if not (abs(sum(p) - 1.0) < 1e-09 and abs(sum(q) - 1.0) < 1e-09):\n        return 0.0\n    bc = sum((np.sqrt(pi * qi) for (pi, qi) in zip(p, q)))\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 5, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_array = np.array(p)\n    q_array = np.array(q)\n    if not (np.allclose(np.sum(p_array), 1.0) and np.allclose(np.sum(q_array), 1.0)):\n        return 0.0\n    bc = np.sum(np.sqrt(p_array * q_array))\n    if bc == 0:\n        return 0.0\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 120, "completion_id": 6, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) == 0 or len(q) == 0 or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 7, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 8, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p, dtype=float)\n    q = np.array(q, dtype=float)\n    if p.size == 0 or q.size == 0 or p.size != q.size:\n        return 0.0\n    bc = np.sum(np.sqrt(p * q))\n    epsilon = 1e-12\n    if bc < epsilon:\n        return float('inf')\n    distance = -np.log(bc)\n    return round(distance, 4)"}
{"task_id": 120, "completion_id": 9, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p, dtype=np.float64)\n    q = np.array(q, dtype=np.float64)\n    bc = np.sum(np.sqrt(p * q))\n    if bc <= 0:\n        return 0.0\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 1, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 2, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 3, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 4, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 5, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 121, "completion_id": 6, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 7, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 121, "completion_id": 8, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 121, "completion_id": 9, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        G = []\n        returns = np.zeros(len(episode))\n        for t in reversed(range(len(episode))):\n            (s, a, r) = episode[t]\n            G.append(r)\n            returns[t] = np.sum(G)\n        returns -= np.mean(returns)\n        returns /= np.std(returns) + 1e-08\n        for t in range(len(episode)):\n            (s, a, r) = episode[t]\n            action_probs = np.exp(theta[s, :])\n            action_probs /= np.sum(action_probs)\n            one_hot = np.zeros(num_actions)\n            one_hot[a] = 1\n            grad = one_hot - action_probs\n            gradient[s, :] += grad * returns[t]\n    gradient /= len(episodes)\n    return [np.around(z, 4).tolist() for z in gradient]"}
{"task_id": 122, "completion_id": 1, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef compute_policy_gradient(theta: np.ndarray, episodes: List[List[Tuple[int, int, float]]]) -> List[float]:\n    (num_states, num_actions) = theta.shape\n    grad = np.zeros(theta.shape)\n    for episode in episodes:\n        returns = []\n        G = 0\n        for (_, _, reward) in reversed(episode):\n            G = reward + G\n            returns.insert(0, G)\n        for ((state, action, _), G) in zip(episode, returns):\n            logits = theta[state, :]\n            exp_logits = np.exp(logits - np.max(logits))\n            probs = exp_logits / exp_logits.sum()\n            grad[state, action] += G * (1 - probs[action]) / probs[action]\n            grad[state, [a for a in range(num_actions) if a != action]] += -G * probs[action] / probs[action]\n    grad /= len(episodes)\n    return np.round(grad, 4).tolist()"}
{"task_id": 122, "completion_id": 2, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    grad = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0\n        grads = []\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            G = reward + G\n            exp_theta = np.exp(theta[state])\n            pi = exp_theta / exp_theta.sum()\n            dlogpi = np.zeros(num_actions)\n            dlogpi[action] = 1 - pi[action]\n            for a in range(num_actions):\n                if a != action:\n                    dlogpi[a] = -pi[a]\n            grads.append(dlogpi * G)\n        grads.reverse()\n        for g in grads:\n            grad[episode[0][0]] += g\n    grad /= len(episodes)\n    rounded_grad = np.round(grad, 4)\n    return rounded_grad.tolist()"}
{"task_id": 122, "completion_id": 3, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient_sum = np.zeros_like(theta)\n    for episode in episodes:\n        G = 0.0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            G += reward\n            exp_theta = np.exp(theta[state, :])\n            pi = exp_theta / exp_theta.sum()\n            log_pi = np.log(pi[action])\n            grad_log_pi = np.zeros(num_actions)\n            grad_log_pi[action] = 1 - pi[action]\n            grad_log_pi -= pi\n            gradient_sum[state] += log_pi * grad_log_pi * G\n    gradient_avg = gradient_sum / np.array([len([tr for tr in episode if tr[0] == s]) for s in range(num_states)]).reshape(-1, 1)\n    gradient_avg = np.nan_to_num(gradient_avg)\n    return gradient_avg.round(4).tolist()"}
{"task_id": 122, "completion_id": 4, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = []\n        for (state, action, reward) in episode:\n            returns.append(reward)\n        returns = np.cumsum(returns[::-1])[::-1]\n        for (t, (state, action, reward)) in enumerate(episode):\n            logits = theta[state, :]\n            exp_logits = np.exp(logits - np.max(logits))\n            probs = exp_logits / np.sum(exp_logits)\n            log_prob = np.log(probs[action])\n            grad_log_prob = (np.diag(probs) - np.outer(probs, probs))[:, action]\n            gradient[state] += grad_log_prob * returns[t]\n    total_timesteps = sum((len(episode) for episode in episodes))\n    gradient /= total_timesteps\n    return np.round(gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 5, "solution": "import numpy as np\ndef softmax(logits):\n    \"\"\"Compute softmax function over the action dimension.\"\"\"\n    exps = np.exp(logits)\n    return exps / exps.sum(axis=1, keepdims=True)\ndef compute_log_prob(theta, state, action):\n    \"\"\"Compute the log probability of an action given a state under the policy theta.\"\"\"\n    action_probs = softmax(theta)\n    return np.log(action_probs[state, action])\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the average policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n        theta (np.ndarray): The policy parameter matrix of shape (num_states, num_actions).\n        episodes (list): List of episodes where each episode is a list of (state, action, reward) tuples.\n    \n    Returns:\n        list: Average gradient of the policy parameters (as a Python list).\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient_sum = np.zeros(theta.shape)\n    for episode in episodes:\n        for t in range(len(episode)):\n            (state, action, reward) = episode[t]\n            return_t = sum((step[2] for step in episode[t:]))\n            log_prob = compute_log_prob(theta, state, action)\n            grad_component = 1.0 / len(episode) * return_t * np.eye(num_actions)[action]\n            gradient_sum[state] += grad_component\n    average_gradient = gradient_sum / len(episodes)\n    return np.round(average_gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 6, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = []\n        rewards = [r for (s, a, r) in episode]\n        G = 0\n        for r in reversed(rewards):\n            G = r + G\n            returns.append(G)\n        returns = np.array(returns[::-1])\n        for (t, (s, a, r)) in enumerate(episode):\n            logits = theta[s]\n            exp_logits = np.exp(logits)\n            probs = exp_logits / np.sum(exp_logits)\n            d_log_pi = np.zeros_like(probs)\n            d_log_pi[a] = 1 - probs[a]\n            gradient[s] += returns[t] * d_log_pi\n    average_gradient = gradient / len(episodes)\n    return np.round(average_gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 7, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros((num_states, num_actions))\n    for episode in episodes:\n        t = 0\n        for (state, action, reward) in episode:\n            G = sum((reward for (_, _, reward) in episode[t:]))\n            exp_theta = np.exp(theta[state, :])\n            pi = exp_theta / np.sum(exp_theta)\n            grad_log_pi = np.zeros(num_actions)\n            grad_log_pi[action] = 1 - pi[action]\n            for a in range(num_actions):\n                if a != action:\n                    grad_log_pi[a] = -pi[a]\n            gradient[state] += G * grad_log_pi\n            t += 1\n    total_steps = sum((len(episode) for episode in episodes))\n    gradient /= total_steps\n    return np.round(gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 8, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef compute_policy_gradient(theta: np.ndarray, episodes: List[List[Tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        states = [s for (s, a, r) in episode]\n        actions = [a for (s, a, r) in episode]\n        rewards = [r for (s, a, r) in episode]\n        returns = [0.0] * len(rewards)\n        cumulative_return = 0.0\n        for t in reversed(range(len(rewards))):\n            cumulative_return += rewards[t]\n            returns[t] = cumulative_return\n        for t in range(len(returns)):\n            s = states[t]\n            a = actions[t]\n            R_t = returns[t]\n            exp_theta = np.exp(theta[s])\n            policy = exp_theta / np.sum(exp_theta)\n            log_policy_gradient = np.zeros(num_actions)\n            log_policy_gradient[a] = 1 - policy[a]\n            for i in range(num_actions):\n                if i != a:\n                    log_policy_gradient[i] = -policy[i]\n            gradient[s] += R_t * log_policy_gradient\n    avg_gradient = gradient / sum((len(episode) for episode in episodes))\n    return np.round(avg_gradient, 4).tolist()"}
{"task_id": 122, "completion_id": 9, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each row in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = []\n        rewards = [reward for (_, _, reward) in episode]\n        G = 0\n        for r in reversed(rewards):\n            G = r + G\n            returns.append(G)\n        returns.reverse()\n        for t in range(len(episode)):\n            (s, a, _) = episode[t]\n            G_t = returns[t]\n            pi_s = softmax(theta)\n            pi_s_a = pi_s[s, a]\n            grad_log_pi = np.zeros((num_states, num_actions))\n            for action in range(num_actions):\n                grad_log_pi[s, action] = (1 if action == a else 0) - pi_s[s, action]\n            gradient += grad_log_pi * G_t\n    gradient_avg = gradient / len(episode) if episodes else gradient\n    return np.round(gradient_avg, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_per_expert = d_in * d_out\n    flops_dense = flops_per_expert\n    flops_moe = k_active * flops_per_expert\n    flops_all_experts = n_experts * flops_per_expert\n    saving_percentage = (flops_all_experts - flops_moe) / flops_all_experts * 100\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    saving_percentage = round(saving_percentage, 1)\n    return (flops_dense, flops_moe, saving_percentage)"}
{"task_id": 123, "completion_id": 1, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * (2 * d_in * d_out) / n_experts\n    savings_flops = dense_flops - moe_flops\n    savings_percentage = savings_flops / dense_flops * 100\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return {'dense_flops': dense_flops, 'moe_flops': moe_flops, 'savings_percentage': savings_percentage}"}
{"task_id": 123, "completion_id": 2, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100 if flops_dense != 0 else 0\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return (flops_dense, flops_moe, savings_percentage)"}
{"task_id": 123, "completion_id": 3, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = d_in * d_out\n    moe_flops = dense_flops * (k_active / n_experts)\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings = round(savings, 1)\n    return (dense_flops, moe_flops, savings)"}
{"task_id": 123, "completion_id": 4, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * dense_flops\n    savings_flops = dense_flops * n_experts - moe_flops\n    savings_percentage = savings_flops / (dense_flops * n_experts) * 100\n    moe_flops = round(moe_flops, 1)\n    dense_flops_total = round(dense_flops * n_experts, 1)\n    savings_flops = round(savings_flops, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return {'dense_flops': dense_flops_total, 'moe_flops': moe_flops, 'savings_flops': savings_flops, 'savings_percentage': savings_percentage}"}
{"task_id": 123, "completion_id": 5, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost efficiency of a MoE layer compared to a dense layer.\n    \n    Parameters:\n        n_experts (int): Number of experts in the MoE layer.\n        k_active (int): Number of active experts for a given input.\n        d_in (int): Input dimension.\n        d_out (int): Output dimension.\n    \n    Returns:\n        dict: A dictionary containing:\n            - 'dense_flops': FLOPs for a dense layer\n            - 'moe_flops': FLOPs for the MoE layer\n            - 'saving_percentage': Relative cost savings (%)\n    \"\"\"\n    dense_flops = d_in * d_out\n    moe_flops = k_active * d_in * d_out\n    if dense_flops == 0:\n        saving_percentage = 0.0\n    else:\n        saving_percentage = (1 - moe_flops / dense_flops) * 100\n    result = {'dense_flops': round(dense_flops, 1), 'moe_flops': round(moe_flops, 1), 'saving_percentage': round(saving_percentage, 1)}\n    return result"}
{"task_id": 123, "completion_id": 6, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * 2 * d_in * d_out / n_experts\n    savings_percent = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_percent = round(savings_percent, 1)\n    return {'flops_dense': flops_dense, 'flops_moe': flops_moe, 'savings_percent': savings_percent}"}
{"task_id": 123, "completion_id": 7, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * 2 * d_in * d_out\n    if flops_dense == 0:\n        savings_percent = 0.0\n    else:\n        savings_percent = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percent_rounded = round(savings_percent, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percent_rounded)"}
{"task_id": 123, "completion_id": 8, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational efficiency of a Mixture-of-Experts (MoE) layer \n    compared to a dense layer.\n\n    Parameters:\n    - n_experts (int): Total number of experts\n    - k_active (int): Number of active experts\n    - d_in (int): Input dimension\n    - d_out (int): Output dimension\n\n    Returns:\n    - efficiency_percent (float): Percentage of FLOPs saved by using MoE\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * 2 * d_in * d_out\n    flops_savings = flops_dense - flops_moe\n    efficiency_percent = flops_savings / flops_dense * 100 if flops_dense != 0 else 0\n    return round(efficiency_percent, 1)"}
{"task_id": 123, "completion_id": 9, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an Mixture-of-Experts (MoE)\n    layer compared to a dense layer.\n\n    Parameters:\n    - n_experts: Total number of experts in the MoE model.\n    - k_active: Number of active (used) experts per example (sparsity).\n    - d_in: Input dimension.\n    - d_out: Output dimension.\n\n    Returns:\n    - A tuple (dense_flops, moe_flops, savings_percent)\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = 2 * k_active * (d_in * d_out)\n    savings = 1 - moe_flops / dense_flops\n    savings_percent = savings * 100\n    dense_flops = round(dense_flops, 1)\n    moe_flops = round(moe_flops, 1)\n    savings_percent = round(savings_percent, 1)\n    return (dense_flops, moe_flops, savings_percent)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating function used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    - X (np.ndarray): Input matrix of shape (n_samples, in_features)\n    - W_g (np.ndarray): Gating (logits) weight matrix of shape (in_features, n_experts)\n    - W_noise (np.ndarray): Noise network weight matrix of shape (in_features, n_experts)\n    - N (np.ndarray): Pre-sampled noise matrix of shape (n_samples, n_experts)\n    - k (int): Sparsity constraint (top k experts to be selected per input)\n\n    Returns:\n    - probabilities (list): Final probabilities with top-k gating applied\n    \"\"\"\n    gate_scores = X @ W_g\n    noise = X @ W_noise\n    noisy_scores = gate_scores + noise * N\n    (topk_values, topk_indices) = map(np.ravel, np.topk(noisy_scores, k=k, axis=1))\n    (n_samples, n_experts) = noisy_scores.shape\n    gate_mask = np.zeros_like(noisy_scores)\n    gate_mask[np.arange(n_samples)[:, None], topk_indices] = 1.0\n    probabilities = gate_mask * np.exp(gate_scores) / np.exp(gate_scores).sum(axis=1, keepdims=True)\n    probabilities = np.round(probabilities, 4)\n    return probabilities.tolist()\nX = np.array([[1, 2], [3, 4]])\nW_g = np.array([[0.1, 0.2], [0.3, 0.4]])\nW_noise = np.array([[0.01, 0.02], [0.03, 0.04]])\nN = np.array([[1.0, -1.0], [-1.0, 1.0]])\nk = 1"}
{"task_id": 124, "completion_id": 1, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the final gating probabilities via the Noisy Top-K Gating mechanism.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n        W_g (np.ndarray): Gating weight matrix of shape (input_dim, num_experts).\n        W_noise (np.ndarray): Noise weight matrix of shape (input_dim, num_Experts).\n        N (np.ndarray): Pre-sampled noise values of shape (batch_size, num_Experts).\n        k (int): Top-K sparsity to be applied.\n\n    Returns:\n        list: Final gating probabilities after top-k masking, as Python list.\n    \"\"\"\n    (batch_size, _) = X.shape\n    num_experts = W_g.shape[1]\n    logits = np.reshape(X, (batch_size, 1, -1)) @ np.reshape(W_g, (1, num_experts, -1))\n    logits += np.sqrt(0.5) * np.reshape(X, (batch_size, 1, -1)) @ np.reshape(W_noise, (1, num_experts, -1)) + np.sqrt(0.5) * N\n    logits = np.reshape(logits, (batch_size, num_experts))\n    logits -= np.max(logits, axis=1, keepdims=True)\n    logits = np.exp(logits)\n    soft_vals = logits / (np.sum(logits, axis=1, keepdims=True) + 1e-20)\n    top_k_groups = batch_size * num_experts // k\n    top_k_vals = np.zeros((batch_size, top_k_groups), dtype=np.float32)\n    top_k_indices = np.zeros((batch_size, top_k_groups), dtype=np.int32)\n    for i in range(batch_size):\n        top_k_numbers = np.argpartition(soft_vals[i], -k)[-k:]\n        top_k_numbers.sort()\n        top_k_vals[i, :k] = soft_vals[i, top_k_numbers]\n        top_k_indices[i, :k] = top_k_numbers\n    top_k_vals /= top_k_vals.sum(axis=1, keepdims=True)\n    gates = np.zeros((batch_size, num_experts))\n    for i in range(batch_size):\n        gates[i, top_k_indices[i, :k]] = top_k_vals[i, :k]\n    gates = np.round(gates, decimals=4)\n    return gates.tolist()"}
{"task_id": 124, "completion_id": 2, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Perform Noisy Top-k Gating for Mixture of Experts.\n    \n    Parameters:\n    - X: Input matrix of shape (batch_size, input_dim)\n    - W_g: Gating weight matrix of shape (input_dim, num_experts)\n    - W_noise: Noise weight matrix of shape (input_dim, num_Experts)\n    - N: Noise tensor of shape (batch_size, num_Experts)\n    - k: Sparsity constraint (top-k fraction of experts to activate)\n\n    Returns:\n    - gate_output: List of final gating probabilities of shape (batch_size, num_Experts)\n    \"\"\"\n    (batch_size, input_dim) = X.shape\n    (_, num_experts) = W_g.shape\n    gates_logits = X @ W_g\n    noise_logits = X @ W_noise + N\n    topk_idx = np.argpartition(gates_logits, -k, axis=1)[:, -k:]\n    topk_idx_sorted = np.argsort(-gates_logits[np.arange(batch_size)[:, None], topk_idx], axis=1)\n    topk_idx = topk_idx[np.arange(batch_size)[:, None], topk_idx_sorted]\n    topk_gates = gates_logits[np.arange(batch_size)[:, None], topk_idx]\n    other_gates = gates_logits - (1 << 10)\n    other_gates = np.where(np.expand_dims(np.arange(num_experts), 0) == topk_idx[:, None, :], topk_gates, other_gates)\n    other_gates -= other_gates.max(axis=1, keepdims=True)\n    exponential = np.exp(other_gates)\n    row_sums = exponential.sum(axis=1, keepdims=True)\n    gate_output = np.where(row_sums == 0, 0, exponential / row_sums)\n    gate_output_rounded = np.round(gate_output, 4)\n    return gate_output_rounded.tolist()"}
{"task_id": 124, "completion_id": 3, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating Function used in Mixture-of-Experts.\n    \n    Parameters:\n        X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n        W_g (np.ndarray): Weight matrix for the gating network of shape (input_dim, num_experts).\n        W_noise (np.ndarray): Weight matrix for the noise network of shape (input_dim, num_Experts).\n        N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_Experts).\n        k (int): Sparsity parameter; top-k experts will be used per batch element.\n    \n    Returns:\n        A python list containing the final gating probabilities matrix,\n        where only the top-k probabilities are non-zero per batch element.\n    \"\"\"\n    (batch_size, input_dim) = X.shape\n    num_experts = W_g.shape[-1]\n    g_logits = np.matmul(X, W_g)\n    noise_logits = np.matmul(X, W_noise) + N\n    noise_logits = np.logaddexp(noise_logits, 0.0)\n    logits_noisy = g_logits + noise_logits\n    (top_logits, indices) = ([], [])\n    for i in range(batch_size):\n        topk_values = np.sort(logits_noisy[i])[-k:]\n        relevant_indices = np.argsort(logits_noisy[i])[-k:]\n        top_logits.append(topk_values)\n        indices.append(relevant_indices)\n    top_logits = np.array(top_logits)\n    indices = np.array(indices)\n    probs = np.exp(top_logits - np.logaddexp.reduce(top_logits, axis=1)[:, np.newaxis])\n    final_probs = np.zeros((batch_size, num_experts))\n    for i in range(batch_size):\n        final_probs[i, indices[i]] = probs[i, :]\n    return np.round(final_probs, 4).tolist()"}
{"task_id": 124, "completion_id": 4, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements Noisy Top-k Gating as used in Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    - X: input matrix of shape (n_samples, input_dim)\n    - W_g: expert gate weight matrix of shape (n_experts, input_dim)\n    - W_noise: learnable noise weight matrix of shape (n_experts, input_dim)\n    - N: pre-sampled noise of shape (n_samples, n_experts)\n    - k: sparsity constraint, number of experts to activate per sample\n    \n    Returns:\n    - G: A list of shape (n_samples, n_experts), final top-k normalized gate probabilities\n    \"\"\"\n    (n_samples, input_dim) = X.shape\n    n_experts = W_g.shape[0]\n    gate_logits = X @ W_g.T\n    noise_logits = N @ W_noise.T\n    noisy_logits = gate_logits + noise_logits\n    logits = np.log1p(np.exp(noisy_logits))\n    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n    denom = np.sum(exp_logits, axis=-1, keepdims=True) + 1e-06\n    gates = exp_logits / denom\n    topk_indices = np.argpartition(gates, -k, axis=1)[:, -k:]\n    topk_mask = np.zeros_like(gates)\n    topk_mask[np.arange(n_samples)[:, None], topk_indices] = 1\n    gates = gates * topk_mask\n    gates_sum = np.sum(gates, axis=-1, keepdims=True) + 1e-06\n    gates = gates / gates_sum\n    gates_rounded = np.round(gates, 4)\n    return gates_rounded.tolist()\nX = np.random.rand(32, 64)\nW_g = np.random.rand(16, 64)\nW_noise = np.random.rand(16, 64)\nN = np.random.randn(32, 16)\nk = 2\ngates = noisy_topk_gating(X, W_g, W_noise, N, k)"}
{"task_id": 124, "completion_id": 5, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Computes the Noisy Top-k Gating probabilities for MoE models.\n    \n    Parameters:\n    - X (np.ndarray): input data of shape (batch_size, features)\n    - W_g (np.ndarray): gating weight matrix of shape (features, num_experts)\n    - W_noise (np.ndarray): noise weight matrix of shape (features, num_Experts)\n    - N (np.ndarray): pre-sampled noise matrix of shape (batch_size, num_Experts)\n    - k (int): sparsity level (top-k experts will be used)\n    \n    Returns:\n    - python list of the final top-k probabilities matrix (after rounding and reshaping)\n    \"\"\"\n    (batch_size, num_features) = X.shape\n    num_Experts = W_g.shape[1]\n    logits = X @ W_g\n    noise_logits = X @ W_noise + N\n    logits += noise_logits\n    gates = np.softmax(logits, axis=1)\n    topk_indices = np.argsort(gates, axis=1)[:, -k:]\n    mask = np.zeros_like(gates)\n    batch_indices = np.arange(batch_size)[:, None]\n    mask[batch_indices, topk_indices] = 1.0\n    masked_gates = gates * mask\n    masked_gates = masked_gates / (masked_gates.sum(axis=1, keepdims=True) + 1e-08)\n    result = (masked_gates * 10000).round() / 10000\n    return result.tolist()"}
{"task_id": 124, "completion_id": 6, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating function used in Mixture-of-Experts models.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (batch_size, input_dim)\n        W_g (np.ndarray): Weight matrix for the gate of shape (input_dim, num_experts)\n        W_noise (np.ndarray): Weight matrix for the noise of shape (input_dim, num_Experts)\n        N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_Experts)\n        k (int): Sparsity constraint (fraction of experts to activate)\n\n    Returns:\n        list: Final gates as a list after applying softmax and rounding\n    \"\"\"\n    (batch_size, num_experts) = N.shape\n    gate_logits = X @ W_g\n    noisy_logits = gate_logits + N @ W_noise.T\n    exp_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    noisy_gates = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_k_idx = np.argsort(noisy_gates, axis=1)[:, -k:]\n    top_k_mask = np.zeros_like(noisy_gates)\n    np.put_along_axis(top_k_mask, top_k_idx, 1.0, axis=1)\n    final_gates = np.round(noisy_gates * top_k_mask, 4)\n    return final_gates.tolist()\nX = np.array([[1, 2], [3, 4]])\nW_g = np.array([[0.1, -0.2, 0.3], [0.4, -0.1, 0.5]])\nN = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nk = 2"}
{"task_id": 124, "completion_id": 7, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Computes the Noisy Top-k Gating mechanism for Mixture-of-Experts (MoE).\n    \n    Parameters:\n    - X: Input matrix of shape (n, d) where n is the number of examples and d the input dimension\n    - W_g: Weight matrix for gates of shape (d, m) where m is the number of experts\n    - W_noise: Weight matrix for noise of shape (d, m)\n    - N: Pre-sample noise matrix of shape (n, m)\n    - k: Number of top experts to activate per example (1 \u2264 k \u2264 m)\n    \n    Returns:\n    - gate_result: List of shape (n, m) with top-k gating probabilities, all values rounded to 4 decimals.\n    \"\"\"\n    (n, d) = X.shape\n    m = W_g.shape[1]\n    logits = X @ W_g + X @ W_noise * N\n    uniform_noise = np.random.uniform(-0.01, 0.01, logits.shape)\n    logits += uniform_noise\n    topk_indices = np.argpartition(logits, -k, axis=1)[:, -k:]\n    topk_logits = np.take_along_axis(logits, topk_indices, axis=1)\n    topk_softmax = np.zeros_like(logits)\n    for i in range(n):\n        masked_logits = logits[i].copy()\n        masked_logits[masked_logits < np.partition(logits[i], -k)[-k]] = -np.inf\n        masked_logits -= masked_logits.max()\n        masked_logits = np.exp(masked_logits)\n        masked_logits /= masked_logits.sum()\n        topk_softmax[i] = masked_logits\n    topk_softmax[topk_softmax >= 1] = 0\n    topk_softmax /= topk_softmax.sum(axis=1, keepdims=True)\n    gate_result = np.round(topk_softmax, 4).tolist()\n    return gate_result"}
{"task_id": 124, "completion_id": 8, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Perform Noisy Top-k Gating for Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    - X: Input matrix of shape (batch_size, input_dim)\n    - W_g: Gate weight matrix of shape (input_dim, num_experts)\n    - W_noise: Noise weight matrix of shape (input_dim, num_experts)\n    - N: Pre-sampled noise matrix of shape (batch_size, num_experts)\n    - k: Top-k sparsity level\n\n    Returns:\n    - g_hat: Final routing probabilities of shape (batch_size, num_experts), rounded to 4th decimal.\n    \"\"\"\n    (batch_size, input_dim) = X.shape\n    (_, num_experts) = W_g.shape\n    pre_g = X @ W_g + N * np.exp(X @ W_noise)\n    scores = np.softmax(pre_g, axis=1)\n    topk_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n    batch_idx = np.arange(batch_size).reshape(-1, 1)\n    topk_mask = np.zeros_like(scores)\n    topk_mask[batch_idx, topk_indices] = 1.0\n    topk_scores = scores * topk_mask\n    g_hat = topk_scores / (topk_scores.sum(axis=1, keepdims=True) + 1e-10)\n    return np.round(g_hat, decimals=4).tolist()\nX = np.array([[1.0, 0.5], [0.3, 0.8]])\nW_g = np.array([[1, 2, 3], [4, 5, 6]])\nW_noise = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nN = np.array([[0, 1, 2], [1, 0, 0]])\nk = 2"}
{"task_id": 124, "completion_id": 9, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-k Gating Function for MoE models.\n\n    Parameters:\n    - X: Input matrix (num_samples, input_dim)\n    - W_g: Weight matrix for the gating network (input_dim, num_experts)\n    - W_noise: Weight matrix for the noise projection (input_dim, num_experts)\n    - N: Noise matrix (num_samples, num_experts)\n    - k: Number of top experts to select (sparsity)\n\n    Returns:\n    - Final routing matrix (num_samples, num_experts), with values rounded to 4 decimals.\n    \"\"\"\n    (num_samples, num_experts) = (X.shape[0], W_g.shape[1])\n    gate_outputs = X @ W_g\n    noise_outputs = X @ W_noise + N\n    logits = gate_outputs + noise_outputs\n    noisy_gates = np.exp(logits - np.max(logits, axis=1, keepdims=True)) / np.sum(np.exp(logits - np.max(logits, axis=1, keepdims=True)), axis=1, keepdims=True)\n    top_k_indices = np.argsort(logits, axis=1)[:, -k:]\n    top_k_mask = np.zeros_like(logits)\n    top_k_mask[np.arange(num_samples)[:, None], top_k_indices] = 1\n    router = noisy_gates * top_k_mask\n    non_zero_counts = np.sum(top_k_mask, axis=1, keepdims=True)\n    router /= non_zero_counts + 1e-08\n    return router.round(4).tolist()\nX = np.array([[1.0, 0.5], [0.0, 0.25]])\nW_g = np.array([[0.5, 0.5], [0.25, 0.75]])\nW_noise = np.array([[0.1, 0.1], [0.15, 0.05]])\nN = np.array([[0.1, -0.1], [0.05, 0.05]])\nk = 1"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture of Experts layer with softmax gating and top-k routing.\n    \n    Parameters:\n    - x (np.ndarray): Input tensor of shape (n_tokens, input_dim)\n    - We (np.ndarray): Expert weights of shape (n_experts, output_dim, input_dim)\n    - Wg (np.ndarray): Gating weights of shape (n_tokens, n_experts)\n    - n_experts (int): Total number of experts\n    - top_k (int): Number of top experts to activate\n    Returns:\n    - List: Output of shape (n_tokens, output_dim), after rounding to 4 decimals\n    \"\"\"\n    unnormalized_gates = np.dot(x, Wg.T)\n    exp_gates = np.exp(unnormalized_gates - np.max(unnormalized_gates, axis=1, keepdims=True))\n    gate_softmax = exp_gates / np.sum(exp_gates, axis=1, keepdims=True)\n    top_k_indices = np.argsort(gate_softmax, axis=1)[:, -top_k:]\n    top_k_values = np.take_along_axis(gate_softmax, top_k_indices, axis=1)\n    top_k_weights = top_k_values / np.sum(top_k_values, axis=1, keepdims=True)\n    (n_tokens, output_dim) = We[0].shape\n    final_output = np.zeros((n_tokens, output_dim))\n    for i in range(n_tokens):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            weight = top_k_weights[i, j]\n            expert = We[expert_idx]\n            final_output[i] += weight * x[i] @ expert.T\n    final_output_rounded = np.round(final_output, 4)\n    return final_output_rounded.tolist()\nx = np.array([[1, 2], [3, 4]])\nWe = np.array([[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[-0.1, -0.2], [-0.3, -0.4]]])\ntop_k = 2"}
{"task_id": 125, "completion_id": 1, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Mixture of Experts (MoE) layer with top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weights of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weights of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to activate per token\n    \n    Returns:\n    - output: Final output after applying top-k experts as a python list\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        gate_logits = x[i] @ Wg\n        gate_probs = np.exp(gate_logits - np.max(gate_logits))\n        gate_probs /= np.sum(gate_probs)\n        top_k_indices = np.argsort(gate_probs)[-top_k:]\n        top_k_probs = gate_probs[top_k_indices]\n        top_k_probs /= np.sum(top_k_probs)\n        for (idx, expert_idx) in enumerate(top_k_indices):\n            expert_weight = We[expert_idx]\n            expert_output = x[i] @ expert_weight\n            output[i] += expert_output * top_k_probs[idx]\n    return np.round(output, 4).tolist()\nx = np.array([[1, 2, 3], [4, 5, 6]])\nWe = np.array([[[1, -1], [2, -2]], [[-1, 1], [-2, 2]], [[1, 1], [2, 2]]])\nWg = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\ntop_k = 2"}
{"task_id": 125, "completion_id": 2, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture of Experts (MoE) layer with top-k routing.\n\n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weights of shape (n_experts, expert_dim, input_dim)\n    - Wg: Gating weights of shape (input_dim, n_experts)\n    - n_experts: Total number of experts\n    - top_k: Number of experts to select per token\n\n    Returns:\n    - Output tensor of shape (batch_size, expert_dim), rounded to 4 decimal places\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    expert_dim = We.shape[1]\n    gate_scores = x @ Wg\n    top_k_indices = np.argsort(gate_scores, axis=1)[:, -top_k:]\n    top_k_scores = np.take_along_axis(gate_scores, top_k_indices, axis=1)\n    top_k_scores_softmax = np.exp(top_k_scores - np.max(top_k_scores, axis=1, keepdims=True))\n    top_k_scores_softmax /= np.sum(top_k_scores_softmax, axis=1, keepdims=True)\n    output = np.zeros((batch_size, expert_dim))\n    for i in range(batch_size):\n        for k in range(top_k):\n            expert_idx = top_k_indices[i, k]\n            weight = top_k_scores_softmax[i, k]\n            expert_output = x[i] @ We[expert_idx].T\n            output[i] += weight * expert_output\n    return np.round(output, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 3, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture-of-Experts (MoE) Layer with Softmax Gating and Top-k Routing.\n\n    Parameters:\n    - x: input tensor of shape (batch_size, input_dim)\n    - We: expert weight matrices of shape (n_experts, input_dim, output_dim)\n    - Wg: gating weight matrix of shape (input_dim, n_experts)\n    - n_experts: number of experts\n    - top_k: number of top experts to select per token\n\n    Returns:\n    - output: result of the MoE layer, shaped like (batch_size, output_dim)\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gate_scores = x @ Wg\n    gate_probs = np.exp(gate_scores - np.max(gate_scores, axis=1, keepdims=True))\n    gate_probs /= gate_probs.sum(axis=1, keepdims=True)\n    top_k_indices = np.argsort(gate_probs, axis=1)[:, -top_k:][:, ::-1]\n    top_k_weights = np.take_along_axis(gate_probs, top_k_indices, axis=1)\n    output = np.zeros((batch_size, output_dim), dtype=np.float64)\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            weight = top_k_weights[i, j]\n            expert_out = x[i] @ We[expert_idx]\n            output[i] += weight * expert_out\n    output /= top_k_weights.sum(axis=1, keepdims=True) + 1e-10\n    return np.round(output, 4).tolist()"}
{"task_id": 125, "completion_id": 4, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture-of-Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (n_tokens, input_dim)\n    - We: Expert weights of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weights of shape (input_dim, n_experts)\n    - n_experts: Total number of experts\n    - top_k: Number of top experts to select per token\n    \n    Returns:\n    - Output tensor of shape (n_tokens, output_dim), rounded to 4 decimal places\n    \"\"\"\n    (n_tokens, input_dim) = x.shape\n    output_dim = We.shape[2]\n    output = np.zeros((n_tokens, output_dim))\n    for i in range(n_tokens):\n        x_i = x[i]\n        gate_scores = x_i @ Wg\n        gate_probs = np.exp(gate_scores) / np.sum(np.exp(gate_scores))\n        top_k_indices = np.argsort(gate_probs)[-top_k:][::-1]\n        top_k_weights = gate_probs[top_k_indices]\n        top_k_weights /= np.sum(top_k_weights)\n        output_i = np.zeros(output_dim)\n        for (j, expert_idx) in enumerate(top_k_indices):\n            output_i += top_k_weights[j] * x_i @ We[expert_idx]\n        output[i] = output_i\n    return np.round(output, 4).tolist()\nx = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nWe = np.random.rand(3, 3, 2)\nWg = np.random.rand(3, 3)\ntop_k = 2"}
{"task_id": 125, "completion_id": 5, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture-of-Experts layer using softmax gating and top-k selection.\n\n    Parameters:\n    - x: Input tensor (shape: [batch_size, input_dim])\n    - We: Expert weight matrices (shape: [input_dim, output_dim, n_experts])\n    - Wg: Gating weight matrix (shape: [input_dim, n_experts])\n    - n_experts: Number of experts\n    - top_k: Number of top experts to select per token\n\n    Returns:\n    - Python list of the MoE output tensor (shape: [batch_size, output_dim])\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[1]\n    gate_logits = x @ Wg\n    gate_probs = np.exp(gate_logits - np.max(gate_logits, axis=1, keepdims=True))\n    gate_probs /= np.sum(gate_probs, axis=1, keepdims=True)\n    moe_output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        top_k_indices = np.argsort(gate_probs[i])[-top_k:][::-1]\n        top_k_probs = gate_probs[i, top_k_indices]\n        top_k_probs /= np.sum(top_k_probs)\n        token_output = np.zeros(output_dim)\n        for expert_idx in range(top_k):\n            expert_id = top_k_indices[expert_idx]\n            expert_weight = We[:, :, expert_id]\n            expert_output = x[i] @ expert_weight\n            token_output += top_k_probs[expert_idx] * expert_output\n        moe_output[i] = token_output\n    return np.round(moe_output, 4).tolist()\nx = np.random.randn(2, 3)\nWe = np.random.randn(3, 2, 4)\nWg = np.random.randn(3, 4)\ntop_k = 2"}
{"task_id": 125, "completion_id": 6, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Mixture-of-Experts layer.\n\n    Parameters:\n    x (np.ndarray): Input tensor of shape (n_input, d)\n    We (np.ndarray): Expert weights of shape (n_experts, d_output, d)\n    Wg (np.ndarray): Gating weights of shape (n_experts, d)\n    n_experts (int): Number of experts\n    top_k (int): Number of top experts to select per token\n\n    Returns:\n    list: Final output of the MoE layer after processing, rounded and converted to list\n    \"\"\"\n    (n_input, d) = x.shape\n    d_output = We.shape[1]\n    gate_scores = x @ Wg.T\n    gate_probs = np_softmax(gate_scores, axis=1)\n    top_k_indices = []\n    top_k_weights = []\n    for i in range(n_input):\n        top_k_idx = np.argpartition(gate_probs[i], -top_k)[-top_k:]\n        top_k_mask = np.isin(np.arange(n_experts), top_k_idx)\n        top_k_weights.append(gate_probs[i, top_k_mask])\n        top_k_indices.append(top_k_idx)\n    y = np.zeros((n_input, d_output))\n    for i in range(n_input):\n        expert_indices = top_k_indices[i]\n        weights = top_k_weights[i]\n        weights /= weights.sum()\n        for (j, idx) in enumerate(expert_indices):\n            expert_weights = We[idx, :, :]\n            y[i] += weights[j] * x[i] @ expert_weights.T\n    result = y.round(4).tolist()\n    return result\ndef np_softmax(x, axis=0):\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)"}
{"task_id": 125, "completion_id": 7, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Mixture of Experts (MoE) with top-k routing using softmax gating.\n    \n    Parameters:\n        x (np.ndarray): Input tensor of shape (batch_size, input_dim)\n        We (np.ndarray): Expert weights of shape (n_experts, input_dim, output_dim)\n        Wg (np.ndarray): Gating weights of shape (input_dim, n_experts)\n        n_experts (int): Number of experts\n        top_k (int): Number of top experts to select per token\n    \n    Returns:\n        list: Final MoE output weighted by normalized gating probabilities, reshaped and rounded to 4 decimals\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gate_scores = x @ Wg\n    gate_weights = np.softmax(gate_scores, axis=1)\n    top_k_indices = np.argsort(-gate_weights, axis=1)[:, :top_k]\n    top_k_weights = np.take_along_axis(gate_weights, top_k_indices, axis=1)\n    output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        for k in range(top_k):\n            expert_idx = top_k_indices[i, k]\n            weight = top_k_weights[i, k]\n            expert_output = x[i] @ We[expert_idx]\n            output[i] += weight * expert_output\n    return np.round(output, 4).tolist()"}
{"task_id": 125, "completion_id": 8, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture of Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weights of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weights of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts selected per token\n    \n    Returns:\n    - A list of the final MoE output, rounded to 4 decimal places and converted to a Python list.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    (input_dim_we, output_dim) = We[0].shape\n    gate_logits = x @ Wg\n    exp_logits = np.exp(gate_logits - np.max(gate_logits, axis=1, keepdims=True))\n    gate_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n    top_k_indices = np.argsort(gate_probs, axis=1)[:, -top_k:]\n    top_k_weights = np.take_along_axis(gate_probs, top_k_indices, axis=1)\n    top_k_weights /= np.sum(top_k_weights, axis=1, keepdims=True)\n    output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        for k in range(top_k):\n            expert_idx = top_k_indices[i, k]\n            weight = top_k_weights[i, k]\n            expert_output = x[i] @ We[expert_idx]\n            output[i] += weight * expert_output\n    return np.round(output, 4).tolist()"}
{"task_id": 125, "completion_id": 9, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    - x: Input tensor with shape (batch_size, input_dim)\n    - We: Expert weight matrix with shape (n_experts, hidden_dim, input_dim)\n    - Wg: Gating weight matrix with shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to select per token\n    \n    Returns:\n    - output: Output after applying top-k selection and aggregation, returned as a python list\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    n_experts = We.shape[0]\n    gate_scores = x @ Wg\n    top_k_indices = np.argsort(gate_scores, axis=1)[:, -top_k:]\n    top_k_values = np.take_along_axis(gate_scores, top_k_indices, axis=1)\n    exp_top_k = np.exp(top_k_values - top_k_values.max(axis=1, keepdims=True))\n    top_k_probs = exp_top_k / exp_top_k.sum(axis=1, keepdims=True)\n    expert_weights = We[top_k_indices]\n    outputs = np.einsum('bki...,bki->b...', expert_weights, x[:, np.newaxis, :, np.newaxis])\n    final_output = (outputs * top_k_probs[..., np.newaxis]).sum(axis=1)\n    output = np.round(final_output, 4).tolist()\n    return output"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Apply Group Normalization to a 4D input tensor of shape (B, C, H, W).\n    \n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Scale parameters of shape (C,)\n    - beta: Offset parameters of shape (C,)\n    - num_groups: Number of groups to divide the channels into\n    - epsilon: Small value to avoid division by zero\n    \n    Returns:\n    - Normalized output tensor as a Python list (rounded to 4 decimals)\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of groups must divide the number of channels'\n    X_grouped = X.reshape((B, num_groups, C // num_groups, H, W))\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape((B, C, H, W))\n    gamma = gamma.reshape((1, C, 1, 1))\n    beta = beta.reshape((1, C, 1, 1))\n    X_output = X_normalized * gamma + beta\n    return np.round(X_output, 4).tolist()"}
{"task_id": 126, "completion_id": 1, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D tensor X of shape (B, C, H, W).\n\n    Args:\n        X (np.ndarray): Input tensor of shape (B, C, H, W)\n        gamma (np.ndarray): Scale parameter of shape (C,)\n        beta (np.ndarray): Shift parameter of shape (C,)\n        num_groups (int): Number of groups to divide the channels into\n        epsilon (float): A small value for numerical stability\n\n    Returns:\n        python list: Normalized output as a flattened list rounded to 4 decimals\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups.'\n    group_size = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, group_size, H, W)\n    X_grouped = X_reshaped\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    X_scaled_shifted = X_normalized * gamma + beta\n    result = np.round(X_scaled_shifted, decimals=4).tolist()\n    return result"}
{"task_id": 126, "completion_id": 2, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor with shape (B, C, H, W).\n\n    Args:\n        X (np.ndarray): Input tensor with shape (B, C, H, W).\n        gamma (np.ndarray): Scale parameter with shape (C,).\n        beta (np.ndarray): Shift parameter with shape (C,).\n        num_groups (int): Number of groups to divide the channels into.\n        epsilon (float): Small value added to the denominator for numerical stability.\n\n    Returns:\n        list: Normalized output as a Python list, rounded to the 4th decimal.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    X_grouped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    X_output = X_normalized * gamma + beta\n    return np.round(X_output, 4).tolist()"}
{"task_id": 126, "completion_id": 3, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D tensor.\n\n    Parameters:\n    X (np.ndarray): Input tensor of shape (B, C, H, W)\n    gamma (np.ndarray): Scale parameter of shape (C,)\n    beta (np.ndarray): Shift parameter of shape (C,)\n    num_groups (int): Number of groups to divide channels into\n    epsilon (float): Small value added to variance to avoid division by zero\n\n    Returns:\n    list: Normalized and transformed tensor as a Python list, rounded to 4 decimals\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of groups must divide the number of channels'\n    X_group = X.reshape(B, num_groups, -1, H, W)\n    X_group = X_group.reshape(B, num_groups, -1)\n    mean = X_group.mean(axis=2, keepdims=True)\n    var = X_group.var(axis=2, keepdims=True)\n    X_normalized = (X_group - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, num_groups, C // num_groups, H, W)\n    X_normalized = X_normalized.reshape(X.shape)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    output = gamma * X_normalized + beta\n    return np.around(output, decimals=4).tolist()"}
{"task_id": 126, "completion_id": 4, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameters of shape (C,)\n    - beta: Learnable shift parameters of shape (C,)\n    - num_groups: Number of groups to divide the channels into.\n    - epsilon: Small value to avoid division by zero.\n\n    Returns:\n    - Normalized tensor (as a python list after reshaping and rounding).\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    X = X.reshape(B, num_groups, -1)\n    X = X.astype(np.float64)\n    mean = X.mean(axis=(0, 2), keepdims=True)\n    var = X.var(axis=(0, 2), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    Y = gamma * X_normalized + beta\n    return np.round(Y, decimals=4).tolist()"}
{"task_id": 126, "completion_id": 5, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_reshaped = X.reshape((B, num_groups, C // num_groups, H, W))\n    mean = np.mean(X_reshaped, axis=(2, 3, 4))\n    variance = np.var(X_reshaped, axis=(2, 3, 4))\n    mean = np.expand_dims(np.expand_dims(np.expand_dims(mean, axis=2), axis=3), axis=4)\n    variance = np.expand_dims(np.expand_dims(np.expand_dims(variance, axis=2), axis=3), axis=4)\n    X_normalized = (X_reshaped - mean) / np.sqrt(variance + epsilon)\n    X_normalized = X_normalized.reshape(X.shape)\n    X_output = gamma * X_normalized + beta\n    return np.round(X_output, 4).tolist()"}
{"task_id": 126, "completion_id": 6, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor (B, C, H, W).\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameter of shape (C,)\n    - beta: Learnable shift parameter of shape (C,)\n    - num_groups: Number of groups to divide channels into\n    - epsilon: Small value to avoid division by zero\n\n    Returns:\n    - Output tensor of shape (B, C, H, W), normalized and scaled/shifted.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    output = gamma * X_normalized + beta\n    return np.round(output, 4).tolist()\nX = np.random.randn(2, 8, 4, 4)\ngamma = np.random.rand(8)\nbeta = np.random.rand(8)\nnum_groups = 4\noutput = group_normalization(X, gamma, beta, num_groups)"}
{"task_id": 126, "completion_id": 7, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Apply Group Normalization to a 4D input tensor.\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameter of shape (C,)\n    - beta: Learnable shift parameter of shape (C,)\n    - num_groups: Number of groups to divide the channels into\n    - epsilon: Small value to prevent division by zero\n\n    Returns:\n    - Y: Normalized tensor, returned as a Python list after rounding and tolist()\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, f'Number of groups {num_groups} must divide number of channels {C}'\n    X_grouped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_group_norm = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_group_norm.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    Y = gamma * X_norm + beta\n    Y = np.round(Y, 4)\n    Y = Y.tolist()\n    return Y"}
{"task_id": 126, "completion_id": 8, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor with shape (B, C, H, W).\n    \n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scaling parameter of shape (C,)\n    - beta: Learnable shifting parameter of shape (C,)\n    - num_groups: Number of channel groups to split into\n    - epsilon: Small value to avoid division by zero\n    \n    Returns:\n    - A list of normalized output with shape (B, C, H, W), rounded to 4 decimals\n    \"\"\"\n    (B, C, H, W) = X.shape\n    group_size = C // num_groups\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_output = gamma * X_normalized + beta\n    return np.round(X_output, 4).tolist()"}
{"task_id": 126, "completion_id": 9, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (B, C, H, W)\n    - gamma (np.ndarray): Scale parameter of shape (C,)\n    - beta (np.ndarray): Shift parameter of shape (C,)\n    - num_groups (int): Number of groups to split channels into\n    - epsilon (float): Small value to avoid division by zero\n\n    Returns:\n    - Y (np.ndarray): Normalized output tensor of shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by the number of groups.')\n    C_per_group = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, C_per_group, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    Y = gamma * X_normalized + beta\n    Y_rounded = np.round(Y, decimals=4)\n    return Y_rounded.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for iter in range(max_iters):\n        grad = df(x)\n        new_x = x - learning_rate * grad\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 1, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        grad = df(x)\n        if abs(grad) < tolerance:\n            break\n        x -= learning_rate * grad\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 2, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    current_x = start_x\n    for _ in range(max_iters):\n        grad = df(current_x)\n        next_x = current_x - learning_rate * grad\n        if abs(next_x - current_x) < tolerance:\n            break\n        current_x = next_x\n    return round(current_x, 4)"}
{"task_id": 127, "completion_id": 3, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def derivative(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = derivative(x)\n        if abs(gradient) < tolerance:\n            break\n        x = x - learning_rate * gradient\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 4, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * derivative\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 5, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        grad = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * grad\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 6, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def f_prime(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        grad = f_prime(x)\n        x_new = x - learning_rate * grad\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 7, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 8, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum using gradient descent.\n\n    Parameters:\n        start_x (float): Starting point for the search.\n        learning_rate (float): Step size used in gradient descent.\n        tolerance (float): Stopping condition for small changes in x.\n        max_iters (int): Maximum number of iterations to perform.\n\n    Returns:\n        float: The x value at the minimum of the function, rounded to 4 decimal places.\n    \"\"\"\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        next_x = x - learning_rate * gradient\n        if abs(next_x - x) < tolerance:\n            break\n        x = next_x\n    return round(x, 4)"}
{"task_id": 127, "completion_id": 9, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Uses gradient descent to find the x that minimizes f(x) = x^4 - 3x^3 + 2\n    \n    Args:\n        start_x (float): Initial guess for x\n        learning_rate (float): Step size for updates\n        tolerance (float): Stop when step size is smaller than this\n        max_iters (int): Maximum number of iterations\n        \n    Returns:\n        float: x where f(x) reaches minimum (rounded to 4 decimals)\n    \"\"\"\n    x = start_x\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    for i in range(max_iters):\n        grad = df(x)\n        step = -learning_rate * grad\n        x_next = x + step\n        if abs(step) < tolerance:\n            break\n        x = x_next\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function for normalization-free transformations.\n    \n    Parameters:\n        x (np.ndarray): Input tensor (can be any dimension).\n        alpha (float): Scaling factor for dynamic adjustment.\n        gamma (float): Scaling parameter for the output.\n        beta (float): Offset parameter for the output.\n        \n    Returns:\n        list[float]: Resulting tensor as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    dynamic_range = np.max(np.abs(x), axis=-1, keepdims=True)\n    x_normalized = x / (dynamic_range + 1e-08)\n    tanh_output = np.tanh(x_normalized)\n    dyt_output = gamma * tanh_output * alpha + beta\n    return np.round(dyt_output, 4).tolist()"}
{"task_id": 128, "completion_id": 1, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mean = np.mean(x)\n    std = np.std(x)\n    x_normalized = (x - mean) / (std + 1e-07)\n    dyt_output = alpha * np.tanh(gamma * x_normalized + beta)\n    return np.round(dyt_output, 4).tolist()"}
{"task_id": 128, "completion_id": 2, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    scaled_x = gamma * x + beta\n    dyt_output = np.tanh(alpha * scaled_x)\n    dyt_output = np.round(dyt_output, 4)\n    return dyt_output.tolist()\nx = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nalpha = 1.2\ngamma = 0.8\nbeta = 0.5"}
{"task_id": 128, "completion_id": 3, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mean = np.mean(x)\n    std = np.std(x)\n    x_rescaled = (x - mean) / (std + 1e-08)\n    dyt_output = alpha * np.tanh(gamma * x_rescaled) + beta * x_rescaled\n    dyt_output_rounded = np.round(dyt_output, 4)\n    return dyt_output_rounded.tolist()"}
{"task_id": 128, "completion_id": 4, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function: A normalization-free activation function designed for Transformers.\n    \n    Parameters:\n        x (np.ndarray): Input array.\n        alpha (float): A parameter controlling the \"slope\" or steepness of the function.\n        gamma (float): A parameter adjusting the behavior or shape of the function.\n        beta (float): A shift parameter for output dynamics.\n    \n    Returns:\n        list[float]: Output after applying the Dynamic Tanh function, rounded and reshaped.\n    \"\"\"\n    norm = alpha * np.tanh(gamma * x) + beta\n    norm_rounded = np.round(norm, 4)\n    return norm_rounded.tolist()"}
{"task_id": 128, "completion_id": 5, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    x_scaled = gamma * ((x - mean) / (std + 1e-07)) + beta\n    dyt_output = np.tanh(alpha * x_scaled)\n    rounded_output = np.round(dyt_output, 4)\n    return rounded_output.tolist()"}
{"task_id": 128, "completion_id": 6, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    numerator = alpha * np.tanh(gamma * x + beta)\n    denominator = np.sqrt((1 + np.power(numerator, 2)) / 2)\n    output = numerator / denominator\n    return np.around(output, decimals=4).flatten().tolist()\nx = np.array([[1.0, 2.0], [3.0, 4.0]])\nalpha = 1.2\ngamma = 0.8\nbeta = 0.3"}
{"task_id": 128, "completion_id": 7, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function that replaces layer normalization in Transformers.\n    \n    Args:\n        x (np.ndarray): Input array of shape (batch_size, features).\n        alpha (float): Scaling factor for the hyperbolic tangent.\n        gamma (float): Scale parameter.\n        beta (float): Shift parameter.\n    \n    Returns:\n        list[float]: DyT output in Python list format, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(x, axis=0, keepdims=True)\n    var = np.var(x, axis=0, keepdims=True)\n    z = gamma * (x - mean) / np.sqrt(var + 1e-05)\n    dyt_output = alpha * np.tanh(z) + beta\n    dyt_output = np.round(dyt_output, 4)\n    return dyt_output.tolist()"}
{"task_id": 128, "completion_id": 8, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) activation function.\n\n    Parameters:\n    - x: Input array (np.ndarray)\n    - alpha: Scaling factor (float)\n    - gamma: Scaling factor for adaptive tanh width (float)\n    - beta: Bias offset (float)\n\n    Returns:\n    - List of floats representing the Dynamic Tanh output, rounded to 4 decimal places.\n    \"\"\"\n    input_norm = np.max(np.abs(x), axis=1, keepdims=True)\n    adaptive_scale = gamma / (input_norm + 1e-06)\n    dyt_output = alpha * np.tanh(adaptive_scale * x - beta)\n    dyt_output_rounded = np.round(dyt_output, decimals=4)\n    dyt_output_list = dyt_output_rounded.tolist()\n    return dyt_output_list"}
{"task_id": 128, "completion_id": 9, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) activation function inspired by the Tanh function.\n    It dynamically scales and squashes inputs, eliminating the need for normalization layers.\n    \n    Inputs:\n    - x: input numpy array of any shape\n    - alpha: scaling parameter (non-negative)\n    - gamma: dynamic scaling factor\n    - beta: dynamic shift parameter\n\n    Returns:\n    - A list of values after applying Dynamic Tanh, rounded to 4 decimal places.\n    \"\"\"\n    dyt_output = np.tanh(alpha * (gamma * x + beta))\n    rounded_output = np.round(dyt_output, 4)\n    return rounded_output.tolist()"}
