{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 4, "completion_id": 0, "solution": ""}
{"task_id": 5, "completion_id": 0, "solution": ""}
{"task_id": 6, "completion_id": 0, "solution": ""}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 8, "completion_id": 0, "solution": ""}
{"task_id": 9, "completion_id": 0, "solution": ""}
{"task_id": 10, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": ""}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    U = np.eye(2)\n    V = np.eye(2)\n    B = A.T @ A\n\n    def jacobian_update(B):\n        b12 = B[0, 1]\n        b21 = B[1, 0]\n        theta = 0.5 * np.arctan2(2 * b12, B[0, 0] - B[1, 1])\n        J = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        return J\n    tolerance = 1e-10\n    max_iterations = 1000\n    iteration = 0\n    while iteration < max_iterations:\n        J = jacobian_update(B)\n        B_new = J.T @ B @ J\n        if np.abs(B_new[0, 1]) < tolerance and np.abs(B_new[1, 0]) < tolerance:\n            break\n        B = B_new\n        U = U @ J\n        iteration += 1\n    singular_values = np.sqrt(np.diag(B))\n    sort_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sort_indices]\n    U = U[:, sort_indices]\n    V = A @ np.linalg.inv(U) @ np.diag(1 / singular_values)\n    U = np.round(U, 4)\n    singular_values = np.round(singular_values, 4)\n    V = np.round(V, 4)\n    return (U, singular_values, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": ""}
{"task_id": 14, "completion_id": 0, "solution": ""}
{"task_id": 15, "completion_id": 0, "solution": ""}
{"task_id": 16, "completion_id": 0, "solution": ""}
{"task_id": 17, "completion_id": 0, "solution": ""}
{"task_id": 18, "completion_id": 0, "solution": ""}
{"task_id": 19, "completion_id": 0, "solution": ""}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 22, "completion_id": 0, "solution": ""}
{"task_id": 23, "completion_id": 0, "solution": ""}
{"task_id": 24, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 26, "completion_id": 0, "solution": ""}
{"task_id": 27, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 30, "completion_id": 0, "solution": ""}
{"task_id": 31, "completion_id": 0, "solution": ""}
{"task_id": 32, "completion_id": 0, "solution": ""}
{"task_id": 33, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 35, "completion_id": 0, "solution": ""}
{"task_id": 36, "completion_id": 0, "solution": ""}
{"task_id": 37, "completion_id": 0, "solution": ""}
{"task_id": 38, "completion_id": 0, "solution": ""}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 41, "completion_id": 0, "solution": ""}
{"task_id": 42, "completion_id": 0, "solution": ""}
{"task_id": 43, "completion_id": 0, "solution": ""}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 45, "completion_id": 0, "solution": ""}
{"task_id": 46, "completion_id": 0, "solution": ""}
{"task_id": 47, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 49, "completion_id": 0, "solution": ""}
{"task_id": 50, "completion_id": 0, "solution": ""}
{"task_id": 51, "completion_id": 0, "solution": ""}
{"task_id": 52, "completion_id": 0, "solution": ""}
{"task_id": 53, "completion_id": 0, "solution": ""}
{"task_id": 54, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 57, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 59, "completion_id": 0, "solution": ""}
{"task_id": 60, "completion_id": 0, "solution": ""}
{"task_id": 61, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 64, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": ""}
{"task_id": 66, "completion_id": 0, "solution": ""}
{"task_id": 67, "completion_id": 0, "solution": ""}
{"task_id": 68, "completion_id": 0, "solution": ""}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 71, "completion_id": 0, "solution": ""}
{"task_id": 72, "completion_id": 0, "solution": ""}
{"task_id": 73, "completion_id": 0, "solution": ""}
{"task_id": 74, "completion_id": 0, "solution": ""}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 78, "completion_id": 0, "solution": ""}
{"task_id": 79, "completion_id": 0, "solution": ""}
{"task_id": 80, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 83, "completion_id": 0, "solution": ""}
{"task_id": 84, "completion_id": 0, "solution": ""}
{"task_id": 85, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 87, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, d_model) = q.shape\n    d_k = d_model // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\ndef feed_forward(x, d_model, d_ff):\n    w1 = np.random.rand(d_model, d_ff)\n    b1 = np.random.rand(d_ff)\n    w2 = np.random.rand(d_ff, d_model)\n    b2 = np.random.rand(d_model)\n    return np.matmul(np.maximum(np.matmul(x, w1) + b1, 0), w2) + b2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    d_model = params['wte'].shape[1]\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]\n    input_ids = np.array(input_ids).reshape(1, -1)\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][input_ids]\n        position_ids = np.arange(input_ids.shape[1]).reshape(1, -1)\n        positional_embeddings = params['wpe'][position_ids]\n        x = token_embeddings + positional_embeddings\n        q = k = v = x\n        attn_output = multi_head_attention(q, k, v, n_head)\n        x = x + attn_output\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        ff_output = feed_forward(x, d_model, d_model * 4)\n        x = x + ff_output\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.matmul(x, params['wte'].T)\n        next_token_id = np.argmax(logits[:, -1, :], axis=-1)\n        input_ids = np.concatenate([input_ids, next_token_id.reshape(1, 1)], axis=1)\n        if input_ids.shape[1] > n_ctx:\n            input_ids = input_ids[:, 1:]\n    generated_text = encoder.decode(input_ids.flatten().tolist())\n    return generated_text\nprompt = 'hello'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)"}
{"task_id": 89, "completion_id": 0, "solution": ""}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 91, "completion_id": 0, "solution": ""}
{"task_id": 92, "completion_id": 0, "solution": ""}
{"task_id": 93, "completion_id": 0, "solution": ""}
{"task_id": 94, "completion_id": 0, "solution": ""}
{"task_id": 95, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 97, "completion_id": 0, "solution": ""}
{"task_id": 98, "completion_id": 0, "solution": ""}
{"task_id": 99, "completion_id": 0, "solution": ""}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": ""}
{"task_id": 102, "completion_id": 0, "solution": ""}
{"task_id": 103, "completion_id": 0, "solution": ""}
{"task_id": 104, "completion_id": 0, "solution": ""}
{"task_id": 105, "completion_id": 0, "solution": ""}
{"task_id": 106, "completion_id": 0, "solution": ""}
{"task_id": 107, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": ""}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 110, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 112, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 114, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 116, "completion_id": 0, "solution": ""}
{"task_id": 117, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": ""}
{"task_id": 119, "completion_id": 0, "solution": ""}
{"task_id": 120, "completion_id": 0, "solution": ""}
{"task_id": 121, "completion_id": 0, "solution": ""}
{"task_id": 122, "completion_id": 0, "solution": ""}
{"task_id": 123, "completion_id": 0, "solution": ""}
{"task_id": 124, "completion_id": 0, "solution": ""}
{"task_id": 125, "completion_id": 0, "solution": ""}
{"task_id": 126, "completion_id": 0, "solution": ""}
{"task_id": 127, "completion_id": 0, "solution": ""}
{"task_id": 128, "completion_id": 0, "solution": ""}
