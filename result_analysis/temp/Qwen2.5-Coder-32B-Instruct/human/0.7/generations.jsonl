{"task_id": 1, "completion_id": 0, "completion": "```\npython\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the matrix and vector dimensions are compatible\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result vector\n    result = []\n    \n    # Compute the dot product for each row of the matrix with the vector\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\n# matrix = [[1, 2, 3], [4, 5, 6]]\n# vector = [7, 8, 9]\n# print(matrix_dot_vector(matrix, vector))  # Output: [50, 122]\n```\n\nThis script defines a function `matrix_dot_vector` that takes a matrix `a` and a vector `b` as inputs and returns the dot product of the matrix and vector if the dimensions are compatible. If the dimensions are not compatible, it returns `-1`. The function iterates over each row of the matrix, computes the dot product with the vector, and appends the result to the result list."}
{"task_id": 2, "completion_id": 0, "completion": "```\npython\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if the matrix is empty\n    if not a or not a[0]:\n        return []\n    \n    # Use list comprehension to compute the transpose\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    \n    return transposed\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\ntransposed_matrix = transpose_matrix(matrix)\nprint(transposed_matrix)\n```\n\nThis script defines a function `transpose_matrix` that takes a matrix `a` as input and returns its transpose. The function uses list comprehension to iterate over the columns of the input matrix and construct the rows of the transposed matrix. An example usage is provided to demonstrate how the function works."}
{"task_id": 3, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input list to a numpy array\n    np_array = np.array(a)\n    \n    # Check if the total number of elements matches the new shape\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the numpy array to the new shape\n    reshaped_array = np_array.reshape(new_shape)\n    \n    # Convert the reshaped numpy array back to a list\n    return reshaped_array.tolist()\n\n# Example usage:\n# matrix = [[1, 2], [3, 4]]\n# new_shape = (4, 1)\n# print(reshape_matrix(matrix, new_shape))  # Output: [[1], [2], [3], [4]]\n```\n\nThis script defines a function `reshape_matrix` that takes a 2D list `a` and a tuple `new_shape` as inputs. It attempts to reshape the list into the specified shape using NumPy. If the reshape operation is not possible (i.e., the total number of elements does not match the product of the new shape dimensions), it returns an empty list. Otherwise, it returns the reshaped matrix as a list of lists."}
{"task_id": 4, "completion_id": 0, "completion": "```\npython\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        return [sum(row) / len(row) for row in matrix]\n    \n    elif mode == 'column':\n        # Calculate mean for each column\n        if not matrix:\n            return []\n        \n        num_columns = len(matrix[0])\n        column_sums = [0] * num_columns\n        num_rows = len(matrix)\n        \n        for row in matrix:\n            if len(row) != num_columns:\n                raise ValueError(\"All rows must have the same number of columns\")\n            for i in range(num_columns):\n                column_sums[i] += row[i]\n        \n        return [column_sum / num_rows for column_sum in column_sums]\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nprint(calculate_matrix_mean(matrix, 'row'))    # Output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix, 'column')) # Output: [4.0, 5.0, 6.0]\n```\n\nThis script defines a function `calculate_matrix_mean` that calculates the mean of a matrix either by row or by column, based on the specified mode. It includes error handling for invalid modes and mismatched row lengths. The example usage demonstrates how to use the function with a sample matrix."}
{"task_id": 5, "completion_id": 0, "completion": "```\npython\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list of integers or floats representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    # Create a new matrix to store the result\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in matrix:\n        # Multiply each element in the row by the scalar and add to the result matrix\n        result_row = [element * scalar for element in row]\n        result.append(result_row)\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 3\n    result = scalar_multiply(matrix, scalar)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nScalar:\", scalar)\n    print(\"\\nResulting Matrix:\")\n    for row in result:\n        print(row)\n```\nThis script defines a function `scalar_multiply` that takes a matrix and a scalar as inputs, multiplies each element of the matrix by the scalar, and returns the resulting matrix. The example usage demonstrates how to use this function with a sample matrix and scalar."}
{"task_id": 6, "completion_id": 0, "completion": "```\npython\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    # Ensure the matrix is 2x2\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"The input matrix must be a 2x2 matrix.\")\n    \n    # Extract elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the discriminant\n    discriminant = (a + d)**2 - 4 * (a * d - b * c)\n    \n    # Calculate the eigenvalues\n    eigenvalue1 = ((a + d) + discriminant**0.5) / 2\n    eigenvalue2 = ((a + d) - discriminant**0.5) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\n# matrix = [[2, 1], [1, 2]]\n# print(calculate_eigenvalues(matrix))  # Output: [3.0, 1.0]\n```\n\nThis script defines a function `calculate_eigenvalues` that computes the eigenvalues of a given 2x2 matrix and returns them sorted from highest to lowest. The function first checks if the input matrix is indeed 2x2, then calculates the eigenvalues using the characteristic equation of a 2x2 matrix and returns them in the desired order."}
{"task_id": 7, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays for easier matrix operations\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by calculating their determinants\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    \n    if det_T == 0 or det_S == 0:\n        return -1\n    \n    # Calculate the inverse of T and S\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    \n    # Perform the transformation T^{-1}AS\n    transformed_matrix = T_inv @ A @ S\n    \n    # Round the result to the nearest 4th decimal\n    transformed_matrix = np.round(transformed_matrix, 4)\n    \n    # Convert the result back to a list of lists\n    return transformed_matrix.tolist()\n\n# Example usage:\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[1, 0], [0, 1]]\n\nresult = transform_matrix(A, T, S)\nprint(result)\n```\n\nThis script defines a function `transform_matrix` that takes three matrices `A`, `T`, and `S`, checks if `T` and `S` are invertible, and then performs the transformation $T^{-1}AS$. The result is rounded to the nearest 4th decimal and returned as a list of lists. If `T` or `S` is not invertible, the function returns `-1`."}
{"task_id": 8, "completion_id": 0, "completion": "```\npython\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    determinant = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse\n    inverse = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    return inverse\n\n# Example usage:\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)\nprint(inverse)  # Output should be [[0.6, -0.7], [-0.2, 0.4]]\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```\npython\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]]:\n    # Get the number of rows and columns for matrices a and b\n    rows_a, cols_a = len(a), len(a[0])\n    rows_b, cols_b = len(b), len(b[0])\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# a = [[1, 2, 3], [4, 5, 6]]\n# b = [[7, 8], [9, 10], [11, 12]]\n# print(matrixmul(a, b))  # Output: [[58, 64], [139, 154]]\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```\npython\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    \n    # Convert the list of lists to a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Calculate the mean of each feature\n    means = np.mean(data, axis=1)\n    \n    # Center the data by subtracting the mean from each observation\n    centered_data = data - means[:, np.newaxis]\n    \n    # Calculate the covariance matrix\n    # Cov(X, Y) = (1 / (n - 1)) * sum((X_i - mean_X) * (Y_i - mean_Y))\n    n = data.shape[1]\n    covariance_matrix = (1 / (n - 1)) * np.dot(centered_data, centered_data.T)\n    \n    # Convert the result back to a list of lists\n    return covariance_matrix.tolist()\n\n# Test cases to verify the correctness of the implementation\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2x2 matrix\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    expected1 = [\n        [0.6666666666666666, 0.6666666666666666],\n        [0.6666666666666666, 0.6666666666666666]\n    ]\n    result1 = calculate_covariance_matrix(vectors1)\n    assert np.allclose(result1, expected1), f\"Test case 1 failed: {result1} != {expected1}\"\n\n    # Test case 2: 3x3 matrix\n    vectors2 = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12]\n    ]\n    expected2 = [\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25]\n    ]\n    result2 = calculate_covariance_matrix(vectors2)\n    assert np.allclose(result2, expected2), f\"Test case 2 failed: {result2} != {expected2}\"\n\n    # Test case 3: Single feature\n    vectors3 = [\n        [1, 2, 3, 4, 5]\n    ]\n    expected3 = [\n        [2.0]\n    ]\n    result3 = calculate_covariance_matrix(vectors3)\n    assert np.allclose(result3, expected3), f\"Test case 3 failed: {result3} != {expected3}\"\n\n    # Test case 4: Features with zero variance\n    vectors4 = [\n        [1, 1, 1],\n        [2, 2, 2]\n    ]\n    expected4 = [\n        [0.0, 0.0],\n        [0.0, 0.0]\n    ]\n    result4 = calculate_covariance_matrix(vectors4)\n    assert np.allclose(result4, expected4), f\"Test case 4 failed: {result4} != {expected4}\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\nThis script defines a function `calculate_covariance_matrix` that computes the covariance matrix for a given set of vectors. It also includes a set of test cases to verify the correctness of the implementation. The test cases cover various scenarios, including simple matrices, single features, and features with zero variance."}
{"task_id": 11, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Get the number of equations\n    N = len(b)\n    \n    # Initialize the solution vector with zeros\n    x = np.zeros(N)\n    \n    # Perform n iterations\n    for _ in range(n):\n        # Create a new array to store the new values of x\n        x_new = np.zeros(N)\n        \n        for i in range(N):\n            # Calculate the sum of the other terms\n            s = sum(A[i][j] * x[j] for j in range(N) if i != j)\n            \n            # Calculate the new value for x[i]\n            x_new[i] = (b[i] - s) / A[i][i]\n        \n        # Update the solution vector\n        x = x_new\n        \n        # Round each element to 4 decimal places\n        x = np.round(x, 4)\n    \n    # Convert the solution to a list\n    return x.tolist()\n\n# Example usage:\nA = np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 4]], dtype=float)\nb = np.array([1, 2, 3], dtype=float)\nn = 10\n\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\nThis script defines a function `solve_jacobi` that uses the Jacobi method to solve a system of linear equations. It iterates `n` times, rounding each intermediate solution to four decimal places, and returns the approximate solution as a Python list. The example usage demonstrates how to call the function with a sample matrix `A` and vector `b`."}
{"task_id": 12, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that performs Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method. The script calculates the singular values and returns them rounded to the nearest 4th decimal place.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure the input is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Initialize the singular values\n    sigma1, sigma2 = np.linalg.norm(A, ord='fro') / np.sqrt(2), 0.0\n    \n    # Define the Jacobian iteration function\n    def jacobian_iteration(A, sigma1, sigma2, tol=1e-10, max_iter=1000):\n        for _ in range(max_iter):\n            # Construct the matrix B\n            B = np.array([[sigma1, 0], [0, sigma2]])\n            U, _, Vt = np.linalg.svd(A @ np.linalg.inv(B))\n            \n            # Update sigma1 and sigma2\n            sigma1_new = np.sqrt((U[0, 0]**2 + U[1, 0]**2) * (A[0, 0]**2 + A[1, 0]**2) +\n                                 (U[0, 1]**2 + U[1, 1]**2) * (A[0, 1]**2 + A[1, 1]**2))\n            sigma2_new = np.sqrt((Vt[0, 0]**2 + Vt[1, 0]**2) * (A[0, 0]**2 + A[0, 1]**2) +\n                                 (Vt[0, 1]**2 + Vt[1, 1]**2) * (A[1, 0]**2 + A[1, 1]**2))\n            \n            # Check for convergence\n            if abs(sigma1_new - sigma1) < tol and abs(sigma2_new - sigma2) < tol:\n                break\n            \n            sigma1, sigma2 = sigma1_new, sigma2_new\n        \n        return round(sigma1, 4), round(sigma2, 4)\n    \n    # Perform the Jacobian iteration\n    return jacobian_iteration(A, sigma1, sigma2)\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(\"Singular Values:\", singular_values)\n```\n\nThis script defines a function `svd_2x2_singular_values` that takes a 2x2 matrix `A` as input and returns a tuple of its singular values, rounded to the nearest 4th decimal place. The Jacobian method is used to iteratively approximate the singular values. The example usage at the end demonstrates how to call the function with a sample 2x2 matrix."}
{"task_id": 13, "completion_id": 0, "completion": "```\npython\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def determinant(matrix: list[list[int|float]]) -> float:\n        n = len(matrix)\n        if n == 1:\n            return matrix[0][0]\n        elif n == 2:\n            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n        else:\n            det = 0\n            for c in range(n):\n                det += ((-1) ** c) * matrix[0][c] * determinant(minor(matrix, 0, c))\n            return det\n\n    def minor(matrix: list[list[int|float]], i: int, j: int) -> list[list[int|float]]:\n        return [row[:j] + row[j+1:] for row in (matrix[:i] + matrix[i+1:])]\n\n    return determinant(matrix)\n\n# Example usage:\nmatrix_4x4 = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix_4x4))  # Output should be 0, as this is a singular matrix\n```\n\nThis script defines a function `determinant_4x4` that calculates the determinant of a 4x4 matrix using Laplace's Expansion. The function `determinant` is a recursive helper function that computes the determinant of a matrix of any size, and `minor` is a helper function that generates the minor matrix by removing the specified row and column. The example usage demonstrates how to call the function with a 4x4 matrix."}
{"task_id": 14, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert the input lists to numpy arrays for easier manipulation\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term in the linear model\n    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n    \n    # Calculate the coefficients using the normal equation\n    # The normal equation is: theta = (X_b^T * X_b)^(-1) * X_b^T * y\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the numpy array back to a list before returning\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [3, 5, 7]\n# print(linear_regression_normal_equation(X, y))\n```\n\nThis script defines a function `linear_regression_normal_equation` that takes a matrix `X` and a vector `y` as input and returns the coefficients of the linear regression model using the normal equation. The coefficients are rounded to four decimal places. The example usage at the bottom shows how to call the function with sample data."}
{"task_id": 15, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    # Initialize coefficients with zeros\n    m, n = X.shape\n    theta = np.zeros(n)\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Calculate the predictions\n        predictions = np.dot(X, theta)\n        \n        # Calculate the errors\n        errors = predictions - y\n        \n        # Update the coefficients\n        gradient = np.dot(X.T, errors) / m\n        theta -= alpha * gradient\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Return the coefficients as a list\n    return theta_rounded.tolist()\n\n# Example usage:\n# X should include a column of ones for the intercept term\n# X = np.array([[1, x1_1, x2_1, ..., xn_1],\n#               [1, x1_2, x2_2, ..., xn_2],\n#               ...,\n#               [1, x1_m, x2_m, ..., xn_m]])\n# y = np.array([y1, y2, ..., ym])\n# alpha = learning rate\n# iterations = number of iterations\n\n# X = np.array([[1, 1], [1, 2], [1, 3]])\n# y = np.array([2, 3, 4])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\nThis script defines a function `linear_regression_gradient_descent` that performs linear regression using gradient descent. The function initializes the coefficients to zero, iteratively updates them using the gradient descent algorithm, and finally rounds the coefficients to four decimal places before returning them as a list. The example usage at the bottom shows how to call the function with a simple dataset."}
{"task_id": 16, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data = np.round(standardized_data, 4).tolist()\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_data = (data - min_val) / (max_val - min_val)\n    min_max_data = np.round(min_max_data, 4).tolist()\n    \n    return standardized_data, min_max_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    standardized, min_max = feature_scaling(data)\n    print(\"Standardized Data:\")\n    print(standardized)\n    print(\"Min-Max Normalized Data:\")\n    print(min_max)\n```\n\nThis script defines a function `feature_scaling` that takes a 2D NumPy array as input and returns two lists: one with the data standardized and the other with the data min-max normalized. The results are rounded to the nearest 4th decimal place. The example usage demonstrates how to call the function with a sample dataset."}
{"task_id": 17, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the k-Means clustering algorithm as specified. The script includes the function `k_means_clustering` which takes the required inputs and returns the final centroids rounded to the nearest fourth decimal.\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Calculate the distance from each point to each centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        \n        # Assign each point to the nearest centroid\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids to be the mean of the points assigned to them\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        \n        # If centroids do not change, break the loop\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the centroids to the nearest fourth decimal\n    rounded_centroids = np.round(centroids, 4)\n    \n    return [tuple(centroid) for centroid in rounded_centroids]\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\n### Explanation:\n- **Input Conversion**: The input points and initial centroids are converted to NumPy arrays to facilitate efficient computation.\n- **Distance Calculation**: The Euclidean distance from each point to each centroid is calculated.\n- **Label Assignment**: Each point is assigned to the nearest centroid based on the calculated distances.\n- **Centroid Update**: The centroids are updated to be the mean of the points assigned to each cluster.\n- **Convergence Check**: The algorithm checks if the centroids have changed; if not, it breaks out of the loop.\n- **Rounding**: The final centroids are rounded to the nearest fourth decimal before being returned.\n\nThis script should work for 2D points, but the algorithm can be extended to higher dimensions by adjusting the input data accordingly."}
{"task_id": 18, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The input features of the dataset.\n    y (np.ndarray): The target labels of the dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Random seed for reproducibility if shuffle is True.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Check if the number of folds is valid\n    if k <= 1 or k > len(X):\n        raise ValueError(\"Number of folds must be between 2 and the number of samples\")\n    \n    # Shuffle the data if requested\n    if shuffle:\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    \n    # Calculate the size of each fold\n    fold_size = len(X) // k\n    remainder = len(X) % k\n    \n    # Initialize the list to store the train-test indices for each fold\n    folds = []\n    \n    # Generate the folds\n    start = 0\n    for i in range(k):\n        # Determine the end index for the current fold\n        end = start + fold_size + (1 if i < remainder else 0)\n        \n        # Get the test indices for the current fold\n        test_indices = np.arange(start, end)\n        \n        # Get the train indices for the current fold\n        train_indices = np.concatenate((np.arange(0, start), np.arange(end, len(X))))\n        \n        # Append the train-test indices for the current fold to the list\n        folds.append((train_indices, test_indices))\n        \n        # Update the start index for the next fold\n        start = end\n    \n    return folds\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]])\n    y = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n    \n    # Perform 3-fold cross-validation\n    folds = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\n    \n    # Print the train-test indices for each fold\n    for i, (train_indices, test_indices) in enumerate(folds):\n        print(f\"Fold {i+1}:\")\n        print(f\"  Train indices: {train_indices}\")\n        print(f\"  Test indices: {test_indices}\")\n        print(f\"  X_train: {X[train_indices]}\")\n        print(f\"  y_train: {y[train_indices]}\")\n        print(f\"  X_test: {X[test_indices]}\")\n        print(f\"  y_test: {y[test_indices]}\")\n        print()\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Step 4: Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Round the results to the nearest 4th decimal\n    principal_components_rounded = np.round(principal_components, 4)\n    \n    return principal_components_rounded.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [3, 4], [5, 6]])\n# k = 2\n# print(pca(data, k))\n```\n\nThis script defines a function `pca` that performs Principal Component Analysis on a given dataset. It standardizes the data, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the top `k` principal components rounded to four decimal places."}
{"task_id": 20, "completion_id": 0, "completion": "```\npython\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    def entropy(class_counts: Counter) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total) * math.log2(count / total) for count in class_counts.values() if count > 0)\n\n    def information_gain(split_data: dict, overall_entropy: float) -> float:\n        total_examples = sum(len(subset) for subset in split_data.values())\n        subset_entropies = sum((len(subset) / total_examples) * entropy(Counter([example[target_attr] for example in subset])) for subset in split_data.values())\n        return overall_entropy - subset_entropies\n\n    def split_data(examples: list[dict], attribute: str) -> dict:\n        return {value: [example for example in examples if example[attribute] == value] for value in set(example[attribute] for example in examples)}\n\n    def majority_class(examples: list[dict]) -> str:\n        return Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n\n    def build_tree(examples: list[dict], attributes: list[str]) -> dict:\n        class_counts = Counter(example[target_attr] for example in examples)\n        if len(class_counts) == 1:\n            return next(iter(class_counts))\n        if not attributes:\n            return majority_class(examples)\n        \n        overall_entropy = entropy(class_counts)\n        best_gain = -1\n        best_attribute = None\n        best_split = None\n\n        for attribute in attributes:\n            split = split_data(examples, attribute)\n            gain = information_gain(split, overall_entropy)\n            if gain > best_gain:\n                best_gain = gain\n                best_attribute = attribute\n                best_split = split\n\n        if best_gain == 0:\n            return majority_class(examples)\n\n        subtree = {value: build_tree(subset, [attr for attr in attributes if attr != best_attribute]) for value, subset in best_split.items()}\n        return {best_attribute: subtree}\n\n    return build_tree(examples, attributes)\n\n# Example usage:\nexamples = [\n    {'color': 'red', 'size': 'small', 'label': 'apple'},\n    {'color': 'red', 'size': 'medium', 'label': 'apple'},\n    {'color': 'red', 'size': 'large', 'label': 'apple'},\n    {'color': 'green', 'size': 'small', 'label': 'grape'},\n    {'color': 'green', 'size': 'medium', 'label': 'grape'},\n    {'color': 'green', 'size': 'large', 'label': 'orange'},\n    {'color': 'yellow', 'size': 'small', 'label': 'grape'},\n    {'color': 'yellow', 'size': 'medium', 'label': 'orange'},\n    {'color': 'yellow', 'size': 'large', 'label': 'orange'},\n]\n\nattributes = ['color', 'size']\ntarget_attr = 'label'\n\ndecision_tree = learn_decision_tree(examples, attributes, target_attr)\nprint(decision_tree)\n```\n\nThis script defines a function `learn_decision_tree` that constructs a decision tree using recursive binary splitting based on entropy and information gain. The function takes a list of examples, a list of attribute names, and the target attribute name as input, and returns a nested dictionary representing the decision tree. The example usage demonstrates how to use the function with a sample dataset."}
{"task_id": 21, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n    \n    def rbf_kernel(x1, x2, sigma=1.0):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * (sigma ** 2)))\n    \n    def compute_kernel_matrix(data, kernel_type, sigma):\n        n_samples = data.shape[0]\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel_type == 'linear':\n                    K[i, j] = linear_kernel(data[i], data[j])\n                elif kernel_type == 'rbf':\n                    K[i, j] = rbf_kernel(data[i], data[j], sigma)\n        return K\n    \n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    \n    K = compute_kernel_matrix(data, kernel, sigma)\n    \n    for t in range(1, iterations + 1):\n        step_size = 1.0 / (lambda_val * t)\n        \n        for i in range(n_samples):\n            prediction = np.sum(alpha * labels * K[:, i]) + bias\n            if labels[i] * prediction < 1:\n                alpha[i] = (1 - step_size * lambda_val) * alpha[i] + step_size * labels[i]\n                bias = bias + step_size * labels[i]\n            else:\n                alpha[i] = (1 - step_size * lambda_val) * alpha[i]\n    \n    # Round the results to the nearest 4th decimal\n    alpha_rounded = np.round(alpha, 4)\n    bias_rounded = np.round(bias, 4)\n    \n    return alpha_rounded.tolist(), bias_rounded.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n# labels = np.array([1, 1, 1, -1, -1])\n# alpha, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.1, iterations=100, sigma=0.5)\n# print(\"Alpha coefficients:\", alpha)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `pegasos_kernel_svm` that implements a deterministic version of the Pegasos algorithm for training a kernel SVM classifier. The function supports both linear and RBF kernels and returns the alpha coefficients and bias rounded to the nearest 4th decimal place."}
{"task_id": 22, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef sigmoid(z: float) -> float:\n    # Calculate the sigmoid value\n    result = 1 / (1 + math.exp(-z))\n    # Return the result rounded to four decimal places\n    return round(result, 4)\n\n# Example usage:\n# print(sigmoid(0))  # Output: 0.5\n# print(sigmoid(1))  # Output: 0.7311\n# print(sigmoid(-1)) # Output: 0.2689\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Calculate the maximum score to improve numerical stability\n    max_score = max(scores)\n    # Compute the exponentials of the scores minus the max score\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    # Compute the sum of the exponentials\n    sum_exp_scores = sum(exp_scores)\n    # Compute the softmax values\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    # Round each value to four decimal places\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\n\n# Example usage:\nscores = [3.0, 1.0, 0.2]\nprint(softmax(scores))  # Output: [0.8360, 0.1131, 0.0509]\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Calculate the weighted sum for each feature vector and apply the sigmoid activation function\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    \n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum(w * f for w, f in zip(weights, feature_vector)) + bias\n        predicted_probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(predicted_probability)\n    \n    # Calculate the mean squared error between predicted probabilities and true labels\n    mean_squared_error = sum((p - l) ** 2 for p, l in zip(predicted_probabilities, labels)) / len(labels)\n    \n    # Round the results to four decimal places\n    predicted_probabilities_rounded = [round(p, 4) for p in predicted_probabilities]\n    mean_squared_error_rounded = round(mean_squared_error, 4)\n    \n    return np.array(predicted_probabilities_rounded).tolist(), mean_squared_error_rounded\n\n# Example usage:\nfeatures = [[0.5, 0.3], [0.2, 0.8], [0.7, 0.6]]\nlabels = [0, 1, 1]\nweights = [0.1, 0.2]\nbias = -0.1\n\npredicted_probabilities, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probabilities)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines a function `single_neuron_model` that simulates a single neuron with a sigmoid activation function for binary classification. It calculates the predicted probabilities for each input feature vector and computes the mean squared error between these probabilities and the true labels. The results are rounded to four decimal places and returned as specified."}
{"task_id": 25, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ndef mean_squared_error(predictions, targets):\n    return ((predictions - targets) ** 2).mean()\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n        \n        # Calculate MSE\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        \n        # Backward pass\n        error = predictions - labels\n        d_loss_d_predictions = error\n        d_predictions_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_predictions * d_predictions_d_z\n        \n        # Calculate gradients\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n        \n        # Update weights and bias\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\n# features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# labels = np.array([0, 1, 1, 0])\n# initial_weights = np.array([0.5, 0.5])\n# initial_bias = 0.5\n# learning_rate = 0.1\n# epochs = 1000\n# updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n# print(\"Updated Weights:\", updated_weights)\n# print(\"Updated Bias:\", updated_bias)\n# print(\"MSE Values:\", mse_values)\n```\n\nThis script defines a function `train_neuron` that simulates a single neuron with a sigmoid activation function. It uses backpropagation to update the weights and bias based on the mean squared error (MSE) loss. The function returns the updated weights, bias, and a list of MSE values for each epoch, all rounded to four decimal places. The example usage at the bottom demonstrates how to call the function with some sample data."}
{"task_id": 26, "completion_id": 0, "completion": "```\npython\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = a * b + b**3\ne = c * d + c\nf = e.relu()\nf.backward()\n\nprint(f)  # Should print the value of f and its gradient\nprint(a.grad)  # Should print the gradient of f with respect to a\nprint(b.grad)  # Should print the gradient of f with respect to b\n```\n\nThis script defines a `Value` class that supports basic autograd operations for addition, multiplication, and ReLU activation. It includes a `backward` method to compute gradients through automatic differentiation. The example usage at the end demonstrates how to use the class to compute gradients for a simple expression."}
{"task_id": 27, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis lists to numpy arrays\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    \n    # Calculate the transformation matrix P from B to C\n    # P = C * B_inv\n    B_inv = np.linalg.inv(B_matrix)\n    P = np.dot(C_matrix, B_inv)\n    \n    # Round the result to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the result to a list and return\n    return P_rounded.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]  # Another basis\n\nP = transform_basis(B, C)\nprint(P)\n```\n\nThis script defines a function `transform_basis` that computes the transformation matrix from basis B to basis C in R^3. The function takes two lists of lists (each inner list representing a basis vector) and returns the transformation matrix as a list of lists, rounded to the nearest 4th decimal place. The example usage demonstrates how to call the function with two bases, B and C."}
{"task_id": 28, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the Singular Value Decomposition (SVD) of a 2x2 matrix using the method based on eigendecomposition of \\(A^T A\\). The script includes the function `svd_2x2` which returns the matrices \\(U\\), \\(S\\), and \\(V\\) as lists rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Step 1: Compute A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Step 2: Perform eigendecomposition on A^T * A\n    eigenvalues, eigenvectors = np.linalg.eigh(ATA)\n    \n    # Step 3: Construct the singular values as the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Step 4: Sort singular values in descending order and reorder eigenvectors\n    idx = singular_values.argsort()[::-1]\n    singular_values = singular_values[idx]\n    V = eigenvectors[:, idx]\n    \n    # Step 5: Construct the diagonal matrix S\n    S = np.diag(singular_values)\n    \n    # Step 6: Compute U by normalizing A * v_i for each non-zero singular value\n    U = np.zeros_like(A)\n    for i in range(len(singular_values)):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n    \n    # Step 7: Round all results to the nearest 4th decimal and convert to lists\n    U_rounded = np.round(U, 4).tolist()\n    S_rounded = np.round(S, 4).tolist()\n    V_rounded = np.round(V, 4).tolist()\n    \n    return U_rounded, S_rounded, V_rounded\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines the `svd_2x2` function, which performs the SVD of a 2x2 matrix using the specified method. The function returns the matrices \\(U\\), \\(S\\), and \\(V\\) as lists, with all values rounded to the nearest 4th decimal. The example usage at the end demonstrates how to use the function with a sample 2x2 matrix."}
{"task_id": 29, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The label dataset.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of shuffled data (X, y) converted to list using numpy's tolist() method.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Get the number of samples\n    num_samples = X.shape[0]\n    \n    # Create an array of indices and shuffle them\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    \n    # Shuffle X and y based on the shuffled indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert to list and return\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    shuffled_data = shuffle_data(X, y, seed=42)\n    print(\"Shuffled X:\", shuffled_data[0])\n    print(\"Shuffled y:\", shuffled_data[1])\n```\n\nThis script defines a function `shuffle_data` that takes two numpy arrays `X` and `y` and an optional `seed` parameter. It shuffles the rows of `X` and `y` while maintaining the correspondence between them, and returns the shuffled data as a list of lists. The example usage demonstrates how to call the function and print the shuffled data."}
{"task_id": 30, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset X and optionally y.\n    \n    Parameters:\n    - X (numpy.ndarray): The input data.\n    - y (numpy.ndarray, optional): The target data.\n    - batch_size (int): The size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch).\n    - If y is not provided, yields X_batch.\n    \"\"\"\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n    if y is not None:\n        assert isinstance(y, np.ndarray), \"y must be a numpy array\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n    \n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        if y is not None:\n            yield (X[batch_indices].tolist(), y[batch_indices].tolist())\n        else:\n            yield X[batch_indices].tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n    y = np.array([0, 1, 0, 1, 0, 1])\n    \n    for batch in batch_iterator(X, y, batch_size=2):\n        print(batch)\n    \n    # Example without y\n    for batch in batch_iterator(X, batch_size=2):\n        print(batch)\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on whether the value of the specified feature\n    is greater than or equal to the given threshold.\n\n    Parameters:\n    X (np.ndarray): The input dataset.\n    feature_i (int): The index of the feature to divide on.\n    threshold (float): The threshold value to compare the feature against.\n\n    Returns:\n    tuple: Two lists, the first with samples where the feature value is >= threshold,\n           and the second with samples where the feature value is < threshold.\n    \"\"\"\n    # Convert to numpy array if X is not already\n    X = np.array(X)\n    \n    # Find indices where the feature is greater than or equal to the threshold\n    indices_ge = X[:, feature_i] >= threshold\n    indices_lt = X[:, feature_i] < threshold\n    \n    # Split the dataset based on the indices\n    X_ge = X[indices_ge]\n    X_lt = X[indices_lt]\n    \n    # Convert the numpy arrays to lists\n    return X_ge.tolist(), X_lt.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dataset\n    dataset = [\n        [2.5, 3.0, 1.5],\n        [1.0, 1.5, 0.5],\n        [3.0, 4.5, 2.5],\n        [2.0, 2.5, 1.0]\n    ]\n    \n    # Feature index and threshold\n    feature_index = 0\n    threshold_value = 2.0\n    \n    # Divide the dataset\n    subset_ge, subset_lt = divide_on_feature(dataset, feature_index, threshold_value)\n    \n    # Print the results\n    print(\"Subset where feature >= threshold:\", subset_ge)\n    print(\"Subset where feature < threshold:\", subset_lt)\n```\n\nThis script defines a function `divide_on_feature` that takes a dataset `X`, a feature index `feature_i`, and a `threshold`. It divides the dataset into two subsets based on whether the specified feature's value is greater than or equal to the threshold. The subsets are returned as Python lists. The example usage demonstrates how to use this function with a sample dataset."}
{"task_id": 32, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    # Generate all combinations of features with replacement up to the given degree\n    combinations = combinations_with_replacement(range(n_features), degree)\n    \n    # Initialize a list to store the polynomial features\n    poly_features = []\n    \n    # Iterate over each combination\n    for combo in combinations:\n        # Compute the product of the features for the current combination\n        feature_product = np.prod(X[:, combo], axis=1)\n        # Append the feature product to the list of polynomial features\n        poly_features.append(feature_product)\n    \n    # Stack all polynomial features into a single array\n    poly_features_array = np.column_stack(poly_features)\n    \n    # Add the bias term (x^0 = 1) to the polynomial features\n    poly_features_array = np.hstack([np.ones((n_samples, 1)), poly_features_array])\n    \n    # Return the polynomial features as a list\n    return poly_features_array.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis script defines a function `polynomial_features` that generates polynomial features for a given dataset `X` up to a specified `degree`. The function uses `combinations_with_replacement` from the `itertools` module to generate all possible combinations of features, computes the product of features for each combination, and stacks them into a new array. It also adds a bias term (x^0 = 1) to the features. Finally, it returns the polynomial features as a Python list."}
{"task_id": 33, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    \n    for _ in range(n_subsets):\n        if replacements:\n            # With replacement\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            # Without replacement\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        \n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    # Generate 3 random subsets with replacement\n    subsets_with_replacement = get_random_subsets(X, y, n_subsets=3, replacements=True)\n    print(\"Subsets with replacement:\")\n    for subset in subsets_with_replacement:\n        print(subset)\n    \n    # Generate 3 random subsets without replacement\n    subsets_without_replacement = get_random_subsets(X, y, n_subsets=3, replacements=False)\n    print(\"\\nSubsets without replacement:\")\n    for subset in subsets_without_replacement:\n        print(subset)\n```\n\nThis script defines a function `get_random_subsets` that generates random subsets of a given dataset `X` and `y`. The function takes parameters to specify the number of subsets and whether to sample with or without replacement. The subsets are returned as a list of tuples, where each tuple contains the feature matrix and target vector for a subset, converted to lists using `tolist()`. The example usage demonstrates generating subsets both with and without replacement."}
{"task_id": 34, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Create a zero matrix of shape (len(x), n_col)\n    one_hot_matrix = np.zeros((len(x), n_col), dtype=int)\n    \n    # Fill in the appropriate indices with ones\n    one_hot_matrix[np.arange(len(x)), x] = 1\n    \n    # Convert to list and return\n    return one_hot_matrix.tolist()\n\n# Example usage:\n# x = np.array([0, 1, 2, 1])\n# print(to_categorical(x))  # Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]\n# print(to_categorical(x, n_col=4))  # Output: [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D numpy array x\n    diagonal_matrix = np.diag(x)\n    # Convert the diagonal matrix to a python list\n    return diagonal_matrix.tolist()\n\n# Example usage:\nx = np.array([1, 2, 3])\ndiagonal_matrix_list = make_diagonal(x)\nprint(diagonal_matrix_list)\n```\n\nThis script defines a function `make_diagonal` that takes a 1D numpy array `x` and returns a 2D list representing the diagonal matrix. The example usage demonstrates how to use this function with a sample input."}
{"task_id": 36, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = y_true.shape[0]\n    \n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 1, 0, 0, 1])\n    print(\"Accuracy Score:\", accuracy_score(y_true, y_pred))\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (np.ndarray): A 2D numpy array.\n    Y (np.ndarray, optional): A 2D numpy array. Defaults to None.\n    \n    Returns:\n    list: The correlation matrix as a python list.\n    \"\"\"\n    if Y is None:\n        # Calculate the correlation matrix of X with itself\n        data = X\n    else:\n        # Concatenate X and Y to calculate the correlation matrix between them\n        data = np.concatenate((X, Y), axis=1)\n    \n    # Calculate the correlation matrix using numpy's corrcoef function\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    \n    # Convert the numpy array to a list\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    \n    return correlation_matrix_list\n\n# Example usage:\n# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n# print(calculate_correlation_matrix(X, Y))\n# print(calculate_correlation_matrix(X))\n```\n\nThis script defines a function `calculate_correlation_matrix` that computes the correlation matrix for a given dataset. It handles both cases where a second dataset `Y` is provided and where it is not. The results are rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 38, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    # Initialize sample weights\n    n_samples, n_features = X.shape\n    weights = np.ones(n_samples) / n_samples\n    \n    classifiers = []\n    \n    for _ in range(n_clf):\n        # Find the best threshold for each feature\n        best_feature = None\n        best_threshold = None\n        min_error = float('inf')\n        best_predictions = None\n        \n        for feature in range(n_features):\n            # Get unique values in the feature column\n            unique_values = np.unique(X[:, feature])\n            # Try each unique value as a threshold\n            for threshold in unique_values:\n                # Predictions if the feature value is less than the threshold\n                predictions = np.where(X[:, feature] < threshold, 1, -1)\n                # Weighted error\n                error = np.sum(weights[predictions != y])\n                \n                # Check if this threshold is better than the current best\n                if error < min_error:\n                    min_error = error\n                    best_predictions = predictions.copy()\n                    best_threshold = threshold\n                    best_feature = feature\n        \n        # Calculate the classifier weight\n        alpha = 0.5 * math.log((1 - min_error) / (min_error + 1e-10))\n        \n        # Update sample weights\n        weights *= np.exp(-alpha * y * best_predictions)\n        weights /= np.sum(weights)\n        \n        # Store the classifier\n        classifiers.append({\n            'feature': best_feature,\n            'threshold': round(best_threshold, 4),\n            'alpha': round(alpha, 4)\n        })\n    \n    return classifiers\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n# y = np.array([1, 1, 1, -1, -1])\n# n_clf = 3\n# classifiers = adaboost_fit(X, y, n_clf)\n# print(classifiers)\n```\n\nThis script implements the AdaBoost fit method. It initializes sample weights, iteratively finds the best threshold for each feature to create a weak classifier, calculates the error and weight of each classifier, updates the sample weights, and returns a list of classifiers with their parameters. Each parameter is rounded to the nearest 4th decimal place."}
{"task_id": 39, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores_array = np.array(scores)\n    \n    # For numerical stability, subtract the maximum score from all scores\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    \n    # Compute the softmax probabilities\n    softmax_probs = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    \n    # Compute the log-softmax\n    log_softmax_values = np.log(softmax_probs)\n    \n    # Round the results to the nearest 4th decimal\n    rounded_log_softmax = np.round(log_softmax_values, 4)\n    \n    # Convert the numpy array back to a list\n    result_list = rounded_log_softmax.tolist()\n    \n    return result_list\n\n# Example usage:\nscores = [2.0, 1.0, 0.1]\nprint(log_softmax(scores))\n```\n\nThis script defines a function `log_softmax` that computes the log-softmax of a given 1D list of scores. It includes steps for numerical stability by subtracting the maximum score from all scores before computing the softmax and log-softmax. The results are rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 40, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Optimizer:\n    def update(self, params, grads):\n        raise NotImplementedError()\n\nclass SGD(Optimizer):\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, params, grads):\n        for param, grad in zip(params, grads):\n            param -= self.learning_rate * grad\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n\n        if self.trainable:\n            grad_w = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n\n        return np.dot(accum_grad, W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a Dense layer with 10 neurons and input shape of 5\n    dense_layer = Dense(n_units=10, input_shape=(5,))\n    \n    # Initialize with SGD optimizer\n    dense_layer.initialize(optimizer=SGD(learning_rate=0.01))\n    \n    # Example input\n    X = np.random.rand(1, 5)\n    \n    # Forward pass\n    output = dense_layer.forward_pass(X)\n    print(\"Output:\", output.tolist())\n    \n    # Backward pass with some accumulated gradient\n    accum_grad = np.random.rand(1, 10)\n    grad_input = dense_layer.backward_pass(accum_grad)\n    print(\"Gradient with respect to input:\", grad_input.tolist())\n```\n\nThis script defines a `Dense` layer that extends the `Layer` class and includes methods for weight initialization, parameter counting, forward and backward passes, and output shape determination. The `SGD` optimizer is used for updating weights and biases. The example usage demonstrates how to create a `Dense` layer, initialize it, perform a forward pass, and then a backward pass with a random accumulated gradient."}
{"task_id": 41, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Add padding to the input matrix\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    \n    # Get dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate output dimensions\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    \n    # Initialize output matrix\n    output = np.zeros((output_height, output_width))\n    \n    # Perform convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Calculate the slice of the input matrix to convolve\n            input_slice = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            # Perform element-wise multiplication and sum to get the convolution result\n            output[i, j] = np.sum(input_slice * kernel)\n    \n    # Round to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Convert to list and return\n    return output.tolist()\n\n# Example usage:\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1\n\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```\n\nThis script defines a function `simple_conv2d` that performs a 2D convolution on an input matrix using a specified kernel, padding, and stride. The result is rounded to the nearest 4th decimal and returned as a Python list. The example usage at the end demonstrates how to call the function with a sample input matrix and kernel."}
{"task_id": 42, "completion_id": 0, "completion": "```\npython\ndef relu(z: float) -> float:\n    \"\"\"Apply the Rectified Linear Unit (ReLU) activation function to the input.\"\"\"\n    return z if z > 0 else 0\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-1.0, 0.0, 1.0, 2.5, -3.3]\n    results = {z: relu(z) for z in test_values}\n    print(results)\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate the predictions\n    y_pred = np.dot(X, w)\n    \n    # Calculate the Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate the regularization term\n    regularization = alpha * np.sum(w ** 2)\n    \n    # Calculate the Ridge loss\n    ridge_loss_value = mse + regularization\n    \n    # Return the Ridge loss rounded to the nearest 4th decimal\n    return round(ridge_loss_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [2, 3], [3, 4]])\n    w = np.array([0.5, 0.5])\n    y_true = np.array([3, 5, 7])\n    alpha = 0.1\n    \n    # Calculate Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```\n\nThis script defines the `ridge_loss` function, which calculates the Ridge Regression loss by combining the Mean Squared Error (MSE) with a regularization term. The function takes a feature matrix `X`, a coefficient vector `w`, true labels `y_true`, and a regularization parameter `alpha`. The result is rounded to the nearest 4th decimal place. An example usage is provided to demonstrate how to use the function."}
{"task_id": 44, "completion_id": 0, "completion": "```\npython\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs, default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z > 0 else alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(leaky_relu(1.0))    # Output: 1.0\n    print(leaky_relu(-1.0))   # Output: -0.01\n    print(leaky_relu(0.0))    # Output: 0.0\n    print(leaky_relu(-2.5))   # Output: -0.025\n    print(leaky_relu(3.7, 0.1)) # Output: 3.7\n    print(leaky_relu(-3.7, 0.1))# Output: -0.37\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product (inner product) of two vectors.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The result of the linear kernel function.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage:\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nresult = kernel_function(x1, x2)\nprint(\"Linear kernel result:\", result)\n```\n\nThis script defines a function `kernel_function` that calculates the linear kernel (dot product) of two input vectors `x1` and `x2`. It also includes an example usage that demonstrates how to call the function and print the result."}
{"task_id": 46, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives and false positives\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if true_positives + false_positives == 0:\n        return 0.0  # To handle the case where there are no positive predictions\n    \n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 1])\nprint(precision(y_true, y_pred))  # Output should be 0.75\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization for linear regression using Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    - X: numpy array, feature matrix (including the bias term if needed)\n    - y: numpy array, target vector\n    - weights: numpy array, initial weights\n    - learning_rate: float, learning rate for gradient descent\n    - n_iterations: int, number of iterations\n    - batch_size: int, size of the mini-batch (only used for mini-batch gradient descent)\n    - method: str, specifies the type of gradient descent: 'batch', 'stochastic', or 'mini-batch'\n    \n    Returns:\n    - List of final weights rounded to the nearest 4th decimal.\n    \"\"\"\n    m = len(y)  # Number of samples\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            gradient = (1/m) * X.T.dot(X.dot(weights) - y)\n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            idx = np.random.randint(m)\n            xi = X[idx:idx+1]\n            yi = y[idx:idx+1]\n            gradient = xi.T.dot(xi.dot(weights) - yi)\n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            idx = np.random.randint(m, size=batch_size)\n            xi = X[idx]\n            yi = y[idx]\n            gradient = (1/batch_size) * xi.T.dot(xi.dot(weights) - yi)\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n        \n        weights -= learning_rate * gradient\n    \n    return weights.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])\n    y = np.array([2, 3, 4, 5])\n    weights = np.array([0, 0])\n    learning_rate = 0.01\n    n_iterations = 1000\n    batch_size = 2\n    \n    # Run batch gradient descent\n    final_weights_batch = gradient_descent(X, y, weights.copy(), learning_rate, n_iterations, method='batch')\n    print(\"Batch Gradient Descent Final Weights:\", final_weights_batch)\n    \n    # Run stochastic gradient descent\n    final_weights_stochastic = gradient_descent(X, y, weights.copy(), learning_rate, n_iterations, method='stochastic')\n    print(\"Stochastic Gradient Descent Final Weights:\", final_weights_stochastic)\n    \n    # Run mini-batch gradient descent\n    final_weights_mini_batch = gradient_descent(X, y, weights.copy(), learning_rate, n_iterations, batch_size=batch_size, method='mini-batch')\n    print(\"Mini-Batch Gradient Descent Final Weights:\", final_weights_mini_batch)\n```\nThis script defines a function `gradient_descent` that implements three variants of gradient descent: Batch, Stochastic, and Mini-Batch. The function takes as input the feature matrix `X`, target vector `y`, initial weights, learning rate, number of iterations, batch size (for Mini-Batch), and the method type. It returns the final weights after the specified number of iterations, rounded to four decimal places. The example usage demonstrates how to use the function with sample data."}
{"task_id": 48, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the input matrix to a numpy array for easier manipulation\n    A = np.array(matrix, dtype=float)\n    rows, cols = A.shape\n    lead = 0\n\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    break\n        A[[i, r]] = A[[r, i]]\n        div = A[r, lead]\n        A[r] /= div\n        for i in range(rows):\n            if i != r:\n                mult = A[i, lead]\n                A[i] -= mult * A[r]\n        lead += 1\n\n    # Convert the numpy array back to a list\n    return A.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nprint(rref(matrix))\n```\n\nThis script defines a function `rref` that takes a matrix (as a list of lists) and converts it to its Reduced Row Echelon Form using Gaussian elimination with partial pivoting. The result is returned as a list of lists. The example usage at the end demonstrates how to call the function with a sample matrix."}
{"task_id": 49, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    \n    for t in range(1, num_iterations + 1):\n        # Compute the gradient\n        g = grad(x)\n        \n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n        \n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n        \n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return the optimized parameters as a list rounded to 4 decimal places\n    return x.round(4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial parameter values\nx0 = [1.0, 1.0]\n\n# Run the Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0, num_iterations=1000)\nprint(optimized_params)\n```\n\nThis script defines the `adam_optimizer` function, which implements the Adam optimization algorithm. It also includes an example usage with a simple quadratic function and its gradient. The optimized parameters are returned as a list rounded to four decimal places."}
{"task_id": 50, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Initialize weights and bias\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Function to compute the gradient of the L1 penalty\n    def l1_penalty_gradient(weights):\n        return alpha * np.sign(weights)\n    \n    # Gradient Descent\n    for iteration in range(max_iter):\n        # Predictions\n        y_pred = np.dot(X, weights) + bias\n        \n        # Compute the gradient of the loss function (Mean Squared Error)\n        gradient_weights = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n        gradient_bias = (1 / n_samples) * np.sum(y_pred - y)\n        \n        # Update weights and bias with L1 penalty\n        weights -= learning_rate * (gradient_weights + l1_penalty_gradient(weights))\n        bias -= learning_rate * gradient_bias\n        \n        # Check for convergence\n        if np.linalg.norm(gradient_weights) < tol:\n            break\n    \n    # Return the rounded weights and bias as a list\n    return (np.round(weights, 4).tolist(), np.round(bias, 4).tolist())\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([1, 2, 3])\n# alpha = 0.1\n# learning_rate = 0.01\n# max_iter = 1000\n# tol = 1e-4\n# weights, bias = l1_regularization_gradient_descent(X, y, alpha, learning_rate, max_iter, tol)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `l1_regularization_gradient_descent` that implements Lasso Regression using Gradient Descent. The function takes in the feature matrix `X`, the target vector `y`, the regularization parameter `alpha`, the learning rate, the maximum number of iterations, and a tolerance for convergence. It returns the learned weights and bias, rounded to the nearest 4th decimal place."}
{"task_id": 51, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Create a matrix to store distances\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the matrix\n    for i in range(m + 1):\n        dp[i, 0] = i\n    for j in range(n + 1):\n        dp[0, j] = j\n\n    # Compute the distances\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # Cost of substitution\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i, j] = min(dp[i - 1, j] + 1,    # Deletion\n                           dp[i, j - 1] + 1,    # Insertion\n                           dp[i - 1, j - 1] + cost)  # Substitution\n\n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)  # Transposition\n\n    return dp[m, n]\n\n# Example usage\nif __name__ == \"__main__\":\n    s1 = \"caper\"\n    s2 = \"acer\"\n    print(f\"OSA distance between '{s1}' and '{s2}' is {OSA(s1, s2)}\")\n```\n\nThis script defines a function `OSA` that calculates the Optimal String Alignment distance between two strings using dynamic programming. The function initializes a matrix to store the distances and iteratively computes the minimum cost of converting the source string to the target string, considering the allowed operations: deletion, insertion, substitution, and transposition. The example usage at the bottom demonstrates how to use the function with the strings \"caper\" and \"acer\"."}
{"task_id": 52, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert inputs to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Recall\n    if (TP + FN) == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)\n\n# Example usage:\n# y_true = [0, 1, 1, 1, 0, 1]\n# y_pred = [0, 0, 1, 1, 0, 1]\n# print(recall(y_true, y_pred))  # Output should be 0.833\n```\n\nThis script defines the `recall` function, which calculates the recall metric for binary classification. It uses numpy for efficient computation and handles the case where the denominator is zero to avoid division by zero errors."}
{"task_id": 53, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Linear transformations to project inputs to query, key, and value spaces\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the attention scores (Q * K^T) / sqrt(d_k)\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply softmax to get attention weights\n    attention_weights = np.softmax(scores, axis=-1)\n    \n    # Compute the weighted sum of values\n    output = np.dot(attention_weights, V)\n    \n    # Round the result to the nearest 4th decimal and convert to list\n    return np.round(output, 4).tolist()\n\n# Example usage:\n# X: input sequence, shape (sequence_length, embedding_dim)\n# W_q, W_k, W_v: weight matrices for query, key, and value, shape (embedding_dim, transformed_dim)\nX = np.random.rand(5, 10)  # Example input sequence\nW_q = np.random.rand(10, 10)  # Example weight matrix for query\nW_k = np.random.rand(10, 10)  # Example weight matrix for key\nW_v = np.random.rand(10, 10)  # Example weight matrix for value\n\n# Get the self-attention output\noutput = self_attention(X, W_q, W_k, W_v)\nprint(output)\n```\n\nThis script defines a function `self_attention` that computes the self-attention mechanism for a given input sequence `X` and weight matrices `W_q`, `W_k`, and `W_v`. The function returns the self-attention output as a Python list with values rounded to the nearest 4th decimal. The example usage demonstrates how to call the function with random input data."}
{"task_id": 54, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state\n    h = initial_hidden_state\n    \n    # Iterate over each input vector in the sequence\n    for x in input_sequence:\n        # Update the hidden state using the RNN formula\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    \n    # Return the final hidden state rounded to four decimal places\n    return h.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a simple sequence of input vectors\n    input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    # Define an initial hidden state\n    initial_hidden_state = [0.0, 0.0]\n    # Define weight matrices and bias vector\n    Wx = [[0.5, 0.1], [0.2, 0.3]]\n    Wh = [[0.4, 0.5], [0.6, 0.7]]\n    b = [0.1, 0.2]\n    \n    # Compute the final hidden state\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(\"Final Hidden State:\", final_hidden_state)\n```\n\nThis script defines a function `rnn_forward` that implements a simple RNN cell. It processes a sequence of input vectors and returns the final hidden state after applying the RNN formula with the tanh activation function. The result is rounded to four decimal places and returned as a Python list. The example usage demonstrates how to call the function with sample data."}
{"task_id": 55, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create a translation matrix\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Convert the list of points to a homogeneous coordinate matrix\n    homogeneous_points = np.array([[x, y, 1] for x, y in points]).T\n    \n    # Apply the translation matrix\n    translated_homogeneous_points = np.dot(translation_matrix, homogeneous_points)\n    \n    # Convert back to Cartesian coordinates and reshape\n    translated_points = translated_homogeneous_points[:2, :].T\n    \n    # Convert to list\n    return translated_points.tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)  # Output: [[3, 5], [5, 7], [7, 9]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    # Calculate the KL divergence using the formula\n    kl_div = np.log(sigma_q / sigma_p) + ((sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)) - 0.5\n    return kl_div\n\n# Example usage:\n# mu_p, sigma_p = 0, 1  # Standard normal distribution\n# mu_q, sigma_q = 1, 2  # Another normal distribution\n# print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n```\n\nThis script defines a function `kl_divergence_normal` that computes the KL divergence between two normal distributions given their means and standard deviations. The formula used is derived from the properties of normal distributions and the definition of KL divergence."}
{"task_id": 57, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Ensure A is a numpy array for matrix operations\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the number of equations\n    N = len(b)\n    \n    # Initialize the solution vector\n    if x_ini is None:\n        x = np.zeros(N)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform Gauss-Seidel iterations\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i+1:], x[i+1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    \n    # Return the rounded solution as a list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# A = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\n# b = [4, 7, 3]\n# n = 10\n# x_ini = [0, 0, 0]\n# print(gauss_seidel(A, b, n, x_ini))\n```\n\nThis script defines the `gauss_seidel` function, which solves a system of linear equations using the Gauss-Seidel method. The function takes a coefficient matrix `A`, a right-hand side vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. It returns the approximate solution vector after the specified number of iterations, rounded to four decimal places."}
{"task_id": 58, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = len(b)\n    \n    # Augment A with b\n    Ab = np.hstack((A, b.reshape(n, 1)))\n    \n    # Partial pivoting and forward elimination\n    for i in range(n):\n        # Find the pivot row\n        max_row = np.argmax(abs(Ab[i:, i])) + i\n        \n        # Swap the current row with the pivot row\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        # Eliminate entries below the pivot\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i+1:n], x[i+1:n])) / Ab[i, i]\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Convert the numpy array to a list and return\n    return x.tolist()\n\n# Example usage:\n# A = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\n# b = [1, -2, 0]\n# print(gaussian_elimination(A, b))  # Output: [1.0, -2.0, -2.0]\n```\n\nThis script defines the `gaussian_elimination` function, which performs Gaussian elimination with partial pivoting to solve a system of linear equations. The function returns the solution vector `x` rounded to the nearest fourth decimal place. The example usage at the bottom demonstrates how to call the function with a sample matrix `A` and vector `b`."}
{"task_id": 59, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # x is a list of input vectors, each of shape (input_size, 1)\n        # Convert initial hidden state and cell state to column vectors if they are not\n        h_prev = np.array(initial_hidden_state).reshape(-1, 1)\n        c_prev = np.array(initial_cell_state).reshape(-1, 1)\n\n        # List to store hidden states at each time step\n        h_states = []\n\n        for x_t in x:\n            # Concatenate input and hidden state\n            x_t = np.array(x_t).reshape(-1, 1)\n            combined = np.vstack((x_t, h_prev))\n\n            # Compute gates\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_t_hat = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n\n            # Update cell state\n            c_t = f_t * c_prev + i_t * c_t_hat\n            # Update hidden state\n            h_t = o_t * np.tanh(c_t)\n\n            # Store the hidden state\n            h_states.append(h_t)\n\n            # Update previous hidden state and cell state for next iteration\n            h_prev = h_t\n            c_prev = c_t\n\n        # Round the results to the nearest 4th decimal\n        h_states_rounded = [h.round(4).tolist() for h in h_states]\n        final_hidden_state_rounded = h_prev.round(4).tolist()\n        final_cell_state_rounded = c_prev.round(4).tolist()\n\n        return h_states_rounded, final_hidden_state_rounded, final_cell_state_rounded\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Parameters\n    input_size = 10\n    hidden_size = 20\n    sequence_length = 5\n\n    # Initialize LSTM\n    lstm = LSTM(input_size, hidden_size)\n\n    # Create a dummy sequence of inputs\n    x = [np.random.randn(input_size) for _ in range(sequence_length)]\n\n    # Initial hidden state and cell state\n    initial_hidden_state = np.zeros(hidden_size)\n    initial_cell_state = np.zeros(hidden_size)\n\n    # Forward pass\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n\n    # Print results\n    print(\"Hidden states at each time step:\", hidden_states)\n    print(\"Final hidden state:\", final_hidden_state)\n    print(\"Final cell state:\", final_cell_state)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import defaultdict\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus is empty. Please provide a non-empty list of documents.\")\n    \n    # Number of documents in the corpus\n    N = len(corpus)\n    \n    # Dictionary to store term frequencies in each document\n    tf = [defaultdict(int) for _ in range(N)]\n    \n    # Dictionary to store document frequencies for each term\n    df = defaultdict(int)\n    \n    # Calculate term frequencies and document frequencies\n    for i, document in enumerate(corpus):\n        for word in document:\n            tf[i][word] += 1\n            df[word] += 1\n    \n    # Calculate TF-IDF scores for the query\n    tf_idf_scores = []\n    \n    for i, document in enumerate(corpus):\n        doc_tf_idf = []\n        for term in query:\n            # Term Frequency in the current document\n            term_tf = tf[i][term]\n            \n            # Document Frequency of the term in the corpus\n            term_df = df[term]\n            \n            # Smoothed Inverse Document Frequency\n            idf = np.log((N + 1) / (term_df + 1)) + 1\n            \n            # TF-IDF score for the term in the current document\n            tf_idf = term_tf * idf\n            \n            doc_tf_idf.append(round(tf_idf, 4))\n        \n        tf_idf_scores.append(doc_tf_idf)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\"],\n    [\"the\", \"fox\"],\n    [\"the\", \"quick\", \"quick\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n]\nquery = [\"the\", \"fox\", \"dog\"]\n\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script defines the `compute_tf_idf` function, which calculates the TF-IDF scores for a given query against a corpus of documents. It handles division by zero by using smoothing in the IDF calculation and raises an error if the corpus is empty. The results are rounded to four decimal places and returned as a list of lists."}
{"task_id": 61, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives, False Positives, and False Negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_score_value, 3)\n\n# Example usage:\ny_true = np.array([0, 1, 1, 0, 1, 1, 0, 1])\ny_pred = np.array([0, 0, 1, 0, 1, 0, 0, 1])\nbeta = 1.0\n\nprint(f\"F-Score: {f_score(y_true, y_pred, beta)}\")\n```\n\nThis script defines a function `f_score` that calculates the F-Score for a binary classification task given true labels, predicted labels, and a beta value. The example usage demonstrates how to call the function with sample data."}
{"task_id": 62, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Parameters:\n        input_sequence (list of numpy.ndarray): List of input vectors, each of shape (input_size, 1)\n        \n        Returns:\n        outputs (list of numpy.ndarray): List of output vectors, each of shape (output_size, 1)\n        last_inputs (list of numpy.ndarray): List of input vectors used in the forward pass\n        last_hiddens (list of numpy.ndarray): List of hidden states used in the forward pass\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        \n        for x in input_sequence:\n            h_prev = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h_prev) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h_prev)\n        \n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \n        Parameters:\n        input_sequence (list of numpy.ndarray): List of input vectors, each of shape (input_size, 1)\n        expected_output (list of numpy.ndarray): List of expected output vectors, each of shape (output_size, 1)\n        outputs (list of numpy.ndarray): List of output vectors from forward pass, each of shape (output_size, 1)\n        last_inputs (list of numpy.ndarray): List of input vectors used in the forward pass\n        last_hiddens (list of numpy.ndarray): List of hidden states used in the forward pass\n        learning_rate (float): Learning rate for weight updates\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        \n        dh_next = np.zeros_like(last_hiddens[0])\n        \n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t]\n            db_y += dy\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            \n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh  # tanh derivative\n            \n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t-1].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        \n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n            \n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Parameters\n    input_size = 3\n    hidden_size = 5\n    output_size = 3\n    learning_rate = 0.01\n    \n    # Initialize RNN\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n    \n    # Example sequences\n    input_sequence = [np.random.randn(input_size, 1) for _ in range(10)]\n    expected_output = [np.random.randn(output_size, 1) for _ in range(10)]\n    \n    # Forward pass\n    outputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n    \n    # Backward pass\n    rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n```\n\nThis script defines a `SimpleRNN` class that includes methods for the forward and backward passes of an RNN. The forward pass processes a sequence of inputs and returns the outputs, inputs, and hidden states. The backward pass performs backpropagation through time (BPTT) to update the weights and biases based on the expected output and the calculated loss. The example usage demonstrates how to initialize the RNN, perform a forward pass with random input data, and then perform a backward pass with random expected output data."}
{"task_id": 63, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    \n    r = b - np.dot(A, x)\n    p = r.copy()\n    rsold = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n    \n    return x.round(8).tolist()\n\n# Example usage:\n# A = np.array([[4, 1], [1, 3]])\n# b = np.array([1, 2])\n# solution = conjugate_gradient(A, b, 100)\n# print(solution)\n```\n\nThis script defines the `conjugate_gradient` function, which implements the Conjugate Gradient method to solve a linear system \\(Ax = b\\). The function takes a symmetric, positive-definite matrix \\(A\\), a vector \\(b\\), a maximum number of iterations \\(n\\), an optional initial guess \\(x0\\), and a tolerance level \\(tol\\). The solution vector \\(x\\) is returned as a list rounded to 8 decimal places. The example usage at the bottom demonstrates how to call the function with a simple 2x2 matrix and vector."}
{"task_id": 64, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n\n    # Calculate the frequency of each class\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n\n    # Calculate Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n\n    return round(gini, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y = [1, 1, 2, 2, 2, 3]\n    print(f\"Gini Impurity: {gini_impurity(y)}\")\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```\npython\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n\n    for row in dense_matrix:\n        current_non_zero_count = 0\n        for col_idx, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                current_non_zero_count += 1\n        row_pointer.append(row_pointer[-1] + current_non_zero_count)\n\n    return values, column_indices, row_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 0, 0, 5],\n    [0, 6, 0, 0]\n]\n\nvalues, column_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", values)\nprint(\"Column indices array:\", column_indices)\nprint(\"Row pointer array:\", row_pointer)\n```\n\nThis script defines a function `compressed_row_sparse_matrix` that converts a given dense matrix into its Compressed Row Sparse (CSR) format. The function returns three lists: the values array, the column indices array, and the row pointer array. The example usage demonstrates how to use this function with a sample dense matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```\npython\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n\n    # Convert the input lists to numpy arrays for vector operations\n    v = np.array(v)\n    L = np.array(L)\n\n    # Calculate the dot product of v and L\n    dot_product = np.dot(v, L)\n\n    # Calculate the dot product of L with itself\n    dot_product_L_L = np.dot(L, L)\n\n    # Calculate the scalar projection\n    scalar_projection = dot_product / dot_product_L_L\n\n    # Calculate the vector projection\n    projection = scalar_projection * L\n\n    # Return the projection as a list rounded to three decimal places\n    return projection.round(3).tolist()\n\n# Example usage:\n# v = [3, 4]\n# L = [1, 0]\n# print(orthogonal_projection(v, L))  # Output: [3.0, 0.0]\n```\n\nThis script defines a function `orthogonal_projection` that computes the orthogonal projection of a vector `v` onto a line defined by vector `L`. The function uses numpy for efficient vector operations and returns the result as a list rounded to three decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "```\npython\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return [], [], []\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n\n    return values, row_indices, column_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 5],\n    [0, 6, 0, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\nThis script defines a function `compressed_col_sparse_matrix` that converts a dense matrix into its Compressed Column Sparse (CSC) representation. The function returns a tuple containing the list of non-zero values, their corresponding row indices, and the column pointer array. The example usage demonstrates how to use this function with a sample dense matrix."}
{"task_id": 68, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the input matrix to a numpy array for easier manipulation\n    A = np.array(A, dtype=float)\n    \n    # Perform row reduction to find the row echelon form\n    _, pivot_columns = np.linalg.qr(A, mode='r')\n    pivot_indices = np.where(np.abs(pivot_columns) > 1e-10)[1]\n    \n    # Extract the independent columns from the original matrix\n    basis_vectors = A[:, pivot_indices]\n    \n    # Round the basis vectors to 8 decimal places and convert to list\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    \n    return basis_vectors_rounded\n\n# Example usage:\nA = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nprint(matrix_image(A))\n```\n\nThis script defines the `matrix_image` function, which calculates the column space of a given matrix `A` by identifying the independent columns through row reduction. The function returns the basis vectors that span the column space, rounded to 8 decimal places and converted to a list. The example usage demonstrates how to call the function with a sample matrix `A`."}
{"task_id": 69, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression model.\n    \n    Parameters:\n    y_true (array-like): True values of the target variable.\n    y_pred (array-like): Predicted values of the target variable.\n    \n    Returns:\n    float: R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    return round(r2, 3)\n\n# Example usage:\n# y_true = [3, -0.5, 2, 7]\n# y_pred = [2.5, 0.0, 2, 8]\n# print(r_squared(y_true, y_pred))  # Output should be 0.977\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```\npython\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Initialize variables to calculate total brightness and count of pixels\n    total_brightness = 0\n    pixel_count = 0\n    \n    # Iterate over each row in the image matrix\n    for row in img:\n        # Check if the current row length is inconsistent with the first row\n        if len(row) != len(img[0]):\n            return -1\n        \n        # Iterate over each pixel in the row\n        for pixel in row:\n            # Check if the pixel value is outside the valid range (0-255)\n            if not (0 <= pixel <= 255):\n                return -1\n            \n            # Add the pixel value to the total brightness\n            total_brightness += pixel\n            # Increment the pixel count\n            pixel_count += 1\n    \n    # Calculate the average brightness and round it to two decimal places\n    average_brightness = round(total_brightness / pixel_count, 2)\n    \n    return average_brightness\n\n# Example usage:\n# img = [\n#     [100, 150, 200],\n#     [50, 100, 150],\n#     [0, 50, 100]\n# ]\n# print(calculate_brightness(img))  # Output: 102.78\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays must not be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Calculate RMSE\n    n = y_true.shape[0]\n    residuals = y_true - y_pred\n    rmse_value = np.sqrt(np.sum(residuals ** 2) / n)\n    \n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(rmse(y_true, y_pred))  # Output should be 0.308\n```\n\nThis script defines the `rmse` function, which calculates the Root Mean Square Error between two numpy arrays, `y_true` and `y_pred`. It includes checks for input validity, such as ensuring the inputs are numpy arrays, are not empty, and have the same shape. The RMSE is computed and returned rounded to three decimal places."}
{"task_id": 72, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Ensure both arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    \n    # Handle the edge case where both arrays are all zeros\n    if union == 0:\n        return 1.0  # If both are all zeros, we consider them perfectly overlapping\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 0, 1]\n# y_pred = [1, 0, 0, 0, 1]\n# print(jaccard_index(y_true, y_pred))  # Output: 0.667\n```\n\nThis script defines the `jaccard_index` function, which calculates the Jaccard Index for two binary arrays. It handles edge cases, such as when there is no overlap or when both arrays are all zeros. The result is rounded to three decimal places as specified."}
{"task_id": 73, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Ensure y_true and y_pred are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of y_true and y_pred\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the sum of y_true and y_pred\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    \n    # Handle the edge case where both sums are zero\n    if sum_true_pred == 0:\n        return 1.0  # Perfect score if both are empty\n    \n    # Calculate the Dice Score\n    dice = (2 * intersection) / sum_true_pred\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 0]\n# y_pred = [1, 1, 1, 0, 0, 0]\n# print(dice_score(y_true, y_pred))  # Output should be 0.500\n```\n\nThis script defines the `dice_score` function, which calculates the Dice Score for binary classification tasks. It handles edge cases, such as when there are no true or predicted positives, and returns the result rounded to three decimal places."}
{"task_id": 74, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    def generate_random_hv(seed, dim):\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n    \n    def bind_hvs(hv1, hv2):\n        return np.multiply(hv1, hv2)\n    \n    def bundle_hvs(hvs):\n        return np.sign(np.sum(hvs, axis=0))\n    \n    # Initialize a list to store the bundled hypervectors for each feature\n    bundled_hvs = []\n    \n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_seed = random_seeds.get(feature, 0)  # Default seed is 0 if not provided\n        feature_hv = generate_random_hv(feature_seed, dim)\n        \n        # Generate hypervector for the feature value using the same feature seed\n        value_seed = hash((feature_seed, value))  # Use hash of (feature_seed, value) for reproducibility\n        value_hv = generate_random_hv(value_seed, dim)\n        \n        # Bind the feature name hypervector with the feature value hypervector\n        bound_hv = bind_hvs(feature_hv, value_hv)\n        \n        # Add the bound hypervector to the list\n        bundled_hvs.append(bound_hv)\n    \n    # Bundle all the bound hypervectors to create a composite hypervector for the row\n    composite_hv = bundle_hvs(bundled_hvs)\n    \n    # Convert the composite hypervector to a list and return\n    return composite_hv.tolist()\n```\n\nThis script defines the function `create_row_hv` which generates a composite hypervector for a given dataset row using Hyperdimensional Computing principles. It includes helper functions for generating random hypervectors, binding them, and bundling them to form the final composite hypervector. The composite hypervector is returned as a list."}
{"task_id": 75, "completion_id": 0, "completion": "```\npython\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize counters for the confusion matrix elements\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    # Iterate over each pair of true and predicted labels\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            true_positives += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negatives += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positives += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negatives += 1\n    \n    # Construct the confusion matrix as a list of lists\n    confusion_matrix = [\n        [true_negatives, false_positives],\n        [false_negatives, true_positives]\n    ]\n    \n    return confusion_matrix\n\n# Example usage:\ndata = [\n    [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [0, 1]\n]\nprint(confusion_matrix(data))\n```\n\nThis script defines a function `confusion_matrix` that takes a list of pairs `[y_true, y_pred]` and calculates the confusion matrix for binary classification. The example usage at the end demonstrates how to call the function with sample data."}
{"task_id": 76, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Check if the input vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Check if the input vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    \n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Check if the magnitudes are not zero\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the cosine similarity rounded to three decimal places\n    return round(cosine_sim, 3)\n\n# Example usage:\n# v1 = np.array([1, 2, 3])\n# v2 = np.array([4, 5, 6])\n# print(cosine_similarity(v1, v2))  # Output: 0.974\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```\npython\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    # Ensure the inputs are valid\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n    \n    # Count true positives, true negatives, false positives, false negatives\n    tp = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n    tn = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n    fp = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n    fn = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n    \n    # Construct the confusion matrix\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    \n    # Calculate accuracy\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n    \n    # Calculate F1 Score\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    # Calculate Specificity\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    \n    # Calculate Negative Predictive Value\n    negative_predictive_value = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n    \n    # Return the results rounded to three decimal places\n    return (\n        confusion_matrix,\n        round(accuracy, 3),\n        round(f1_score, 3),\n        round(specificity, 3),\n        round(negative_predictive_value, 3)\n    )\n\n# Example usage:\n# actual = [1, 0, 1, 1, 0, 1]\n# predicted = [1, 0, 0, 1, 0, 1]\n# print(performance_metrics(actual, predicted))\n```\n\nThis script defines the `performance_metrics` function, which calculates the required performance metrics for a binary classification model. It includes input validation, computes the necessary counts, and calculates each metric. The results are returned as a tuple with the confusion matrix and the other metrics rounded to three decimal places."}
{"task_id": 78, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Convert input to a NumPy array if it isn't already\n    data = np.array(data)\n    \n    # Calculate the required statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=False).mode[0]  # stats.mode returns a ModeResult object\n    variance = np.var(data, ddof=0)  # Population variance\n    standard_deviation = np.std(data, ddof=0)  # Population standard deviation\n    percentiles = np.percentile(data, [25, 50, 75])\n    interquartile_range = percentiles[2] - percentiles[0]\n    \n    # Prepare the results dictionary\n    results = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(mode, 4),\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles[0], 4),\n        '50th_percentile': round(percentiles[1], 4),\n        '75th_percentile': round(percentiles[2], 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return results\n\n# Example usage:\n# data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]\n# print(descriptive_statistics(data))\n```\n\nThis script defines a function `descriptive_statistics` that calculates and returns a dictionary of various descriptive statistics for a given dataset. The function handles both lists and NumPy arrays as input and rounds the results to four decimal places where necessary."}
{"task_id": 79, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    \n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# n = 10, k = 3, p = 0.5\nprint(binomial_probability(10, 3, 0.5))  # Output: 0.11719\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\n\n# Example usage:\n# Calculate the PDF of a normal distribution with mean 0 and standard deviation 1 at x = 0.5\nprint(normal_pdf(0.5, 0, 1))  # Output should be approximately 0.35207\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        raise ValueError(\"k must be a non-negative integer\")\n    if lam <= 0:\n        raise ValueError(\"lam must be a positive number\")\n    \n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage:\n# k = 3, lam = 5\n# print(poisson_probability(3, 5))  # Output: 0.14037\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError(\"The input image must be a 2D grayscale image.\")\n    \n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    \n    contrast = max_pixel - min_pixel\n    return contrast\n\n# Example usage:\n# Create a sample grayscale image (2D numpy array)\nsample_image = np.array([[0, 50, 100], [150, 200, 250]], dtype=np.uint8)\n\n# Calculate the contrast of the sample image\ncontrast = calculate_contrast(sample_image)\nprint(f\"The contrast of the sample image is: {contrast}\")\n```\n\nThis script defines a function `calculate_contrast` that computes the contrast of a given grayscale image by finding the difference between the maximum and minimum pixel values. The example usage demonstrates how to use this function with a sample 2D numpy array representing a grayscale image."}
{"task_id": 83, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    vector1 = np.array([1, 2, 3])\n    vector2 = np.array([4, 5, 6])\n    result = calculate_dot_product(vector1, vector2)\n    print(\"Dot product:\", result)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A nested list where each inner list represents the transformed features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the input data to a numpy array for easier manipulation\n    data_array = np.array(data)\n    \n    # Use numpy's polynomial expansion to generate the features\n    # We use np.vander to create a Vandermonde matrix, which is essentially the polynomial features\n    # The columns of the Vandermonde matrix are powers of the input data, starting from the highest degree\n    # We then reverse the columns to start from degree 0\n    transformed_data = np.vander(data_array, degree + 1)[:, ::-1]\n    \n    # Round the result to 8 decimal places\n    transformed_data = np.round(transformed_data, 8)\n    \n    # Convert the numpy array back to a list of lists\n    return transformed_data.tolist()\n\n# Example usage:\n# data = [1.0, 2.0, 3.0]\n# degree = 2\n# print(phi_transform(data, degree))\n# Output: [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0], [1.0, 3.0, 9.0]]\n```\n\nThis script defines the `phi_transform` function, which takes a list of numerical data and a degree as inputs, and returns a nested list of polynomial features. The function uses NumPy's `vander` function to generate the polynomial features and rounds the results to 8 decimal places. If the degree is less than 0, it returns an empty list."}
{"task_id": 85, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the positional encoding matrix\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Compute the positional encodings\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n            if i + 1 < d_model:  # Check to avoid index out of range\n                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n    \n    # Convert to list and return\n    return pe.tolist()\n\n# Example usage:\n# print(pos_encoding(5, 10))\n```\n\nThis script defines a function `pos_encoding` that calculates the positional encodings for a given sequence length (`position`) and model dimensionality (`d_model`). It uses sine and cosine functions as specified in the Transformer architecture. The function returns `-1` if the input conditions are not met. The resulting positional encoding array is converted to `np.float16` and then to a list before being returned."}
{"task_id": 86, "completion_id": 0, "completion": "```\npython\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    else:\n        return 0  # Good fit\n\n# Example usage:\n# print(model_fit_quality(0.9, 0.6))  # Output: 1 (Overfitting)\n# print(model_fit_quality(0.5, 0.4))  # Output: -1 (Underfitting)\n# print(model_fit_quality(0.8, 0.75)) # Output: 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    \n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return updated values rounded to 5 decimal places and converted to lists\n    return parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist()\n\n# Example usage:\n# parameter = [0.0, 0.0]\n# grad = [0.1, 0.2]\n# m = [0.0, 0.0]\n# v = [0.0, 0.0]\n# t = 1\n# updated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\n# print(updated_parameter, updated_m, updated_v)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    norm_x = (x - mean) / np.sqrt(variance + eps)\n    return g * norm_x + b\n\ndef multi_head_attention(q, k, v, mask=None, n_head=12):\n    batch_size, seq_len, d_model = q.shape\n    d_k = d_v = d_model // n_head\n    \n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_v).transpose(0, 2, 1, 3)\n    \n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    if mask is not None:\n        scores += mask\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    output = np.matmul(attention_weights, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\n\ndef feed_forward(x, d_model=10, d_ff=20):\n    x = np.maximum(0, np.dot(x, np.random.rand(d_model, d_ff)) + np.random.rand(d_ff))\n    x = np.dot(x, np.random.rand(d_ff, d_model)) + np.random.rand(d_model)\n    return x\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    input_ids = np.array(input_ids).reshape(1, -1)\n    \n    for _ in range(n_tokens_to_generate):\n        position_ids = np.arange(input_ids.shape[1]).reshape(1, -1)\n        \n        token_embeddings = params[\"wte\"][input_ids]\n        position_embeddings = params[\"wpe\"][position_ids]\n        x = token_embeddings + position_embeddings\n        \n        for block in params[\"blocks\"]:\n            attn_output = multi_head_attention(x, x, x, n_head=hparams[\"n_head\"])\n            attn_output = layer_norm(x + attn_output, block[\"ln_1\"][\"g\"], block[\"ln_1\"][\"b\"])\n            ffn_output = feed_forward(attn_output)\n            x = layer_norm(attn_output + ffn_output, block[\"ln_2\"][\"g\"], block[\"ln_2\"][\"b\"])\n        \n        lm_logits = np.dot(x[:, -1, :], params[\"wte\"].T)\n        next_token = np.argmax(lm_logits, axis=-1).reshape(1, 1)\n        input_ids = np.concatenate([input_ids, next_token], axis=-1)\n    \n    generated_text = encoder.decode(input_ids[0])\n    return generated_text\n\n# Example usage\nprint(gen_text(\"hello\"))\n```\n\nThis script implements a simplified version of GPT-2 text generation. It includes token embeddings, positional embeddings, multi-head attention, a feed-forward network, and layer normalization. The `gen_text` function generates text based on a given prompt. Note that the actual GPT-2 model uses more sophisticated components and a larger vocabulary, but this script provides a basic framework to understand the core concepts."}
{"task_id": 89, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the simplified self-attention mechanism as described in the task. The script includes the `softmax` function and calculates the final weighted pattern for each crystal.\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function\n        exp_values = np.exp(values - np.max(values))  # Subtract max for numerical stability\n        return exp_values / np.sum(exp_values)\n\n    # Convert crystal values to a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values)\n    \n    # Initialize a matrix to hold the attention scores\n    attention_scores = np.zeros((n, n))\n    \n    # Calculate the raw attention scores (dot product of crystal values)\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(crystal_values[i].reshape(1, dimension), \n                                          crystal_values[j].reshape(dimension, 1))\n    \n    # Apply softmax to each row to get the normalized attention scores\n    normalized_attention_scores = np.apply_along_axis(softmax, 1, attention_scores)\n    \n    # Calculate the final weighted pattern for each crystal\n    weighted_patterns = np.dot(normalized_attention_scores, crystal_values)\n    \n    # Calculate the sum of the weighted patterns for each crystal\n    final_patterns = np.sum(weighted_patterns, axis=1)\n    \n    # Return the final patterns rounded to 4 decimal places\n    return [round(pattern, 4) for pattern in final_patterns]\n\n# Example usage:\nn = 3\ncrystal_values = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\ndimension = 3\n\nresult = pattern_weaver(n, crystal_values, dimension)\nprint(result)\n```\n\n### Explanation:\n1. **Softmax Function**: The `softmax` function is implemented to normalize the attention scores. It ensures that the scores for each crystal sum to 1.\n2. **Attention Scores Calculation**: The raw attention scores are calculated using the dot product of each crystal's value vector with every other crystal's value vector.\n3. **Normalization**: The attention scores are normalized using the softmax function for each crystal.\n4. **Weighted Patterns**: The final weighted pattern for each crystal is computed by taking the dot product of the normalized attention scores with the crystal values.\n5. **Output**: The sum of the weighted patterns for each crystal is calculated and returned, rounded to four decimal places.\n\nThis script should work for any number of crystals `n` and any dimension of crystal values, as long as the input format is consistent."}
{"task_id": 90, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Preprocess the corpus and query\n    def preprocess(text):\n        return text.lower().split()\n    \n    corpus = [preprocess(doc) for doc in corpus]\n    query = preprocess(query)\n    \n    # Calculate document frequencies\n    doc_freqs = Counter(term for doc in corpus for term in doc)\n    \n    # Calculate average document length\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    \n    # Calculate BM25 scores\n    scores = []\n    for doc in corpus:\n        doc_len = len(doc)\n        score = 0\n        doc_term_freqs = Counter(doc)\n        for term in query:\n            if term in doc_term_freqs:\n                f = doc_term_freqs[term]\n                n = doc_freqs[term]\n                idf = np.log((len(corpus) - n + 0.5) / (n + 0.5))\n                tf = (f * (k1 + 1)) / (f + k1 * (1 - b + b * (doc_len / avg_doc_len)))\n                score += idf * tf\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage\ncorpus = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Never jump over the lazy dog quickly\",\n    \"A quick brown dog outpaces a quick fox\"\n]\nquery = \"quick brown fox\"\nprint(calculate_bm25_scores(corpus, query))\n```\n\nThis script defines a function `calculate_bm25_scores` that calculates the BM25 scores for a given query against a corpus of documents. The function preprocesses the text, calculates document frequencies, computes the average document length, and then applies the BM25 formula to each document in the corpus. The scores are rounded to three decimal places and returned as a list."}
{"task_id": 91, "completion_id": 0, "completion": "```\npython\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of true labels and predicted labels must be the same.\")\n    \n    # Calculate True Positives, False Positives, and False Negatives\n    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return round(f1_score, 3)\n\n# Example usage:\n# y_true = [0, 1, 0, 1, 1]\n# y_pred = [0, 1, 1, 1, 0]\n# print(calculate_f1_score(y_true, y_pred))  # Output: 0.500\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = []\n    for i in range(1, len(consumption_data) + 1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_value = consumption_data[i - 1] - fluctuation\n        detrended_data.append(detrended_value)\n    \n    # 2) Perform linear regression on the detrended data.\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # 3) Predict day 15's base consumption.\n    day_15_base = model.predict(np.array([[15]]))[0]\n    \n    # 4) Add the day 15 fluctuation back.\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base + day_15_fluctuation\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    \n    # 6) Return the final integer.\n    return final_consumption\n\n# Example usage:\nconsumption_data = [120, 135, 155, 175, 190, 210, 225, 245, 260, 280]\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script defines a function `power_grid_forecast` that takes a list of daily power consumption data, removes the known daily fluctuation, fits a linear regression model to the detrended data, predicts the power consumption for day 15, adds back the fluctuation for that day, and finally applies a 5% safety margin to ensure the colony's future needs are met. The result is returned as an integer."}
{"task_id": 93, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(mae(y_true, y_pred))  # Output should be 0.375\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"Compute Query, Key, and Value matrices.\"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the self-attention mechanism.\"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Compute the multi-head attention mechanism.\"\"\"\n    batch_size, seq_length, d_model = X.shape\n    d_k = d_v = d_model // n_heads\n\n    # Initialize weights for each head\n    W_q_heads = np.array_split(W_q, n_heads, axis=0)\n    W_k_heads = np.array_split(W_k, n_heads, axis=0)\n    W_v_heads = np.array_split(W_v, n_heads, axis=0)\n\n    head_outputs = []\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        head_output = self_attention(Q, K, V)\n        head_outputs.append(head_output)\n\n    # Concatenate all head outputs\n    multi_head_output = np.concatenate(head_outputs, axis=-1)\n    \n    # Reshape to match the original dimensions\n    multi_head_output = multi_head_output.reshape(batch_size, seq_length, d_model)\n    \n    # Round to the nearest 4th decimal and convert to list\n    return multi_head_output.round(4).tolist()\n\n# Example usage:\n# X = np.random.rand(2, 5, 12)  # Batch size of 2, sequence length of 5, model dimension of 12\n# W_q = np.random.rand(12, 12)\n# W_k = np.random.rand(12, 12)\n# W_v = np.random.rand(12, 12)\n# n_heads = 3\n# result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n# print(result)\n```\n\nThis script defines the necessary functions to implement the multi-head attention mechanism as described. The `compute_qkv` function calculates the query, key, and value matrices. The `self_attention` function computes the self-attention for a single head. The `multi_head_attention` function orchestrates the multi-head process, concatenates the results, and returns them as a rounded list."}
{"task_id": 95, "completion_id": 0, "completion": "```\npython\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Both lists must have the same length.\")\n    \n    n = len(x)\n    n11 = sum(xi and yi for xi, yi in zip(x, y))\n    n10 = sum(xi and not yi for xi, yi in zip(x, y))\n    n01 = sum(not xi and yi for xi, yi in zip(x, y))\n    n00 = sum(not xi and not yi for xi, yi in zip(x, y))\n    \n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    \n    if denominator == 0:\n        return 0.0  # To handle the case where any of the marginal totals is zero\n    \n    phi = numerator / denominator\n    return round(phi, 4)\n\n# Example usage:\nx = [1, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0]\nprint(phi_corr(x, y))  # Output: 0.1690\n```\n\nThis script defines a function `phi_corr` that calculates the Phi coefficient for two binary variables provided as lists. It includes error handling for lists of different lengths and a special case where the denominator is zero. The result is rounded to four decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```\npython\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Hard Sigmoid formula: f(x) = max(0, min(1, 0.2 * x + 0.5))\n    return max(0, min(1, 0.2 * x + 0.5))\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-5, -1, 0, 1, 5]\n    results = {x: hard_sigmoid(x) for x in test_values}\n    for x, result in results.items():\n        print(f\"hard_sigmoid({x}) = {result}\")\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)\n\n# Example usage:\n# print(elu(1.0))  # Output: 1.0\n# print(elu(-1.0)) # Output: -0.6321\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```\npython\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x >= 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(prelu(5.0))    # Expected output: 5.0\n    print(prelu(-5.0))   # Expected output: -1.25\n    print(prelu(0.0))    # Expected output: 0.0\n    print(prelu(-2.0, 0.1))  # Expected output: -0.2\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handling edge cases to prevent numerical overflow or underflow\n    if x > 20:  # For large x, softplus(x) is approximately x\n        return round(x, 4)\n    elif x < -20:  # For very negative x, softplus(x) is approximately 0\n        return round(math.log1p(math.exp(x)), 4)\n    else:\n        return round(math.log1p(math.exp(x)), 4)\n\n# Example usage:\nprint(softplus(1.0))   # Expected output: 1.3133\nprint(softplus(-1.0))  # Expected output: 0.3133\nprint(softplus(20.0))  # Expected output: 20.0\nprint(softplus(-20.0)) # Expected output: 0.0\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```\npython\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-10.0, -1.0, 0.0, 1.0, 10.0]\n    for value in test_values:\n        print(f\"Softsign({value}) = {softsign(value)}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Clipping the likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the clipped objective term\n    clipped_term = np.minimum(rhos * A, clipped_rhos * A)\n    \n    # Compute the KL divergence between the old and reference policies\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Compute the GRPO objective\n    grpo_obj = np.mean(clipped_term) - beta * kl_divergence\n    \n    # Return the result rounded to the nearest 6th decimal\n    return round(grpo_obj, 6)\n\n# Example usage:\n# rhos = [1.1, 0.9, 1.2]\n# A = [0.5, -0.3, 0.2]\n# pi_theta_old = [0.3, 0.4, 0.3]\n# pi_theta_ref = [0.4, 0.4, 0.2]\n# print(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\nThis script defines the `grpo_objective` function, which computes the GRPO objective function using the provided parameters. The function uses vectorized operations for efficiency and ensures that the result is rounded to the nearest 6th decimal place. An example usage is commented out at the end of the script."}
{"task_id": 102, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    results = {x: swish(x) for x in test_values}\n    print(results)\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(selu(1.0))  # Example input\n# print(selu(-1.0)) # Example input\n```\n\nThis script defines the SELU activation function as specified, using the given `alpha` and `scale` values. The function computes the SELU value for a given input `x` and returns the result rounded to the nearest 4th decimal place."}
{"task_id": 104, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Convert probabilities to binary predictions based on a threshold of 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return predictions as a python list\n    return predictions.tolist()\n```\n\nThis script defines a function `predict_logistic` that takes an input feature matrix `X`, model weights `weights`, and a model bias `bias`. It computes the logistic regression predictions by first calculating the linear combination of the inputs and weights, applying the sigmoid function to obtain probabilities, and then thresholding these probabilities to make binary predictions. The predictions are returned as a Python list."}
{"task_id": 105, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of samples and features\n    N, M = X.shape\n    # Number of classes\n    C = np.max(y) + 1\n    \n    # Initialize weights randomly\n    B = np.random.randn(M, C)\n    \n    # One-hot encode the labels\n    y_one_hot = np.eye(C)[y]\n    \n    # List to store loss values\n    losses = []\n    \n    # Gradient Descent loop\n    for _ in range(iterations):\n        # Compute the logits\n        logits = np.dot(X, B)\n        \n        # Apply softmax to get probabilities\n        softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        # Compute the gradient of the loss with respect to B\n        gradient = np.dot(X.T, (softmax_probs - y_one_hot))\n        \n        # Update the weights\n        B -= learning_rate * gradient\n        \n        # Compute the loss (Cross Entropy)\n        log_probs = -np.log(softmax_probs[np.arange(N), y])\n        loss = np.sum(log_probs) / N\n        \n        # Append the loss to the list\n        losses.append(round(loss, 4))\n    \n    # Return the updated weights and the list of losses\n    return B.round(4).tolist(), losses\n\n# Example usage:\n# X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n# y = np.array([0, 0, 1, 1])\n# learning_rate = 0.1\n# iterations = 100\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(\"Optimized Coefficients:\", B)\n# print(\"Losses over iterations:\", losses)\n```\n\nThis script defines a function `train_softmaxreg` that performs gradient descent to train a Softmax regression model. It initializes random weights, computes the softmax probabilities, calculates the gradient of the cross-entropy loss, updates the weights, and collects the loss values over the specified number of iterations. The results are rounded to four decimal places and returned as lists."}
{"task_id": 106, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add a column of ones to X for the bias term\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # Initialize weights to zeros\n    weights = np.zeros(X.shape[1])\n    \n    # List to store loss values\n    loss_values = []\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    # Binary Cross Entropy Loss\n    def compute_loss(y_true, y_pred):\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Predicted probabilities\n        y_pred = sigmoid(np.dot(X, weights))\n        \n        # Compute loss\n        loss = compute_loss(y, y_pred)\n        loss_values.append(round(loss, 4))\n        \n        # Compute gradients\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        \n        # Update weights\n        weights -= learning_rate * gradient\n    \n    # Return weights and loss values\n    return weights.tolist(), loss_values\n\n# Example usage:\n# X = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n# y = np.array([0, 0, 0, 1, 1, 1])\n# learning_rate = 0.1\n# iterations = 1000\n# coefficients, losses = train_logreg(X, y, learning_rate, iterations)\n# print(\"Coefficients:\", coefficients)\n# print(\"Losses:\", losses)\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute the dot product between Q and K^T\n    dot_product = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    \n    # Apply the mask\n    dot_product = dot_product + mask\n    \n    # Compute the softmax to get the attention weights\n    attention_weights = np.exp(dot_product - np.max(dot_product, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the values\n    output = np.dot(attention_weights, V)\n    \n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input data\n    X = np.random.rand(4, 5)  # Batch size of 4, sequence length of 5\n    W_q = np.random.rand(5, 3)  # Input dim of 5, hidden dim of 3\n    W_k = np.random.rand(5, 3)\n    W_v = np.random.rand(5, 3)\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Create a mask for the sequence (lower triangular matrix for causal masking)\n    mask = np.triu(np.ones((5, 5)), k=1) * -np.inf\n    \n    # Compute masked self-attention\n    result = masked_attention(Q, K, V, mask)\n    print(result)\n```\n\nThis script defines two functions: `compute_qkv` to compute the Query, Key, and Value matrices, and `masked_attention` to compute the masked self-attention. The example usage at the bottom demonstrates how to use these functions with random input data. The mask used is a lower triangular matrix with `-inf` in the upper triangle to ensure that each position only attends to previous positions, which is typical in tasks like text generation."}
{"task_id": 108, "completion_id": 0, "completion": "```\npython\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    \n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    \n    # Calculate the total number of apples\n    total_apples = len(apples)\n    \n    # Calculate the disorder using the Shannon entropy formula\n    disorder = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        disorder -= probability * (probability * total_apples).bit_length()\n    \n    # Normalize the disorder to a value between 0 and 1\n    max_disorder = (total_apples).bit_length() * len(color_counts)\n    normalized_disorder = disorder / max_disorder if max_disorder > 0 else 0.0\n    \n    # Return the normalized disorder rounded to the nearest 4th decimal\n    return round(normalized_disorder, 4)\n\n# Test cases\nprint(disorder([0, 0, 0, 0]))          # Expected output: 0.0000\nprint(disorder([1, 1, 0, 0]))          # Expected output: > 0.0000\nprint(disorder([0, 1, 2, 3]))          # Expected output: > [1,1,0,0] disorder\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Expected output: > [0,0,0,0,0,1,2,3] disorder\n```\n\nThis script defines a function `disorder` that calculates a measure of disorder in a basket of apples based on their colors using a modified version of Shannon entropy. The function counts the frequency of each color, calculates the probability of each color, and then computes the disorder. The result is normalized and rounded to the nearest 4th decimal place."}
{"task_id": 109, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - normalized_X: list representation of the normalized X rounded to 5 decimal places\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize X\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Scale and shift\n    normalized_X = gamma * X_normalized + beta\n    \n    # Round to 5 decimal places and convert to list\n    return normalized_X.round(5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Batch size = 2, Sequence length = 3, Feature dimension = 4\n    X = np.array([[[1.0, 2.0, 3.0, 4.0],\n                   [5.0, 6.0, 7.0, 8.0],\n                   [9.0, 10.0, 11.0, 12.0]],\n                  [[13.0, 14.0, 15.0, 16.0],\n                   [17.0, 18.0, 19.0, 20.0],\n                   [21.0, 22.0, 23.0, 24.0]]])\n    \n    # Scaling and shifting parameters\n    gamma = np.array([1.0, 1.0, 1.0, 1.0])\n    beta = np.array([0.0, 0.0, 0.0, 0.0])\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import Counter\nfrom math import log\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the reference and candidate\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Calculate unigram matches\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    \n    # Intersection of counts\n    intersection = ref_counter & cand_counter\n    matches = sum(intersection.values())\n    \n    # Precision and Recall\n    precision = matches / len(cand_tokens) if cand_tokens else 0\n    recall = matches / len(ref_tokens) if ref_tokens else 0\n    \n    # F-mean\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    # Calculate chunk penalty\n    def get_chunks(tokens):\n        chunks = []\n        chunk = []\n        for token in tokens:\n            if token in ref_tokens:\n                chunk.append(token)\n            else:\n                if chunk:\n                    chunks.append(chunk)\n                    chunk = []\n        if chunk:\n            chunks.append(chunk)\n        return chunks\n    \n    ref_chunks = get_chunks(ref_tokens)\n    cand_chunks = get_chunks(cand_tokens)\n    \n    # Calculate chunk similarity\n    def chunk_similarity(chunk1, chunk2):\n        common = set(chunk1) & set(chunk2)\n        if not common:\n            return 0\n        return len(common) / (log(len(chunk1) + len(chunk2)) + 1)\n    \n    total_similarity = 0\n    for ref_chunk in ref_chunks:\n        max_similarity = 0\n        for cand_chunk in cand_chunks:\n            similarity = chunk_similarity(ref_chunk, cand_chunk)\n            if similarity > max_similarity:\n                max_similarity = similarity\n        total_similarity += max_similarity\n    \n    # Penalty for word order fragmentation\n    penalty = 0 if not ref_chunks else total_similarity / len(ref_chunks)\n    \n    # METEOR score\n    meteor = (1 - gamma) * f_mean + gamma * penalty\n    \n    return round(meteor, 3)\n\n# Example usage\nreference = \"the cat sat on the mat\"\ncandidate = \"the mat sat on the cat\"\nprint(meteor_score(reference, candidate))\n```\n\nThis script defines a function `meteor_score` that calculates the METEOR score for evaluating machine translation quality. The function takes a reference translation and a candidate translation as input and returns the METEOR score rounded to three decimal places. The example usage at the end demonstrates how to use the function."}
{"task_id": 111, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the joint occurrence count of the two events.\n    - total_counts_x: int, the total occurrence count of the first event.\n    - total_counts_y: int, the total occurrence count of the second event.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate the expected joint probability under the assumption of independence\n    expected_joint_prob = (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    \n    # Calculate the actual joint probability\n    actual_joint_prob = joint_counts / total_samples\n    \n    # Calculate PMI\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    \n    # Return PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\n# joint_counts = 10, total_counts_x = 100, total_counts_y = 50, total_samples = 1000\n# pmi_value = compute_pmi(10, 100, 50, 1000)\n# print(pmi_value)  # Output should be the PMI value rounded to 3 decimal places\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```\npython\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Avoid division by zero if all values are the same\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage:\n# data = [10, 20, 30, 40, 50]\n# print(min_max(data))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    # First weight layer with ReLU activation\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    \n    # Second weight layer with ReLU activation\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    \n    # Shortcut connection and final ReLU activation\n    output = np.maximum(0, a2 + x)\n    \n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n\n# Example usage:\n# Input vector\nx = np.array([1.0, 2.0, 3.0])\n# Weights for the first layer (3x4 matrix)\nw1 = np.array([[0.1, 0.2, 0.3, 0.4],\n               [0.5, 0.6, 0.7, 0.8],\n               [0.9, 1.0, 1.1, 1.2]])\n# Weights for the second layer (4x3 matrix)\nw2 = np.array([[0.13, 0.14, 0.15],\n               [0.16, 0.17, 0.18],\n               [0.19, 0.20, 0.21],\n               [0.22, 0.23, 0.24]])\n\n# Call the residual block function\nresult = residual_block(x, w1, w2)\nprint(result)\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    # Check if the input array has the correct shape\n    if x.ndim != 3:\n        raise ValueError(\"Input array must have 3 dimensions (height, width, channels).\")\n    \n    # Calculate the average across the height and width dimensions for each channel\n    avg_pooled = np.mean(x, axis=(0, 1))\n    \n    return avg_pooled\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a random 3D array to simulate feature maps\n    feature_maps = np.random.rand(10, 10, 3)  # Shape (height=10, width=10, channels=3)\n    \n    # Perform Global Average Pooling\n    result = global_avg_pool(feature_maps)\n    \n    # Print the result\n    print(\"Global Average Pooled Result:\", result)\n    print(\"Shape of Result:\", result.shape)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # X: shape (batch_size, channels, height, width)\n    # gamma: shape (channels,)\n    # beta: shape (channels,)\n    \n    batch_size, channels, height, width = X.shape\n    \n    # Reshape X to (batch_size, channels, height * width)\n    X_reshaped = X.reshape(batch_size, channels, -1)\n    \n    # Calculate mean and variance across the batch and spatial dimensions for each channel\n    mean = X_reshaped.mean(axis=(0, 2), keepdims=True)\n    var = X_reshaped.var(axis=(0, 2), keepdims=True)\n    \n    # Normalize the input\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape X_normalized back to (batch_size, channels, height, width)\n    X_normalized = X_normalized.reshape(batch_size, channels, height, width)\n    \n    # Apply scale (gamma) and shift (beta) parameters\n    Y = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    \n    # Round the final output to 4 decimal places and convert to list\n    return Y.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input\n    X = np.random.randn(2, 3, 4, 4)  # 2 batches, 3 channels, 4x4 images\n    gamma = np.random.randn(3)       # 3 scale parameters, one for each channel\n    beta = np.random.randn(3)        # 3 shift parameters, one for each channel\n    \n    # Perform batch normalization\n    result = batch_normalization(X, gamma, beta)\n    print(result)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```\npython\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    # Using the power rule: d/dx(c * x^n) = c * n * x^(n-1)\n    derivative_value = c * n * (x ** (n - 1))\n    return round(derivative_value, 4)\n\n# Example usage:\n# The derivative of 3 * x^4 at x = 2\nprint(poly_term_derivative(3, 2, 4))  # Output should be 96.0\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    # Convert input list of vectors to a numpy array for easier manipulation\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_vectors = []\n\n    for v in vectors:\n        # Subtract the projection of the current vector onto all previously computed orthonormal vectors\n        for u in orthonormal_vectors:\n            v -= np.dot(v, u) * u\n        \n        # Calculate the norm of the resulting vector\n        norm = np.linalg.norm(v)\n        \n        # If the norm is greater than the tolerance, normalize the vector and add it to the list\n        if norm > tol:\n            orthonormal_vectors.append(v / norm)\n    \n    # Round each orthonormal vector to 4 decimal places and convert to list\n    orthonormal_vectors = [np.round(u, 4).tolist() for u in orthonormal_vectors]\n    \n    return orthonormal_vectors\n\n# Example usage:\n# vectors = [[1, 0], [1, 1]]\n# print(orthonormal_basis(vectors))\n```\n\nThis script defines a function `orthonormal_basis` that computes an orthonormal basis for a given list of 2D vectors using the Gram-Schmidt process. The function takes into account a tolerance level to determine linear independence. The resulting orthonormal vectors are rounded to four decimal places and returned as lists."}
{"task_id": 118, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cross_product(a, b):\n    # Calculate the cross product using numpy's cross function\n    result = np.cross(a, b)\n    # Round the result to 4 decimal places and convert to list\n    return result.round(4).tolist()\n\n# Example usage:\n# vector_a = [1, 2, 3]\n# vector_b = [4, 5, 6]\n# print(cross_product(vector_a, vector_b))  # Output: [-3.0, 6.0, -3.0]\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, the system has no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Get the number of equations\n    n = len(b)\n    \n    # Initialize the solution vector\n    x = np.zeros(n)\n    \n    # Calculate each variable using Cramer's Rule\n    for i in range(n):\n        # Create a matrix A_i by replacing the i-th column of A with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        \n        # Calculate the determinant of A_i\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the i-th variable\n        x[i] = det_A_i / det_A\n    \n    # Round the results to the nearest 4th decimal place\n    x = np.round(x, 4)\n    \n    # Convert the numpy array to a list and return\n    return x.tolist()\n\n# Example usage:\n# A = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]\n# b = [8, -11, -3]\n# print(cramers_rule(A, b))  # Output: [2.0, -1.0, 3.0]\n```\n\nThis script defines a function `cramers_rule` that solves a system of linear equations using Cramer's Rule. It checks if the determinant of the coefficient matrix `A` is zero to determine if there is a unique solution. If there is a unique solution, it calculates each variable by replacing the corresponding column of `A` with the vector `b` and computing the determinant of the resulting matrix. The results are rounded to four decimal places and returned as a list."}
{"task_id": 120, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the inputs are valid\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n    \n    # Convert lists to numpy arrays for efficient computation\n    p = np.array(p)\n    q = np.array(q)\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p * q))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    \n    # Return the Bhattacharyya distance rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage:\n# p = [0.1, 0.4, 0.5]\n# q = [0.2, 0.3, 0.5]\n# print(bhattacharyya_distance(p, q))  # Output: 0.2231\n```\n\nThis script defines a function `bhattacharyya_distance` that calculates the Bhattacharyya distance between two discrete probability distributions. It checks for invalid inputs, computes the Bhattacharyya coefficient, and then derives the distance. The result is rounded to four decimal places as specified."}
{"task_id": 121, "completion_id": 0, "completion": "```\npython\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum of the vectors\n    result = [x + y for x, y in zip(a, b)]\n    \n    return result\n\n# Example usage:\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\nprint(vector_sum(vector_a, vector_b))  # Output: [5, 7, 9]\n\nvector_c = [1, 2]\nvector_d = [3, 4, 5]\nprint(vector_sum(vector_c, vector_d))  # Output: -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Calculate the return (G) for each time step\n        G = 0\n        returns = {}\n        for t in reversed(range(len(episode))):\n            state, action, reward = episode[t]\n            G = reward + G\n            returns[(state, action, t)] = G\n        \n        # Compute the gradient for each time step\n        for t in range(len(episode)):\n            state, action, reward = episode[t]\n            G = returns[(state, action, t)]\n            \n            # Compute the softmax probabilities for the current state\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            \n            # Compute the gradient for the current state-action pair\n            gradient[state, :] += G * (np.eye(num_actions)[action] - probs)\n    \n    # Average the gradient over the number of episodes\n    gradient /= len(episodes)\n    \n    # Round the gradient to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    \n    # Convert to list\n    return gradient.tolist()\n\n# Example usage:\n# theta = np.array([[1.0, 2.0], [3.0, 4.0]])\n# episodes = [[(0, 0, 1.0), (1, 1, 2.0)], [(0, 1, 0.5), (1, 0, 1.5)]]\n# print(compute_policy_gradient(theta, episodes))\n```\n\nThis script defines the `compute_policy_gradient` function, which calculates the policy gradient using the REINFORCE algorithm. It processes a list of episodes, where each episode is a list of (state, action, reward) tuples. The function computes the gradient of the log-policy multiplied by the return for each time step, averages it over all episodes, rounds the result to the nearest 4th decimal, and returns it as a Python list."}
{"task_id": 123, "completion_id": 0, "completion": "```\npython\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # Calculate FLOPs for the dense layer\n    flops_dense = n_experts * d_in * d_out\n    \n    # Calculate FLOPs for the MoE layer\n    flops_moe = k_active * d_in * d_out\n    \n    # Calculate the savings percentage\n    savings_percentage = ((flops_dense - flops_moe) / flops_dense) * 100\n    \n    # Round results to the nearest 1st decimal\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    \n    return flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded\n\n# Example usage:\nn_experts = 32\nk_active = 4\nd_in = 512\nd_out = 256\n\nflops_dense, flops_moe, savings = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"FLOPs for Dense Layer: {flops_dense}\")\nprint(f\"FLOPs for MoE Layer: {flops_moe}\")\nprint(f\"Computational Savings: {savings}%\")\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost in terms of floating-point operations (FLOPs) for both a dense layer and a Mixture-of-Experts (MoE) layer. It then computes the savings percentage and returns these values rounded to the nearest tenth. The example usage demonstrates how to call this function with specific parameters."}
{"task_id": 124, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute the logits for gating\n    logits = np.dot(X, W_g.T)\n    \n    # Add noisy logits\n    noisy_logits = logits + np.dot(N, W_noise.T)\n    \n    # Apply softmax to get initial gating probabilities\n    softmax_scores = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    softmax_probs = softmax_scores / np.sum(softmax_scores, axis=1, keepdims=True)\n    \n    # Perform top-k gating\n    topk_indices = np.argsort(-softmax_probs)[:, :k]\n    topk_mask = np.zeros_like(softmax_probs)\n    np.put_along_axis(topk_mask, topk_indices, 1, axis=1)\n    \n    # Compute the sum of top-k probabilities\n    topk_probs_sum = np.sum(softmax_probs * topk_mask, axis=1, keepdims=True)\n    \n    # Compute the final gating probabilities\n    final_probs = softmax_probs * topk_mask / topk_probs_sum\n    \n    # Round the results to the nearest 4th decimal\n    final_probs_rounded = np.round(final_probs, 4)\n    \n    # Convert to list\n    final_probs_list = final_probs_rounded.tolist()\n    \n    return final_probs_list\n```\n\nThis script defines the `noisy_topk_gating` function which computes the gating probabilities for a Mixture-of-Experts model using the Noisy Top-K mechanism. The function takes input matrix `X`, weight matrices `W_g` and `W_noise`, pre-sampled noise `N`, and sparsity constraint `k`. It returns the final gating probabilities as a Python list, rounded to the nearest 4th decimal."}
{"task_id": 125, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # x: input tensor of shape (batch_size, d_model)\n    # We: expert weight matrices of shape (n_experts, d_model, d_ff)\n    # Wg: gating weight matrix of shape (d_model, n_experts)\n    # n_experts: number of experts\n    # top_k: number of top experts to consider per token\n\n    batch_size, d_model = x.shape\n    _, d_ff = We.shape[1:]\n\n    # Compute gating scores\n    gate_scores = x @ Wg  # Shape: (batch_size, n_experts)\n    gate_scores = np.exp(gate_scores - np.max(gate_scores, axis=1, keepdims=True))  # Softmax for numerical stability\n    gate_probs = gate_scores / np.sum(gate_scores, axis=1, keepdims=True)  # Shape: (batch_size, n_experts)\n\n    # Find top-k experts for each token\n    topk_probs, topk_indices = np.partition(gate_probs, -top_k, axis=1)[:, -top_k:], \\\n                               np.argpartition(gate_probs, -top_k, axis=1)[:, -top_k:]\n\n    # Sort the top-k experts by their probabilities\n    sorted_indices = np.argsort(-topk_probs, axis=1)\n    topk_probs = np.take_along_axis(topk_probs, sorted_indices, axis=1)\n    topk_indices = np.take_along_axis(topk_indices, sorted_indices, axis=1)\n\n    # Initialize output tensor\n    moe_output = np.zeros((batch_size, d_ff))\n\n    # Apply top-k routing and compute weighted outputs\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = topk_indices[i, j]\n            expert_prob = topk_probs[i, j]\n            moe_output[i] += expert_prob * (x[i] @ We[expert_idx])\n\n    # Round the results to the nearest 4th decimal\n    moe_output = np.round(moe_output, 4)\n\n    # Return as a python list\n    return moe_output.tolist()\n\n# Example usage:\n# batch_size = 2, d_model = 3, n_experts = 4, d_ff = 5, top_k = 2\nx = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\nWe = np.random.rand(4, 3, 5)\nWg = np.random.rand(3, 4)\nn_experts = 4\ntop_k = 2\n\noutput = moe(x, We, Wg, n_experts, top_k)\nprint(output)\n```\n\nThis script defines a function `moe` that implements a Mixture-of-Experts (MoE) layer with softmax gating and top-k routing. The function takes an input tensor `x`, expert weight matrices `We`, a gating weight matrix `Wg`, the number of experts `n_experts`, and the number of top experts to consider per token `top_k`. It computes the final MoE output by selecting the top-k experts per token, applying their transformations, and aggregating the results weighted by the normalized gating probabilities. The output is rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 126, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Get the shape of the input tensor\n    B, C, H, W = X.shape\n    \n    # Calculate the number of channels per group\n    channels_per_group = C // num_groups\n    \n    # Reshape X to (B, num_groups, channels_per_group, H, W)\n    X_reshaped = X.reshape(B, num_groups, channels_per_group, H, W)\n    \n    # Calculate the mean and variance for each group\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize X_reshaped\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape X_normalized back to (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply learned scale (gamma) and shift (beta)\n    X_normalized = gamma * X_normalized + beta\n    \n    # Round the results to the nearest 4th decimal\n    X_normalized_rounded = np.round(X_normalized, 4)\n    \n    # Convert the result to a Python list\n    result = X_normalized_rounded.tolist()\n    \n    return result\n\n# Example usage:\n# B=2, C=8, H=4, W=4, num_groups=4\nX = np.random.rand(2, 8, 4, 4)\ngamma = np.random.rand(8)\nbeta = np.random.rand(8)\nnum_groups = 4\n\noutput = group_normalization(X, gamma, beta, num_groups)\nprint(output)\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function f(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    # Define the derivative of the function f'(x)\n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    # Initialize the current position\n    x = start_x\n    \n    # Gradient descent loop\n    for i in range(max_iters):\n        # Calculate the gradient at the current position\n        gradient = df(x)\n        \n        # Update the position\n        new_x = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(new_x - x) < tolerance:\n            break\n        \n        # Update x for the next iteration\n        x = new_x\n    \n    # Return the rounded result to the nearest 4th decimal\n    return round(x, 4)\n\n# Example usage:\n# start_x = np.random.rand() * 10  # Random initial position between 0 and 10\n# print(find_treasure(start_x))\n```\n\nThis script defines a function `find_treasure` that uses gradient descent to find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\). The function takes an initial position `start_x`, a `learning_rate`, a `tolerance` for convergence, and a maximum number of iterations `max_iters`. The result is rounded to the nearest 4th decimal place."}
{"task_id": 128, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the Dynamic Tanh transformation\n    transformed = alpha * np.tanh(gamma * x + beta)\n    \n    # Round the results to the nearest 4th decimal place\n    rounded_transformed = np.round(transformed, 4)\n    \n    # Convert the numpy array to a list\n    result_list = rounded_transformed.tolist()\n    \n    return result_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample input\n    x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    alpha = 1.5\n    gamma = 0.5\n    beta = -0.2\n    \n    # Get the Dynamic Tanh output\n    output = dynamic_tanh(x, alpha, gamma, beta)\n    \n    # Print the output\n    print(output)\n```\n\nThis script defines the `dynamic_tanh` function, which applies the Dynamic Tanh transformation to a given input array `x` using the parameters `alpha`, `gamma`, and `beta`. The results are rounded to the nearest 4th decimal place and returned as a Python list. The example usage demonstrates how to call the function with sample input values."}
