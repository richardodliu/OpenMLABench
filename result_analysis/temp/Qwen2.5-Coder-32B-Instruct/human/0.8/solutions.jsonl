{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 4, "completion_id": 0, "solution": ""}
{"task_id": 5, "completion_id": 0, "solution": ""}
{"task_id": 6, "completion_id": 0, "solution": ""}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 8, "completion_id": 0, "solution": ""}
{"task_id": 9, "completion_id": 0, "solution": ""}
{"task_id": 10, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": ""}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    v = np.array([1.0, 0.0])\n    v = v / np.linalg.norm(v)\n    tol = 1e-10\n    max_iterations = 1000\n    iteration = 0\n    while iteration < max_iterations:\n        Av = A @ v\n        AT = A.T\n        ATAv = AT @ Av\n        v_new = ATAv / np.linalg.norm(AtAv)\n        if np.linalg.norm(v_new - v) < tol:\n            break\n        v = v_new\n        iteration += 1\n    sigma1 = np.linalg.norm(Av)\n    det_A = np.linalg.det(A)\n    sigma2 = np.abs(det_A / sigma1)\n    return (round(sigma1, 4), round(sigma2, 4))\nA = np.array([[1.0, 2.0], [3.0, 4.0]])"}
{"task_id": 13, "completion_id": 0, "solution": ""}
{"task_id": 14, "completion_id": 0, "solution": ""}
{"task_id": 15, "completion_id": 0, "solution": ""}
{"task_id": 16, "completion_id": 0, "solution": ""}
{"task_id": 17, "completion_id": 0, "solution": ""}
{"task_id": 18, "completion_id": 0, "solution": ""}
{"task_id": 19, "completion_id": 0, "solution": ""}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 22, "completion_id": 0, "solution": ""}
{"task_id": 23, "completion_id": 0, "solution": ""}
{"task_id": 24, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 26, "completion_id": 0, "solution": ""}
{"task_id": 27, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 30, "completion_id": 0, "solution": ""}
{"task_id": 31, "completion_id": 0, "solution": ""}
{"task_id": 32, "completion_id": 0, "solution": ""}
{"task_id": 33, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 35, "completion_id": 0, "solution": ""}
{"task_id": 36, "completion_id": 0, "solution": ""}
{"task_id": 37, "completion_id": 0, "solution": ""}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_clf_error = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        for feature_i in range(n_features):\n            feature_values = X[:, feature_i]\n            thresholds = np.unique(feature_values)\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(n_samples)\n                    predictions[feature_values * polarity < threshold * polarity] = -1\n                    weighted_error = np.sum(sample_weights[predictions != y])\n                    if weighted_error > 0.5:\n                        weighted_error = 1 - weighted_error\n                        polarity = -polarity\n                    if weighted_error < best_clf_error:\n                        best_clf_error = weighted_error\n                        best_feature = feature_i\n                        best_threshold = threshold\n                        best_polarity = polarity\n        clf_accuracy = 1 - best_clf_error\n        clf_weight = 0.5 * math.log((1 - best_clf_error) / (best_clf_error + 1e-10))\n        predictions = np.ones(n_samples)\n        feature_values = X[:, best_feature]\n        predictions[feature_values * best_polarity < best_threshold * best_polarity] = -1\n        sample_weights *= np.exp(-clf_weight * y * predictions)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append({'feature': best_feature, 'threshold': best_threshold, 'polarity': best_polarity, 'weight': round(clf_weight, 4)})\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 41, "completion_id": 0, "solution": ""}
{"task_id": 42, "completion_id": 0, "solution": ""}
{"task_id": 43, "completion_id": 0, "solution": ""}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 45, "completion_id": 0, "solution": ""}
{"task_id": 46, "completion_id": 0, "solution": ""}
{"task_id": 47, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 49, "completion_id": 0, "solution": ""}
{"task_id": 50, "completion_id": 0, "solution": ""}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(1, len_source + 1):\n        dp[i, 0] = i\n    for j in range(1, len_target + 1):\n        dp[0, j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            cost = 1 if source[i - 1] != target[j - 1] else 0\n            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i, j] = min(dp[i, j], dp[i - 2, j - 2] + 1)\n    return dp[len_source, len_target]"}
{"task_id": 52, "completion_id": 0, "solution": ""}
{"task_id": 53, "completion_id": 0, "solution": ""}
{"task_id": 54, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 57, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 59, "completion_id": 0, "solution": ""}
{"task_id": 60, "completion_id": 0, "solution": ""}
{"task_id": 61, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 64, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": ""}
{"task_id": 66, "completion_id": 0, "solution": ""}
{"task_id": 67, "completion_id": 0, "solution": ""}
{"task_id": 68, "completion_id": 0, "solution": ""}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 71, "completion_id": 0, "solution": ""}
{"task_id": 72, "completion_id": 0, "solution": ""}
{"task_id": 73, "completion_id": 0, "solution": ""}
{"task_id": 74, "completion_id": 0, "solution": ""}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 78, "completion_id": 0, "solution": ""}
{"task_id": 79, "completion_id": 0, "solution": ""}
{"task_id": 80, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 83, "completion_id": 0, "solution": ""}
{"task_id": 84, "completion_id": 0, "solution": ""}
{"task_id": 85, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 87, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps: float=1e-05):\n    mu = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    x = (x - mu) / np.sqrt(var + eps)\n    return g * x + b\ndef multi_head_attention(q, k, v, n_head: int):\n    (batch_size, seq_len, d_model) = q.shape\n    d_k = d_model // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = weights / np.sum(weights, axis=-1, keepdims=True)\n    context = np.matmul(weights, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return context\ndef feed_forward(x, d_model: int, d_ff: int):\n    w1 = np.random.rand(d_model, d_ff)\n    b1 = np.random.rand(d_ff)\n    w2 = np.random.rand(d_ff, d_model)\n    b2 = np.random.rand(d_model)\n    return np.matmul(np.maximum(np.matmul(x, w1) + b1, 0), w2) + b2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    d_model = params['wte'].shape[1]\n    n_head = hparams['n_head']\n    context_len = hparams['n_ctx']\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[:context_len]\n    input_ids = np.array(input_ids).reshape(1, -1)\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][input_ids]\n        positions = np.arange(input_ids.shape[1])\n        positional_embeddings = params['wpe'][positions]\n        x = token_embeddings + positional_embeddings\n        for _ in params['blocks']:\n            attn_output = multi_head_attention(x, x, x, n_head)\n            attn_output = layer_norm(x + attn_output, params['ln_f']['g'], params['ln_f']['b'])\n            ffn_output = feed_forward(attn_output, d_model, d_model * 4)\n            x = layer_norm(attn_output + ffn_output, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.matmul(x[:, -1], params['wte'].T)\n        next_token = np.argmax(logits, axis=-1)\n        input_ids = np.concatenate([input_ids, next_token.reshape(1, 1)], axis=1)\n        if input_ids.shape[1] > context_len:\n            input_ids = input_ids[:, -context_len:]\n    generated_text = encoder.decode(input_ids[0])\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystal_values = np.array(crystal_values)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(crystal_values[i], crystal_values[j])\n    attention_weights = np.apply_along_axis(softmax, 1, attention_scores)\n    weighted_patterns = np.dot(attention_weights, crystal_values)\n    weighted_patterns_rounded = np.round(weighted_patterns, 4)\n    return weighted_patterns_rounded.tolist()\nn = 3\ncrystal_values = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 91, "completion_id": 0, "solution": ""}
{"task_id": 92, "completion_id": 0, "solution": ""}
{"task_id": 93, "completion_id": 0, "solution": ""}
{"task_id": 94, "completion_id": 0, "solution": ""}
{"task_id": 95, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 97, "completion_id": 0, "solution": ""}
{"task_id": 98, "completion_id": 0, "solution": ""}
{"task_id": 99, "completion_id": 0, "solution": ""}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": ""}
{"task_id": 102, "completion_id": 0, "solution": ""}
{"task_id": 103, "completion_id": 0, "solution": ""}
{"task_id": 104, "completion_id": 0, "solution": ""}
{"task_id": 105, "completion_id": 0, "solution": ""}
{"task_id": 106, "completion_id": 0, "solution": ""}
{"task_id": 107, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": ""}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 110, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 112, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 114, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 116, "completion_id": 0, "solution": ""}
{"task_id": 117, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": ""}
{"task_id": 119, "completion_id": 0, "solution": ""}
{"task_id": 120, "completion_id": 0, "solution": ""}
{"task_id": 121, "completion_id": 0, "solution": ""}
{"task_id": 122, "completion_id": 0, "solution": ""}
{"task_id": 123, "completion_id": 0, "solution": ""}
{"task_id": 124, "completion_id": 0, "solution": ""}
{"task_id": 125, "completion_id": 0, "solution": ""}
{"task_id": 126, "completion_id": 0, "solution": ""}
{"task_id": 127, "completion_id": 0, "solution": ""}
{"task_id": 128, "completion_id": 0, "solution": ""}
