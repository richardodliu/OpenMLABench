{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 4, "completion_id": 0, "solution": ""}
{"task_id": 5, "completion_id": 0, "solution": ""}
{"task_id": 6, "completion_id": 0, "solution": ""}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 8, "completion_id": 0, "solution": ""}
{"task_id": 9, "completion_id": 0, "solution": ""}
{"task_id": 10, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": ""}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    U = np.eye(2)\n    V = np.eye(2)\n    A_hat = A.copy()\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    while iter_count < max_iter:\n        J_U = np.dot(A_hat, V.T) - np.dot(U, np.dot(U.T, np.dot(A_hat, V.T)))\n        J_V = np.dot(A_hat.T, U) - np.dot(V, np.dot(V.T, np.dot(A_hat.T, U)))\n        U += J_U\n        V += J_V\n        (U, _) = np.linalg.qr(U)\n        (V, _) = np.linalg.qr(V)\n        A_hat = np.dot(U.T, np.dot(A, V))\n        if np.allclose(np.dot(U.T, np.dot(A, V)) - np.diag(np.diag(A_hat)), 0, atol=tol):\n            break\n        iter_count += 1\n    singular_values = np.diag(A_hat)\n    singular_values = np.abs(singular_values)\n    sort_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sort_indices]\n    singular_values = np.round(singular_values, 4)\n    return tuple(singular_values)\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 13, "completion_id": 0, "solution": ""}
{"task_id": 14, "completion_id": 0, "solution": ""}
{"task_id": 15, "completion_id": 0, "solution": ""}
{"task_id": 16, "completion_id": 0, "solution": ""}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in final_centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": ""}
{"task_id": 19, "completion_id": 0, "solution": ""}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 22, "completion_id": 0, "solution": ""}
{"task_id": 23, "completion_id": 0, "solution": ""}
{"task_id": 24, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 26, "completion_id": 0, "solution": ""}
{"task_id": 27, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 30, "completion_id": 0, "solution": ""}
{"task_id": 31, "completion_id": 0, "solution": ""}
{"task_id": 32, "completion_id": 0, "solution": ""}
{"task_id": 33, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 35, "completion_id": 0, "solution": ""}
{"task_id": 36, "completion_id": 0, "solution": ""}
{"task_id": 37, "completion_id": 0, "solution": ""}
{"task_id": 38, "completion_id": 0, "solution": ""}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 41, "completion_id": 0, "solution": ""}
{"task_id": 42, "completion_id": 0, "solution": ""}
{"task_id": 43, "completion_id": 0, "solution": ""}
{"task_id": 44, "completion_id": 0, "solution": ""}
{"task_id": 45, "completion_id": 0, "solution": ""}
{"task_id": 46, "completion_id": 0, "solution": ""}
{"task_id": 47, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 49, "completion_id": 0, "solution": ""}
{"task_id": 50, "completion_id": 0, "solution": ""}
{"task_id": 51, "completion_id": 0, "solution": ""}
{"task_id": 52, "completion_id": 0, "solution": ""}
{"task_id": 53, "completion_id": 0, "solution": ""}
{"task_id": 54, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 57, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 59, "completion_id": 0, "solution": ""}
{"task_id": 60, "completion_id": 0, "solution": ""}
{"task_id": 61, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 64, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": ""}
{"task_id": 66, "completion_id": 0, "solution": ""}
{"task_id": 67, "completion_id": 0, "solution": ""}
{"task_id": 68, "completion_id": 0, "solution": ""}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 71, "completion_id": 0, "solution": ""}
{"task_id": 72, "completion_id": 0, "solution": ""}
{"task_id": 73, "completion_id": 0, "solution": ""}
{"task_id": 74, "completion_id": 0, "solution": ""}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 78, "completion_id": 0, "solution": ""}
{"task_id": 79, "completion_id": 0, "solution": ""}
{"task_id": 80, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 83, "completion_id": 0, "solution": ""}
{"task_id": 84, "completion_id": 0, "solution": ""}
{"task_id": 85, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 87, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, d_model) = q.shape\n    d_k = d_model // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\ndef feed_forward(x, d_model, d_ff):\n    w1 = np.random.rand(d_model, d_ff)\n    b1 = np.random.rand(d_ff)\n    w2 = np.random.rand(d_ff, d_model)\n    b2 = np.random.rand(d_model)\n    return np.matmul(np.maximum(np.matmul(x, w1) + b1, 0), w2) + b2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    wte = params['wte']\n    wpe = params['wpe']\n    ln_f_g = params['ln_f']['g']\n    ln_f_b = params['ln_f']['b']\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]\n    for _ in range(n_tokens_to_generate):\n        context_length = len(input_ids)\n        if context_length > n_ctx:\n            input_ids = input_ids[-n_ctx:]\n        positions = np.arange(context_length, dtype=np.int32)\n        token_embeddings = wte[input_ids]\n        positional_embeddings = wpe[positions]\n        x = token_embeddings + positional_embeddings\n        q = x\n        k = x\n        v = x\n        attention_output = multi_head_attention(q, k, v, n_head)\n        x = layer_norm(x + attention_output, ln_f_g, ln_f_b)\n        feed_forward_output = feed_forward(x, x.shape[-1], 20)\n        x = layer_norm(x + feed_forward_output, ln_f_g, ln_f_b)\n        logits = np.matmul(x[-1], wte.T)\n        next_token = np.argmax(logits, axis=-1)\n        input_ids.append(next_token)\n    generated_text = encoder.decode(input_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": ""}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 91, "completion_id": 0, "solution": ""}
{"task_id": 92, "completion_id": 0, "solution": ""}
{"task_id": 93, "completion_id": 0, "solution": ""}
{"task_id": 94, "completion_id": 0, "solution": ""}
{"task_id": 95, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 97, "completion_id": 0, "solution": ""}
{"task_id": 98, "completion_id": 0, "solution": ""}
{"task_id": 99, "completion_id": 0, "solution": ""}
{"task_id": 100, "completion_id": 0, "solution": ""}
{"task_id": 101, "completion_id": 0, "solution": ""}
{"task_id": 102, "completion_id": 0, "solution": ""}
{"task_id": 103, "completion_id": 0, "solution": ""}
{"task_id": 104, "completion_id": 0, "solution": ""}
{"task_id": 105, "completion_id": 0, "solution": ""}
{"task_id": 106, "completion_id": 0, "solution": ""}
{"task_id": 107, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": ""}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 110, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 112, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 114, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 116, "completion_id": 0, "solution": ""}
{"task_id": 117, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": ""}
{"task_id": 119, "completion_id": 0, "solution": ""}
{"task_id": 120, "completion_id": 0, "solution": ""}
{"task_id": 121, "completion_id": 0, "solution": ""}
{"task_id": 122, "completion_id": 0, "solution": ""}
{"task_id": 123, "completion_id": 0, "solution": ""}
{"task_id": 124, "completion_id": 0, "solution": ""}
{"task_id": 125, "completion_id": 0, "solution": ""}
{"task_id": 126, "completion_id": 0, "solution": ""}
{"task_id": 127, "completion_id": 0, "solution": ""}
{"task_id": 128, "completion_id": 0, "solution": ""}
