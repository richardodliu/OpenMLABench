{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [1, 0, -1]\nresult = matrix_dot_vector(matrix, vector)"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    if not matrix or not matrix[0]:\n        return []\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    else:\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[0 for _ in range(len(matrix[0]))] for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[0])):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import cmath\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    lambda1 = (trace + cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    lambda2 = (trace - cmath.sqrt(trace ** 2 - 4 * determinant)) / 2\n    eigenvalues = [float(lambda1.real) if lambda1.imag == 0 else lambda1, float(lambda2.real) if lambda2.imag == 0 else lambda2]\n    eigenvalues.sort(reverse=True)\n    return eigenvalues\nmatrix = [[2, -1], [1, 1]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    try:\n        transformed_matrix = np.dot(np.dot(T_inv, A), S)\n    except ValueError:\n        return -1\n    transformed_matrix = np.round(transformed_matrix, 4)\n    return transformed_matrix.tolist()\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[2, 0], [0, 2]]"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse = [[d / det, -b / det], [-c / det, a / det]]\n    return inverse\nmatrix = [[4, 7], [2, 6]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1)\n    centered_data = data - means[:, np.newaxis]\n    covariance_matrix = np.dot(centered_data, centered_data.T) / (centered_data.shape[1] - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n    sigma1 = np.linalg.norm(A, ord='fro')\n    sigma2 = 0.0\n    tol = 1e-10\n    max_iter = 1000\n    iter_count = 0\n    while iter_count < max_iter:\n        (U, _) = np.linalg.qr(A / sigma1)\n        (V, _) = np.linalg.qr(A.T / sigma1)\n        sigma1_new = np.sqrt(np.sum(U.T @ A @ V) ** 2 + np.sum(U.T @ A @ V[::-1, ::-1]) ** 2) / np.sqrt(2)\n        sigma2_new = np.linalg.norm(A - sigma1_new * U @ V.T)\n        if abs(sigma1 - sigma1_new) < tol and abs(sigma2 - sigma2_new) < tol:\n            break\n        (sigma1, sigma2) = (sigma1_new, sigma2_new)\n        iter_count += 1\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n        (U, V) = (V, U)\n    return (round(sigma1, 4), round(sigma2, 4))\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix.')\n    det = 0\n    for c in range(4):\n        minor = [row[:c] + row[c + 1:] for row in matrix[1:]]\n        det += (-1) ** c * matrix[0][c] * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        hypothesis = X @ theta\n        residuals = hypothesis - y\n        gradient = 1 / m * (X.T @ residuals)\n        theta -= alpha * gradient\n    return theta.round(4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_scaled_data_rounded = np.round(min_max_scaled_data, 4)\n    standardized_data_list = standardized_data_rounded.tolist()\n    min_max_scaled_data_list = min_max_scaled_data_rounded.tolist()\n    return (standardized_data_list, min_max_scaled_data_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in final_centroids]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The feature dataset.\n    y (np.ndarray): The target dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Random seed for shuffling.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if shuffle:\n        indices = np.arange(X.shape[0])\n        np.random.shuffle(indices)\n        X = X[indices]\n        y = y[indices]\n    fold_size = X.shape[0] // k\n    folds = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size if i != k - 1 else X.shape[0]\n        test_indices = np.arange(start, end)\n        train_indices = np.concatenate([np.arange(start), np.arange(end, X.shape[0])])\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4).tolist()\n    return principal_components_rounded"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: Counter) -> float:\n        total_count = sum(class_counts.values())\n        if total_count == 0:\n            return 0\n        return -sum((count / total_count * math.log2(count / total_count) for count in class_counts.values()))\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        class_counts = Counter((example[target_attr] for example in examples))\n        base_entropy = entropy(class_counts)\n        weighted_entropy = 0\n        for value in set((example[attribute] for example in examples)):\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            weighted_entropy += len(subset) / len(examples) * subset_entropy\n        return base_entropy - weighted_entropy\n\n    def choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n        best_attribute = None\n        max_gain = -1\n        for attribute in attributes:\n            gain = information_gain(examples, attribute, target_attr)\n            if gain > max_gain:\n                max_gain = gain\n                best_attribute = attribute\n        return best_attribute\n\n    def majority_value(examples: list[dict], target_attr: str) -> str:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return {}\n    target_values = [example[target_attr] for example in examples]\n    if len(set(target_values)) == 1:\n        return target_values[0]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attribute = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attribute: {}}\n    for value in set((example[best_attribute] for example in examples)):\n        subset = [example for example in examples if example[best_attribute] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    if not np.array_equal(np.unique(labels), np.array([-1, 1])):\n        raise ValueError('Labels must be either -1 or 1.')\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0.0\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def rbf_kernel(x1, x2, sigma=sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n    if kernel == 'linear':\n        K = np.array([[linear_kernel(data[i], data[j]) for j in range(n_samples)] for i in range(n_samples)])\n    elif kernel == 'rbf':\n        K = np.array([[rbf_kernel(data[i], data[j], sigma) for j in range(n_samples)] for i in range(n_samples)])\n    else:\n        raise ValueError(\"Kernel must be either 'linear' or 'rbf'.\")\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        yKalpha = labels * (K @ alpha + b)\n        for i in range(n_samples):\n            if yKalpha[i] < 1:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i] + learning_rate * labels[i]\n                b = b + learning_rate * labels[i]\n            else:\n                alpha[i] = (1 - learning_rate * lambda_val) * alpha[i]\n    alpha_rounded = np.round(alpha, 4).tolist()\n    b_rounded = np.round(b, 4).tolist()\n    return (alpha_rounded, b_rounded)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function of the input z, rounded to four decimal places.\"\"\"\n    output = 1 / (1 + math.exp(-z))\n    return round(output, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    return [round(value, 4) for value in softmax_values]\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predicted_probabilities = []\n    for feature_vector in features:\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_prob = sigmoid(weighted_sum)\n        predicted_probabilities.append(predicted_prob)\n    mse = sum(((p - y) ** 2 for (p, y) in zip(predicted_probabilities, labels))) / len(labels)\n    predicted_probabilities = np.round(predicted_probabilities, 4).tolist()\n    mse = round(mse, 4)\n    return (predicted_probabilities, mse)\nfeatures = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\nlabels = [0, 1, 0]\nweights = [0.5, 0.5]\nbias = 0.1"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mean_squared_error(predictions, labels):\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_predictions = error * sigmoid_derivative(predictions)\n        weights_update = np.dot(features.T, d_predictions) * learning_rate\n        bias_update = np.sum(d_predictions) * learning_rate\n        weights -= weights_update\n        bias -= bias_update\n    return (weights.round(4).tolist(), round(bias, 4), mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = C_matrix @ B_inv\n    P_rounded = np.round(P, 4)\n    P_list = P_rounded.tolist()\n    return P_list\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 1], [1, 2, 3], [1, 3, 6]]\nP = transform_basis(B, C)"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    idx = singular_values.argsort()[::-1]\n    singular_values = singular_values[idx]\n    V = eigenvectors[:, idx]\n    S = np.diag(singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n        else:\n            U[:, i] = np.zeros(2)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The label dataset.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list containing two lists, shuffled X and y datasets reshaped by using numpy's tolist() method.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same number of samples'\n    indices = np.arange(len(X))\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    X_shuffled_list = X_shuffled.tolist()\n    y_shuffled_list = y_shuffled.tolist()\n    return [X_shuffled_list, y_shuffled_list]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset X and optional target y.\n    \n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array (optional), the target data.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (batch_X, batch_y).\n    - If y is not provided, yields batch_X.\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_X = X[indices[start_idx:end_idx]]\n        if y is not None:\n            batch_y = y[indices[start_idx:end_idx]]\n            yield (batch_X.tolist(), batch_y.tolist())\n        else:\n            yield batch_X.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on a feature threshold.\n    \n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value for splitting the dataset.\n    \n    Returns:\n    list: A list containing two subsets of the dataset.\n          The first subset contains samples where the feature value is greater than or equal to the threshold.\n          The second subset contains samples where the feature value is less than the threshold.\n    \"\"\"\n    X = np.array(X)\n    mask = X[:, feature_i] >= threshold\n    subset_above = X[mask]\n    subset_below = X[~mask]\n    return [subset_above.tolist(), subset_below.tolist()]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    poly_features = []\n    for combination in combinations:\n        feature_product = np.ones(n_samples)\n        for index in combination:\n            feature_product *= X[:, index]\n        poly_features.append(feature_product)\n    poly_features = np.column_stack([np.ones(n_samples)] + poly_features)\n    return poly_features.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    one_hot_encoded[np.arange(x.size), x] = 1\n    return one_hot_encoded.tolist()\nx = np.array([0, 1, 2, 1])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array containing the true labels.\n    y_pred (numpy.ndarray): 1D array containing the predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array\n    - Y: Optional 2D numpy array\n    \n    Returns:\n    - Correlation matrix as a 2D python list with values rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        data = X\n    else:\n        data = np.hstack((X, Y))\n    correlation_matrix = np.corrcoef(data, rowvar=False)\n    rounded_correlation_matrix = np.round(correlation_matrix, 4)\n    correlation_matrix_list = rounded_correlation_matrix.tolist()\n    return correlation_matrix_list\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n\n    def calculate_error(weights, predictions, y):\n        return round(np.sum(weights * (predictions != y)) / np.sum(weights), 4)\n\n    def find_best_threshold(X, y, weights):\n        (n_samples, n_features) = X.shape\n        best_threshold = 0\n        best_feature = 0\n        min_error = float('inf')\n        best_predictions = np.zeros(n_samples)\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                predictions = np.where(X[:, feature] < threshold, -1, 1)\n                error = calculate_error(weights, predictions, y)\n                if error < min_error:\n                    min_error = error\n                    best_threshold = threshold\n                    best_feature = feature\n                    best_predictions = predictions.copy()\n        return (best_feature, best_threshold, min_error, best_predictions)\n    (n_samples, n_features) = X.shape\n    weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        (feature, threshold, error, predictions) = find_best_threshold(X, y, weights)\n        if error >= 0.5:\n            break\n        alpha = round(0.5 * math.log((1 - error) / max(error, 1e-10)), 4)\n        weights *= np.exp(-alpha * y * predictions)\n        weights /= np.sum(weights)\n        classifiers.append({'feature': feature, 'threshold': threshold, 'alpha': alpha})\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    softmax_values = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax_values)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    result_list = rounded_log_softmax_values.tolist()\n    return result_list\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        self.W = np.random.uniform(-1 / math.sqrt(self.input_shape[0]), 1 / math.sqrt(self.input_shape[0]), (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        if self.trainable:\n            grad_w = np.dot(self.layer_input.T, accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        input_layer_grad = np.dot(accum_grad, self.W.T)\n        return input_layer_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    padded_matrix = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    (input_height, input_width) = padded_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"Apply the Rectified Linear Unit (ReLU) activation function to a single float input.\"\"\"\n    return z if z > 0 else 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for negative inputs, default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): First input vector.\n    x2 (np.array): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0\n    precision_value = true_positives / (true_positives + false_positives)\n    return precision_value\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y)\n    weights = np.array(weights).reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    X = np.hstack((np.ones((n_samples, 1)), X))\n    for i in range(n_iterations):\n        if method == 'batch':\n            predictions = X.dot(weights)\n            errors = predictions - y.reshape(-1, 1)\n            gradient = X.T.dot(errors) / n_samples\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for j in range(n_samples):\n                xi = X[j, :].reshape(1, -1)\n                yi = y[j]\n                prediction = xi.dot(weights)\n                error = prediction - yi\n                gradient = xi.T.dot(error)\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for j in range(0, n_samples, batch_size):\n                xi = X[j:j + batch_size, :]\n                yi = y[j:j + batch_size]\n                predictions = xi.dot(weights)\n                errors = predictions - yi.reshape(-1, 1)\n                gradient = xi.T.dot(errors) / batch_size\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n    return weights.round(4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    mat = np.array(matrix, dtype=float)\n    (rows, cols) = mat.shape\n    r = 0\n    for c in range(cols):\n        if r >= rows:\n            break\n        pivot_row = r\n        while pivot_row < rows and mat[pivot_row, c] == 0:\n            pivot_row += 1\n        if pivot_row < rows:\n            mat[[r, pivot_row]] = mat[[pivot_row, r]]\n            pivot = mat[r, c]\n            mat[r] /= pivot\n            for i in range(rows):\n                if i != r:\n                    factor = mat[i, c]\n                    mat[i] -= factor * mat[r]\n            r += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = [1.0, 1.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    weights = np.zeros(p)\n    bias = 0.0\n    prev_cost = float('inf')\n    for _ in range(max_iter):\n        predictions = np.dot(X, weights) + bias\n        error = predictions - y\n        gradient_weights = 1 / n * np.dot(X.T, error)\n        gradient_bias = 1 / n * np.sum(error)\n        l1_penalty = alpha * np.sign(weights)\n        gradient_weights += l1_penalty\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n        cost = 1 / (2 * n) * np.sum(error ** 2) + alpha * np.sum(np.abs(weights))\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (weights.round(4).tolist(), round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    dp = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n    for i in range(len(source) + 1):\n        dp[i][0] = i\n    for j in range(len(target) + 1):\n        dp[0][j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[len(source)][len(target)]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n    return round(recall_value, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    dot_product = np.dot(Q, K.T)\n    scaled_dot_product = dot_product / np.sqrt(K.shape[-1])\n    attention_weights = np.softmax(scaled_dot_product, axis=-1)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    output_list = output_rounded.tolist()\n    return output_list\nX = np.random.rand(2, 3, 4)\nW_q = np.random.rand(4, 5)\nW_k = np.random.rand(4, 5)\nW_v = np.random.rand(4, 5)"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h_next = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n        h_prev = h_next\n    return h_prev.round(4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.array([[x, y, 1] for (x, y) in points])\n    translated_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = translated_homogeneous_points[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler (KL) divergence between two normal distributions.\n\n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n\n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('A must be a square matrix')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Number of rows in A must be equal to the number of elements in b')\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x_ini, dtype=float)\n    N = len(b)\n    for _ in range(n):\n        x_new = np.copy(x)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]], dtype=float)\nb = np.array([1, -2, 0], dtype=float)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_states = []\n        h_t = initial_hidden_state.reshape(self.hidden_size, 1)\n        c_t = initial_cell_state.reshape(self.hidden_size, 1)\n        for t in range(len(x)):\n            x_t = x[t].reshape(self.input_size, 1)\n            combined = np.vstack((x_t, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_hat_t = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_t = f_t * c_t + i_t * c_hat_t\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t)\n            cell_states.append(c_t)\n        hidden_states_list = [h.flatten().tolist() for h in hidden_states]\n        hidden_states_list = [list(map(lambda x: round(x, 4), h)) for h in hidden_states_list]\n        final_hidden_state = h_t.flatten().tolist()\n        final_hidden_state = list(map(lambda x: round(x, 4), final_hidden_state))\n        final_cell_state = c_t.flatten().tolist()\n        final_cell_state = list(map(lambda x: round(x, 4), final_cell_state))\n        return (hidden_states_list, final_hidden_state, final_cell_state)\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus should not be empty.')\n    if not query:\n        return [[] for _ in corpus]\n    num_docs = len(corpus)\n    tf = []\n    for doc in corpus:\n        doc_tf = defaultdict(float)\n        if doc:\n            doc_word_count = len(doc)\n            for word in doc:\n                doc_tf[word] += 1\n            for word in doc_tf:\n                doc_tf[word] /= doc_word_count\n        tf.append(doc_tf)\n    df = defaultdict(float)\n    for doc in corpus:\n        for word in set(doc):\n            df[word] += 1\n    idf = {}\n    for word in df:\n        idf[word] = np.log((num_docs + 1) / (df[word] + 1)) + 1\n    tf_idf_scores = []\n    for doc_tf in tf:\n        doc_scores = []\n        for word in query:\n            tf_value = doc_tf.get(word, 0.0)\n            idf_value = idf.get(word, 1.0)\n            tf_idf_scores_word = tf_value * idf_value\n            doc_scores.append(tf_idf_scores_word)\n        tf_idf_scores.append(doc_scores)\n    tf_idf_scores = np.array(tf_idf_scores).round(4).tolist()\n    return tf_idf_scores"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp != 0 else 0\n    recall = tp / (tp + fn) if tp + fn != 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x in input_sequence:\n            x = np.array(x).reshape(-1, 1)\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h_prev)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT to adjust weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(len(input_sequence))):\n            dy = np.array(outputs[t]) - np.array(expected_output[t]).reshape(-1, 1)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else 0\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -1, 1, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def compute_loss(self, outputs, expected_output):\n        \"\"\"\n        Computes the 1/2 * Mean Squared Error (MSE) loss.\n        \"\"\"\n        loss = 0\n        for t in range(len(outputs)):\n            diff = np.array(outputs[t]) - np.array(expected_output[t]).reshape(-1, 1)\n            loss += 0.5 * np.sum(diff * diff)\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - np.dot(A, x)\n    p = r\n    r_k_norm = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = r_k_norm / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        new_r_k_norm = np.dot(r, r)\n        if np.sqrt(new_r_k_norm) < tol:\n            break\n        beta = new_r_k_norm / r_k_norm\n        p = r + beta * p\n        r_k_norm = new_r_k_norm\n    return x.tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    total_samples = len(y)\n    class_counts = np.bincount(y)\n    class_probabilities = class_counts / total_samples\n    gini = 1.0 - np.sum(class_probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                non_zero_count += 1\n        row_pointer.append(row_pointer[-1] + non_zero_count)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 5, 0, 6], [0, 0, 0, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v, dtype=float)\n    L = np.array(L, dtype=float)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    projection_coefficient = dot_product_vL / dot_product_LL\n    projection_vector = projection_coefficient * L\n    return projection_vector.round(3).tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_pointer[col + 1] = len(values)\n    return (values, row_indices, col_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 5, 0, 0], [0, 0, 0, 6]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    A = np.array(A, dtype=float)\n    (_, pivots) = np.linalg.qr(A.T, mode='r', pivoting=True)\n    basis = A[:, pivots[:np.sum(pivots >= 0)]]\n    basis_rounded = np.round(basis, 8)\n    return basis_rounded.tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    mean_y_true = np.mean(y_true)\n    tss = np.sum((y_true - mean_y_true) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    num_columns = len(img[0])\n    for row in img:\n        if len(row) != num_columns:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    average_brightness = round(total_brightness / pixel_count, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('y_true and y_pred must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    residuals = y_true - y_pred\n    mse = np.mean(residuals ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 0.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true & y_pred)\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    if sum_true_pred == 0:\n        return 1.0\n    dice = 2 * intersection / sum_true_pred\n    return round(dice, 3)\ny_true = np.array([1, 0, 1, 1, 0, 0, 1])\ny_pred = np.array([1, 0, 0, 1, 0, 1, 1])"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n\n    def generate_hv(seed, dim):\n        \"\"\"Generates a random hypervector with a given seed and dimension.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], dim)\n\n    def bundle_hvs(hvs):\n        \"\"\"Bundles hypervectors by element-wise multiplication.\"\"\"\n        result = hvs[0]\n        for hv in hvs[1:]:\n            result = np.multiply(result, hv)\n        return result\n    feature_hvs = []\n    for (feature, value) in row.items():\n        feature_name_seed = random_seeds[feature]\n        feature_name_hv = generate_hv(feature_name_seed, dim)\n        feature_value_seed = hash((feature_name_seed, value))\n        feature_value_hv = generate_hv(feature_value_seed, dim)\n        bound_hv = np.multiply(feature_name_hv, feature_value_hv)\n        feature_hvs.append(bound_hv)\n    composite_hv = bundle_hvs(feature_hvs)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    true_positive = 0\n    true_negative = 0\n    false_positive = 0\n    false_negative = 0\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            true_positive += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negative += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positive += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negative += 1\n    matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n    return matrix\ndata = [[1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Vectors cannot have zero magnitude')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positives += 1\n        elif a == 0 and p == 0:\n            true_negatives += 1\n        elif a == 0 and p == 1:\n            false_positives += 1\n        elif a == 1 and p == 0:\n            false_negatives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    if true_positives == 0:\n        f1_score = 0.0\n    else:\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0.0\n    specificity = true_negatives / (true_negatives + false_positives) if true_negatives + false_positives != 0 else 0.0\n    negative_predictive_value = true_negatives / (true_negatives + false_negatives) if true_negatives + false_negatives != 0 else 0.0\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else None\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr = p75 - p25\n    result = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if mode is not None else mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return result"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than 0')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError('k must be a non-negative integer and lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D grayscale image.')\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\nsample_img = np.array([[50, 60, 100], [20, 30, 80], [10, 40, 70]])\ncontrast = calculate_contrast(sample_img)"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data = np.array(data)\n    num_features = data.shape[0]\n    feature_combinations = combinations_with_replacement(range(num_features), degree)\n    transformed_data = []\n    for point in data:\n        polynomial_features = [1]\n        for combo in feature_combinations:\n            feature_product = np.prod(point[list(combo)])\n            polynomial_features.append(feature_product)\n        transformed_data.append(polynomial_features)\n    transformed_data = np.round(transformed_data, 8).tolist()\n    return transformed_data\ndata = [2.0, 3.0]\ndegree = 2"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter.round(5).tolist()\n    updated_m = m.round(5).tolist()\n    updated_v = v.round(5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps: float=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef feed_forward(x, params):\n    hidden = np.tanh(np.matmul(x, params['c_fc']))\n    output = np.matmul(hidden, params['c_proj'])\n    return output\ndef multi_head_attention(x, params, n_head):\n    (B, T, C) = x.shape\n    q = np.matmul(x, params['q_proj'])\n    k = np.matmul(x, params['k_proj'])\n    v = np.matmul(x, params['v_proj'])\n    q = q.reshape(B, T, n_head, C // n_head).transpose(0, 2, 1, 3)\n    k = k.reshape(B, T, n_head, C // n_head).transpose(0, 2, 1, 3)\n    v = v.reshape(B, T, n_head, C // n_head).transpose(0, 2, 1, 3)\n    att = np.matmul(q, k.transpose(0, 1, 3, 2)) * (1.0 / np.sqrt(k.shape[-1]))\n    att = np.exp(att - np.max(att, axis=-1, keepdims=True))\n    att = att / np.sum(att, axis=-1, keepdims=True)\n    y = np.matmul(att, v).transpose(0, 2, 1, 3).reshape(B, T, C)\n    y = np.matmul(y, params['out_proj'])\n    return y\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    params['blocks'].append({'attn': {'q_proj': np.random.rand(10, 10), 'k_proj': np.random.rand(10, 10), 'v_proj': np.random.rand(10, 10), 'out_proj': np.random.rand(10, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}})\n    input_ids = encoder.encode(prompt)\n    input_ids = np.array(input_ids)[:n_ctx]\n    for _ in range(n_tokens_to_generate):\n        if len(input_ids) >= n_ctx:\n            input_ids = input_ids[-n_ctx:]\n        token_embeddings = params['wte'][input_ids]\n        positions = np.arange(len(input_ids))\n        positional_embeddings = params['wpe'][positions]\n        x = token_embeddings + positional_embeddings\n        for block in params['blocks']:\n            attn_output = multi_head_attention(x, block['attn'], n_head)\n            x = x + attn_output\n            x = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n            mlp_output = feed_forward(x, block['mlp'])\n            x = x + mlp_output\n            x = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.matmul(x[-1], params['wte'].T)\n        next_token_id = np.argmax(logits, axis=-1)\n        input_ids = np.append(input_ids, next_token_id)\n    generated_text = encoder.decode(input_ids)\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    weight_matrix = np.random.rand(dimension, dimension)\n    crystal_vectors = np.array([np.full(dimension, value) for value in crystal_values])\n    attention_scores = np.dot(crystal_vectors, weight_matrix.T)\n    attention_weights = np.array([softmax(score) for score in attention_scores])\n    weighted_patterns = np.dot(attention_weights, crystal_vectors)\n    final_patterns = np.sum(weighted_patterns, axis=1)\n    return [round(pattern, 4) for pattern in final_patterns]\ncrystal_values = [1.0, 2.0, 3.0]\ndimension = 5"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n\n    def document_frequency(term):\n        return sum((1 for doc in corpus if term in doc))\n\n    def inverse_document_frequency(term):\n        N = len(corpus)\n        df = document_frequency(term)\n        return np.log((N - df + 0.5) / (df + 0.5))\n    avg_dl = np.mean([len(doc) for doc in corpus])\n    doc_lengths = [len(doc) for doc in corpus]\n    dl_normalizations = [1 - b + b * (dl / avg_dl) for dl in doc_lengths]\n    query_tokens = query.split()\n    scores = []\n    for (doc_index, doc) in enumerate(corpus):\n        doc_score = 0\n        doc_token_counts = Counter(doc)\n        for term in query_tokens:\n            f_qi_D = doc_token_counts.get(term, 0)\n            idf_qi = inverse_document_frequency(term)\n            doc_score += idf_qi * (f_qi_D * (k1 + 1) / (f_qi_D + k1 * dl_normalizations[doc_index]))\n        scores.append(round(doc_score, 3))\n    return scores\ncorpus = [['this', 'is', 'a', 'sample', 'document'], ['this', 'document', 'is', 'another', 'example'], ['information', 'retrieval', 'is', 'interesting', 'and', 'useful']]"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 1))\n    fp = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 0 and pred == 1))\n    fn = sum((1 for (true, pred) in zip(y_true, y_pred) if true == 1 and pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom math import sin, pi\nfrom scipy.stats import linregress\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * sin(2 * PI * i / 10)\n        detrended_data.append(consumption - fluctuation)\n    days = list(range(1, len(consumption_data) + 1))\n    (slope, intercept, r_value, p_value, std_err) = linregress(days, detrended_data)\n    day_15_base_consumption = slope * 15 + intercept\n    day_15_fluctuation = 10 * sin(2 * PI * 15 / 10)\n    day_15_total_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = math.ceil(day_15_total_consumption * 1.05)\n    return final_consumption\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    mae_value = np.mean(np.abs(y_true - y_pred))\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Q, K, V matrices by multiplying the input X with their respective weight matrices.\n    \n    Parameters:\n    X (np.ndarray): Input data of shape (batch_size, seq_length, d_model).\n    W_q (np.ndarray): Weight matrix for Q of shape (d_model, d_k).\n    W_k (np.ndarray): Weight matrix for K of shape (d_model, d_k).\n    W_v (np.ndarray): Weight matrix for V of shape (d_model, d_v).\n    \n    Returns:\n    tuple: A tuple containing Q, K, V matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    Parameters:\n    Q (np.ndarray): Query matrix of shape (batch_size, seq_length, d_k).\n    K (np.ndarray): Key matrix of shape (batch_size, seq_length, d_k).\n    V (np.ndarray): Value matrix of shape (batch_size, seq_length, d_v).\n    \n    Returns:\n    np.ndarray: Output of self-attention mechanism of shape (batch_size, seq_length, d_v).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\ndef softmax(x: np.ndarray, axis: int=-1) -> np.ndarray:\n    \"\"\"\n    Compute the softmax of vector x in a numerically stable way.\n    \n    Parameters:\n    x (np.ndarray): Input array.\n    axis (int): Axis along which to compute the softmax.\n    \n    Returns:\n    np.ndarray: Softmax of the input array.\n    \"\"\"\n    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input data of shape (batch_size, seq_length, d_model).\n    W_q (np.ndarray): Weight matrix for Q of shape (n_heads, d_model, d_k).\n    W_k (np.ndarray): Weight matrix for K of shape (n_heads, d_model, d_k).\n    W_v (np.ndarray): Weight matrix for V of shape (n_heads, d_model, d_v).\n    n_heads (int): Number of attention heads.\n    \n    Returns:\n    list: Output of multi-head attention mechanism reshaped to (batch_size, seq_length, d_model).\n    \"\"\"\n    (batch_size, seq_length, d_model) = X.shape\n    d_k = W_q.shape[-1]\n    d_v = W_v.shape[-1]\n    heads = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        head = self_attention(Q, K, V)\n        heads.append(head)\n    output = np.concatenate(heads, axis=-1)\n    W_o = np.random.rand(n_heads * d_v, d_model)\n    output = np.dot(output, W_o)\n    output = output.reshape(batch_size, seq_length, d_model).round(4)\n    return output.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('The input lists must have the same length.')\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    n = len(x)\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n01 + n00) * (n10 + n00)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 20:\n        return x\n    elif x < -20:\n        return math.exp(x)\n    else:\n        return round(math.log1p(math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_term = np.minimum(rhos * A, clipped_rhos * A)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(clipped_term) - beta * kl_divergence\n    return round(float(grpo_obj), 6)\nrhos = [1.1, 0.9, 1.05, 0.95]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.3, 0.4, 0.2, 0.1]\npi_theta_ref = [0.25, 0.45, 0.2, 0.1]\nepsilon = 0.2\nbeta = 0.01"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    result = x / (1 + math.exp(-x))\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = len(np.unique(y))\n    M = X.shape[1]\n    B = np.random.randn(C, M)\n    y_one_hot = np.eye(C)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y_one_hot * np.log(probs)) / X.shape[0]\n        losses.append(round(float(loss), 4))\n        dscores = probs - y_one_hot\n        dB = np.dot(dscores.T, X) / X.shape[0]\n        B -= learning_rate * dB\n    return (B.round(4).tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Binary Cross Entropy Loss.\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0.0\n    loss_values = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_predicted = sigmoid(linear_model)\n        dw = 1 / X.shape[0] * np.dot(X.T, y_predicted - y)\n        db = 1 / X.shape[0] * np.sum(y_predicted - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = binary_cross_entropy_loss(y, y_predicted)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), bias, loss_values)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    dot_product = np.dot(Q, K.T)\n    d_k = K.shape[-1]\n    scaled_dot_product = dot_product / np.sqrt(d_k)\n    scaled_dot_product = scaled_dot_product + mask\n    attention_weights = softmax(scaled_dot_product)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\ndef softmax(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the softmax of x.\n    \"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / np.sum(e_x, axis=-1, keepdims=True)"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    from collections import Counter\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    disorder_value = 0.0\n    for count in color_counts.values():\n        frequency = count / total_apples\n        disorder_value -= frequency * frequency ** 0.5\n    max_disorder = (1 / len(color_counts)) ** 0.5 * len(color_counts)\n    if max_disorder > 0:\n        disorder_value = -disorder_value / max_disorder\n    return round(disorder_value, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: float, a small constant for numerical stability\n    \n    Returns:\n    - A list representing the layer normalized X, rounded to 5 decimal places.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def _tokenize(text):\n        return text.lower().split()\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n    ref_counts = Counter(ref_tokens)\n    cand_counts = Counter(cand_tokens)\n    overlap = 0\n    for word in ref_counts:\n        overlap += min(ref_counts[word], cand_counts[word])\n    precision = overlap / len(cand_tokens) if cand_tokens else 0\n    recall = overlap / len(ref_tokens) if ref_tokens else 0\n    if precision == 0 and recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n\n    def _get_chunks(tokens):\n        chunks = []\n        chunk = []\n        for token in tokens:\n            if token in ref_counts:\n                chunk.append(token)\n            elif chunk:\n                chunks.append(chunk)\n                chunk = []\n        if chunk:\n            chunks.append(chunk)\n        return chunks\n    ref_chunks = _get_chunks(ref_tokens)\n    cand_chunks = _get_chunks(cand_tokens)\n    chunk_match = 0\n    for chunk in cand_chunks:\n        chunk_str = ' '.join(chunk)\n        for ref_chunk in ref_chunks:\n            ref_chunk_str = ' '.join(ref_chunk)\n            if chunk_str == ref_chunk_str:\n                chunk_match += len(chunk)\n                break\n    if not cand_chunks:\n        penalty = 1\n    else:\n        frag_frac = 1 - chunk_match / len(cand_tokens)\n        penalty = gamma * frag_frac ** alpha\n    meteor = f_mean * (1 - penalty)\n    return round(meteor, 3)\nreference = 'the cat sat on the mat'\ncandidate = 'the dog sat on the mat'"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the joint occurrence count of the two events.\n    - total_counts_x: int, the total occurrence count of event X.\n    - total_counts_y: int, the total occurrence count of event Y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    output = np.maximum(0, a2 + x)\n    return output.round(4).tolist()\nx = np.array([1.0, 2.0, 3.0])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): A 3D NumPy array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: A 1D NumPy array of shape (channels,) where each element is the\n                average of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    pooled_output = np.mean(x, axis=(0, 1))\n    return pooled_output"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized_scaled = gamma.reshape(1, -1, 1, 1) * X_normalized + beta.reshape(1, -1, 1, 1)\n    return X_normalized_scaled.round(4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at the point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    V = np.array(vectors, dtype=float)\n    basis = []\n    for v in V:\n        w = v\n        for u in basis:\n            w = w - np.dot(w, u) * u\n        if np.linalg.norm(w) > tol:\n            u = w / np.linalg.norm(w)\n            basis.append(u)\n    return [u.round(4).tolist() for u in basis]\nvectors = [[1, 2], [3, 4], [5, 6]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(A)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p:\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    return round(bd, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=float)\n    for episode in episodes:\n        G = 0.0\n        returns = []\n        for (_, _, reward) in reversed(episode):\n            G += reward\n            returns.insert(0, G)\n        for ((state, action, _), G) in zip(episode, returns):\n            probabilities = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            for a in range(num_actions):\n                if a == action:\n                    gradient[state, a] += (1 - probabilities[a]) * G\n                else:\n                    gradient[state, a] -= probabilities[a] * G\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = n_experts * d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nn_experts = 32\nk_active = 4\nd_in = 1024\nd_out = 1024"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = X @ W_g + np.sqrt(N) * (X @ W_noise)\n    exp_logits = np.exp(logits)\n    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n    softmax = exp_logits / sum_exp_logits\n    (topk_values, topk_indices) = (np.partition(-softmax, k, axis=1)[:, :k], np.argpartition(-softmax, k, axis=1)[:, :k])\n    gating_probabilities = np.zeros(softmax.shape)\n    np.put_along_axis(gating_probabilities, topk_indices, -topk_values, axis=1)\n    sum_gating_probabilities = np.sum(gating_probabilities, axis=1, keepdims=True)\n    gating_probabilities /= sum_gating_probabilities\n    gating_probabilities_rounded = np.round(gating_probabilities, 4).tolist()\n    return gating_probabilities_rounded"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    (batch_size, input_dim) = x.shape\n    expert_dim = We.shape[1]\n    gates = x @ Wg.T\n    gates = softmax(gates)\n    (top_k_values, top_k_indices) = get_top_k(gates, top_k)\n    moe_output = np.zeros((batch_size, expert_dim))\n    for i in range(batch_size):\n        expert_indices = top_k_indices[i]\n        expert_probs = top_k_values[i]\n        token_output = np.zeros(expert_dim)\n        for (expert_idx, prob) in zip(expert_indices, expert_probs):\n            expert_weights = We[expert_idx * input_dim:(expert_idx + 1) * input_dim]\n            expert_output = x[i] @ expert_weights.T\n            token_output += prob * expert_output\n        moe_output[i] = token_output\n    moe_output = np.round(moe_output, 4)\n    return moe_output.tolist()\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef get_top_k(matrix, k):\n    top_k_values = np.partition(-matrix, k - 1, axis=1)[:, :k]\n    top_k_indices = np.argpartition(-matrix, k - 1, axis=1)[:, :k]\n    sorted_indices = np.argsort(-top_k_values, axis=1)\n    top_k_values = np.take_along_axis(top_k_values, sorted_indices, axis=1)\n    top_k_indices = np.take_along_axis(top_k_indices, sorted_indices, axis=1)\n    top_k_values = -top_k_values\n    return (top_k_values, top_k_indices)"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = X_reshaped.mean(axis=(2, 3, 4), keepdims=True)\n    var = X_reshaped.var(axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = gamma * X_normalized + beta\n    result = np.round(X_normalized, 4).tolist()\n    return result"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    transformed = alpha * np.tanh(gamma * x + beta)\n    rounded_transformed = np.round(transformed, 4)\n    result_list = rounded_transformed.tolist()\n    return result_list"}
