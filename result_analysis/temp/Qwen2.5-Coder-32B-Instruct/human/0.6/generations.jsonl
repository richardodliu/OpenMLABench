{"task_id": 1, "completion_id": 0, "completion": "```\npython\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the number of columns in the matrix matches the length of the vector\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result vector\n    result = []\n    \n    # Compute the dot product for each row in the matrix\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nvector = [1, 0, -1]\n\n# This should output: [2, 4, 6]\nprint(matrix_dot_vector(matrix, vector))\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```\npython\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Check if the input matrix is empty\n    if not a or not a[0]:\n        return []\n    \n    # Use list comprehension to compute the transpose\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    \n    return transposed\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    \n    transposed_matrix = transpose_matrix(matrix)\n    print(\"Original matrix:\")\n    for row in matrix:\n        print(row)\n    \n    print(\"\\nTransposed matrix:\")\n    for row in transposed_matrix:\n        print(row)\n```\n\nThis script defines a function `transpose_matrix` that takes a matrix `a` as input and returns its transpose. The function uses list comprehension to iterate over the columns of the original matrix and create rows for the transposed matrix. The example usage demonstrates how to use the function with a 3x3 matrix."}
{"task_id": 3, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input list to a numpy array\n    array = np.array(a)\n    \n    # Check if the total number of elements matches the new shape\n    if array.size != np.prod(new_shape):\n        return []\n    \n    # Reshape the array and convert it back to a list\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()\n\n# Example usage:\n# matrix = [[1, 2, 3], [4, 5, 6]]\n# new_shape = (3, 2)\n# print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2], [3, 4], [5, 6]]\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```\npython\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    if mode == 'row':\n        # Calculate mean for each row\n        means = [sum(row) / len(row) for row in matrix]\n    else:  # mode == 'column'\n        # Calculate mean for each column\n        num_columns = len(matrix[0])\n        means = [sum(matrix[row][col] for row in range(len(matrix))) / len(matrix) for col in range(num_columns)]\n    \n    return means\n\n# Example usage:\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nprint(\"Mean by row:\", calculate_matrix_mean(matrix, 'row'))    # Output: [2.0, 5.0, 8.0]\nprint(\"Mean by column:\", calculate_matrix_mean(matrix, 'column'))  # Output: [4.0, 5.0, 6.0]\n```\n\nThis script defines a function `calculate_matrix_mean` that computes the mean of a matrix either by row or by column, depending on the specified mode. It includes error handling for invalid modes and provides example usage to demonstrate its functionality."}
{"task_id": 5, "completion_id": 0, "completion": "```\npython\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list of integers or floats representing the resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 3\n    result = scalar_multiply(matrix, scalar)\n    for row in result:\n        print(row)\n```\n\nThis script defines a function `scalar_multiply` that takes a matrix and a scalar as input and returns the matrix after being multiplied by the scalar. The example usage demonstrates how to use this function and prints the resulting matrix."}
{"task_id": 6, "completion_id": 0, "completion": "```\npython\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    # Ensure the matrix is 2x2\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"The input matrix must be a 2x2 matrix.\")\n    \n    # Extract elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant (D) and trace (T) of the matrix\n    D = a * d - b * c\n    T = a + d\n    \n    # Calculate the eigenvalues using the quadratic formula\n    # Eigenvalues are the roots of the characteristic equation: \u03bb^2 - T\u03bb + D = 0\n    import math\n    eigenvalue1 = (T + math.sqrt(T**2 - 4 * D)) / 2\n    eigenvalue2 = (T - math.sqrt(T**2 - 4 * D)) / 2\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\nmatrix = [[2, 1], [1, 2]]\neigenvalues = calculate_eigenvalues(matrix)\nprint(\"Eigenvalues:\", eigenvalues)\n```\n\nThis script defines a function `calculate_eigenvalues` that computes the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest. The example usage demonstrates how to call the function with a sample 2x2 matrix."}
{"task_id": 7, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert input lists to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    \n    # Check if T and S are square matrices\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by calculating their determinants\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    \n    if det_T == 0 or det_S == 0:\n        return -1  # Matrices are not invertible\n    \n    # Calculate the inverse of T and S\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    \n    # Perform the transformation T^{-1}AS\n    try:\n        result = T_inv @ A @ S\n    except ValueError:\n        return -1  # Matrix dimensions do not allow for multiplication\n    \n    # Round the result to the nearest 4th decimal place\n    result = np.round(result, 4)\n    \n    # Convert the result back to a list of lists\n    return result.tolist()\n\n# Example usage:\n# A = [[1, 2], [3, 4]]\n# T = [[1, 0], [0, 1]]\n# S = [[1, 0], [0, 1]]\n# print(transform_matrix(A, T, S))\n```\n\nThis script defines a function `transform_matrix` that takes three matrices $A$, $T$, and $S$ as input, checks if $T$ and $S$ are invertible, and then performs the transformation $T^{-1}AS$. The result is rounded to the nearest 4th decimal and returned as a list of lists. If any of the matrices are not invertible or the dimensions do not allow for the transformation, the function returns -1."}
{"task_id": 8, "completion_id": 0, "completion": "```\npython\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    det = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if det == 0:\n        return None\n    \n    # Calculate the inverse\n    inverse = [\n        [d / det, -b / det],\n        [-c / det, a / det]\n    ]\n    \n    return inverse\n\n# Example usage:\nmatrix = [[1, 2], [3, 4]]\ninverse = inverse_2x2(matrix)\nprint(inverse)  # Output: [[-2.0, 1.0], [1.5, -0.5]]\n\nnon_invertible_matrix = [[1, 2], [2, 4]]\nnon_invertible_inverse = inverse_2x2(non_invertible_matrix)\nprint(non_invertible_inverse)  # Output: None\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```\npython\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Get the number of rows and columns for matrices a and b\n    rows_a, cols_a = len(a), len(a[0])\n    rows_b, cols_b = len(b), len(b[0])\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# a = [[1, 2, 3], [4, 5, 6]]\n# b = [[7, 8], [9, 10], [11, 12]]\n# print(matrixmul(a, b))  # Output: [[58, 64], [139, 154]]\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```\npython\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    import numpy as np\n    \n    # Convert the list of lists into a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Calculate the mean of each feature\n    means = np.mean(data, axis=1, keepdims=True)\n    \n    # Center the data by subtracting the mean from each observation\n    centered_data = data - means\n    \n    # Calculate the covariance matrix\n    # The formula for covariance matrix is (1/(n-1)) * (centered_data) * (centered_data.T)\n    n = data.shape[1]\n    covariance_matrix = (1 / (n - 1)) * np.dot(centered_data, centered_data.T)\n    \n    # Convert the result back to a list of lists\n    return covariance_matrix.tolist()\n\n# Test cases to verify the correctness of the implementation\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2x2 matrix\n    vectors1 = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    expected1 = [\n        [0.6666666666666666, 0.6666666666666666],\n        [0.6666666666666666, 0.6666666666666666]\n    ]\n    result1 = calculate_covariance_matrix(vectors1)\n    assert np.allclose(result1, expected1), f\"Test case 1 failed: {result1}\"\n\n    # Test case 2: 3x3 matrix\n    vectors2 = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12]\n    ]\n    expected2 = [\n        [1.6666666666666667, 1.6666666666666667, 1.6666666666666667],\n        [1.6666666666666667, 1.6666666666666667, 1.6666666666666667],\n        [1.6666666666666667, 1.6666666666666667, 1.6666666666666667]\n    ]\n    result2 = calculate_covariance_matrix(vectors2)\n    assert np.allclose(result2, expected2), f\"Test case 2 failed: {result2}\"\n\n    # Test case 3: Single feature\n    vectors3 = [\n        [1, 2, 3, 4, 5]\n    ]\n    expected3 = [\n        [2.0]\n    ]\n    result3 = calculate_covariance_matrix(vectors3)\n    assert np.allclose(result3, expected3), f\"Test case 3 failed: {result3}\"\n\n    # Test case 4: Two features with zero covariance\n    vectors4 = [\n        [1, 2, 3, 4, 5],\n        [5, 4, 3, 2, 1]\n    ]\n    expected4 = [\n        [2.0, -2.0],\n        [-2.0, 2.0]\n    ]\n    result4 = calculate_covariance_matrix(vectors4)\n    assert np.allclose(result4, expected4), f\"Test case 4 failed: {result4}\"\n\n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\nThis script defines a function `calculate_covariance_matrix` that computes the covariance matrix for a given set of vectors. It also includes a set of test cases to verify the correctness of the implementation. The test cases cover various scenarios, including simple matrices, single features, and features with zero covariance."}
{"task_id": 11, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Check if the input matrix A is square\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Check if the dimensions of A and b are compatible\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"Matrix A and vector b must have the same number of rows.\")\n    \n    # Initialize the solution vector x with zeros\n    x = np.zeros_like(b, dtype=float)\n    \n    # Get the number of equations\n    N = A.shape[0]\n    \n    # Perform n iterations of the Jacobi method\n    for _ in range(n):\n        x_new = np.zeros_like(x, dtype=float)\n        for i in range(N):\n            # Compute the sum of the other terms\n            s = sum(A[i, j] * x[j] for j in range(N) if j != i)\n            # Update the value of x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        \n        # Round the intermediate solution to four decimal places\n        x_new = np.round(x_new, 4)\n        \n        # Update the solution vector\n        x = x_new\n    \n    # Convert the final solution to a list and return\n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\n    b = np.array([4, 7, 3], dtype=float)\n    n = 10  # Number of iterations\n    solution = solve_jacobi(A, b, n)\n    print(\"Approximate solution:\", solution)\n```\n\nThis script defines a function `solve_jacobi` that uses the Jacobi method to solve a system of linear equations. It iterates `n` times, rounding each intermediate solution to four decimal places, and returns the final solution as a Python list. The example usage at the bottom demonstrates how to call the function with a sample matrix `A` and vector `b`."}
{"task_id": 12, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that performs Singular Value Decomposition (SVD) on a 2x2 matrix using the Jacobian method. The script avoids using NumPy's built-in SVD function and returns the singular values rounded to the nearest 4th decimal.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Ensure the input is a 2x2 matrix\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Initialize matrices U and V as identity matrices\n    U = np.eye(2)\n    V = np.eye(2)\n    \n    # Define the tolerance and maximum number of iterations\n    tol = 1e-10\n    max_iter = 1000\n    \n    # Perform the Jacobi method\n    for _ in range(max_iter):\n        # Compute A^T * A\n        ATA = A.T @ A\n        \n        # Compute the off-diagonal elements\n        a = ATA[0, 1]\n        \n        # Compute the rotation matrix\n        if a != 0:\n            c = (ATA[0, 0] - ATA[1, 1]) / (2 * a)\n            t = 1 / (np.abs(c) + np.sqrt(1 + c**2))\n            if c < 0:\n                t = -t\n            c = 1 / np.sqrt(1 + t**2)\n            s = t * c\n        else:\n            c = 1\n            s = 0\n        \n        # Update U and V\n        J = np.array([[c, s], [-s, c]])\n        U = U @ J\n        V = V @ J.T\n        \n        # Update A\n        A = J.T @ A\n        \n        # Check for convergence\n        if np.abs(a) < tol:\n            break\n    \n    # Compute the singular values\n    s1 = np.sqrt(ATA[0, 0])\n    s2 = np.sqrt(ATA[1, 1])\n    \n    # Sort singular values in descending order\n    if s1 < s2:\n        s1, s2 = s2, s1\n        U[:, [0, 1]] = U[:, [1, 0]]\n        V[:, [0, 1]] = V[:, [1, 0]]\n    \n    # Return the singular values rounded to 4 decimal places\n    return (round(s1, 4), round(s2, 4))\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(\"Singular Values:\", singular_values)\n```\n\nThis script defines a function `svd_2x2_singular_values` that takes a 2x2 matrix `A` as input and returns a tuple of its singular values, rounded to the nearest 4th decimal. The function uses the Jacobi method to iteratively diagonalize the matrix \\( A^T A \\) to find the singular values."}
{"task_id": 13, "completion_id": 0, "completion": "```\npython\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    def determinant_3x3(minor: list[list[int|float]]) -> float:\n        a, b, c = minor[0]\n        d, e, f = minor[1]\n        g, h, i = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix.\")\n\n    det = 0\n    for c in range(4):\n        minor = [\n            [matrix[r][c2] for c2 in range(4) if c2 != c]\n            for r in range(1, 4)\n        ]\n        det += ((-1) ** c) * matrix[0][c] * determinant_3x3(minor)\n    \n    return det\n\n# Example usage:\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix))  # Output should be 0, as this is a singular matrix\n```\n\nThis script defines a function `determinant_4x4` that calculates the determinant of a 4x4 matrix using Laplace's Expansion. It includes a helper function `determinant_3x3` to compute the determinant of 3x3 matrices, which are minors of the original matrix. The main function iterates over the first row of the matrix, calculates the determinant of each 3x3 minor, and sums them up with the appropriate sign."}
{"task_id": 14, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    \n    # Calculate the normal equation: theta = (X^T * X)^(-1) * X^T * y\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the numpy array back to a list before returning\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4]]\n# y = [2, 3, 4]\n# print(linear_regression_normal_equation(X, y))\n```\n\nThis script defines a function `linear_regression_normal_equation` that performs linear regression using the normal equation. It takes a matrix `X` and a vector `y` as input, computes the coefficients of the linear regression model, rounds them to four decimal places, and returns them as a list. The example usage at the bottom shows how to call the function with a sample dataset."}
{"task_id": 15, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Initialize the coefficients (theta) to zeros\n    theta = np.zeros(X.shape[1])\n    \n    # Number of training examples\n    m = len(y)\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Calculate the predictions\n        predictions = X.dot(theta)\n        \n        # Calculate the errors\n        errors = predictions - y\n        \n        # Update the coefficients\n        theta -= (alpha / m) * (X.T.dot(errors))\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert to list and return\n    return theta_rounded.tolist()\n\n# Example usage:\n# X should include a column of ones for the intercept term\n# X = np.array([[1, x1_1, x2_1, ..., xn_1],\n#               [1, x1_2, x2_2, ..., xn_2],\n#               ...\n#               [1, x1_m, x2_m, ..., xn_m]])\n# y = np.array([y1, y2, ..., ym])\n# alpha is the learning rate\n# iterations is the number of iterations for gradient descent\n\n# X = np.array([[1, 1], [1, 2], [1, 3]])\n# y = np.array([1, 2, 3])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\nThis script defines a function `linear_regression_gradient_descent` that performs linear regression using gradient descent. It initializes the coefficients to zero, iteratively updates them using the gradient descent algorithm, and returns the rounded coefficients as a list. The example usage at the bottom shows how to call the function with a simple dataset."}
{"task_id": 16, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_data = (data - min_val) / (max_val - min_val)\n    min_max_data_rounded = np.round(min_max_data, 4).tolist()\n    \n    return standardized_data_rounded, min_max_data_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    standardized, min_max = feature_scaling(data)\n    print(\"Standardized Data:\")\n    print(standardized)\n    print(\"Min-Max Normalized Data:\")\n    print(min_max)\n```\n\nThis script defines a function `feature_scaling` that takes a 2D NumPy array as input and returns two lists: one with the data standardized and the other with the data min-max normalized. The results are rounded to the nearest 4th decimal place. The example usage at the bottom demonstrates how to call the function and print the results."}
{"task_id": 17, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Calculate the distance from each point to each centroid\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        \n        # Assign each point to the nearest centroid\n        labels = np.argmin(distances, axis=1)\n        \n        # Calculate new centroids as the mean of all points assigned to each centroid\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = np.round(centroids, 4)\n    \n    # Convert numpy array back to list of tuples\n    return [tuple(centroid) for centroid in final_centroids]\n\n# Example usage:\n# points = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\n# k = 3\n# initial_centroids = [(1.0, 2.0), (5.0, 8.0), (8.0, 8.0)]\n# max_iterations = 100\n# print(k_means_clustering(points, k, initial_centroids, max_iterations))\n```\n\nThis script defines a function `k_means_clustering` that performs k-Means clustering on a set of points. It takes initial centroids and a maximum number of iterations as input and returns the final centroids after the algorithm converges or reaches the maximum number of iterations. The centroids are rounded to the nearest fourth decimal for precision."}
{"task_id": 18, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    - X: np.ndarray, the feature dataset.\n    - y: np.ndarray, the target dataset.\n    - k: int, the number of folds.\n    - shuffle: bool, whether to shuffle the data before splitting.\n    - random_seed: int, optional, seed for the random number generator for reproducibility.\n\n    Returns:\n    - A list of tuples, each containing the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    fold_size = n_samples // k\n    folds = []\n    \n    for i in range(k):\n        start = i * fold_size\n        end = start + fold_size if i != k - 1 else n_samples\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        folds.append((train_indices, test_indices))\n    \n    return folds\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n    y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    \n    folds = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\n    \n    for i, (train_indices, test_indices) in enumerate(folds):\n        print(f\"Fold {i+1}:\")\n        print(f\"  Train indices: {train_indices}\")\n        print(f\"  Test indices: {test_indices}\")\n```\n\nThis script defines a function `k_fold_cross_validation` that takes in a feature dataset `X`, a target dataset `y`, the number of folds `k`, a boolean `shuffle` to determine if the data should be shuffled, and an optional `random_seed` for reproducibility. The function returns a list of tuples, each containing the train and test indices for each fold. The example usage at the bottom demonstrates how to use this function with a sample dataset."}
{"task_id": 19, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the dataset\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    \n    # Step 2: Compute the covariance matrix\n    cov_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Find the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Step 4: Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 5: Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Round the principal components to the nearest 4th decimal\n    principal_components = np.round(principal_components, 4).tolist()\n    \n    return principal_components\n\n# Example usage:\n# data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n# k = 2\n# print(pca(data, k))\n```\n\nThis script defines a function `pca` that performs Principal Component Analysis on a given dataset. It standardizes the data, computes the covariance matrix, finds the eigenvalues and eigenvectors, and returns the top `k` principal components. The results are rounded to the nearest 4th decimal place."}
{"task_id": 20, "completion_id": 0, "completion": "```\npython\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    def entropy(class_counts: Counter) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total) * math.log2(count / total) for count in class_counts.values() if count > 0)\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        class_counts = Counter(example[target_attr] for example in examples)\n        total_entropy = entropy(class_counts)\n        \n        value_counts = Counter(example[attribute] for example in examples)\n        weighted_entropy = 0.0\n        for value in value_counts:\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter(example[target_attr] for example in subset))\n            weighted_entropy += (value_counts[value] / len(examples)) * subset_entropy\n        \n        return total_entropy - weighted_entropy\n\n    def choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n        return max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n\n    def build_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n        class_counts = Counter(example[target_attr] for example in examples)\n        \n        # If all examples have the same class, return a leaf node with that class\n        if len(class_counts) == 1:\n            return next(iter(class_counts))\n        \n        # If there are no attributes left, return a leaf node with the majority class\n        if not attributes:\n            return class_counts.most_common(1)[0][0]\n        \n        # Choose the best attribute to split on\n        best_attr = choose_best_attribute(examples, attributes, target_attr)\n        tree = {best_attr: {}}\n        \n        # Recursively build the subtree for each value of the best attribute\n        for value in set(example[best_attr] for example in examples):\n            subset = [example for example in examples if example[best_attr] == value]\n            if not subset:\n                tree[best_attr][value] = class_counts.most_common(1)[0][0]\n            else:\n                remaining_attributes = [attr for attr in attributes if attr != best_attr]\n                subtree = build_tree(subset, remaining_attributes, target_attr)\n                tree[best_attr][value] = subtree\n        \n        return tree\n\n    return build_tree(examples, attributes, target_attr)\n\n# Example usage:\nexamples = [\n    {'color': 'red', 'size': 'small', 'label': 'apple'},\n    {'color': 'red', 'size': 'medium', 'label': 'apple'},\n    {'color': 'red', 'size': 'large', 'label': 'orange'},\n    {'color': 'orange', 'size': 'small', 'label': 'orange'},\n    {'color': 'orange', 'size': 'medium', 'label': 'orange'},\n    {'color': 'orange', 'size': 'large', 'label': 'orange'},\n    {'color': 'yellow', 'size': 'small', 'label': 'banana'},\n    {'color': 'yellow', 'size': 'medium', 'label': 'banana'},\n    {'color': 'yellow', 'size': 'large', 'label': 'banana'}\n]\n\nattributes = ['color', 'size']\ntarget_attr = 'label'\n\ndecision_tree = learn_decision_tree(examples, attributes, target_attr)\nprint(decision_tree)\n```\n\nThis script defines a function `learn_decision_tree` that constructs a decision tree using recursive binary splitting based on entropy and information gain. The function takes a list of examples, a list of attribute names, and the target attribute name as input and returns a nested dictionary representing the decision tree. The example usage demonstrates how to use the function with a small dataset."}
{"task_id": 21, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n    \n    def rbf_kernel(x1, x2, sigma=1.0):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * (sigma ** 2)))\n    \n    def kernel_function(x1, x2):\n        if kernel == 'linear':\n            return linear_kernel(x1, x2)\n        elif kernel == 'rbf':\n            return rbf_kernel(x1, x2, sigma)\n        else:\n            raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n    \n    n_samples, n_features = data.shape\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    \n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            xi = data[i]\n            yi = labels[i]\n            prediction = sum(alphas[j] * yi * kernel_function(data[j], xi) for j in range(n_samples)) + bias\n            hinge_loss = max(0, 1 - yi * prediction)\n            \n            if hinge_loss > 0:\n                alphas[i] += learning_rate * yi\n                bias += learning_rate * yi\n        \n        alphas = np.clip(alphas, 0, 1.0 / (lambda_val * n_samples))\n    \n    alphas = np.round(alphas, 4).tolist()\n    bias = np.round(bias, 4)\n    \n    return alphas, bias\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 3], [2, 1], [3, 2]])\n# labels = np.array([1, 1, 1, -1, -1])\n# alphas, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.1, iterations=100, sigma=1.0)\n# print(\"Alphas:\", alphas)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `pegasos_kernel_svm` that implements a deterministic version of the Pegasos algorithm for training a kernel SVM classifier. The function supports both linear and RBF kernels and returns the model's alpha coefficients and bias, rounded to the nearest 4th decimal."}
{"task_id": 22, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"Compute the sigmoid activation function for a given input z.\"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(sigmoid(0))    # Output: 0.5\n    print(sigmoid(1))    # Output: 0.7311\n    print(sigmoid(-1))   # Output: 0.2689\n    print(sigmoid(10))   # Output: 1.0\n    print(sigmoid(-10))  # Output: 0.0\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Calculate the maximum score to avoid overflow\n    max_score = max(scores)\n    # Calculate the exponentials of the scores minus the max score for numerical stability\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    # Calculate the sum of the exponentials\n    sum_exp_scores = sum(exp_scores)\n    # Calculate the softmax values\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    # Round each softmax value to four decimal places\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded\n\n# Example usage:\nscores = [2.0, 1.0, 0.1]\nprint(softmax(scores))  # Output: [0.6590, 0.2424, 0.0986]\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Initialize predictions list\n    predictions = []\n    \n    # Calculate the weighted sum for each feature vector and apply sigmoid activation\n    for feature_vector in features:\n        weighted_sum = sum(w * x for w, x in zip(weights, feature_vector)) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    \n    # Calculate mean squared error\n    mse = sum((p - y) ** 2 for p, y in zip(predictions, labels)) / len(labels)\n    \n    # Round predictions and mse to 4 decimal places\n    predictions_rounded = [round(p, 4) for p in predictions]\n    mse_rounded = round(mse, 4)\n    \n    # Return predictions as a list and mse as a float\n    return predictions_rounded, mse_rounded\n\n# Example usage:\nfeatures = [[0.5, 0.3], [0.2, 0.8], [0.7, 0.4]]\nlabels = [0, 1, 0]\nweights = [0.1, 0.2]\nbias = 0.05\n\npredicted_probabilities, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probabilities)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines a `single_neuron_model` function that simulates a single neuron with a sigmoid activation function for binary classification. It calculates the predicted probabilities for each input feature vector and computes the mean squared error between these predictions and the true labels. All results are rounded to four decimal places."}
{"task_id": 25, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    return x * (1 - x)\n\ndef mean_squared_error(predictions: np.ndarray, labels: np.ndarray) -> float:\n    return np.mean((predictions - labels) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n\n    for epoch in range(epochs):\n        # Forward pass\n        linear_combination = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_combination)\n\n        # Calculate loss\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n\n        # Backward pass\n        error = predictions - labels\n        d_loss_d_pred = 2 * error / len(labels)\n        d_pred_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_pred * d_pred_d_z\n\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n\n        # Update weights and bias\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\n# features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n# labels = np.array([0, 1, 1, 0])\n# initial_weights = np.array([0.5, 0.5])\n# initial_bias = 0.5\n# learning_rate = 0.1\n# epochs = 1000\n# updated_weights, updated_bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n# print(\"Updated Weights:\", updated_weights)\n# print(\"Updated Bias:\", updated_bias)\n# print(\"MSE Values:\", mse_values)\n```\n\nThis script defines a function `train_neuron` that simulates a single neuron with a sigmoid activation function and uses backpropagation to update its weights and bias based on the mean squared error loss. The function returns the updated weights, bias, and a list of MSE values for each epoch, all rounded to four decimal places."}
{"task_id": 26, "completion_id": 0, "completion": "```\npython\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n# Example usage:\na = Value(-4.0)\nb = Value(2.0)\nc = a + b\nd = Value(3.0) * (b - a)\ne = c * d\nf = e.relu()\nf.backward()\n\nprint(f)  # Should print the value of f and its gradient\nprint(a.grad)  # Should print the gradient of f with respect to a\nprint(b.grad)  # Should print the gradient of f with respect to b\nprint(c.grad)  # Should print the gradient of f with respect to c\nprint(d.grad)  # Should print the gradient of f with respect to d\nprint(e.grad)  # Should print the gradient of f with respect to e\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    # Convert the basis vectors to numpy arrays\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    \n    # Compute the transformation matrix P from B to C\n    P = C_matrix @ np.linalg.inv(B_matrix)\n    \n    # Round the result to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the result to a list and return\n    return P_rounded.tolist()\n\n# Example usage:\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # Standard basis\nC = [[1, 1, 1], [1, 2, 3], [1, 3, 6]]  # Another basis\nP = transform_basis(B, C)\nprint(P)\n```\n\nThis script defines a function `transform_basis` that computes the transformation matrix from basis B to basis C. It uses NumPy for matrix operations and ensures the result is rounded to four decimal places before returning it as a Python list."}
{"task_id": 28, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Step 1: Compute A^T A\n    A_transpose_A = np.dot(A.T, A)\n    \n    # Step 2: Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(A_transpose_A)\n    \n    # Step 3: Construct the singular values as the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Sort singular values in descending order and reorder eigenvectors accordingly\n    sorted_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Step 4: Use the eigenvectors of A^T A as the columns of V\n    V = eigenvectors\n    \n    # Step 5: Compute U by normalizing A * v_i for each non-zero singular value\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n        else:\n            # If the singular value is zero, we can choose any orthogonal vector\n            # For simplicity, we can use the cross product with the first column of U\n            if i == 1:\n                U[:, i] = np.cross(U[:, 0], np.array([1, 0]))\n                U[:, i] /= np.linalg.norm(U[:, i])\n    \n    # Step 6: Form the diagonal matrix S with singular values\n    S = np.diag(singular_values)\n    \n    # Round results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to list\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    \n    return U_list, S_list, V_list\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines a function `svd_2x2` that computes the Singular Value Decomposition (SVD) of a 2x2 matrix using the eigendecomposition method. The function returns the matrices U, S, and V, each rounded to the nearest 4th decimal and converted to a Python list."}
{"task_id": 29, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in the numpy arrays X and y while maintaining\n    the correspondence between them. An optional seed can be provided\n    for reproducibility.\n\n    Parameters:\n    X (numpy.ndarray): The features dataset.\n    y (numpy.ndarray): The labels dataset.\n    seed (int, optional): Seed for random number generator.\n\n    Returns:\n    list: A list of shuffled data in the form [X_shuffled, y_shuffled].\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Get the number of samples\n    num_samples = X.shape[0]\n    \n    # Create an array of indices and shuffle them\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    \n    # Shuffle X and y using the shuffled indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert to list and return\n    return [X_shuffled.tolist(), y_shuffled.tolist()]\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    shuffled_data = shuffle_data(X, y, seed=42)\n    print(\"Shuffled X:\", shuffled_data[0])\n    print(\"Shuffled y:\", shuffled_data[1])\n```\n\nThis script defines a function `shuffle_data` that takes two numpy arrays `X` and `y`, and an optional `seed` for reproducibility. It shuffles the rows of `X` and `y` while maintaining the correspondence between them, and returns the shuffled data as a list of lists. The example usage at the bottom demonstrates how to use this function."}
{"task_id": 30, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the dataset X and optionally y.\n    \n    Parameters:\n    - X: numpy array, the dataset features.\n    - y: numpy array (optional), the dataset labels.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch) as lists.\n    - If y is not provided, yields X_batch as a list.\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)  # Shuffle the indices to randomize the batches\n    \n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n        \n        if y is not None:\n            yield (X[batch_indices].tolist(), y[batch_indices].tolist())\n        else:\n            yield X[batch_indices].tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.random.rand(100, 10)  # 100 samples, 10 features each\n    y = np.random.randint(0, 2, 100)  # 100 binary labels\n\n    # Create batch iterator\n    batch_gen = batch_iterator(X, y, batch_size=16)\n\n    # Print batches\n    for batch in batch_gen:\n        print(batch)\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of the specified feature\n    is greater than or equal to the given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for dividing the dataset.\n    threshold (float): The threshold value for dividing the dataset.\n\n    Returns:\n    tuple: Two lists, the first with samples where the feature value is >= threshold,\n           and the second with samples where the feature value is < threshold.\n    \"\"\"\n    # Convert input to numpy array if it's not already\n    X = np.array(X)\n    \n    # Find indices where the feature value is greater than or equal to the threshold\n    indices_above = np.where(X[:, feature_i] >= threshold)[0]\n    indices_below = np.where(X[:, feature_i] < threshold)[0]\n    \n    # Split the dataset into two subsets based on the indices\n    subset_above = X[indices_above]\n    subset_below = X[indices_below]\n    \n    # Convert the numpy arrays to lists\n    subset_above_list = subset_above.tolist()\n    subset_below_list = subset_below.tolist()\n    \n    return subset_above_list, subset_below_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dataset\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9],\n                  [10, 11, 12]])\n    \n    feature_i = 1  # Use the second feature\n    threshold = 6  # Threshold value\n    \n    subset_above, subset_below = divide_on_feature(X, feature_i, threshold)\n    \n    print(\"Subset with feature value >= threshold:\", subset_above)\n    print(\"Subset with feature value < threshold:\", subset_below)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    # Generate combinations of features up to the specified degree\n    combinations = combinations_with_replacement(range(n_features), degree)\n    \n    # Prepare a list to store the polynomial features\n    poly_features = []\n    \n    for combo in combinations:\n        # For each combination, compute the product of the features raised to the specified powers\n        feature_product = np.prod(X[:, combo], axis=1)\n        poly_features.append(feature_product)\n    \n    # Stack all polynomial features into a single array\n    poly_features = np.column_stack(poly_features)\n    \n    # Include the bias term (x^0 = 1)\n    poly_features = np.hstack((np.ones((n_samples, 1)), poly_features))\n    \n    # Return the polynomial features as a list\n    return poly_features.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4]])\n# degree = 2\n# print(polynomial_features(X, degree))\n```\n\nThis script defines a function `polynomial_features` that generates polynomial features for a given dataset `X` up to a specified `degree`. The function uses `combinations_with_replacement` to generate all combinations of features, computes the product of these features for each combination, and then stacks them into a single array. The bias term (a column of ones) is also included. Finally, the function returns the polynomial features as a Python list."}
{"task_id": 33, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n    \n    # Get the number of samples in the dataset\n    n_samples = X.shape[0]\n    \n    # Initialize a list to store the subsets\n    subsets = []\n    \n    for _ in range(n_subsets):\n        # Randomly select indices for the subset\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        \n        # Create the subset using the selected indices\n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Append the subset as a tuple to the list\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    # Generate 3 random subsets with replacements\n    subsets_with_replacements = get_random_subsets(X, y, n_subsets=3, replacements=True)\n    print(\"Subsets with replacements:\")\n    for subset in subsets_with_replacements:\n        print(subset)\n    \n    # Generate 3 random subsets without replacements\n    subsets_without_replacements = get_random_subsets(X, y, n_subsets=3, replacements=False)\n    print(\"\\nSubsets without replacements:\")\n    for subset in subsets_without_replacements:\n        print(subset)\n```\n\nThis script defines a function `get_random_subsets` that generates random subsets of a given dataset. The function takes a 2D numpy array `X`, a 1D numpy array `y`, the number of subsets `n_subsets`, a boolean `replacements` to indicate whether to sample with or without replacement, and an optional `seed` for reproducibility. The function returns a list of tuples, where each tuple contains a subset of `X` and `y` as lists. The example usage demonstrates how to use the function with both replacement and without replacement."}
{"task_id": 34, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    # Determine the number of columns if not provided\n    if n_col is None:\n        n_col = np.max(x) + 1\n    \n    # Initialize the one-hot encoded array with zeros\n    one_hot_encoded = np.zeros((x.size, n_col), dtype=int)\n    \n    # Fill in the appropriate indices with ones\n    one_hot_encoded[np.arange(x.size), x] = 1\n    \n    # Convert the numpy array to a list and return\n    return one_hot_encoded.tolist()\n\n# Example usage:\n# x = np.array([0, 1, 2, 1, 0])\n# print(to_categorical(x))\n# Output: [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D numpy array\n    diagonal_matrix = np.diag(x)\n    # Convert the resulting 2D numpy array to a list\n    return diagonal_matrix.tolist()\n\n# Example usage:\n# x = np.array([1, 2, 3])\n# print(make_diagonal(x))\n# Output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```"}
{"task_id": 36, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    \n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\n\n# Example usage:\n# y_true = np.array([0, 1, 2, 2, 0, 1])\n# y_pred = np.array([0, 2, 1, 2, 0, 0])\n# print(accuracy_score(y_true, y_pred))  # Output: 0.6667\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    # If Y is not provided, calculate the correlation matrix of X with itself\n    if Y is None:\n        Y = X\n    \n    # Calculate the correlation matrix using numpy's corrcoef function\n    correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    \n    # Convert the numpy array to a list\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    \n    return correlation_matrix_list\n\n# Example usage:\n# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# Y = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n# print(calculate_correlation_matrix(X))\n# print(calculate_correlation_matrix(X, Y))\n```\n\nThis script defines a function `calculate_correlation_matrix` that computes the correlation matrix for a given dataset. It handles both cases where a second dataset `Y` is provided and where it is not, defaulting to computing the correlation matrix of `X` with itself. The results are rounded to four decimal places and returned as a Python list."}
{"task_id": 38, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_clf = {}\n        min_error = float('inf')\n\n        for feature_i in range(n_features):\n            feature_values = np.unique(X[:, feature_i])\n            thresholds = (feature_values[:-1] + feature_values[1:]) / 2.0\n\n            for threshold in thresholds:\n                p = 1\n                predictions = np.ones(n_samples)\n                predictions[X[:, feature_i] < threshold] = -1\n\n                error = sum(sample_weights[y != predictions])\n\n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n\n                if error < min_error:\n                    min_error = error\n                    best_clf = {\n                        'feature_index': feature_i,\n                        'threshold': threshold,\n                        'polarity': p\n                    }\n\n        clf_weight = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(n_samples)\n        feature_i = best_clf['feature_index']\n        threshold = best_clf['threshold']\n        p = best_clf['polarity']\n\n        predictions[X[:, feature_i] < threshold] = -1\n        predictions *= p\n\n        sample_weights *= np.exp(-clf_weight * y * predictions)\n        sample_weights /= np.sum(sample_weights)\n\n        best_clf['alpha'] = clf_weight\n        classifiers.append(best_clf)\n\n    return [{k: round(v, 4) if isinstance(v, float) else v for k, v in clf.items()} for clf in classifiers]\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 1, -1, -1])\n# n_clf = 3\n# print(adaboost_fit(X, y, n_clf))\n```\n\nThis script defines the `adaboost_fit` function, which implements the AdaBoost algorithm to fit a classifier to the given dataset. The function initializes sample weights, iteratively selects the best weak classifier (decision stump) for each iteration, updates the sample weights, and stores the classifier parameters. The final list of classifiers is returned, with all floating-point values rounded to four decimal places."}
{"task_id": 39, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the input list to a numpy array\n    scores = np.array(scores)\n    \n    # For numerical stability, subtract the max score from all scores\n    max_score = np.max(scores)\n    stabilized_scores = scores - max_score\n    \n    # Compute the softmax of the stabilized scores\n    softmax_scores = np.exp(stabilized_scores) / np.sum(np.exp(stabilized_scores))\n    \n    # Compute the log-softmax\n    log_softmax_scores = np.log(softmax_scores)\n    \n    # Round the results to the nearest 4th decimal\n    log_softmax_scores_rounded = np.round(log_softmax_scores, 4)\n    \n    # Convert the numpy array back to a list\n    return log_softmax_scores_rounded.tolist()\n\n# Example usage:\n# scores = [2.0, 1.0, 0.1]\n# print(log_softmax(scores))\n```\n\nThis script defines a function `log_softmax` that computes the log-softmax of a given 1D list of scores. It includes numerical stability by subtracting the maximum score from all scores before computing the softmax. The results are rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 40, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Calculate gradient w.r.t. input\n        weight = self.W\n        grad_wrt_input = np.dot(accum_grad, weight.T)\n\n        # Calculate gradient w.r.t. weights\n        grad_wrt_weight = np.dot(self.layer_input.T, accum_grad)\n\n        # Calculate gradient w.r.t. biases\n        grad_wrt_biases = np.sum(accum_grad, axis=0, keepdims=True)\n\n        # Update weights and biases if trainable\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, grad_wrt_weight)\n            self.w0 = self.w0_opt.update(self.w0, grad_wrt_biases)\n\n        return grad_wrt_input\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example Optimizer (SGD)\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, w, grad_loss):\n        return w - self.learning_rate * grad_loss\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a dense layer with 3 units and input shape of 4\n    dense_layer = Dense(n_units=3, input_shape=(4,))\n    \n    # Initialize the layer with an optimizer\n    sgd_optimizer = SGD(learning_rate=0.01)\n    dense_layer.initialize(optimizer=sgd_optimizer)\n    \n    # Example input\n    X = np.array([[1, 2, 3, 4]], dtype=np.float32)\n    \n    # Forward pass\n    output = dense_layer.forward_pass(X)\n    print(\"Output:\", np.round(output, 4).tolist())\n    \n    # Example gradient from the next layer\n    accum_grad = np.array([[0.1, 0.2, 0.3]], dtype=np.float32)\n    \n    # Backward pass\n    grad_wrt_input = dense_layer.backward_pass(accum_grad)\n    print(\"Gradient w.r.t. input:\", np.round(grad_wrt_input, 4).tolist())\n    \n    # Updated weights and biases\n    print(\"Updated weights:\", np.round(dense_layer.W, 4).tolist())\n    print(\"Updated biases:\", np.round(dense_layer.w0, 4).tolist())\n```\n\nThis script defines a `Dense` layer that extends the `Layer` class, implementing the required methods for weight initialization, parameter counting, forward and backward passes, and output shape calculation. An example usage with a simple SGD optimizer is also provided to demonstrate how the `Dense` layer can be used in a neural network context."}
{"task_id": 41, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Add padding to the input matrix\n    padded_matrix = np.pad(input_matrix, padding, mode='constant')\n    \n    # Get dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Calculate output dimensions\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    \n    # Initialize output matrix\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Perform convolution\n    for i in range(output_height):\n        for j in range(output_width):\n            # Extract the region from the input matrix\n            region = padded_matrix[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            # Perform element-wise multiplication and sum with the kernel\n            output_matrix[i, j] = np.sum(region * kernel)\n    \n    # Round the results to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, 4)\n    \n    # Convert to list and return\n    return output_matrix.tolist()\n\n# Example usage:\ninput_matrix = np.array([[1, 2, 3, 4],\n                         [5, 6, 7, 8],\n                         [9, 10, 11, 12],\n                         [13, 14, 15, 16]])\n\nkernel = np.array([[1, 0, -1],\n                   [1, 0, -1],\n                   [1, 0, -1]])\n\npadding = 1\nstride = 1\n\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```\n\nThis script defines a function `simple_conv2d` that performs a 2D convolution on an input matrix using a specified kernel, padding, and stride. The result is rounded to the nearest 4th decimal and returned as a Python list. The example usage demonstrates how to call the function with a sample input matrix and kernel."}
{"task_id": 42, "completion_id": 0, "completion": "```\npython\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The output value after applying the ReLU function.\n    \"\"\"\n    return z if z > 0 else 0\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-1.0, 0.0, 1.0, 2.5, -3.3]\n    results = {z: relu(z) for z in test_values}\n    for z, result in results.items():\n        print(f\"ReLU({z}) = {result}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate the predictions\n    y_pred = np.dot(X, w)\n    \n    # Calculate the Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate the regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    \n    # Calculate the Ridge loss\n    ridge_loss_value = mse + reg_term\n    \n    # Return the Ridge loss rounded to the nearest 4th decimal\n    return round(ridge_loss_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.1, 0.2])\n    y_true = np.array([3, 7, 11])\n    alpha = 0.01\n    \n    # Calculate Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```\n\nThis script defines the `ridge_loss` function, which calculates the Ridge Regression loss by combining the Mean Squared Error (MSE) and a regularization term. The function takes a feature matrix `X`, a coefficient vector `w`, true labels `y_true`, and a regularization parameter `alpha`. The result is rounded to the nearest 4th decimal place. The example usage demonstrates how to use the function with sample data."}
{"task_id": 44, "completion_id": 0, "completion": "```\npython\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    return z if z >= 0 else alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(leaky_relu(10))        # Output: 10\n    print(leaky_relu(-10))       # Output: -0.1\n    print(leaky_relu(0))         # Output: 0\n    print(leaky_relu(-5, 0.02))  # Output: -0.1\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (numpy.ndarray): The first input vector.\n    x2 (numpy.ndarray): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage:\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nprint(kernel_function(x1, x2))  # Output should be 32\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives and false positives\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if true_positives + false_positives == 0:\n        return 0.0  # To handle the case where there are no positive predictions\n    \n    precision_score = true_positives / (true_positives + false_positives)\n    return precision_score\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nprint(\"Precision:\", precision(y_true, y_pred))\n```\n\nThis script defines a function `precision` that calculates the precision metric for binary classification. It also includes an example usage to demonstrate how the function works."}
{"task_id": 47, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    # Ensure X is a 2D array and y is a 1D array\n    X = np.array(X)\n    y = np.array(y).flatten()\n    \n    # Add a column of ones to X for the bias term\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # Ensure weights is a 1D array\n    weights = np.array(weights).flatten()\n    \n    n_samples, n_features = X.shape\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Batch Gradient Descent\n            indices = np.arange(n_samples)\n        elif method == 'stochastic':\n            # Stochastic Gradient Descent\n            indices = np.random.choice(n_samples, 1, replace=False)\n        elif method == 'mini_batch':\n            # Mini-Batch Gradient Descent\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini_batch'\")\n        \n        X_batch = X[indices]\n        y_batch = y[indices]\n        \n        # Predictions\n        y_pred = X_batch.dot(weights)\n        \n        # Compute errors\n        errors = y_pred - y_batch\n        \n        # Compute gradient\n        gradient = (2 / len(X_batch)) * X_batch.T.dot(errors)\n        \n        # Update weights\n        weights -= learning_rate * gradient\n    \n    # Return weights rounded to 4 decimal places\n    return weights.round(4).tolist()\n\n# Example usage:\n# X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n# y = [3, 5, 7, 9]\n# weights = [0, 0, 0]\n# learning_rate = 0.01\n# n_iterations = 1000\n# batch_size = 2\n# method = 'mini_batch'\n# print(gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method))\n```\n\nThis script defines a `gradient_descent` function that can perform Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent using Mean Squared Error (MSE) as the loss function. The function takes parameters for the input data `X`, target values `y`, initial `weights`, `learning_rate`, number of iterations `n_iterations`, `batch_size` (only relevant for mini-batch), and the `method` to specify which variant of gradient descent to use. The weights are updated iteratively, and the final weights are returned as a Python list rounded to four decimal places."}
{"task_id": 48, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the input matrix to a numpy array for easier manipulation\n    mat = np.array(matrix, dtype=float)\n    rows, cols = mat.shape\n    \n    # Initialize the row index\n    row = 0\n    \n    for col in range(cols):\n        # Find the pivot row for the current column\n        if row >= rows:\n            break\n        \n        # Check if the current column has a non-zero entry in the current row or below\n        if mat[row, col] == 0:\n            # Find a non-zero entry in the current column\n            for r in range(row + 1, rows):\n                if mat[r, col] != 0:\n                    # Swap the current row with the row containing the non-zero entry\n                    mat[[row, r]] = mat[[r, row]]\n                    break\n            else:\n                # No non-zero entry found in the current column, move to the next column\n                continue\n        \n        # Normalize the pivot row so that the pivot element is 1\n        pivot = mat[row, col]\n        mat[row] /= pivot\n        \n        # Eliminate all other entries in the current column\n        for r in range(rows):\n            if r != row:\n                factor = mat[r, col]\n                mat[r] -= factor * mat[row]\n        \n        # Move to the next row and column\n        row += 1\n    \n    # Convert the matrix back to a list and return\n    return mat.tolist()\n\n# Example usage:\n# matrix = [[1, 2, 1, -1], [-2, -3, 1, 7], [3, 5, 0, -2]]\n# print(rref(matrix))\n```\n\nThis script defines a function `rref` that takes a matrix as input and returns its Reduced Row Echelon Form (RREF). The function uses numpy for matrix operations and follows the standard algorithm for converting a matrix to RREF, including finding pivots, normalizing rows, and eliminating entries in the pivot column."}
{"task_id": 49, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)  # First moment estimate\n    v = np.zeros_like(x)  # Second moment estimate\n    t = 0  # Time step\n\n    for t in range(1, num_iterations + 1):\n        # Compute the gradient\n        g = grad(x)\n\n        # Update biased first moment estimate\n        m = beta1 * m + (1 - beta1) * g\n\n        # Update biased second raw moment estimate\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1 ** t)\n\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2 ** t)\n\n        # Update parameters\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    # Return the optimized parameters rounded to the nearest 4th decimal\n    return x.round(4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial parameter values\nx0 = [1.0, 1.0]\n\n# Run the Adam optimizer\noptimized_params = adam_optimizer(f, grad, x0, num_iterations=1000)\nprint(optimized_params)\n```\n\nThis script defines the `adam_optimizer` function, which implements the Adam optimization algorithm. It also includes an example usage with a simple quadratic function and its gradient. The optimized parameters are returned as a list rounded to the nearest 4th decimal."}
{"task_id": 50, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Initialize weights and bias\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    \n    # Function to compute the L1 penalty gradient\n    def l1_penalty_gradient(weights):\n        return alpha * np.sign(weights)\n    \n    # Gradient Descent\n    for _ in range(max_iter):\n        # Compute predictions\n        y_pred = np.dot(X, weights) + bias\n        \n        # Compute errors\n        errors = y_pred - y\n        \n        # Compute the gradient for weights and bias\n        gradient_weights = (1/n_samples) * np.dot(X.T, errors) + l1_penalty_gradient(weights)\n        gradient_bias = (1/n_samples) * np.sum(errors)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n        \n        # Check for convergence\n        if np.linalg.norm(gradient_weights) < tol:\n            break\n    \n    # Return rounded results\n    return weights.round(4).tolist(), bias.round(4)\n\n# Example usage:\n# X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# y = np.dot(X, np.array([1, 2])) + 3\n# weights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000, tol=1e-4)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `l1_regularization_gradient_descent` that implements Lasso Regression using Gradient Descent. The function takes in the feature matrix `X`, the target vector `y`, the regularization parameter `alpha`, the learning rate, the maximum number of iterations, and a tolerance for convergence. It returns the learned weights and bias, rounded to four decimal places. The example usage at the bottom demonstrates how to call the function with a simple dataset."}
{"task_id": 51, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    # Create a matrix to store distances\n    m, n = len(source), len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n\n    # Initialize the matrix\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n\n    # Compute the distances\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n\n            dp[i][j] = min(dp[i - 1][j] + 1,    # Deletion\n                           dp[i][j - 1] + 1,    # Insertion\n                           dp[i - 1][j - 1] + cost)  # Substitution\n\n            # Check for transposition\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)  # Transposition\n\n    return dp[m][n]\n\n# Example usage\ns1 = \"caper\"\ns2 = \"acer\"\nprint(f\"The OSA distance between '{s1}' and '{s2}' is {OSA(s1, s2)}.\")\n```\n\nThis script defines a function `OSA` that calculates the Optimal String Alignment distance between two strings using dynamic programming. The function initializes a matrix to store the distances and iteratively computes the minimum cost of transforming one string into the other, considering the allowed edit operations: deletion, insertion, substitution, and transposition. The example usage demonstrates how to use the function with the strings \"caper\" and \"acer\"."}
{"task_id": 52, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert inputs to numpy arrays for easier computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n        return round(recall_value, 3)\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(recall(y_true, y_pred))  # Output should be 0.667\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Linear transformations to obtain Queries, Keys, and Values\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the dot product between Queries and Keys\n    dot_product = np.dot(Q, K.T)\n    \n    # Scale the dot product by the square root of the dimension of K\n    d_k = K.shape[-1]\n    scaled_dot_product = dot_product / np.sqrt(d_k)\n    \n    # Apply softmax to obtain attention weights\n    attention_weights = np.softmax(scaled_dot_product, axis=-1)\n    \n    # Compute the weighted sum of Values\n    output = np.dot(attention_weights, V)\n    \n    # Round the result to the nearest 4th decimal\n    output_rounded = np.round(output, 4)\n    \n    # Convert the result to a list\n    output_list = output_rounded.tolist()\n    \n    return output_list\n\n# Example usage:\n# Let's assume we have a batch of 2 sequences, each of length 3, and each word is represented by a 4-dimensional vector\nX = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n              [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])\n\n# Weight matrices for Queries, Keys, and Values\nW_q = np.random.rand(4, 4)\nW_k = np.random.rand(4, 4)\nW_v = np.random.rand(4, 4)\n\n# Compute self-attention\nresult = self_attention(X, W_q, W_k, W_v)\nprint(result)\n```\n\nThis script defines a function `self_attention` that computes the self-attention mechanism for a given input matrix `X` and weight matrices `W_q`, `W_k`, and `W_v`. The function returns the self-attention output as a Python list, with all values rounded to the nearest 4th decimal. An example usage is also provided to demonstrate how the function can be used."}
{"task_id": 54, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state with the initial hidden state\n    hidden_state = initial_hidden_state\n    \n    # Process each input in the sequence\n    for input_vector in input_sequence:\n        # Update the hidden state using the RNN cell formula\n        hidden_state = np.tanh(np.dot(input_vector, Wx) + np.dot(hidden_state, Wh) + b)\n    \n    # Return the final hidden state rounded to four decimal places\n    return hidden_state.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input sequence, initial hidden state, weights, and bias\n    input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    initial_hidden_state = [0.0, 0.0]\n    Wx = [[0.5, 0.6], [0.7, 0.8]]\n    Wh = [[0.9, 0.1], [0.2, 0.3]]\n    b = [0.1, 0.2]\n    \n    # Compute the final hidden state\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(\"Final Hidden State:\", final_hidden_state)\n```\n\nThis script defines a function `rnn_forward` that implements a simple RNN cell. It processes a sequence of input vectors and returns the final hidden state after applying the tanh activation function at each step. The function uses numpy for efficient array operations and ensures the final result is rounded to four decimal places. The example usage at the bottom demonstrates how to call the function with sample data."}
{"task_id": 55, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    # Create a translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Convert the list of points to a homogeneous coordinates matrix\n    # Adding a third coordinate '1' for each point\n    homogeneous_points = np.array([point + [1] for point in points]).T\n    \n    # Apply the translation matrix to the points\n    translated_homogeneous_points = np.dot(translation_matrix, homogeneous_points)\n    \n    # Convert back to Cartesian coordinates and reshape\n    translated_points = translated_homogeneous_points[:2, :].T\n    \n    # Convert to list and return\n    return translated_points.tolist()\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)  # Output: [[3, 5], [5, 7], [7, 9]]\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first normal distribution P.\n    - sigma_p: Standard deviation of the first normal distribution P.\n    - mu_q: Mean of the second normal distribution Q.\n    - sigma_q: Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    - kl_divergence: The KL divergence from Q to P.\n    \"\"\"\n    # Calculate the KL divergence using the formula\n    kl_divergence = (np.log(sigma_q / sigma_p) + \n                     ((sigma_p ** 2) + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - \n                     0.5)\n    return kl_divergence\n\n# Example usage:\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\n\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"The KL divergence from Q to P is: {kl_div}\")\n```\n\nThis script defines a function `kl_divergence_normal` that calculates the KL divergence between two normal distributions given their means and standard deviations. The example usage demonstrates how to call this function with specific parameters."}
{"task_id": 57, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Check if A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Check if b is a column vector with the same number of rows as A\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"The number of rows in A must match the number of elements in b.\")\n    \n    # Initialize x with x_ini if provided, otherwise with zeros\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Number of unknowns\n    m = len(b)\n    \n    # Perform n iterations\n    for _ in range(n):\n        # Create a copy of the current solution vector\n        x_new = np.copy(x)\n        \n        # Update each component of x\n        for i in range(m):\n            # Compute the sum of terms involving x\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i+1:], x[i+1:])\n            \n            # Update x[i] using the Gauss-Seidel formula\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        \n        # Update the solution vector\n        x = x_new\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# A = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\n# b = [4, 7, 3]\n# n = 10\n# print(gauss_seidel(A, b, n))\n```\n\nThis script defines the `gauss_seidel` function, which implements the Gauss-Seidel method to solve a system of linear equations. The function takes a square matrix `A`, a vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. It returns the approximated solution vector after the specified number of iterations, rounded to the nearest 4th decimal place."}
{"task_id": 58, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    \n    # Augment A with b\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    \n    # Perform Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        \n        # Swap the current row with the pivot row\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        # Make all rows below this one 0 in the current column\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    \n    # Round the results to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Convert the solution vector to a list\n    return x.tolist()\n\n# Example usage:\nA = [\n    [3, 2, -1],\n    [2, -2, 4],\n    [-1, 0.5, -1]\n]\nb = [1, -2, 0]\n\nsolution = gaussian_elimination(A, b)\nprint(solution)\n```\n\nThis script defines the `gaussian_elimination` function, which performs Gaussian Elimination with partial pivoting to solve a system of linear equations. The function returns the solution vector rounded to the nearest 4th decimal place as a Python list. The example usage at the end demonstrates how to use the function with a sample system of equations."}
{"task_id": 59, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # Ensure input x is a numpy array\n        x = np.array(x)\n        \n        # Initialize hidden state and cell state\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        \n        # List to store hidden states at each time step\n        hidden_states = []\n        \n        # Process each time step\n        for t in range(x.shape[0]):\n            # Concatenate input and previous hidden state\n            x_t = x[t].reshape(-1, 1)\n            concat = np.vstack((h_prev, x_t))\n            \n            # Compute gates\n            f_t = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_t_hat = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            \n            # Update cell state\n            c_t = f_t * c_prev + i_t * c_t_hat\n            \n            # Update hidden state\n            h_t = o_t * np.tanh(c_t)\n            \n            # Store the hidden state\n            hidden_states.append(h_t)\n            \n            # Update previous hidden state and cell state\n            h_prev = h_t\n            c_prev = c_t\n        \n        # Return hidden states, final hidden state, and final cell state\n        hidden_states = [hs.flatten().tolist() for hs in hidden_states]\n        final_hidden_state = h_t.flatten().tolist()\n        final_cell_state = c_t.flatten().tolist()\n        \n        return hidden_states, final_hidden_state, final_cell_state\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define input size and hidden size\n    input_size = 10\n    hidden_size = 20\n    \n    # Initialize LSTM\n    lstm = LSTM(input_size, hidden_size)\n    \n    # Create a sequence of inputs (e.g., 5 time steps)\n    x = np.random.randn(5, input_size)\n    \n    # Initial hidden state and cell state\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n    \n    # Forward pass\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n    \n    # Print results\n    print(\"Hidden States:\", hidden_states)\n    print(\"Final Hidden State:\", final_hidden_state)\n    print(\"Final Cell State:\", final_cell_state)\n```\n\nThis script defines an LSTM class with an initialization method to set up the weights and biases, and a forward method to process a sequence of inputs. The forward method computes the hidden states, final hidden state, and final cell state, rounding the results to the nearest fourth decimal place. The example usage demonstrates how to initialize the LSTM, create a sequence of inputs, and perform a forward pass."}
{"task_id": 60, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import defaultdict\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus is empty. Please provide a list of documents.\")\n    \n    # Number of documents in the corpus\n    N = len(corpus)\n    \n    # Calculate document frequency (df) for each term in the corpus\n    df = defaultdict(int)\n    for document in corpus:\n        unique_terms = set(document)\n        for term in unique_terms:\n            df[term] += 1\n    \n    # Calculate the TF-IDF scores for the query in each document\n    tf_idf_scores = []\n    \n    for document in corpus:\n        # Calculate term frequency (tf) for each term in the document\n        tf = defaultdict(int)\n        for term in document:\n            tf[term] += 1\n        \n        # Calculate the TF-IDF scores for the query terms in the document\n        doc_scores = []\n        for term in query:\n            # Term frequency in the current document\n            term_tf = tf[term] / len(document) if document else 0\n            \n            # Inverse document frequency with smoothing\n            term_idf = np.log((N + 1) / (df[term] + 1)) + 1\n            \n            # Calculate TF-IDF score\n            tf_idf_score = term_tf * term_idf\n            \n            # Append the score to the list for the current document\n            doc_scores.append(tf_idf_score)\n        \n        # Append the scores for the current document to the main list\n        tf_idf_scores.append(doc_scores)\n    \n    # Convert the scores to a numpy array, round to 4 decimal places, and convert back to a list\n    tf_idf_scores_rounded = np.array(tf_idf_scores).round(4).tolist()\n    \n    return tf_idf_scores_rounded\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\"],\n    [\"the\", \"lazy\", \"dog\"],\n    [\"the\", \"quick\", \"brown\", \"dog\"]\n]\nquery = [\"the\", \"quick\", \"lazy\"]\n\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script defines the `compute_tf_idf` function, which calculates the TF-IDF scores for a given query against a corpus of documents. It handles edge cases such as an empty corpus and ensures that the results are rounded to four decimal places. The function uses smoothing to avoid division by zero when calculating the inverse document frequency (IDF)."}
{"task_id": 61, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives, False Positives, and False Negatives\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_score = 0\n    else:\n        f_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_score, 3)\n\n# Example usage:\n# y_true = np.array([0, 1, 1, 0, 1])\n# y_pred = np.array([0, 0, 1, 0, 1])\n# beta = 1.0\n# print(f_score(y_true, y_pred, beta))  # Output should be the F1-Score rounded to three decimal places\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n\n        for x in input_sequence:\n            x = x.reshape(-1, 1)  # Make sure x is a column vector\n            h_prev = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y_pred = np.dot(self.W_hy, h_prev) + self.b_y\n            outputs.append(y_pred)\n            last_inputs.append(x)\n            last_hiddens.append(h_prev)\n\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t].reshape(-1, 1)\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh  # tanh derivative\n            db_h += dh_raw\n\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t-1].T) if t > 0 else np.zeros_like(self.W_hh)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def compute_loss(self, outputs, expected_output):\n        \"\"\"\n        Computes the loss between the predicted outputs and the expected outputs.\n        \"\"\"\n        loss = 0\n        for y_pred, y_true in zip(outputs, expected_output):\n            loss += 0.5 * np.sum((y_pred - y_true) ** 2)\n        return loss\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define parameters\n    input_size = 2\n    hidden_size = 4\n    output_size = 2\n    learning_rate = 0.01\n\n    # Initialize RNN\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n\n    # Example input and output sequences\n    input_sequence = [np.array([1, 0]), np.array([0, 1]), np.array([1, 1])]\n    expected_output = [np.array([0, 1]), np.array([1, 0]), np.array([0, 0])]\n\n    # Training loop\n    for epoch in range(1000):\n        outputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n        loss = rnn.compute_loss(outputs, expected_output)\n        rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss}\")\n\n    # Final outputs after training\n    final_outputs, _, _ = rnn.forward(input_sequence)\n    for i, output in enumerate(final_outputs):\n        print(f\"Input: {input_sequence[i]}, Predicted Output: {output.flatten()}, Expected Output: {expected_output[i]}\")\n```\n\nThis script defines a simple RNN with methods for forward and backward passes, using backpropagation through time (BPTT) for training. The RNN is trained on a sequence prediction task, and the loss is computed using the mean squared error (MSE) loss function. The example usage at the end demonstrates how to train the RNN and print the final predictions."}
{"task_id": 63, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    \n    r = b - A @ x\n    p = r\n    r_norm_squared = r @ r\n    \n    for _ in range(n):\n        Ap = A @ p\n        alpha = r_norm_squared / (p @ Ap)\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        r_new_norm_squared = r_new @ r_new\n        \n        if np.sqrt(r_new_norm_squared) < tol:\n            break\n        \n        beta = r_new_norm_squared / r_norm_squared\n        p = r_new + beta * p\n        r = r_new\n        r_norm_squared = r_new_norm_squared\n    \n    return x.round(8).tolist()\n\n# Example usage:\n# A = np.array([[4, 1], [1, 3]])\n# b = np.array([1, 2])\n# solution = conjugate_gradient(A, b, n=100)\n# print(solution)\n```\n\nThis script defines the `conjugate_gradient` function, which solves the linear system \\( Ax = b \\) using the Conjugate Gradient method. The function takes a symmetric positive-definite matrix \\( A \\), a vector \\( b \\), a maximum number of iterations \\( n \\), an optional initial guess \\( x0 \\), and a tolerance \\( tol \\). The solution vector \\( x \\) is returned rounded to 8 decimal places as a list."}
{"task_id": 64, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    \n    # Calculate the frequency of each class\n    class_counts = np.bincount(y)\n    total_samples = len(y)\n    \n    # Calculate the Gini Impurity\n    gini = 1.0 - np.sum((class_counts / total_samples) ** 2)\n    \n    return round(gini, 3)\n\n# Example usage:\ny = [1, 1, 2, 2, 2, 3]\nprint(gini_impurity(y))  # Output: 0.611\n```\n\nThis script defines a function `gini_impurity` that calculates the Gini Impurity for a given list of class labels. The function first checks if the list is empty and returns 0.0 if it is. It then calculates the frequency of each class using `np.bincount`, computes the Gini Impurity, and returns the result rounded to three decimal places. An example usage is also provided to demonstrate how the function works."}
{"task_id": 65, "completion_id": 0, "completion": "```\npython\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]  # Initialize with 0 to represent the start of the first row\n\n    for row in dense_matrix:\n        non_zero_count = 0\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                non_zero_count += 1\n        row_pointer.append(row_pointer[-1] + non_zero_count)\n\n    return values, column_indices, row_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 5, 0, 6],\n    [0, 0, 0, 0]\n]\n\nvalues, column_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointer:\", row_pointer)\n```\n\nThis script defines a function `compressed_row_sparse_matrix` that converts a given dense matrix into the CSR format. The function returns three lists: the values array, the column indices array, and the row pointer array. The example usage demonstrates how to use this function with a sample dense matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```\npython\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    \n    # Convert lists to numpy arrays for vector operations\n    v = np.array(v)\n    L = np.array(L)\n    \n    # Calculate the dot product of v and L\n    dot_product_vL = np.dot(v, L)\n    \n    # Calculate the dot product of L with itself\n    dot_product_LL = np.dot(L, L)\n    \n    # Calculate the scalar projection factor\n    projection_factor = dot_product_vL / dot_product_LL\n    \n    # Calculate the projection vector\n    projection_vector = projection_factor * L\n    \n    # Return the projection vector rounded to three decimal places\n    return projection_vector.round(3).tolist()\n\n# Example usage:\n# v = [3, 4]\n# L = [1, 0]\n# print(orthogonal_projection(v, L))  # Output: [3.0, 0.0]\n```\n\nThis script defines the `orthogonal_projection` function, which computes the orthogonal projection of vector `v` onto the line defined by vector `L`. The function uses numpy for efficient vector operations and returns the result rounded to three decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "```\npython\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    \n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    \n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    \n    return values, row_indices, column_pointer\n\n# Example usage:\ndense_matrix = [\n    [0, 0, 3, 0],\n    [4, 0, 0, 0],\n    [0, 5, 0, 6],\n    [0, 0, 0, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\nThis script defines a function `compressed_col_sparse_matrix` that converts a given dense matrix into its Compressed Column Sparse (CSC) representation. The function returns three lists: `values`, `row_indices`, and `column_pointer`, which together represent the CSC format of the input matrix. The example usage demonstrates how to use this function with a sample dense matrix."}
{"task_id": 68, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the input matrix to a numpy array\n    A = np.array(A, dtype=float)\n    \n    # Perform row reduction to find the row echelon form\n    _, pivots = np.linalg.qr(A, mode='r')\n    row_echelon, _ = np.linalg.qr(A[:, pivots], mode='r')\n    \n    # Identify the pivot columns in the original matrix\n    pivot_columns = np.unique(pivots[:np.sum(np.abs(np.diag(row_echelon)) > 1e-10)])\n    \n    # Extract the independent columns from the original matrix\n    basis_vectors = A[:, pivot_columns]\n    \n    # Round the basis vectors to 8 decimal places\n    basis_vectors_rounded = np.round(basis_vectors, 8)\n    \n    # Convert the basis vectors to a list and return\n    return basis_vectors_rounded.tolist()\n\n# Example usage:\n# A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# print(matrix_image(A))\n```\n\nThis script defines the `matrix_image` function, which calculates the column space of a given matrix `A` by identifying the pivot columns after performing row reduction. The independent columns corresponding to these pivots form the basis of the column space. The result is rounded to 8 decimal places and returned as a list of lists."}
{"task_id": 69, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Calculate the mean of the true values\n    y_mean = np.mean(y_true)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_mean) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    # Return R-squared rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(r_squared(y_true, y_pred))  # Output should be 0.948\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```\npython\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Check for inconsistent row lengths and invalid pixel values\n    total_pixels = 0\n    total_brightness = 0\n    row_length = len(img[0])\n    \n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    \n    # Calculate and return the average brightness rounded to two decimal places\n    if total_pixels == 0:\n        return -1\n    \n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)\n\n# Example usage:\n# img = [\n#     [255, 0, 0],\n#     [128, 128, 128],\n#     [0, 255, 255]\n# ]\n# print(calculate_brightness(img))  # Output: 108.33\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are numpy arrays\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"y_true and y_pred must not be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    # Calculate RMSE\n    n = y_true.shape[0]\n    residuals = y_true - y_pred\n    mse = np.mean(residuals ** 2)\n    rmse_value = np.sqrt(mse)\n    \n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(rmse(y_true, y_pred))  # Output should be 0.374\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    # Ensure both arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    \n    # Handle the edge case where both arrays are zeros\n    if union == 0:\n        return 1.0  # or 0.0, depending on how you want to define it; here, we assume perfect match for no positive class\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    # Return the Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n\n# Example usage:\ny_true = [0, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 0, 1]\nprint(jaccard_index(y_true, y_pred))  # Output should be 0.667\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of true and predicted labels\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the sum of true labels and predicted labels\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    \n    # Handle the edge case where both sums are zero\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0  # Perfect score since both are empty\n    \n    # Calculate the Dice Score\n    dice = (2 * intersection) / (sum_true + sum_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\n# y_true = [1, 0, 1, 1, 0, 1]\n# y_pred = [1, 1, 0, 1, 0, 0]\n# print(dice_score(y_true, y_pred))  # Output: 0.571\n```\n\nThis script defines the `dice_score` function, which calculates the Dice Score for binary classification tasks. It handles edge cases, such as when there are no true or predicted positives, and returns the score rounded to three decimal places."}
{"task_id": 74, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    def generate_random_hv(seed, dim):\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n\n    composite_hv = np.zeros(dim)\n    \n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_seed = random_seeds.get(feature, None)\n        if feature_seed is None:\n            raise ValueError(f\"Seed for feature '{feature}' not provided in random_seeds.\")\n        \n        feature_hv = generate_random_hv(feature_seed, dim)\n        \n        # Generate hypervector for the feature value\n        # Here we use a simple hash of the value combined with the feature seed to ensure reproducibility\n        value_seed = hash(str(value)) + feature_seed\n        value_hv = generate_random_hv(value_seed, dim)\n        \n        # Bind the feature name and value hypervectors\n        bound_hv = feature_hv * value_hv\n        \n        # Bundle the bound hypervector into the composite hypervector\n        composite_hv += bound_hv\n    \n    # Normalization step (optional but often used in HDC)\n    composite_hv = np.sign(composite_hv)\n    \n    return composite_hv.tolist()\n\n# Example usage:\n# row = {'age': 25, 'gender': 'male'}\n# dim = 1000\n# random_seeds = {'age': 42, 'gender': 24}\n# composite_hv = create_row_hv(row, dim, random_seeds)\n# print(composite_hv)\n```\n\nThis script defines the `create_row_hv` function, which generates a composite hypervector for a given dataset row using Hyperdimensional Computing principles. The function uses random seeds for reproducibility and binds hypervectors for feature names and their corresponding values. The final composite hypervector is normalized and returned as a list."}
{"task_id": 75, "completion_id": 0, "completion": "```\npython\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize counters for each cell in the confusion matrix\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    # Iterate over each pair of true and predicted labels\n    for y_true, y_pred in data:\n        if y_true == 1 and y_pred == 1:\n            true_positives += 1\n        elif y_true == 0 and y_pred == 0:\n            true_negatives += 1\n        elif y_true == 0 and y_pred == 1:\n            false_positives += 1\n        elif y_true == 1 and y_pred == 0:\n            false_negatives += 1\n    \n    # Construct the confusion matrix\n    confusion_matrix = [\n        [true_negatives, false_positives],\n        [false_negatives, true_positives]\n    ]\n    \n    return confusion_matrix\n\n# Example usage:\ndata = [\n    [1, 1],  # True positive\n    [0, 0],  # True negative\n    [0, 1],  # False positive\n    [1, 0],  # False negative\n    [1, 1],  # True positive\n    [0, 0],  # True negative\n]\n\nprint(confusion_matrix(data))\n```\n\nThis script defines a function `confusion_matrix` that takes a list of `[y_true, y_pred]` pairs and calculates the confusion matrix for a binary classification problem. The example usage at the end demonstrates how to use the function with a sample dataset."}
{"task_id": 76, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure both vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both input vectors must have the same shape.\")\n    \n    # Ensure input vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    \n    # Calculate the dot product of the two vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitude of each vector\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Ensure vectors do not have zero magnitude\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Calculate the cosine similarity\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the cosine similarity rounded to three decimal places\n    return round(cos_sim, 3)\n\n# Example usage:\n# v1 = np.array([1, 2, 3])\n# v2 = np.array([4, 5, 6])\n# print(cosine_similarity(v1, v2))  # Output: 0.974\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```\npython\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    # Check if the input lists are of the same length\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    \n    # Check if all elements in the lists are either 0 or 1\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    \n    # Initialize counters for the confusion matrix\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    # Populate the confusion matrix counters\n    for a, p in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positives += 1\n        elif a == 0 and p == 0:\n            true_negatives += 1\n        elif a == 0 and p == 1:\n            false_positives += 1\n        elif a == 1 and p == 0:\n            false_negatives += 1\n    \n    # Construct the confusion matrix\n    confusion_matrix = [\n        [true_negatives, false_positives],\n        [false_negatives, true_positives]\n    ]\n    \n    # Calculate accuracy\n    total = true_positives + true_negatives + false_positives + false_negatives\n    accuracy = round((true_positives + true_negatives) / total, 3) if total != 0 else 0.0\n    \n    # Calculate F1 Score\n    precision = round(true_positives / (true_positives + false_positives), 3) if (true_positives + false_positives) != 0 else 0.0\n    recall = round(true_positives / (true_positives + false_negatives), 3) if (true_positives + false_negatives) != 0 else 0.0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) != 0 else 0.0\n    \n    # Calculate Specificity\n    specificity = round(true_negatives / (true_negatives + false_positives), 3) if (true_negatives + false_positives) != 0 else 0.0\n    \n    # Calculate Negative Predictive Value\n    negative_predictive_value = round(true_negatives / (true_negatives + false_negatives), 3) if (true_negatives + false_negatives) != 0 else 0.0\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage:\n# actual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n# predicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n# print(performance_metrics(actual, predicted))\n```\n\nThis script defines the `performance_metrics` function, which calculates the confusion matrix and several performance metrics for a binary classification problem. The function checks for input validity, computes the necessary statistics, and returns them in the specified format."}
{"task_id": 78, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Convert data to a NumPy array if it isn't already\n    data = np.array(data)\n    \n    # Calculate the mean\n    mean = np.mean(data)\n    \n    # Calculate the median\n    median = np.median(data)\n    \n    # Calculate the mode\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else np.nan\n    \n    # Calculate the variance\n    variance = np.var(data, ddof=1)  # Sample variance\n    \n    # Calculate the standard deviation\n    standard_deviation = np.std(data, ddof=1)  # Sample standard deviation\n    \n    # Calculate the percentiles\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    \n    # Calculate the interquartile range (IQR)\n    interquartile_range = percentile_75 - percentile_25\n    \n    # Prepare the results dictionary\n    results = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': round(mode, 4) if not np.isnan(mode) else mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return results\n\n# Example usage:\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]\nprint(descriptive_statistics(data))\n```\n\nThis script defines a function `descriptive_statistics` that calculates and returns a dictionary of descriptive statistics for a given dataset. The function uses NumPy for most calculations and SciPy for mode calculation. The results are rounded to four decimal places where applicable."}
{"task_id": 79, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    \n    # Calculate the probability of k successes and (n-k) failures\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# n = 10, k = 3, p = 0.5\nprint(binomial_probability(10, 3, 0.5))  # Output: 0.11719\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError(\"Standard deviation must be greater than 0.\")\n    \n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    \n    return round(pdf_value, 5)\n\n# Example usage:\n# x = 0, mean = 0, std_dev = 1 (standard normal distribution)\nprint(normal_pdf(0, 0, 1))  # Output: 0.39894\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or lam <= 0:\n        raise ValueError(\"k must be a non-negative integer and lam must be a positive number.\")\n    \n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage:\n# k = 5, lam = 3\nprint(poisson_probability(5, 3))  # Output: 0.10082\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D array (grayscale image).\")\n    \n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    \n    contrast = max_pixel - min_pixel\n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample grayscale image (2D numpy array)\n    sample_img = np.array([\n        [50, 60, 100],\n        [20, 255, 150],\n        [100, 200, 50]\n    ])\n    \n    # Calculate the contrast\n    contrast = calculate_contrast(sample_img)\n    print(f\"The contrast of the sample image is: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Vectors must be of the same length\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\n# vec1 = np.array([1, 2, 3])\n# vec2 = np.array([4, 5, 6])\n# result = calculate_dot_product(vec1, vec2)\n# print(result)  # Output: 32\n```\n\nThis script defines a function `calculate_dot_product` that computes the dot product of two 1D NumPy arrays. It includes a check to ensure the vectors are of the same length and uses NumPy's `dot` function to perform the calculation. An example usage is provided in comments."}
{"task_id": 84, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the input data to a numpy array for easier manipulation\n    data_array = np.array(data)\n    \n    # Use numpy's polynomial expansion function to generate the features\n    # We use np.vander to create a Vandermonde matrix which is a type of polynomial feature matrix\n    # The columns of the Vandermonde matrix are powers of the input vector, starting from the first power up to the degree\n    # We then transpose the matrix to have each data point's features in a row\n    transformed_features = np.vander(data_array, degree + 1, increasing=True)\n    \n    # Round the features to 8 decimal places\n    transformed_features = np.round(transformed_features, 8)\n    \n    # Convert the numpy array back to a list of lists\n    return transformed_features.tolist()\n\n# Example usage:\n# data = [1.0, 2.0, 3.0]\n# degree = 2\n# print(phi_transform(data, degree))\n# Output: [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0], [1.0, 3.0, 9.0]]\n```\n\nThis script defines the `phi_transform` function, which takes a list of numerical data and a degree as inputs and returns a nested list of polynomial features. The function uses NumPy's `vander` function to generate the polynomial features and rounds them to 8 decimal places. If the degree is less than 0, it returns an empty list."}
{"task_id": 85, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the positional encoding matrix\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Compute the positional encodings\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i < d_model:\n                pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n    \n    # Convert to list before returning\n    return pe.tolist()\n\n# Example usage:\n# print(pos_encoding(5, 10))\n```\n\nThis script defines a function `pos_encoding` that calculates the positional encodings for a given sequence length and model dimensionality using sine and cosine functions. It returns -1 if the input conditions are not met. The resulting positional encoding matrix is converted to a list of lists with `dtype` `np.float16` before being returned."}
{"task_id": 86, "completion_id": 0, "completion": "```\npython\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    else:\n        return 0  # Good fit\n\n# Example usage:\n# print(model_fit_quality(0.9, 0.6))  # Should return 1 (Overfitting)\n# print(model_fit_quality(0.5, 0.4))  # Should return -1 (Underfitting)\n# print(model_fit_quality(0.8, 0.7))  # Should return 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return updated values rounded to 5 decimal places and converted to list\n    return (parameter.tolist(), m.tolist(), v.tolist())\n\n# Example usage:\n# parameter = np.array([0.0, 0.0])\n# grad = np.array([0.1, 0.2])\n# m = np.array([0.0, 0.0])\n# v = np.array([0.0, 0.0])\n# t = 1\n# updated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\n# print(updated_parameter, updated_m, updated_v)\n```\n\nThis script defines the `adam_optimizer` function, which performs the Adam optimization update step. It handles both scalar and array inputs, applies bias correction, and returns the updated parameter and moving averages rounded to five decimal places and converted to lists."}
{"task_id": 88, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements a simplified GPT-2-like text generation function based on the provided requirements and helper function. This script includes the necessary components such as token embeddings, positional embeddings, multi-head attention, feed-forward network, and layer normalization.\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps: float = 1e-5):\n    mu = np.mean(x, axis=-1, keepdims=True)\n    sigma = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mu) / (sigma + eps) + b\n\ndef multi_head_attention(q, k, v, n_head: int):\n    batch_size, seq_len, d_model = q.shape\n    d_k = d_model // n_head\n\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    weights = weights / np.sum(weights, axis=-1, keepdims=True)\n\n    context = np.matmul(weights, v)\n    context = context.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return context\n\ndef feed_forward(x, d_ff: int):\n    x = np.maximum(0.01 * x, x)  # GELU approximation\n    x = np.dot(x, np.random.rand(x.shape[-1], d_ff))\n    x = np.maximum(0.01 * x, x)  # GELU approximation\n    x = np.dot(x, np.random.rand(d_ff, x.shape[-1]))\n    return x\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    n_ctx = hparams[\"n_ctx\"]\n    n_head = hparams[\"n_head\"]\n    d_model = params[\"wte\"].shape[-1]\n    d_ff = 4 * d_model  # Commonly used in transformer architectures\n\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]  # Truncate to context length if necessary\n\n    for _ in range(n_tokens_to_generate):\n        context = np.array([input_ids])\n        token_embeddings = params[\"wte\"][context]\n        positions = np.arange(context.shape[1]).reshape(1, -1)\n        positional_embeddings = params[\"wpe\"][positions]\n        x = token_embeddings + positional_embeddings\n\n        for _ in range(len(params[\"blocks\"])):\n            # Multi-head Attention\n            q = k = v = x\n            context = multi_head_attention(q, k, v, n_head)\n            x = x + context\n            x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n\n            # Feed-Forward Network\n            ff_output = feed_forward(x, d_ff)\n            x = x + ff_output\n            x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n\n        logits = np.dot(x[0, -1], params[\"wte\"].T)\n        next_token = np.argmax(logits)\n        input_ids.append(next_token)\n\n    generated_text = encoder.decode(input_ids)\n    return generated_text\n\n# Example usage\nprompt = \"hello\"\ngenerated_text = gen_text(prompt, n_tokens_to_generate=10)\nprint(generated_text)\n```\n\n### Explanation:\n- **Token Embeddings**: Mapped using a dummy encoder.\n- **Positional Embeddings**: Added to token embeddings to provide positional information.\n- **Multi-head Attention**: Implemented to allow the model to attend to different parts of the sequence.\n- **Feed-Forward Network**: Processes the attention outputs through a dense layer with GELU activation.\n- **Layer Normalization**: Applied to stabilize the training process.\n\nThis script is a simplified version and does not include the full complexity of a real GPT-2 model, but it captures the core concepts."}
{"task_id": 89, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function first\n        exp_values = np.exp(values - np.max(values))  # Subtract max for numerical stability\n        return exp_values / np.sum(exp_values)\n\n    # Convert crystal values to a numpy array for easier manipulation\n    crystal_values = np.array(crystal_values)\n    \n    # Initialize a matrix to hold the attention scores\n    attention_scores = np.zeros((n, n))\n    \n    # Calculate the raw attention scores (dot product of each crystal with every other crystal)\n    for i in range(n):\n        for j in range(n):\n            # Here we assume a simple dot product for the relationship between crystals\n            # In a real self-attention mechanism, we would use learned weights (queries, keys, values)\n            attention_scores[i, j] = np.dot(crystal_values[i], crystal_values[j])\n    \n    # Apply softmax to each row to get the attention weights for each crystal\n    attention_weights = np.apply_along_axis(softmax, 1, attention_scores)\n    \n    # Calculate the final weighted pattern for each crystal\n    weighted_patterns = np.dot(attention_weights, crystal_values)\n    \n    # Return the final weighted patterns rounded to 4 decimal places\n    return np.round(weighted_patterns, 4).tolist()\n\n# Example usage:\nn = 3\ncrystal_values = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\ndimension = 3\n\nresult = pattern_weaver(n, crystal_values, dimension)\nprint(result)\n```\n\nThis script defines a `pattern_weaver` function that implements a simplified self-attention mechanism. It calculates the attention scores based on the dot product of crystal values, applies the softmax function to these scores to get attention weights, and then computes the final weighted pattern for each crystal. The result is returned as a list of floats, each rounded to four decimal places."}
{"task_id": 90, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Calculate the document frequency for each term in the corpus\n    df = Counter()\n    for document in corpus:\n        unique_terms = set(document)\n        df.update(unique_terms)\n    \n    # Calculate the inverse document frequency for each term\n    N = len(corpus)\n    idf = {}\n    for term in df:\n        idf[term] = np.log((N - df[term] + 0.5) / (df[term] + 0.5))\n    \n    # Calculate the average document length\n    avgdl = np.mean([len(doc) for doc in corpus])\n    \n    # Calculate BM25 scores for each document\n    scores = []\n    for doc in corpus:\n        doc_len = len(doc)\n        score = 0.0\n        query_terms = Counter(query)\n        for term, query_tf in query_terms.items():\n            if term in doc:\n                doc_tf = doc.count(term)\n                # BM25 formula\n                score += idf[term] * (doc_tf * (k1 + 1)) / (doc_tf + k1 * (1 - b + b * doc_len / avgdl))\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage:\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"],\n    [\"cats\", \"and\", \"dogs\", \"are\", \"animals\"]\n]\nquery = [\"cat\", \"dog\"]\nprint(calculate_bm25_scores(corpus, query))\n```\n\nThis script defines a function `calculate_bm25_scores` that computes the BM25 scores for a given query against a corpus of documents. The function uses the BM25 formula, which includes term frequency, document frequency, document length normalization, and configurable parameters `k1` and `b`. The scores are rounded to three decimal places as required."}
{"task_id": 91, "completion_id": 0, "completion": "```\npython\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of true labels and predicted labels must be the same.\")\n\n    # Calculate True Positives, False Positives, and False Negatives\n    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\n    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\n    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\n\n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # Calculate F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return round(f1_score, 3)\n\n# Example usage:\ny_true = [0, 1, 0, 1, 1, 0, 1]\ny_pred = [0, 1, 1, 1, 0, 0, 1]\nprint(calculate_f1_score(y_true, y_pred))  # Output: 0.667\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = []\n    for i in range(1, 11):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_data.append(consumption_data[i-1] - fluctuation)\n    \n    # 2) Perform linear regression on the detrended data.\n    days = np.array(range(1, 11)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # 3) Predict day 15's base consumption.\n    day_15_base_consumption = model.predict(np.array([[15]]))[0]\n    \n    # 4) Add the day 15 fluctuation back.\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    final_consumption = math.ceil(day_15_consumption * 1.05)\n    \n    # 6) Return the final integer.\n    return final_consumption\n\n# Example usage:\nconsumption_data = [120, 135, 150, 165, 180, 195, 210, 225, 240, 255]  # Example data\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script defines a function `power_grid_forecast` that takes a list of daily power consumption data, detrends it by removing the known sinusoidal fluctuation, fits a linear regression model to the detrended data, predicts the power consumption for day 15, adds back the fluctuation for that day, and finally applies a 5% safety margin to ensure the colony's future needs are met. The result is returned as an integer."}
{"task_id": 93, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# y_true = np.array([3.0, -0.5, 2.0, 7.0])\n# y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n# print(mae(y_true, y_pred))  # Output should be 0.375\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"Compute the Query, Key, and Value matrices.\"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the self-attention mechanism.\"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Compute the multi-head attention mechanism.\"\"\"\n    batch_size, seq_length, d_model = X.shape\n    d_k = d_v = d_model // n_heads\n\n    W_q_heads = W_q.reshape(d_model, n_heads, d_k)\n    W_k_heads = W_k.reshape(d_model, n_heads, d_k)\n    W_v_heads = W_v.reshape(d_model, n_heads, d_v)\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    Q_heads = Q.reshape(batch_size, seq_length, n_heads, d_k).transpose(0, 2, 1, 3)\n    K_heads = K.reshape(batch_size, seq_length, n_heads, d_k).transpose(0, 2, 1, 3)\n    V_heads = V.reshape(batch_size, seq_length, n_heads, d_v).transpose(0, 2, 1, 3)\n\n    attention_heads = []\n    for i in range(n_heads):\n        attention_head = self_attention(Q_heads[:, i], K_heads[:, i], V_heads[:, i])\n        attention_heads.append(attention_head)\n\n    attention_output = np.concatenate(attention_heads, axis=-1)\n    attention_output = attention_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_length, d_model)\n    \n    return attention_output.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Random initialization for demonstration\n    np.random.seed(0)\n    batch_size, seq_length, d_model = 2, 3, 8\n    n_heads = 2\n\n    X = np.random.rand(batch_size, seq_length, d_model)\n    W_q = np.random.rand(d_model, d_model)\n    W_k = np.random.rand(d_model, d_model)\n    W_v = np.random.rand(d_model, d_model)\n\n    result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(result)\n```"}
{"task_id": 95, "completion_id": 0, "completion": "```\npython\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"The input lists must have the same length.\")\n    \n    # Count occurrences\n    a = sum(xi and yi for xi, yi in zip(x, y))  # both 1\n    b = sum(xi and not yi for xi, yi in zip(x, y))  # x is 1, y is 0\n    c = sum(not xi and yi for xi, yi in zip(x, y))  # x is 0, y is 1\n    d = sum(not xi and not yi for xi, yi in zip(x, y))  # both 0\n    \n    # Calculate the Phi coefficient\n    n = len(x)\n    numerator = (a * d) - (b * c)\n    denominator = ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    \n    if denominator == 0:\n        return 0.0  # To handle the case where the denominator is zero to avoid division by zero error\n    \n    phi = numerator / denominator\n    return round(phi, 4)\n\n# Example usage:\nx = [1, 0, 1, 1, 0]\ny = [1, 1, 0, 1, 0]\nprint(phi_corr(x, y))  # Output: 0.1690\n```\n\nThis script defines a function `phi_corr` that calculates the Phi coefficient for two binary variables provided as lists. It includes error handling for lists of unequal length and checks for division by zero. The result is rounded to four decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```\npython\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0, min(1, 0.2 * x + 0.5))\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    results = {x: hard_sigmoid(x) for x in test_values}\n    for x, result in results.items():\n        print(f\"hard_sigmoid({x}) = {result}\")\n```\n\nThis script defines the `hard_sigmoid` function according to the standard definition of the Hard Sigmoid activation function, which is a piecewise linear function that approximates the sigmoid function. The function is then tested with a few example values to demonstrate its behavior."}
{"task_id": 97, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage:\n# print(elu(1.0))  # Output: 1.0\n# print(elu(-1.0)) # Output: -0.6321\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```\npython\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test cases\n    print(prelu(10))        # Expected output: 10\n    print(prelu(-10))       # Expected output: -2.5 (since alpha is 0.25 by default)\n    print(prelu(-5, 0.1))   # Expected output: -0.5 (since alpha is 0.1)\n    print(prelu(0))         # Expected output: 0\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```\npython\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # To prevent overflow, we use the fact that for large x, softplus(x) is approximately x\n    if x > 20:  # 20 is a common threshold to avoid overflow\n        return round(x, 4)\n    # For very small x, we use the approximation log(1 + x) for e^x\n    elif x < -20:  # -20 is a common threshold to avoid underflow\n        return round(math.log1p(math.exp(x)), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage:\n# print(softplus(10))  # Should be close to 10\n# print(softplus(-10)) # Should be close to 0\n# print(softplus(0))   # Should be 0.6931\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```\npython\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    softsign_value = x / (1 + abs(x))\n    return round(softsign_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    results = {x: softsign(x) for x in test_values}\n    print(results)\n```\n\nThis script defines the `softsign` function, which computes the Softsign activation function for a given input and returns the result rounded to four decimal places. The example usage demonstrates how to use the function with a set of test values."}
{"task_id": 101, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the first term of the GRPO objective\n    term1 = np.minimum(rhos * A, clipped_rhos * A)\n    \n    # Compute the KL divergence between the old policy and the reference policy\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Compute the GRPO objective\n    grpo_obj = np.mean(term1) - beta * kl_divergence\n    \n    # Return the result rounded to the nearest 6th decimal\n    return round(grpo_obj, 6)\n\n# Example usage\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.25, 0.25, 0.25, 0.25]\npi_theta_ref = [0.3, 0.2, 0.2, 0.3]\n\nobjective_value = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(f\"GRPO Objective Value: {objective_value}\")\n```\n\nThis script defines the `grpo_objective` function, which computes the GRPO objective using the provided parameters. It includes vectorized operations for efficiency and ensures the result is rounded to six decimal places. An example usage is also provided to demonstrate how the function can be called."}
{"task_id": 102, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x / (1 + math.exp(-x))\n    return round(swish_value, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for value in test_values:\n        print(f\"Swish({value}) = {swish(value)}\")\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```\npython\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    results = {x: selu(x) for x in test_values}\n    print(results)\n```\n\nThis script defines the SELU activation function and includes an example usage section that demonstrates how the function works with a few test values. The results are rounded to the nearest fourth decimal place as specified."}
{"task_id": 104, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights, add bias\n    z = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n    \n    # Convert probabilities to binary predictions based on threshold 0.5\n    predictions = (probabilities >= 0.5).astype(int)\n    \n    # Return predictions as a python list\n    return predictions.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# weights = np.array([0.5, -0.5])\n# bias = 0.1\n# print(predict_logistic(X, weights, bias))\n```\n\nThis script defines a function `predict_logistic` that takes an input feature matrix `X`, model weights `weights`, and a model bias `bias`. It computes the logistic regression predictions by first calculating the linear combination of the inputs and weights, applying a sigmoid function to obtain probabilities, and then thresholding these probabilities to produce binary predictions. The predictions are returned as a Python list."}
{"task_id": 105, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of classes\n    C = np.max(y) + 1\n    # Number of features\n    M = X.shape[1]\n    \n    # Initialize parameters randomly\n    B = np.random.randn(C, M)\n    \n    # One-hot encode y\n    y_one_hot = np.eye(C)[y]\n    \n    losses = []\n    \n    for _ in range(iterations):\n        # Compute scores\n        scores = np.dot(X, B.T)\n        \n        # Apply softmax\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        \n        # Compute cross-entropy loss\n        loss = -np.sum(y_one_hot * np.log(softmax_scores)) / len(y)\n        losses.append(round(loss, 4))\n        \n        # Compute gradient\n        gradient = np.dot((softmax_scores - y_one_hot).T, X) / len(y)\n        \n        # Update parameters\n        B -= learning_rate * gradient\n    \n    return B.round(4).tolist(), losses\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([0, 1, 0])\n# learning_rate = 0.1\n# iterations = 100\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(\"Optimized coefficients:\", B)\n# print(\"Losses over iterations:\", losses)\n```\n\nThis script defines a function `train_softmaxreg` that performs gradient descent to train a softmax regression model. It initializes random parameters, computes the softmax probabilities, calculates the cross-entropy loss, computes the gradient, and updates the parameters iteratively. The function returns the optimized coefficients and the list of losses rounded to four decimal places."}
{"task_id": 106, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add intercept term to X\n    X = np.c_[np.ones((X.shape[0], 1)), X]\n    # Initialize weights\n    weights = np.zeros(X.shape[1])\n    # List to store loss values\n    loss_values = []\n\n    # Sigmoid function\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    # Binary Cross Entropy Loss\n    def compute_loss(y, y_pred):\n        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n\n    for i in range(iterations):\n        # Predicted values\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        \n        # Compute gradients\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        \n        # Update weights\n        weights -= learning_rate * gradient\n        \n        # Compute loss\n        loss = compute_loss(y, y_pred)\n        loss_values.append(round(loss, 4))\n\n    # Return weights and loss values\n    return weights.tolist(), loss_values\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4]])\n# y = np.array([0, 1, 1])\n# learning_rate = 0.1\n# iterations = 1000\n# coefficients, losses = train_logreg(X, y, learning_rate, iterations)\n# print(\"Coefficients:\", coefficients)\n# print(\"Losses:\", losses)\n```\n\nThis script defines a function `train_logreg` that trains a logistic regression model using gradient descent. It includes the addition of an intercept term to the input features, initializes the model parameters, and iteratively updates them using the gradient of the binary cross-entropy loss. The function returns the optimized model coefficients and a list of loss values rounded to four decimal places."}
{"task_id": 107, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute the dot product between Q and K^T\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    \n    # Apply the mask\n    scores = scores + mask * -1e9  # Large negative value for masked positions\n    \n    # Compute the softmax along the last dimension\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the values\n    output = np.dot(attention_weights, V)\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input dimensions\n    batch_size, seq_len, embed_dim = 2, 5, 3\n    \n    # Randomly initialize input, weights, and mask\n    X = np.random.rand(batch_size, seq_len, embed_dim)\n    W_q = np.random.rand(embed_dim, embed_dim)\n    W_k = np.random.rand(embed_dim, embed_dim)\n    W_v = np.random.rand(embed_dim, embed_dim)\n    mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9  # Upper triangular mask for auto-regressive modeling\n    \n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Compute masked self-attention\n    result = masked_attention(Q, K, V, mask)\n    print(result)\n```\n\nThis script defines two functions: `compute_qkv` to compute the Query (Q), Key (K), and Value (V) matrices from the input data and weight matrices, and `masked_attention` to compute the masked self-attention mechanism. The example usage at the bottom demonstrates how to use these functions with randomly generated data. The mask used in this example is an upper triangular mask, which is typical for tasks like text generation where each position in the sequence can only attend to previous positions."}
{"task_id": 108, "completion_id": 0, "completion": "```\npython\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    \n    # Calculate the probability of each color\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate the entropy (disorder) using the formula for Shannon entropy\n    entropy = -sum(p * (p ** 0.5) for p in probabilities if p > 0)\n    \n    # Return the entropy rounded to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Test cases\nprint(disorder([0, 0, 0, 0]))          # Expected output: 0.0000\nprint(disorder([1, 1, 0, 0]))          # Expected output: a value greater than 0.0000\nprint(disorder([0, 1, 2, 3]))          # Expected output: a value greater than the previous one\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Expected output: a value greater than [0, 0, 0, 0, 0, 1, 2, 3]\n```\n\nThis script defines a function `disorder` that calculates the disorder in a basket of apples based on their colors using a modified entropy formula. The entropy is a common measure of disorder or uncertainty, and by modifying it slightly (using `p ** 0.5` instead of `p * log(p)`), we can ensure that the disorder increases with the variety of colors while still satisfying the given constraints. The results are rounded to the nearest 4th decimal place as required."}
{"task_id": 109, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - Normalized X as a list, rounded to 5 decimal places.\n    \"\"\"\n    # Mean and variance calculation across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalization\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    \n    # Scaling and shifting\n    X_normalized = gamma * X_normalized + beta\n    \n    # Rounding and converting to list\n    return X_normalized.round(5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.random.rand(2, 3, 4)  # batch_size=2, sequence_length=3, feature_dim=4\n    gamma = np.random.rand(4)    # feature_dim=4\n    beta = np.random.rand(4)     # feature_dim=4\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    print(normalized_X)\n```\nThis script defines a function `layer_normalization` that performs layer normalization on a 3D numpy array `X` with scaling parameters `gamma` and shifting parameters `beta`. The function computes the mean and variance across the feature dimension, normalizes the data, applies the scaling and shifting, and returns the result as a list rounded to five decimal places. An example usage is provided to demonstrate how the function can be used."}
{"task_id": 110, "completion_id": 0, "completion": "```\npython\nimport numpy as np\nfrom collections import Counter\nfrom math import exp\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    def _match(ngram, cands):\n        \"\"\"Given a ngram, return the number of candidates it matches with.\"\"\"\n        for cand in cands:\n            if ngram in cand:\n                return 1\n        return 0\n\n    def _ngram_counts(sentence, n):\n        \"\"\"Return a Counter object of n-grams in the sentence.\"\"\"\n        return Counter([' '.join(sentence[i:i+n]) for i in range(len(sentence) - n + 1)])\n\n    def _word_order(candidate, reference):\n        \"\"\"Calculate the word order penalty.\"\"\"\n        candidate_chunks = []\n        ref_chunks = []\n        candidate_words = candidate.split()\n        ref_words = reference.split()\n        \n        for i in range(len(candidate_words)):\n            for j in range(i, len(candidate_words)):\n                chunk = ' '.join(candidate_words[i:j+1])\n                if chunk in reference:\n                    candidate_chunks.append(chunk)\n        \n        for i in range(len(ref_words)):\n            for j in range(i, len(ref_words)):\n                chunk = ' '.join(ref_words[i:j+1])\n                if chunk in candidate:\n                    ref_chunks.append(chunk)\n        \n        if not candidate_chunks or not ref_chunks:\n            return 0\n        \n        candidate_chunks = sorted(candidate_chunks, key=len, reverse=True)\n        ref_chunks = sorted(ref_chunks, key=len, reverse=True)\n        \n        matches = 0\n        for chunk in candidate_chunks:\n            if chunk in ref_chunks:\n                matches += 1\n                ref_chunks.remove(chunk)\n        \n        return matches / len(ref_chunks)\n\n    ref_words = reference.split()\n    cand_words = candidate.split()\n\n    ref_counts = Counter(ref_words)\n    cand_counts = Counter(cand_words)\n\n    # Unigram matches\n    overlap = sum((ref_counts & cand_counts).values())\n    precision = overlap / len(cand_words) if cand_words else 0\n    recall = overlap / len(ref_words) if ref_words else 0\n\n    # F-mean\n    if precision + recall > 0:\n        f_mean = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    else:\n        f_mean = 0\n\n    # Word order penalty\n    word_order_penalty = _word_order(candidate, reference)\n    if word_order_penalty == 0:\n        word_order_penalty = exp(-gamma * abs(len(cand_words) - len(ref_words)))\n\n    # METEOR score\n    meteor = (1 - alpha) * f_mean + alpha * word_order_penalty\n\n    return round(meteor, 3)\n\n# Example usage:\nreference = \"the cat sat on the mat\"\ncandidate = \"the cat is on the mat\"\nprint(meteor_score(reference, candidate))\n```\n\nThis script defines a function `meteor_score` that calculates the METEOR score for evaluating machine translation quality. The function takes a reference translation and a candidate translation as input, along with optional parameters `alpha`, `beta`, and `gamma` for tuning the score. The function returns the METEOR score rounded to three decimal places."}
{"task_id": 111, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n\n    Parameters:\n    - joint_counts: int, the number of times both events occur together.\n    - total_counts_x: int, the number of times event X occurs.\n    - total_counts_y: int, the number of times event Y occurs.\n    - total_samples: int, the total number of samples.\n\n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate the expected joint probability under independence\n    expected_joint_prob = (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    \n    # Calculate the actual joint probability\n    actual_joint_prob = joint_counts / total_samples\n    \n    # Calculate PMI\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        pmi = 0.0\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    \n    # Return PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\n# joint_counts = 10, total_counts_x = 100, total_counts_y = 50, total_samples = 1000\n# pmi = compute_pmi(10, 100, 50, 1000)\n# print(pmi)  # Output should be the PMI value rounded to 3 decimal places\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```\npython\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    # Avoid division by zero if all values are the same\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage:\n# data = [10, 20, 30, 40, 50]\n# print(min_max(data))  # Output: [0.0, 0.25, 0.5, 0.75, 1.0]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    # First weight layer with ReLU activation\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    \n    # Second weight layer with ReLU activation\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    \n    # Shortcut connection and final ReLU activation\n    output = np.maximum(0, a2 + x)\n    \n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Initialize random input and weights for demonstration\n    np.random.seed(0)\n    x = np.random.rand(1, 5)  # Example input with 5 features\n    w1 = np.random.rand(5, 5)  # First weight matrix (5x5)\n    w2 = np.random.rand(5, 5)  # Second weight matrix (5x5)\n    \n    # Compute the output of the residual block\n    output = residual_block(x, w1, w2)\n    print(output)\n```\n\nThis script defines a `residual_block` function that processes a 1D input array through two weight layers with ReLU activations, and includes a shortcut connection that adds the original input to the output of the second layer before a final ReLU activation. The output is rounded to four decimal places and converted to a list before being returned. The example usage demonstrates how to use this function with random input and weight matrices."}
{"task_id": 114, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    # Check if the input array has the correct shape\n    if x.ndim != 3:\n        raise ValueError(\"Input array must have 3 dimensions (height, width, channels).\")\n    \n    # Compute the average along the height and width dimensions\n    avg_pooled = np.mean(x, axis=(0, 1))\n    \n    return avg_pooled\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a random 3D array to simulate feature maps\n    feature_maps = np.random.rand(10, 10, 3)  # Example with height=10, width=10, channels=3\n    \n    # Perform global average pooling\n    result = global_avg_pool(feature_maps)\n    \n    # Print the result\n    print(\"Global Average Pooled Result:\", result)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    # X: (batch_size, channels, height, width)\n    # gamma: (channels,)\n    # beta: (channels,)\n    \n    # Calculate the mean and variance for each channel across the batch and spatial dimensions\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)  # Shape: (1, channels, 1, 1)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)  # Shape: (1, channels, 1, 1)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)  # Shape: (batch_size, channels, height, width)\n    \n    # Apply scale (gamma) and shift (beta)\n    X_normalized_scaled = gamma[:, np.newaxis, np.newaxis] * X_normalized + beta[:, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places and convert to list\n    return X_normalized_scaled.round(4).tolist()\n\n# Example usage:\n# batch_size, channels, height, width = 2, 3, 4, 4\n# X = np.random.rand(batch_size, channels, height, width)\n# gamma = np.random.rand(channels)\n# beta = np.random.rand(channels)\n# result = batch_normalization(X, gamma, beta)\n# print(result)\n```\n\nThis script defines a function `batch_normalization` that performs batch normalization on a 4D NumPy array in the BCHW format. It calculates the mean and variance for each channel across the batch and spatial dimensions, normalizes the input, applies scale and shift parameters, and finally rounds the result to four decimal places before converting it to a list."}
{"task_id": 116, "completion_id": 0, "completion": "```\npython\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative rounded to 4 decimal places.\n    \"\"\"\n    # Using the power rule: d/dx (c * x^n) = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n\n# Example usage:\n# The derivative of 3 * x^2 at x = 4\nprint(poly_term_derivative(3, 4, 2))  # Output should be 24.0\n\n# The derivative of 5 * x^3 at x = 2\nprint(poly_term_derivative(5, 2, 3))  # Output should be 60.0\n\n# The derivative of 1.5 * x^0.5 at x = 4\nprint(poly_term_derivative(1.5, 4, 0.5))  # Output should be 0.9375\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    # Convert the list of vectors to a numpy array for easier manipulation\n    vectors = np.array(vectors, dtype=float)\n    \n    # Initialize an empty list to store the orthonormal basis\n    basis = []\n    \n    for v in vectors:\n        # Convert v to a numpy array\n        v = np.array(v, dtype=float)\n        \n        # Subtract the projection of v onto all current basis vectors\n        for u in basis:\n            u = np.array(u, dtype=float)\n            v -= np.dot(v, u) * u\n        \n        # Check if the resulting vector is linearly independent\n        if np.linalg.norm(v) > tol:\n            # Normalize the vector and add it to the basis\n            v_normalized = v / np.linalg.norm(v)\n            basis.append(v_normalized)\n    \n    # Round each basis vector to 4 decimal places and convert to list\n    basis_rounded = [v.round(4).tolist() for v in basis]\n    \n    return basis_rounded\n\n# Example usage:\nvectors = [[1, 0], [1, 1], [2, 2]]\nprint(orthonormal_basis(vectors))\n```\n\nThis script defines a function `orthonormal_basis` that computes an orthonormal basis for a given list of 2D vectors using the Gram-Schmidt process. The function checks for linear independence using a specified tolerance and returns the orthonormal basis vectors rounded to four decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cross_product(a, b):\n    # Convert input lists to numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a, b)\n    \n    # Round the result to 4 decimal places and convert to list\n    return result.round(4).tolist()\n\n# Example usage:\n# vector_a = [1, 2, 3]\n# vector_b = [4, 5, 6]\n# print(cross_product(vector_a, vector_b))  # Output: [-3.0, 6.0, -3.0]\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert A and b to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, return -1 indicating no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column to find the solution for each variable\n    for i in range(len(b)):\n        # Create a matrix A_i by replacing the i-th column of A with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        \n        # Calculate the determinant of A_i\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the i-th solution using Cramer's rule\n        x_i = det_A_i / det_A\n        \n        # Append the rounded solution to the solution vector\n        x.append(round(x_i, 4))\n    \n    # Return the solution vector as a list\n    return x\n\n# Example usage:\n# A = [[2, 1], [5, 3]]\n# b = [4, 11]\n# print(cramers_rule(A, b))  # Output: [1.0, 2.0]\n```\n\nThis script defines a function `cramers_rule` that solves a system of linear equations using Cramer's Rule. It checks if the determinant of the coefficient matrix `A` is zero to determine if there is a unique solution. If there is a unique solution, it calculates each variable using the determinants of matrices derived by replacing columns of `A` with the vector `b`. The results are rounded to four decimal places and returned as a list."}
{"task_id": 120, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the inputs have different lengths or are empty\n    if len(p) != len(q) or not p or not q:\n        return 0.0\n    \n    # Convert lists to numpy arrays for element-wise operations\n    p = np.array(p)\n    q = np.array(q)\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p * q))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    \n    # Return the Bhattacharyya distance rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage:\n# p = [0.1, 0.2, 0.3, 0.4]\n# q = [0.4, 0.3, 0.2, 0.1]\n# print(bhattacharyya_distance(p, q))  # Output: 0.4855\n```\n\nThis script defines a function `bhattacharyya_distance` that calculates the Bhattacharyya distance between two discrete probability distributions. It checks for invalid inputs and performs the necessary calculations using NumPy for efficient array operations. The result is rounded to four decimal places as specified."}
{"task_id": 121, "completion_id": 0, "completion": "```\npython\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    if len(a) != len(b):\n        return -1\n    return [x + y for x, y in zip(a, b)]\n\n# Example usage:\nvector_a = [1, 2, 3]\nvector_b = [4, 5, 6]\nresult = vector_sum(vector_a, vector_b)\nprint(result)  # Output: [5, 7, 9]\n\nvector_c = [1, 2]\nvector_d = [3, 4, 5]\nresult = vector_sum(vector_c, vector_d)\nprint(result)  # Output: -1\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Calculate the total return (G) for each time step\n        G = 0\n        returns = []\n        for state, action, reward in reversed(episode):\n            G = reward + G\n            returns.append((state, action, G))\n        returns.reverse()\n        \n        # Compute the gradient for this episode\n        for state, action, G in returns:\n            # Compute the softmax probabilities for the current state\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            \n            # Compute the gradient for this state-action pair\n            gradient[state, action] += G * (1 - probs[action])\n            for a in range(num_actions):\n                if a != action:\n                    gradient[state, a] -= G * probs[a]\n    \n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round the results to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    \n    # Convert to list\n    return gradient.tolist()\n\n# Example usage:\n# theta = np.array([[1.0, 2.0], [3.0, 4.0]])\n# episodes = [[(0, 0, 1.0), (1, 1, 1.0)], [(0, 1, 0.0), (1, 0, 1.0)]]\n# print(compute_policy_gradient(theta, episodes))\n```\n\nThis script defines the `compute_policy_gradient` function, which calculates the policy gradient using the REINFORCE algorithm. It processes each episode to compute the return for each time step, then calculates the gradient of the log-policy multiplied by the return. The results are averaged over all episodes, rounded to four decimal places, and returned as a nested list."}
{"task_id": 123, "completion_id": 0, "completion": "```\npython\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # Calculate FLOPs for the dense layer\n    # Each neuron in the output layer is connected to each neuron in the input layer\n    flops_dense = d_in * d_out\n    \n    # Calculate FLOPs for the MoE layer\n    # Each of the k_active experts is connected to the input and output layers\n    flops_moe = k_active * (d_in * d_out)\n    \n    # Calculate the savings in FLOPs\n    flops_savings = flops_dense - flops_moe\n    \n    # Calculate the savings percentage\n    savings_percentage = (flops_savings / flops_dense) * 100 if flops_dense != 0 else 0\n    \n    # Round the results to the nearest 1st decimal\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    \n    return flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded\n\n# Example usage:\nn_experts = 16\nk_active = 4\nd_in = 512\nd_out = 512\n\nflops_dense, flops_moe, savings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"FLOPs for Dense Layer: {flops_dense}\")\nprint(f\"FLOPs for MoE Layer: {flops_moe}\")\nprint(f\"Computational Savings Percentage: {savings_percentage}%\")\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost (in terms of floating-point operations, FLOPs) for both a dense layer and a Mixture-of-Experts (MoE) layer, given the number of experts, the number of active experts, and the input/output dimensions. It then computes the savings in FLOPs and expresses this savings as a percentage. The results are rounded to the nearest first decimal place."}
{"task_id": 124, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n) -> list:\n    # Compute the logits for gating\n    logits = X @ W_g\n    \n    # Add noise to the logits\n    noisy_logits = logits + (X @ W_noise) * N\n    \n    # Compute the softmax of the noisy logits\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    noisy_probs = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    \n    # Top-k gating\n    topk_indices = np.argsort(noisy_probs, axis=1)[:, -k:]\n    topk_mask = np.zeros_like(noisy_probs)\n    np.put_along_axis(topk_mask, topk_indices[:, :, np.newaxis], 1, axis=1)\n    \n    # Renormalize the top-k probabilities\n    topk_probs = noisy_probs * topk_mask\n    topk_probs = topk_probs / np.sum(topk_probs, axis=1, keepdims=True)\n    \n    # Round to the nearest 4th decimal\n    topk_probs_rounded = np.round(topk_probs, 4)\n    \n    # Return as a python list\n    return topk_probs_rounded.tolist()\n\n# Example usage:\n# X = np.random.rand(5, 10)  # 5 samples, 10 features\n# W_g = np.random.rand(10, 3)  # 10 features, 3 experts\n# W_noise = np.random.rand(10, 3)  # 10 features, 3 experts\n# N = np.random.rand(5, 3)  # 5 samples, 3 experts noise\n# k = 2  # Top-2 gating\n# print(noisy_topk_gating(X, W_g, W_noise, N, k))\n```\n\nThis script defines the `noisy_topk_gating` function, which implements the Noisy Top-K Gating mechanism as described. It computes the gating probabilities by adding noise to the logits, performs top-k gating, renormalizes the probabilities, rounds them, and returns the result as a Python list."}
{"task_id": 125, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Number of tokens in the input\n    n_tokens, d_model = x.shape\n    \n    # Compute gating scores for each expert\n    gating_scores = x @ Wg.T  # Shape: (n_tokens, n_experts)\n    \n    # Apply softmax to get gating probabilities\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1, keepdims=True)  # Shape: (n_tokens, n_experts)\n    \n    # Get top-k experts for each token\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]  # Shape: (n_tokens, top_k)\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)  # Shape: (n_tokens, top_k)\n    \n    # Normalize the top-k probabilities\n    top_k_probs /= np.sum(top_k_probs, axis=1, keepdims=True)  # Shape: (n_tokens, top_k)\n    \n    # Initialize output\n    output = np.zeros((n_tokens, d_model))  # Shape: (n_tokens, d_model)\n    \n    # Compute the output for each token\n    for i in range(n_tokens):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            expert_weight = We[expert_idx]  # Shape: (d_model, d_model)\n            output[i] += top_k_probs[i, j] * (x[i] @ expert_weight.T)  # Shape: (d_model,)\n    \n    # Round the results to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Return the output as a Python list\n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Random seed for reproducibility\n    np.random.seed(0)\n    \n    # Example input\n    x = np.random.rand(5, 4)  # 5 tokens, each of dimension 4\n    We = np.random.rand(3, 4, 4)  # 3 experts, each with a weight matrix of shape (4, 4)\n    Wg = np.random.rand(4, 3)  # Gating network weights, shape (4, 3)\n    n_experts = 3\n    top_k = 2\n    \n    # Compute MoE output\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Check if the number of channels is divisible by the number of groups\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of channels must be divisible by number of groups\"\n    \n    # Reshape X to (B, num_groups, C // num_groups, H, W)\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    \n    # Compute mean and variance along the group, height, and width dimensions\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    \n    # Normalize X_reshaped\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape X_normalized back to (B, C, H, W)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply learned scale (gamma) and shift (beta)\n    X_normalized = gamma * X_normalized + beta\n    \n    # Round the result to the nearest 4th decimal\n    X_normalized = np.round(X_normalized, 4)\n    \n    # Convert to list\n    result = X_normalized.tolist()\n    \n    return result\n\n# Example usage:\n# B, C, H, W = 2, 6, 4, 4\n# X = np.random.randn(B, C, H, W)\n# gamma = np.random.randn(C)\n# beta = np.random.randn(C)\n# num_groups = 2\n# print(group_normalization(X, gamma, beta, num_groups))\n```\n\nThis script defines a function `group_normalization` that performs group normalization on a 4D input tensor. It includes checks, reshaping, normalization, and application of learned parameters, followed by rounding and conversion to a list. An example usage is provided in comments."}
{"task_id": 127, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function f(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    # Define the derivative of the function f'(x)\n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    # Initialize the starting point\n    x = start_x\n    \n    # Gradient descent loop\n    for _ in range(max_iters):\n        # Calculate the gradient at the current point\n        gradient = df(x)\n        \n        # Update the position\n        new_x = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(new_x - x) < tolerance:\n            break\n        \n        # Update x for the next iteration\n        x = new_x\n    \n    # Return the result rounded to the nearest 4th decimal\n    return round(x, 4)\n\n# Example usage:\n# start_x can be any random initial position, e.g., 0.0\ninitial_position = 0.0\ntreasure_location = find_treasure(initial_position)\nprint(f\"The treasure is located at x = {treasure_location}\")\n```\n\nThis script defines a function `find_treasure` that uses gradient descent to find the minimum of the function \\( f(x) = x^4 - 3x^3 + 2 \\). The function takes an initial position `start_x`, a `learning_rate`, a `tolerance` for convergence, and a maximum number of iterations `max_iters`. The result is rounded to the nearest 4th decimal place."}
{"task_id": 128, "completion_id": 0, "completion": "```\npython\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the Dynamic Tanh function\n    # y = alpha * tanh(gamma * x + beta)\n    y = alpha * np.tanh(gamma * x + beta)\n    \n    # Round all results to the nearest 4th decimal\n    y_rounded = np.round(y, 4)\n    \n    # Convert to list\n    y_list = y_rounded.tolist()\n    \n    return y_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample input\n    x = np.array([0.5, 1.0, -0.5, -1.0])\n    alpha = 1.5\n    gamma = 2.0\n    beta = 0.1\n    \n    # Compute Dynamic Tanh\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    \n    # Print result\n    print(result)\n```\n\nThis script defines the `dynamic_tanh` function as specified, applying the transformation and rounding the results to the nearest 4th decimal place. The example usage demonstrates how to call the function with sample input values."}
