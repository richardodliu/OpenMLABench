{"task_id": 0, "completion_id": 0, "solution": ""}
{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": ""}
{"task_id": 13, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef knn_recommend(data: np.ndarray, user_ind: int, k: int, criterion: str='cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    (n_users, n_items) = data.shape\n    S = np.zeros((n_items, n_items))\n    for i in range(n_items):\n        for j in range(i + 1, n_items):\n            rated_both = np.where((data[:, i] > 0) & (data[:, j] > 0))[0]\n            if len(rated_both) == 0:\n                S[i, j] = S[j, i] = 0\n                continue\n            v1 = data[rated_both, i]\n            v2 = data[rated_both, j]\n            if criterion == 'cosine':\n                if np.std(v1) > 0.001 and np.std(v2) > 0.001:\n                    v1_centered = v1 - np.mean(v1)\n                    v2_centered = v2 - np.mean(v2)\n                    S[i, j] = np.dot(v1_centered, v2_centered) / (np.linalg.norm(v1_centered) * np.linalg.norm(v2_centered))\n                else:\n                    S[i, j] = S[j, i] = 0\n            elif criterion == 'pearson':\n                S[i, j] = np.corrcoef(v1, v2)[0, 1]\n            S[j, i] = S[i, j]\n    rated_items = np.where(data[user_ind, :] > 0)[0]\n    r = data[user_ind, rated_items]\n    scores = defaultdict(float)\n    for t in range(n_items):\n        if t not in rated_items:\n            numerator = np.sum(r * S[t, rated_items])\n            denominator = np.sum(np.abs(S[t, rated_items]))\n            if denominator > 0:\n                scores[t] = numerator / denominator\n    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    top_k_items = [item for (item, score) in sorted_scores][:k]\n    return top_k_items"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef best_split(X: np.ndarray, g: np.ndarray, h: np.ndarray, gamma: float, lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    best_gain = -np.inf\n    best_feature = None\n    best_threshold = None\n    G = np.sum(g)\n    H = np.sum(h)\n    L = -0.5 * G ** 2 / (H + lam) + gamma\n    for feature_index in range(n_features):\n        unique_thresholds = np.unique(X[:, feature_index])\n        (GL, HL) = (0, 0)\n        sorted_indices = np.argsort(X[:, feature_index])\n        for i in sorted_indices:\n            GL += g[i]\n            HL += h[i]\n            if i < n_samples - 1:\n                GR = G - GL\n                HR = H - HL\n                if HL > 0 and HR > 0:\n                    gain = L - (-0.5 * GL ** 2 / (HL + lam) + gamma) - (-0.5 * GR ** 2 / (HR + lam) + gamma)\n                    if gain > best_gain:\n                        best_gain = gain\n                        best_feature = feature_index\n                        best_threshold = (X[sorted_indices[i], feature_index] + X[sorted_indices[i + 1], feature_index]) / 2\n    return (best_feature, best_threshold) if best_gain > 0 else None"}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": ""}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": ""}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": ""}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int=5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    if n_clf < 1:\n        n_clf = 1\n    (m, n) = X_train.shape\n    weights = np.ones(m) / m\n    stumps = []\n    stump_weights = []\n\n    def weighted_error(y_true, y_pred, weights):\n        return np.sum(weights * (y_true != y_pred)) / np.sum(weights)\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_stump = None\n        best_threshold = None\n        best_polarity = None\n        for feature_i in range(n):\n            thresholds = np.unique(X_train[:, feature_i])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = np.ones(m)\n                    predictions[X_train[:, feature_i] * polarity < threshold * polarity] = -1\n                    error = weighted_error(y_train, predictions, weights)\n                    if error < min_error:\n                        min_error = error\n                        best_stump = feature_i\n                        best_threshold = threshold\n                        best_polarity = polarity\n        epsilon = min_error\n        alpha = 0.5 * np.log((1 - epsilon) / (epsilon + 1e-10))\n        stump_weights.append(alpha)\n        predictions = np.ones(m)\n        predictions[X_train[:, best_stump] * best_polarity < best_threshold * best_polarity] = -1\n        weights *= np.exp(-alpha * y_train * predictions)\n        weights /= np.sum(weights)\n        stumps.append((best_stump, best_threshold, best_polarity))\n\n    def predict_sample(sample, stumps, stump_weights):\n        stump_sum = 0\n        for ((feature_i, threshold, polarity), alpha) in zip(stumps, stump_weights):\n            stump_sum += alpha * (-1 if sample[feature_i] * polarity < threshold * polarity else 1)\n        return np.sign(stump_sum)\n    return [predict_sample(x, stumps, stump_weights) for x in X_test]"}
{"task_id": 128, "completion_id": 0, "solution": ""}
{"task_id": 140, "completion_id": 0, "solution": "from collections import deque\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    n = len(grid)\n    if n == 1 and grid[0] == '.':\n        return 0\n    if grid[0][0] == 'F' or grid[n - 1][n - 1] == 'F':\n        return -1\n    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    fire_queue = deque()\n    for i in range(n):\n        for j in range(n):\n            if grid[i][j] == 'F':\n                fire_queue.append((i, j, 0))\n    agent_queue = deque([(0, 0, 0)])\n    agent_visited = {(0, 0, 0)}\n    while fire_queue and agent_queue:\n        while fire_queue and fire_queue[0][2] == fire_queue[-1][2]:\n            (fx, fy, ft) = fire_queue.popleft()\n            for (dx, dy) in directions:\n                (nx, ny) = (fx + dx, fy + dy)\n                if 0 <= nx < n and 0 <= ny < n and (grid[nx][ny] == '.'):\n                    grid[nx] = grid[nx][:ny] + 'F' + grid[nx][ny + 1:]\n        (ax, ay, at) = agent_queue.popleft()\n        if ax == n - 1 and ay == n - 1:\n            return at\n        for (dx, dy) in directions:\n            (nx, ny) = (ax + dx, ay + dy)\n            nt = at + 1\n            if 0 <= nx < n and 0 <= ny < n and (grid[nx][ny] == '.') and ((nx, ny, nt) not in agent_visited):\n                agent_visited.add((nx, ny, nt))\n                agent_queue.append((nx, ny, nt))\n    return -1"}
{"task_id": 141, "completion_id": 0, "solution": ""}
{"task_id": 146, "completion_id": 0, "solution": ""}
{"task_id": 155, "completion_id": 0, "solution": ""}
{"task_id": 160, "completion_id": 0, "solution": ""}
{"task_id": 165, "completion_id": 0, "solution": ""}
{"task_id": 169, "completion_id": 0, "solution": ""}
{"task_id": 171, "completion_id": 0, "solution": ""}
{"task_id": 176, "completion_id": 0, "solution": ""}
{"task_id": 178, "completion_id": 0, "solution": ""}
{"task_id": 180, "completion_id": 0, "solution": ""}
{"task_id": 184, "completion_id": 0, "solution": ""}
{"task_id": 190, "completion_id": 0, "solution": ""}
{"task_id": 191, "completion_id": 0, "solution": ""}
{"task_id": 197, "completion_id": 0, "solution": ""}
{"task_id": 198, "completion_id": 0, "solution": ""}
{"task_id": 202, "completion_id": 0, "solution": ""}
{"task_id": 216, "completion_id": 0, "solution": ""}
{"task_id": 217, "completion_id": 0, "solution": ""}
{"task_id": 218, "completion_id": 0, "solution": "def blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n\n    def hand_value(hand: list[int]) -> int:\n        \"\"\"Calculate the value of a hand in Blackjack.\"\"\"\n        value = sum(hand)\n        aces = hand.count(1)\n        while value + 10 <= 21 and aces > 0:\n            value += 10\n            aces -= 1\n        return value\n\n    def is_natural_blackjack(hand: list[int]) -> bool:\n        \"\"\"Check if a hand is a natural blackjack.\"\"\"\n        return len(hand) == 2 and 1 in hand and (10 in hand)\n    player_busts = hand_value(player) > 21\n    dealer_busts = hand_value(dealer) > 21\n    player_score = hand_value(player)\n    dealer_score = hand_value(dealer)\n    if player_busts:\n        return -1\n    elif dealer_busts:\n        return 1\n    elif is_natural_blackjack(player):\n        return 1.5\n    elif player_score > dealer_score:\n        return 1\n    elif player_score < dealer_score:\n        return -1\n    else:\n        return 0"}
{"task_id": 221, "completion_id": 0, "solution": ""}
{"task_id": 222, "completion_id": 0, "solution": ""}
{"task_id": 224, "completion_id": 0, "solution": ""}
{"task_id": 226, "completion_id": 0, "solution": ""}
{"task_id": 241, "completion_id": 0, "solution": ""}
{"task_id": 243, "completion_id": 0, "solution": ""}
{"task_id": 249, "completion_id": 0, "solution": ""}
{"task_id": 253, "completion_id": 0, "solution": ""}
{"task_id": 256, "completion_id": 0, "solution": ""}
{"task_id": 257, "completion_id": 0, "solution": ""}
{"task_id": 261, "completion_id": 0, "solution": ""}
{"task_id": 266, "completion_id": 0, "solution": ""}
{"task_id": 267, "completion_id": 0, "solution": ""}
{"task_id": 273, "completion_id": 0, "solution": ""}
{"task_id": 286, "completion_id": 0, "solution": ""}
{"task_id": 287, "completion_id": 0, "solution": ""}
{"task_id": 290, "completion_id": 0, "solution": ""}
{"task_id": 292, "completion_id": 0, "solution": ""}
{"task_id": 294, "completion_id": 0, "solution": ""}
{"task_id": 296, "completion_id": 0, "solution": ""}
{"task_id": 298, "completion_id": 0, "solution": ""}
{"task_id": 302, "completion_id": 0, "solution": ""}
{"task_id": 303, "completion_id": 0, "solution": ""}
{"task_id": 304, "completion_id": 0, "solution": ""}
{"task_id": 308, "completion_id": 0, "solution": ""}
{"task_id": 312, "completion_id": 0, "solution": "import numpy as np\ndef blackman_harris(window_len: int, symmetric: bool=False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    if window_len <= 0:\n        return []\n    if window_len == 1:\n        return [1.0]\n    if symmetric:\n        N = window_len - 1\n    else:\n        N = window_len\n    a0 = 0.35875\n    a1 = 0.48829\n    a2 = 0.14128\n    a3 = 0.01168\n    n = np.arange(window_len)\n    bh_window = a0 - a1 * np.cos(2 * np.pi * n / N) + a2 * np.cos(4 * np.pi * n / N) - a3 * np.cos(6 * np.pi * n / N)\n    if not symmetric:\n        bh_window = bh_window[:-1]\n    return [round(x, 4) for x in bh_window]"}
{"task_id": 313, "completion_id": 0, "solution": ""}
{"task_id": 317, "completion_id": 0, "solution": ""}
{"task_id": 318, "completion_id": 0, "solution": ""}
{"task_id": 329, "completion_id": 0, "solution": ""}
{"task_id": 331, "completion_id": 0, "solution": ""}
{"task_id": 332, "completion_id": 0, "solution": ""}
{"task_id": 336, "completion_id": 0, "solution": ""}
{"task_id": 340, "completion_id": 0, "solution": ""}
{"task_id": 343, "completion_id": 0, "solution": ""}
{"task_id": 353, "completion_id": 0, "solution": ""}
{"task_id": 354, "completion_id": 0, "solution": ""}
{"task_id": 355, "completion_id": 0, "solution": ""}
{"task_id": 356, "completion_id": 0, "solution": "import numpy"}
{"task_id": 357, "completion_id": 0, "solution": ""}
{"task_id": 362, "completion_id": 0, "solution": ""}
{"task_id": 363, "completion_id": 0, "solution": ""}
{"task_id": 369, "completion_id": 0, "solution": ""}
{"task_id": 371, "completion_id": 0, "solution": ""}
{"task_id": 373, "completion_id": 0, "solution": ""}
{"task_id": 374, "completion_id": 0, "solution": ""}
{"task_id": 376, "completion_id": 0, "solution": ""}
{"task_id": 377, "completion_id": 0, "solution": ""}
{"task_id": 380, "completion_id": 0, "solution": ""}
{"task_id": 387, "completion_id": 0, "solution": ""}
{"task_id": 394, "completion_id": 0, "solution": ""}
{"task_id": 398, "completion_id": 0, "solution": ""}
{"task_id": 411, "completion_id": 0, "solution": ""}
{"task_id": 413, "completion_id": 0, "solution": ""}
{"task_id": 416, "completion_id": 0, "solution": ""}
{"task_id": 419, "completion_id": 0, "solution": ""}
{"task_id": 423, "completion_id": 0, "solution": ""}
{"task_id": 428, "completion_id": 0, "solution": ""}
{"task_id": 433, "completion_id": 0, "solution": ""}
{"task_id": 435, "completion_id": 0, "solution": ""}
{"task_id": 437, "completion_id": 0, "solution": ""}
{"task_id": 438, "completion_id": 0, "solution": ""}
{"task_id": 439, "completion_id": 0, "solution": ""}
{"task_id": 440, "completion_id": 0, "solution": ""}
{"task_id": 444, "completion_id": 0, "solution": ""}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef entropy(labels):\n    \"\"\"Calculate the Shannon entropy of a list of labels.\"\"\"\n    if len(labels) == 0:\n        return 0.0\n    label_counts = Counter(labels)\n    probabilities = [count / len(labels) for count in label_counts.values()]\n    return -sum((p * np.log2(p) for p in probabilities if p > 0))\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    (n_samples, n_features) = X.shape\n    parent_entropy = entropy(y)\n    best_gain = 0.0\n    best_feature_index = -1\n    best_threshold = None\n    left_majority_label = None\n    right_majority_label = None\n    majority_label = Counter(y).most_common(1)[0][0]\n    for j in range(n_features):\n        sorted_indices = np.argsort(X[:, j])\n        sorted_X = X[sorted_indices, j]\n        sorted_y = y[sorted_indices]\n        for i in range(n_samples - 1):\n            if sorted_X[i] != sorted_X[i + 1]:\n                t = (sorted_X[i] + sorted_X[i + 1]) / 2.0\n                left_y = sorted_y[:i + 1]\n                right_y = sorted_y[i + 1:]\n                p_left = len(left_y) / n_samples\n                p_right = len(right_y) / n_samples\n                current_gain = parent_entropy - (p_left * entropy(left_y) + p_right * entropy(right_y))\n                if current_gain > best_gain:\n                    best_gain = current_gain\n                    best_feature_index = j\n                    best_threshold = round(t, 4)\n                    left_majority_label = Counter(left_y).most_common(1)[0][0]\n                    right_majority_label = Counter(right_y).most_common(1)[0][0]\n    if best_gain == 0.0:\n        return (-1, None, majority_label, majority_label)\n    return (best_feature_index, best_threshold, left_majority_label, right_majority_label)"}
{"task_id": 452, "completion_id": 0, "solution": ""}
{"task_id": 453, "completion_id": 0, "solution": ""}
{"task_id": 458, "completion_id": 0, "solution": ""}
{"task_id": 461, "completion_id": 0, "solution": ""}
{"task_id": 471, "completion_id": 0, "solution": ""}
{"task_id": 474, "completion_id": 0, "solution": ""}
{"task_id": 475, "completion_id": 0, "solution": ""}
{"task_id": 479, "completion_id": 0, "solution": ""}
{"task_id": 481, "completion_id": 0, "solution": ""}
{"task_id": 482, "completion_id": 0, "solution": ""}
{"task_id": 485, "completion_id": 0, "solution": ""}
{"task_id": 490, "completion_id": 0, "solution": ""}
{"task_id": 491, "completion_id": 0, "solution": ""}
{"task_id": 492, "completion_id": 0, "solution": ""}
{"task_id": 493, "completion_id": 0, "solution": ""}
{"task_id": 496, "completion_id": 0, "solution": ""}
{"task_id": 499, "completion_id": 0, "solution": ""}
{"task_id": 500, "completion_id": 0, "solution": ""}
{"task_id": 505, "completion_id": 0, "solution": ""}
{"task_id": 509, "completion_id": 0, "solution": ""}
{"task_id": 510, "completion_id": 0, "solution": ""}
{"task_id": 513, "completion_id": 0, "solution": ""}
{"task_id": 517, "completion_id": 0, "solution": ""}
{"task_id": 518, "completion_id": 0, "solution": ""}
{"task_id": 520, "completion_id": 0, "solution": ""}
{"task_id": 528, "completion_id": 0, "solution": ""}
{"task_id": 537, "completion_id": 0, "solution": ""}
{"task_id": 539, "completion_id": 0, "solution": ""}
{"task_id": 552, "completion_id": 0, "solution": ""}
{"task_id": 555, "completion_id": 0, "solution": ""}
{"task_id": 561, "completion_id": 0, "solution": ""}
{"task_id": 562, "completion_id": 0, "solution": ""}
{"task_id": 563, "completion_id": 0, "solution": ""}
{"task_id": 564, "completion_id": 0, "solution": ""}
{"task_id": 565, "completion_id": 0, "solution": ""}
{"task_id": 566, "completion_id": 0, "solution": ""}
{"task_id": 568, "completion_id": 0, "solution": "import math\nfrom collections import defaultdict\nimport numpy as np\ndef maxent_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, epsilon: float=0.001, n_iter: int=100) -> list[int]:\n    \"\"\"Predict labels for X_test using a Maximum Entropy Classifier with GIS.\"\"\"\n    unique_features = set()\n    unique_labels = set(y_train)\n    for sample in X_train:\n        for feature in sample:\n            unique_features.add(feature)\n    empirical_expectations = defaultdict(float)\n    model_expectations = defaultdict(float)\n    for (x, y) in zip(X_train, y_train):\n        for feature in x:\n            empirical_expectations[feature, y] += 1\n    total_samples = len(X_train)\n    for key in empirical_expectations:\n        empirical_expectations[key] /= total_samples\n    weights = defaultdict(float)\n    for iteration in range(n_iter):\n        model_expectations.clear()\n        Z = {}\n        for (i, x) in enumerate(X_train):\n            Z_x = 0\n            for y in unique_labels:\n                score = 0\n                for feature in x:\n                    score += weights[feature, y]\n                Z_x += math.exp(score)\n            Z[i] = Z_x\n        for (i, (x, y)) in enumerate(zip(X_train, y_train)):\n            for feature in x:\n                score = 0\n                for label in unique_labels:\n                    score += weights[feature, label]\n                prob = math.exp(score) / Z[i]\n                model_expectations[feature, y] += prob\n        for key in model_expectations:\n            model_expectations[key] /= total_samples\n        max_update = 0\n        for (feature, y) in empirical_expectations:\n            update = empirical_expectations[feature, y] - model_expectations[feature, y]\n            weights[feature, y] += update\n            max_update = max(max_update, abs(update))\n        if max_update < epsilon:\n            break\n    predictions = []\n    for x in X_test:\n        max_score = -float('inf')\n        best_label = None\n        for y in unique_labels:\n            score = 0\n            for feature in x:\n                score += weights[feature, y]\n            if score > max_score:\n                max_score = score\n                best_label = y\n        predictions.append(best_label)\n    return predictions"}
{"task_id": 569, "completion_id": 0, "solution": ""}
{"task_id": 571, "completion_id": 0, "solution": ""}
{"task_id": 572, "completion_id": 0, "solution": ""}
{"task_id": 574, "completion_id": 0, "solution": ""}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    centres = X[:k].copy()\n    for _ in range(max_iters):\n        labels = []\n        for point in X:\n            distances = np.linalg.norm(point - centres, axis=1)\n            min_index = np.argmin(distances)\n            labels.append(min_index)\n        new_centres = np.zeros_like(centres)\n        counts = np.zeros(k, dtype=int)\n        for (label, point) in zip(labels, X):\n            new_centres[label] += point\n            counts[label] += 1\n        for i in range(k):\n            if counts[i] > 0:\n                new_centres[i] /= counts[i]\n            else:\n                new_centres[i] = centres[i]\n        if np.allclose(new_centres, centres, atol=0.0001):\n            break\n        centres = new_centres.copy()\n    centres = np.round(centres, 4).tolist()\n    return (labels, centres)"}
{"task_id": 581, "completion_id": 0, "solution": ""}
{"task_id": 591, "completion_id": 0, "solution": ""}
{"task_id": 595, "completion_id": 0, "solution": ""}
