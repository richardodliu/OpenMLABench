{"task_id": 0, "completion_id": 0, "solution": ""}
{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": ""}
{"task_id": 13, "completion_id": 0, "solution": ""}
{"task_id": 19, "completion_id": 0, "solution": ""}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": "import numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    if not A or not B or (not pi) or (not obs):\n        return 0.0\n    A = np.array(A)\n    B = np.array(B)\n    pi = np.array(pi)\n    obs = np.array(obs)\n    N = A.shape[0]\n    T = len(obs)\n    beta = np.zeros((N, T))\n    beta[:, T - 1] = 1.0\n    for t in range(T - 2, -1, -1):\n        for i in range(N):\n            beta[i, t] = np.sum(A[i, :] * B[:, obs[t + 1]] * beta[:, t + 1])\n    prob = np.sum(pi * B[:, obs[0]] * beta[:, 0])\n    return round(prob, 6)"}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nfrom typing import List, Tuple\nclass FPNode:\n\n    def __init__(self, item: str, support: int=0):\n        self.item = item\n        self.support = support\n        self.parent = None\n        self.children = defaultdict(FPNode)\n        self.link = None\nclass FPTree:\n\n    def __init__(self):\n        self.root = FPNode(None)\n        self.header_table = defaultdict(lambda : {'head': None, 'tail': None, 'support': 0})\n\n    def add_transaction(self, transaction: List[str], support: int=1):\n        current_node = self.root\n        for item in transaction:\n            if item not in current_node.children:\n                new_node = FPNode(item, support)\n                current_node.children[item] = new_node\n                new_node.parent = current_node\n                self.update_header_table(item, new_node)\n            else:\n                current_node.children[item].support += support\n            current_node = current_node.children[item]\n\n    def update_header_table(self, item: str, node: FPNode):\n        self.header_table[item]['support'] += node.support\n        if self.header_table[item]['head'] is None:\n            self.header_table[item]['head'] = node\n        if self.header_table[item]['tail']:\n            self.header_table[item]['tail'].link = node\n        self.header_table[item]['tail'] = node\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    if not transactions:\n        return []\n    item_counts = Counter((item for transaction in transactions for item in transaction))\n    frequent_items = {item: support for (item, support) in item_counts.items() if support >= min_support}\n    if not frequent_items:\n        return []\n    frequent_items = dict(sorted(frequent_items.items(), key=lambda x: (-x[1], x[0])))\n    fp_tree = FPTree()\n    for transaction in transactions:\n        filtered_transaction = [item for item in transaction if item in frequent_items]\n        filtered_transaction.sort(key=lambda x: frequent_items[x], reverse=True)\n        fp_tree.add_transaction(filtered_transaction)\n    frequent_itemsets = []\n    for (base_pat, value) in fp_tree.header_table.items():\n        support_count = value['support']\n        frequent_itemsets.append([base_pat])\n        conditional_base_pat = []\n        node = value['tail']\n        while node:\n            prefix_path = []\n            parent = node.parent\n            while parent and parent.item:\n                prefix_path.append(parent.item)\n                parent = parent.parent\n            if prefix_path:\n                conditional_base_pat.append((prefix_path, node.support))\n            node = node.link\n        if conditional_base_pat:\n            cond_tree = FPTree()\n            for (path, support) in conditional_base_pat:\n                cond_tree.add_transaction(path, support)\n            cond_tree.header_table = {v: fp_tree.header_table[v] for v in cond_tree.header_table if cond_tree.header_table[v]['support'] >= min_support}\n            if cond_tree.header_table:\n                mined_sets = fp_growth([t for (t, _) in conditional_base_pat], min_support)\n                for item_set in mined_sets:\n                    frequent_itemsets.append(item_set + [base_pat])\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets"}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": ""}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": ""}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": ""}
{"task_id": 128, "completion_id": 0, "solution": ""}
{"task_id": 140, "completion_id": 0, "solution": ""}
{"task_id": 141, "completion_id": 0, "solution": ""}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    n_train = X.shape[0]\n    n_test = X_test.shape[0]\n    predictions = []\n    for i in range(n_test):\n        distances = []\n        for j in range(n_train):\n            if metric == 'euclidean':\n                dist = np.sqrt(np.sum((X[j] - X_test[i]) ** 2))\n            elif metric == 'manhattan':\n                dist = np.sum(np.abs(X[j] - X_test[i]))\n            elif metric == 'cosine':\n                numerator = np.dot(X[j], X_test[i])\n                denominator = np.linalg.norm(X[j]) * np.linalg.norm(X_test[i])\n                if denominator == 0:\n                    denominator += 1e-12\n                dist = 1 - numerator / denominator\n            else:\n                raise ValueError('Metric not supported')\n            distances.append((dist, y[j]))\n        distances.sort(key=lambda x: x[0])\n        k_nearest = distances[:k]\n        votes = {}\n        for (_, label) in k_nearest:\n            if label in votes:\n                votes[label] += 1\n            else:\n                votes[label] = 1\n        predicted_label = min([label for (label, count) in votes.items() if count == max(votes.values())])\n        predictions.append(predicted_label)\n    return predictions"}
{"task_id": 155, "completion_id": 0, "solution": ""}
{"task_id": 160, "completion_id": 0, "solution": ""}
{"task_id": 165, "completion_id": 0, "solution": ""}
{"task_id": 169, "completion_id": 0, "solution": ""}
{"task_id": 171, "completion_id": 0, "solution": ""}
{"task_id": 176, "completion_id": 0, "solution": ""}
{"task_id": 178, "completion_id": 0, "solution": ""}
{"task_id": 180, "completion_id": 0, "solution": ""}
{"task_id": 184, "completion_id": 0, "solution": ""}
{"task_id": 190, "completion_id": 0, "solution": ""}
{"task_id": 191, "completion_id": 0, "solution": ""}
{"task_id": 197, "completion_id": 0, "solution": ""}
{"task_id": 198, "completion_id": 0, "solution": ""}
{"task_id": 202, "completion_id": 0, "solution": ""}
{"task_id": 216, "completion_id": 0, "solution": ""}
{"task_id": 217, "completion_id": 0, "solution": ""}
{"task_id": 218, "completion_id": 0, "solution": ""}
{"task_id": 221, "completion_id": 0, "solution": ""}
{"task_id": 222, "completion_id": 0, "solution": ""}
{"task_id": 224, "completion_id": 0, "solution": ""}
{"task_id": 226, "completion_id": 0, "solution": ""}
{"task_id": 241, "completion_id": 0, "solution": ""}
{"task_id": 243, "completion_id": 0, "solution": ""}
{"task_id": 249, "completion_id": 0, "solution": ""}
{"task_id": 253, "completion_id": 0, "solution": ""}
{"task_id": 256, "completion_id": 0, "solution": ""}
{"task_id": 257, "completion_id": 0, "solution": ""}
{"task_id": 261, "completion_id": 0, "solution": ""}
{"task_id": 266, "completion_id": 0, "solution": ""}
{"task_id": 267, "completion_id": 0, "solution": ""}
{"task_id": 273, "completion_id": 0, "solution": ""}
{"task_id": 286, "completion_id": 0, "solution": "import random\nfrom typing import Callable, Tuple, List\ndef get_initializer(name: str) -> Callable:\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    initializer = globals().get(name)\n    if callable(initializer):\n        return initializer\n    else:\n        raise ValueError('Invalid initialization function.')"}
{"task_id": 287, "completion_id": 0, "solution": ""}
{"task_id": 290, "completion_id": 0, "solution": ""}
{"task_id": 292, "completion_id": 0, "solution": "import numpy as np\ndef single_point_crossover(parent1: list[list[int | float]], parent2: list[list[int | float]], cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    parent1_np = np.array(parent1)\n    parent2_np = np.array(parent2)\n    if parent1_np.shape != parent2_np.shape:\n        return -1\n    num_cols = parent1_np.shape[1]\n    child1_np = np.concatenate((parent1_np[:, :cutoff], parent2_np[:, cutoff:]), axis=1)\n    child2_np = np.concatenate((parent2_np[:, :cutoff], parent1_np[:, cutoff:]), axis=1)\n    child1 = child1_np.tolist()\n    child2 = child2_np.tolist()\n    return (child1, child2)\nparent1 = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\nparent2 = [[0.9, 0.8, 0.7], [0.6, 0.5, 0.4]]\ncutoff = 1"}
{"task_id": 294, "completion_id": 0, "solution": ""}
{"task_id": 296, "completion_id": 0, "solution": ""}
{"task_id": 298, "completion_id": 0, "solution": ""}
{"task_id": 302, "completion_id": 0, "solution": ""}
{"task_id": 303, "completion_id": 0, "solution": ""}
{"task_id": 304, "completion_id": 0, "solution": ""}
{"task_id": 308, "completion_id": 0, "solution": ""}
{"task_id": 312, "completion_id": 0, "solution": ""}
{"task_id": 313, "completion_id": 0, "solution": ""}
{"task_id": 317, "completion_id": 0, "solution": ""}
{"task_id": 318, "completion_id": 0, "solution": ""}
{"task_id": 329, "completion_id": 0, "solution": ""}
{"task_id": 331, "completion_id": 0, "solution": ""}
{"task_id": 332, "completion_id": 0, "solution": ""}
{"task_id": 336, "completion_id": 0, "solution": ""}
{"task_id": 340, "completion_id": 0, "solution": ""}
{"task_id": 343, "completion_id": 0, "solution": ""}
{"task_id": 353, "completion_id": 0, "solution": ""}
{"task_id": 354, "completion_id": 0, "solution": ""}
{"task_id": 355, "completion_id": 0, "solution": ""}
{"task_id": 356, "completion_id": 0, "solution": "import numpy as np"}
{"task_id": 357, "completion_id": 0, "solution": ""}
{"task_id": 362, "completion_id": 0, "solution": ""}
{"task_id": 363, "completion_id": 0, "solution": ""}
{"task_id": 369, "completion_id": 0, "solution": ""}
{"task_id": 371, "completion_id": 0, "solution": ""}
{"task_id": 373, "completion_id": 0, "solution": ""}
{"task_id": 374, "completion_id": 0, "solution": ""}
{"task_id": 376, "completion_id": 0, "solution": ""}
{"task_id": 377, "completion_id": 0, "solution": ""}
{"task_id": 380, "completion_id": 0, "solution": ""}
{"task_id": 387, "completion_id": 0, "solution": ""}
{"task_id": 394, "completion_id": 0, "solution": ""}
{"task_id": 398, "completion_id": 0, "solution": ""}
{"task_id": 411, "completion_id": 0, "solution": ""}
{"task_id": 413, "completion_id": 0, "solution": "import numpy as np\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int=10, learning_rate: float=0.1, max_depth: int=3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n\n    def mean_squared_error(y_true, y_pred):\n        return np.mean((y_true - y_pred) ** 2)\n\n    def best_split(X, y):\n        min_mse = np.inf\n        best_feature = None\n        best_threshold = None\n        for feature_idx in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                y_left = y[X[:, feature_idx] <= threshold]\n                y_right = y[X[:, feature_idx] > threshold]\n                mse_left = mean_squared_error(y_left, np.mean(y_left)) if y_left.size > 0 else 0\n                mse_right = mean_squared_error(y_right, np.mean(y_right)) if y_right.size > 0 else 0\n                mse = (mse_left * y_left.size + mse_right * y_right.size) / y.size\n                if mse < min_mse:\n                    min_mse = mse\n                    best_feature = feature_idx\n                    best_threshold = threshold\n        return (best_feature, best_threshold)\n\n    def build_tree(X, y, depth=0):\n        if depth >= max_depth or len(np.unique(y)) == 1:\n            return np.mean(y)\n        (best_feature, best_threshold) = best_split(X, y)\n        if best_feature is None or best_threshold is None:\n            return np.mean(y)\n        X_left = X[X[:, best_feature] <= best_threshold]\n        y_left = y[X[:, best_feature] <= best_threshold]\n        X_right = X[X[:, best_feature] > best_threshold]\n        y_right = y[X[:, best_feature] > best_threshold]\n        left_tree = build_tree(X_left, y_left, depth + 1)\n        right_tree = build_tree(X_right, y_right, depth + 1)\n        return (best_feature, best_threshold, left_tree, right_tree)\n\n    def predict_tree(tree, X):\n        if isinstance(tree, np.float64):\n            return tree\n        (feature_idx, threshold, left_tree, right_tree) = tree\n        if X[feature_idx] <= threshold:\n            return predict_tree(left_tree, X)\n        else:\n            return predict_tree(right_tree, X)\n\n    def predict_trees(trees, X):\n        return np.sum([predict_tree(tree, X) for tree in trees], axis=0)\n    F = np.full(y_train.shape, np.mean(y_train))\n    trees = []\n    for _ in range(n_estimators):\n        residuals = y_train - F\n        tree = build_tree(X_train, residuals)\n        trees.append(tree)\n        F += learning_rate * predict_trees([tree], X_train)\n    final_predictions = np.mean(y_train) + learning_rate * predict_trees(trees, X_test)\n    return [round(pred, 4) for pred in final_predictions.tolist()]"}
{"task_id": 416, "completion_id": 0, "solution": ""}
{"task_id": 419, "completion_id": 0, "solution": ""}
{"task_id": 423, "completion_id": 0, "solution": ""}
{"task_id": 428, "completion_id": 0, "solution": ""}
{"task_id": 433, "completion_id": 0, "solution": ""}
{"task_id": 435, "completion_id": 0, "solution": ""}
{"task_id": 437, "completion_id": 0, "solution": ""}
{"task_id": 438, "completion_id": 0, "solution": ""}
{"task_id": 439, "completion_id": 0, "solution": "import itertools\nfrom collections import defaultdict\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n\n    def generate_candidates(itemsets, k):\n        candidates = set()\n        itemsets = sorted(itemsets)\n        n_items = len(itemsets)\n        for i in range(n_items):\n            for j in range(i + 1, n_items):\n                if itemsets[i][:k - 2] == itemsets[j][:k - 2]:\n                    new_candidate = tuple(sorted(set(itemsets[i]) | set(itemsets[j])))\n                    if all((tuple(new_candidate[:i] + new_candidate[i + 1:]) in itemsets for i in range(k))):\n                        candidates.add(new_candidate)\n        return candidates\n\n    def itemset_support(itemset):\n        return sum((1 for transaction in transactions if set(itemset).issubset(transaction))) / len(transactions)\n    itemsets = [frozenset([item]) for transaction in transactions for item in transaction]\n    itemset_counts = defaultdict(int)\n    for itemset in itemsets:\n        itemset_counts[itemset] += 1\n    min_support_count = min_sup * len(transactions)\n    frequent_itemsets = {itemset for (itemset, count) in itemset_counts.items() if count >= min_support_count}\n    frequent_itemsets = {tuple(itemset) for itemset in frequent_itemsets}\n    size_k_updated = frequent_itemsets\n    frequent_itemsets_list = sorted(frequent_itemsets)\n    k = 2\n    while size_k_updated:\n        candidates = generate_candidates(size_k_updated, k)\n        candidates_support = {candidate: itemset_support(candidate) for candidate in candidates}\n        size_k_updated = {candidate for (candidate, support) in candidates_support.items() if support >= min_sup}\n        frequent_itemsets_list.extend(sorted(size_k_updated))\n        k += 1\n    frequent_itemsets_list.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets_list\ntransactions = [[1, 2, 3, 4], [2, 3, 5], [1, 3, 5], [1, 2, 5], [2, 3]]\nmin_sup = 0.3"}
{"task_id": 440, "completion_id": 0, "solution": ""}
{"task_id": 444, "completion_id": 0, "solution": "import numpy as np\ndef rbf_kernel(X: list[list[int | float]], Y: list[list[int | float]] | None=None, sigma: float | list[float] | None=None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    X = np.array(X)\n    if Y is None:\n        Y = X\n    else:\n        Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        return -1\n    C = X.shape[1]\n    if sigma is None:\n        sigma = np.sqrt(C / 2)\n    elif isinstance(sigma, (int, float)):\n        if sigma <= 0:\n            return -1\n    elif isinstance(sigma, list):\n        if len(sigma) != C or not all((isinstance(s, (int, float)) and s > 0 for s in sigma)):\n            return -1\n    else:\n        return -1\n    diff = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n    if isinstance(sigma, (int, float)):\n        scaled_diff = diff / sigma\n    else:\n        scaled_diff = diff / np.array(sigma)\n    squared_dist = np.sum(scaled_diff ** 2, axis=2)\n    kernel_matrix = np.exp(-0.5 * squared_dist)\n    kernel_matrix_rounded = np.round(kernel_matrix, 4)\n    return kernel_matrix_rounded.tolist()\nX = [[1, 0], [0, 1]]\nY = [[1, 0], [0, 1]]\nsigma = 1.0"}
{"task_id": 446, "completion_id": 0, "solution": ""}
{"task_id": 452, "completion_id": 0, "solution": ""}
{"task_id": 453, "completion_id": 0, "solution": ""}
{"task_id": 458, "completion_id": 0, "solution": ""}
{"task_id": 461, "completion_id": 0, "solution": ""}
{"task_id": 471, "completion_id": 0, "solution": ""}
{"task_id": 474, "completion_id": 0, "solution": ""}
{"task_id": 475, "completion_id": 0, "solution": ""}
{"task_id": 479, "completion_id": 0, "solution": ""}
{"task_id": 481, "completion_id": 0, "solution": ""}
{"task_id": 482, "completion_id": 0, "solution": ""}
{"task_id": 485, "completion_id": 0, "solution": ""}
{"task_id": 490, "completion_id": 0, "solution": ""}
{"task_id": 491, "completion_id": 0, "solution": ""}
{"task_id": 492, "completion_id": 0, "solution": ""}
{"task_id": 493, "completion_id": 0, "solution": ""}
{"task_id": 496, "completion_id": 0, "solution": ""}
{"task_id": 499, "completion_id": 0, "solution": ""}
{"task_id": 500, "completion_id": 0, "solution": ""}
{"task_id": 505, "completion_id": 0, "solution": ""}
{"task_id": 509, "completion_id": 0, "solution": ""}
{"task_id": 510, "completion_id": 0, "solution": ""}
{"task_id": 513, "completion_id": 0, "solution": ""}
{"task_id": 517, "completion_id": 0, "solution": ""}
{"task_id": 518, "completion_id": 0, "solution": "import numpy as np\nfrom typing import Callable\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n\n    def wrapper(actual: np.ndarray, predicted: np.ndarray) -> float:\n        if actual.ndim == 2 and actual.shape[1] > 1:\n            actual = actual.argmax(axis=1)\n        if predicted.ndim == 2 and predicted.shape[1] > 1:\n            predicted = predicted.argmax(axis=1)\n        return function(actual, predicted)\n    return wrapper\nactual = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npredicted = np.array([[0, 1, 0], [0, 1, 0], [0, 0, 1]])"}
{"task_id": 520, "completion_id": 0, "solution": ""}
{"task_id": 528, "completion_id": 0, "solution": ""}
{"task_id": 537, "completion_id": 0, "solution": ""}
{"task_id": 539, "completion_id": 0, "solution": ""}
{"task_id": 552, "completion_id": 0, "solution": ""}
{"task_id": 555, "completion_id": 0, "solution": ""}
{"task_id": 561, "completion_id": 0, "solution": ""}
{"task_id": 562, "completion_id": 0, "solution": ""}
{"task_id": 563, "completion_id": 0, "solution": ""}
{"task_id": 564, "completion_id": 0, "solution": ""}
{"task_id": 565, "completion_id": 0, "solution": ""}
{"task_id": 566, "completion_id": 0, "solution": ""}
{"task_id": 568, "completion_id": 0, "solution": ""}
{"task_id": 569, "completion_id": 0, "solution": ""}
{"task_id": 571, "completion_id": 0, "solution": ""}
{"task_id": 572, "completion_id": 0, "solution": ""}
{"task_id": 574, "completion_id": 0, "solution": ""}
{"task_id": 579, "completion_id": 0, "solution": ""}
{"task_id": 581, "completion_id": 0, "solution": ""}
{"task_id": 591, "completion_id": 0, "solution": ""}
{"task_id": 595, "completion_id": 0, "solution": ""}
