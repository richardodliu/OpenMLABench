{"task_id": 0, "completion_id": 0, "solution": ""}
{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    np.random.seed(0)\n\n    def build_tree(indices):\n        if len(indices) < 3:\n            return {'type': 'leaf', 'indices': indices}\n        feature_idx = np.random.choice(data.shape[1])\n        feature_values = data[indices, feature_idx]\n        (down, up) = (feature_values.min(), feature_values.max())\n        v = np.random.uniform(down, up)\n        left_indices = indices[feature_values <= v]\n        right_indices = indices[feature_values > v]\n        return {'type': 'internal', 'feature_idx': feature_idx, 'v': v, 'left': build_tree(left_indices), 'right': build_tree(right_indices)}\n\n    def get_depth(node, sample_idx, depth=0):\n        if node['type'] == 'leaf':\n            return depth\n        feature_idx = node['feature_idx']\n        v = node['v']\n        if data[sample_idx, feature_idx] <= v:\n            return get_depth(node['left'], sample_idx, depth + 1)\n        else:\n            return get_depth(node['right'], sample_idx, depth + 1)\n    indices = np.arange(data.shape[0])\n    tree = build_tree(indices)\n    depths = [get_depth(tree, i) for i in range(data.shape[0])]\n    return depths\ndata = np.arange(5).reshape(-1, 1)"}
{"task_id": 7, "completion_id": 0, "solution": ""}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float=0.001, max_iter: int=2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    centroids = data[:k].copy()\n    for iteration in range(max_iter):\n        labels = np.argmin(np.sum((data[:, np.newaxis] - centroids) ** 2, axis=2), axis=1)\n        new_centroids = np.array([data[labels == i].mean(axis=0) if np.any(labels == i) else data[np.random.randint(data.shape[0])] for i in range(k)])\n        for i in range(k):\n            if not np.any(labels == i):\n                new_centroids[i] = data[np.random.randint(data.shape[0])]\n        max_movement = np.max(np.sum((new_centroids - centroids) ** 2, axis=1))\n        if max_movement < epsilon:\n            break\n        centroids = new_centroids\n    rounded_centroids = np.round(centroids, 4).tolist()\n    return (rounded_centroids, labels.tolist())"}
{"task_id": 13, "completion_id": 0, "solution": ""}
{"task_id": 19, "completion_id": 0, "solution": ""}
{"task_id": 20, "completion_id": 0, "solution": ""}
{"task_id": 21, "completion_id": 0, "solution": ""}
{"task_id": 25, "completion_id": 0, "solution": ""}
{"task_id": 28, "completion_id": 0, "solution": ""}
{"task_id": 29, "completion_id": 0, "solution": ""}
{"task_id": 34, "completion_id": 0, "solution": ""}
{"task_id": 39, "completion_id": 0, "solution": ""}
{"task_id": 40, "completion_id": 0, "solution": ""}
{"task_id": 48, "completion_id": 0, "solution": ""}
{"task_id": 55, "completion_id": 0, "solution": ""}
{"task_id": 56, "completion_id": 0, "solution": ""}
{"task_id": 58, "completion_id": 0, "solution": ""}
{"task_id": 62, "completion_id": 0, "solution": ""}
{"task_id": 63, "completion_id": 0, "solution": ""}
{"task_id": 65, "completion_id": 0, "solution": ""}
{"task_id": 69, "completion_id": 0, "solution": ""}
{"task_id": 70, "completion_id": 0, "solution": ""}
{"task_id": 75, "completion_id": 0, "solution": ""}
{"task_id": 76, "completion_id": 0, "solution": ""}
{"task_id": 77, "completion_id": 0, "solution": ""}
{"task_id": 81, "completion_id": 0, "solution": ""}
{"task_id": 82, "completion_id": 0, "solution": ""}
{"task_id": 86, "completion_id": 0, "solution": ""}
{"task_id": 88, "completion_id": 0, "solution": ""}
{"task_id": 90, "completion_id": 0, "solution": ""}
{"task_id": 96, "completion_id": 0, "solution": ""}
{"task_id": 108, "completion_id": 0, "solution": "import numpy as np\ndef als_factorization(X: np.ndarray, K: int, alpha: float=1.0, max_iter: int=200, tol: float=0.0001) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    np.random.seed(0)\n    (N, M) = X.shape\n    W = np.random.rand(N, K)\n    H = np.random.rand(K, M)\n\n    def compute_loss():\n        return np.linalg.norm(X - W @ H, 'fro') ** 2 + alpha * (np.linalg.norm(W, 'fro') ** 2 + np.linalg.norm(H, 'fro') ** 2)\n    for _ in range(max_iter):\n        for i in range(N):\n            W[i, :] = np.linalg.solve(H @ H.T + alpha * np.eye(K), H @ X[i, :])\n        for j in range(M):\n            H[:, j] = np.linalg.solve(W.T @ W + alpha * np.eye(K), W.T @ X[:, j])\n        loss = compute_loss()\n        if loss < tol:\n            break\n    X_hat = W @ H\n    return X_hat.round(4).tolist()"}
{"task_id": 109, "completion_id": 0, "solution": ""}
{"task_id": 111, "completion_id": 0, "solution": ""}
{"task_id": 113, "completion_id": 0, "solution": ""}
{"task_id": 115, "completion_id": 0, "solution": ""}
{"task_id": 118, "completion_id": 0, "solution": ""}
{"task_id": 128, "completion_id": 0, "solution": ""}
{"task_id": 140, "completion_id": 0, "solution": ""}
{"task_id": 141, "completion_id": 0, "solution": ""}
{"task_id": 146, "completion_id": 0, "solution": "import numpy as np\ndef knn_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray, k: int=3, metric: str='euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    k = min(k, len(X))\n\n    def euclidean_distance(a, b):\n        return np.sqrt(np.sum((a - b) ** 2))\n\n    def manhattan_distance(a, b):\n        return np.sum(np.abs(a - b))\n\n    def cosine_distance(a, b):\n        epsilon = 1e-12\n        dot_product = np.dot(a, b)\n        norm_a = np.linalg.norm(a)\n        norm_b = np.linalg.norm(b)\n        return 1 - dot_product / (norm_a * norm_b + epsilon)\n    if metric == 'euclidean':\n        distance_func = euclidean_distance\n    elif metric == 'manhattan':\n        distance_func = manhattan_distance\n    elif metric == 'cosine':\n        distance_func = cosine_distance\n    else:\n        raise ValueError(\"Unsupported distance metric. Choose from 'euclidean', 'manhattan', or 'cosine'.\")\n    predictions = []\n    for test_sample in X_test:\n        distances = [distance_func(test_sample, x_train) for x_train in X]\n        k_indices = np.argsort(distances)[:k]\n        k_nearest_labels = [y[i] for i in k_indices]\n        label_counts = {}\n        for label in k_nearest_labels:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n        predicted_label = min(label_counts, key=lambda x: (-label_counts[x], x))\n        predictions.append(predicted_label)\n    return predictions"}
{"task_id": 155, "completion_id": 0, "solution": ""}
{"task_id": 160, "completion_id": 0, "solution": ""}
{"task_id": 165, "completion_id": 0, "solution": ""}
{"task_id": 169, "completion_id": 0, "solution": ""}
{"task_id": 171, "completion_id": 0, "solution": ""}
{"task_id": 176, "completion_id": 0, "solution": ""}
{"task_id": 178, "completion_id": 0, "solution": ""}
{"task_id": 180, "completion_id": 0, "solution": ""}
{"task_id": 184, "completion_id": 0, "solution": ""}
{"task_id": 190, "completion_id": 0, "solution": ""}
{"task_id": 191, "completion_id": 0, "solution": ""}
{"task_id": 197, "completion_id": 0, "solution": ""}
{"task_id": 198, "completion_id": 0, "solution": ""}
{"task_id": 202, "completion_id": 0, "solution": ""}
{"task_id": 216, "completion_id": 0, "solution": ""}
{"task_id": 217, "completion_id": 0, "solution": ""}
{"task_id": 218, "completion_id": 0, "solution": ""}
{"task_id": 221, "completion_id": 0, "solution": ""}
{"task_id": 222, "completion_id": 0, "solution": ""}
{"task_id": 224, "completion_id": 0, "solution": ""}
{"task_id": 226, "completion_id": 0, "solution": ""}
{"task_id": 241, "completion_id": 0, "solution": ""}
{"task_id": 243, "completion_id": 0, "solution": ""}
{"task_id": 249, "completion_id": 0, "solution": ""}
{"task_id": 253, "completion_id": 0, "solution": ""}
{"task_id": 256, "completion_id": 0, "solution": ""}
{"task_id": 257, "completion_id": 0, "solution": ""}
{"task_id": 261, "completion_id": 0, "solution": ""}
{"task_id": 266, "completion_id": 0, "solution": ""}
{"task_id": 267, "completion_id": 0, "solution": ""}
{"task_id": 273, "completion_id": 0, "solution": ""}
{"task_id": 286, "completion_id": 0, "solution": ""}
{"task_id": 287, "completion_id": 0, "solution": ""}
{"task_id": 290, "completion_id": 0, "solution": ""}
{"task_id": 292, "completion_id": 0, "solution": ""}
{"task_id": 294, "completion_id": 0, "solution": ""}
{"task_id": 296, "completion_id": 0, "solution": ""}
{"task_id": 298, "completion_id": 0, "solution": ""}
{"task_id": 302, "completion_id": 0, "solution": ""}
{"task_id": 303, "completion_id": 0, "solution": ""}
{"task_id": 304, "completion_id": 0, "solution": ""}
{"task_id": 308, "completion_id": 0, "solution": ""}
{"task_id": 312, "completion_id": 0, "solution": ""}
{"task_id": 313, "completion_id": 0, "solution": ""}
{"task_id": 317, "completion_id": 0, "solution": ""}
{"task_id": 318, "completion_id": 0, "solution": ""}
{"task_id": 329, "completion_id": 0, "solution": ""}
{"task_id": 331, "completion_id": 0, "solution": ""}
{"task_id": 332, "completion_id": 0, "solution": ""}
{"task_id": 336, "completion_id": 0, "solution": ""}
{"task_id": 340, "completion_id": 0, "solution": ""}
{"task_id": 343, "completion_id": 0, "solution": ""}
{"task_id": 353, "completion_id": 0, "solution": ""}
{"task_id": 354, "completion_id": 0, "solution": ""}
{"task_id": 355, "completion_id": 0, "solution": ""}
{"task_id": 356, "completion_id": 0, "solution": ""}
{"task_id": 357, "completion_id": 0, "solution": ""}
{"task_id": 362, "completion_id": 0, "solution": ""}
{"task_id": 363, "completion_id": 0, "solution": "from typing import Any, Dict, Iterable, Tuple as PyTuple\nclass Space:\n    pass\nclass Box(Space):\n\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\nclass Tuple(Space):\n\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\nclass Dict(Space):\n\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool) -> Tuple[bool, bool]:\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n\n    def all_boxes(spaces: Iterable[Space]) -> bool:\n        \"\"\"Check if all spaces in the iterable are instances of Box.\"\"\"\n        return all((isinstance(space, Box) for space in spaces))\n\n    def is_space_continuous(space: Space, is_tuple: bool) -> bool:\n        \"\"\"Determine if a space is continuous based on its type and sub-spaces.\"\"\"\n        if isinstance(space, Box):\n            return True\n        if is_tuple:\n            if isinstance(space, Tuple):\n                return all_boxes(space.spaces)\n            elif isinstance(space, Dict):\n                return all_boxes(space.spaces.values())\n        return False\n    cont_action = is_space_continuous(env.action_space, tuple_action)\n    cont_obs = is_space_continuous(env.observation_space, tuple_obs)\n    return (cont_action, cont_obs)"}
{"task_id": 369, "completion_id": 0, "solution": ""}
{"task_id": 371, "completion_id": 0, "solution": ""}
{"task_id": 373, "completion_id": 0, "solution": ""}
{"task_id": 374, "completion_id": 0, "solution": ""}
{"task_id": 376, "completion_id": 0, "solution": ""}
{"task_id": 377, "completion_id": 0, "solution": ""}
{"task_id": 380, "completion_id": 0, "solution": ""}
{"task_id": 387, "completion_id": 0, "solution": ""}
{"task_id": 394, "completion_id": 0, "solution": ""}
{"task_id": 398, "completion_id": 0, "solution": ""}
{"task_id": 411, "completion_id": 0, "solution": ""}
{"task_id": 413, "completion_id": 0, "solution": ""}
{"task_id": 416, "completion_id": 0, "solution": ""}
{"task_id": 419, "completion_id": 0, "solution": ""}
{"task_id": 423, "completion_id": 0, "solution": ""}
{"task_id": 428, "completion_id": 0, "solution": ""}
{"task_id": 433, "completion_id": 0, "solution": ""}
{"task_id": 435, "completion_id": 0, "solution": ""}
{"task_id": 437, "completion_id": 0, "solution": ""}
{"task_id": 438, "completion_id": 0, "solution": ""}
{"task_id": 439, "completion_id": 0, "solution": ""}
{"task_id": 440, "completion_id": 0, "solution": ""}
{"task_id": 444, "completion_id": 0, "solution": ""}
{"task_id": 446, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    X = np.array(X)\n    y = np.array(y)\n    (n_samples, n_features) = X.shape\n\n    def entropy(labels):\n        if len(labels) == 0:\n            return 0.0\n        value_counts = Counter(labels)\n        probabilities = [count / len(labels) for count in value_counts.values()]\n        return -sum((p * np.log2(p) if p > 0 else 0 for p in probabilities))\n    majority_label = Counter(y).most_common(1)[0][0]\n    if len(set(y)) == 1:\n        return (-1, None, majority_label, majority_label)\n    best_gain = 0.0\n    best_feature_index = -1\n    best_threshold = None\n    best_left_majority_label = None\n    best_right_majority_label = None\n    for feature_index in range(n_features):\n        unique_values = np.unique(X[:, feature_index])\n        thresholds = [(unique_values[i] + unique_values[i + 1]) / 2 for i in range(len(unique_values) - 1)]\n        for threshold in thresholds:\n            left_indices = X[:, feature_index] <= threshold\n            right_indices = X[:, feature_index] > threshold\n            y_left = y[left_indices]\n            y_right = y[right_indices]\n            p_left = len(y_left) / n_samples\n            p_right = len(y_right) / n_samples\n            current_gain = entropy(y) - p_left * entropy(y_left) - p_right * entropy(y_right)\n            if current_gain > best_gain:\n                best_gain = current_gain\n                best_feature_index = feature_index\n                best_threshold = round(threshold, 4)\n                best_left_majority_label = Counter(y_left).most_common(1)[0][0]\n                best_right_majority_label = Counter(y_right).most_common(1)[0][0]\n    if best_feature_index == -1:\n        return (-1, None, majority_label, majority_label)\n    return (best_feature_index, best_threshold, best_left_majority_label, best_right_majority_label)"}
{"task_id": 452, "completion_id": 0, "solution": ""}
{"task_id": 453, "completion_id": 0, "solution": ""}
{"task_id": 458, "completion_id": 0, "solution": ""}
{"task_id": 461, "completion_id": 0, "solution": ""}
{"task_id": 471, "completion_id": 0, "solution": ""}
{"task_id": 474, "completion_id": 0, "solution": ""}
{"task_id": 475, "completion_id": 0, "solution": ""}
{"task_id": 479, "completion_id": 0, "solution": ""}
{"task_id": 481, "completion_id": 0, "solution": ""}
{"task_id": 482, "completion_id": 0, "solution": ""}
{"task_id": 485, "completion_id": 0, "solution": ""}
{"task_id": 490, "completion_id": 0, "solution": ""}
{"task_id": 491, "completion_id": 0, "solution": "from collections import Counter, defaultdict\nimport itertools\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    item_counts = Counter()\n    for transaction in transactions:\n        item_counts.update(transaction)\n    frequent_items = {item: count for (item, count) in item_counts.items() if count >= min_sup}\n    if not frequent_items:\n        return []\n    items_order = sorted(frequent_items, key=lambda x: (-frequent_items[x], x))\n    fp_tree = defaultdict(int)\n    header_table = defaultdict(list)\n\n    def insert_transaction(transaction):\n        current_node = fp_tree\n        for item in transaction:\n            if item not in current_node:\n                current_node[item] = defaultdict(int)\n            current_node = current_node[item]\n            current_node['count'] += 1\n            header_table[item].append(current_node)\n    for transaction in transactions:\n        filtered_transaction = [item for item in transaction if item in frequent_items]\n        filtered_transaction.sort(key=lambda x: (-frequent_items[x], x))\n        if filtered_transaction:\n            insert_transaction(filtered_transaction)\n\n    def mine_tree(prefix, node, prefix_support, frequent_itemsets):\n        if node != fp_tree:\n            prefix = tuple(sorted(prefix + [node]))\n            frequent_itemsets.append(prefix)\n        for item in items_order:\n            if item in node:\n                support = min(prefix_support, node[item]['count'])\n                conditional_pattern_base = []\n                for path_node in header_table[item]:\n                    path = []\n                    parent = path_node\n                    while 'count' not in parent:\n                        path.append(parent)\n                        parent = parent['parent']\n                    path_count = path_node['count']\n                    for prefix_item in reversed(path):\n                        conditional_pattern_base.append((prefix_item, path_count * support))\n                conditional_fp_tree = defaultdict(int)\n                conditional_header_table = defaultdict(list)\n\n                def insert_conditional_transaction(transaction, support):\n                    current_node = conditional_fp_tree\n                    for (prefix_item, count) in transaction:\n                        if prefix_item not in current_node:\n                            current_node[prefix_item] = defaultdict(int)\n                        current_node = current_node[prefix_item]\n                        current_node['count'] = count\n                        current_node['parent'] = prefix_item\n                        conditional_header_table[prefix_item].append(current_node)\n                for (transaction, count) in conditional_pattern_base:\n                    insert_conditional_transaction(transaction, count)\n                if conditional_fp_tree:\n                    mine_tree(prefix, conditional_fp_tree, support, frequent_itemsets)\n    frequent_itemsets = []\n    mine_tree([], fp_tree, float('inf'), frequent_itemsets)\n    frequent_itemsets.sort(key=lambda x: (len(x), x))\n    return frequent_itemsets\ntransactions = [['A', 'B', 'D', 'E'], ['B', 'C', 'E'], ['A', 'B', 'D', 'E'], ['A', 'B', 'C', 'E'], ['A', 'B', 'C', 'D', 'E'], ['B', 'C', 'D']]\nmin_sup = 3"}
{"task_id": 492, "completion_id": 0, "solution": ""}
{"task_id": 493, "completion_id": 0, "solution": ""}
{"task_id": 496, "completion_id": 0, "solution": ""}
{"task_id": 499, "completion_id": 0, "solution": ""}
{"task_id": 500, "completion_id": 0, "solution": ""}
{"task_id": 505, "completion_id": 0, "solution": ""}
{"task_id": 509, "completion_id": 0, "solution": ""}
{"task_id": 510, "completion_id": 0, "solution": ""}
{"task_id": 513, "completion_id": 0, "solution": "import numpy as np\ndef fm_predict(X: list[list[int | float]], w0: float, w: list[float], V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    X = np.array(X)\n    w = np.array(w)\n    V = np.array(V)\n    (n_samples, n_features) = X.shape\n    predictions = np.full(n_samples, w0)\n    predictions += X @ w\n    sum_squared = (X @ V) ** 2\n    squared_sum = X ** 2 @ V ** 2\n    second_order = 0.5 * (sum_squared - squared_sum)\n    predictions += np.sum(second_order, axis=1)\n    predictions = np.round(predictions, 4)\n    return predictions.tolist()"}
{"task_id": 517, "completion_id": 0, "solution": ""}
{"task_id": 518, "completion_id": 0, "solution": ""}
{"task_id": 520, "completion_id": 0, "solution": ""}
{"task_id": 528, "completion_id": 0, "solution": ""}
{"task_id": 537, "completion_id": 0, "solution": ""}
{"task_id": 539, "completion_id": 0, "solution": ""}
{"task_id": 552, "completion_id": 0, "solution": ""}
{"task_id": 555, "completion_id": 0, "solution": "import numpy as np\nfrom collections import deque\nfrom scipy.spatial.distance import cdist\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    n_samples = data.shape[0]\n    labels = [-1] * n_samples\n    cluster_id = 0\n    distances = cdist(data, data, metric='euclidean')\n    neighbors = [np.where(distances[i] <= eps)[0] for i in range(n_samples)]\n    core_points = [i for (i, nb) in enumerate(neighbors) if len(nb) >= min_samples]\n\n    def expand_cluster(point_id, cluster_id):\n        labels[point_id] = cluster_id\n        queue = deque([point_id])\n        while queue:\n            current_point_id = queue.popleft()\n            if len(neighbors[current_point_id]) < min_samples:\n                continue\n            for neighbor_id in neighbors[current_point_id]:\n                if labels[neighbor_id] == -1:\n                    labels[neighbor_id] = cluster_id\n                    if len(neighbors[neighbor_id]) >= min_samples:\n                        queue.append(neighbor_id)\n    for point_id in core_points:\n        if labels[point_id] == -1:\n            cluster_id += 1\n            expand_cluster(point_id, cluster_id)\n    return labels"}
{"task_id": 561, "completion_id": 0, "solution": "import numpy as np\ndef gmm_em(data: np.ndarray, k: int, max_iter: int=200, epsilon: float=0.0001) -> list[int]:\n    \"\"\"\n    Implement the Expectation\u2013Maximization (EM) algorithm for a Gaussian Mixture Model (GMM).\n\n    Parameters:\n    - data: np.ndarray, a 2-D array of shape (N, D) containing N samples with D features.\n    - k: int, the number of Gaussian components.\n    - max_iter: int, the maximum number of iterations for the EM algorithm.\n    - epsilon: float, the tolerance for convergence based on the change in mixture weights.\n\n    Returns:\n    - A list of length N containing the final cluster label of each sample.\n    \"\"\"\n    (N, D) = data.shape\n    if k == 1:\n        return [0] * N\n    np.random.seed(0)\n    weights = np.ones(k) / k\n    means = data[np.random.choice(N, k, replace=False)]\n    covariances = np.array([np.cov(data, rowvar=False) for _ in range(k)])\n\n    def gaussian_pdf(x, mean, cov):\n        det_cov = np.linalg.det(cov)\n        inv_cov = np.linalg.inv(cov)\n        diff = x - mean\n        exponent = -0.5 * np.dot(diff.T, np.dot(inv_cov, diff))\n        return 1.0 / np.sqrt((2 * np.pi) ** D * det_cov) * np.exp(exponent)\n    for _ in range(max_iter):\n        responsibilities = np.zeros((N, k))\n        for i in range(k):\n            responsibilities[:, i] = weights[i] * gaussian_pdf(data, means[i], covariances[i])\n        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n        new_weights = responsibilities.mean(axis=0)\n        new_means = np.dot(responsibilities.T, data) / responsibilities.sum(axis=0, keepdims=True)\n        new_covariances = np.array([np.dot((data - new_means[i]).T, (data - new_means[i]) * responsibilities[:, i][:, np.newaxis]) / responsibilities[:, i].sum() for i in range(k)])\n        if np.allclose(weights, new_weights, atol=epsilon):\n            break\n        (weights, means, covariances) = (new_weights, new_means, new_covariances)\n    labels = np.argmax(responsibilities, axis=1)\n    sorted_indices = np.argsort(means[:, 0])\n    labels = np.array([np.where(sorted_indices == label)[0][0] for label in labels])\n    return labels.tolist()"}
{"task_id": 562, "completion_id": 0, "solution": ""}
{"task_id": 563, "completion_id": 0, "solution": ""}
{"task_id": 564, "completion_id": 0, "solution": ""}
{"task_id": 565, "completion_id": 0, "solution": ""}
{"task_id": 566, "completion_id": 0, "solution": ""}
{"task_id": 568, "completion_id": 0, "solution": ""}
{"task_id": 569, "completion_id": 0, "solution": ""}
{"task_id": 571, "completion_id": 0, "solution": ""}
{"task_id": 572, "completion_id": 0, "solution": ""}
{"task_id": 574, "completion_id": 0, "solution": ""}
{"task_id": 579, "completion_id": 0, "solution": "import numpy as np\ndef kmeans(X: np.ndarray, k: int, max_iters: int=100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    centres = [X[i] for i in range(k)]\n\n    def euclidean_distance(p1, p2):\n        return np.sqrt(np.sum((p1 - p2) ** 2))\n    labels = [-1] * len(X)\n    for _ in range(max_iters):\n        new_labels = [-1] * len(X)\n        for (i, point) in enumerate(X):\n            min_distance = float('inf')\n            closest_centre = 0\n            for (j, centre) in enumerate(centres):\n                distance = euclidean_distance(point, centre)\n                if distance < min_distance:\n                    min_distance = distance\n                    closest_centre = j\n            new_labels[i] = closest_centre\n        if new_labels == labels:\n            break\n        labels = new_labels\n        new_centres = [np.zeros_like(centres[0]) for _ in range(k)]\n        counts = [0] * k\n        for (i, label) in enumerate(labels):\n            new_centres[label] += X[i]\n            counts[label] += 1\n        for j in range(k):\n            if counts[j] > 0:\n                new_centres[j] /= counts[j]\n        centres = new_centres\n    rounded_centres = [list(map(lambda x: round(x, 4), centre)) for centre in centres]\n    return (labels, rounded_centres)"}
{"task_id": 581, "completion_id": 0, "solution": ""}
{"task_id": 591, "completion_id": 0, "solution": ""}
{"task_id": 595, "completion_id": 0, "solution": ""}
