{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for i in range(len(a)):\n        dot_product = 0\n        for j in range(len(a[i])):\n            dot_product += a[i][j] * b[j]\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a) == 0:\n        return []\n    rows = len(a)\n    cols = len(a[0])\n    transposed = [[0 for _ in range(rows)] for _ in range(cols)]\n    for i in range(rows):\n        for j in range(cols):\n            transposed[j][i] = a[i][j]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    try:\n        np_arr = np.array(a)\n        reshaped_arr = np_arr.reshape(new_shape)\n        reshaped_list = reshaped_arr.tolist()\n        return reshaped_list\n    except ValueError:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        return [sum(col) / len(matrix) for col in zip(*matrix)]\n    else:\n        raise ValueError(\"Invalid mode. Choose either 'row' or 'column'.\")\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    result = []\n    for row in matrix:\n        new_row = [element * scalar for element in row]\n        result.append(new_row)\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        return -1\n    T_inv = np.linalg.inv(T)\n    S_inv = np.linalg.inv(S)\n    result = np.dot(np.dot(T_inv, A), S)\n    result = np.round(result, 4).tolist()\n    return result"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inverse = [[0, 0], [0, 0]]\n    inverse[0][0] = matrix[1][1] / det\n    inverse[0][1] = -matrix[0][1] / det\n    inverse[1][0] = -matrix[1][0] / det\n    inverse[1][1] = matrix[0][0] / det\n    return inverse"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    data = np.array(vectors)\n    cov_matrix = np.cov(data)\n    cov_matrix_list = cov_matrix.tolist()\n    return cov_matrix_list\nvectors = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    D = np.diag(A)\n    R = A - np.diagflat(D)\n    x = np.zeros(A.shape[0])\n    for i in range(n):\n        x = (b - np.dot(R, x)) / D\n        x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n\n    def jacobi(a, n):\n\n        def max_elem(a):\n            n = len(a)\n            amax = 0.0\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    if abs(a[i][j]) > amax:\n                        amax = abs(a[i][j])\n                        k = i\n                        l = j\n            return (amax, k, l)\n        rot_mat = np.identity(n)\n        a_new = np.copy(a)\n        for j in range(100):\n            (amax, k, l) = max_elem(a_new)\n            if amax < 0.0001:\n                return (np.diagonal(a_new), rot_mat)\n            if abs(a_new[k][k]) > abs(a_new[l][l]):\n                t = a_new[k][k] - a_new[l][l]\n                tfr = -a_new[k][l] / t\n                tfi = a_new[l][k] / t\n            else:\n                t = a_new[l][l] - a_new[k][k]\n                tfr = a_new[k][l] / t\n                tfi = -a_new[l][k] / t\n            c = 1 / np.sqrt(tfr ** 2 + tfi ** 2)\n            s = tfr * c\n            t = tfi * c\n            for i in range(n):\n                temp = a_new[k][i]\n                a_new[k][i] = temp * c - a_new[l][i] * s\n                a_new[l][i] = temp * s + a_new[l][i] * c\n                temp = rot_mat[k][i]\n                rot_mat[k][i] = temp * c - rot_mat[l][i] * s\n                rot_mat[l][i] = temp * s + rot_mat[l][i] * c\n            temp = a_new[k][k]\n            a_new[k][k] = temp * c - a_new[l][k] * s\n            a_new[l][k] = 0.0\n            a_new[k][l] = 0.0\n            a_new[l][l] = temp * c + a_new[l][k] * s\n            for i in range(n):\n                temp = rot_mat[k][i]\n                rot_mat[k][i] = temp * c - rot_mat[l][i] * s\n                rot_mat[l][i] = temp * s + rot_mat[l][i] * c\n        return (np.diagonal(a_new), rot_mat)\n    (eigenvalues, eigenvectors) = jacobi(np.dot(A.T, A), 2)\n    eigenvectors = eigenvectors.T\n    singular_values = np.sqrt(np.abs(eigenvalues))\n    return tuple(np.round(singular_values, 4))"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    if len(matrix) != len(matrix[0]):\n        raise ValueError('The matrix is not square')\n    if len(matrix) != 4:\n        raise ValueError('The matrix is not 4x4')\n    determinant = 0\n    for j in range(4):\n        determinant += (-1) ** j * matrix[0][j] * determinant_3x3([row[:j] + row[j + 1:] for row in matrix[1:]])\n    return determinant\ndef determinant_3x3(matrix: list[list[int | float]]) -> float:\n    if len(matrix) != len(matrix[0]):\n        raise ValueError('The matrix is not square')\n    if len(matrix) != 3:\n        raise ValueError('The matrix is not 3x3')\n    if len(matrix) == 2:\n        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    determinant = 0\n    for j in range(3):\n        determinant += (-1) ** j * matrix[0][j] * determinant_3x3([row[:j] + row[j + 1:] for row in matrix[1:]])\n    return determinant\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y)\n    X = np.c_[np.ones(X.shape[0]), X]\n    beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    beta = [round(coef, 4) for coef in beta]\n    return beta"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        gradient = 2 / X.shape[0] * X.T @ (X @ theta - y)\n        theta = theta - alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_max_norm_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_norm_data = np.round(min_max_norm_data, 4).tolist()\n    return (standardized_data, min_max_norm_data)\ndata = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\ndef k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=None):\n    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=random_seed)\n    return [(train_index, test_index) for (train_index, test_index) in skf.split(X, y)]"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(data.T)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    eigenvectors = eigenvectors[:, np.argsort(eigenvalues)[::-1]]\n    eigenvalues = eigenvalues[np.argsort(eigenvalues)[::-1]]\n    principal_components = eigenvectors[:, :k]\n    return np.round(principal_components, 4).tolist()"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(probs):\n    return sum((-prob * math.log2(prob) for prob in probs))\ndef info_gain(examples, attr, target_attr):\n    attr_values = set([ex[attr] for ex in examples])\n    target_values = set([ex[target_attr] for ex in examples])\n    total_examples = len(examples)\n    weighted_entropy = sum((len([ex for ex in examples if ex[attr] == val]) / total_examples * entropy([len([ex for ex in examples if ex[target_attr] == target and ex[attr] == val]) / len([ex for ex in examples if ex[attr] == val]) for target in target_values]) for val in attr_values))\n    return entropy([len([ex for ex in examples if ex[target_attr] == target]) / total_examples for target in target_values]) - weighted_entropy\ndef majority_value(examples, target_attr):\n    data = [ex[target_attr] for ex in examples]\n    return Counter(data).most_common(1)[0][0]\ndef learn_decision_tree(examples, attributes, target_attr):\n    target_values = [ex[target_attr] for ex in examples]\n    if not examples or len(attributes) <= 1 or len(set(target_values)) <= 1:\n        return majority_value(examples, target_attr)\n    else:\n        gains = [(attr, info_gain(examples, attr, target_attr)) for attr in attributes]\n        max_gain_attr = max(gains, key=lambda x: x[1])[0]\n        tree = {max_gain_attr: {}}\n        remaining_attributes = [attr for attr in attributes if attr != max_gain_attr]\n        for val in set([ex[max_gain_attr] for ex in examples]):\n            subtree = learn_decision_tree([ex for ex in examples if ex[max_gain_attr] == val], remaining_attributes, target_attr)\n            tree[max_gain_attr][val] = subtree\n        return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef linear_kernel(x, y):\n    return np.dot(x, y)\ndef rbf_kernel(x, y, sigma):\n    return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\ndef compute_kernel_matrix(data, kernel, sigma):\n    kernel_matrix = np.zeros((data.shape[0], data.shape[0]))\n    for i in range(data.shape[0]):\n        for j in range(data.shape[0]):\n            if kernel == 'linear':\n                kernel_matrix[i, j] = linear_kernel(data[i], data[j])\n            elif kernel == 'rbf':\n                kernel_matrix[i, j] = rbf_kernel(data[i], data[j], sigma)\n    return kernel_matrix\ndef pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    kernel_matrix = compute_kernel_matrix(data, kernel, sigma)\n    (num_samples, num_features) = data.shape\n    alpha = np.zeros(num_samples)\n    bias = 0\n    for _ in range(iterations):\n        for i in range(num_samples):\n            result = labels[i] * (np.dot(alpha * labels, kernel_matrix[i]) + bias)\n            if result < 1:\n                alpha[i] += 1\n                bias += labels[i]\n            alpha[i] = min(1, alpha[i])\n    return (alpha.tolist(), round(bias, 4))\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nlabels = np.array([1, -1, 1])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    denominator = 1 + math.exp(-z)\n    return round(1 / denominator, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    weighted_sums = [np.dot(feature, weights) + bias for feature in features]\n    predictions = [round(sigmoid(weighted_sum), 4) for weighted_sum in weighted_sums]\n    mse = round(sum([(prediction - label) ** 2 for (prediction, label) in zip(predictions, labels)]) / len(labels), 4)\n    return (predictions, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predicted = sigmoid(weighted_sum)\n        mse = np.mean((labels - predicted) ** 2)\n        mse_values.append(round(mse, 4))\n        error = labels - predicted\n        d_predicted = error * sigmoid_derivative(predicted)\n        weights += learning_rate * np.dot(features.T, d_predicted)\n        bias += learning_rate * np.sum(d_predicted)\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B = np.array(B)\n    C = np.array(C)\n    P_B_to_C = np.linalg.inv(B) @ C\n    P_B_to_C = np.round(P_B_to_C, 4)\n    P_B_to_C = P_B_to_C.tolist()\n    return P_B_to_C"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A, dtype=np.float64)\n    for i in range(V.shape[1]):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    permutation = np.random.permutation(len(X))\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    num_batches = int(np.ceil(X.shape[0] / batch_size))\n    batches = []\n    for i in range(num_batches):\n        start_index = i * batch_size\n        end_index = min((i + 1) * batch_size, X.shape[0])\n        batch_X = X[start_index:end_index]\n        if y is not None:\n            batch_y = y[start_index:end_index]\n            batches.append((batch_X.tolist(), batch_y.tolist()))\n        else:\n            batches.append((batch_X.tolist(),))\n    return batches"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Args:\n        X (np.array): The dataset to be divided.\n        feature_i (int): The index of the feature to be used for division.\n        threshold (float): The threshold to be used for division.\n\n    Returns:\n        list: A list of two subsets of the dataset. The first subset contains samples where the feature value is greater than or equal to the threshold, and the second subset contains samples where the feature value is less than the threshold.\n    \"\"\"\n    if feature_i >= X.shape[1]:\n        raise ValueError('feature_i is out of bounds for axis 1 with size {}'.format(X.shape[1]))\n    meets_condition = X[X[:, feature_i] >= threshold]\n    does_not_meet_condition = X[X[:, feature_i] < threshold]\n    return (meets_condition.tolist(), does_not_meet_condition.tolist())"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = np.shape(X)\n\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    new_features = [np.prod(X[:, index], axis=1) for index in index_combinations()]\n    return np.stack(new_features, axis=-1).tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Converts a vector of integers into a one-hot encoding matrix.\n    \n    Parameters:\n    x (1D numpy array): Array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be automatically determined from the input array.\n    \n    Returns:\n    2D numpy array: One-hot encoding matrix.\n    \"\"\"\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.zeros((x.shape[0], n_col))\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    if not isinstance(x, np.ndarray):\n        raise TypeError('Input should be a numpy array.')\n    if len(x.shape) != 1:\n        raise ValueError('Input should be a 1D numpy array.')\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3, 4])\ndiagonal_matrix = make_diagonal(x)"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise TypeError('Inputs should be numpy arrays')\n    if len(y_true) != len(y_pred):\n        raise ValueError('Inputs should have the same length')\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    if Y is None:\n        Y = X\n    X_mean = np.mean(X, axis=0)\n    Y_mean = np.mean(Y, axis=0)\n    X_diff = X - X_mean\n    Y_diff = Y - Y_mean\n    X_std = np.std(X, axis=0)\n    Y_std = np.std(Y, axis=0)\n    X_norm = X_diff / X_std\n    Y_norm = Y_diff / Y_std\n    correlation_matrix = np.dot(X_norm.T, Y_norm) / X.shape[0]\n    return correlation_matrix.round(4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    clfs = []\n    for _ in range(n_clf):\n        clf = DecisionTreeClassifier(max_depth=1, random_state=0)\n        (best_feat, best_thresh, min_error, sample_weight) = find_best_thresh(clf, X, y, w)\n        clf.fit(X, y, sample_weight=sample_weight)\n        clfs.append((clf, best_feat, best_thresh))\n        w = update_weights(clf, X, y, w, sample_weight)\n    return clfs\ndef find_best_thresh(clf, X, y, w):\n    min_error = float('inf')\n    for feature_i in range(X.shape[1]):\n        X_column = X[:, feature_i]\n        thresholds = np.unique(X_column)\n        for t in thresholds:\n            pred = np.ones(np.shape(y))\n            pred[X_column < t] = -1\n            error = np.sum(w[pred != y])\n            if error > 0.5:\n                error = 1 - error\n                pred *= -1\n            if error < min_error:\n                min_error = error\n                best_thresh = t\n                best_feat = feature_i\n    return (best_feat, best_thresh, min_error, w)\ndef update_weights(clf, X, y, w, sample_weight):\n    y_pred = clf.predict(X)\n    error = np.sum(sample_weight[y != y_pred])\n    alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n    new_sample_weights = sample_weight * np.exp(-alpha * y * y_pred)\n    new_sample_weights /= np.sum(new_sample_weights)\n    return new_sample_weights"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores)\n    scores = np.exp(scores - np.max(scores))\n    scores = scores / scores.sum()\n    scores = np.log(scores)\n    return scores.round(4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        limit = 1.0 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.optimizer.update(self.W, grad_w)\n            self.w0 = self.optimizer.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    input_size = input_matrix.shape[0]\n    kernel_size = kernel.shape[0]\n    output_size = (input_size - kernel_size + 2 * padding) // stride + 1\n    output_matrix = np.zeros((output_size, output_size))\n    for i in range(0, input_size - kernel_size + 1, stride):\n        for j in range(0, input_size - kernel_size + 1, stride):\n            submatrix = input_matrix[i:i + kernel_size, j:j + kernel_size]\n            output_matrix[i // stride, j // stride] = np.sum(submatrix * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge loss function.\n    \n    Parameters:\n    X (np.ndarray): 2D feature matrix.\n    w (np.ndarray): 1D coefficient vector.\n    y_true (np.ndarray): 1D true label vector.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss.\n    \"\"\"\n    y_pred = np.dot(X, w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    ridge_loss = mse + alpha * np.dot(w, w)\n    ridge_loss = round(ridge_loss, 4)\n    return ridge_loss"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    if z < 0:\n        return alpha * z\n    else:\n        return z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    kernel = np.dot(x1, x2)\n    return kernel\nx1 = [1, 2, 3]\nx2 = [4, 5, 6]"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    true_positives = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    false_positives = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    precision = true_positives / (true_positives + false_positives)\n    return precision"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    (n_samples, n_features) = X.shape\n    X_transpose = X.T\n    losses = []\n    for i in range(n_iterations):\n        loss = np.mean((np.dot(X, weights) - y) ** 2) / 2\n        losses.append(loss)\n        if method == 'batch':\n            gradient = np.dot(X_transpose, np.dot(X, weights) - y) / n_samples\n        elif method == 'stochastic':\n            random_index = np.random.randint(n_samples)\n            xi = X[random_index:random_index + 1]\n            yi = y[random_index:random_index + 1]\n            gradient = np.dot(xi.T, np.dot(xi, weights) - yi)\n        elif method == 'mini-batch':\n            indices = np.random.randint(0, n_samples, batch_size)\n            xi = X[indices]\n            yi = y[indices]\n            gradient = np.dot(xi.T, np.dot(xi, weights) - yi) / batch_size\n        weights = weights - learning_rate * gradient\n    return (np.around(weights, decimals=4).tolist(), np.around(losses, decimals=4).tolist())"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    This function converts a given matrix to its Reduced Row Echelon Form (RREF).\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (r, c) = matrix.shape\n    pivot = 0\n    for j in range(c):\n        found_pivot = False\n        for i in range(pivot, r):\n            if matrix[i, j] != 0:\n                found_pivot = True\n                break\n        if not found_pivot:\n            continue\n        matrix[[pivot, i], :] = matrix[[i, pivot], :]\n        matrix[pivot, :] = matrix[pivot, :] / matrix[pivot, j]\n        for i in range(r):\n            if i != pivot:\n                matrix[i, :] = matrix[i, :] - matrix[i, j] * matrix[pivot, :]\n        pivot += 1\n    return matrix.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros(len(x0))\n    v = np.zeros(len(x0))\n    x = x0\n    for t in range(1, num_iterations + 1):\n        g = np.array(grad(x))\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x.tolist(), 4)"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n, p) = X.shape\n    weights = np.zeros(p)\n    bias = 0\n    costs = []\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        cost = 1 / (2 * n) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        costs.append(cost)\n        if len(costs) > 1 and np.abs(costs[-2] - costs[-1]) < tol:\n            break\n        weights_gradient = -(2 / n) * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        bias_gradient = -(2 / n) * np.sum(y - y_pred)\n        weights = weights - learning_rate * weights_gradient\n        bias = bias - learning_rate * bias_gradient\n    return (np.round(weights, 4).tolist(), np.round(bias, 4), np.round(costs[-1], 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    n = len(source)\n    m = len(target)\n    dp = np.zeros((n + 1, m + 1))\n    for i in range(n + 1):\n        dp[i][0] = i\n    for j in range(m + 1):\n        dp[0][j] = j\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            cost = 0 if source[i - 1] == target[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[n][m]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(TP / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    Q = np.dot(W_q, X)\n    K = np.dot(W_k, X)\n    V = np.dot(W_v, X)\n    scores = np.dot(Q.T, K)\n    scores = np.divide(scores, np.sqrt(K.shape[0]))\n    probs = np.exp(scores) / np.sum(np.exp(scores), axis=0)\n    output = np.dot(V, probs)\n    output = np.around(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for x in input_sequence:\n        hidden_state = np.tanh(np.dot(Wx, x) + np.dot(Wh, hidden_state) + b)\n    hidden_state = np.round(hidden_state, 4)\n    return hidden_state.tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    T = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    transformed_points = []\n    for point in points:\n        point_h = np.array([point[0], point[1], 1])\n        point_t = np.dot(T, point_h)\n        point_t = [point_t[0], point_t[1]]\n        transformed_points.append(point_t)\n    return np.array(transformed_points).tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    k = len(mu_p)\n    Sigma_p = np.diag(np.square(sigma_p))\n    Sigma_q = np.diag(np.square(sigma_q))\n    term1 = np.trace(np.dot(np.linalg.inv(Sigma_q), Sigma_p))\n    term2 = np.dot((mu_q - mu_p).T, np.dot(np.linalg.inv(Sigma_q), mu_q - mu_p))\n    term3 = k\n    term4 = np.log(np.linalg.det(Sigma_q) / np.linalg.det(Sigma_p))\n    kl_divergence = 0.5 * (term1 + term2 - term3 + term4)\n    return kl_divergence"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    L = np.tril(A)\n    U = np.triu(A, 1)\n    if x_ini is None:\n        x_ini = np.zeros_like(b)\n    for _ in range(n):\n        x_ini = np.dot(np.linalg.inv(L), b - np.dot(U, x_ini))\n    return np.around(x_ini.tolist(), 4)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    n = len(b)\n    for i in range(n):\n        A[i].append(b[i])\n    for i in range(n):\n        max_row = max(range(i, n), key=lambda j: abs(A[j][i]))\n        (A[i], A[max_row]) = (A[max_row], A[i])\n        pivot = A[i][i]\n        A[i] = [element / pivot for element in A[i]]\n        for j in range(i + 1, n):\n            factor = A[j][i]\n            for k in range(i, n + 1):\n                A[j][k] -= factor * A[i][k]\n    x = [0] * n\n    for i in range(n - 1, -1, -1):\n        x[i] = A[i][-1] - sum((A[i][k] * x[k] for k in range(i + 1, n)))\n        x[i] /= A[i][i]\n    x = [round(element, 4) for element in x]\n    return x"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        seq_len = len(x)\n        hidden_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(seq_len):\n            x_t = np.expand_dims(x[t], axis=1)\n            z = np.concatenate((h_t, x_t), axis=0)\n            ft = sigmoid(np.dot(self.Wf, z) + self.bf)\n            it = sigmoid(np.dot(self.Wi, z) + self.bi)\n            Ct_candidate = tanh(np.dot(self.Wc, z) + self.bc)\n            ot = sigmoid(np.dot(self.Wo, z) + self.bo)\n            c_t = np.add(np.multiply(ft, c_t), np.multiply(it, Ct_candidate))\n            h_t = np.multiply(ot, tanh(c_t))\n            hidden_states.append(h_t.tolist())\n        return (np.round(hidden_states, 4), np.round(h_t.tolist(), 4), np.round(c_t.tolist(), 4))\ninput_size = 3\nhidden_size = 5\nx = [np.random.randn(input_size, 1) for _ in range(5)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_tf_idf(corpus, query):\n    if len(corpus) == 0:\n        return []\n    term_frequencies = [Counter(doc) for doc in corpus]\n    document_frequencies = Counter()\n    for doc in corpus:\n        document_frequencies.update(set(doc))\n    idf = {}\n    for term in document_frequencies:\n        idf[term] = np.log((1 + len(corpus)) / (1 + document_frequencies[term])) + 1\n    tf_idf = []\n    for doc in term_frequencies:\n        scores = []\n        for term in query:\n            tf = doc[term] / sum(doc.values())\n            scores.append(round(tf * idf[term], 5))\n        tf_idf.append(scores)\n    return np.array(tf_idf).tolist()\ncorpus = [['this', 'is', 'a', 'sample'], ['this', 'is', 'another', 'example'], ['this', 'is', 'a', 'third', 'example']]\nquery = ['this', 'is', 'a']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum(np.logical_and(y_pred == 1, y_true == 1))\n    fp = np.sum(np.logical_and(y_pred == 1, y_true == 0))\n    fn = np.sum(np.logical_and(y_pred == 0, y_true == 1))\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        outputs = []\n        for i in x:\n            h = np.tanh(np.dot(self.W_xh, i) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n        return (outputs, h)\n\n    def backward(self, x, y, outputs, h, learning_rate=0.1):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(h)\n        for t in reversed(range(len(x))):\n            dy = outputs[t] - y[t]\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - h[t] ** 2) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, x[t].T)\n            dW_hh += np.dot(dh_raw, h.T)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros(A.shape[1])\n    r = b - np.dot(A, x0)\n    p = r\n    k = 0\n    while np.linalg.norm(r) > tol and k < n:\n        Ap = np.dot(A, p)\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x0 = x0 + alpha * p\n        r_new = r - alpha * Ap\n        beta = np.dot(r_new, r_new) / np.dot(r, r)\n        p = r_new + beta * p\n        r = r_new\n        k += 1\n    return np.round(x0, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (_, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    impurity = 1 - np.sum(np.square(probabilities))\n    return round(impurity, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        for (col_idx, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n        row_pointer.append(len(values))\n    return (values, column_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((x * y for (x, y) in zip(v, L)))\n    L_norm_squared = sum((x * x for x in L))\n    projection_scaler = dot_product / L_norm_squared\n    projection_vector = [projection_scaler * x for x in L]\n    projection_vector = [round(x, 3) for x in projection_vector]\n    return projection_vector"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    num_columns = len(dense_matrix[0])\n    for column in range(num_columns):\n        for row in range(len(dense_matrix)):\n            if dense_matrix[row][column] != 0:\n                values.append(dense_matrix[row][column])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    rref_A = np.around(np.linalg.qr(A)[0], 8)\n    independent_columns = []\n    for i in range(rref_A.shape[1]):\n        if not np.allclose(rref_A[:, i], np.zeros(rref_A.shape[0])):\n            independent_columns.append(A[:, i])\n    return [np.around(column, 8).tolist() for column in independent_columns]\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_true_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r_squared = 1 - rss / tss\n    return round(r_squared, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = total_brightness / total_pixels\n    average_brightness = round(average_brightness, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, (np.ndarray, list)) or not isinstance(y_pred, (np.ndarray, list)):\n        raise TypeError('Invalid input type. Expected numpy array or list.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Mismatched array shapes. The shapes of y_true and y_pred should be the same.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('Empty arrays. Both y_true and y_pred should not be empty.')\n    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(rmse, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    assert all(np.array(y_true) < 2), 'y_true should be a binary array'\n    assert all(np.array(y_pred) < 2), 'y_pred should be a binary array'\n    assert len(y_true) == len(y_pred), 'y_true and y_pred should have the same length'\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    if np.sum(union) == 0:\n        jaccard_index = 1\n    else:\n        jaccard_index = np.sum(intersection) / np.sum(union)\n    return round(jaccard_index, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    if sum_true_pred == 0:\n        return 1.0\n    dice_score = 2.0 * intersection / sum_true_pred\n    return round(dice_score, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    row_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        seed = random_seeds[feature]\n        np.random.seed(seed)\n        feature_hv = np.random.normal(size=dim)\n        value_hv = np.random.normal(size=dim)\n        combined_hv = feature_hv + value_hv\n        row_hv += combined_hv\n    return row_hv.tolist()\ndim = 10\nrandom_seeds = {'feature1': 0, 'feature2': 1}"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    cm = Counter()\n    for pair in data:\n        (y_true, y_pred) = pair\n        if y_true == y_pred:\n            if y_true == 1:\n                cm['TP'] += 1\n            else:\n                cm['TN'] += 1\n        elif y_true == 1:\n            cm['FN'] += 1\n        else:\n            cm['FP'] += 1\n    confusion_matrix = [[cm['TP'], cm['FP']], [cm['FN'], cm['TN']]]\n    return confusion_matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty')\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude')\n    similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score\nimport numpy as np\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    cm = confusion_matrix(actual, predicted)\n    accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\n    accuracy = round(accuracy, 3)\n    f1 = f1_score(actual, predicted)\n    f1 = round(f1, 3)\n    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n    specificity = round(specificity, 3)\n    npv = cm[0, 0] / (cm[0, 0] + cm[1, 0])\n    npv = round(npv, 3)\n    return (cm, accuracy, f1, specificity, npv)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    std_dev = np.std(data)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr = p75 - p25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4), 'variance': round(variance, 4), 'standard_deviation': round(std_dev, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr, 4)}\n    return results"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binomial_coeff = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    prob = binomial_coeff * p ** k * (1 - p) ** (n - k)\n    return round(prob, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2 / (2 * std_dev ** 2)))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    if not isinstance(k, int) or k < 0:\n        raise ValueError('k must be a non-negative integer')\n    if not isinstance(lam, (int, float)) or lam <= 0:\n        raise ValueError('lam must be a positive number')\n    prob = lam ** k * math.exp(-lam) / math.factorial(k)\n    return round(prob, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if len(img.shape) != 2:\n        raise ValueError('The image is not grayscale.')\n    contrast = img.max() - img.min()\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for point in data:\n        transformed_point = []\n        for i in range(1, degree + 1):\n            transformed_point.append(round(point ** i, 8))\n        transformed_data.append(transformed_point)\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    pos_range = np.arange(position)\n    i_range = np.arange(d_model)\n    denominator = np.power(10000, 2 * i_range / d_model)\n    encoding[:, 0::2] = np.sin(pos_range[:, np.newaxis] / denominator[0::2])\n    encoding[:, 1::2] = np.cos(pos_range[:, np.newaxis] / denominator[1::2])\n    return encoding.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_corrected = m / (1 - beta1 ** t)\n    v_corrected = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n    return (parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    prompt_tokens = encoder.encode(prompt)\n    inputs = np.zeros((1, hparams['n_ctx']), dtype=np.int32)\n    inputs[0, :len(prompt_tokens)] = prompt_tokens\n    for i in range(len(prompt_tokens), len(prompt_tokens) + n_tokens_to_generate):\n        h = np.matmul(inputs[0, :i + 1], params['wte'].T)\n        h += params['wpe'][:i + 1]\n        attn_weights = np.ones((1, hparams['n_head'], i + 1, i + 1)) / (i + 1)\n        h = np.matmul(attn_weights, h[None, :, :, None]).squeeze(3).transpose(1, 0, 2)\n        h_ffn = np.tanh(np.matmul(h, params['wte'].T))\n        h_norm = h_ffn * params['ln_f']['g'] + params['ln_f']['b']\n        next_token = np.argmax(h_norm[i])\n        inputs[0, i] = next_token\n    generated_text = encoder.decode(inputs[0].tolist())\n    return generated_text"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n\n    def self_attention(query, key, value):\n        scores = softmax(np.dot(query, key.T) / np.sqrt(dimension))\n        return np.dot(scores, value)\n    crystal_values = np.array(crystal_values).reshape(n, dimension)\n    weights = []\n    for i in range(n):\n        query = crystal_values[i].reshape(dimension, 1)\n        key = crystal_values.reshape(n, dimension)\n        value = crystal_values.reshape(n, dimension)\n        weights.append(self_attention(query, key, value))\n    return [round(weight, 4) for weight in weights]\nn = 3\ncrystal_values = [1, 2, 3, 4, 5, 6]\ndimension = 2"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nimport math\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    corpus = [document.split() for document in corpus]\n    query = query.split()\n    word_counts = [Counter(document) for document in corpus]\n    word_set = set().union(*corpus)\n    avg_doc_len = np.mean([len(doc) for doc in corpus])\n    N = len(corpus)\n    idf = {}\n    for word in word_set:\n        count = sum((1 for document in corpus if word in document))\n        idf[word] = math.log((N - count + 0.5) / (count + 0.5))\n    scores = []\n    for (i, document) in enumerate(corpus):\n        score = 0\n        for word in query:\n            if word in document:\n                tf = (k1 + 1) * word_counts[i][word] / (k1 * (1 - b + b * len(document) / avg_doc_len) + word_counts[i][word])\n                score += idf[word] * tf\n        scores.append(score)\n    scores = [round(score, 3) for score in scores]\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    true_positives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == yp == True])\n    false_positives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == False and yp == True])\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    false_negatives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == True and yp == False])\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef power_grid_forecast(consumption_data):\n    days = np.arange(1, 11)\n    fluctuation = 10 * np.sin(2 * np.pi * days / 10)\n    detrended_data = consumption_data - fluctuation\n    (slope, intercept, _, _, _) = stats.linregress(days, detrended_data)\n    day_15_base_consumption = slope * 15 + intercept\n    day_15_fluctuation = 10 * np.sin(2 * np.pi * 15 / 10)\n    day_15_consumption = day_15_base_consumption + day_15_fluctuation\n    final_consumption = np.ceil(day_15_consumption * 1.05)\n    return int(final_consumption)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    assert len(y_true) == len(y_pred), 'Lengths of y_true and y_pred must be equal'\n    errors = np.abs(y_true - y_pred)\n    mae = np.mean(errors)\n    return round(mae, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    def compute_qkv(X, W_q, W_k, W_v):\n        return (np.matmul(X, W_q), np.matmul(X, W_k), np.matmul(X, W_v))\n\n    def self_attention(Q, K, V):\n        scores = np.matmul(Q, K.T) / np.sqrt(K.shape[1])\n        attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        attention_weights = attention_weights / np.sum(attention_weights, axis=1, keepdims=True)\n        return np.matmul(attention_weights, V)\n\n    def split_heads(x, n_heads):\n        return np.array(np.split(x, n_heads, axis=1))\n\n    def combine_heads(x):\n        return np.concatenate(x, axis=1)\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = split_heads(Q, n_heads)\n    K_heads = split_heads(K, n_heads)\n    V_heads = split_heads(V, n_heads)\n    outputs = [self_attention(Q_heads[i], K_heads[i], V_heads[i]) for i in range(n_heads)]\n    output = combine_heads(outputs)\n    return output.round(4).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Both lists must have the same length')\n    n00 = n01 = n10 = n11 = 0\n    for i in range(len(x)):\n        if x[i] == 0 and y[i] == 0:\n            n00 += 1\n        elif x[i] == 0 and y[i] == 1:\n            n01 += 1\n        elif x[i] == 1 and y[i] == 0:\n            n10 += 1\n        elif x[i] == 1 and y[i] == 1:\n            n11 += 1\n    phi = (n00 * n11 - n01 * n10) / ((n00 + n01) * (n00 + n10) * (n10 + n11) * (n01 + n11)) ** 0.5\n    return round(phi, 4)\nx = [0, 1, 0, 1, 0, 1, 0, 1]\ny = [0, 0, 1, 1, 0, 0, 1, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x < -2.5:\n        return 0.0\n    elif x > 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x >= 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x >= 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    try:\n        result = math.log(1 + math.exp(x))\n        return round(result, 4)\n    except OverflowError:\n        return float('inf')\n    except (ValueError, TypeError):\n        print('Invalid input, please enter a number.')"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_loss = np.minimum(clipped_rhos * A, rhos * A)\n    kl_div = np.sum(pi_theta_old * (np.log(pi_theta_old) - np.log(pi_theta_ref)))\n    grpo_objective = np.mean(surrogate_loss) + beta * kl_div\n    grpo_objective = np.round(grpo_objective, 6)\n    return grpo_objective\nrhos = [1.2, 0.8, 1.5]\nA = [2.0, 1.0, 1.5]\npi_theta_old = [0.5, 0.3, 0.2]\npi_theta_ref = [0.4, 0.3, 0.3]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x * (1 / (1 + math.exp(-x))), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x >= 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Computes the sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    y_pred = sigmoid(z)\n    y_pred = (y_pred >= 0.5).astype(int)\n    return y_pred.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef softmax(z):\n    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\ndef cross_entropy(y_pred, y_true):\n    m = y_true.shape[0]\n    cost = -(1 / m) * np.sum(y_true * np.log(y_pred))\n    return cost\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = y.shape[1]\n    W = np.random.randn(n_features, n_classes)\n    b = np.random.randn(n_classes)\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, W) + b\n        y_pred = softmax(z)\n        loss = cross_entropy(y_pred, y)\n        losses.append(loss)\n        dW = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y, axis=0, keepdims=True)\n        W -= learning_rate * dW\n        b -= learning_rate * db\n    B = np.concatenate([b.reshape(-1), W.reshape(-1)]).tolist()\n    B = [round(x, 4) for x in B]\n    losses = [round(x, 4) for x in losses]\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef compute_loss(X, y, theta):\n    \"\"\"\n    Binary Cross Entropy loss\n    \"\"\"\n    m = len(y)\n    h = sigmoid(X @ theta)\n    epsilon = 1e-05\n    cost = 1 / m * ((-y).T @ np.log(h + epsilon) - (1 - y).T @ np.log(1 - h + epsilon))\n    return cost\ndef compute_gradient(X, y, theta):\n    \"\"\"\n    Gradient of the loss function\n    \"\"\"\n    m = len(y)\n    h = sigmoid(X @ theta)\n    grad = 1 / m * (X.T @ (h - y))\n    return grad\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    loss_history = []\n    for i in range(iterations):\n        gradient = compute_gradient(X, y, theta)\n        theta = theta - learning_rate * gradient\n        loss = compute_loss(X, y, theta)\n        loss_history.append(loss)\n    return ([round(theta[i], 4) for i in range(len(theta))], [round(loss, 4) for loss in loss_history])"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    masked_scores = scores * mask\n    attention = np.dot(np.exp(masked_scores), V)\n    return attention.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if len(apples) == 0:\n        return 0.0\n    unique_colors = len(set(apples))\n    total_apples = len(apples)\n    disorder_value = unique_colors / total_apples\n    return round(disorder_value, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    normalized = (X - mean) / np.sqrt(variance + epsilon)\n    output = gamma * normalized + beta\n    return np.round(output, 5).tolist()\nX = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\ngamma = np.ones((2, 1))\nbeta = np.zeros((2, 1))"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference_words = reference.split(' ')\n    candidate_words = candidate.split(' ')\n    reference_counter = Counter(reference_words)\n    candidate_counter = Counter(candidate_words)\n    matching_words = list((reference_counter & candidate_counter).elements())\n    precision = len(matching_words) / len(candidate_words)\n    recall = len(matching_words) / len(reference_words)\n    if precision == 0 and recall == 0:\n        fmean = 0\n    else:\n        fmean = (1 + gamma * gamma) * precision * recall / (recall + gamma * gamma * precision)\n    penalty = 1.0\n    for i in range(1, len(candidate_words) + 1):\n        if ' '.join(candidate_words[:i]) in ' '.join(reference_words):\n            continue\n        else:\n            penalty *= 1 - (i / len(candidate_words)) ** beta\n    score = alpha * fmean * penalty\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_val = min(x)\n    max_val = max(x)\n    return [round((i - min_val) / (max_val - min_val), 4) for i in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    x = np.matmul(x, w1)\n    x = np.maximum(x, 0)\n    x = np.matmul(x, w2)\n    x = x + np.matmul(np.maximum(x, 0), w2)\n    x = np.maximum(x, 0)\n    x = np.round(x, 4).tolist()\n    return x"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    return np.mean(x, axis=(0, 1))\nx = np.random.rand(10, 10, 3)"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    std_dev = np.std(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(std_dev ** 2 + epsilon)\n    out = gamma * X_norm + beta\n    return np.round(out, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    basis = []\n    for v in vectors:\n        w = np.array(v)\n        for vi in basis:\n            vi = np.array(vi)\n            w = w - np.dot(vi, w) * vi\n        if np.linalg.norm(w) > tol:\n            basis.append(list(w / np.linalg.norm(w)))\n    return basis\nvectors = [[1, 1], [2, 2]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    cross = np.cross(a, b)\n    cross = np.round(cross, 4)\n    cross_list = cross.tolist()\n    return cross_list"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    if np.linalg.det(A) == 0:\n        return -1\n    n = A.shape[0]\n    x = np.zeros(n)\n    det_A = np.linalg.det(A)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q):\n        return 0.0\n    if len(p) == 0 or len(q) == 0:\n        return 0.0\n    bc_distance = -np.log(np.sum(np.sqrt(np.multiply(p, q))))\n    bc_distance = round(bc_distance, 4)\n    return bc_distance"}
{"task_id": 121, "completion_id": 0, "solution": "from typing import List, Union\ndef vector_sum(a: List[Union[int, float]], b: List[Union[int, float]]) -> Union[List[Union[int, float]], int]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    gradients = []\n    for episode in episodes:\n        total_rewards = sum([e[2] for e in episode])\n        for (state, action, _) in episode:\n            prob = np.exp(theta[state, action]) / np.sum(np.exp(theta[state, :]))\n            grad = prob * total_rewards\n            gradients.append(grad)\n    avg_grad = np.mean(gradients)\n    return np.around(avg_grad, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "import math\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_moe = 2 * d_in * d_out * n_experts + d_in * d_out * n_experts + d_in * n_experts\n    flops_dense = 2 * d_in * d_out\n    savings = flops_dense / flops_moe\n    savings_percentage = (1 - savings) * 100\n    flops_moe = round(flops_moe, 1)\n    flops_dense = round(flops_dense, 1)\n    savings_percentage = round(savings_percentage, 1)\n    return (flops_moe, flops_dense, savings_percentage)\nn_experts = 10\nd_in = 1024\nd_out = 1024"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    S = np.matmul(X, W_g.T)\n    S = S + W_noise\n    S_topk = np.zeros_like(S)\n    for i in range(S.shape[0]):\n        indices = np.argpartition(S[i, :], -k)[-k:]\n        S_topk[i, indices] = S[i, indices]\n    S_topk_sum = np.sum(S_topk, axis=1)\n    S_topk_sum = S_topk_sum.reshape((len(S_topk_sum), 1))\n    G = np.divide(S_topk, S_topk_sum, out=np.zeros_like(S_topk), where=S_topk_sum != 0)\n    G = G * N\n    G = np.round(G, 4)\n    G = G.tolist()\n    return G"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    G = np.matmul(x, Wg.T)\n    G = G - np.max(G, axis=1, keepdims=True)\n    G = np.exp(G)\n    G = G / np.sum(G, axis=1, keepdims=True)\n    R = np.zeros_like(G)\n    R[np.arange(x.shape[0]).reshape(-1, 1), G.argsort(axis=1)[:, -top_k:]] = 1\n    O = np.matmul(x[:, None, :], We[None, :, :]).squeeze()\n    output = np.matmul(R, O)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    assert len(X.shape) == 4, 'Input X should be a 4D tensor'\n    assert len(gamma.shape) == 1 and len(beta.shape) == 1, 'gamma and beta should be 1D tensors'\n    assert gamma.shape[0] == beta.shape[0] == X.shape[1], \"gamma and beta should have the same length as X's channel dimension\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'num_groups should divide the number of channels'\n    X_hat = np.zeros_like(X)\n    gamma = gamma.reshape(1, num_groups, 1, 1)\n    beta = beta.reshape(1, num_groups, 1, 1)\n    for i in range(0, C, num_groups):\n        mean = np.mean(X[:, i:i + num_groups, :, :], axis=(2, 3), keepdims=True)\n        var = np.var(X[:, i:i + num_groups, :, :], axis=(2, 3), keepdims=True)\n        X_hat[:, i:i + num_groups, :, :] = (X[:, i:i + num_groups, :, :] - mean) / np.sqrt(var + epsilon)\n        X_hat[:, i:i + num_groups, :, :] = gamma * X_hat[:, i:i + num_groups, :, :] + beta\n    return X_hat.round(4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if np.abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return np.round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    dyt = alpha * np.tanh(beta * x) * gamma\n    dyt = np.round(dyt, 4)\n    dyt_list = dyt.tolist()\n    return dyt_list"}
