{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b:\n        return -1\n    num_cols = len(a[0])\n    if any((len(row) != num_cols for row in a)):\n        return -1\n    if num_cols != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((row[i] * b[i] for i in range(num_cols)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    return [list(row) for row in zip(*a)]"}
{"task_id": 3, "completion_id": 0, "solution": "def reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    flat_list = [item for sublist in a for item in sublist]\n    total_elements = len(flat_list)\n    new_total = new_shape[0] * new_shape[1]\n    if total_elements != new_total:\n        return []\n    return np.reshape(flat_list, new_shape).tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        return [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the scalar.\n\n    Parameters:\n        matrix (list[list[int | float]]): The input matrix.\n        scalar (int | float): The scalar to multiply with.\n\n    Returns:\n        list[list[int | float]]: The resulting matrix after scalar multiplication.\n    \"\"\"\n    result = [[element * scalar for element in row] for row in matrix]\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted in descending order.\n\n    :param matrix: A 2x2 matrix represented as a list of lists [[a, b], [c, d]]\n    :return: A list of eigenvalues sorted from highest to lowest\n    \"\"\"\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigenvalue1 = (trace + math.sqrt(discriminant)) / 2\n    eigenvalue2 = (trace - math.sqrt(discriminant)) / 2\n    return [eigenvalue1, eigenvalue2] if eigenvalue1 >= eigenvalue2 else [eigenvalue2, eigenvalue1]\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T_np), 0) or np.isclose(np.linalg.det(S_np), 0):\n            return -1\n        result = np.round(np.linalg.inv(T_np) @ A_np @ S_np, 4)\n        return result.tolist()\n    except np.linalg.LinAlgError:\n        return -1"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inv_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a list of feature vectors.\n    \n    Parameters:\n        vectors (list[list[float]]): A list of feature vectors, where each vector contains observations of a feature.\n    \n    Returns:\n        list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    means = [sum(feature) / n_observations for feature in vectors]\n    covariance_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            covariance = 0.0\n            for k in range(n_observations):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= n_observations - 1\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s = sum((A[i][j] * x[j] for j in range(N) if j != i))\n            x_new[i] = (b[i] - s) / A[i][i]\n        x = np.round(x_new, 4)\n    return np.round(x, 4).tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute approximate SVD for a 2x2 matrix using the Jacobian (Jacobi) method.\n    Returns (U, Sigma, V) where A \u2248 U @ Sigma @ V.T (rounded to 4 decimals).\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('Input matrix must be 2x2.')\n\n    def jacobi_rotation(theta, phi):\n        \"\"\"Construct 2x2 rotation matrix for Jacobi method.\"\"\"\n        c = np.cos(theta)\n        s = np.sin(theta)\n        return np.array([[c, s], [-s, c]])\n    M = A @ A.T\n    eps = 1e-08\n    max_iter = 1000\n    for _ in range(max_iter):\n        if abs(M[0, 1]) < eps:\n            break\n        tau = (M[1, 1] - M[0, 0]) / (2 * M[0, 1])\n        t = np.sign(tau) / (abs(tau) + np.sqrt(1 + tau ** 2))\n        c = 1 / np.sqrt(1 + t ** 2)\n        s = t * c\n        V = jacobi_rotation(c, s)\n        M = V.T @ M @ V\n    svals = np.sqrt(np.diag(M))\n    Sigma = np.diag(svals)\n    U = V\n    V = A.T @ U\n    non_zero = svals > 1e-08\n    V[:, non_zero] /= svals[non_zero]\n    V = np.linalg.qr(V.T)[0].T\n    U = np.round(U, 4)\n    Sigma = np.round(Sigma, 4)\n    V = np.round(V, 4)\n    return (U, Sigma, V.T)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_2x2(mat):\n        return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n\n    def determinant_3x3(mat):\n        return mat[0][0] * determinant_2x2([mat[1][1:], mat[2][1:]]) - mat[0][1] * determinant_2x2([mat[1][0:1] + mat[1][2:], mat[2][0:1] + mat[2][2:]]) + mat[0][2] * determinant_2x2([mat[1][:2], mat[2][:2]])\n    det = 0\n    for col in range(4):\n        minor = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        det += (-1) ** col * matrix[0][col] * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\ndet = determinant_4x4(matrix)"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_array = np.array(X, dtype=float)\n    y_array = np.array(y, dtype=float).reshape(-1, 1)\n    X_array = np.hstack((np.ones((X_array.shape[0], 1)), X_array))\n    theta = np.linalg.inv(X_array.T @ X_array) @ X_array.T @ y_array\n    return [round(float(val), 4) for val in theta]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y\n        gradient = 1 / m * X.T @ errors\n        theta -= alpha * gradient\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0, ddof=0)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_vals) / (max_vals - min_vals)\n    standardized_data_rounded = np.round(standardized_data, 4).tolist()\n    min_max_scaled_data_rounded = np.round(min_max_scaled_data, 4).tolist()\n    return (standardized_data_rounded, min_max_scaled_data_rounded)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering and return the final centroids.\n    \n    Parameters:\n    - points: List of points (tuples of coordinates)\n    - k: Number of clusters\n    - initial_centroids: List of initial centroids (tuples of coordinates)\n    - max_iterations: Maximum number of iterations\n    \n    Returns:\n    - List of final centroids (tuples with coordinates rounded to 4 decimals)\n    \"\"\"\n    points = np.array(points, dtype=np.float64)\n    centroids = np.array(initial_centroids, dtype=np.float64)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points[:, np.newaxis] - centroids[np.newaxis, :]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        new_centroids = []\n        for i in range(k):\n            cluster_points = points[labels == i]\n            if cluster_points.shape[0] > 0:\n                new_centroids.append(np.mean(cluster_points, axis=0))\n            else:\n                new_centroids.append(centroids[i])\n        new_centroids = np.array(new_centroids)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    rounded_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return rounded_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.1, 1.9), (9.0, 11.0)]\ninitial_centroids = [(2.0, 2.0), (8.0, 8.0)]\nk = 2\nmax_iterations = 100"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate a list of train and test indices for K-Fold Cross-Validation.\n    \n    Parameters:\n    X (np.ndarray): The input features.\n    y (np.ndarray): The target labels.\n    k (int): Number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int or None): Seed for random number generator for reproducibility.\n    \n    Returns:\n    List[Tuple[np.ndarray, np.ndarray]]: A list of (train_indices, test_indices) for each fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if shuffle:\n        np.random.shuffle(indices)\n    folds = []\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]), axis=0)\n        folds.append((train_indices, test_indices))\n        start = end\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k].real\n    principal_components_rounded = [list(np.round(component, 4)) for component in principal_components.T]\n    return principal_components_rounded"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List, Any\ndef entropy(values: List[Any]) -> float:\n    \"\"\"Compute the entropy of a list of class values.\"\"\"\n    counter = Counter(values)\n    total = len(values)\n    return -sum((count / total * math.log(count / total, 2) for count in counter.values()))\ndef information_gain(parent_entropy: float, left: List[Dict], right: List[Dict], target_attr: str) -> float:\n    \"\"\"Compute the information gain of a split.\"\"\"\n    total = len(left) + len(right)\n    gain = parent_entropy\n    for subset in [left, right]:\n        if subset:\n            child_entropy = entropy([ex[target_attr] for ex in subset])\n            gain -= len(subset) / total * child_entropy\n    return gain\ndef best_split(examples: List[Dict], attributes: List[str], target_attr: str) -> tuple:\n    \"\"\"Find the best attribute and split value to divide the dataset.\"\"\"\n    parent_entropy = entropy([ex[target_attr] for ex in examples])\n    best_attr = None\n    best_value = None\n    best_gain = -1\n    best_split_result = (examples, [])\n    for attr in attributes:\n        sorted_values = sorted(set((ex[attr] for ex in examples)))\n        for i in range(len(sorted_values) - 1):\n            mid = (sorted_values[i] + sorted_values[i + 1]) / 2\n            left = [ex for ex in examples if ex[attr] <= mid]\n            right = [ex for ex in examples if ex[attr] > mid]\n            gain = information_gain(parent_entropy, left, right, target_attr)\n            if gain > best_gain:\n                best_gain = gain\n                best_attr = attr\n                best_value = mid\n                best_split_result = (left, right)\n    return (best_attr, best_value, best_gain, best_split_result)\ndef most_common_class(examples: List[Dict], target_attr: str) -> Any:\n    \"\"\"Return the most common class among examples.\"\"\"\n    classes = [ex[target_attr] for ex in examples]\n    return max(set(classes), key=classes.count)\ndef learn_decision_tree(examples: List[Dict], attributes: List[str], target_attr: str) -> Dict:\n    \"\"\"\n    Build a decision tree using recursive binary splitting based on entropy and information gain.\n    \n    Parameters:\n    - examples: List of examples. Each example is a dictionary of attribute-value pairs.\n    - attributes: List of attribute names to use for building the tree.\n    - target_attr: Name of the target attribute (class label).\n    \n    Returns:\n    - A nested dictionary representing the decision tree.\n    \"\"\"\n    if len(set((ex[target_attr] for ex in examples))) == 1:\n        return {'class': examples[0][target_attr]}\n    if not attributes:\n        return {'class': most_common_class(examples, target_attr)}\n    (best_attr, best_value, _, (left, right)) = best_split(examples, attributes, target_attr)\n    if best_attr is None:\n        return {'class': most_common_class(examples, target_attr)}\n    left_tree = learn_decision_tree(left, [a for a in attributes if a != best_attr], target_attr)\n    right_tree = learn_decision_tree(right, [a for a in attributes if a != best_attr], target_attr)\n    return {'attribute': best_attr, 'split_value': best_value, 'left': left_tree, 'right': right_tree}"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic version of the Pegasos algorithm for training a kernel SVM classifier.\n\n    Parameters:\n    - data: np.ndarray, shape (n_samples, n_features)\n    - labels: np.ndarray, shape (n_samples,)\n    - kernel: str, either 'linear' or 'rbf'\n    - lambda_val: float, regularization parameter\n    - iterations: int, number of training iterations\n    - sigma: float, standard deviation for RBF kernel\n\n    Returns:\n    - alphas: list, shape (n_samples,), alpha coefficients rounded to 4 decimals\n    - bias: float, bias term rounded to 4 decimals\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        gradient = np.zeros(n_samples)\n        predictions = np.zeros(n_samples)\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    k = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    k = np.exp(-np.sum((data[i] - data[j]) ** 2) / (2 * sigma ** 2))\n                predictions[i] += alphas[j] * labels[j] * k\n            predictions[i] += bias\n        for i in range(n_samples):\n            hinge = max(1 - labels[i] * predictions[i], 0)\n            if hinge > 0:\n                for j in range(n_samples):\n                    if kernel == 'linear':\n                        k = np.dot(data[i], data[j])\n                    elif kernel == 'rbf':\n                        k = np.exp(-np.sum((data[i] - data[j]) ** 2) / (2 * sigma ** 2))\n                    gradient[j] += -labels[i] * labels[j] * k\n        alphas = alphas - eta * (alphas / t + gradient)\n        bias -= eta * (-labels).sum() if any((max(1 - labels[i] * predictions[i], 0) > 0 for i in range(n_samples))) else 0\n        alphas = np.clip(alphas, 0, 1.0 / (lambda_val * t))\n    return ([float(np.round(alpha, 4)) for alpha in alphas.tolist()], round(bias, 4))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid of a single input value z.\n\n    Parameters:\n    z (float): Input value.\n\n    Returns:\n    float: The result of the sigmoid function applied to z, rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    return [round(value, 4) for value in softmax_values]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    linear_combinations = [sum((f * w for (f, w) in zip(feature, weights))) + bias for feature in features]\n    predictions = [1 / (1 + math.exp(-z)) for z in linear_combinations]\n    rounded_predictions = np.round(predictions, 4).tolist()\n    mse = np.round(np.mean([(y - p) ** 2 for (y, p) in zip(labels, predictions)]), 4)\n    return (rounded_predictions, mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_per_epoch = []\n    for _ in range(epochs):\n        linear = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_per_epoch.append(np.round(mse, 4))\n        error = predictions - labels\n        d_weights = np.dot(features.T, error) / len(labels)\n        d_bias = np.mean(error)\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    weights = np.round(weights, 4)\n    bias = round(bias, 4)\n    mse_per_epoch = [round(val, 4) for val in mse_per_epoch]\n    return (weights.tolist(), bias, mse_per_epoch)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, (self,), 'relu')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'Only int/float exponents supported'\n        out = Value(self.data ** other, (self,), f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to C.\n\n    Parameters:\n    B (list of list of int): The basis vectors in basis B.\n    C (list of list of int): The basis vectors in basis C.\n\n    Returns:\n    list of list of float: The transformation matrix P from B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_mat = np.array(B, dtype=float)\n    C_mat = np.array(C, dtype=float)\n    P = np.linalg.inv(C_mat) @ B_mat\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros((2, 2))\n    for i in range(2):\n        u_i = A @ V[:, i]\n        if np.isclose(singular_values[i], 0):\n            u_i = np.array([0, 0])\n        else:\n            u_i = u_i / np.linalg.norm(u_i)\n        U[:, i] = u_i\n    for i in range(2):\n        if np.sign(U[0, i]) != np.sign(A[0, 0]):\n            U[:, i] *= -1\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    VT = np.round(V.T, 4).tolist()\n    return (U, S, VT)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles two NumPy arrays X and y while maintaining their correspondence.\n    \n    Parameters:\n    - X: numpy.ndarray, input features array.\n    - y: numpy.ndarray, target labels array.\n    - seed: int, optional. Seed for random number generator for reproducibility.\n    \n    Returns:\n    - List of shuffled X and y, as lists (output is a list of [X_shuffled.tolist(), y_shuffled.tolist()])\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of samples.')\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(X.shape[0])\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterator that yields batches of size `batch_size` from the input arrays `X` and optionally `y`.\n    \n    Parameters:\n        X (np.ndarray): Input features array.\n        y (np.ndarray or None): Optional input labels array.\n        batch_size (int): Size of each batch.\n    \n    Yields:\n        If y is not None: list of tuples (X_batch.tolist(), y_batch.tolist())\n        If y is None: list of X_batch.tolist()\n    \"\"\"\n    num_samples = X.shape[0]\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[start_idx:end_idx]\n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy.ndarray or list of lists): The dataset.\n    feature_i (int): Index of the feature to use for splitting.\n    threshold (float): Threshold value to split on.\n\n    Returns:\n    list: Two subsets of the dataset.\n    \"\"\"\n    X = np.array(X)\n    left = X[X[:, feature_i] >= threshold]\n    right = X[X[:, feature_i] < threshold]\n    return [left.tolist(), right.tolist()]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    features = []\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), d):\n            feature = np.prod(X[:, indices], axis=1)\n            features.append(feature)\n    poly_X = np.column_stack(features)\n    return poly_X.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    - X (np.ndarray): 2D array of features.\n    - y (np.ndarray): 1D array of labels.\n    - n_subsets (int): Number of subsets to generate.\n    - replacements (bool): If True, sample with replacement; else, without replacement.\n    - seed (int): Random seed for reproducibility.\n\n    Returns:\n    - List of tuples: Each tuple is a (X_subset, y_subset) subset of the dataset.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integer values to a one-hot encoded 2D list.\n    \n    Parameters:\n    - x (np.ndarray): A 1D numpy array of integers.\n    - n_col (int, optional): Number of columns in the one-hot encoded matrix.\n                              If None (default), it is determined from the maximum value in x.\n    \n    Returns:\n    - list: A 2D list representing the one-hot encoded array.\n    \"\"\"\n    x = np.array(x)\n    if n_col is None:\n        n_col = x.max() + 1\n    if (x >= n_col).any():\n        raise ValueError(f\"One-hot encoding failed: values in 'x' cannot exceed {n_col - 1} when 'n_col' is {n_col}.\")\n    one_hot = np.zeros((x.size, n_col), dtype=int)\n    one_hot[np.arange(x.size), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be the same.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for the given dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array of shape (n_samples, n_features).\n    Y (np.ndarray, optional): 2D numpy array, if provided, \n                              the correlation matrix between X and Y is computed. Defaults to None.\n    \n    Returns:\n    list: Correlation matrix as a 2D list with values rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    X_std = X_centered / np.std(X, axis=0, ddof=1)\n    Y_std = Y_centered / np.std(Y, axis=0, ddof=1)\n    corr_matrix = np.dot(X_std.T, Y_std) / (X.shape[0] - 1)\n    corr_matrix_rounded = np.round(corr_matrix, 4)\n    return corr_matrix_rounded.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    D = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_threshold = None\n        best_feature = -1\n        best_polarity = 1\n        for feat_idx in range(n_features):\n            thresholds = np.unique(X[:, feat_idx])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    predictions = polarity * np.where(X[:, feat_idx] > threshold, 1, -1)\n                    error = D[y != predictions].sum()\n                    if error < best_error:\n                        best_error = error\n                        best_threshold = threshold\n                        best_feature = feat_idx\n                        best_polarity = polarity\n        if best_error >= 0.5:\n            break\n        epsilon = max(best_error, 1e-10)\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        alpha = round(alpha, 4)\n        classifier = {'feature': best_feature, 'threshold': best_threshold, 'polarity': best_polarity, 'alpha': alpha}\n        classifiers.append(classifier)\n        predictions = best_polarity * np.where(X[:, best_feature] > best_threshold, 1, -1)\n        D *= np.exp(-alpha * y * predictions)\n        D /= D.sum()\n    for clf in classifiers:\n        clf['threshold'] = round(clf['threshold'], 4)\n        clf['alpha'] = round(clf['alpha'], 4)\n    return classifiers\nX = np.array([[0.1, 0.4], [0.5, 0.6], [0.3, 0.3], [0.8, 0.1], [1.0, 0.9]])\ny = np.array([1, 1, -1, -1, 1])\nclassifiers = adaboost_fit(X, y, n_clf=3)"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    shifted_scores = scores - np.max(scores)\n    exps = np.exp(shifted_scores)\n    log_sum_exps = np.log(np.sum(exps))\n    return (shifted_scores - log_sum_exps).round(4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.input_shape = input_shape\n        self.optimizer = None\n        self.optimized_weights = None\n        self.optimized_biases = None\n\n    def initialize(self, optimizer):\n        limit = 1 / np.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimized_weights = copy.copy(optimizer)\n        self.optimized_biases = copy.copy(optimizer)\n\n    def parameters(self):\n        if self.W is not None and self.w0 is not None:\n            return self.W.size + self.w0.size\n        return 0\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        n_samples = accum_grad.shape[0]\n        grad_weights = np.dot(self.layer_input.T, accum_grad)\n        grad_weights = grad_weights / n_samples\n        grad_biases = np.mean(accum_grad, axis=0, keepdims=True)\n        if self.trainable:\n            self.W = self.optimized_weights.update(self.W, grad_weights)\n            self.w0 = self.optimized_biases.update(self.w0, grad_biases)\n        grad = np.dot(accum_grad, self.W.T)\n        return grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int) -> list:\n    \"\"\"\n    Applies a 2D convolution operation on the input matrix.\n\n    Parameters:\n    - input_matrix (np.ndarray): Input 2D matrix (image).\n    - kernel (np.ndarray): 2D convolution kernel.\n    - padding (int): Number of zero-padding layers added to both sides of the input.\n    - stride (int): Stride with which kernel is applied.\n\n    Returns:\n    - list: Output as a Python list after reshaping and rounding.\n    \"\"\"\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded_input = np.pad(input_matrix, padding, mode='constant', constant_values=0)\n    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            h_start = i * stride\n            w_start = j * stride\n            h_end = h_start + kernel_h\n            w_end = w_start + kernel_w\n            region = padded_input[h_start:h_end, w_start:w_end]\n            conv_result = np.sum(region * kernel)\n            output[i, j] = round(conv_result, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(0.0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value.\n    alpha (float, optional): The slope for negative values. Defaults to 0.01.\n\n    Returns:\n    float: The result of applying Leaky ReLU to the input.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1)\n    y_pred (np.ndarray): Predicted binary labels (0 or 1)\n\n    Returns:\n    float: Precision metric\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent for linear regression using MSE loss.\n    Supports Batch, Stochastic, and Mini-Batch variants.\n\n    Parameters:\n    - X (np.ndarray): Input features (shape: [n_samples, n_features]).\n    - y (np.ndarray): Target labels (shape: [n_samples,]).\n    - weights (np.ndarray): Initial weights (shape: [n_features,]).\n    - learning_rate (float): Learning rate for updates.\n    - n_iterations (int): Number of iterations to run.\n    - batch_size (int): Number of samples per batch (for Mini-Batch).\n    - method (str): One of 'batch', 'stochastic', or 'mini-batch'.\n\n    Returns:\n    - weights (list): Updated weights rounded to 4 decimal places.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.array(weights)\n    for _ in range(n_iterations):\n        if method == 'batch':\n            batch_X = X\n            batch_y = y\n        elif method == 'stochastic':\n            idx = np.random.randint(n_samples)\n            batch_X = X[idx:idx + 1]\n            batch_y = y[idx:idx + 1]\n        elif method == 'mini-batch':\n            indices = np.random.choice(n_samples, batch_size, replace=False)\n            batch_X = X[indices]\n            batch_y = y[indices]\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'.\")\n        y_pred = np.dot(batch_X, weights)\n        gradient = 2 * np.dot(batch_X.T, y_pred - batch_y) / batch_size\n        weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    - matrix (list of lists or numpy.ndarray): The input matrix.\n\n    Returns:\n    - list of lists: The matrix in RREF format.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (m, n) = matrix.shape\n    r = 0\n    for c in range(n):\n        pivot_row = None\n        for i in range(r, m):\n            if abs(matrix[i][c]) > 1e-08:\n                pivot_row = i\n                break\n        if pivot_row is None:\n            continue\n        matrix[[r, pivot_row]] = matrix[[pivot_row, r]]\n        matrix[r] = matrix[r] / matrix[r][c]\n        for i in range(m):\n            if i != r and abs(matrix[i][c]) > 1e-08:\n                matrix[i] = matrix[i] - matrix[i][c] * matrix[r]\n        r += 1\n        if r >= m:\n            break\n    return matrix.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm to minimize a given function.\n    \n    Parameters:\n    - f: Objective function to be optimized\n    - grad: Function that computes the gradient of f\n    - x0: Initial parameter values\n    - learning_rate: Step size\n    - beta1: Exponential decay rate for the first moment\n    - beta2: Exponential decay rate for the second moment\n    - epsilon: Small constant for numerical stability\n    - num_iterations: Number of iterations\n    \n    Returns:\n    - Optimized parameters (as a list after rounding and reshaping)\n    \"\"\"\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        g_t = grad(x)\n        m = beta1 * m + (1 - beta1) * g_t\n        v = beta2 * v + (1 - beta2) * g_t ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent with L1 Regularization.\n    \n    Parameters:\n    X (np.array): Input feature matrix (n_samples, n_features)\n    y (np.array): Target variable (n_samples,)\n    alpha (float): Regularization strength (L1 penalty)\n    learning_rate (float): Learning rate for gradient descent\n    max_iter (int): Maximum number of iterations\n    tol (float): Tolerance for stopping criteria\n    \n    Returns:\n    tuple: (weights, bias) where weights and bias are rounded to 4 decimals\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -1 / n_samples * np.dot(X.T, y - y_pred)\n        db = -1 / n_samples * np.sum(y - y_pred)\n        dw += alpha / n_samples * np.sign(weights)\n        new_weights = weights - learning_rate * dw\n        new_bias = bias - learning_rate * db\n        if np.linalg.norm(new_weights - weights) < tol:\n            break\n        (weights, bias) = (new_weights, new_bias)\n    return (np.round(weights, 4).tolist(), np.round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                dp[i][j] = 1 + min(dp[i][j - 1], dp[i - 1][j], dp[i - 1][j - 1])\n                if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len_source][len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the Recall metric for binary classification.\n\n    Parameters:\n    - y_true (list): True binary labels (0 or 1)\n    - y_pred (list): Predicted binary labels (0 or 1)\n\n    Returns:\n    - recall (float): Recall score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(true_positives / denominator, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 0]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n\n    Parameters:\n    X : np.ndarray\n        Input matrix of shape (seq_length, input_dim).\n    W_q : np.ndarray\n        Query weight matrix of shape (input_dim, d_k).\n    W_k : np.ndarray\n        Key weight matrix of shape (input_dim, d_k).\n    W_v : np.ndarray\n        Value weight matrix of shape (input_dim, d_v).\n\n    Returns:\n    output : list\n        Self-attention output, rounded to 4 decimal places, in list form.\n    \"\"\"\n    seq_length = X.shape[0]\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    attention_scores = Q @ K.T\n    exp_scores = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = attention_weights @ V\n    return np.round(output, 4).tolist()\nX = np.array([[1, 0], [0, 1], [1, 1]])\nW_q = np.array([[0.1, 0.2], [0.3, 0.4]])\nW_k = np.array([[0.5, 0.6], [0.7, 0.8]])\nW_v = np.array([[0.9, 0.1], [0.2, 0.3]])"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Performs the forward pass of a simple RNN cell.\n    \n    Args:\n        input_sequence: List of input vectors at each time step.\n        initial_hidden_state: Initial hidden state.\n        Wx: Weight matrix for input-to-hidden connections.\n        Wh: Weight matrix for hidden-to-hidden connections.\n        b: Bias vector.\n        \n    Returns:\n        Final hidden state after processing the entire sequence (rounded to 4 decimals).\n    \"\"\"\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        h_prev = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b)\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points, dtype=np.float32)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    translated_homogeneous = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = translated_homogeneous[:, :2]\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n\n    Parameters:\n        mu_p (float): Mean of distribution P\n        sigma_p (float): Standard deviation of distribution P\n        mu_q (float): Mean of distribution Q\n        sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n        float: KL divergence D_{KL}(P || Q)\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + 0.5 * (sigma_p ** 2 / sigma_q ** 2 + (mu_p - mu_q) ** 2 / sigma_q ** 2 - 1)"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Apply the Gauss-Seidel method to solve the linear system Ax = b.\n    \n    Parameters:\n        A (np.ndarray): An n x n matrix of coefficients.\n        b (np.ndarray): A right-hand side vector of size n.\n        n (int): Number of iterations to run.\n        x_ini (np.ndarray, optional): Initial guess vector. Defaults to all zeros.\n    \n    Returns:\n        list: The solution vector after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(N)\n    else:\n        x = np.array(x_ini)\n    for _ in range(n):\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.around(x, 4).tolist()\nA = [[4, 1], [1, 3]]\nb = [1, 2]"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    Returns the solution vector x.\n    \"\"\"\n    n = len(b)\n    A = np.array(A, float)\n    b = np.array(b, float)\n    for i in range(n):\n        max_row = np.argmax(np.abs(A[i:, i])) + i\n        if max_row != i:\n            A[[i, max_row]] = A[[max_row, i]]\n            b[[i, max_row]] = b[[max_row, i]]\n        pivot = A[i, i]\n        A[i] = A[i] / pivot\n        b[i] = b[i] / pivot\n        for j in range(i + 1, n):\n            factor = A[j, i]\n            A[j] = A[j] - factor * A[i]\n            b[j] = b[j] - factor * b[i]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = b[i] - np.dot(A[i, i + 1:], x[i + 1:])\n    return np.round(x, 4).tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        (h_t, c_t) = (initial_hidden_state, initial_cell_state)\n        h_sequence = []\n        for seq in x:\n            seq = np.reshape(seq, (len(seq), 1))\n            concat_input = np.row_stack((seq, h_t))\n            f_t = self.sigmoid(np.dot(self.Wf, concat_input) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, concat_input) + self.bi)\n            g_t = np.tanh(np.dot(self.Wc, concat_input) + self.bc)\n            c_t = f_t * c_t + i_t * g_t\n            o_t = self.sigmoid(np.dot(self.Wo, concat_input) + self.bo)\n            h_t = o_t * np.tanh(c_t)\n            h_sequence.append(h_t)\n        h_sequence = [np.round(h, 4).tolist() for h in h_sequence]\n        final_h = np.round(h_t, 4).tolist()\n        final_c = np.round(c_t, 4).tolist()\n        return (h_sequence, final_h, final_c)\ninput_size = 2\nhidden_size = 3\nx = [np.array([0.1, 2.3]), np.array([4.5, -0.7]), np.array([-1.0, 3.0])]"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty, cannot compute TF-IDF scores.')\n    vocabulary = sorted(set((word for doc in corpus for word in doc)))\n    query_vocab = list(set(query))\n    if not query_vocab:\n        return []\n    tf = np.zeros((len(corpus), len(vocabulary)))\n    for (doc_idx, doc) in enumerate(corpus):\n        for word in doc:\n            if word in vocabulary:\n                word_idx = vocabulary.index(word)\n                tf[doc_idx, word_idx] += 1\n        if len(doc) > 0:\n            tf[doc_idx] /= len(doc)\n    df = np.zeros(len(vocabulary))\n    total_docs = len(corpus)\n    for (word_idx, word) in enumerate(vocabulary):\n        doc_count = sum((1 for doc in corpus if word in doc))\n        df[word_idx] = np.log((total_docs + 1) / (doc_count + 1))\n    query_indices = [vocabulary.index(word) for word in query_vocab if word in vocabulary]\n    if not query_indices:\n        return []\n    tf_idf = np.zeros((len(corpus), len(query)))\n    for (i, word) in enumerate(query_vocab):\n        if word in vocabulary:\n            word_idx = vocabulary.index(word)\n            tf_idf[:, i] = tf[:, word_idx] * df[word_idx]\n    tf_idf = np.round(tf_idf, 4)\n    return tf_idf.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    beta_squared = beta ** 2\n    score = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall) if beta_squared * precision + recall > 0 else 0\n    return round(score, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n        - x: Input sequence of shape (input_size, sequence_length)\n        \n        Returns:\n        - outputs: Predicted outputs for each time step of shape (output_size, sequence_length)\n        - hiddens: Hidden states for each time step of shape (hidden_size, sequence_length)\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = np.zeros((self.hidden_size, sequence_length))\n        y = np.zeros((self.W_hy.shape[0], sequence_length))\n        for t in range(sequence_length):\n            h[:, t] = np.tanh(self.W_xh @ x[:, t] + self.W_hh @ h[:, t - 1] + self.b_h[:, 0])\n            y[:, t] = self.W_hy @ h[:, t] + self.b_y[:, 0]\n        return (y, h)\n\n    def backward(self, x, y, targets, learning_rate):\n        \"\"\"\n        Backward pass using BPTT algorithm to compute gradients and update parameters.\n        \n        Args:\n        - x: Input sequence of shape (input_size, sequence_length)\n        - y: Output sequence from forward pass of shape (output_size, sequence_length)\n        - targets: Expected output sequence of shape (output_size, sequence_length)\n        - learning_rate: Learning rate for parameter updates\n        \n        Returns:\n        - loss: Total loss\n        \"\"\"\n        sequence_length = x.shape[1]\n        loss = 0\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        h = np.zeros((self.hidden_size, sequence_length))\n        h[:, 0] = np.zeros((self.hidden_size,))\n        for t in range(sequence_length):\n            h[:, t] = np.tanh(self.W_xh @ x[:, t] + self.W_hh @ h[:, t - 1] + self.b_h[:, 0])\n        dh_next = np.zeros_like(h[:, 0])\n        for t in reversed(range(sequence_length)):\n            dy = y[:, t] - targets[:, t]\n            dW_hy += dy.reshape(-1, 1) @ h[:, t].reshape(1, -1)\n            db_y += dy.reshape(-1, 1)\n            loss += 0.5 * np.sum(dy ** 2)\n            dh = self.W_hy.T @ dy + dh_next\n            dhraw = (1 - h[:, t] ** 2) * dh\n            dW_xh += dhraw.reshape(-1, 1) @ x[:, t].reshape(1, -1)\n            dW_hh += dhraw.reshape(-1, 1) @ h[:, t - 1].reshape(1, -1)\n            db_h += dhraw.reshape(-1, 1)\n            dh_next = self.W_hh.T @ dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    for _ in range(n):\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = r @ r\n        if np.sqrt(rs_new) < tol:\n            break\n        p = r + rs_new / rs_old * p\n        rs_old = rs_new\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    n = len(y)\n    impurity = 1.0\n    for count in counts.values():\n        prob = count / n\n        impurity -= prob ** 2\n    return round(impurity, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                column_indices.append(col_idx)\n                count += 1\n        row_pointer.append(row_pointer[-1] + count)\n    return (values, column_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_v_L = np.dot(v, L)\n    dot_L_L = np.dot(L, L)\n    if dot_L_L == 0:\n        return [round(0.0, 3) for _ in v]\n    scalar = dot_v_L / dot_L_L\n    projection = scalar * L\n    return [round(x, 3) for x in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space (image) of a matrix A.\n    \n    Parameters:\n        A (ndarray): A 2D NumPy array representing the matrix.\n\n    Returns:\n        list: A list of column vectors that form a basis for the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    rank = np.linalg.matrix_rank(A)\n    (_, pivots) = np.linalg.qr(A, mode='r')\n    pivot_indices = np.where(np.abs(pivots) > 1e-10)[0]\n    basis = A[:, pivot_indices]\n    return np.round(basis, 8).tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_true_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_true_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    num_rows = len(img)\n    num_cols = len(img[0])\n    if not all((len(row) == num_cols for row in img)):\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    average_brightness = round(total_brightness / total_pixels, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n        y_true (array-like): Ground truth (correct) target values.\n        y_pred (array-like): Estimated target values.\n\n    Returns:\n        float: The RMSE value, rounded to three decimal places.\n\n    Raises:\n        ValueError: If input arrays are not of the same shape or if either is empty.\n        TypeError: If inputs are not array-like (e.g., not list or numpy array).\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except Exception as e:\n        raise TypeError(f'Inputs must be array-like. Received error: {e}')\n    if y_true.shape != y_pred.shape:\n        raise ValueError(f'Input arrays must have the same shape. Got shapes: {y_true.shape} and {y_pred.shape}')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    squared_diffs = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_diffs)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    - y_true: numpy array of true binary labels (0 or 1)\n    - y_pred: numpy array of predicted binary labels (0 or 1)\n    \n    Returns:\n    - Jaccard Index as a float rounded to three decimal places\n    \"\"\"\n    intersection = np.logical_and(y_true, y_pred)\n    union = np.logical_or(y_true, y_pred)\n    intersection_count = np.sum(intersection)\n    union_count = np.sum(union)\n    if union_count == 0:\n        return 1.0\n    jaccard = intersection_count / union_count\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n    \n    Parameters:\n    y_true (np.array): Ground truth (correct) binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true + sum_pred == 0:\n        return 0.0\n    score = 2.0 * intersection / (sum_true + sum_pred)\n    return round(score, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    Feature hypervectors are created using the same feature seed provided in the `random_seeds` dictionary.\n    All feature hypervectors are bundled to create a composite hypervector for the row.\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row where keys are feature names and values are their corresponding values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary of seeds for random number generators keyed by feature name.\n    \n    Returns:\n    - List[float]: A composite hypervector representing the entire row.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        rng_name = np.random.RandomState(seed=random_seeds.get(feature_name, 0))\n        feature_name_hv = rng_name.normal(0, 1, dim)\n        value_seed = random_seeds.get(feature_name, 0) ^ hash(feature_value)\n        rng_value = np.random.RandomState(seed=value_seed)\n        feature_value_hv = rng_value.normal(0, 1, dim)\n        bound_hv = feature_name_hv * feature_value_hv\n        composite_hv += bound_hv\n    composite_hv = composite_hv / np.sqrt(np.sum(composite_hv ** 2) + 1e-08)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    matrix = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            matrix[1][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            matrix[1][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            matrix[0][1] += 1\n        elif y_true == 0 and y_pred == 0:\n            matrix[0][0] += 1\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Vectors cannot be empty.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError('Vectors cannot have zero magnitude.')\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0\n    accuracy = round(accuracy, 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    precision = round(precision, 3)\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    recall = round(recall, 3)\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    f1_score = round(f1_score, 3)\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    specificity = round(specificity, 3)\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    data = np.asarray(data)\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    mode_val = float(np.squeeze(np.unique(data, return_counts=True)[0][np.unique(data, return_counts=True)[1].argmax()]))\n    variance_val = np.var(data, ddof=1)\n    std_dev_val = np.std(data, ddof=1)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr_val = percentile_75 - percentile_25\n    results = {'mean': round(mean_val, 4), 'median': round(median_val, 4), 'mode': mode_val, 'variance': round(variance_val, 4), 'standard_deviation': round(std_dev_val, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr_val, 4)}\n    return results"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0:\n        return 0\n    return round(math.exp(-lam) * lam ** k / math.factorial(k), 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array.')\n    if img.dtype not in (np.uint8, np.int8, np.uint16, np.int16):\n        raise ValueError('Input image must have integer-type pixel values.')\n    max_pixel = img.max()\n    min_pixel = img.min()\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    ndim = len(data)\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(ndim), d):\n            feature = np.prod([data[i] for i in indices])\n            result.append(feature)\n    return [list(map(lambda x: round(x, 8), result))]\ndegree = 2"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            div_term = np.exp(np.float16(-np.log(10000.0) * (i / d_model)))\n            pe[pos, i] = np.sin(np.float16(pos) * div_term)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(np.float16(pos) * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / e_x.sum(axis=-1, keepdims=True)\ndef scaled_dot_product_attention(Q, K, V):\n    \"\"\"Calculate the attention given query, key, and value matrices.\"\"\"\n    d_k = K.shape[-1]\n    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = softmax(scores)\n    output = np.matmul(attention_weights, V)\n    return output\ndef build_block(hparams):\n    \"\"\"Construct a simplified GPT-2 Transformer block with random parameters.\"\"\"\n    n_embd = hparams.get('n_embd', 10)\n    n_head = hparams['n_head']\n    block = {'ln_1': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'attn': {'c_attn': {'w': np.random.rand(n_embd, 3 * n_embd), 'b': np.zeros(3 * n_embd)}, 'c_proj': {'w': np.random.rand(n_embd, n_embd), 'b': np.zeros(n_embd)}}, 'ln_2': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'mlp': {'c_fc': {'w': np.random.rand(n_embd, 4 * n_embd), 'b': np.zeros(4 * n_embd)}, 'c_proj': {'w': np.random.rand(4 * n_embd, n_embd), 'b': np.zeros(n_embd)}}}\n    return block\ndef layer_norm(x, g, b, eps=1e-05):\n    \"\"\"Layer normalization implementation.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    std = np.sqrt(variance + eps)\n    x_norm = (x - mean) / std\n    return g * x_norm + b\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    \"\"\"Generates text using a GPT-2 like architecture with dummy parameters.\"\"\"\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    hparams['n_embd'] = 10\n    params['blocks'] = [build_block(hparams) for _ in range(2)]\n    params['op'] = np.random.rand(hparams['n_embd'], len(encoder.encoder_dict))\n    context_tokens = encoder.encode(prompt)\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    n_embd = hparams['n_embd']\n    token_ids = np.array(context_tokens)\n    input_seq_len = len(token_ids)\n    pos = np.arange(0, n_tokens_to_generate + input_seq_len)\n    wpe = params['wpe'][:n_tokens_to_generate + input_seq_len]\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][token_ids]\n        positional_embeddings = wpe[:len(token_ids)]\n        x = token_embeddings + positional_embeddings\n        for block in params['blocks']:\n            x_ln1 = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n            qkv = np.dot(x_ln1, block['attn']['c_attn']['w']) + block['attn']['c_attn']['b']\n            q = qkv[:, :, :n_embd]\n            k = qkv[:, :, n_embd:2 * n_embd]\n            v = qkv[:, :, 2 * n_embd:]\n            batch_size = q.shape[0]\n            q = q.reshape(batch_size, -1, n_head, n_embd // n_head).transpose(0, 2, 1, 3)\n            k = k.reshape(batch_size, -1, n_head, n_embd // n_head).transpose(0, 2, 1, 3)\n            v = v.reshape(batch_size, -1, n_head, n_embd // n_head).transpose(0, 2, 1, 3)\n            attn_output = scaled_dot_product_attention(q, k, v)\n            attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, -1, n_embd)\n            attn_output = np.dot(attn_output, block['attn']['c_proj']['w']) + block['attn']['c_proj']['b']\n            x = x + attn_output\n            x_ln2 = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n            mlp_output = np.dot(x_ln2, block['mlp']['c_fc']['w']) + block['mlp']['c_fc']['b']\n            mlp_output = np.where(mlp_output > 0, mlp_output, 0)\n            mlp_output = np.dot(mlp_output, block['mlp']['c_proj']['w']) + block['mlp']['c_proj']['b']\n            x = x + mlp_output\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.dot(x, params['op'])\n        final_logits = logits[-1, :]\n        next_token_id = np.argmax(final_logits)\n        token_ids = np.append(token_ids, next_token_id)\n    return encoder.decode(token_ids[input_seq_len:])"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n    crystals = np.array(crystal_values)\n    if crystals.shape[0] != n:\n        crystals = crystals.reshape(n, -1)\n    if crystals.shape[1] < dimension:\n        zero_padding = np.zeros((n, dimension - crystals.shape[1]))\n        crystals = np.concatenate([crystals, zero_padding], axis=1)\n    Q = crystals\n    K = crystals\n    V = crystals\n    attn_scores = np.dot(Q, K.T) / np.sqrt(dimension)\n    attn_weights = softmax(attn_scores)\n    final_pattern = np.dot(attn_weights, V)\n    return [float(round(num, 4)) for num in final_pattern.flatten().tolist()]"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input lists must be of the same length.')\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    days = list(range(1, 11))\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(10)]\n    n = len(days)\n    sum_x = sum(days)\n    sum_y = sum(detrended_data)\n    sum_x2 = sum([x ** 2 for x in days])\n    sum_xy = sum([x * y for (x, y) in zip(days, detrended_data)])\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n    day_15_base = m * 15 + b\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    day_15_total = day_15_base + day_15_fluctuation\n    safety_margin = math.ceil(day_15_total * 0.05)\n    final_forecast = math.ceil(day_15_total + safety_margin)\n    return final_forecast\nconsumption_data = [500 + i + 10 * math.sin(2 * PI * i / 10) for i in range(1, 11)]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred must be the same.')\n    return np.mean(np.abs(y_true - y_pred)).round(3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute queries, keys, and values.\n    \n    Parameters:\n    - X (np.ndarray): Input data of shape (n_tokens, d_model)\n    - W_q, W_k, W_v (np.ndarray): Linear projection weights for queries, keys, values\n    \n    Returns:\n    - q, k, v (np.ndarray): Query, Key, Value matrices\n    \"\"\"\n    q = X @ W_q\n    k = X @ W_k\n    v = X @ W_v\n    return (q, k, v)\ndef self_attention(q: np.ndarray, k: np.ndarray, v: np.ndarray):\n    \"\"\"\n    Compute attention mechanism for one head.\n    \n    Parameters:\n    - q, k, v: Query, Key, Value matrices\n    \n    Returns:\n    - context (np.ndarray): Final output of self-attention mechanism\n    \"\"\"\n    d_k = k.shape[1]\n    scores = q @ k.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    context = attention_weights @ v\n    return context\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Perform multi-head attention.\n    \n    Parameters:\n    - X (np.ndarray): Input data of shape (n_tokens, d_model)\n    - W_q, W_k, W_v (np.ndarray): Weights for queries, keys, values of shape (d_model, d_model)\n    - n_heads (int): Number of attention heads\n    \n    Returns:\n    - list: Context vectors after multi-head attention with rounding to 4 decimal places\n    \"\"\"\n    (n_tokens, d_model) = X.shape\n    d_head = d_model // n_heads\n    W_q_heads = np.split(W_q, n_heads, axis=0)\n    W_k_heads = np.split(W_k, n_heads, axis=0)\n    W_v_heads = np.split(W_v, n_heads, axis=0)\n    outputs = []\n    for i in range(n_heads):\n        (q_head, k_head, v_head) = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        ctx = self_attention(q_head, k_head, v_head)\n        outputs.append(ctx)\n    outputs = np.concatenate(outputs, axis=1)\n    outputs = np.round(outputs, 4)\n    return outputs.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "from typing import List\nfrom collections import Counter\nimport math\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    pairs = list(zip(x, y))\n    counter = Counter(pairs)\n    a = counter[1, 1]\n    b = counter[1, 0]\n    c = counter[0, 1]\n    d = counter[0, 0]\n    n11 = a * d\n    n00 = b * c\n    denom = (a + b) * (a + c) * (b + d) * (c + d)\n    if denom == 0:\n        return 0.0\n    phi = (n11 - n00) / math.sqrt(denom)\n    return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    return max(0.0, min(1.0, 0.2 * x + 0.5))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1.0 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos_pi_old_pi_ref = np.array(pi_theta_old) / np.array(pi_theta_ref)\n    numerator = np.array(rhos) * np.array(rhos_pi_old_pi_ref)\n    denominator = 1 + epsilon * np.array(rhos_pi_old_pi_ref)\n    clipped_group_ratio = np.clip(numerator / denominator, 1 - epsilon, 1 + epsilon)\n    policy_gradient_term = np.sum(clipped_group_ratio * np.array(A))\n    kl_divergence = np.sum(np.array(pi_theta_ref) - np.array(pi_theta_old) - np.log(np.array(pi_theta_ref) / np.array(pi_theta_old)))\n    objective_value = policy_gradient_term - beta * kl_divergence\n    return round(objective_value, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value (rounded to the nearest 4th decimal place)\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (np.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Args:\n        X: Input feature matrix (N x M)\n        y: Output labels (N)\n        learning_rate: Learning rate for gradient descent.\n        iterations: Number of iterations to run.\n\n    Returns:\n        B: list[float], C x M updated parameter vector rounded to 4 floating points\n        losses: list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (N, M) = X.shape\n    C = len(np.unique(y))\n    Y_one_hot = np.zeros((N, C))\n    Y_one_hot[np.arange(N), y] = 1\n    B = np.random.randn(C, M) * 0.01\n    losses = []\n    for _ in range(iterations):\n        scores = X @ B.T\n        exp_scores = np.exp(scores)\n        probabilities = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n        loss = -np.sum(Y_one_hot * np.log(probabilities + 1e-10)) / N\n        losses.append(round(loss, 4))\n        grad_B = (probabilities - Y_one_hot).T @ X\n        grad_B /= N\n        B -= learning_rate * grad_B\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = np.round(losses, 4).tolist()\n    return (B_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n        X (np.ndarray): Input feature matrix of shape (n_samples, n_features)\n        y (np.ndarray): Target labels of shape (n_samples,)\n        learning_rate (float): Learning rate for gradient descent\n        iterations (int): Number of iterations to perform\n    Returns:\n        tuple[list[float], list[float]]: \n            - Optimized coefficients (weights and bias)\n            - List of loss values per iteration\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    losses = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    coefficients = np.round(np.concatenate([weights, [bias]]), 4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return (coefficients, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    exp_scores = np.exp(scores)\n    masked_exp = np.where(mask == 0, 0, exp_scores)\n    softmax_scores = masked_exp / masked_exp.sum(axis=-1, keepdims=True)\n    output = np.dot(softmax_scores, V)\n    return output"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nimport math\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors\n    using Shannon Entropy.\n    \"\"\"\n    n = len(apples)\n    if n == 0:\n        return 0.0\n    counts = Counter(apples)\n    probabilities = [count / n for count in counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities if p > 0))\n    max_entropy = math.log2(n) if n > 0 else 0\n    if max_entropy == 0:\n        return 0.0\n    normalized_disorder = entropy / max_entropy\n    return round(normalized_disorder + 1e-08, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, sequence_length, feature_dim)\n    - gamma (np.ndarray): Scale parameters of shape (feature_dim,)\n    - beta (np.ndarray): Shift parameters of shape (feature_dim,)\n    - epsilon (float): A small value to prevent division by zero\n\n    Returns:\n    - (list): The normalized tensor, rounded to 5 decimals and converted to a list\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = X_normalized * gamma + beta\n    return np.round(X_scaled, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter, defaultdict\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.split()\n    candidate = candidate.split()\n    unigrams_in_candidate = set(candidate)\n    unigrams_in_reference = set(reference)\n    match_count = len(unigrams_in_candidate & unigrams_in_reference)\n    precision = match_count / len(candidate) if candidate else 0\n    recall = match_count / len(reference) if reference else 0\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n\n    def get_positions(sentence):\n        return {token: index for (index, token) in enumerate(sentence)}\n    ref_positions = get_positions(reference)\n    can_positions = get_positions(candidate)\n    common_unigrams = list(set(reference) & set(candidate))\n    idxs = [(ref_positions[wp], can_positions[wp]) for wp in common_unigrams]\n    if match_count <= 1:\n        fragmentation_penalty = 0\n    else:\n        total = 0\n        for i in range(match_count):\n            for j in range(i + 1, match_count):\n                total += abs(idxs[i][1] - idxs[j][1]) - abs(idxs[i][0] - idxs[j][0])\n        n = match_count * (match_count - 1) / 2\n        avg_log_ratio = total / n if n != 0 else 0\n        fragmentation_penalty = 1 - np.exp(-gamma * avg_log_ratio)\n    score = f_mean * (1 - fragmentation_penalty)\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    return [round((val - min_val) / (max_val - min_val), 4) for val in x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n\n    def relu(z):\n        return np.maximum(0, z)\n    z1 = np.dot(x, w1)\n    a1 = relu(z1)\n    z2 = np.dot(a1, w2)\n    output = z2 + x\n    output = relu(output)\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Performs Global Average Pooling on a 3D input array.\n\n    Parameters:\n    x (np.ndarray): A 3D array with shape (height, width, channels)\n\n    Returns:\n    np.ndarray: A 1D array with shape (channels,), where each element is\n                the average of the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array (height, width, channels).')\n    avg = np.mean(x, axis=(0, 1))\n    return avg"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D input with shape (Batch, Channels, Height, Width).\n\n    Parameters:\n        X (np.ndarray): Input tensor of shape (B, C, H, W)\n        gamma (np.ndarray): Scale parameters of shape (C,)\n        beta (np.ndarray): Shift parameters of shape (C,)\n        epsilon (float): Small value for numerical stability\n\n    Returns:\n        np.ndarray: Normalized output tensor of shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    X_normalized = (X - mean.reshape(1, C, 1, 1)) / np.sqrt(var.reshape(1, C, 1, 1) + epsilon)\n    X_hat = gamma.reshape(1, C, 1, 1) * X_normalized + beta.reshape(1, C, 1, 1)\n    return np.round(X_hat, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Compute the derivative of the polynomial term c * x^n at a given point x.\n    \n    Parameters:\n        c (float): The coefficient of the term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent of the term.\n    \n    Returns:\n        float: The derivative evaluated at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    V = np.array(vectors)\n    (n, d) = V.shape\n    B = []\n    for v in V:\n        u = v.copy()\n        for b in B:\n            proj = np.dot(v, b) / np.dot(b, b)\n            u -= proj * b\n        if np.linalg.norm(u) > tol:\n            u_normalized = u / np.linalg.norm(u)\n            B.append(u_normalized)\n    return [np.round(b, 4).tolist() for b in B]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n        A (np.ndarray): A square coefficient matrix (n x n).\n        b (np.ndarray): A constant vector (n x 1).\n\n    Returns:\n        list: A solution vector x (rounded to 4 decimal places), or -1 if A is singular.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b).flatten()\n    n = len(A)\n    if A.shape[0] != A.shape[1]:\n        return -1\n    if A.shape[0] != b.shape[0]:\n        return -1\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc_coefficient = np.sum(np.sqrt(p * q))\n    if bc_coefficient <= 0:\n        return 0.0\n    distance = -np.log(bc_coefficient)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [a[i] + b[i] for i in range(len(a))]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    total_gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = []\n        return_so_far = 0.0\n        for (state, action, reward) in reversed(episode):\n            return_so_far = reward + return_so_far\n            returns.append((state, action, return_so_far))\n        returns.reverse()\n        for (state, action, return_value) in returns:\n            logits = theta[state, :]\n            exp_logits = np.exp(logits)\n            pi_probs = exp_logits / np.sum(exp_logits)\n            log_pi = np.log(pi_probs[action])\n            grad_log_pi = np.zeros_like(theta)\n            grad_log_pi[state, action] = 1.0 - pi_probs[action]\n            grad_log_pi[state, :] -= pi_probs\n            total_gradient += grad_log_pi * return_value\n    avg_gradient = total_gradient / sum((len(episode) for episode in episodes))\n    result = np.round(avg_gradient, 4).tolist()\n    return result"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active * 2 * d_in * d_out\n    efficiency_gain = 100 * (1 - flops_moe / flops_dense)\n    return round(efficiency_gain, 1)"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts.\n\n    Parameters:\n    - X: Input matrix of shape (batch_size, input_dim)\n    - W_g: Gating weight matrix of shape (num_experts, input_dim)\n    - W_noise: Noise weight matrix of shape (num_experts, input_dim)\n    - N: Pre-sampled noise matrix of shape (batch_size, num_experts)\n    - k: Sparsity constraint (number of top experts to assign per example)\n\n    Returns:\n    - List of final gating probability matrices of shape (batch_size, num_experts)\n    \"\"\"\n    (batch_size, num_experts) = (X.shape[0], W_g.shape[0])\n    raw_gates = np.dot(X, W_g.T)\n    noise_gates = np.dot(X, W_noise.T)\n    noised_raw_gates = raw_gates + noise_gates * N\n    exp_gates = np.exp(noised_raw_gates - np.max(noised_raw_gates, axis=1, keepdims=True))\n    prob_gates = exp_gates / np.sum(exp_gates, axis=1, keepdims=True)\n    topk_indices = np.argpartition(prob_gates, -k, axis=1)[:, -k:]\n    mask = np.zeros_like(prob_gates)\n    for i in range(batch_size):\n        mask[i][topk_indices[i]] = 1\n    topk_probs = prob_gates * mask\n    topk_probs /= np.sum(topk_probs, axis=1, keepdims=True) + 1e-30\n    topk_probs = np.round(topk_probs, 4)\n    return topk_probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int) -> list:\n    \"\"\"\n    Computes the output of a Mixture of Experts layer with top-k routing.\n\n    Parameters:\n    - x (np.ndarray): Input tensor of shape (batch_size, input_dim)\n    - We (np.ndarray): Expert weights of shape (n_experts, input_dim, output_dim)\n    - Wg (np.ndarray): Gating weights of shape (input_dim, n_experts)\n    - n_experts (int): Number of experts\n    - top_k (int): Number of top experts to use per input token\n\n    Returns:\n    - list: Output after MoE computation, rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[-1]\n    gate_logits = x @ Wg\n    gate_probs = np.exp(gate_logits - np.max(gate_logits, axis=1, keepdims=True))\n    gate_probs /= np.sum(gate_probs, axis=1, keepdims=True)\n    final_output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        top_k_indices = np.argsort(gate_probs[i])[-top_k:]\n        top_k_weights = gate_probs[i, top_k_indices]\n        top_k_weights /= np.sum(top_k_weights)\n        expert_outputs = np.array([x[i] @ We[expert_idx] for expert_idx in top_k_indices])\n        weighted_output = (expert_outputs.T @ top_k_weights).T\n        final_output[i] = weighted_output\n    return np.round(final_output, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups'\n    X_grouped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    output = gamma * X_normalized + beta\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\ngamma = np.random.randn(4)\nbeta = np.random.randn(4)"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef df(x):\n    \"\"\"Derivative of the function f(x)\"\"\"\n    return 4 * x ** 3 - 9 * x ** 2\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        x_new = x - learning_rate * gradient\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh activation function for normalization-free Transformers.\n    Replaces layer normalization with a dynamic scaling.\n    \n    Parameters:\n    - x: Input numpy array\n    - alpha: Scaling factor (should be > 0)\n    - gamma: Dynamic scaling factor (should be > 0)\n    - beta: Dynamic shifting factor\n    \n    Returns:\n    - Output as a Python list of floats, rounded and reshaped.\n    \"\"\"\n    x_scaled = gamma * x + beta\n    output = np.tanh(alpha * x_scaled)\n    rounded_output = np.round(output, 4)\n    return rounded_output.tolist()"}
