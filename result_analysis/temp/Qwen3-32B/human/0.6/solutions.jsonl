{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if len(a) == 0 or len(b) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n        a (list[list[int | float]]): A 2D list representing the matrix.\n    \n    Returns:\n        list[list[int | float]]: The transposed matrix.\n    \"\"\"\n    return [[row[i] for row in a] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    arr = np.array(a)\n    if arr.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped = arr.reshape(new_shape)\n    return reshaped.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_rows = len(matrix)\n        num_cols = len(matrix[0]) if num_rows > 0 else 0\n        return [sum((matrix[row][col] for row in range(num_rows))) / num_rows for col in range(num_cols)]\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Parameters:\n        matrix (list of list of int/float): The input matrix.\n        scalar (int/float): The scalar to multiply the matrix by.\n    \n    Returns:\n        list of list of int/float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "def calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    import math\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigenvalue1 = (trace + math.sqrt(discriminant)) / 2\n    eigenvalue2 = (trace - math.sqrt(discriminant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        if np.isclose(np.linalg.det(T_np), 0) or np.isclose(np.linalg.det(S_np), 0):\n            return -1\n        result = np.linalg.inv(T_np) @ A_np @ S_np\n        result = np.round(result, 4).tolist()\n        return result\n    except Exception as e:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 0], [0, 2]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inv"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a list of feature vectors.\n\n    Parameters:\n    - vectors: A list of lists, where each inner list is a feature vector.\n\n    Returns:\n    - A covariance matrix as a list of lists.\n    \"\"\"\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    if not all((len(vec) == n_observations for vec in vectors)):\n        raise ValueError('All feature vectors must have the same number of observations.')\n    means = [sum(feature) / n_observations for feature in vectors]\n    centered_vectors = [[vectors[i][j] - means[i] for i in range(n_features)] for j in range(n_observations)]\n    covariance_matrix = [[0.0 for _ in range(n_features)] for _ in range(n_features)]\n    for i in range(n_features):\n        for j in range(n_features):\n            covariance = sum((centered_vectors[k][i] * centered_vectors[k][j] for k in range(n_observations))) / (n_observations - 1)\n            covariance_matrix[i][j] = covariance\n    return covariance_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix (n x n)\n    b (np.ndarray): Right-hand side vector (n x 1)\n    n (int): Number of iterations\n    \n    Returns:\n    list: Approximate solution vector x after n iterations, rounded to 4 decimals.\n    \"\"\"\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    assert A.shape == (2, 2), 'Input matrix must be 2x2'\n    A_T_A = A.T @ A\n\n    def jacobi_2x2(matrix, tolerance=1e-10, max_iter=1000):\n        M = matrix.copy()\n        for _ in range(max_iter):\n            if abs(M[1, 0]) < tolerance:\n                break\n            theta = 0.5 * np.arctan2(2 * M[1, 0], M[0, 0] - M[1, 1])\n            c = np.cos(theta)\n            s = np.sin(theta)\n            R = np.array([[c, s], [-s, c]])\n            M = R.T @ M @ R\n        return M.diagonal()\n    eigenvalues = jacobi_2x2(A_T_A)\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = np.sort(singular_values)[::-1]\n    return tuple((round(val, 4) for val in singular_values))\nA = np.array([[3, 4], [1, 2]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        return mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) - mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) + mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0])\n\n    def determinant_2x2(mat: list[list[int | float]]) -> float:\n        return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]\n\n    def get_minor_matrix(mat: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        return [[mat[i][j] for j in range(len(mat)) if j != col] for i in range(len(mat)) if i != row]\n    det = 0.0\n    for col in range(4):\n        minor = get_minor_matrix(matrix, 0, col)\n        if col % 2 == 1:\n            sign = -1\n        else:\n            sign = 1\n        if len(minor) == 3:\n            det += sign * matrix[0][col] * determinant_3x3(minor)\n        elif len(minor) == 2:\n            det += sign * matrix[0][col] * determinant_2x2(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_np\n    theta_rounded = [round(float(val), 4) for val in theta]\n    return theta_rounded\nX = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]\ny = [3.0, 5.0, 7.0, 9.0]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = y.shape[0]\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        h = X @ theta\n        error = h - y\n        gradient = 1 / m * X.T @ error\n        theta -= alpha * gradient\n    return np.round(theta, decimals=4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4)\n    data_min = np.min(data, axis=0)\n    data_max = np.max(data, axis=0)\n    min_max_data = (data - data_min) / (data_max - data_min)\n    min_max_data = np.round(min_max_data, 4)\n    standardized_list = standardized_data.tolist()\n    min_max_list = min_max_data.tolist()\n    return (standardized_list, min_max_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\nfrom typing import List, Tuple\ndef k_means_clustering(points: List[Tuple[float, float]], k: int, initial_centroids: List[Tuple[float, float]], max_iterations: int) -> List[Tuple[float, float]]:\n    \"\"\"\n    Perform k-means clustering on a list of 2D points.\n\n    Parameters:\n    - points: List of 2D points as tuples (x, y)\n    - k: Number of clusters\n    - initial_centroids: List of initial centroid coordinates (x, y)\n    - max_iterations: Maximum number of iterations to perform\n\n    Returns:\n    - List of final centroids after convergence or max iterations, rounded to 4 decimals\n    \"\"\"\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    n_points = len(points_array)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        cluster_assignments = np.argmin(distances, axis=1)\n        new_centroids = np.array([points_array[cluster_assignments == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 1.0), (9.0, 9.0)]\nmax_iterations = 100"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test indices for K-Fold Cross-Validation.\n\n    Parameters:\n    - X (np.ndarray): Input feature array of shape (n_samples, n_features)\n    - y (np.ndarray): Target array of shape (n_samples,)\n    - k (int): Number of folds\n    - shuffle (bool): Whether to shuffle the data before splitting\n    - random_seed (int): Random seed for reproducibility\n\n    Returns:\n    - List of tuples: Each tuple contains (train_indices, test_indices)\n    \"\"\"\n    n_samples = X.shape[0]\n    if n_samples % k != 0:\n        raise ValueError('Number of samples must be divisible by k for even splits.')\n    indices = np.arange(n_samples)\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    folds = np.array_split(indices, k)\n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate([folds[j] for j in range(k) if j != i])\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k].real\n    principal_components = [list(map(lambda x: round(x, 4), row)) for row in principal_components]\n    return principal_components"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\nfrom typing import Dict, List, Any, Tuple\nimport pprint\ndef entropy(examples: List[Dict[str, Any]], target_attr: str) -> float:\n    \"\"\"Calculate the entropy of a set of examples with respect to the target attribute.\"\"\"\n    counts = Counter((example[target_attr] for example in examples))\n    n = len(examples)\n    return -sum((count / n * math.log2(count / n) for count in counts.values()))\ndef information_gain(parent_entropy: float, subsets: List[Tuple[List[Dict[str, Any]], float]]) -> float:\n    \"\"\"Calculate the information gain from splitting on an attribute.\"\"\"\n    total = sum((len(subset) for (subset, _) in subsets))\n    return parent_entropy - sum((len(subset) / total * child_entropy for (subset, child_entropy) in subsets))\ndef split_on_attribute(examples: List[Dict[str, Any]], attribute: str) -> Dict[Any, List[Dict[str, Any]]]:\n    \"\"\"Split examples based on the values of a given attribute.\"\"\"\n    splits = {}\n    for example in examples:\n        value = example[attribute]\n        if value not in splits:\n            splits[value] = []\n        splits[value].append(example)\n    return splits\ndef majority_class(examples: List[Dict[str, Any]], target_attr: str) -> Any:\n    \"\"\"Return the majority class of the examples.\"\"\"\n    return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\ndef learn_decision_tree(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Recursively build a decision tree using entropy and information gain.\n    \n    Args:\n        examples: List of examples, each is a dictionary of attribute-value pairs.\n        attributes: List of attribute names to consider for splitting.\n        target_attr: Name of the target/class attribute.\n    \n    Returns:\n        A nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return majority_class(examples, target_attr)\n    classes = set((example[target_attr] for example in examples))\n    if len(classes) == 1:\n        return classes.pop()\n    if not attributes:\n        return majority_class(examples, target_attr)\n    best_attr = None\n    best_gain = -float('inf')\n    parent_entropy = entropy(examples, target_attr)\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n        splits = split_on_attribute(examples, attr)\n        child_entropies = [(subset, entropy(subset, target_attr)) for subset in splits.values()]\n        gain = information_gain(parent_entropy, child_entropies)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attr\n    if best_gain <= 0:\n        return majority_class(examples, target_attr)\n    tree = {best_attr: {}}\n    remaining_attributes = [a for a in attributes if a != best_attr]\n    for (value, subset) in split_on_attribute(examples, best_attr).items():\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree\nexamples = [{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Temperature': 'Mild', 'Humidity': 'Normal', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'Normal', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'}]\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\ntarget_attr = 'PlayTennis'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        eta_t = 1 / (lambda_val * t)\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            if kernel == 'linear':\n                K[i, :] = np.dot(data[i, :], data.T)\n            elif kernel == 'rbf':\n                K[i, :] = np.exp(-np.sum((data[i, :] - data) ** 2, axis=1) / (2 * sigma ** 2))\n            else:\n                raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n        decision_values = np.dot(K, alphas * labels) + b\n        gradient = np.zeros(n_samples)\n        for i in range(n_samples):\n            if labels[i] * decision_values[i] < 1:\n                gradient[i] = -labels[i] * K[i, :].dot(alphas * labels) - b * labels[i]\n            else:\n                gradient[i] = 0\n        alphas -= eta_t * gradient\n        alphas = np.clip(alphas, a_min=0, a_max=None)\n        alphas = (1 - eta_t * lambda_val) * alphas\n        for i in range(n_samples):\n            if labels[i] * decision_values[i] < 1:\n                b += eta_t * labels[i]\n    alphas_rounded = np.round(alphas, 4).tolist()\n    bias_rounded = round(b, 4)\n    return (alphas_rounded, bias_rounded)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of the input z.\n    \n    Parameters:\n    z (float): The input value.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp = sum(exp_scores)\n    softmax_values = [round(exp / sum_exp, 4) for exp in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features: List of feature vectors (2D list).\n    - labels: True binary labels (0 or 1).\n    - weights: List of weights for each feature.\n    - bias: Bias term for the neuron.\n    \n    Returns:\n    - predicted_probabilities: Predicted probabilities after sigmoid activation (as a list).\n    - mean_squared_error: Mean squared error between predicted and true labels (rounded to 4 decimals).\n    \"\"\"\n    predicted_probabilities = []\n    weights = np.array(weights)\n    features = np.array(features)\n    bias = np.array([bias])\n    z = np.dot(features, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predicted_probabilities = np.round(probabilities, 4).tolist()\n    labels = np.array(labels)\n    mse = np.mean((probabilities - labels) ** 2)\n    mean_squared_error = round(float(mse), 4)\n    return (predicted_probabilities, mean_squared_error)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = 1 / (1 + np.exp(-linear_output))\n        mse = np.mean((predictions - labels) ** 2)\n        mse_list.append(round(mse, 4))\n        error = predictions - labels\n        d_weights = np.dot(features.T, error * predictions * (1 - predictions)) / len(labels)\n        d_bias = np.mean(error * predictions * (1 - predictions))\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n    updated_weights = np.round(weights, 4).tolist()\n    updated_bias = round(bias, 4)\n    return (updated_weights, updated_bias, mse_list)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='+')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + -other\n\n    def __rsub__(self, other):\n        return other + -self\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** (-1)\n\n    def __rtruediv__(self, other):\n        return other * self ** (-1)\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), 'Only int/float exponents are allowed'\n        out = Value(self.data ** other, _children=(self,), _op=f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * self.data ** (other - 1)\n        out._backward = _backward\n        return out\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    - B: List of basis vectors (3x3 matrix in list form) representing basis B.\n    - C: List of basis vectors (3x3 matrix in list form) representing basis C.\n    \n    Returns:\n    - P: Transformation matrix from B to C as a 3x3 list of lists, rounded to 4 decimals.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    C_inv = np.linalg.inv(C_array)\n    P = np.dot(C_inv, B_array)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using eigendecomposition of A^T A.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 input matrix.\n    \n    Returns:\n    tuple: A tuple of (U, S, V) where:\n        - U is a 2x2 orthogonal matrix,\n        - S is a diagonal matrix of singular values,\n        - V is a 2x2 orthogonal matrix.\n    \"\"\"\n    AtA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(AtA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    V = eigenvectors\n    sigma = np.sqrt(eigenvalues)\n    S = np.diag(sigma)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if sigma[i] > 1e-10:\n            U[:, i] = A @ V[:, i] / sigma[i]\n        else:\n            U[:, i] = np.zeros(2)\n    (U, _) = np.linalg.qr(U)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles two numpy arrays X and y in the same random order.\n    \n    Parameters:\n    - X: numpy array of features\n    - y: numpy array of labels\n    - seed: optional integer for random seed (for reproducibility)\n    \n    Returns:\n    - A list of two lists: [X_shuffled.tolist(), y_shuffled.tolist()]\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    indices = np.random.permutation(len(X))\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return [X_shuffled.tolist(), y_shuffled.tolist()]"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from the input arrays X and y.\n    \n    Parameters:\n        X (np.ndarray): Input data of shape (n_samples, ...).\n        y (np.ndarray, optional): Labels of shape (n_samples, ...).\n        batch_size (int): Number of samples per batch.\n    \n    Yields:\n        A batch of data. If y is provided, yields (X_batch, y_batch); otherwise, yields X_batch.\n    \"\"\"\n    n_samples = X.shape[0]\n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        X_batch = X[start_idx:end_idx]\n        if y is not None:\n            y_batch = y[start_idx:end_idx]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset based on whether the value of a specified feature\n    is greater than or equal to a given threshold.\n    \n    Parameters:\n    - X: numpy array, the dataset with shape (n_samples, n_features)\n    - feature_i: int, index of the feature to use for splitting\n    - threshold: float, the threshold to compare the feature values against\n    \n    Returns:\n    - left: list, subset of samples where the feature value is >= threshold\n    - right: list, subset of samples where the feature value is < threshold\n    \"\"\"\n    left_mask = X[:, feature_i] >= threshold\n    right_mask = X[:, feature_i] < threshold\n    left = X[left_mask].tolist()\n    right = X[right_mask].tolist()\n    return (left, right)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    poly = []\n    for d in range(1, degree + 1):\n        for indices in combinations_with_replacement(range(n_features), d):\n            poly.append(np.prod(X[:, indices], axis=1))\n    return np.column_stack(poly).tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - y: 1D numpy array of shape (n_samples,)\n    - n_subsets: Number of random subsets to generate\n    - replacements: If True, samples with replacement; otherwise without replacement\n    - seed: Seed for random number generator for reproducibility\n\n    Returns:\n    - List of tuples, where each tuple contains (X_subset, y_subset)\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    return [(X_sub.tolist(), y_sub.tolist()) for (X_sub, y_sub) in subsets]"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n\n    Parameters:\n    x (np.ndarray): 1D array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If not provided, it is inferred from the maximum value in x.\n\n    Returns:\n    list: One-hot encoded array as a Python list.\n    \"\"\"\n    x = np.array(x)\n    if n_col is None:\n        n_col = np.max(x) + 1\n    n_row = x.shape[0]\n    one_hot = np.zeros((n_row, n_col))\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n\n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n\n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    x = np.atleast_1d(x)\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('Input arrays y_true and y_pred must be of the same length.')\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for the given dataset X and optional dataset Y.\n    \n    If Y is not provided, the correlation matrix of X with itself is computed.\n    \n    Parameters:\n    - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    - Y (np.ndarray, optional): A 2D numpy array of shape (n_samples, m_features).\n    \n    Returns:\n    - list: Correlation matrix as a 2D list (rounded to 4 decimal places).\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    std_X = np.std(X, axis=0, ddof=0)\n    std_Y = np.std(Y, axis=0, ddof=0)\n    std_X[std_X == 0] = 1.0\n    std_Y[std_Y == 0] = 1.0\n    numerator = np.dot(X_centered.T, Y_centered)\n    denominator = np.outer(std_X, std_Y)\n    correlation_matrix = numerator / denominator\n    return np.round(correlation_matrix, 4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_threshold = None\n        best_feature = None\n        best_direction = None\n        best_predictions = None\n        for feature in range(n_features):\n            unique_values = np.unique(X[:, feature])\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n            for threshold in thresholds:\n                for direction in [0, 1]:\n                    predictions = np.where(X[:, feature] <= threshold if direction == 0 else X[:, feature] > threshold, 1, -1)\n                    error = np.sum(sample_weights[y != predictions])\n                    if error < best_error:\n                        best_error = error\n                        best_threshold = threshold\n                        best_feature = feature\n                        best_direction = direction\n                        best_predictions = predictions\n        epsilon = best_error\n        if epsilon == 0:\n            epsilon = 1e-10\n        alpha = 0.5 * np.log((1 - epsilon) / epsilon)\n        sample_weights *= np.exp(-alpha * y * best_predictions)\n        sample_weights /= np.sum(sample_weights)\n        classifiers.append({'feature': best_feature, 'threshold': round(best_threshold, 4), 'direction': best_direction, 'alpha': round(alpha, 4)})\n    return classifiers\nX = np.array([[1, 2], [2, 1], [3, 3], [4, 4]])\ny = np.array([1, 1, -1, -1])\nclassifiers = adaboost_fit(X, y, n_clf=2)"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    shifted_scores = scores - np.max(scores)\n    log_sum_exp = np.log(np.sum(np.exp(shifted_scores)))\n    log_softmax_values = shifted_scores - log_sum_exp\n    return np.round(log_softmax_values, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, size=(self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        X = self.layer_input\n        grad_W = np.dot(X.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0)\n        grad_input = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            updated_W = self.optimizer_W.update(self.W, grad_W)\n            updated_w0 = self.optimizer_w0.update(self.w0, grad_w0)\n            self.W = updated_W\n            self.w0 = updated_w0\n        return grad_input.round(4)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n    if output_h <= 0 or output_w <= 0:\n        raise ValueError('Invalid convolution parameters, output dimensions are not positive.')\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            region = padded_input[i * stride:i * stride + kernel_h, j * stride:j * stride + kernel_w]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss.\n    \n    Parameters:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n        w (np.ndarray): Coefficient vector of shape (n_features,)\n        y_true (np.ndarray): True labels of shape (n_samples,)\n        alpha (float): Regularization strength (lambda)\n\n    Returns:\n        float: Ridge loss rounded to the nearest 4th decimal\n    \"\"\"\n    n_samples = X.shape[0]\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    l2_penalty = alpha * np.sum(w ** 2)\n    loss = mse + l2_penalty\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function.\n\n    Parameters:\n    - z (float): The input value.\n    - alpha (float): The slope for negative inputs (default is 0.01).\n\n    Returns:\n    - float: The output after applying Leaky ReLU.\n    \"\"\"\n    return z if z > 0 else alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: Precision score.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using the specified method.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples,)\n    - weights: numpy array of shape (n_features,)\n    - learning_rate: float\n    - n_iterations: int\n    - batch_size: int\n    - method: str, one of 'batch', 'sgd', or 'mini-batch'\n    \n    Returns:\n    - weights: numpy array of shape (n_features,)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.array(weights).copy()\n    for _ in range(n_iterations):\n        if method == 'batch':\n            indices = np.arange(n_samples)\n        elif method == 'sgd':\n            indices = np.random.choice(n_samples, size=1, replace=False)\n        elif method == 'mini-batch':\n            indices = np.random.choice(n_samples, size=batch_size, replace=False)\n        else:\n            raise ValueError(\"Invalid method. Choose from 'batch', 'sgd', or 'mini-batch'.\")\n        X_batch = X[indices]\n        y_batch = y[indices]\n        gradient = 2 / len(indices) * X_batch.T @ (X_batch @ weights - y_batch)\n        weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n        matrix (list of lists or np.ndarray): The input matrix.\n        \n    Returns:\n        list: The RREF of the input matrix, returned as a Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            return matrix.tolist()\n        i = r\n        while i < rows and matrix[i, lead] == 0:\n            i += 1\n        if i == rows:\n            lead += 1\n            continue\n        matrix[[i, r]] = matrix[[r, i]]\n        pivot = matrix[r, lead]\n        matrix[r] = matrix[r] / pivot\n        for i in range(rows):\n            if i != r and matrix[i, lead] != 0:\n                matrix[i] = matrix[i] - matrix[i, lead] * matrix[r]\n        lead += 1\n    return matrix.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_val = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_val\n        v = beta2 * v + (1 - beta2) * grad_val ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 regularization.\n\n    Parameters:\n    - X: np.array, shape (n_samples, n_features)\n    - y: np.array, shape (n_samples,)\n    - alpha: float, regularization strength (L1 penalty)\n    - learning_rate: float, learning rate for gradient descent\n    - max_iter: int, maximum number of iterations\n    - tol: float, stopping tolerance\n\n    Returns:\n    - weights: list of weights (reshaped and rounded to 4 decimal places)\n    - bias: float (rounded to 4 decimal places)\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        dw = 1 / n_samples * np.dot(X.T, error) + alpha / n_samples * np.sign(weights)\n        db = 1 / n_samples * np.sum(error)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        if np.linalg.norm(dw) < tol:\n            break\n    weights = np.round(weights, 4).tolist()\n    bias = round(bias, 4)\n    return (weights, bias)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                substitution = dp[i - 1][j - 1] + 1\n                deletion = dp[i - 1][j] + 1\n                insertion = dp[i][j - 1] + 1\n                transposition = dp[i - 2][j - 2] + 1 if i >= 2 and j >= 2 and (source[i - 2] == target[j - 1]) and (source[i - 1] == target[j - 2]) else float('inf')\n                dp[i][j] = min(substitution, deletion, insertion, transposition)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true (list): List of true binary labels (0 or 1).\n    - y_pred (list): List of predicted binary labels (0 or 1).\n\n    Returns:\n    - float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    denominator = TP + FN\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(TP / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention output for a given input matrix X.\n    \n    Parameters:\n    X (np.ndarray): Input matrix of shape (seq_len, input_dim)\n    W_q (np.ndarray): Query weight matrix of shape (input_dim, d_k)\n    W_k (np.ndarray): Key weight matrix of shape (input_dim, d_k)\n    W_v (np.ndarray): Value weight matrix of shape (input_dim, d_v)\n    \n    Returns:\n    list: Self-attention output as a list (after rounding to 4 decimals)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = W_q.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), axis=1, arr=scores)\n    attention_output = attention_weights @ V\n    attention_output_rounded = np.round(attention_output, 4)\n    return attention_output_rounded.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Performs forward pass through a simple RNN cell.\n\n    Parameters:\n    - input_sequence: List of input vectors (each is a list of floats).\n    - initial_hidden_state: Initial hidden state vector.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n\n    Returns:\n    - Final hidden state as a list of floats (rounded to 4 decimals).\n    \"\"\"\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h_prev = np.tanh(np.dot(Wx, x) + np.dot(Wh, h_prev) + b)\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of [x, y] points using a translation matrix.\n    \n    Parameters:\n    - points: List of [x, y] coordinates\n    - tx: Translation distance in the x direction\n    - ty: Translation distance in the y direction\n    \n    Returns:\n    - A new list of translated points\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    homogeneous_points = np.hstack((points_array, ones))\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    result = translated_points[:, :2]\n    return result.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence from P to Q,\n    where P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2).\n    \n    Parameters:\n    - mu_p (float): mean of distribution P\n    - sigma_p (float): standard deviation of distribution P\n    - mu_q (float): mean of distribution Q\n    - sigma_q (float): standard deviation of distribution Q\n    \n    Returns:\n    - kl (float): KL divergence D_KL(P || Q)\n    \"\"\"\n    return np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(size):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n        A (np.ndarray): Coefficient matrix (n x n)\n        b (np.ndarray): Right-hand side vector (n)\n    \n    Returns:\n        list: Solution vector x, rounded to 4 decimal places\n    \"\"\"\n    n = len(b)\n    Ab = np.hstack([A, b.reshape(n, 1)])\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        if Ab[max_row, i] == 0:\n            raise ValueError('Matrix is singular or nearly singular.')\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] = Ab[j, i:] - factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in reversed(range(n)):\n        x[i] = (Ab[i, n] - np.dot(Ab[i, i:n], x[i:n])) / Ab[i, i]\n    return [round(val, 4) for val in x.tolist()]"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \n        Args:\n            x (np.ndarray): Input sequence of shape (input_size, seq_len)\n            initial_hidden_state (np.ndarray): Initial hidden state of shape (hidden_size, 1)\n            initial_cell_state (np.ndarray): Initial cell state of shape (hidden_size, 1)\n        \n        Returns:\n            tuple:\n                - hidden_states (list): List of hidden states for each time step\n                - final_hidden_state (np.ndarray): Final hidden state\n                - final_cell_state (np.ndarray): Final cell state\n        \"\"\"\n        seq_len = x.shape[1]\n        hidden_states = []\n        ht = initial_hidden_state\n        ct = initial_cell_state\n        for t in range(seq_len):\n            xt = x[:, t:t + 1]\n            concat = np.concatenate((xt, ht), axis=0)\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            cct = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ct = ft * ct + it * cct\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            ht = ot * np.tanh(ct)\n            hidden_states.append(np.round(ht, 4).flatten())\n        final_hidden_state = np.round(ht, 4)\n        final_cell_state = np.round(ct, 4)\n        hidden_states = [np.round(state, 4).tolist() for state in hidden_states]\n        return (hidden_states, final_hidden_state.tolist(), final_cell_state.tolist())"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty.')\n    vocabulary = set()\n    for doc in corpus:\n        vocabulary.update(doc)\n    vocabulary.update(query)\n    vocabulary = list(vocabulary)\n    vocab_size = len(vocabulary)\n    word_to_index = {word: i for (i, word) in enumerate(vocabulary)}\n    num_docs = len(corpus)\n    tf = np.zeros((num_docs, vocab_size))\n    df = np.zeros(vocab_size)\n    for (doc_idx, doc) in enumerate(corpus):\n        word_count = {}\n        for word in doc:\n            word_count[word] = word_count.get(word, 0) + 1\n        total_words = len(doc)\n        for (word, count) in word_count.items():\n            idx = word_to_index[word]\n            tf[doc_idx, idx] = count / total_words\n    for doc in corpus:\n        unique_words = set(doc)\n        for word in unique_words:\n            idx = word_to_index[word]\n            df[idx] += 1\n    idf = np.log((num_docs + 1) / (df + 1)) + 1\n    tf_idf = tf * idf\n    tf_idf_rounded = np.round(tf_idf, 4)\n    return tf_idf_rounded.tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f_score_value = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall) if precision + recall > 0 else 0\n    return round(f_score_value, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            \n        Returns:\n            outputs: Output of the RNN at each time step\n            hiddens: Hidden states at each time step\n            inputs: Inputs at each time step\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = np.zeros((self.hidden_size, 1))\n        outputs = []\n        hiddens = []\n        inputs = []\n        for t in range(sequence_length):\n            x_t = x[:, t].reshape(-1, 1)\n            inputs.append(x_t)\n            h = np.tanh(self.W_xh @ x_t + self.W_hh @ h + self.b_h)\n            hiddens.append(h)\n            y = self.W_hy @ h + self.b_y\n            outputs.append(y)\n        return (np.array(outputs), np.array(hiddens), np.array(inputs))\n\n    def backward(self, x, y, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            y: Expected output sequence of shape (output_size, sequence_length)\n            learning_rate: Learning rate for weight updates\n            \n        Returns:\n            loss: Total loss for the sequence\n        \"\"\"\n        sequence_length = x.shape[1]\n        (outputs, hiddens, inputs) = self.forward(x)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n        loss = 0\n        for t in reversed(range(sequence_length)):\n            dy = outputs[t] - y[:, t].reshape(-1, 1)\n            dW_hy += dy @ hiddens[t].T\n            db_y += dy\n            loss += 0.5 * np.sum(dy ** 2)\n            dy_h = self.W_hy.T @ dy + dh_next\n            dhraw = (1 - hiddens[t] ** 2) * dy_h\n            db_h += dhraw\n            dW_xh += dhraw @ inputs[t].T\n            dW_hh += dhraw @ hiddens[t - 1].T if t > 0 else dhraw @ np.zeros_like(hiddens[0])\n            dh_next = self.W_hh.T @ dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\n\n    @staticmethod\n    def initialize_weights(input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \n        Args:\n            input_size: Size of the input\n            hidden_size: Number of hidden units\n            output_size: Size of the output\n            \n        Returns:\n            W_xh: Input-to-hidden weights\n            W_hh: Hidden-to-hidden weights\n            W_hy: Hidden-to-output weights\n            b_h: Hidden biases\n            b_y: Output biases\n        \"\"\"\n        W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        b_h = np.zeros((hidden_size, 1))\n        b_y = np.zeros((output_size, 1))\n        return (W_xh, W_hh, W_hy, b_h, b_y)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rsold = np.dot(r, r)\n    for i in range(n):\n        if np.sqrt(rsold) < tol:\n            break\n        Ap = A @ p\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        beta = rsnew / rsold\n        p = r + beta * p\n        rsold = rsnew\n    return np.round(x, 8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nx = conjugate_gradient(A, b, n=100)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    counts = np.bincount(y)\n    total = len(y)\n    impurity = 1.0 - np.sum(((count / total) ** 2 for count in counts))\n    return round(impurity, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_ptr = [0]\n    for row in dense_matrix:\n        count = 0\n        for (col, val) in enumerate(row):\n            if val != 0:\n                values.append(val)\n                col_indices.append(col)\n                count += 1\n        row_ptr.append(row_ptr[-1] + count)\n    return (values, col_indices, row_ptr)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    dot_product = sum((x * y for (x, y) in zip(v, L)))\n    norm_L_squared = sum((x * x for x in L))\n    if norm_L_squared == 0:\n        raise ValueError('The line vector L cannot be a zero vector.')\n    scalar_projection = dot_product / norm_L_squared\n    projection = [scalar_projection * x for x in L]\n    return [round(x, 3) for x in projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Computes the basis for the image (column space) of matrix A.\n    \n    Parameters:\n    A (np.ndarray): The input matrix as a NumPy array.\n\n    Returns:\n    list: A list of basis vectors (rounded to 8 decimal places) that span the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    (_, pivots) = np.linalg.qr(A, mode='reduced')\n    independent_cols = np.argsort(np.abs(pivots), axis=0)[0, :]\n    image_basis = A[:, independent_cols].copy()\n    return [col.tolist() for col in image_basis.T]"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    mean_y_true = np.mean(y_true)\n    tss = np.sum((y_true - mean_y_true) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img:\n        return -1\n    row_length = len(img[0])\n    total_pixels = 0\n    total_brightness = 0\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if not isinstance(pixel, int) or not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = round(total_brightness / total_pixels, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input arrays are not of the same length or are empty.\n    TypeError: If inputs are not array-like or contain non-numeric values.\n    \"\"\"\n    try:\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except Exception:\n        raise TypeError('Both y_true and y_pred must be array-like and contain numeric values.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n    return round(root_mean_squared_error, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    Parameters:\n    y_true (array-like): True binary labels (0s and 1s).\n    y_pred (array-like): Predicted binary labels (0s and 1s).\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.logical_and(y_true, y_pred).sum()\n    union = np.logical_or(y_true, y_pred).sum()\n    if union == 0:\n        return 1.0\n    else:\n        return round(intersection / union, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) binary labels.\n    y_pred (array-like): Predicted binary labels.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Creates a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Each feature is represented by binding the hypervector for the feature name with the hypervector\n    for its value. All feature hypervectors are bundled to form the composite hypervector.\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row with feature names as keys.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary mapping feature names to random seeds.\n    \n    Returns:\n    - list: The composite hypervector as a list.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        seed = random_seeds.get(feature_name)\n        if seed is None:\n            raise ValueError(f'No seed provided for feature: {feature_name}')\n        np.random.seed(seed)\n        feature_hv = np.random.choice([-1, 1], size=dim)\n        value_hv = np.random.choice([-1, 1], size=dim)\n        bound_hv = feature_hv * value_hv\n        composite_hv += bound_hv\n    composite_hv = np.sign(composite_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for a binary classification problem.\n    \n    Parameters:\n        data (list of lists): Each inner list is [y_true, y_pred] for one observation.\n        \n    Returns:\n        list of lists: A 2x2 confusion matrix in the format:\n                       [[TN, FP],\n                        [FN, TP]]\n    \"\"\"\n    cm = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            cm[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            cm[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            cm[1][0] += 1\n        elif y_true == 1 and y_pred == 1:\n            cm[1][1] += 1\n    return cm"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1, v2: Numpy arrays representing the input vectors.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    tp = sum((a == 1 and p == 1 for (a, p) in zip(actual, predicted)))\n    fp = sum((a == 0 and p == 1 for (a, p) in zip(actual, predicted)))\n    tn = sum((a == 0 and p == 0 for (a, p) in zip(actual, predicted)))\n    fn = sum((a == 1 and p == 0 for (a, p) in zip(actual, predicted)))\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    total = len(actual)\n    accuracy = round((tp + tn) / total, 3) if total > 0 else 0.0\n    precision = round(tp / (tp + fp), 3) if tp + fp > 0 else 0.0\n    recall = round(tp / (tp + fn), 3) if tp + fn > 0 else 0.0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0.0\n    specificity = round(tn / (tn + fp), 3) if tn + fp > 0 else 0.0\n    npv = round(tn / (tn + fn), 3) if tn + fn > 0 else 0.0\n    return (confusion_matrix, accuracy, f1_score, specificity, npv)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n    \n    Parameters:\n        data (list or np.ndarray): A list or NumPy array of numerical values.\n        \n    Returns:\n        dict: A dictionary containing the computed statistics.\n    \"\"\"\n    data = np.array(data)\n    stats_dict = {}\n    stats_dict['mean'] = round(np.mean(data), 4)\n    stats_dict['median'] = round(np.median(data), 4)\n    mode_result = stats.mode(data, keepdims=True)\n    stats_dict['mode'] = mode_result.mode[0]\n    stats_dict['variance'] = round(np.var(data, ddof=1), 4)\n    stats_dict['standard_deviation'] = round(np.std(data, ddof=1), 4)\n    stats_dict['25th_percentile'] = round(np.percentile(data, 25), 4)\n    stats_dict['50th_percentile'] = round(np.percentile(data, 50), 4)\n    stats_dict['75th_percentile'] = round(np.percentile(data, 75), 4)\n    iqr = stats_dict['75th_percentile'] - stats_dict['25th_percentile']\n    stats_dict['interquartile_range'] = iqr\n    return stats_dict"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: Contrast value as the difference between max and min pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError('Input image must be a 2D array (grayscale).')\n    if img.dtype != np.uint8:\n        raise ValueError('Input image must have pixel values of type uint8 (0-255).')\n    max_val = np.max(img)\n    min_val = np.min(img)\n    return max_val - min_val"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        features = [x ** i for i in range(1, degree + 1)]\n        transformed_data.append([round(f, 8) for f in features])\n    return transformed_data"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    encoding = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            denominator = np.power(10000, 2 * (i // 2) / d_model)\n            encoding[pos, i] = np.sin(pos / denominator)\n            if i + 1 < d_model:\n                encoding[pos, i + 1] = np.cos(pos / denominator)\n    return encoding.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    context = token_ids.copy()\n    n_embd = params['wte'].shape[1]\n    n_vocab = params['wte'].shape[0]\n    block = {'attn': {'c_attn': {'w': np.random.rand(3 * n_embd, n_embd), 'b': np.zeros(3 * n_embd)}, 'c_proj': {'w': np.random.rand(n_embd, n_embd), 'b': np.zeros(n_embd)}}, 'ln_1': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'mlp': {'c_fc': {'w': np.random.rand(4 * n_embd, n_embd), 'b': np.zeros(4 * n_embd)}, 'c_proj': {'w': np.random.rand(n_embd, 4 * n_embd), 'b': np.zeros(n_embd)}}, 'ln_2': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}}\n    params['blocks'].append(block)\n\n    def softmax(x):\n        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_x / exp_x.sum(axis=-1, keepdims=True)\n\n    def layer_norm(x, g, b, eps=1e-05):\n        mean = x.mean(axis=-1, keepdims=True)\n        var = x.var(axis=-1, keepdims=True)\n        return g * (x - mean) / np.sqrt(var + eps) + b\n\n    def attention(q, k, v, n_head):\n        scores = np.matmul(q, k.T) / k.shape[-1] ** 0.5\n        weights = softmax(scores)\n        context = np.matmul(weights, v)\n        return context\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][context]\n        position_embeddings = params['wpe'][:len(context)]\n        x = token_embeddings + position_embeddings\n        for block in params['blocks']:\n            x_ln1 = layer_norm(x, block['ln_1']['g'], block['ln_1']['b'])\n            qkv = np.matmul(x_ln1, block['attn']['c_attn']['w']) + block['attn']['c_attn']['b']\n            n_embd = x.shape[-1]\n            (q, k, v) = np.split(qkv, 3, axis=-1)\n            n_head = hparams['n_head']\n            head_size = n_embd // n_head\n            q = q.reshape(q.shape[0], n_head, head_size)\n            k = k.reshape(k.shape[0], n_head, head_size)\n            v = v.reshape(v.shape[0], n_head, head_size)\n            q = q.transpose(1, 0, 2)\n            k = k.transpose(1, 0, 2)\n            v = v.transpose(1, 0, 2)\n            attn_out = []\n            for i in range(n_head):\n                head_out = attention(q[i], k[i], v[i], 1)\n                attn_out.append(head_out)\n            attn_out = np.concatenate(attn_out, axis=-1).transpose(1, 0, 2)\n            attn_proj = np.matmul(attn_out, block['attn']['c_proj']['w']) + block['attn']['c_proj']['b']\n            x = x + attn_proj\n            x_ln2 = layer_norm(x, block['ln_2']['g'], block['ln_2']['b'])\n            mlp_out = np.matmul(x_ln2, block['mlp']['c_fc']['w']) + block['mlp']['c_fc']['b']\n            mlp_out = np.maximum(0, mlp_out)\n            mlp_out = np.matmul(mlp_out, block['mlp']['c_proj']['w']) + block['mlp']['c_proj']['b']\n            x = x + mlp_out\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.matmul(x, params['wte'].T)\n        probs = softmax(logits[-1])\n        next_token = np.random.choice(n_vocab, p=probs)\n        context.append(next_token)\n    return encoder.decode(context)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum(axis=1, keepdims=True)\n    Q = np.array(crystal_values).reshape(n, dimension)\n    K = Q\n    V = Q\n    attention_scores = Q @ K.T\n    attention_weights = softmax(attention_scores)\n    final_pattern = attention_weights @ V\n    result = [round(val, 4) for val in final_pattern.flatten()]\n    return result"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    corpus_terms = [set(doc.split()) for doc in corpus]\n    query_terms = set(query.split())\n    doc_lengths = [len(doc.split()) for doc in corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    scores = []\n    for (doc_index, doc) in enumerate(corpus):\n        doc_terms = doc.split()\n        doc_term_count = Counter(doc_terms)\n        score = 0.0\n        for term in query_terms:\n            df = sum((1 for doc_terms_set in corpus_terms if term in doc_terms_set))\n            N = len(corpus)\n            idf = np.log((N - df + 0.5) / (df + 0.5) + 1)\n            tf = doc_term_count.get(term, 0)\n            doc_len = doc_lengths[doc_index]\n            numerator = tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_length))\n            score += idf * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    tp = sum((t == 1 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fp = sum((t == 0 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fn = sum((t == 1 and p == 0 for (t, p) in zip(y_true, y_pred)))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 0]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    n = len(detrended_data)\n    x = [i + 1 for i in range(n)]\n    y = detrended_data\n    x_mean = sum(x) / n\n    y_mean = sum(y) / n\n    numerator = sum(((x[i] - x_mean) * (y[i] - y_mean) for i in range(n)))\n    denominator = sum(((xi - x_mean) ** 2 for xi in x))\n    m = numerator / denominator\n    b = y_mean - m * x_mean\n    base_consumption_day_15 = m * 15 + b\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    safety_margin = 1.05\n    final_consumption = math.ceil(total_consumption_day_15 * safety_margin)\n    return final_consumption"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n\n    def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n        Q = X @ W_q\n        K = X @ W_k\n        V = X @ W_v\n        return (Q, K, V)\n\n    def self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n        d_k = K.shape[1]\n        scores = Q @ K.T / np.sqrt(d_k)\n        attention_weights = np.softmax(scores, axis=1)\n        output = attention_weights @ V\n        return output\n\n    def split_heads(matrix: np.ndarray, n_heads: int) -> np.ndarray:\n        (batch_size, seq_len, d_model) = matrix.shape\n        d_head = d_model // n_heads\n        reshaped = matrix.reshape(batch_size, seq_len, n_heads, d_head)\n        return reshaped.transpose(0, 2, 1, 3)\n\n    def combine_heads(matrix: np.ndarray) -> np.ndarray:\n        (batch_size, n_heads, seq_len, d_head) = matrix.shape\n        reshaped = matrix.transpose(0, 2, 1, 3)\n        return reshaped.reshape(batch_size, seq_len, n_heads * d_head)\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = split_heads(Q, n_heads)\n    K_heads = split_heads(K, n_heads)\n    V_heads = split_heads(V, n_heads)\n    head_outputs = []\n    for i in range(n_heads):\n        head_output = self_attention(Q_heads[0, i], K_heads[0, i], V_heads[0, i])\n        head_outputs.append(np.expand_dims(head_output, axis=0))\n    head_outputs = np.stack(head_outputs, axis=1)\n    final_output = combine_heads(head_outputs)\n    return np.round(final_output[0], 4).tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('Input lists must have the same length.')\n    n11 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 1))\n    n10 = sum((1 for (a, b) in zip(x, y) if a == 1 and b == 0))\n    n01 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 1))\n    n00 = sum((1 for (a, b) in zip(x, y) if a == 0 and b == 0))\n    n = len(x)\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 0, 1]\ny = [1, 1, 0, 1, 0, 1, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif -2.5 < x < 2.5:\n        return x / 5.0 + 0.5\n    else:\n        return 1.0"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return x if x > 0 else alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x < -30:\n        return round(math.exp(x), 4)\n    elif x > 30:\n        return round(x, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    kl_ref_old = np.sum(pi_theta_ref * np.log(pi_theta_ref / pi_theta_old))\n    surrogate = np.minimum(rhos * A, rhos_clipped * A)\n    objective = np.mean(surrogate) - beta * kl_ref_old\n    return round(objective, 6)\nrhos = [1.2, 0.95, 1.1]\nA = [2.5, -1.3, 0.8]\npi_theta_old = [0.3, 0.4, 0.3]\npi_theta_ref = [0.25, 0.45, 0.3]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value, rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (np.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    z = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    B = np.random.randn(n_features, n_classes) * 0.01\n    losses = []\n    X = np.c_[np.ones(n_samples), X]\n    n_features = X.shape[1]\n    y_one_hot = np.zeros((n_samples, n_classes))\n    y_one_hot[np.arange(n_samples), y] = 1\n    for _ in range(iterations):\n        logits = X @ B\n        exp_logits = np.exp(logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-15), axis=1))\n        losses.append(round(loss, 4))\n        grad = X.T @ (probs - y_one_hot) / n_samples\n        B -= learning_rate * grad\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_rounded, losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    - X: Input features (shape: [n_samples, n_features])\n    - y: True labels (shape: [n_samples])\n    - learning_rate: Step size for gradient descent\n    - iterations: Number of iterations to run gradient descent\n    \n    Returns:\n    - coefficients: Optimized parameters (weights and bias)\n    - losses: Loss values over each iteration\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    losses = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -(1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        dw = 1 / n_samples * np.dot(X.T, y_pred - y)\n        db = 1 / n_samples * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    coefficients = np.append(weights, bias)\n    coefficients = np.round(coefficients, 4).tolist()\n    losses = np.round(losses, 4).tolist()\n    return (coefficients, losses)\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nlearning_rate = 0.1\niterations = 100"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    scores = np.where(mask == 0, -1000000000.0, scores)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    counts = Counter(apples)\n    probabilities = [count / total for count in counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities if p > 0))\n    max_entropy = math.log2(len(counts))\n    if max_entropy == 0:\n        normalized_entropy = 0.0\n    else:\n        normalized_entropy = entropy / max_entropy\n    return round(normalized_entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n    \n    Args:\n        X (np.ndarray): Input tensor of shape (batch_size, sequence_length, features)\n        gamma (np.ndarray): Scale parameters, shape (features,)\n        beta (np.ndarray): Shift parameters, shape (features,)\n        epsilon (float): Small value to avoid division by zero\n    \n    Returns:\n        list: Normalized tensor as a list, rounded to 5 decimal places\n    \"\"\"\n    if X.shape[-1] != gamma.shape[0] or X.shape[-1] != beta.shape[0]:\n        raise ValueError('Last dimension of X must match the length of gamma and beta.')\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_scaled = X_normalized * gamma + beta\n    return np.round(X_scaled, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nfrom itertools import product\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.lower().split()\n    candidate = candidate.lower().split()\n    matches = set(reference) & set(candidate)\n    if not matches:\n        return 0.0\n    precision = len(matches) / len(candidate)\n    recall = len(matches) / len(reference)\n    f_mean = precision * recall / (alpha * precision + (1 - alpha) * recall)\n    f_mean = np.nan_to_num(f_mean)\n    length_ratio = min(len(candidate) / len(reference), 1)\n    length_penalty = 1 - np.exp(-gamma * length_ratio)\n    ref_matches = [i for (i, word) in enumerate(candidate) if word in matches]\n    if not ref_matches:\n        frag_penalty = 0.0\n    else:\n        fragments = 1\n        for i in range(1, len(ref_matches)):\n            if ref_matches[i] > ref_matches[i - 1] + 1:\n                fragments += 1\n        frag_penalty = 1 - np.exp(-beta * fragments / len(candidate))\n    score = (1 - length_penalty - frag_penalty) * f_mean\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        raise ValueError('Input list cannot be empty.')\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0 for _ in x]\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    out = np.maximum(0, np.dot(x, w1))\n    out = np.maximum(0, np.dot(out, w2))\n    out += x\n    out = np.maximum(0, out)\n    return np.round(out, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels)\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) with the average of each channel\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D array with shape (batch, channels, height, width).\n    \n    Parameters:\n        X (np.ndarray): Input data of shape (batch, channels, height, width)\n        gamma (np.ndarray): Scale parameter of shape (channels,)\n        beta (np.ndarray): Shift parameter of shape (channels,)\n        epsilon (float): Small value to avoid division by zero\n    \n    Returns:\n        np.ndarray: Normalized output rounded to 4 decimal places\n    \"\"\"\n    (batch, channels, height, width) = X.shape\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(channels, -1)\n    mean = np.mean(X_reshaped, axis=1)\n    var = np.var(X_reshaped, axis=1)\n    X_normalized = (X_reshaped - mean[:, np.newaxis]) / np.sqrt(var[:, np.newaxis] + epsilon)\n    X_normalized = X_normalized.reshape(channels, batch, height, width).transpose(1, 0, 2, 3)\n    output = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    return np.round(output, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term c * x^n at a given x.\n    Using the power rule: d/dx [c * x^n] = c * n * x^(n - 1)\n    \n    Args:\n        c (float): The coefficient of the term.\n        x (float): The point at which to evaluate the derivative.\n        n (float): The exponent of x.\n    \n    Returns:\n        float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors)\n    (n, d) = vectors.shape\n    if d != 2:\n        raise ValueError('All vectors must be 2-dimensional.')\n    basis = []\n    for v in vectors:\n        v = np.array(v, dtype=float)\n        for u in basis:\n            v = v - np.dot(v, u) * u\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            u = v / norm\n            basis.append(u)\n    basis = [np.round(vec, 4).tolist() for vec in basis]\n    return basis\nvectors = [[1, 1], [1, 2], [2, 3]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()\nvector_a = np.array([1, 2, 3])\nvector_b = np.array([4, 5, 6])\nresult = cross_product(vector_a, vector_b)"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (list of lists or numpy.ndarray): Square coefficient matrix.\n    b (list or numpy.ndarray): Constant vector (right-hand side).\n    \n    Returns:\n    list: Solution vector rounded to 4 decimal places.\n    int: -1 if the determinant of A is zero (no unique solution).\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    n = A.shape[0]\n    if A.shape[1] != n:\n        raise ValueError('Matrix A must be square.')\n    if len(b) != n:\n        raise ValueError('Vector b must have the same length as the size of matrix A.')\n    det_A = np.linalg.det(A)\n    if np.isclose(det_A, 0):\n        return -1\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return x"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    if np.any(p < 0) or np.any(q < 0) or abs(np.sum(p) - 1) > 1e-06 or (abs(np.sum(q) - 1) > 1e-06):\n        return 0.0\n    bc = np.sum(np.sqrt(p * q))\n    epsilon = 1e-10\n    bc = max(bc, epsilon)\n    distance = -np.log(bc)\n    return round(distance, 4)\np = [0.1, 0.4, 0.5]\nq = [0.2, 0.3, 0.5]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    total_timesteps = 0\n    for episode in episodes:\n        G = 0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            G += reward\n            exp_theta = np.exp(theta[state, :])\n            pi = exp_theta / exp_theta.sum()\n            log_pi = np.log(pi[action])\n            grad_log_pi = np.zeros(num_actions)\n            grad_log_pi[action] = 1 - pi[action]\n            grad_log_pi -= pi\n            gradient[state, :] += -grad_log_pi * G\n            total_timesteps += 1\n    if total_timesteps > 0:\n        gradient /= total_timesteps\n    return np.round(gradient, 4).tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n\n    Parameters:\n    - n_experts (int): Total number of experts.\n    - k_active (int): Number of active experts per input token.\n    - d_in (int): Input dimension.\n    - d_out (int): Output dimension.\n\n    Returns:\n    - A tuple (dense_flops, moe_flops, savings_percent) rounded to the nearest 1st decimal.\n    \"\"\"\n    dense_flops = 2 * d_in * d_out\n    moe_flops = k_active * (2 * d_in * d_out) / n_experts\n    savings_percent = (dense_flops - moe_flops) / dense_flops * 100\n    return (round(dense_flops, 1), round(moe_flops, 1), round(savings_percent, 1))"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Compute the noisy top-k gating probabilities.\n    \n    Parameters:\n    - X: Input data matrix of shape (batch_size, input_dim)\n    - W_g: Gate weight matrix of shape (input_dim, num_experts)\n    - W_noise: Noisy gate weight matrix of shape (input_dim, num_Experts)\n    - N: Pre-sampled noise of shape (batch_size, num_Experts)\n    - k: Sparsity constraint (top-k experts to activate)\n    \n    Returns:\n    - A list of final top-k gating probabilities with shape (batch_size, num_Experts)\n    \"\"\"\n    (batch_size, num_experts) = N.shape\n    logits = X @ W_g\n    logits_noise = X @ W_noise + N\n    logits_total = logits + logits_noise\n    gate_probs = np.softmax(logits_total, axis=1)\n    topk_indices = np.argpartition(-gate_probs, k, axis=1)[:, :k]\n    gate_probs_topk = np.zeros_like(gate_probs)\n    for i in range(batch_size):\n        gate_probs_topk[i, topk_indices[i]] = gate_probs[i, topk_indices[i]]\n    gate_probs_topk /= np.sum(gate_probs_topk, axis=1, keepdims=True)\n    gate_probs_topk = np.round(gate_probs_topk, 4)\n    return gate_probs_topk.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Mixture-of-Experts (MoE) layer with top-k routing.\n    \n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weight matrices of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weight matrix of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to select per token\n    \n    Returns:\n    - Output tensor of shape (batch_size, output_dim), rounded to 4 decimals.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    output = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        gate_logits = x[i] @ Wg\n        top_k_indices = np.argsort(gate_logits)[-top_k:][::-1]\n        top_k_logits = gate_logits[top_k_indices]\n        gate_weights = np.exp(top_k_logits - np.max(top_k_logits))\n        gate_weights /= gate_weights.sum()\n        for j in range(top_k):\n            expert_idx = top_k_indices[j]\n            expert_output = x[i] @ We[expert_idx]\n            output[i] += gate_weights[j] * expert_output\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor of shape (B, C, H, W).\n    \n    Parameters:\n        X (np.ndarray): Input tensor of shape (B, C, H, W)\n        gamma (np.ndarray): Scale parameter of shape (C,)\n        beta (np.ndarray): Shift parameter of shape (C,)\n        num_groups (int): Number of groups to divide channels into\n        epsilon (float): Small value to avoid division by zero\n    \n    Returns:\n        list: Normalized output as a Python list, rounded to 4 decimal places.\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by number of groups.'\n    group_size = C // num_groups\n    X_grouped = X.reshape(B, num_groups, group_size, H, W)\n    mean = np.mean(X_grouped, axis=(2, 3, 4))\n    var = np.var(X_grouped, axis=(2, 3, 4))\n    mean = mean[:, :, np.newaxis, np.newaxis, np.newaxis]\n    var = var[:, :, np.newaxis, np.newaxis, np.newaxis]\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    X_normalized = X_normalized * gamma + beta\n    return np.round(X_normalized, 4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        if abs(gradient) < tolerance:\n            break\n        x = x - learning_rate * gradient\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) activation function.\n    \n    Parameters:\n    - x (np.ndarray): Input tensor.\n    - alpha (float): Scaling parameter.\n    - gamma (float): Scaling factor for output.\n    - beta (float): Shifting factor for output.\n    \n    Returns:\n    - list[float]: Output after applying Dynamic Tanh, rounded to 4 decimal places.\n    \"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    x_normalized = (x - mean) / (std + 1e-08)\n    dyt_output = gamma * np.tanh(alpha * x_normalized) + beta\n    dyt_output_rounded = np.round(dyt_output, 4)\n    return dyt_output_rounded.tolist()"}
